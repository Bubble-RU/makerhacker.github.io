<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-12" href="#">jmlr2009-12</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</h1>
<br/><p>Source: <a title="jmlr-2009-12-pdf" href="http://jmlr.org/papers/volume10/rosset09a/rosset09a.pdf">pdf</a></p><p>Author: Saharon Rosset</p><p>Abstract: We show how to follow the path of cross validated solutions to families of regularized optimization problems, deﬁned by a combination of a parameterized loss function and a regularization term. A primary example is kernel quantile regression, where the parameter of the loss function is the quantile being estimated. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on real and artiﬁcial data. This algorithm allows us to efﬁciently solve the whole family of bi-level problems. We show how it can be extended to cover other modeling problems, like support vector regression, and alternative in-sample model selection approaches.1</p><p>Reference: <a title="jmlr-2009-12-reference" href="../jmlr2009_reference/jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A primary example is kernel quantile regression, where the parameter of the loss function is the quantile being estimated. [sent-5, score-0.933]
</p><p>2 Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. [sent-6, score-0.536]
</p><p>3 A major motivating example for this setting is regularized quantile regression (Koenker, 2005). [sent-30, score-0.474]
</p><p>4 (2) c  Because quantile loss has this optimizer, the solution of the quantile regression problems for the whole range 0 < τ < 1 has often been advocated as an approach to estimating the full conditional probability of P(Y |X) (Koenker, 2005; Perlich et al. [sent-32, score-0.983]
</p><p>5 , 2007); and work on practical uses of extreme quantile estimation for data mining applications Perlich et al. [sent-40, score-0.446]
</p><p>6 Figure 1 shows a graphical representation of Lτ for several values of τ, and a demonstration of the conditional quantile curves in a univariate regression setting, where the linear model is correct for the median, but the noise has a non-homoscedastic distribution. [sent-42, score-0.474]
</p><p>7 In addition to parameterized quantile regression, there are other modeling problems in the literature which combine a parameterized loss function problem with the existence of efﬁcient path following algorithms. [sent-62, score-0.704]
</p><p>8 (2008) for the support vector machine, which is very similar to quantile regression from an optimization perspective (piecewise linear objective with quadratic penalty). [sent-94, score-0.474]
</p><p>9 The concept of a parameterized family of bi-level regularized quantile regression problems is demonstrated in Figure 2, where we see the cross-validation curves of the objective of (4) as a function of λ for several values of τ on the same data set. [sent-97, score-0.513]
</p><p>10 As we can see, the optimal level of regularization varies with the quantile, and correct choice of the regularization parameter can have a signiﬁcant effect on the success of our quantile prediction model. [sent-98, score-0.52]
</p><p>11 We concentrate our attention on the quantile regression case (both kernelized and linear), as one where the parameterized-loss problem is well motivated and historically useful, but we also discuss the similarities and differences in algorithms for the other examples we mentioned above. [sent-103, score-0.474]
</p><p>12 In Section 2 we discuss the properties of the quantile regression solution paths fˆ(τ, λ) and their evolution as τ changes. [sent-130, score-0.546]
</p><p>13 This is because the optimal solution always corresponds to a situation where either one of the validation points is crossing the non-differentiability elbow in the cross validation loss Lcv , or the regularization path is going thorough a knot in its piecewise linear change. [sent-132, score-1.484]
</p><p>14 Quantile Regression Solution Paths We concentrate our discussion on the kernel quantile regression (KQR) formulation in (3), with the understanding that it subsumes the linear formulation (1) with ℓ2 regularization by using the linear ˜ ˜ kernel K(x, x) = xT x. [sent-139, score-0.511]
</p><p>15 (5)  For a proposed solution f (x) deﬁne: • E = {i : yi − f (xi ) = 0} (points on elbow of Lτ ) • L = {i : yi − f (xi ) < 0} (left of elbow) • R = {i : yi − f (xi ) > 0} (right of elbow). [sent-145, score-0.342]
</p><p>16 The knots λk are points on the path where an observation passes between E and either L or R , that is ∃i ∈ E such that exactly θi = τ or θi = −(1 − τ). [sent-152, score-0.503]
</p><p>17 Then in the range Γ = {λ ≥ 0 : 0 < τ(λ) < 1} there exist knots λ0 < . [sent-172, score-0.389]
</p><p>18 Armed with this result, we next show the main result of this section: that the knots themselves move in a (piecewise) straight line as τ changes, and can therefore be tracked as τ and the regularization path change. [sent-180, score-0.568]
</p><p>19 Fix a quantile τ0 and assume that λk is a knot in the λ-solution path for quantile τ0 . [sent-181, score-1.487]
</p><p>20 Further, let ik be the observation that is passing in or out of the elbow at knot λk . [sent-182, score-0.685]
</p><p>21 Theorem 2 Any knot λk moves linearly as τ changes. [sent-187, score-0.481]
</p><p>22 That is, there exists a constant ck such that for quantile τ0 + δ there is a knot in the λ-solution path at λk + ck δ, for δ ∈ [−εk , νk ], a non-empty neighborhood of 0. [sent-188, score-1.199]
</p><p>23 ROSSET  And the ﬁt at this knot progresses as 1 λk fˆ(λk , τ0 )(x) + δhk (x) λk + ck δ ˜i bk K(x, xi ) + ∑ K(x, xi ) . [sent-190, score-0.675]
</p><p>24 fˆ(τ0 + δ, λk + ck δ) = ˜0 hk (x) = bk +  ∑  i∈Ek −ik  (7) (8)  i∈L ∪R ∪{ik }  Proof For small δ, the modiﬁed knot should be characterized by θik = τ0 +δ, and L , R , E remaining the same. [sent-191, score-0.714]
</p><p>25 If we can ﬁnd a ck such that this holds for quantile τ0 + δ and λ = λk + ck δ, and also the KKT conditions are maintained, we have our knot for quantile τ0 + δ. [sent-192, score-1.531]
</p><p>26 This theorem tells us that we can in fact track the knots in the solution efﬁciently as τ changes. [sent-200, score-0.411]
</p><p>27 We still have to account for various types of events that can change the direction the knot is moving in. [sent-201, score-0.52]
</p><p>28 These events correspond to knots crossings, that is, the knot λk is encountering another knot (which is tracking the other event). [sent-203, score-1.417]
</p><p>29 There are also knot birth events, and knots merge events, which are possible but rare, and somewhat counter-intuitive. [sent-204, score-0.963]
</p><p>30 When any of these events occurs, the set of knots has to be updated and their directions have to be re-calculated using Lemma 1, Theorem 2 and the new identity of the sets E , L , R and the observation ik . [sent-206, score-0.501]
</p><p>31 The Bi-level Optimization Problem Our next task is to show how our ability to track the knots as τ changes allows us to track the solution of the bi-level optimization Problem (4) as τ changes. [sent-209, score-0.411]
</p><p>32 Theorem 3 When the cross validation loss is the quantile loss (i. [sent-211, score-0.755]
</p><p>33 , LCV = Lτ ), then any minimizer3 of (4) is always either at a knot in the λ-path for this τ or a point where a validation observation crosses the elbow. [sent-213, score-0.66]
</p><p>34 ’s now combines both knots and validation crossings), and we take advantage of the representation in (6). [sent-226, score-0.568]
</p><p>35 , no validation crossing occurs) and hk is ﬁxed (i. [sent-229, score-0.492]
</p><p>36 Therefore any local (or global) extremum must be at a knot or a validation crossing. [sent-232, score-0.66]
</p><p>37 Corollary 4 Given the complete solution path for τ = τ0 , the solutions of the bi-level Problem (4) for a range of quantiles around τ0 can be obtained by following the paths of the knots and the validation crossings only, as τ changes. [sent-233, score-1.036]
</p><p>38 How do we determine which one of the knots and validation crossings is going to be optimal for every value of τ? [sent-237, score-0.745]
</p><p>39 The ﬁrst question is easy to answer when we consider the similarity between the knot following problem we solve in Theorem 2 and the validation crossing following problem. [sent-238, score-0.888]
</p><p>40 2481  ROSSET  ˆ have a set of elbow observations whose ﬁt must remain ﬁxed as τ changes, but whose θ values may ˆ vary; sets L , R whose θ are changing in a pre-determined manner with τ, but whose ﬁt may vary freely; and one special observation which characterizes the knot or validation crossing. [sent-242, score-0.837]
</p><p>41 The only difference is that in a knot this is a border observation from the training set, so both its ﬁt and its ˆ θ are pre-determined, while in the case of validation crossing it is a validation observation, whose ˆ ﬁt must remain ﬁxed (at the elbow), but which does not even have a θ value. [sent-243, score-1.095]
</p><p>42 Assume there is a validation crossing at λv for quantile τ0 , and that validation set observation jv is the one crossing the elbow, that is, fˆ(τ0 , λv )(˜ jv ) = 0 . [sent-245, score-1.332]
</p><p>43 That is, there exists a constant dv such that for quantile τ0 + δ there is a validation crossing in the λ-solution path at λv + dv δ, for δ ∈ [−εv , νv ], a non-empty neighborhood of 0. [sent-249, score-0.967]
</p><p>44 , Lτ on the validation set) at every knot and validation crossing in terms of δ, so we can compare them and identify the optimum at every value of δ. [sent-255, score-1.067]
</p><p>45 ˜ i∈R  ˜ i∈L  2482  B I -L EVEL PATH F OLLOWING  A similar expression can be derived for validation crossings (with f k , ck , hk replaced by f v , dv , hv in the obvious way). [sent-257, score-0.542]
</p><p>46 We also have to re-calculate some of the validation loss scores whenever an event happens on the training solution path (like a knot crossing). [sent-264, score-0.873]
</p><p>47 To calculate the meeting point of two elements with neighboring scores (assume WLOG that they are two knots k, l) we ﬁnd the zeros of the cubic equation obtained by requiring equality for the two rational functions of the form (10) corresponding to the two elements. [sent-265, score-0.435]
</p><p>48 Figure 3 gives a simple illustration of the process of following the validation loss scores, and identifying their optimum, while updating the directions of the knots and validation crossings as events occur. [sent-268, score-1.028]
</p><p>49 It shows the set of training and validation loss scores for two knots and the validation loss only for one validation crossing. [sent-269, score-1.008]
</p><p>50 As we can see, in this example the ﬁrst (left) switch is between two knots as a result of a knot crossing, while the second (right) is a result of a validation crossing becoming optimal. [sent-272, score-1.277]
</p><p>51 It should be noted, 2483  12  13  ROSSET  knot 1 − train knot 1 − CV knot 2 − train knot 2 − CV valid. [sent-273, score-1.924]
</p><p>52 37  τ  Figure 3: Illustration of the process of lower envelope tracking, in the presence of two knots and one validation crossing being tracked. [sent-282, score-0.826]
</p><p>53 The algorithm follows the knots of the λ-solution path as τ changes using the results of Section 2, and keeps track of the cross-validated solution using the results of Section 3. [sent-288, score-0.525]
</p><p>54 Every time an event happens (like a knot crossing), the direction in which two of the knots are moving has to be changed, or knots have to be added or deleted. [sent-289, score-1.295]
</p><p>55 Between these events, the evolution of the crossvalidation objective at all knots and validation crossings has to be sorted and followed. [sent-290, score-0.768]
</p><p>56 In our case the outer loop of Algorithm 1 implements a 2-dimensional path following problem, that can be thought of as following O(n) 1-dimensional paths traversed by the knots of the path. [sent-297, score-0.53]
</p><p>57 Each iteration of either loop requires a re-calculation of up to three directions (of knots or validation crossings), using Theorem 2 or Proposition 5. [sent-305, score-0.592]
</p><p>58 1 Support Vector Regression and Weighted Support Vector Machines One possible view of regularized ε-SVR (Smola and Sch¨ lkopf, 2004) is similar to the quantile o regression problem for τ = 0. [sent-321, score-0.474]
</p><p>59 Because of the fundamental similarity in the optimization setting, all our results regarding behavior of λ-paths and knots as τ changes in quantile regression (e. [sent-329, score-0.863]
</p><p>60 , Theorem 2) can be adapted in a reasonably straight forward manner to follow paths and knots of λ-solution paths in ε-SVR, as ε varies. [sent-331, score-0.471]
</p><p>61 There is, however, a fundamental difference in the statistical setting between parameterized quantile loss and parameterized ε-SVR loss. [sent-332, score-0.565]
</p><p>62 While every quantile loss function deﬁnes an interesting modeling problem of estimation of a given conditional quantile, there is no such clear motivation for varying ε. [sent-333, score-0.512]
</p><p>63 5 (the symmetric quantile loss, sometimes called absolute loss), then our observations on the bi-level path following problem (e. [sent-339, score-0.582]
</p><p>64 The loss function is differentiable, there is no concept of elbow (although 2487  ROSSET  there are still knots), the KKT conditions are quite different, and if we also use a differentiable Lcv , the cross validation procedure would be affect as well. [sent-352, score-0.423]
</p><p>65 This is guaranteed by the following simple result: Proposition 6 For any ﬁxed τ, the minimizer λ∗ (τ) of SIC, GACV and any similar model selection criterion which is monotone in both ∑n Lτ (yi − fˆ(τ, λ)(xi )) and |E |, is always at one of the knots i=1 of the solution path. [sent-364, score-0.411]
</p><p>66 Thus, if we wish to use SIC or similar measures for selecting λ∗ (τ), the inner loop of Algorithm 1 (lines 12-21) can be omitted and replaced with a simple tracking of the value of SIC at the knots that are being followed. [sent-368, score-0.416]
</p><p>67 However, we can offer a partial remedy to the quantile crossing problem through observation of the guaranteed sub-optimality of the resulting solutions, and a consequent envelope tracking modiﬁcation. [sent-377, score-0.731]
</p><p>68 ˜ x ˜ x  Thus, we can improve our holdout performance at either quantile τ0 or τ1 by appropriately enforcing monotonicity. [sent-401, score-0.446]
</p><p>69 In terms of practical implications, it is easy to see (though not trivial to implement) how our algorithm can be extended to identify quantile crossings. [sent-403, score-0.446]
</p><p>70 When these occur, at least one knot will be moving in the ‘wrong direction’, that is, the expression in (7) will be decreasing in δ. [sent-404, score-0.481]
</p><p>71 Experiments Our methodology offers a new approach for generating the full set of cross-validated kernel quantile regression models. [sent-408, score-0.474]
</p><p>72 In the left panel, we see that the quantile estimates all capture the general shape of the true curve, with some “smoothing” due to regularization. [sent-454, score-0.446]
</p><p>73 A confounding factor in this analysis is the fact that the scale of quantile error need not be comparable for different quantiles. [sent-459, score-0.446]
</p><p>74 This smoothness is in fact guaranteed for the validation loss Lcv , since it is easily seen that the “jumps” are points where validation loss is equal at two knots or validation crossings. [sent-465, score-1.008]
</p><p>75 Our conjecture that the number of knots in the 2-dimensional path behaves like O(n2 ) seems to be consistent with these results, as is the hypothesized overall time complexity dependence of O(n3 ). [sent-490, score-0.503]
</p><p>76 In the context of house prices, we can think of estimating a high (but not extreme4 ) conditional quantile as the seller’s search for a favorable bargaining position in negotiations. [sent-503, score-0.495]
</p><p>77 Similarly for salaries, estimating a high conditional quantile can serve as a measure of what an employee can expect to receive optimistically (but still realistically), given his characteristics and performance. [sent-504, score-0.446]
</p><p>78 Extreme quantile estimation is also of interest in some contexts, but we do not demonstrate it here due to the inherent statistical difﬁculty and questionable results, see some discussion in Conclusion section. [sent-507, score-0.446]
</p><p>79 We can see that the selected ﬁts using CV and SIC are quite similar, with possible exception to the more jumpy ﬁt selected by SIC for quantile 0. [sent-551, score-0.488]
</p><p>80 Formulation of a practical algorithm for generating the full set of cross-validated solutions for the family of kernel quantile regression problems. [sent-559, score-0.5]
</p><p>81 As we have mentioned, modeling with the quantile loss function Lτ leads to estimation of the τth quantile of P(Y |x) in the decision theoretic sense that the population optimizer of the loss function is this quantile (see Equation 2). [sent-563, score-1.445]
</p><p>82 This can be thought of in terms of bias and variance, where the model generating quantile η is similar enough to the one for quantile τ (i. [sent-566, score-0.892]
</p><p>83 A particularly important and difﬁcult type of quantile estimation problems pertains to estimation of extreme quantiles (e. [sent-570, score-0.548]
</p><p>84 We also emphasize the aspects of the algorithm not covered in technical detail in the main text, such as the differentiation of different types of events (knot crossing, knot merging, knot splitting). [sent-601, score-1.001]
</p><p>85 2497  ROSSET  Algorithm 2: Algorithm description Input: The entire λ-solution path for quantile τ0 characterized by its m knots λ = λ1 , . [sent-614, score-0.949]
</p><p>86 ≤ lm+v , where: rk is an indicator in {knot, valx} whether this is a knot or a validation crossing λk is its “location” on the path lk is its cross validation loss hk is its direction τk is knot meeting point 1 Find knot directions h1 , . [sent-653, score-2.419]
</p><p>87 , hm according to Theorem 2; /* Identify all validation crossings in the solution path for τ0 2 Set V = Φ the empty set; 3 for k = 1, . [sent-656, score-0.492]
</p><p>88 , m do Calculate knot validation loss: lk = ∑N Lτ0 ( fˆ(τ0 , λk )(˜ i ), yi ); x ˜ i=1 end for k = 1, . [sent-665, score-0.747]
</p><p>89 , v do ˜ x ˜ Calculate validation crossing loss: lm+k = ∑N Lτ0 ( fˆ(τ0 , λk )(˜ i ), yi ); i=1 end Create list M = {(rk , λk , lk , hk , τk ) : k = 1, . [sent-668, score-0.606]
</p><p>90 ,m−1 {˜ k : τk > 0} ; τ ˜ ˜ ˜ ˜ 3 Set τnew = τknew if τknew = τknew +1 then type=merge ; 4 /* Knots merging 3 ⇒ 1 */ 5 else 6 /* Two knots crossing */ type=cross; 7 end /* Next observation-knot crossing = knot birth */ 8 for k = 1, . [sent-689, score-1.403]
</p><p>91 τk ; /* Now find the next time a validation observation hits a knot ⇒ new validation crossing */ fˆ(τ  4 5 6 7 8 9 10 11 12 13 14  15 16 17  18 19  20 21 22 23 24 25 26  ,M. [sent-710, score-1.141]
</p><p>92 ,m {∆τ(i, k) : ∆τ(i, k) > 0}; ˜ if τnow > τnew and τ > τnew then /* No validation event before τnew */ τnow = τnew; return; ˜ else if τnow > τ then /* New validation xing appears---add it to list */ ˜ = (˜ − τkeep )M. [sent-732, score-0.444]
</p><p>93 Two of k, k + 1, k + 2 are validation crossings of the same observation, the knot is the third involved in the crossing. [sent-735, score-0.837]
</p><p>94 rk = knot then 5 remove entries k + 1, k + 2 from M; 6 else if M. [sent-740, score-0.504]
</p><p>95 rk+1 = knot then 7 remove entries k, k + 2 from M ; 8 else /* M. [sent-741, score-0.504]
</p><p>96 rk+2 = knot */ 9 remove entries k, k + 1 from M ; 10 Update M. [sent-742, score-0.481]
</p><p>97 The effect of school quality on student performance: A quantile regression approach. [sent-771, score-0.474]
</p><p>98 High quantile modeling for customer wallet estimation with other applications. [sent-846, score-0.471]
</p><p>99 Bi-level path following for cross validated solution of kernel quantile regression. [sent-850, score-0.63]
</p><p>100 Nonparametric conditional density estimation using piecewise-linear solution path of kernel quantile regression. [sent-903, score-0.582]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('knot', 0.481), ('quantile', 0.446), ('knots', 0.389), ('crossing', 0.228), ('validation', 0.179), ('crossings', 0.177), ('elbow', 0.155), ('sic', 0.155), ('rosset', 0.124), ('lcv', 0.12), ('path', 0.114), ('evel', 0.113), ('ollowing', 0.113), ('kqr', 0.106), ('quantiles', 0.102), ('hk', 0.085), ('ck', 0.079), ('takeuchi', 0.078), ('find', 0.074), ('knew', 0.074), ('bk', 0.069), ('koenker', 0.064), ('cv', 0.062), ('irel', 0.057), ('yi', 0.055), ('birth', 0.054), ('house', 0.049), ('salary', 0.049), ('ik', 0.049), ('cross', 0.048), ('meeting', 0.046), ('gacv', 0.042), ('itude', 0.042), ('jumpy', 0.042), ('latitud', 0.042), ('el', 0.042), ('ek', 0.042), ('loss', 0.041), ('parameterized', 0.039), ('events', 0.039), ('merge', 0.039), ('regularization', 0.037), ('zhu', 0.037), ('li', 0.037), ('event', 0.036), ('jv', 0.036), ('median', 0.036), ('ho', 0.035), ('lk', 0.032), ('rie', 0.032), ('ru', 0.032), ('kkt', 0.031), ('envelope', 0.03), ('lrk', 0.03), ('svr', 0.03), ('cl', 0.03), ('regression', 0.028), ('border', 0.028), ('huberized', 0.028), ('ice', 0.028), ('lak', 0.028), ('lok', 0.028), ('lrl', 0.028), ('perlich', 0.028), ('ns', 0.028), ('straight', 0.028), ('list', 0.027), ('paths', 0.027), ('tracking', 0.027), ('hastie', 0.026), ('sec', 0.026), ('solutions', 0.026), ('modeling', 0.025), ('directions', 0.024), ('kek', 0.024), ('xi', 0.023), ('evolution', 0.023), ('else', 0.023), ('solution', 0.022), ('dy', 0.022), ('observations', 0.022), ('bv', 0.022), ('hv', 0.022), ('update', 0.021), ('pe', 0.021), ('birel', 0.021), ('gunther', 0.021), ('inew', 0.021), ('knotcross', 0.021), ('latitude', 0.021), ('lek', 0.021), ('lol', 0.021), ('longitude', 0.021), ('meetings', 0.021), ('rel', 0.021), ('sacramento', 0.021), ('sek', 0.021), ('unknowns', 0.021), ('updatevalidlist', 0.021), ('valx', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="12-tfidf-1" href="./jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</a></p>
<p>Author: Saharon Rosset</p><p>Abstract: We show how to follow the path of cross validated solutions to families of regularized optimization problems, deﬁned by a combination of a parameterized loss function and a regularization term. A primary example is kernel quantile regression, where the parameter of the loss function is the quantile being estimated. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on real and artiﬁcial data. This algorithm allows us to efﬁciently solve the whole family of bi-level problems. We show how it can be extended to cover other modeling problems, like support vector regression, and alternative in-sample model selection approaches.1</p><p>2 0.048894048 <a title="12-tfidf-2" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: We continue our recent study on constructing a reﬁnement kernel for a given kernel so that the reproducing kernel Hilbert space associated with the reﬁnement kernel contains that with the original kernel as a subspace. To motivate this study, we ﬁrst develop a reﬁnement kernel method for learning, which gives an efﬁcient algorithm for updating a learning predictor. Several characterizations of reﬁnement kernels are then presented. It is shown that a nontrivial reﬁnement kernel for a given kernel always exists if the input space has an inﬁnite cardinal number. Reﬁnement kernels for translation invariant kernels and Hilbert-Schmidt kernels are investigated. Various concrete examples are provided. Keywords: reproducing kernels, reproducing kernel Hilbert spaces, learning with kernels, reﬁnement kernels, translation invariant kernels, Hilbert-Schmidt kernels</p><p>3 0.035726253 <a title="12-tfidf-3" href="./jmlr-2009-Online_Learning_with_Samples_Drawn_from_Non-identical_Distributions.html">68 jmlr-2009-Online Learning with Samples Drawn from Non-identical Distributions</a></p>
<p>Author: Ting Hu, Ding-Xuan Zhou</p><p>Abstract: Learning algorithms are based on samples which are often drawn independently from an identical distribution (i.i.d.). In this paper we consider a different setting with samples drawn according to a non-identical sequence of probability distributions. Each time a sample is drawn from a different distribution. In this setting we investigate a fully online learning algorithm associated with a general convex loss function and a reproducing kernel Hilbert space (RKHS). Error analysis is conducted under the assumption that the sequence of marginal distributions converges polynomially in the dual of a H¨ lder space. For regression with least square or insensitive loss, learning rates are given o in both the RKHS norm and the L2 norm. For classiﬁcation with hinge loss and support vector machine q-norm loss, rates are explicitly stated with respect to the excess misclassiﬁcation error. Keywords: sampling with non-identical distributions, online learning, classiﬁcation with a general convex loss, regression with insensitive loss and least square loss, reproducing kernel Hilbert space</p><p>4 0.035367794 <a title="12-tfidf-4" href="./jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Stijn Goedertier, David Martens, Jan Vanthienen, Bart Baesens</p><p>Abstract: Process discovery is the automated construction of structured process models from information system event logs. Such event logs often contain positive examples only. Without negative examples, it is a challenge to strike the right balance between recall and speciﬁcity, and to deal with problems such as expressiveness, noise, incomplete event logs, or the inclusion of prior knowledge. In this paper, we present a conﬁgurable technique that deals with these challenges by representing process discovery as a multi-relational classiﬁcation problem on event logs supplemented with Artiﬁcially Generated Negative Events (AGNEs). This problem formulation allows using learning algorithms and evaluation techniques that are well-know in the machine learning community. Moreover, it allows users to have a declarative control over the inductive bias and language bias. Keywords: graph pattern discovery, inductive logic programming, Petri net, process discovery, positive data only</p><p>5 0.03312831 <a title="12-tfidf-5" href="./jmlr-2009-Computing_Maximum_Likelihood_Estimates_in_Recursive_Linear_Models_with_Correlated_Errors.html">17 jmlr-2009-Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors</a></p>
<p>Author: Mathias Drton, Michael Eichler, Thomas S. Richardson</p><p>Abstract: In recursive linear models, the multivariate normal joint distribution of all variables exhibits a dependence structure induced by a recursive (or acyclic) system of linear structural equations. These linear models have a long tradition and appear in seemingly unrelated regressions, structural equation modelling, and approaches to causal inference. They are also related to Gaussian graphical models via a classical representation known as a path diagram. Despite the models’ long history, a number of problems remain open. In this paper, we address the problem of computing maximum likelihood estimates in the subclass of ‘bow-free’ recursive linear models. The term ‘bow-free’ refers to the condition that the errors for variables i and j be uncorrelated if variable i occurs in the structural equation for variable j. We introduce a new algorithm, termed Residual Iterative Conditional Fitting (RICF), that can be implemented using only least squares computations. In contrast to existing algorithms, RICF has clear convergence properties and yields exact maximum likelihood estimates after the ﬁrst iteration whenever the MLE is available in closed form. Keywords: linear regression, maximum likelihood estimation, path diagram, structural equation model, recursive semi-Markov model, residual iterative conditional ﬁtting</p><p>6 0.032945983 <a title="12-tfidf-6" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>7 0.032567397 <a title="12-tfidf-7" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>8 0.032051485 <a title="12-tfidf-8" href="./jmlr-2009-The_P-Norm_Push%3A_A_Simple_Convex_Ranking_Algorithm_that_Concentrates_at_the_Top_of_the_List.html">95 jmlr-2009-The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List</a></p>
<p>9 0.028805897 <a title="12-tfidf-9" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>10 0.027409378 <a title="12-tfidf-10" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>11 0.025572602 <a title="12-tfidf-11" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>12 0.025541496 <a title="12-tfidf-12" href="./jmlr-2009-Incorporating_Functional_Knowledge_in_Neural_Networks.html">42 jmlr-2009-Incorporating Functional Knowledge in Neural Networks</a></p>
<p>13 0.02498213 <a title="12-tfidf-13" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>14 0.024901127 <a title="12-tfidf-14" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>15 0.024594795 <a title="12-tfidf-15" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>16 0.024118112 <a title="12-tfidf-16" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>17 0.023216035 <a title="12-tfidf-17" href="./jmlr-2009-Properties_of_Monotonic_Effects_on_Directed_Acyclic_Graphs.html">74 jmlr-2009-Properties of Monotonic Effects on Directed Acyclic Graphs</a></p>
<p>18 0.022696137 <a title="12-tfidf-18" href="./jmlr-2009-Subgroup_Analysis_via_Recursive_Partitioning.html">91 jmlr-2009-Subgroup Analysis via Recursive Partitioning</a></p>
<p>19 0.022615058 <a title="12-tfidf-19" href="./jmlr-2009-Online_Learning_with_Sample_Path_Constraints.html">67 jmlr-2009-Online Learning with Sample Path Constraints</a></p>
<p>20 0.022559056 <a title="12-tfidf-20" href="./jmlr-2009-A_Least-squares_Approach_to_Direct_Importance_Estimation.html">1 jmlr-2009-A Least-squares Approach to Direct Importance Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.125), (1, -0.033), (2, 0.061), (3, -0.001), (4, -0.057), (5, 0.026), (6, -0.025), (7, 0.013), (8, 0.048), (9, 0.037), (10, 0.003), (11, -0.008), (12, 0.049), (13, -0.027), (14, 0.062), (15, -0.047), (16, -0.096), (17, 0.031), (18, -0.047), (19, -0.078), (20, -0.012), (21, -0.028), (22, -0.025), (23, -0.151), (24, 0.185), (25, 0.114), (26, -0.108), (27, -0.089), (28, 0.256), (29, -0.206), (30, -0.116), (31, -0.138), (32, 0.013), (33, -0.072), (34, -0.039), (35, -0.22), (36, 0.141), (37, -0.231), (38, -0.371), (39, 0.012), (40, 0.08), (41, 0.13), (42, -0.225), (43, -0.25), (44, -0.117), (45, 0.139), (46, -0.092), (47, 0.181), (48, 0.089), (49, 0.17)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95519185 <a title="12-lsi-1" href="./jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</a></p>
<p>Author: Saharon Rosset</p><p>Abstract: We show how to follow the path of cross validated solutions to families of regularized optimization problems, deﬁned by a combination of a parameterized loss function and a regularization term. A primary example is kernel quantile regression, where the parameter of the loss function is the quantile being estimated. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on real and artiﬁcial data. This algorithm allows us to efﬁciently solve the whole family of bi-level problems. We show how it can be extended to cover other modeling problems, like support vector regression, and alternative in-sample model selection approaches.1</p><p>2 0.21567667 <a title="12-lsi-2" href="./jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Stijn Goedertier, David Martens, Jan Vanthienen, Bart Baesens</p><p>Abstract: Process discovery is the automated construction of structured process models from information system event logs. Such event logs often contain positive examples only. Without negative examples, it is a challenge to strike the right balance between recall and speciﬁcity, and to deal with problems such as expressiveness, noise, incomplete event logs, or the inclusion of prior knowledge. In this paper, we present a conﬁgurable technique that deals with these challenges by representing process discovery as a multi-relational classiﬁcation problem on event logs supplemented with Artiﬁcially Generated Negative Events (AGNEs). This problem formulation allows using learning algorithms and evaluation techniques that are well-know in the machine learning community. Moreover, it allows users to have a declarative control over the inductive bias and language bias. Keywords: graph pattern discovery, inductive logic programming, Petri net, process discovery, positive data only</p><p>3 0.18915074 <a title="12-lsi-3" href="./jmlr-2009-Computing_Maximum_Likelihood_Estimates_in_Recursive_Linear_Models_with_Correlated_Errors.html">17 jmlr-2009-Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors</a></p>
<p>Author: Mathias Drton, Michael Eichler, Thomas S. Richardson</p><p>Abstract: In recursive linear models, the multivariate normal joint distribution of all variables exhibits a dependence structure induced by a recursive (or acyclic) system of linear structural equations. These linear models have a long tradition and appear in seemingly unrelated regressions, structural equation modelling, and approaches to causal inference. They are also related to Gaussian graphical models via a classical representation known as a path diagram. Despite the models’ long history, a number of problems remain open. In this paper, we address the problem of computing maximum likelihood estimates in the subclass of ‘bow-free’ recursive linear models. The term ‘bow-free’ refers to the condition that the errors for variables i and j be uncorrelated if variable i occurs in the structural equation for variable j. We introduce a new algorithm, termed Residual Iterative Conditional Fitting (RICF), that can be implemented using only least squares computations. In contrast to existing algorithms, RICF has clear convergence properties and yields exact maximum likelihood estimates after the ﬁrst iteration whenever the MLE is available in closed form. Keywords: linear regression, maximum likelihood estimation, path diagram, structural equation model, recursive semi-Markov model, residual iterative conditional ﬁtting</p><p>4 0.18895654 <a title="12-lsi-4" href="./jmlr-2009-Properties_of_Monotonic_Effects_on_Directed_Acyclic_Graphs.html">74 jmlr-2009-Properties of Monotonic Effects on Directed Acyclic Graphs</a></p>
<p>Author: Tyler J. VanderWeele, James M. Robins</p><p>Abstract: Various relationships are shown hold between monotonic effects and weak monotonic effects and the monotonicity of certain conditional expectations. Counterexamples are provided to show that the results do not hold under less restrictive conditions. Monotonic effects are furthermore used to relate signed edges on a causal directed acyclic graph to qualitative effect modiﬁcation. The theory is applied to an example concerning the direct effect of smoking on cardiovascular disease controlling for hypercholesterolemia. Monotonicity assumptions are used to construct a test for whether there is a variable that confounds the relationship between the mediator, hypercholesterolemia, and the outcome, cardiovascular disease. Keywords: Bayesian networks, conditional expectation, covariance, directed acyclic graphs, effect modiﬁcation, monotonicity</p><p>5 0.17923529 <a title="12-lsi-5" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: We continue our recent study on constructing a reﬁnement kernel for a given kernel so that the reproducing kernel Hilbert space associated with the reﬁnement kernel contains that with the original kernel as a subspace. To motivate this study, we ﬁrst develop a reﬁnement kernel method for learning, which gives an efﬁcient algorithm for updating a learning predictor. Several characterizations of reﬁnement kernels are then presented. It is shown that a nontrivial reﬁnement kernel for a given kernel always exists if the input space has an inﬁnite cardinal number. Reﬁnement kernels for translation invariant kernels and Hilbert-Schmidt kernels are investigated. Various concrete examples are provided. Keywords: reproducing kernels, reproducing kernel Hilbert spaces, learning with kernels, reﬁnement kernels, translation invariant kernels, Hilbert-Schmidt kernels</p><p>6 0.17734341 <a title="12-lsi-6" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>7 0.16779347 <a title="12-lsi-7" href="./jmlr-2009-Incorporating_Functional_Knowledge_in_Neural_Networks.html">42 jmlr-2009-Incorporating Functional Knowledge in Neural Networks</a></p>
<p>8 0.16089785 <a title="12-lsi-8" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>9 0.15389836 <a title="12-lsi-9" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>10 0.14512919 <a title="12-lsi-10" href="./jmlr-2009-The_P-Norm_Push%3A_A_Simple_Convex_Ranking_Algorithm_that_Concentrates_at_the_Top_of_the_List.html">95 jmlr-2009-The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List</a></p>
<p>11 0.14197253 <a title="12-lsi-11" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>12 0.13910189 <a title="12-lsi-12" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>13 0.13403825 <a title="12-lsi-13" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>14 0.13284859 <a title="12-lsi-14" href="./jmlr-2009-An_Analysis_of_Convex_Relaxations_for_MAP_Estimation_of_Discrete_MRFs.html">7 jmlr-2009-An Analysis of Convex Relaxations for MAP Estimation of Discrete MRFs</a></p>
<p>15 0.12395725 <a title="12-lsi-15" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>16 0.11946106 <a title="12-lsi-16" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>17 0.11918235 <a title="12-lsi-17" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>18 0.11635871 <a title="12-lsi-18" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>19 0.11440995 <a title="12-lsi-19" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>20 0.11192644 <a title="12-lsi-20" href="./jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">99 jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.03), (11, 0.037), (19, 0.014), (26, 0.016), (38, 0.031), (47, 0.019), (52, 0.039), (55, 0.043), (58, 0.021), (66, 0.116), (68, 0.025), (90, 0.051), (96, 0.034), (99, 0.435)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.69052529 <a title="12-lda-1" href="./jmlr-2009-Adaptive_False_Discovery_Rate_Control_under_Independence_and_Dependence.html">5 jmlr-2009-Adaptive False Discovery Rate Control under Independence and Dependence</a></p>
<p>Author: Gilles Blanchard,  Étienne Roquain</p><p>Abstract: In the context of multiple hypothesis testing, the proportion π0 of true null hypotheses in the pool of hypotheses to test often plays a crucial role, although it is generally unknown a priori. A testing procedure using an implicit or explicit estimate of this quantity in order to improve its efﬁcency is called adaptive. In this paper, we focus on the issue of false discovery rate (FDR) control and we present new adaptive multiple testing procedures with control of the FDR. In a ﬁrst part, assuming independence of the p-values, we present two new procedures and give a uniﬁed review of other existing adaptive procedures that have provably controlled FDR. We report extensive simulation results comparing these procedures and testing their robustness when the independence assumption is violated. The new proposed procedures appear competitive with existing ones. The overall best, though, is reported to be Storey’s estimator, albeit for a speciﬁc parameter setting that does not appear to have been considered before. In a second part, we propose adaptive versions of step-up procedures that have provably controlled FDR under positive dependence and unspeciﬁed dependence of the p-values, respectively. In the latter case, while simulations only show an improvement over non-adaptive procedures in limited situations, these are to our knowledge among the ﬁrst theoretically founded adaptive multiple testing procedures that control the FDR when the p-values are not independent. Keywords: multiple testing, false discovery rate, adaptive procedure, positive regression dependence, p-values</p><p>same-paper 2 0.68189251 <a title="12-lda-2" href="./jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</a></p>
<p>Author: Saharon Rosset</p><p>Abstract: We show how to follow the path of cross validated solutions to families of regularized optimization problems, deﬁned by a combination of a parameterized loss function and a regularization term. A primary example is kernel quantile regression, where the parameter of the loss function is the quantile being estimated. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on real and artiﬁcial data. This algorithm allows us to efﬁciently solve the whole family of bi-level problems. We show how it can be extended to cover other modeling problems, like support vector regression, and alternative in-sample model selection approaches.1</p><p>3 0.32367435 <a title="12-lda-3" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Junning Li, Z. Jane Wang</p><p>Abstract: In real world applications, graphical statistical models are not only a tool for operations such as classiﬁcation or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-speciﬁed level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-speciﬁed level, and a heuristic modiﬁcation of the method is able to control the FDR more accurately around the user-speciﬁed level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models. Keywords: Bayesian networks, false discovery rate, PC algorithm, directed acyclic graph, skeleton</p><p>4 0.32103926 <a title="12-lda-4" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider regularized support vector machines (SVMs) and show that they are precisely equivalent to a new robust optimization formulation. We show that this equivalence of robust optimization and regularization has implications for both algorithms, and analysis. In terms of algorithms, the equivalence suggests more general SVM-like algorithms for classiﬁcation that explicitly build in protection to noise, and at the same time control overﬁtting. On the analysis front, the equivalence of robustness and regularization provides a robust optimization interpretation for the success of regularized SVMs. We use this new robustness interpretation of SVMs to give a new proof of consistency of (kernelized) SVMs, thus establishing robustness as the reason regularized SVMs generalize well. Keywords: robustness, regularization, generalization, kernel, support vector machine</p><p>5 0.32004395 <a title="12-lda-5" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>Author: Sanjoy Dasgupta, Adam Tauman Kalai, Claire Monteleoni</p><p>Abstract: We start by showing that in an active learning setting, the Perceptron algorithm needs Ω( ε12 ) labels to learn linear separators within generalization error ε. We then present a simple active learning algorithm for this problem, which combines a modiﬁcation of the Perceptron update with an adaptive ﬁltering rule for deciding which points to query. For data distributed uniformly over the unit ˜ sphere, we show that our algorithm reaches generalization error ε after asking for just O(d log 1 ) ε labels. This exponential improvement over the usual sample complexity of supervised learning had previously been demonstrated only for the computationally more complex query-by-committee algorithm. Keywords: active learning, perceptron, label complexity bounds, online learning</p><p>6 0.31943938 <a title="12-lda-6" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>7 0.3183274 <a title="12-lda-7" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>8 0.31749317 <a title="12-lda-8" href="./jmlr-2009-Bounded_Kernel-Based_Online_Learning.html">13 jmlr-2009-Bounded Kernel-Based Online Learning</a></p>
<p>9 0.31637684 <a title="12-lda-9" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>10 0.31428179 <a title="12-lda-10" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>11 0.31381911 <a title="12-lda-11" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>12 0.31372586 <a title="12-lda-12" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>13 0.31189162 <a title="12-lda-13" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>14 0.3115668 <a title="12-lda-14" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>15 0.3106702 <a title="12-lda-15" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>16 0.3105689 <a title="12-lda-16" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>17 0.30990219 <a title="12-lda-17" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>18 0.30982748 <a title="12-lda-18" href="./jmlr-2009-Learning_Linear_Ranking_Functions_for_Beam_Search_with_Application_to_Planning.html">47 jmlr-2009-Learning Linear Ranking Functions for Beam Search with Application to Planning</a></p>
<p>19 0.30960801 <a title="12-lda-19" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>20 0.30945978 <a title="12-lda-20" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
