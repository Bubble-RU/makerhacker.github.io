<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 jmlr-2009-Model Monitor (M2): Evaluating, Comparing, and Monitoring Models    (Machine Learning Open Source Software Paper)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-56" href="#">jmlr2009-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 jmlr-2009-Model Monitor (M2): Evaluating, Comparing, and Monitoring Models    (Machine Learning Open Source Software Paper)</h1>
<br/><p>Source: <a title="jmlr-2009-56-pdf" href="http://jmlr.org/papers/volume10/raeder09a/raeder09a.pdf">pdf</a></p><p>Author: Troy Raeder, Nitesh V. Chawla</p><p>Abstract: This paper presents Model Monitor (M 2 ), a Java toolkit for robustly evaluating machine learning algorithms in the presence of changing data distributions. M 2 provides a simple and intuitive framework in which users can evaluate classiﬁers under hypothesized shifts in distribution and therefore determine the best model (or models) for their data under a number of potential scenarios. Additionally, M 2 is fully integrated with the WEKA machine learning environment, so that a variety of commodity classiﬁers can be used if desired. Keywords: machine learning, open-source software, distribution shift, scenario analysis</p><p>Reference: <a title="jmlr-2009-56-reference" href="../jmlr2009_reference/jmlr-2009-Model_Monitor_%28M2%29%3A_Evaluating%2C_Comparing%2C_and_Monitoring_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Department of Computer Science and Engineering University of Notre Dame Notre Dame, IN 46556, USA  Editor: Soeren Sonenberg  Abstract This paper presents Model Monitor (M 2 ), a Java toolkit for robustly evaluating machine learning algorithms in the presence of changing data distributions. [sent-6, score-0.309]
</p><p>2 M 2 provides a simple and intuitive framework in which users can evaluate classiﬁers under hypothesized shifts in distribution and therefore determine the best model (or models) for their data under a number of potential scenarios. [sent-7, score-0.612]
</p><p>3 Additionally, M 2 is fully integrated with the WEKA machine learning environment, so that a variety of commodity classiﬁers can be used if desired. [sent-8, score-0.034]
</p><p>4 Keywords: machine learning, open-source software, distribution shift, scenario analysis  1. [sent-9, score-0.072]
</p><p>5 However, a number of real-world applications present the challenge of a drift in distribution between training and testing (Qui˜ onero n et al. [sent-11, score-0.279]
</p><p>6 At the same time, the prevalent evaluation paradigm does not take into account the effect of changing distributions on the performance of the models being compared. [sent-13, score-0.16]
</p><p>7 We posit that it is important to effectively evaluate the generalization capacity of models or classiﬁers on a range of distribution divergences. [sent-14, score-0.115]
</p><p>8 We propose M 2 , a Java toolkit that is designed to help researchers and practitioners grapple with potential shifts in data distribution. [sent-15, score-0.581]
</p><p>9 The fundamental issues that we address in this toolkit are: a) which of several competing models is most robust to changing distributions? [sent-16, score-0.211]
</p><p>10 Detection of distribution shift: M 2 contains methods for isolating features whose distribution has changed signiﬁcantly between training and testing sets. [sent-20, score-0.243]
</p><p>11 Exploration of hypothetical scenarios: If a user is building a model on new data and expects the distribution to change over time, he or she can inject these changes into the test data and c 2009 Troy Raeder and Nitesh V. [sent-23, score-0.658]
</p><p>12 R AEDER AND C HAWLA  determine which classiﬁers1 (trained on current data) perform best on both the current and hypothetical future distributions. [sent-25, score-0.124]
</p><p>13 Empirical comparison of classiﬁers: M 2 can compare classiﬁers on performance across several data sets and distribution shifts. [sent-27, score-0.177]
</p><p>14 This capability allows researchers to objectively evaluate the robustness of the classiﬁers under distribution shift. [sent-28, score-0.234]
</p><p>15 This can guide the selection of a classiﬁer that is more robust to distributional drifts. [sent-29, score-0.14]
</p><p>16 Various cross-validation schemes are available, and we implement statistical tests, so that the user knows whether the observed differences are statistically signiﬁcant. [sent-30, score-0.208]
</p><p>17 M 2 is, to our knowledge, the only publicly-available software to tackle the important problem of learning under changing distributions. [sent-31, score-0.104]
</p><p>18 Most of the currently-implemented performance measures will compare the performance of the positive class (i. [sent-33, score-0.176]
</p><p>19 , class 1) against the performance on all other classes. [sent-35, score-0.049]
</p><p>20 org, and its distribution package contains a user’s guide, developer’s guide, JavaDoc documentation, and examples. [sent-37, score-0.072]
</p><p>21 To afford ﬂexibility to the user M 2 fully integrates with WEKA (Witten et al. [sent-38, score-0.283]
</p><p>22 Input data can either be in the WEKA format or the comma-delimited format of traditional UCI repository data sets (often called C4. [sent-40, score-0.232]
</p><p>23 Implementation M 2 is implemented in Java and is therefore fully cross-platform. [sent-43, score-0.069]
</p><p>24 It provides a Swing graphical user interface, and implements several different distribution shifts, methods of evaluation such as 10fold, 5x2 CV, etc. [sent-44, score-0.28]
</p><p>25 , performance measures, and statistical measures for the comparison of classiﬁers. [sent-45, score-0.127]
</p><p>26 Each of these critical components has been validated by a series of automated unit tests to insure correct operation and protect against regression. [sent-46, score-0.1]
</p><p>27 1 Performance Measures In addition to the standard performance measures (Accuracy, AUROC,Precision, Recall, F1 -Measure), we have implemented other performance and loss measures commonly used for binary classiﬁcation tasks, including Brier Score and Negative Cross Entropy. [sent-48, score-0.289]
</p><p>28 For any of the tasks described above, M 2 supports all of the standard validation measures including cross-validation (5-by-2 and 10-fold from the GUI, arbitrary conﬁgurations otherwise), splitting a data set into training and testing, and specifying an entirely separate ﬁle of test data. [sent-49, score-0.185]
</p><p>29 2 Distribution Shifts M 2 can introduce a number of different changes in distribution, including missing at random, missing not at random, random noise, mean shift, variance change, and a prior probability shift. [sent-51, score-0.126]
</p><p>30 Speciﬁc details on how each of the distribution shifts are implemented is available in the user’s guide. [sent-52, score-0.505]
</p><p>31 1388  M ODEL M ONITOR  (a) Detecting distribution shift  (b) Plot of classiﬁer sensitivity to (c) Comparison of classiﬁers across mulMAR bias. [sent-55, score-0.393]
</p><p>32 3 Statistical Measures The bulk of the implementation has to do with methods for detecting and evaluating the effect of shifts in distribution. [sent-59, score-0.582]
</p><p>33 For detecting shifts in distribution, we provide both Hellinger distance and the Kolmogorov-Smirnov (KS) test. [sent-60, score-0.52]
</p><p>34 Recent work by Cieslak and Chawla (2007) has shown that systematic shifts in distribution tend to cause signiﬁcant changes in Hellinger distance. [sent-61, score-0.575]
</p><p>35 The KS test helps to distinguish between changes caused by random ﬂuctuation and changes caused by systematic bias. [sent-62, score-0.36]
</p><p>36 For comparing the performance of multiple classiﬁers, we have implemented the Friedman test and the Bonferroni-Dunn test. [sent-63, score-0.191]
</p><p>37 1 Detecting Distribution Shift When a user starts M 2 , he or she can load any data set in either C4. [sent-69, score-0.208]
</p><p>38 The user then speciﬁes a test set (either by selecting hold-out or cross-validation or by choosing a separate test ﬁle). [sent-71, score-0.422]
</p><p>39 At this point, the user can begin evaluating the training and test data sets for shifts in distribution. [sent-72, score-0.775]
</p><p>40 Additionally M 2 automatically calculates the Hellinger distance for each feature between the training and test sets. [sent-74, score-0.172]
</p><p>41 If a feature’s distribution fails the KS test (at the α = 0. [sent-75, score-0.179]
</p><p>42 In this way, users can quickly locate features whose distribution may have changed. [sent-77, score-0.168]
</p><p>43 2 Exploration of Hypothetical Scenarios After loading a data set, the user has the opportunity to introduce any of a number of distribution shifts into the test data. [sent-80, score-0.855]
</p><p>44 Upon doing so, the histograms displayed on the screen will be updated, and new Hellinger distances and KS-test p-values are calculated. [sent-81, score-0.083]
</p><p>45 The user can then quickly evaluate any classiﬁer on both the original and new (“biased”) test sets to compare the results. [sent-82, score-0.358]
</p><p>46 For more sophisticated sensitivity analysis, the user may specify a range of distribution shifts. [sent-84, score-0.327]
</p><p>47 In this case, he or she provides a set of increasingly severe biases, and then the tool evaluates the classiﬁer’s performance under each one. [sent-85, score-0.049]
</p><p>48 The results are presented to the user both graphically (plotting classiﬁer preformance against severity of shift) and in tabular form, to facilitate further ofﬂine processing. [sent-86, score-0.294]
</p><p>49 3 Empirical Comparison of Classiﬁers Finally, users may test any one classiﬁer against several others across multiple data sets and distribution shifts. [sent-88, score-0.295]
</p><p>50 For each speciﬁed distribution shift, the software will evaluate each classiﬁer on all the data sets. [sent-89, score-0.153]
</p><p>51 It ranks the classiﬁers on each data set, calculates the average rank for each classiﬁer, and then performs the Friedman test for analysis of variance on the resulting table of ranks. [sent-90, score-0.219]
</p><p>52 05 level) that there is a signiﬁcant difference between the classiﬁers, we run the Bonferroni-Dunn test to determine exactly which classiﬁers are different from the speciﬁed reference classiﬁer. [sent-92, score-0.107]
</p><p>53 In addition to producing an annotated table of ranks containing the statistical test information, this step produces all the same output that the other portions of the program produce. [sent-93, score-0.231]
</p><p>54 Taken together, these results provide a complete picture of the classiﬁers’ relative performance in the relevant scenarios. [sent-94, score-0.049]
</p><p>55 Acknowledgments The authors would like to acknowledge Ryan Lichtenwalter, Dave Cieslak, and Karsten Steinhaeuser for helpful discussions on both implementation and user interface issues. [sent-95, score-0.26]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('shifts', 0.398), ('hellinger', 0.296), ('weka', 0.271), ('shift', 0.218), ('user', 0.208), ('ers', 0.185), ('cieslak', 0.178), ('toolkit', 0.145), ('hypothetical', 0.124), ('java', 0.124), ('detecting', 0.122), ('aeder', 0.119), ('dame', 0.119), ('hawla', 0.119), ('nitesh', 0.119), ('notre', 0.119), ('onero', 0.119), ('raeder', 0.119), ('troy', 0.119), ('ks', 0.116), ('format', 0.116), ('classi', 0.114), ('test', 0.107), ('qui', 0.101), ('gui', 0.09), ('cse', 0.09), ('chawla', 0.09), ('dem', 0.083), ('monitor', 0.083), ('friedman', 0.08), ('measures', 0.078), ('distributional', 0.072), ('witten', 0.072), ('distribution', 0.072), ('er', 0.07), ('guide', 0.068), ('changing', 0.066), ('calculates', 0.065), ('evaluating', 0.062), ('users', 0.06), ('changes', 0.056), ('across', 0.056), ('interface', 0.052), ('exploration', 0.051), ('developer', 0.05), ('javadoc', 0.05), ('ryan', 0.05), ('arff', 0.05), ('inject', 0.05), ('isolating', 0.05), ('protect', 0.05), ('dave', 0.05), ('holmes', 0.05), ('insure', 0.05), ('tabular', 0.05), ('systematic', 0.049), ('testing', 0.049), ('performance', 0.049), ('ranks', 0.047), ('displayed', 0.047), ('sensitivity', 0.047), ('caused', 0.046), ('scenarios', 0.046), ('objectively', 0.045), ('schwaighofer', 0.045), ('uctuation', 0.045), ('functionality', 0.045), ('prevalent', 0.045), ('brier', 0.045), ('evaluate', 0.043), ('expects', 0.041), ('swing', 0.041), ('afford', 0.041), ('isolated', 0.041), ('portions', 0.041), ('karsten', 0.041), ('plotting', 0.041), ('additionally', 0.039), ('odel', 0.039), ('hypothesized', 0.039), ('drift', 0.039), ('software', 0.038), ('researchers', 0.038), ('updated', 0.037), ('locate', 0.036), ('opportunity', 0.036), ('screen', 0.036), ('robustly', 0.036), ('degrade', 0.036), ('graphically', 0.036), ('monitoring', 0.036), ('carries', 0.036), ('capability', 0.036), ('style', 0.036), ('annotated', 0.036), ('missing', 0.035), ('implemented', 0.035), ('fully', 0.034), ('loading', 0.034), ('icdm', 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="56-tfidf-1" href="./jmlr-2009-Model_Monitor_%28M2%29%3A_Evaluating%2C_Comparing%2C_and_Monitoring_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">56 jmlr-2009-Model Monitor (M2): Evaluating, Comparing, and Monitoring Models    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Troy Raeder, Nitesh V. Chawla</p><p>Abstract: This paper presents Model Monitor (M 2 ), a Java toolkit for robustly evaluating machine learning algorithms in the presence of changing data distributions. M 2 provides a simple and intuitive framework in which users can evaluate classiﬁers under hypothesized shifts in distribution and therefore determine the best model (or models) for their data under a number of potential scenarios. Additionally, M 2 is fully integrated with the WEKA machine learning environment, so that a variety of commodity classiﬁers can be used if desired. Keywords: machine learning, open-source software, distribution shift, scenario analysis</p><p>2 0.10710014 <a title="56-tfidf-2" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Thomas Abeel, Yves Van de Peer, Yvan Saeys</p><p>Abstract: Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classiﬁers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license. Keywords: open source, machine learning, data mining, java library, clustering, feature selection, classiﬁcation</p><p>3 0.10284683 <a title="56-tfidf-3" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>Author: Steffen Bickel, Michael Brückner, Tobias Scheffer</p><p>Abstract: We address classiﬁcation problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution—problems also referred to as classiﬁcation under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. The problem of learning under covariate shift can be written as an integrated optimization problem. Instantiating the general optimization problem leads to a kernel logistic regression and an exponential model classiﬁer for covariate shift. The optimization problem is convex under certain conditions; our ﬁndings also clarify the relationship to the known kernel mean matching procedure. We report on experiments on problems of spam ﬁltering, text classiﬁcation, and landmine detection. Keywords: covariate shift, discriminative learning, transfer learning</p><p>4 0.08279711 <a title="56-tfidf-4" href="./jmlr-2009-A_Survey_of_Accuracy_Evaluation_Metrics_of_Recommendation_Tasks.html">4 jmlr-2009-A Survey of Accuracy Evaluation Metrics of Recommendation Tasks</a></p>
<p>Author: Asela Gunawardana, Guy Shani</p><p>Abstract: Recommender systems are now popular both commercially and in the research community, where many algorithms have been suggested for providing recommendations. These algorithms typically perform differently in various domains and tasks. Therefore, it is important from the research perspective, as well as from a practical view, to be able to decide on an algorithm that matches the domain and the task of interest. The standard way to make such decisions is by comparing a number of algorithms ofﬂine using some evaluation metric. Indeed, many evaluation metrics have been suggested for comparing recommendation algorithms. The decision on the proper evaluation metric is often critical, as each metric may favor a different algorithm. In this paper we review the proper construction of ofﬂine experiments for deciding on the most appropriate algorithm. We discuss three important tasks of recommender systems, and classify a set of appropriate well known evaluation metrics for each task. We demonstrate how using an improper evaluation metric can lead to the selection of an improper algorithm for the task of interest. We also discuss other important considerations when designing ofﬂine experiments. Keywords: recommender systems, collaborative ﬁltering, statistical analysis, comparative studies</p><p>5 0.055762596 <a title="56-tfidf-5" href="./jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">26 jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Davis E. King</p><p>Abstract: There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classiﬁcation, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools. Keywords: kernel-methods, svm, rvm, kernel clustering, C++, Bayesian networks</p><p>6 0.048446052 <a title="56-tfidf-6" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>7 0.047057275 <a title="56-tfidf-7" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>8 0.045683157 <a title="56-tfidf-8" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>9 0.040778421 <a title="56-tfidf-9" href="./jmlr-2009-RL-Glue%3A_Language-Independent_Software_for_Reinforcement-Learning_Experiments%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">77 jmlr-2009-RL-Glue: Language-Independent Software for Reinforcement-Learning Experiments    (Machine Learning Open Source Software Paper)</a></p>
<p>10 0.037805241 <a title="56-tfidf-10" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>11 0.036066603 <a title="56-tfidf-11" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>12 0.034890465 <a title="56-tfidf-12" href="./jmlr-2009-Scalable_Collaborative_Filtering_Approaches_for_Large_Recommender_Systems%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">84 jmlr-2009-Scalable Collaborative Filtering Approaches for Large Recommender Systems    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>13 0.032002985 <a title="56-tfidf-13" href="./jmlr-2009-DL-Learner%3A_Learning_Concepts_in_Description_Logics.html">20 jmlr-2009-DL-Learner: Learning Concepts in Description Logics</a></p>
<p>14 0.029821197 <a title="56-tfidf-14" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>15 0.029653918 <a title="56-tfidf-15" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>16 0.027455714 <a title="56-tfidf-16" href="./jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">99 jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<p>17 0.026611034 <a title="56-tfidf-17" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>18 0.026345011 <a title="56-tfidf-18" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>19 0.026256738 <a title="56-tfidf-19" href="./jmlr-2009-Estimating_Labels_from_Label_Proportions.html">29 jmlr-2009-Estimating Labels from Label Proportions</a></p>
<p>20 0.026045779 <a title="56-tfidf-20" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.136), (1, -0.095), (2, 0.081), (3, -0.076), (4, 0.087), (5, -0.125), (6, 0.186), (7, 0.132), (8, 0.145), (9, -0.053), (10, 0.019), (11, 0.054), (12, -0.056), (13, 0.024), (14, 0.065), (15, -0.199), (16, 0.153), (17, -0.09), (18, -0.039), (19, -0.114), (20, 0.102), (21, 0.021), (22, -0.048), (23, 0.038), (24, -0.091), (25, 0.096), (26, -0.081), (27, 0.045), (28, 0.019), (29, -0.231), (30, 0.108), (31, -0.132), (32, 0.047), (33, 0.123), (34, -0.023), (35, 0.098), (36, 0.004), (37, 0.033), (38, 0.108), (39, -0.062), (40, -0.043), (41, 0.076), (42, 0.059), (43, -0.199), (44, 0.02), (45, -0.198), (46, -0.018), (47, 0.03), (48, 0.042), (49, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95614636 <a title="56-lsi-1" href="./jmlr-2009-Model_Monitor_%28M2%29%3A_Evaluating%2C_Comparing%2C_and_Monitoring_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">56 jmlr-2009-Model Monitor (M2): Evaluating, Comparing, and Monitoring Models    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Troy Raeder, Nitesh V. Chawla</p><p>Abstract: This paper presents Model Monitor (M 2 ), a Java toolkit for robustly evaluating machine learning algorithms in the presence of changing data distributions. M 2 provides a simple and intuitive framework in which users can evaluate classiﬁers under hypothesized shifts in distribution and therefore determine the best model (or models) for their data under a number of potential scenarios. Additionally, M 2 is fully integrated with the WEKA machine learning environment, so that a variety of commodity classiﬁers can be used if desired. Keywords: machine learning, open-source software, distribution shift, scenario analysis</p><p>2 0.52011085 <a title="56-lsi-2" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Thomas Abeel, Yves Van de Peer, Yvan Saeys</p><p>Abstract: Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classiﬁers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license. Keywords: open source, machine learning, data mining, java library, clustering, feature selection, classiﬁcation</p><p>3 0.47120443 <a title="56-lsi-3" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>Author: Steffen Bickel, Michael Brückner, Tobias Scheffer</p><p>Abstract: We address classiﬁcation problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution—problems also referred to as classiﬁcation under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. The problem of learning under covariate shift can be written as an integrated optimization problem. Instantiating the general optimization problem leads to a kernel logistic regression and an exponential model classiﬁer for covariate shift. The optimization problem is convex under certain conditions; our ﬁndings also clarify the relationship to the known kernel mean matching procedure. We report on experiments on problems of spam ﬁltering, text classiﬁcation, and landmine detection. Keywords: covariate shift, discriminative learning, transfer learning</p><p>4 0.45549875 <a title="56-lsi-4" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>Author: Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi, Luca Cazzanti</p><p>Abstract: This paper reviews and extends the ﬁeld of similarity-based classiﬁcation, presenting new analyses, algorithms, data sets, and a comprehensive set of experimental results for a rich collection of classiﬁcation problems. Speciﬁcally, the generalizability of using similarities as features is analyzed, design goals and methods for weighting nearest-neighbors for similarity-based learning are proposed, and different methods for consistently converting similarities into kernels are compared. Experiments on eight real data sets compare eight approaches and their variants to similarity-based learning. Keywords: similarity, dissimilarity, similarity-based learning, indeﬁnite kernels</p><p>5 0.44703072 <a title="56-lsi-5" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>Author: Juan José del Coz, Jorge Díez, Antonio Bahamonde</p><p>Abstract: Nondeterministic classiﬁers are deﬁned as those allowed to predict more than one class for some entries from an input space. Given that the true class should be included in predictions and the number of classes predicted should be as small as possible, these kind of classiﬁers can be considered as Information Retrieval (IR) procedures. In this paper, we propose a family of IR loss functions to measure the performance of nondeterministic learners. After discussing such measures, we derive an algorithm for learning optimal nondeterministic hypotheses. Given an entry from the input space, the algorithm requires the posterior probabilities to compute the subset of classes with the lowest expected loss. From a general point of view, nondeterministic classiﬁers provide an improvement in the proportion of predictions that include the true class compared to their deterministic counterparts; the price to be paid for this increase is usually a tiny proportion of predictions with more than one class. The paper includes an extensive experimental study using three deterministic learners to estimate posterior probabilities: a multiclass Support Vector Machine (SVM), a Logistic Regression, and a Na¨ve Bayes. The data sets considered comprise both UCI ı multi-class learning tasks and microarray expressions of different kinds of cancer. We successfully compare nondeterministic classiﬁers with other alternative approaches. Additionally, we shall see how the quality of posterior probabilities (measured by the Brier score) determines the goodness of nondeterministic predictions. Keywords: nondeterministic, multiclassiﬁcation, reject option, multi-label classiﬁcation, posterior probabilities</p><p>6 0.37921926 <a title="56-lsi-6" href="./jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">26 jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<p>7 0.37416741 <a title="56-lsi-7" href="./jmlr-2009-A_Survey_of_Accuracy_Evaluation_Metrics_of_Recommendation_Tasks.html">4 jmlr-2009-A Survey of Accuracy Evaluation Metrics of Recommendation Tasks</a></p>
<p>8 0.34792307 <a title="56-lsi-8" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>9 0.32196295 <a title="56-lsi-9" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>10 0.28978711 <a title="56-lsi-10" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>11 0.26868039 <a title="56-lsi-11" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>12 0.26000926 <a title="56-lsi-12" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>13 0.23628196 <a title="56-lsi-13" href="./jmlr-2009-Cautious_Collective_Classification.html">15 jmlr-2009-Cautious Collective Classification</a></p>
<p>14 0.23536699 <a title="56-lsi-14" href="./jmlr-2009-Scalable_Collaborative_Filtering_Approaches_for_Large_Recommender_Systems%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">84 jmlr-2009-Scalable Collaborative Filtering Approaches for Large Recommender Systems    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>15 0.21871251 <a title="56-lsi-15" href="./jmlr-2009-DL-Learner%3A_Learning_Concepts_in_Description_Logics.html">20 jmlr-2009-DL-Learner: Learning Concepts in Description Logics</a></p>
<p>16 0.2126077 <a title="56-lsi-16" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>17 0.19893238 <a title="56-lsi-17" href="./jmlr-2009-A_Parameter-Free_Classification_Method_for_Large_Scale_Learning.html">3 jmlr-2009-A Parameter-Free Classification Method for Large Scale Learning</a></p>
<p>18 0.19763549 <a title="56-lsi-18" href="./jmlr-2009-RL-Glue%3A_Language-Independent_Software_for_Reinforcement-Learning_Experiments%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">77 jmlr-2009-RL-Glue: Language-Independent Software for Reinforcement-Learning Experiments    (Machine Learning Open Source Software Paper)</a></p>
<p>19 0.1971958 <a title="56-lsi-19" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>20 0.19658889 <a title="56-lsi-20" href="./jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">99 jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.709), (26, 0.013), (38, 0.03), (52, 0.069), (58, 0.012), (66, 0.027), (90, 0.013), (91, 0.017), (96, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95651919 <a title="56-lda-1" href="./jmlr-2009-Model_Monitor_%28M2%29%3A_Evaluating%2C_Comparing%2C_and_Monitoring_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">56 jmlr-2009-Model Monitor (M2): Evaluating, Comparing, and Monitoring Models    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Troy Raeder, Nitesh V. Chawla</p><p>Abstract: This paper presents Model Monitor (M 2 ), a Java toolkit for robustly evaluating machine learning algorithms in the presence of changing data distributions. M 2 provides a simple and intuitive framework in which users can evaluate classiﬁers under hypothesized shifts in distribution and therefore determine the best model (or models) for their data under a number of potential scenarios. Additionally, M 2 is fully integrated with the WEKA machine learning environment, so that a variety of commodity classiﬁers can be used if desired. Keywords: machine learning, open-source software, distribution shift, scenario analysis</p><p>2 0.68138838 <a title="56-lda-2" href="./jmlr-2009-Data-driven_Calibration_of_Penalties_for_Least-Squares_Regression.html">21 jmlr-2009-Data-driven Calibration of Penalties for Least-Squares Regression</a></p>
<p>Author: Sylvain Arlot, Pascal Massart</p><p>Abstract: Penalization procedures often suffer from their dependence on multiplying factors, whose optimal values are either unknown or hard to estimate from data. We propose a completely data-driven calibration algorithm for these parameters in the least-squares regression framework, without assuming a particular shape for the penalty. Our algorithm relies on the concept of minimal penalty, recently introduced by Birg´ and Massart (2007) in the context of penalized least squares for Gaussian hoe moscedastic regression. On the positive side, the minimal penalty can be evaluated from the data themselves, leading to a data-driven estimation of an optimal penalty which can be used in practice; on the negative side, their approach heavily relies on the homoscedastic Gaussian nature of their stochastic framework. The purpose of this paper is twofold: stating a more general heuristics for designing a datadriven penalty (the slope heuristics) and proving that it works for penalized least-squares regression with a random design, even for heteroscedastic non-Gaussian data. For technical reasons, some exact mathematical results will be proved only for regressogram bin-width selection. This is at least a ﬁrst step towards further results, since the approach and the method that we use are indeed general. Keywords: data-driven calibration, non-parametric regression, model selection by penalization, heteroscedastic data, regressogram</p><p>3 0.26392475 <a title="56-lda-3" href="./jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">43 jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Thomas Abeel, Yves Van de Peer, Yvan Saeys</p><p>Abstract: Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classiﬁers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license. Keywords: open source, machine learning, data mining, java library, clustering, feature selection, classiﬁcation</p><p>4 0.23723587 <a title="56-lda-4" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>Author: Juan José del Coz, Jorge Díez, Antonio Bahamonde</p><p>Abstract: Nondeterministic classiﬁers are deﬁned as those allowed to predict more than one class for some entries from an input space. Given that the true class should be included in predictions and the number of classes predicted should be as small as possible, these kind of classiﬁers can be considered as Information Retrieval (IR) procedures. In this paper, we propose a family of IR loss functions to measure the performance of nondeterministic learners. After discussing such measures, we derive an algorithm for learning optimal nondeterministic hypotheses. Given an entry from the input space, the algorithm requires the posterior probabilities to compute the subset of classes with the lowest expected loss. From a general point of view, nondeterministic classiﬁers provide an improvement in the proportion of predictions that include the true class compared to their deterministic counterparts; the price to be paid for this increase is usually a tiny proportion of predictions with more than one class. The paper includes an extensive experimental study using three deterministic learners to estimate posterior probabilities: a multiclass Support Vector Machine (SVM), a Logistic Regression, and a Na¨ve Bayes. The data sets considered comprise both UCI ı multi-class learning tasks and microarray expressions of different kinds of cancer. We successfully compare nondeterministic classiﬁers with other alternative approaches. Additionally, we shall see how the quality of posterior probabilities (measured by the Brier score) determines the goodness of nondeterministic predictions. Keywords: nondeterministic, multiclassiﬁcation, reject option, multi-label classiﬁcation, posterior probabilities</p><p>5 0.23174994 <a title="56-lda-5" href="./jmlr-2009-A_Survey_of_Accuracy_Evaluation_Metrics_of_Recommendation_Tasks.html">4 jmlr-2009-A Survey of Accuracy Evaluation Metrics of Recommendation Tasks</a></p>
<p>Author: Asela Gunawardana, Guy Shani</p><p>Abstract: Recommender systems are now popular both commercially and in the research community, where many algorithms have been suggested for providing recommendations. These algorithms typically perform differently in various domains and tasks. Therefore, it is important from the research perspective, as well as from a practical view, to be able to decide on an algorithm that matches the domain and the task of interest. The standard way to make such decisions is by comparing a number of algorithms ofﬂine using some evaluation metric. Indeed, many evaluation metrics have been suggested for comparing recommendation algorithms. The decision on the proper evaluation metric is often critical, as each metric may favor a different algorithm. In this paper we review the proper construction of ofﬂine experiments for deciding on the most appropriate algorithm. We discuss three important tasks of recommender systems, and classify a set of appropriate well known evaluation metrics for each task. We demonstrate how using an improper evaluation metric can lead to the selection of an improper algorithm for the task of interest. We also discuss other important considerations when designing ofﬂine experiments. Keywords: recommender systems, collaborative ﬁltering, statistical analysis, comparative studies</p><p>6 0.2287354 <a title="56-lda-6" href="./jmlr-2009-Python_Environment_for_Bayesian_Learning%3A_Inferring_the_Structure_of_Bayesian_Networks_from_Knowledge_and_Data%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">76 jmlr-2009-Python Environment for Bayesian Learning: Inferring the Structure of Bayesian Networks from Knowledge and Data    (Machine Learning Open Source Software Paper)</a></p>
<p>7 0.21790728 <a title="56-lda-7" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>8 0.21675323 <a title="56-lda-8" href="./jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">26 jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<p>9 0.21541259 <a title="56-lda-9" href="./jmlr-2009-Adaptive_False_Discovery_Rate_Control_under_Independence_and_Dependence.html">5 jmlr-2009-Adaptive False Discovery Rate Control under Independence and Dependence</a></p>
<p>10 0.20567837 <a title="56-lda-10" href="./jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</a></p>
<p>11 0.20269382 <a title="56-lda-11" href="./jmlr-2009-Subgroup_Analysis_via_Recursive_Partitioning.html">91 jmlr-2009-Subgroup Analysis via Recursive Partitioning</a></p>
<p>12 0.19380197 <a title="56-lda-12" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>13 0.19327518 <a title="56-lda-13" href="./jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">99 jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<p>14 0.19047835 <a title="56-lda-14" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>15 0.18937562 <a title="56-lda-15" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>16 0.18622455 <a title="56-lda-16" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>17 0.18320861 <a title="56-lda-17" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>18 0.18187751 <a title="56-lda-18" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>19 0.18160483 <a title="56-lda-19" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>20 0.18067643 <a title="56-lda-20" href="./jmlr-2009-A_Parameter-Free_Classification_Method_for_Large_Scale_Learning.html">3 jmlr-2009-A Parameter-Free Classification Method for Large Scale Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
