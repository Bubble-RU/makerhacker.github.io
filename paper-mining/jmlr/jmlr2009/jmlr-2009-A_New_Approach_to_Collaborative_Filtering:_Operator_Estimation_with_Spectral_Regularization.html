<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-2" href="#">jmlr2009-2</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</h1>
<br/><p>Source: <a title="jmlr-2009-2-pdf" href="http://jmlr.org/papers/volume10/abernethy09a/abernethy09a.pdf">pdf</a></p><p>Author: Jacob Abernethy, Francis Bach, Theodoros Evgeniou, Jean-Philippe Vert</p><p>Abstract: We present a general approach for collaborative ﬁltering (CF) using spectral regularization to learn linear operators mapping a set of “users” to a set of possibly desired “objects”. In particular, several recent low-rank type matrix-completion methods for CF are shown to be special cases of our proposed framework. Unlike existing regularization-based CF, our approach can be used to incorporate additional information such as attributes of the users/objects—a feature currently lacking in existing regularization-based CF approaches—using popular and well-known kernel methods. We provide novel representer theorems that we use to develop new estimation methods. We then provide learning algorithms based on low-rank decompositions and test them on a standard CF data set. The experiments indicate the advantages of generalizing the existing regularization-based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach. Keywords: collaborative ﬁltering, matrix completion, kernel methods, spectral regularization</p><p>Reference: <a title="jmlr-2009-2-reference" href="../jmlr2009_reference/jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Keywords: collaborative ﬁltering, matrix completion, kernel methods, spectral regularization  1. [sent-16, score-0.591]
</p><p>2 The ultimate goal of CF is to infer the preferences of users in order to offer them new objects. [sent-25, score-0.283]
</p><p>3 Recent regularization-based CF methods assume that the only data available are the revealed preferences, where no other information such as background information on the objects or users is given. [sent-32, score-0.376]
</p><p>4 (2005), suggests penalizing the predicted matrix by its trace norm, that is, the sum of its singular values. [sent-42, score-0.383]
</p><p>5 An added beneﬁt of the trace norm regularization is that, with a sufﬁciently large regularization parameter, the ﬁnal solution will be low-rank (Fazel et al. [sent-43, score-0.416]
</p><p>6 Intuitively, such information might be useful to guide the inference of preferences, in particular for users and objects with very few known ratings. [sent-50, score-0.376]
</p><p>7 For example, at the extreme, users and objects with no prior ratings can not be considered in the standard CF formulation, while their attributes alone could provide some basic preference inference. [sent-51, score-0.657]
</p><p>8 More precisely we show that CF, while typically seen as a problem of matrix completion, can be thought of more generally as linear operator estimation, where the desired operator maps from a Hilbert space of users to a Hilbert space of objects. [sent-53, score-0.529]
</p><p>9 Equivalently, this can be viewed as learning a bilinear form between users and objects. [sent-54, score-0.294]
</p><p>10 We then develop spectral regularization based methods to learn such linear operators. [sent-55, score-0.339]
</p><p>11 We also show that, with the appropriate choice of kernels for both users and objects, we may consider a number of existing machine learning methods as special cases of our general framework. [sent-59, score-0.322]
</p><p>12 In particular, we show that several CF methods such as rank-constrained optimization, trace-norm regularization, and those based on Frobenius norm regularization, can all be cast as special cases of spectral regularization on operator spaces. [sent-60, score-0.574]
</p><p>13 In Section 2, we review the notion of a compact operator on Hilbert Space, and we show how to cast the collaborative ﬁltering problem within this framework. [sent-64, score-0.303]
</p><p>14 We then introduce spectral regularization and discuss how rank constraint, trace norm regularization, and Frobenius norm regularization are all special cases of spectral regularization. [sent-65, score-1.177]
</p><p>15 In Section 4, we provide three representer theorems for operator estimation with spectral regularization that allow for efﬁcient learning algorithms. [sent-67, score-0.6]
</p><p>16 Learning Compact Operators with Spectral Regularization In this section we propose a mathematical formulation for a general CF problem with spectral regularization. [sent-71, score-0.3]
</p><p>17 Of course, the users and objects may additionally be characterized by known attributes, in which case x or y might contain some representation of this extra information. [sent-76, score-0.376]
</p><p>18 This naturally leads us to model the users as elements in a Hilbert space X , and the objects they rate as elements of another Hilbert space Y . [sent-78, score-0.376]
</p><p>19 We assume that our observation data is in the form of ratings from users to objects, a realvalued score representing the user’s preference for the object. [sent-79, score-0.354]
</p><p>20 ,N in X × Y × R, where ti represents the rating of user xi for object yi , the generalized CF problem is then to infer a function f : X × Y → R that can then be used to predict the rating of any user x ∈ X for any object y ∈ Y by f (x, y). [sent-84, score-0.319]
</p><p>21 If several ratings of a user for different objects are available, as is commonly the case, several xi ’s 805  A BERNETHY, BACH , E VGENIOU AND V ERT  will be identical in X —a slight abuse of notation. [sent-86, score-0.319]
</p><p>22 In the present paper, we uniquely consider learning preference functions f (·, ·) that take the form of a linear operator from X to Y ; that is, bilinear forms on X × Y , f (x, y) = x, Fy X ,  (1)  for some compact operator F. [sent-94, score-0.455]
</p><p>23 In the general case we consider below, if X and Y are not Hilbert spaces, one could also ﬁrst map (implicitly) users x and objects y into possibly inﬁnite dimensional Hilbert feature spaces ΦX (x) and ΨY (y) and use kernels. [sent-97, score-0.376]
</p><p>24 3 Regularization Term For the regularization term, we focus on a class of spectral functions deﬁned as follows. [sent-109, score-0.339]
</p><p>25 Deﬁnition 1 A function Ω : B0 (Y , X ) → R ∪ {+∞} is called a spectral penalty function if it can be written as: d  Ω(F) = ∑ si (σi (F)) ,  (3)  i=1  where for any i ≥ 1, si : R+ → R+ ∪ {+∞} is a non-decreasing penalty function satisfying s(0) = 0, and (σi (F))i=1,. [sent-110, score-0.686]
</p><p>26 806  C OLLABORATIVE F ILTERING VIA O PERATOR E STIMATION WITH S PECTRAL R EGULARIZATION  Note that, by the spectral theorem presented in Appendix A, any compact operator can be decomposed into singular vectors, with singular values being a sequence that tends to zero. [sent-114, score-0.729]
</p><p>27 Spectral penalty functions include as special cases several functions often encountered in matrix completion problems: • For a given integer r, consider taking si = 0 for i = 1, . [sent-115, score-0.35]
</p><p>28 (4)  In other words, the set of operators F that satisfy Ω(F) < +∞ is the set of operators with rank smaller than r. [sent-120, score-0.443]
</p><p>29 • Taking si (u) = u for all i results in the trace norm penalty (see Appendix A): Ω(F) =  F +∞  1  if F ∈ B1 (Y , X ) , otherwise,  (5)  where we note with B1 (Y , X ) the set of operators with ﬁnite trace norm. [sent-121, score-0.788]
</p><p>30 Such operators are referred to as trace class operators. [sent-122, score-0.314]
</p><p>31 For example, we may constrain the rank to be smaller than r while penalizing the trace norm of the matrix, which can be obtained by setting si (u) = u for i = 1, . [sent-126, score-0.47]
</p><p>32 In particular, three elements can be tailored to one’s particular needs: the loss function, the kernels (or equivalently the Hilbert spaces), and the spectral penalty term. [sent-144, score-0.551]
</p><p>33 3, we gave several examples of such constraints including the rank constraint (4), the trace norm constraint (5), the Hilbert-Schmidt norm constraint (6), or the trace norm constraint over low-rank operators (7). [sent-160, score-1.115]
</p><p>34 The choice of a particular penalty might be guided by some considerations about the problem to be solved, for example, ﬁnding low-rank operators as a way to discover low-dimensional latent structures in the data. [sent-161, score-0.334]
</p><p>35 On the other hand, from an algorithmic perspective, the choice of the spectral penalty may affect the efﬁciency or feasibility of our learning algorithm. [sent-162, score-0.442]
</p><p>36 Certain penalty functions, such as the rank constraint for example, will lead to non-convex problems because the corresponding penalty function (4) is itself not convex. [sent-163, score-0.545]
</p><p>37 , embeddings) of the users and objects in their respective Hilbert spaces. [sent-170, score-0.376]
</p><p>38 In the current work, we focus on two basic kernels (Dirac kernels and attribute kernels) and in Section 3. [sent-173, score-0.298]
</p><p>39 In other words, the Dirac kernel amounts to representing the users (resp. [sent-179, score-0.315]
</p><p>40 This kernel can be used whether or Y X not attributes are available for users and objects. [sent-182, score-0.455]
</p><p>41 • The second kernel we consider is a kernel between attributes, when attributes are available to describe the users and/or objects. [sent-186, score-0.557]
</p><p>42 This would typically be a kernel between vectors, such as the inner product or a Gaussian RBF kernel, when the descriptions of users and/or objects take the form of vectors of real-valued attributes, or any kernel on structured objects (Shawe-Taylor and Cristianini, 2004). [sent-188, score-0.743]
</p><p>43 In the following section we illustrate how speciﬁc combinations of loss, spectral penalty and kernels can be relevant for various settings. [sent-192, score-0.551]
</p><p>44 1 Matrix Completion When the Dirac kernel is used for both users and objects, then we can organize the data {xi , i = 1, . [sent-197, score-0.315]
</p><p>45 A bilinear form using Dirac kernels only depends on the identities of the users and the objects, and we only predict the rating ti based on the identities of the groups in both spaces. [sent-211, score-0.459]
</p><p>46 Moreover the spectral regularizer corresponds to the corresponding spectral function of the complete matrix M ∈ RnX ×nY . [sent-219, score-0.574]
</p><p>47 In this context, ﬁnding a low-rank approximation of the observed entries in a matrix is an appealing strategy, which corresponds to taking the rank penalty constraint (4) combined with, for example, the square loss error. [sent-220, score-0.415]
</p><p>48 To circumvent this issue, convex spectral penalty functions can be considered. [sent-222, score-0.481]
</p><p>49 Indeed, in the case 809  A BERNETHY, BACH , E VGENIOU AND V ERT  of binary preferences, combining the hinge loss function with the trace norm penalty (5) leads to the maximum margin matrix factorization (MMMF) approach proposed by Srebro et al. [sent-223, score-0.492]
</p><p>50 2 Multi-Task Learning It may be the case that we have attributes only for objects y (or, similarly, only for users). [sent-227, score-0.303]
</p><p>51 , N} organized in nX groups, we aim to estimate a separate function on objects fi (y) for each of the nX users i. [sent-231, score-0.406]
</p><p>52 In order to adapt our general framework to this scenario, it is natural to consider the attribute Y X kernel kA for the objects, whose attributes are available, and the Dirac kernel kD for the users, for which no attributes are used. [sent-233, score-0.564]
</p><p>53 Again the choice of the loss function depends on the precise task to be solved, and the spectral penalty function can be chosen to enforce some sharing of information between different tasks. [sent-234, score-0.442]
</p><p>54 In particular, taking the rank penalty function (4) enforces a decomposition of the tasks (learning each fi ) into a limited number of factors. [sent-235, score-0.378]
</p><p>55 The resulting problem, however, is not convex due to the use of the non-convex rank penalty function. [sent-237, score-0.354]
</p><p>56 A natural alternative is then to replace the rank constraint by the trace norm penalty function (5), resulting in a convex optimization problem when the loss function is convex. [sent-238, score-0.705]
</p><p>57 When c = 0, that is, when we take a Dirac kernel for the users and an attribute kernel for the objects, then penalizing the HilbertSchmitt norm amounts to estimating independent models for each user, as explained by Evgeniou et al. [sent-246, score-0.64]
</p><p>58 Combining two Dirac kernels for users and objects, respectively, and penalizing the Hilbert-Schmitt norm would not be useful, since the optimal solution would be 0 all points other than training pairs. [sent-248, score-0.465]
</p><p>59 3 Pairwise Learning When attributes are available for both users and objects then it is possible to use the attributes kernels for each. [sent-251, score-0.765]
</p><p>60 Replacing the Hilbert-Schmidt norm by another spectral penalty function, such as the trace norm, would result in new algorithms for learning low-rank functions over pairs. [sent-258, score-0.704]
</p><p>61 There are many situations, however, where the attributes available to describe the users and/or objects are certainly useful for the inference task, but on the other hand do not fully characterize the users and/or objects. [sent-261, score-0.729]
</p><p>62 For example, if we just know the age and gender of users, we would like to use this information to model their preferences, but would also like to allow for prediction of different preferences for different users even when they share the same age and gender. [sent-262, score-0.375]
</p><p>63 This naturally leads us to consider the following convex combinations of Dirac and attributes kernels (Abernethy et al. [sent-264, score-0.288]
</p><p>64 These kernels interpolate between the Dirac kernels (η = 0 and ζ = 0) and the attributes kernels (η = 1 and ζ = 1). [sent-266, score-0.467]
</p><p>65 1, the reformulation of the problem as a ﬁnite-dimensional problem is a simple instance of the representer theorem when the Hilbert-Schmidt norm is used as a penalty function, we prove in Section 4. [sent-280, score-0.41]
</p><p>66 2 a generalized representer theorem that is valid with any spectral penalty function. [sent-281, score-0.57]
</p><p>67 1 The Case of the Hilbert-Schmidt Penalty Function In the particular case where the penalty function Ω(F) is the Hilbert-Schmidt norm (6), then the set {F ∈ B0 (Y , X ) : Ω(F) < ∞} is the set of Hilbert-Schmidt operators. [sent-283, score-0.282]
</p><p>68 This theorem shows that, as soon as a spectral penalty function is used to control the complexity of the compact operators, a solution can be searched in the ﬁnite-dimensional space XN ⊗ YN , which in practice boils down to an optimization problem over the set of matrices of size mX × mY . [sent-314, score-0.612]
</p><p>69 A convenient way to obtain an important decrease in complexity (at the expense of possibly losing convexity) is by constraining the rank of the operator through an adequate choice of a spectral 813  A BERNETHY, BACH , E VGENIOU AND V ERT  penalty. [sent-316, score-0.53]
</p><p>70 Indeed, the set of non-zero singular components of F as an operator is equal to the set of non-zero singular values of α in (13) seen as a matrix. [sent-317, score-0.397]
</p><p>71 Consequently any constraint on the rank of F as an operator results in a constraint on α as a matrix, from which we deduce: Corollary 4 If, in Theorem 3, the spectral penalty function Ω is inﬁnite on operators of rank larger than r (i. [sent-318, score-1.099]
</p><p>72 As a result, if a rank constraint rank(F) ≤ r is added to the optimization problem then the representer theorem still holds but the dimension of the parameter α becomes r × mX + mY instead of mX × mY , which is usually beneﬁcial. [sent-321, score-0.352]
</p><p>73 We note, however, that when a rank constraint is added to the Hilbert-Schmidt norm penalty, then the classical representer Theorem 2 and the expansion of the solution over N vectors (11) are not valid anymore, only Theorem 3 and the expansion (13) can be used. [sent-322, score-0.453]
</p><p>74 As in the classical representer theorems, this could also provide an alternative proof of the representer theorem in that particular situation. [sent-329, score-0.294]
</p><p>75 We also assume that the spectral regularization is such that for all i ∈ N, si = s, where s is a convex function such that s(0) = 0. [sent-340, score-0.41]
</p><p>76 Note that results on functions of eigenvalues of symmetric matrices can be extended to functions of singular values of rectangular matrices by using the equivalence between the singular value decomposition of A and the eigenvalue 0 A decomposition of (Stewart and Sun, 1990). [sent-344, score-0.452]
</p><p>77 (2007), we can ﬁnd the primal variables by noting that once β is known, the singular vectors of α are known and we can ﬁnd the singular values by solving a reduced convex optimization problem. [sent-355, score-0.38]
</p><p>78 2 Collaborative Filtering In the presence of (many) identical columns and rows, which is often the case in collaborative ﬁltering situations, the kernel matrices K and L have some columns (and thus rows) which are identical. [sent-363, score-0.319]
</p><p>79 In such cases, it is computationally more desirable to instead consider the kernel matrices ˜ ˜˜ ˜ ˜˜ (with their square-root decompositions) K = X X ⊤ and L = Y Y ⊤ as the kernel matrices for all distinct elements of X and Y (let nX and nY be their sizes). [sent-364, score-0.326]
</p><p>80 Similar to usual kernel machines and the general case presented above, using the primal or the dual formulation for optimization depends on the number of available ratings N compared to the ˜ ˜ ranks mX and mY of the kernel matrices K and L. [sent-373, score-0.533]
</p><p>81 3 Low-rank Constrained Problem We approximate the spectral norm by an inﬁnitely differentiable spectral function. [sent-376, score-0.668]
</p><p>82 4 Kernel Learning for Spectral Functions In our collaborative ﬁltering context, there are two potentially useful sources of kernel learning: learning the attribute kernels, or learning the weights η and ζ between Dirac kernels and attribute kernels. [sent-395, score-0.471]
</p><p>83 , GM , we look for predictor functions which are sums of the M possible atomic predictor functions, and we penalize by the sum of spectral functions, to obtain the following optimization problem: n  min  k  k  M  ∑ ψi ∑ (Xk αkYk⊤ )ii  ∀k,αk ∈Rmx ×my i=1  k=1  M  + λ ∑ Ω(αk ). [sent-433, score-0.361]
</p><p>84 As discussed in Section 3, by using operator estimation and spectral regularization as a framework for CF, we may use potentially more information to predict preferences. [sent-450, score-0.472]
</p><p>85 The left plot displays the results when using the trace norm spectral penalty. [sent-487, score-0.524]
</p><p>86 The plot on the right uses the same rank-constrained formulation, but with a Frobenius norm penalty instead. [sent-490, score-0.282]
</p><p>87 Conclusions We have presented a method for solving a generalized matrix completion problem where we have attributes describing the matrix dimensions. [sent-510, score-0.328]
</p><p>88 The problem is formalized as the problem of inferring a linear compact operator between two general Hilbert spaces, which generalizes the classical ﬁnitedimensional matrix completion problem. [sent-511, score-0.379]
</p><p>89 We introduced the notion of spectral regularization for operators, which generalizes various spectral penalizations for matrices, and we proved a general representer theorem for this setting. [sent-512, score-0.729]
</p><p>90 This framework is particularly relevant for CF applications where attributes are available for users and/or objects, and preliminary experiments conﬁrm the beneﬁts of our method. [sent-514, score-0.353]
</p><p>91 We denote by B0 (Y , X ) the set of compact linear operators from Y to X , that is, the set of linear operators that map the unit ball of Y to a relatively compact set of X . [sent-525, score-0.448]
</p><p>92 For general Hilbert spaces X and Y , any compact linear operator F ∈ B0 (Y , X ) admits a spectral decomposition: ∞  F = ∑ σi ui ⊗ vi . [sent-529, score-0.561]
</p><p>93 • The set of operators with ﬁnite rank is denoted BF (Y , X ). [sent-538, score-0.289]
</p><p>94 The set of functions fF associated to the Hilbert-Schmidt operators forms itself a Hilbert space of functions X × Y → R, which is the reproducing kernel Hilbert space of the product kernel deﬁned for ((x, y) , (x′ , y′ )) ∈ (X × Y )2 by = x, x′ X y, y′ Y . [sent-544, score-0.358]
</p><p>95 The trace norm of an operator F ∈ B1 (Y , X ) is given by: ∞  F  1  = ∑ σi (F) . [sent-547, score-0.395]
</p><p>96 Proof of Theorem 3 We start with a general result about the decrease of singular values for compact operators composed with projection: Lemma 7 Let G and H be two Hilbert spaces, H a compact linear subspace of H , and ΠH denote the orthogonal projection onto H. [sent-550, score-0.426]
</p><p>97 This implies that the spectral penalty term satisﬁes Ω(G) ≤ Ω(F). [sent-571, score-0.442]
</p><p>98 Let us ﬁrst consider the spectral penalty term Ω(F). [sent-576, score-0.442]
</p><p>99 Given the decomposition (13), the non-zero singular values of F as an operator are exactly the non-zero singular values of α as a matrix, as soon as (u1 , . [sent-577, score-0.43]
</p><p>100 We can now replace the empirical risk RN (FN ) by RN diag XαY ⊤ and the penalty Ω(F) by Ω(α) to deduce the optimization problem (14) from (12), which concludes the proof of Theorem 3. [sent-611, score-0.272]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cf', 0.264), ('spectral', 0.262), ('users', 0.213), ('dirac', 0.203), ('bach', 0.2), ('penalty', 0.18), ('objects', 0.163), ('bernethy', 0.16), ('ert', 0.16), ('vgeniou', 0.16), ('trace', 0.16), ('operators', 0.154), ('pectral', 0.147), ('attributes', 0.14), ('rank', 0.135), ('operator', 0.133), ('singular', 0.132), ('representer', 0.128), ('ollaborative', 0.125), ('perator', 0.125), ('mx', 0.124), ('egularization', 0.112), ('iltering', 0.112), ('kernels', 0.109), ('hilbert', 0.106), ('ratings', 0.103), ('norm', 0.102), ('kernel', 0.102), ('srebro', 0.102), ('collaborative', 0.1), ('ua', 0.093), ('kd', 0.091), ('stimation', 0.09), ('completion', 0.088), ('yn', 0.083), ('rn', 0.081), ('bilinear', 0.081), ('ka', 0.081), ('fenchel', 0.08), ('iti', 0.08), ('attribute', 0.08), ('nx', 0.079), ('regularization', 0.077), ('compact', 0.07), ('preferences', 0.07), ('burer', 0.067), ('rmx', 0.067), ('frobenius', 0.065), ('vi', 0.065), ('uv', 0.062), ('matrices', 0.061), ('jacob', 0.057), ('rating', 0.056), ('evgeniou', 0.053), ('mines', 0.053), ('diag', 0.053), ('user', 0.053), ('ltering', 0.051), ('tensor', 0.051), ('matrix', 0.05), ('dual', 0.05), ('constraint', 0.05), ('age', 0.046), ('abernethy', 0.045), ('fs', 0.044), ('differentiable', 0.042), ('xn', 0.042), ('lanckriet', 0.041), ('movies', 0.041), ('penalizing', 0.041), ('vert', 0.041), ('umx', 0.04), ('vmy', 0.04), ('convex', 0.039), ('optimization', 0.039), ('primal', 0.038), ('preference', 0.038), ('formulation', 0.038), ('classical', 0.038), ('corners', 0.037), ('sr', 0.037), ('yi', 0.035), ('francis', 0.035), ('vec', 0.035), ('ny', 0.034), ('movielens', 0.034), ('object', 0.033), ('decomposition', 0.033), ('si', 0.032), ('books', 0.031), ('dv', 0.031), ('ui', 0.031), ('orthonormal', 0.031), ('amit', 0.03), ('fx', 0.03), ('theodoros', 0.03), ('fi', 0.03), ('predictor', 0.03), ('jaakkola', 0.03), ('columns', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="2-tfidf-1" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>Author: Jacob Abernethy, Francis Bach, Theodoros Evgeniou, Jean-Philippe Vert</p><p>Abstract: We present a general approach for collaborative ﬁltering (CF) using spectral regularization to learn linear operators mapping a set of “users” to a set of possibly desired “objects”. In particular, several recent low-rank type matrix-completion methods for CF are shown to be special cases of our proposed framework. Unlike existing regularization-based CF, our approach can be used to incorporate additional information such as attributes of the users/objects—a feature currently lacking in existing regularization-based CF approaches—using popular and well-known kernel methods. We provide novel representer theorems that we use to develop new estimation methods. We then provide learning algorithms based on low-rank decompositions and test them on a standard CF data set. The experiments indicate the advantages of generalizing the existing regularization-based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach. Keywords: collaborative ﬁltering, matrix completion, kernel methods, spectral regularization</p><p>2 0.19643654 <a title="2-tfidf-2" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, regularization</p><p>3 0.13088408 <a title="2-tfidf-3" href="./jmlr-2009-Scalable_Collaborative_Filtering_Approaches_for_Large_Recommender_Systems%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">84 jmlr-2009-Scalable Collaborative Filtering Approaches for Large Recommender Systems    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Gábor Takács, István Pilászy, Bottyán Németh, Domonkos Tikk</p><p>Abstract: The collaborative ﬁltering (CF) using known user ratings of items has proved to be effective for predicting user preferences in item selection. This thriving subﬁeld of machine learning became popular in the late 1990s with the spread of online services that use recommender systems, such as Amazon, Yahoo! Music, and Netﬂix. CF approaches are usually designed to work on very large data sets. Therefore the scalability of the methods is crucial. In this work, we propose various scalable solutions that are validated against the Netﬂix Prize data set, currently the largest publicly available collection. First, we propose various matrix factorization (MF) based techniques. Second, a neighbor correction method for MF is outlined, which alloys the global perspective of MF and the localized property of neighbor based approaches efﬁciently. In the experimentation section, we ﬁrst report on some implementation issues, and we suggest on how parameter optimization can be performed efﬁciently for MFs. We then show that the proposed scalable approaches compare favorably with existing ones in terms of prediction accuracy and/or required training time. Finally, we report on some experiments performed on MovieLens and Jester data sets. Keywords: collaborative ﬁltering, recommender systems, matrix factorization, neighbor based correction, Netﬂix prize ∗. All authors also afﬁliated with Gravity R&D; Ltd., 1092 Budapest, Kinizsi u. 11., Hungary; info@gravityrd.com. c 2009 G´ bor Tak´ cs, Istv´ n Pil´ szy, Botty´ n N´ meth and Domonkos Tikk. a a a a a e ´ ´ ´ TAK ACS , P IL ASZY, N E METH AND T IKK</p><p>4 0.12037514 <a title="2-tfidf-4" href="./jmlr-2009-A_Survey_of_Accuracy_Evaluation_Metrics_of_Recommendation_Tasks.html">4 jmlr-2009-A Survey of Accuracy Evaluation Metrics of Recommendation Tasks</a></p>
<p>Author: Asela Gunawardana, Guy Shani</p><p>Abstract: Recommender systems are now popular both commercially and in the research community, where many algorithms have been suggested for providing recommendations. These algorithms typically perform differently in various domains and tasks. Therefore, it is important from the research perspective, as well as from a practical view, to be able to decide on an algorithm that matches the domain and the task of interest. The standard way to make such decisions is by comparing a number of algorithms ofﬂine using some evaluation metric. Indeed, many evaluation metrics have been suggested for comparing recommendation algorithms. The decision on the proper evaluation metric is often critical, as each metric may favor a different algorithm. In this paper we review the proper construction of ofﬂine experiments for deciding on the most appropriate algorithm. We discuss three important tasks of recommender systems, and classify a set of appropriate well known evaluation metrics for each task. We demonstrate how using an improper evaluation metric can lead to the selection of an improper algorithm for the task of interest. We also discuss other important considerations when designing ofﬂine experiments. Keywords: recommender systems, collaborative ﬁltering, statistical analysis, comparative studies</p><p>5 0.089676306 <a title="2-tfidf-5" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: We continue our recent study on constructing a reﬁnement kernel for a given kernel so that the reproducing kernel Hilbert space associated with the reﬁnement kernel contains that with the original kernel as a subspace. To motivate this study, we ﬁrst develop a reﬁnement kernel method for learning, which gives an efﬁcient algorithm for updating a learning predictor. Several characterizations of reﬁnement kernels are then presented. It is shown that a nontrivial reﬁnement kernel for a given kernel always exists if the input space has an inﬁnite cardinal number. Reﬁnement kernels for translation invariant kernels and Hilbert-Schmidt kernels are investigated. Various concrete examples are provided. Keywords: reproducing kernels, reproducing kernel Hilbert spaces, learning with kernels, reﬁnement kernels, translation invariant kernels, Hilbert-Schmidt kernels</p><p>6 0.084768213 <a title="2-tfidf-6" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>7 0.071971104 <a title="2-tfidf-7" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>8 0.066645689 <a title="2-tfidf-8" href="./jmlr-2009-Stable_and_Efficient_Gaussian_Process_Calculations.html">88 jmlr-2009-Stable and Efficient Gaussian Process Calculations</a></p>
<p>9 0.063077308 <a title="2-tfidf-9" href="./jmlr-2009-Estimating_Labels_from_Label_Proportions.html">29 jmlr-2009-Estimating Labels from Label Proportions</a></p>
<p>10 0.061210021 <a title="2-tfidf-10" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>11 0.059025157 <a title="2-tfidf-11" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>12 0.058052525 <a title="2-tfidf-12" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>13 0.05634968 <a title="2-tfidf-13" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>14 0.053493094 <a title="2-tfidf-14" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>15 0.052939355 <a title="2-tfidf-15" href="./jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">26 jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<p>16 0.051579192 <a title="2-tfidf-16" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>17 0.05046929 <a title="2-tfidf-17" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>18 0.050015699 <a title="2-tfidf-18" href="./jmlr-2009-Online_Learning_with_Sample_Path_Constraints.html">67 jmlr-2009-Online Learning with Sample Path Constraints</a></p>
<p>19 0.045818541 <a title="2-tfidf-19" href="./jmlr-2009-Marginal_Likelihood_Integrals_for_Mixtures_of_Independence_Models.html">53 jmlr-2009-Marginal Likelihood Integrals for Mixtures of Independence Models</a></p>
<p>20 0.044867903 <a title="2-tfidf-20" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.249), (1, -0.048), (2, 0.152), (3, -0.162), (4, -0.19), (5, 0.095), (6, 0.177), (7, -0.145), (8, 0.134), (9, -0.346), (10, -0.012), (11, -0.049), (12, -0.027), (13, -0.033), (14, 0.019), (15, 0.075), (16, -0.024), (17, 0.018), (18, 0.037), (19, 0.04), (20, 0.06), (21, 0.079), (22, -0.127), (23, 0.026), (24, 0.132), (25, -0.05), (26, 0.074), (27, 0.029), (28, 0.061), (29, 0.11), (30, -0.06), (31, 0.031), (32, -0.033), (33, 0.07), (34, -0.049), (35, 0.074), (36, 0.058), (37, 0.036), (38, 0.052), (39, -0.037), (40, -0.04), (41, -0.03), (42, 0.009), (43, 0.036), (44, -0.045), (45, 0.026), (46, -0.038), (47, 0.013), (48, 0.078), (49, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95604259 <a title="2-lsi-1" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>Author: Jacob Abernethy, Francis Bach, Theodoros Evgeniou, Jean-Philippe Vert</p><p>Abstract: We present a general approach for collaborative ﬁltering (CF) using spectral regularization to learn linear operators mapping a set of “users” to a set of possibly desired “objects”. In particular, several recent low-rank type matrix-completion methods for CF are shown to be special cases of our proposed framework. Unlike existing regularization-based CF, our approach can be used to incorporate additional information such as attributes of the users/objects—a feature currently lacking in existing regularization-based CF approaches—using popular and well-known kernel methods. We provide novel representer theorems that we use to develop new estimation methods. We then provide learning algorithms based on low-rank decompositions and test them on a standard CF data set. The experiments indicate the advantages of generalizing the existing regularization-based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach. Keywords: collaborative ﬁltering, matrix completion, kernel methods, spectral regularization</p><p>2 0.72430348 <a title="2-lsi-2" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, regularization</p><p>3 0.57593352 <a title="2-lsi-3" href="./jmlr-2009-Scalable_Collaborative_Filtering_Approaches_for_Large_Recommender_Systems%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">84 jmlr-2009-Scalable Collaborative Filtering Approaches for Large Recommender Systems    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Gábor Takács, István Pilászy, Bottyán Németh, Domonkos Tikk</p><p>Abstract: The collaborative ﬁltering (CF) using known user ratings of items has proved to be effective for predicting user preferences in item selection. This thriving subﬁeld of machine learning became popular in the late 1990s with the spread of online services that use recommender systems, such as Amazon, Yahoo! Music, and Netﬂix. CF approaches are usually designed to work on very large data sets. Therefore the scalability of the methods is crucial. In this work, we propose various scalable solutions that are validated against the Netﬂix Prize data set, currently the largest publicly available collection. First, we propose various matrix factorization (MF) based techniques. Second, a neighbor correction method for MF is outlined, which alloys the global perspective of MF and the localized property of neighbor based approaches efﬁciently. In the experimentation section, we ﬁrst report on some implementation issues, and we suggest on how parameter optimization can be performed efﬁciently for MFs. We then show that the proposed scalable approaches compare favorably with existing ones in terms of prediction accuracy and/or required training time. Finally, we report on some experiments performed on MovieLens and Jester data sets. Keywords: collaborative ﬁltering, recommender systems, matrix factorization, neighbor based correction, Netﬂix prize ∗. All authors also afﬁliated with Gravity R&D; Ltd., 1092 Budapest, Kinizsi u. 11., Hungary; info@gravityrd.com. c 2009 G´ bor Tak´ cs, Istv´ n Pil´ szy, Botty´ n N´ meth and Domonkos Tikk. a a a a a e ´ ´ ´ TAK ACS , P IL ASZY, N E METH AND T IKK</p><p>4 0.49443996 <a title="2-lsi-4" href="./jmlr-2009-A_Survey_of_Accuracy_Evaluation_Metrics_of_Recommendation_Tasks.html">4 jmlr-2009-A Survey of Accuracy Evaluation Metrics of Recommendation Tasks</a></p>
<p>Author: Asela Gunawardana, Guy Shani</p><p>Abstract: Recommender systems are now popular both commercially and in the research community, where many algorithms have been suggested for providing recommendations. These algorithms typically perform differently in various domains and tasks. Therefore, it is important from the research perspective, as well as from a practical view, to be able to decide on an algorithm that matches the domain and the task of interest. The standard way to make such decisions is by comparing a number of algorithms ofﬂine using some evaluation metric. Indeed, many evaluation metrics have been suggested for comparing recommendation algorithms. The decision on the proper evaluation metric is often critical, as each metric may favor a different algorithm. In this paper we review the proper construction of ofﬂine experiments for deciding on the most appropriate algorithm. We discuss three important tasks of recommender systems, and classify a set of appropriate well known evaluation metrics for each task. We demonstrate how using an improper evaluation metric can lead to the selection of an improper algorithm for the task of interest. We also discuss other important considerations when designing ofﬂine experiments. Keywords: recommender systems, collaborative ﬁltering, statistical analysis, comparative studies</p><p>5 0.45186776 <a title="2-lsi-5" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>Author: Haizhang Zhang, Yuesheng Xu, Jun Zhang</p><p>Abstract: We introduce the notion of reproducing kernel Banach spaces (RKBS) and study special semiinner-product RKBS by making use of semi-inner-products and the duality mapping. Properties of an RKBS and its reproducing kernel are investigated. As applications, we develop in the framework of RKBS standard learning schemes including minimal norm interpolation, regularization network, support vector machines, and kernel principal component analysis. In particular, existence, uniqueness and representer theorems are established. Keywords: reproducing kernel Banach spaces, reproducing kernels, learning theory, semi-innerproducts, representer theorems</p><p>6 0.44395307 <a title="2-lsi-6" href="./jmlr-2009-Stable_and_Efficient_Gaussian_Process_Calculations.html">88 jmlr-2009-Stable and Efficient Gaussian Process Calculations</a></p>
<p>7 0.35691199 <a title="2-lsi-7" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>8 0.35625157 <a title="2-lsi-8" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>9 0.35602942 <a title="2-lsi-9" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>10 0.32830778 <a title="2-lsi-10" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>11 0.32716393 <a title="2-lsi-11" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>12 0.29094559 <a title="2-lsi-12" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>13 0.284302 <a title="2-lsi-13" href="./jmlr-2009-Estimating_Labels_from_Label_Proportions.html">29 jmlr-2009-Estimating Labels from Label Proportions</a></p>
<p>14 0.27522305 <a title="2-lsi-14" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>15 0.26591343 <a title="2-lsi-15" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>16 0.2542316 <a title="2-lsi-16" href="./jmlr-2009-Online_Learning_with_Sample_Path_Constraints.html">67 jmlr-2009-Online Learning with Sample Path Constraints</a></p>
<p>17 0.24401812 <a title="2-lsi-17" href="./jmlr-2009-Marginal_Likelihood_Integrals_for_Mixtures_of_Independence_Models.html">53 jmlr-2009-Marginal Likelihood Integrals for Mixtures of Independence Models</a></p>
<p>18 0.2276841 <a title="2-lsi-18" href="./jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">69 jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>19 0.22579008 <a title="2-lsi-19" href="./jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">16 jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>20 0.22057286 <a title="2-lsi-20" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.01), (11, 0.041), (19, 0.01), (21, 0.474), (26, 0.019), (38, 0.031), (47, 0.025), (52, 0.04), (55, 0.025), (58, 0.025), (66, 0.108), (68, 0.022), (73, 0.022), (90, 0.057), (96, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.85258168 <a title="2-lda-1" href="./jmlr-2009-Cautious_Collective_Classification.html">15 jmlr-2009-Cautious Collective Classification</a></p>
<p>Author: Luke K. McDowell, Kalyan Moy Gupta, David W. Aha</p><p>Abstract: Many collective classiﬁcation (CC) algorithms have been shown to increase accuracy when instances are interrelated. However, CC algorithms must be carefully applied because their use of estimated labels can in some cases decrease accuracy. In this article, we show that managing this label uncertainty through cautious algorithmic behavior is essential to achieving maximal, robust performance. First, we describe cautious inference and explain how four well-known families of CC algorithms can be parameterized to use varying degrees of such caution. Second, we introduce cautious learning and show how it can be used to improve the performance of almost any CC algorithm, with or without cautious inference. We then evaluate cautious inference and learning for the four collective inference families, with three local classiﬁers and a range of both synthetic and real-world data. We ﬁnd that cautious learning and cautious inference typically outperform less cautious approaches. In addition, we identify the data characteristics that predict more substantial performance differences. Our results reveal that the degree of caution used usually has a larger impact on performance than the choice of the underlying inference algorithm. Together, these results identify the most appropriate CC algorithms to use for particular task characteristics and explain multiple conﬂicting ﬁndings from prior CC research. Keywords: collective inference, statistical relational learning, approximate probabilistic inference, networked data, cautious inference</p><p>same-paper 2 0.79357845 <a title="2-lda-2" href="./jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">2 jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>Author: Jacob Abernethy, Francis Bach, Theodoros Evgeniou, Jean-Philippe Vert</p><p>Abstract: We present a general approach for collaborative ﬁltering (CF) using spectral regularization to learn linear operators mapping a set of “users” to a set of possibly desired “objects”. In particular, several recent low-rank type matrix-completion methods for CF are shown to be special cases of our proposed framework. Unlike existing regularization-based CF, our approach can be used to incorporate additional information such as attributes of the users/objects—a feature currently lacking in existing regularization-based CF approaches—using popular and well-known kernel methods. We provide novel representer theorems that we use to develop new estimation methods. We then provide learning algorithms based on low-rank decompositions and test them on a standard CF data set. The experiments indicate the advantages of generalizing the existing regularization-based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach. Keywords: collaborative ﬁltering, matrix completion, kernel methods, spectral regularization</p><p>3 0.3697505 <a title="2-lda-3" href="./jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">100 jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<p>Author: Andreas Argyriou, Charles A. Micchelli, Massimiliano Pontil</p><p>Abstract: We consider a general class of regularization methods which learn a vector of parameters on the basis of linear measurements. It is well known that if the regularizer is a nondecreasing function of the L2 norm, then the learned vector is a linear combination of the input data. This result, known as the representer theorem, lies at the basis of kernel-based methods in machine learning. In this paper, we prove the necessity of the above condition, in the case of differentiable regularizers. We further extend our analysis to regularization methods which learn a matrix, a problem which is motivated by the application to multi-task learning. In this context, we study a more general representer theorem, which holds for a larger class of regularizers. We provide a necessary and sufﬁcient condition characterizing this class of matrix regularizers and we highlight some concrete examples of practical importance. Our analysis uses basic principles from matrix theory, especially the useful notion of matrix nondecreasing functions. Keywords: kernel methods, matrix learning, minimal norm interpolation, multi-task learning, regularization</p><p>4 0.35831857 <a title="2-lda-4" href="./jmlr-2009-Refinement_of_Reproducing_Kernels.html">78 jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: We continue our recent study on constructing a reﬁnement kernel for a given kernel so that the reproducing kernel Hilbert space associated with the reﬁnement kernel contains that with the original kernel as a subspace. To motivate this study, we ﬁrst develop a reﬁnement kernel method for learning, which gives an efﬁcient algorithm for updating a learning predictor. Several characterizations of reﬁnement kernels are then presented. It is shown that a nontrivial reﬁnement kernel for a given kernel always exists if the input space has an inﬁnite cardinal number. Reﬁnement kernels for translation invariant kernels and Hilbert-Schmidt kernels are investigated. Various concrete examples are provided. Keywords: reproducing kernels, reproducing kernel Hilbert spaces, learning with kernels, reﬁnement kernels, translation invariant kernels, Hilbert-Schmidt kernels</p><p>5 0.34695548 <a title="2-lda-5" href="./jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">80 jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>Author: Haizhang Zhang, Yuesheng Xu, Jun Zhang</p><p>Abstract: We introduce the notion of reproducing kernel Banach spaces (RKBS) and study special semiinner-product RKBS by making use of semi-inner-products and the duality mapping. Properties of an RKBS and its reproducing kernel are investigated. As applications, we develop in the framework of RKBS standard learning schemes including minimal norm interpolation, regularization network, support vector machines, and kernel principal component analysis. In particular, existence, uniqueness and representer theorems are established. Keywords: reproducing kernel Banach spaces, reproducing kernels, learning theory, semi-innerproducts, representer theorems</p><p>6 0.34477845 <a title="2-lda-6" href="./jmlr-2009-Scalable_Collaborative_Filtering_Approaches_for_Large_Recommender_Systems%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">84 jmlr-2009-Scalable Collaborative Filtering Approaches for Large Recommender Systems    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>7 0.33726412 <a title="2-lda-7" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>8 0.33240783 <a title="2-lda-8" href="./jmlr-2009-A_Survey_of_Accuracy_Evaluation_Metrics_of_Recommendation_Tasks.html">4 jmlr-2009-A Survey of Accuracy Evaluation Metrics of Recommendation Tasks</a></p>
<p>9 0.32706812 <a title="2-lda-9" href="./jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">85 jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>10 0.32469779 <a title="2-lda-10" href="./jmlr-2009-Nonextensive_Information_Theoretic_Kernels_on_Measures.html">61 jmlr-2009-Nonextensive Information Theoretic Kernels on Measures</a></p>
<p>11 0.32089266 <a title="2-lda-11" href="./jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">22 jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>12 0.31711024 <a title="2-lda-12" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>13 0.31664893 <a title="2-lda-13" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<p>14 0.31085396 <a title="2-lda-14" href="./jmlr-2009-Learning_Linear_Ranking_Functions_for_Beam_Search_with_Application_to_Planning.html">47 jmlr-2009-Learning Linear Ranking Functions for Beam Search with Application to Planning</a></p>
<p>15 0.30707026 <a title="2-lda-15" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>16 0.30591223 <a title="2-lda-16" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>17 0.30340999 <a title="2-lda-17" href="./jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection.html">34 jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</a></p>
<p>18 0.30322385 <a title="2-lda-18" href="./jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">51 jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>19 0.30236107 <a title="2-lda-19" href="./jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">26 jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<p>20 0.29472876 <a title="2-lda-20" href="./jmlr-2009-Markov_Properties_for_Linear_Causal_Models_with_Correlated_Errors%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">54 jmlr-2009-Markov Properties for Linear Causal Models with Correlated Errors    (Special Topic on Causality)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
