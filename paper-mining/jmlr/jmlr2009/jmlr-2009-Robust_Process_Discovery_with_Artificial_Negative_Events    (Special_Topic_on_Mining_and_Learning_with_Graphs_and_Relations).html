<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-81" href="#">jmlr2009-81</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</h1>
<br/><p>Source: <a title="jmlr-2009-81-pdf" href="http://jmlr.org/papers/volume10/goedertier09a/goedertier09a.pdf">pdf</a></p><p>Author: Stijn Goedertier, David Martens, Jan Vanthienen, Bart Baesens</p><p>Abstract: Process discovery is the automated construction of structured process models from information system event logs. Such event logs often contain positive examples only. Without negative examples, it is a challenge to strike the right balance between recall and speciﬁcity, and to deal with problems such as expressiveness, noise, incomplete event logs, or the inclusion of prior knowledge. In this paper, we present a conﬁgurable technique that deals with these challenges by representing process discovery as a multi-relational classiﬁcation problem on event logs supplemented with Artiﬁcially Generated Negative Events (AGNEs). This problem formulation allows using learning algorithms and evaluation techniques that are well-know in the machine learning community. Moreover, it allows users to have a declarative control over the inductive bias and language bias. Keywords: graph pattern discovery, inductive logic programming, Petri net, process discovery, positive data only</p><p>Reference: <a title="jmlr-2009-81-reference" href="../jmlr2009_reference/jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we present a conﬁgurable technique that deals with these challenges by representing process discovery as a multi-relational classiﬁcation problem on event logs supplemented with Artiﬁcially Generated Negative Events (AGNEs). [sent-16, score-0.656]
</p><p>2 The analysis of such event logs can provide insight into how processes actually take place, and to what extent actual processes deviate from a normative process model. [sent-35, score-0.583]
</p><p>3 The goal of process discovery is to construct a useful process model that describes the event sequences recorded in the event log. [sent-38, score-0.966]
</p><p>4 The absence of negative information, makes the learning task aim at accurately summarizing an event log such that the discovered process model allows the observed behavior (recall) but does not include unintended, random behavior that is not present in the event log (speciﬁcity). [sent-66, score-0.998]
</p><p>5 In addition to accuracy, the learning problem faces challenges such as expressiveness, noise, incomplete event logs, and the inclusion of prior knowledge: • accuracy: accuracy refers to the extent to which the induced model ﬁts the behavior in the event log. [sent-67, score-0.73]
</p><p>6 The Petri net in 1306  ROBUST P ROCESS D ISCOVERY WITH A RTIFICIAL N EGATIVE E VENTS  Example 1, called a ﬂower model, is capable of parsing every sequence in the event log. [sent-69, score-0.525]
</p><p>7 • expressiveness: expressiveness relates to the ability to comprehensively summarize an event log using a rich palette of structures such as sequences, or-splits, and-splits (parallel threads), or-joins, and-joins, loops (iterations), history-dependent or-splits, and duplicate activities. [sent-75, score-0.535]
</p><p>8 • incomplete logs: incomplete event logs do not contain the complete set of sequences that occur according to the underlying, real-life process. [sent-79, score-0.648]
</p><p>9 In this paper, these challenges addressed by representing process discovery as an ILP classiﬁcation learning problem on event logs supplemented with artiﬁcially generated negative events (AGNEs). [sent-88, score-0.857]
</p><p>10 The AGNEs technique is capable of constructing Petri net models from event logs and has been implemented as a mining plugin in the ProM framework. [sent-89, score-0.662]
</p><p>11 A benchmark experiment with 34 artiﬁcial event logs and comparison to four state-of-the-art process discovery algorithms indicate that the technique is expressive, robust to noise, and capable of dealing with incomplete event logs. [sent-90, score-1.05]
</p><p>12 For process discovery, this multi-relational property is much desired, as it allows discovering patterns that relate the occurrence of an event to the occurrence of any other event in the event log. [sent-104, score-1.158]
</p><p>13 2 Event Logs In process discovery, an event log is a database of event sequences. [sent-129, score-0.793]
</p><p>14 Each event reports an instantaneous state change of an activity of a particular activity type. [sent-130, score-0.638]
</p><p>15 In process discovery, the MXML format for event logs (van Dongen and van der Aalst, 2005a) is the commonly accepted format for event logs. [sent-132, score-1.08]
</p><p>16 To use event logs with TILDE event logs have to be represented as a logical program. [sent-133, score-1.033]
</p><p>17 Let X be a set of event identiﬁers, P a set of case identiﬁers, the alphabet A a set of activity types, and E = {completed, completeRejected} a set of event types. [sent-134, score-0.82]
</p><p>18 An event predicate is a quintuple Event(x, p, a, e,t), where x ∈ X is the event identiﬁer, p ∈ P is the case identiﬁer, a ∈ A the activity type, e ∈ E the event type, and t ∈ N the time of occurrence of the event. [sent-135, score-1.225]
</p><p>19 The event types E = {completed, completeRejected} respectively indicate the completion of a particular activity or that the completion of a particular activity could not take place, a negative event. [sent-141, score-0.7]
</p><p>20 Let σ ∈ L be an event sequence, an ordered set of event identiﬁers x ∈ X of events pertaining to the same process instance as denoted by the case id; σ = {x | x ∈ X ∧ Case(x) = Case(σ)}. [sent-143, score-0.891]
</p><p>21 To evaluate the extent to which a Petri net is capable of parsing an event sequence, transitions might be forced to ﬁre. [sent-173, score-0.553]
</p><p>22 Process discovery algorithms generally include the assumption that event logs portray the complete behavior of the underlying process and implicitly use this completeness assumption to make a tradeoff between overly general and overly speciﬁc process models. [sent-181, score-0.804]
</p><p>23 The more recurrent behavior a process has, the more different kinds of event sequences a process can produce. [sent-186, score-0.591]
</p><p>24 Negative events record that at a particular position in an event sequence, a particular event cannot occur. [sent-198, score-0.807]
</p><p>25 At each position k in each event sequence τi , it is examined which negative events can be induced for this position. [sent-199, score-0.57]
</p><p>26 In a ﬁrst step, the event log is made more compact, by grouping process instances σ ∈ L that have identical sequences into grouped process instances τ ∈ M (line 1). [sent-201, score-0.655]
</p><p>27 Making a completeness assumption about an event log boils down to assuming that behavior that does not occur in the event log, should not occur in the process model to be learned. [sent-204, score-0.825]
</p><p>28 Negative examples can be introduced in grouped process instances τi by checking at any given positive event xk ∈ τi at position k = Position(xk , τi ) whether another event of interest zk of activity type b ∈ A\{AT(xk )} also could occur. [sent-205, score-0.959]
</p><p>29 For each event xk ∈ τi , it is tested whether there exists a similar sequence τ j ∈ AllParallelVariants(τ j ) : τ j = τi in the event log in which at that point a state transition yk has taken place that is similar to zk (line 6). [sent-206, score-0.794]
</p><p>30 Consequently, zk can be added as a negative event at this point k in the event sequence τi (lines 7–8). [sent-209, score-0.765]
</p><p>31 Finally, the negative events in the grouped process instances are used to induce negative events into the similar non-grouped sequences. [sent-211, score-0.541]
</p><p>32 If a grouped sequence τ contains negative events at position k, then the ungrouped sequence σ contains corresponding negative events at position k. [sent-212, score-0.527]
</p><p>33 To avoid an imbalance in the proportion of negative versus positive events the addition of negative events can be manipulated with a negative event injection probability π (line 13). [sent-214, score-0.837]
</p><p>34 The smaller π, the less negative events are generated at each position in the ungrouped event sequences. [sent-216, score-0.535]
</p><p>35 Example 2 illustrates how in an event log of two (grouped) sequences τ1 and τ2 artiﬁcial negative events can be generated. [sent-220, score-0.633]
</p><p>36 ) DriversLicense—Generating artiﬁcial negative events for a simpliﬁed event log with two abbreviated event sequences. [sent-239, score-0.91]
</p><p>37 Furthermore, the induction of negative events is dependent on the window size windowSize, and negative event injection probability π parameters. [sent-252, score-0.689]
</p><p>38 Alternatively, the history of each event could in part be represented as extra propositions, for instance by including all preceding positive events as extra columns in the event log. [sent-267, score-0.807]
</p><p>39 The classiﬁcation task of TILDE is to predict whether for a given activity type a ∈ A, at a given time point t ∈ N in a given sequence σ ∈ L, a positive or a negative event takes place. [sent-275, score-0.583]
</p><p>40 The primary objective of AGNEs being the construction of a graphical model from an event log, the language bias consists of a logical predicate that can represent the conditions under which a Petri net place contains a token. [sent-279, score-0.562]
</p><p>41 We require TILDE only to consider global sequence between activity types for which both a precedence and a response relationship has been detected from the event log, this is expressed as a language bias constraint. [sent-304, score-0.589]
</p><p>42 Therefore, the logic programs of activity preconditions are submitted to several rule-level pruning steps, to make sure that they do not produce redundant duplicate places in the Petri net to be constructed. [sent-315, score-0.537]
</p><p>43 These pruning steps occur among the conditions within a single precondition, within a set of preconditions to the same activity type and among preconditions of activity types that pertain to the same or-split. [sent-316, score-0.581]
</p><p>44 • true negative rate TN rate or speciﬁcity: the percentage of correctly classiﬁed negative events TN in the event log. [sent-356, score-0.597]
</p><p>45 Because a ﬂower model represents random behavior, it has a perfect recall of the all behavior in the event log but it also has much additional behavior compared to the event log. [sent-364, score-0.773]
</p><p>46 The measure is deﬁned as follows: • parsing measure PM: the number of sequences in the event log that are correctly parsed by the process model, divided by the total number of sequences in the event log. [sent-382, score-0.943]
</p><p>47 Likewise, in the presence of multiple enabled duplicate activities, it is a non-trivial problem of determining the optimal ﬁring, as the ﬁring of one duplicate activity affects the ability of the Petri net to replay the remaining events in a given sequence of events. [sent-408, score-0.698]
</p><p>48 Rozinat and van der Aalst (2008) present two local approaches that are based on heuristics involving the next activity event in the given sequence of events. [sent-409, score-0.754]
</p><p>49 Furthermore, it is to be noted that the behavioral appropriateness aB metric is not guaranteed to account for all non-local behavior in the event log (for instance, a non-local, non-free, historydependent choice construct that is part of a loop will not be detected by the measure). [sent-415, score-0.555]
</p><p>50 We therefore deﬁne: p p • behavioral recall rB : The behavioral recall rB metric is obtained by parsing each grouped event sequence. [sent-420, score-0.6]
</p><p>51 Let k represent the number of grouped sequences, ni the number of process instances, TPi number of events that are correctly parsed, and FN i the number events for which a transition was forced to ﬁre p for each grouped sequence i (1 ≤ i ≤ k). [sent-426, score-0.591]
</p><p>52 In the case of multiple enabled duplicate transitions, sequence replay ﬁres the transition of which the succeeding transition is the next positive event (or makes a random choice). [sent-428, score-0.662]
</p><p>53 In the case of multiple enabled silent transitions, log replay ﬁres the transition of which the succeeding transition is the next positive event (or makes a random choice). [sent-429, score-0.579]
</p><p>54 In contrast, whenever a negative event is encountered during sequence replay for which there is a corresponding transition enabled in the Petri net, the value for FP is increased by one. [sent-435, score-0.585]
</p><p>55 Let k represent the number of grouped sequences, ni the number of process instances, TN i number of negative events for which no transition was enabled, and FPi the number negative events for which a transition was enabled during the replay of each grouped sequence i (1 ≤ i ≤ k). [sent-437, score-0.869]
</p><p>56 For the moment, the negative event generation procedure is conﬁgurable by the negative event injection probability π, and whether or not it must account for the parallel variants of the given sequences of positive events. [sent-440, score-0.923]
</p><p>57 In particular, the frequent temporal constraint induction, the artiﬁcial negative event generation, the language bias constraints, and the pruning and graph construction algorithms all have been written in Prolog. [sent-445, score-0.539]
</p><p>58 For most of the event logs in the experiment, a reference model was available that can be assumed to p represent the behavior in the event log. [sent-472, score-0.865]
</p><p>59 The negative event injection probability π inﬂuences the proportion of artiﬁcially generated negative events in an event log. [sent-492, score-0.97]
</p><p>60 The primary intent of process discovery is to produce a model that accurately describes the event log at hand. [sent-606, score-0.532]
</p><p>61 4  Table 2: Parameter settings written in SWI-Prolog that groups similar sequences, randomly partitions the grouped event log in n = 10 uniform subgroups, and produces n pairs of training and test event logs. [sent-632, score-0.764]
</p><p>62 Training event logs are used for the purpose of process discovery. [sent-633, score-0.583]
</p><p>63 Speciﬁcity metrics must be calculated based on the combination of training and test event logs, the entire event log. [sent-636, score-0.713]
</p><p>64 Although this might seem unintuitive, speciﬁcity and speciﬁcity metrics make a completeness assumption as well, as they account for the amount of extra behavior in a process model vis-` -vis the event log (Rozinat and van der Aalst, 2008). [sent-637, score-0.699]
</p><p>65 To correctly evaluate the proposed a learning technique, it is important that the negative events in the test set accurately indicate the state transitions that are not present in the event log. [sent-638, score-0.598]
</p><p>66 For this reason, the negative events in the test log are created with information from the entire event log. [sent-639, score-0.576]
</p><p>67 Should the negative event generation be based on training set instances only, it is possible that additional, erroneous negative events are injected because it is possible that some behavior is not present in the test set. [sent-640, score-0.629]
</p><p>68 In the experiment, only 19 out of the 34 event logs were retained, as the other event logs have less than 10 different sequences. [sent-646, score-0.998]
</p><p>69 Moreover, the behavioral speciﬁcity metric sn shows genetic miner and heuristics miner to produce B slightly more speciﬁc models. [sent-657, score-0.671]
</p><p>70 The reason that it is not sensitive to incomplete event logs can be attributed to the following. [sent-694, score-0.529]
</p><p>71 However, this proportion of negative events is relatively small, as the negative event injection parameter π is not required to be excessively large. [sent-696, score-0.636]
</p><p>72 Finally, the negative event injection procedure takes into account parallelism and window size. [sent-699, score-0.557]
</p><p>73 In the herbstFig6p33 event log, the activity A occurs in three different contexts and AGNEs draws three different, identically labeled transitions correspondingly. [sent-707, score-0.549]
</p><p>74 Figure 2 compares the results of heuristics miner and AGNEs on this event log. [sent-708, score-0.592]
</p><p>75 The goal of process discovery is to give an idea of how the processes recorded in the event log actually have taken place. [sent-709, score-0.532]
</p><p>76 As is known from the literature, heuristics miner is resilient to noise, whereas the formal approaches of α+ and α++ and the genetic miner are known to overﬁt the noise in event logs. [sent-725, score-0.889]
</p><p>77 A Case Study This section shows the result of the AGNEs process discovery algorithm applied to an event log of customer-initiated processes, recorded by a European telecom provider. [sent-749, score-0.532]
</p><p>78 The case study gives an idea of the scalability of the algorithm towards large event logs and the usefulness of the AGNEs process discovery algorithm on realistic, real-life processes. [sent-752, score-0.656]
</p><p>79 89  Table 4: noise experiments - average, zero-noise training-log-based results  The event log consists of events about customer-initiated processes that are handled at three different locations by the employees of the telecom provider. [sent-1083, score-0.545]
</p><p>80 To account for the prior knowledge that no concurrent behavior is contained in the event log, heuristics miner needs to have an inﬁnite AND threshold. [sent-1138, score-0.658]
</p><p>81 The results of applying these process discovery algorithms on the ﬁltered event log are displayed in Figure 4. [sent-1142, score-0.532]
</p><p>82 Because the ﬂower model represents random behavior, it has a perfect recall of the all behavior in the event log but it also has much additional behavior compared to the event log. [sent-1146, score-0.773]
</p><p>83 p Comparing the available rB and sn outcomes, it can be observed that AGNEs performs better B than genetic miner, but worse than heuristics miner on the obtained event log. [sent-1158, score-0.712]
</p><p>84 The discovered process model by AGNEs has an accuracy of 80%, whereas the models discovered by genetic miner and heuristics miner have an accuracy of 73% and 92% respectively. [sent-1159, score-0.684]
</p><p>85 This case study brings forward that human-centric processes contained in the event log can take place in a less structured fashion than often is assumed by process discovery algorithms. [sent-1160, score-0.532]
</p><p>86 In this paper, a new approach for making the tradeoff between generality and speciﬁcity is proposed, by inducing artiﬁcial negative events using a (highly conﬁgurable) assumption about the completeness of the behavior displayed by the positive examples in the event log. [sent-1176, score-0.567]
</p><p>87 In analogy with the WEKA toolset for data mining (Witten and Frank, 2000), the ProM Framework consists of a large number of plugins for the analysis of event logs (Process Mining Group, TU/Eindhoven, 2008). [sent-1207, score-0.542]
</p><p>88 The Conformance Checker plugin (Rozinat and van der Aalst, 2008), for instance, allows identifying the discrepancies between an idealized process model and an event log. [sent-1208, score-0.581]
</p><p>89 Moreover, with a model that accurately describes the event log, it becomes possible to use the time-information in an event log for the purpose of performance analysis, using, for instance, the Performance Analysis with Petri nets plugin. [sent-1209, score-0.774]
</p><p>90 The α algorithm assumes event logs to be complete with respect to all allowable binary sequences and assumes that the event log does not contain any noise. [sent-1213, score-0.963]
</p><p>91 , 2007) fuzzy miner (G¨ nther and van der Aalst, 2007) u  Derives a so-called stochastic activity graph and converts it into a structured process model in the Adonis Modeling language. [sent-1239, score-0.587]
</p><p>92 A classiﬁcation technique that learns the preconditions of activities with the ICL ILP learner from event logs with user-supplied negative sequences. [sent-1247, score-0.751]
</p><p>93 The genetic miner is capable of detecting non-local patterns in the event log and is described to be fairly robust to noise. [sent-1257, score-0.671]
</p><p>94 G¨ nther and van der Aalst (2007) present an adaptive simpliﬁcation and visualization technique u based on signiﬁcance and correlation measures to visualize the behavior in event logs at various levels of abstraction. [sent-1268, score-0.694]
</p><p>95 The contribution of this approach is that it can also be applied to less structured, or unstructured processes of which the event logs cannot easily be summarized in concise, structured process models. [sent-1269, score-0.583]
</p><p>96 Conclusion Process discovery aims at accurately summarizing an event log in a structured process model. [sent-1284, score-0.532]
</p><p>97 So far, the discovery of structured processes by supplementing event logs with artiﬁcial negative events has not been considered in the literature. [sent-1285, score-0.773]
</p><p>98 In this paper, the generation of artiﬁcial negative events gives rise to a new process discovery algorithm and to new metrics for quantifying the recall and speciﬁcity of a process model vis-` -vis an event log. [sent-1287, score-0.821]
</p><p>99 a Process discovery algorithms must deal with challenges such as expressiveness, noise, incomplete event logs and the inclusion of prior knowledge. [sent-1288, score-0.602]
</p><p>100 In addition, our technique has a new, declarative way of dealing with incomplete event logs that diminishes the effects of concurrent and recurrent behavior on the generation of artiﬁcial negative events. [sent-1294, score-0.657]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('event', 0.334), ('ns', 0.331), ('petri', 0.328), ('agnes', 0.308), ('dispatch', 0.217), ('miner', 0.188), ('logs', 0.165), ('activity', 0.152), ('aalst', 0.146), ('events', 0.139), ('tilde', 0.133), ('alves', 0.121), ('baesens', 0.116), ('preconditions', 0.115), ('medeiros', 0.111), ('vanthienen', 0.111), ('der', 0.105), ('city', 0.091), ('artens', 0.091), ('oedertier', 0.091), ('net', 0.09), ('duplicate', 0.089), ('egative', 0.086), ('rtificial', 0.086), ('vents', 0.086), ('weijters', 0.086), ('process', 0.084), ('wil', 0.081), ('genetic', 0.078), ('activities', 0.075), ('discovery', 0.073), ('heuristics', 0.07), ('behavioral', 0.07), ('rb', 0.069), ('parallelism', 0.069), ('nets', 0.065), ('transitions', 0.063), ('negative', 0.062), ('tokens', 0.061), ('replay', 0.061), ('iscovery', 0.06), ('rocess', 0.06), ('van', 0.058), ('sequences', 0.057), ('grouped', 0.055), ('blockeel', 0.055), ('window', 0.053), ('driverslicensel', 0.05), ('rozinat', 0.05), ('ower', 0.05), ('transition', 0.05), ('ow', 0.05), ('ilp', 0.049), ('pruning', 0.047), ('prom', 0.045), ('metrics', 0.045), ('logic', 0.044), ('mining', 0.043), ('license', 0.043), ('appropriateness', 0.043), ('enabled', 0.043), ('sn', 0.042), ('log', 0.041), ('business', 0.041), ('anton', 0.04), ('dongen', 0.04), ('expressiveness', 0.04), ('gurable', 0.04), ('language', 0.039), ('tness', 0.039), ('injection', 0.039), ('discovered', 0.038), ('ab', 0.037), ('parsing', 0.036), ('occurrence', 0.036), ('sequence', 0.035), ('parallel', 0.035), ('logical', 0.035), ('ferreira', 0.035), ('goedertier', 0.035), ('metric', 0.035), ('predicate', 0.035), ('constructs', 0.035), ('ni', 0.034), ('concurrent', 0.034), ('behavior', 0.032), ('complete', 0.032), ('loops', 0.031), ('noise', 0.031), ('queues', 0.031), ('eindhoven', 0.03), ('karla', 0.03), ('stijn', 0.03), ('incomplete', 0.03), ('capable', 0.03), ('bias', 0.029), ('conjunctions', 0.028), ('locality', 0.028), ('frequent', 0.028), ('pm', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000011 <a title="81-tfidf-1" href="./jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Stijn Goedertier, David Martens, Jan Vanthienen, Bart Baesens</p><p>Abstract: Process discovery is the automated construction of structured process models from information system event logs. Such event logs often contain positive examples only. Without negative examples, it is a challenge to strike the right balance between recall and speciﬁcity, and to deal with problems such as expressiveness, noise, incomplete event logs, or the inclusion of prior knowledge. In this paper, we present a conﬁgurable technique that deals with these challenges by representing process discovery as a multi-relational classiﬁcation problem on event logs supplemented with Artiﬁcially Generated Negative Events (AGNEs). This problem formulation allows using learning algorithms and evaluation techniques that are well-know in the machine learning community. Moreover, it allows users to have a declarative control over the inductive bias and language bias. Keywords: graph pattern discovery, inductive logic programming, Petri net, process discovery, positive data only</p><p>2 0.051818036 <a title="81-tfidf-2" href="./jmlr-2009-Supervised_Descriptive_Rule_Discovery%3A_A_Unifying_Survey_of_Contrast_Set%2C_Emerging_Pattern_and_Subgroup_Mining.html">92 jmlr-2009-Supervised Descriptive Rule Discovery: A Unifying Survey of Contrast Set, Emerging Pattern and Subgroup Mining</a></p>
<p>Author: Petra Kralj Novak, Nada Lavrač, Geoffrey I. Webb</p><p>Abstract: This paper gives a survey of contrast set mining (CSM), emerging pattern mining (EPM), and subgroup discovery (SD) in a unifying framework named supervised descriptive rule discovery. While all these research areas aim at discovering patterns in the form of rules induced from labeled data, they use different terminology and task deﬁnitions, claim to have different goals, claim to use different rule learning heuristics, and use different means for selecting subsets of induced patterns. This paper contributes a novel understanding of these subareas of data mining by presenting a uniﬁed terminology, by explaining the apparent differences between the learning tasks as variants of a unique supervised descriptive rule discovery task and by exploring the apparent differences between the approaches. It also shows that various rule learning heuristics used in CSM, EPM and SD algorithms all aim at optimizing a trade off between rule coverage and precision. The commonalities (and differences) between the approaches are showcased on a selection of best known variants of CSM, EPM and SD algorithms. The paper also provides a critical survey of existing supervised descriptive rule discovery visualization methods. Keywords: descriptive rules, rule learning, contrast set mining, emerging patterns, subgroup discovery</p><p>3 0.045829531 <a title="81-tfidf-3" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>Author: Alexander L. Strehl, Lihong Li, Michael L. Littman</p><p>Abstract: We study the problem of learning near-optimal behavior in ﬁnite Markov Decision Processes (MDPs) with a polynomial number of samples. These “PAC-MDP” algorithms include the wellknown E3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a uniﬁed theoretical framework. A more reﬁned analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX. Keywords: reinforcement learning, Markov decision processes, PAC-MDP, exploration, sample complexity</p><p>4 0.041211572 <a title="81-tfidf-4" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>Author: Zeeshan Syed, Piotr Indyk, John Guttag</p><p>Abstract: In this paper, we present an automated approach to discover patterns that can distinguish between sequences belonging to different labeled groups. Our method searches for approximately conserved motifs that occur with varying statistical properties in positive and negative training examples. We propose a two-step process to discover such patterns. Using locality sensitive hashing (LSH), we ﬁrst estimate the frequency of all subsequences and their approximate matches within a given Hamming radius in labeled examples. The discriminative ability of each pattern is then assessed from the estimated frequencies by concordance and rank sum testing. The use of LSH to identify approximate matches for each candidate pattern helps reduce the runtime of our method. Space requirements are reduced by decomposing the search problem into an iterative method that uses a single LSH table in memory. We propose two further optimizations to the search for discriminative patterns. Clustering with redundancy based on a 2-approximate solution of the k-center problem decreases the number of overlapping approximate groups while providing exhaustive coverage of the search space. Sequential statistical methods allow the search process to use data from only as many training examples as are needed to assess signiﬁcance. We evaluated our algorithm on data sets from different applications to discover sequential patterns for classiﬁcation. On nucleotide sequences from the Drosophila genome compared with random background sequences, our method was able to discover approximate binding sites that were preserved upstream of genes. We observed a similar result in experiments on ChIP-on-chip data. For cardiovascular data from patients admitted with acute coronary syndromes, our pattern discovery approach identiﬁed approximately conserved sequences of morphology variations that were predictive of future death in a test population. Our data showed that the use of LSH, clustering, and sequential statistic</p><p>5 0.036252789 <a title="81-tfidf-5" href="./jmlr-2009-Strong_Limit_Theorems_for_the_Bayesian_Scoring_Criterion_in_Bayesian_Networks.html">89 jmlr-2009-Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks</a></p>
<p>Author: Nikolai Slobodianik, Dmitry Zaporozhets, Neal Madras</p><p>Abstract: In the machine learning community, the Bayesian scoring criterion is widely used for model selection problems. One of the fundamental theoretical properties justifying the usage of the Bayesian scoring criterion is its consistency. In this paper we reﬁne this property for the case of binomial Bayesian network models. As a by-product of our derivations we establish strong consistency and obtain the law of iterated logarithm for the Bayesian scoring criterion. Keywords: Bayesian networks, consistency, scoring criterion, model selection, BIC</p><p>6 0.035367794 <a title="81-tfidf-6" href="./jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</a></p>
<p>7 0.034717493 <a title="81-tfidf-7" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>8 0.033901311 <a title="81-tfidf-8" href="./jmlr-2009-Polynomial-Delay_Enumeration_of_Monotonic_Graph_Classes.html">72 jmlr-2009-Polynomial-Delay Enumeration of Monotonic Graph Classes</a></p>
<p>9 0.031272221 <a title="81-tfidf-9" href="./jmlr-2009-Hybrid_MPI_OpenMP_Parallel_Linear_Support_Vector_Machine_Training.html">39 jmlr-2009-Hybrid MPI OpenMP Parallel Linear Support Vector Machine Training</a></p>
<p>10 0.028905965 <a title="81-tfidf-10" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>11 0.027386967 <a title="81-tfidf-11" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<p>12 0.02702958 <a title="81-tfidf-12" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>13 0.026346173 <a title="81-tfidf-13" href="./jmlr-2009-A_Survey_of_Accuracy_Evaluation_Metrics_of_Recommendation_Tasks.html">4 jmlr-2009-A Survey of Accuracy Evaluation Metrics of Recommendation Tasks</a></p>
<p>14 0.025396524 <a title="81-tfidf-14" href="./jmlr-2009-DL-Learner%3A_Learning_Concepts_in_Description_Logics.html">20 jmlr-2009-DL-Learner: Learning Concepts in Description Logics</a></p>
<p>15 0.025135132 <a title="81-tfidf-15" href="./jmlr-2009-Transfer_Learning_for_Reinforcement_Learning_Domains%3A_A_Survey.html">96 jmlr-2009-Transfer Learning for Reinforcement Learning Domains: A Survey</a></p>
<p>16 0.024166347 <a title="81-tfidf-16" href="./jmlr-2009-Learning_Acyclic_Probabilistic_Circuits_Using_Test_Paths.html">44 jmlr-2009-Learning Acyclic Probabilistic Circuits Using Test Paths</a></p>
<p>17 0.023936661 <a title="81-tfidf-17" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>18 0.023036893 <a title="81-tfidf-18" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>19 0.022964092 <a title="81-tfidf-19" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>20 0.022929745 <a title="81-tfidf-20" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, -0.086), (2, 0.033), (3, 0.034), (4, -0.008), (5, -0.064), (6, -0.019), (7, 0.036), (8, 0.048), (9, 0.023), (10, 0.024), (11, -0.084), (12, 0.004), (13, 0.136), (14, -0.081), (15, 0.023), (16, -0.058), (17, 0.136), (18, 0.031), (19, -0.0), (20, -0.096), (21, 0.049), (22, 0.054), (23, -0.084), (24, -0.032), (25, 0.279), (26, 0.01), (27, -0.124), (28, 0.311), (29, -0.118), (30, -0.077), (31, 0.005), (32, -0.119), (33, 0.026), (34, -0.189), (35, -0.135), (36, 0.342), (37, 0.02), (38, -0.023), (39, 0.03), (40, -0.031), (41, 0.043), (42, 0.181), (43, 0.092), (44, 0.116), (45, 0.017), (46, -0.063), (47, 0.028), (48, -0.413), (49, -0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97850043 <a title="81-lsi-1" href="./jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Stijn Goedertier, David Martens, Jan Vanthienen, Bart Baesens</p><p>Abstract: Process discovery is the automated construction of structured process models from information system event logs. Such event logs often contain positive examples only. Without negative examples, it is a challenge to strike the right balance between recall and speciﬁcity, and to deal with problems such as expressiveness, noise, incomplete event logs, or the inclusion of prior knowledge. In this paper, we present a conﬁgurable technique that deals with these challenges by representing process discovery as a multi-relational classiﬁcation problem on event logs supplemented with Artiﬁcially Generated Negative Events (AGNEs). This problem formulation allows using learning algorithms and evaluation techniques that are well-know in the machine learning community. Moreover, it allows users to have a declarative control over the inductive bias and language bias. Keywords: graph pattern discovery, inductive logic programming, Petri net, process discovery, positive data only</p><p>2 0.27700749 <a title="81-lsi-2" href="./jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">45 jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>Author: Zeeshan Syed, Piotr Indyk, John Guttag</p><p>Abstract: In this paper, we present an automated approach to discover patterns that can distinguish between sequences belonging to different labeled groups. Our method searches for approximately conserved motifs that occur with varying statistical properties in positive and negative training examples. We propose a two-step process to discover such patterns. Using locality sensitive hashing (LSH), we ﬁrst estimate the frequency of all subsequences and their approximate matches within a given Hamming radius in labeled examples. The discriminative ability of each pattern is then assessed from the estimated frequencies by concordance and rank sum testing. The use of LSH to identify approximate matches for each candidate pattern helps reduce the runtime of our method. Space requirements are reduced by decomposing the search problem into an iterative method that uses a single LSH table in memory. We propose two further optimizations to the search for discriminative patterns. Clustering with redundancy based on a 2-approximate solution of the k-center problem decreases the number of overlapping approximate groups while providing exhaustive coverage of the search space. Sequential statistical methods allow the search process to use data from only as many training examples as are needed to assess signiﬁcance. We evaluated our algorithm on data sets from different applications to discover sequential patterns for classiﬁcation. On nucleotide sequences from the Drosophila genome compared with random background sequences, our method was able to discover approximate binding sites that were preserved upstream of genes. We observed a similar result in experiments on ChIP-on-chip data. For cardiovascular data from patients admitted with acute coronary syndromes, our pattern discovery approach identiﬁed approximately conserved sequences of morphology variations that were predictive of future death in a test population. Our data showed that the use of LSH, clustering, and sequential statistic</p><p>3 0.26043761 <a title="81-lsi-3" href="./jmlr-2009-Supervised_Descriptive_Rule_Discovery%3A_A_Unifying_Survey_of_Contrast_Set%2C_Emerging_Pattern_and_Subgroup_Mining.html">92 jmlr-2009-Supervised Descriptive Rule Discovery: A Unifying Survey of Contrast Set, Emerging Pattern and Subgroup Mining</a></p>
<p>Author: Petra Kralj Novak, Nada Lavrač, Geoffrey I. Webb</p><p>Abstract: This paper gives a survey of contrast set mining (CSM), emerging pattern mining (EPM), and subgroup discovery (SD) in a unifying framework named supervised descriptive rule discovery. While all these research areas aim at discovering patterns in the form of rules induced from labeled data, they use different terminology and task deﬁnitions, claim to have different goals, claim to use different rule learning heuristics, and use different means for selecting subsets of induced patterns. This paper contributes a novel understanding of these subareas of data mining by presenting a uniﬁed terminology, by explaining the apparent differences between the learning tasks as variants of a unique supervised descriptive rule discovery task and by exploring the apparent differences between the approaches. It also shows that various rule learning heuristics used in CSM, EPM and SD algorithms all aim at optimizing a trade off between rule coverage and precision. The commonalities (and differences) between the approaches are showcased on a selection of best known variants of CSM, EPM and SD algorithms. The paper also provides a critical survey of existing supervised descriptive rule discovery visualization methods. Keywords: descriptive rules, rule learning, contrast set mining, emerging patterns, subgroup discovery</p><p>4 0.23533069 <a title="81-lsi-4" href="./jmlr-2009-DL-Learner%3A_Learning_Concepts_in_Description_Logics.html">20 jmlr-2009-DL-Learner: Learning Concepts in Description Logics</a></p>
<p>Author: Jens Lehmann</p><p>Abstract: In this paper, we introduce DL-Learner, a framework for learning in description logics and OWL. OWL is the ofÄ?Ĺš cial W3C standard ontology language for the Semantic Web. Concepts in this language can be learned for constructing and maintaining OWL ontologies or for solving problems similar to those in Inductive Logic Programming. DL-Learner includes several learning algorithms, support for different OWL formats, reasoner interfaces, and learning problems. It is a cross-platform framework implemented in Java. The framework allows easy programmatic access and provides a command line interface, a graphical interface as well as a WSDL-based web service. Keywords: concept learning, description logics, OWL, classiÄ?Ĺš cation, open-source</p><p>5 0.22878268 <a title="81-lsi-5" href="./jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">12 jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</a></p>
<p>Author: Saharon Rosset</p><p>Abstract: We show how to follow the path of cross validated solutions to families of regularized optimization problems, deﬁned by a combination of a parameterized loss function and a regularization term. A primary example is kernel quantile regression, where the parameter of the loss function is the quantile being estimated. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on real and artiﬁcial data. This algorithm allows us to efﬁciently solve the whole family of bi-level problems. We show how it can be extended to cover other modeling problems, like support vector regression, and alternative in-sample model selection approaches.1</p><p>6 0.20406248 <a title="81-lsi-6" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>7 0.17482938 <a title="81-lsi-7" href="./jmlr-2009-Learning_Acyclic_Probabilistic_Circuits_Using_Test_Paths.html">44 jmlr-2009-Learning Acyclic Probabilistic Circuits Using Test Paths</a></p>
<p>8 0.17455627 <a title="81-lsi-8" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>9 0.17243844 <a title="81-lsi-9" href="./jmlr-2009-Strong_Limit_Theorems_for_the_Bayesian_Scoring_Criterion_in_Bayesian_Networks.html">89 jmlr-2009-Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks</a></p>
<p>10 0.1710415 <a title="81-lsi-10" href="./jmlr-2009-Python_Environment_for_Bayesian_Learning%3A_Inferring_the_Structure_of_Bayesian_Networks_from_Knowledge_and_Data%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">76 jmlr-2009-Python Environment for Bayesian Learning: Inferring the Structure of Bayesian Networks from Knowledge and Data    (Machine Learning Open Source Software Paper)</a></p>
<p>11 0.16414535 <a title="81-lsi-11" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>12 0.13506284 <a title="81-lsi-12" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>13 0.13219726 <a title="81-lsi-13" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>14 0.13155136 <a title="81-lsi-14" href="./jmlr-2009-Structure_Spaces.html">90 jmlr-2009-Structure Spaces</a></p>
<p>15 0.12761441 <a title="81-lsi-15" href="./jmlr-2009-Improving_the_Reliability_of_Causal_Discovery_from_Small_Data_Sets_Using_Argumentation%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">41 jmlr-2009-Improving the Reliability of Causal Discovery from Small Data Sets Using Argumentation    (Special Topic on Causality)</a></p>
<p>16 0.12550323 <a title="81-lsi-16" href="./jmlr-2009-Cautious_Collective_Classification.html">15 jmlr-2009-Cautious Collective Classification</a></p>
<p>17 0.12273164 <a title="81-lsi-17" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>18 0.117418 <a title="81-lsi-18" href="./jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">36 jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<p>19 0.11415741 <a title="81-lsi-19" href="./jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">55 jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>20 0.1129137 <a title="81-lsi-20" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.022), (11, 0.022), (19, 0.017), (21, 0.011), (26, 0.042), (38, 0.033), (45, 0.457), (47, 0.01), (52, 0.03), (55, 0.027), (58, 0.023), (66, 0.085), (68, 0.015), (85, 0.012), (86, 0.014), (90, 0.044), (96, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.80486429 <a title="81-lda-1" href="./jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">81 jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Stijn Goedertier, David Martens, Jan Vanthienen, Bart Baesens</p><p>Abstract: Process discovery is the automated construction of structured process models from information system event logs. Such event logs often contain positive examples only. Without negative examples, it is a challenge to strike the right balance between recall and speciﬁcity, and to deal with problems such as expressiveness, noise, incomplete event logs, or the inclusion of prior knowledge. In this paper, we present a conﬁgurable technique that deals with these challenges by representing process discovery as a multi-relational classiﬁcation problem on event logs supplemented with Artiﬁcially Generated Negative Events (AGNEs). This problem formulation allows using learning algorithms and evaluation techniques that are well-know in the machine learning community. Moreover, it allows users to have a declarative control over the inductive bias and language bias. Keywords: graph pattern discovery, inductive logic programming, Petri net, process discovery, positive data only</p><p>2 0.6609357 <a title="81-lda-2" href="./jmlr-2009-Strong_Limit_Theorems_for_the_Bayesian_Scoring_Criterion_in_Bayesian_Networks.html">89 jmlr-2009-Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks</a></p>
<p>Author: Nikolai Slobodianik, Dmitry Zaporozhets, Neal Madras</p><p>Abstract: In the machine learning community, the Bayesian scoring criterion is widely used for model selection problems. One of the fundamental theoretical properties justifying the usage of the Bayesian scoring criterion is its consistency. In this paper we reﬁne this property for the case of binomial Bayesian network models. As a by-product of our derivations we establish strong consistency and obtain the law of iterated logarithm for the Bayesian scoring criterion. Keywords: Bayesian networks, consistency, scoring criterion, model selection, BIC</p><p>3 0.27396446 <a title="81-lda-3" href="./jmlr-2009-Consistency_and_Localizability.html">18 jmlr-2009-Consistency and Localizability</a></p>
<p>Author: Alon Zakai, Ya'acov Ritov</p><p>Abstract: We show that all consistent learning methodsĂ˘&euro;&rdquo;that is, that asymptotically achieve the lowest possible expected loss for any distribution on (X,Y )Ă˘&euro;&rdquo;are necessarily localizable, by which we mean that they do not signiÄ?Ĺš cantly change their response at a particular point when we show them only the part of the training set that is close to that point. This is true in particular for methods that appear to be deÄ?Ĺš ned in a non-local manner, such as support vector machines in classiÄ?Ĺš cation and least-squares estimators in regression. Aside from showing that consistency implies a speciÄ?Ĺš c form of localizability, we also show that consistency is logically equivalent to the combination of two properties: (1) a form of localizability, and (2) that the methodĂ˘&euro;&trade;s global mean (over the entire X distribution) correctly estimates the true mean. Consistency can therefore be seen as comprised of two aspects, one local and one global. Keywords: consistency, local learning, regression, classiÄ?Ĺš cation</p><p>4 0.24676557 <a title="81-lda-4" href="./jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">32 jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>Author: Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray, David Page</p><p>Abstract: A Boolean function f is correlation immune if each input variable is independent of the output, under the uniform distribution on inputs. For example, the parity function is correlation immune. We consider the problem of identifying relevant variables of a correlation immune function, in the presence of irrelevant variables. We address this problem in two different contexts. First, we analyze Skewing, a heuristic method that was developed to improve the ability of greedy decision tree algorithms to identify relevant variables of correlation immune Boolean functions, given examples drawn from the uniform distribution (Page and Ray, 2003). We present theoretical results revealing both the capabilities and limitations of skewing. Second, we explore the problem of identifying relevant variables in the Product Distribution Choice (PDC) learning model, a model in which the learner can choose product distributions and obtain examples from them. We prove a lemma establishing a property of Boolean functions that may be of independent interest. Using this lemma, we give two new algorithms for ﬁnding relevant variables of correlation immune functions in the PDC model. Keywords: correlation immune functions, skewing, relevant variables, Boolean functions, product distributions c 2009 Lisa Hellerstein, Bernard Rosell, Eric Bach, Soumya Ray and David Page. H ELLERSTEIN , ROSELL , BACH , R AY AND PAGE</p><p>5 0.24648453 <a title="81-lda-5" href="./jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">75 jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>Author: Emma Brunskill, Bethany R. Leffler, Lihong Li, Michael L. Littman, Nicholas Roy</p><p>Abstract: To quickly achieve good performance, reinforcement-learning algorithms for acting in large continuous-valued domains must use a representation that is both sufﬁciently powerful to capture important domain characteristics, and yet simultaneously allows generalization, or sharing, among experiences. Our algorithm balances this tradeoff by using a stochastic, switching, parametric dynamics representation. We argue that this model characterizes a number of signiﬁcant, real-world domains, such as robot navigation across varying terrain. We prove that this representational assumption allows our algorithm to be probably approximately correct with a sample complexity that scales polynomially with all problem-speciﬁc quantities including the state-space dimension. We also explicitly incorporate the error introduced by approximate planning in our sample complexity bounds, in contrast to prior Probably Approximately Correct (PAC) Markov Decision Processes (MDP) approaches, which typically assume the estimated MDP can be solved exactly. Our experimental results on constructing plans for driving to work using real car trajectory data, as well as a small robot experiment on navigating varying terrain, demonstrate that our dynamics representation enables us to capture real-world dynamics in a sufﬁcient manner to produce good performance. Keywords: reinforcement learning, provably efﬁcient learning</p><p>6 0.24430297 <a title="81-lda-6" href="./jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">19 jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>7 0.24269252 <a title="81-lda-7" href="./jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">85 jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>8 0.24193236 <a title="81-lda-8" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>9 0.24158055 <a title="81-lda-9" href="./jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">27 jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>10 0.24114725 <a title="81-lda-10" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>11 0.24010006 <a title="81-lda-11" href="./jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">31 jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>12 0.23890583 <a title="81-lda-12" href="./jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">93 jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>13 0.23838189 <a title="81-lda-13" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>14 0.23825134 <a title="81-lda-14" href="./jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">66 jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>15 0.23772216 <a title="81-lda-15" href="./jmlr-2009-Learning_Linear_Ranking_Functions_for_Beam_Search_with_Application_to_Planning.html">47 jmlr-2009-Learning Linear Ranking Functions for Beam Search with Application to Planning</a></p>
<p>16 0.23767595 <a title="81-lda-16" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>17 0.23762709 <a title="81-lda-17" href="./jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">79 jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>18 0.23670802 <a title="81-lda-18" href="./jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">94 jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>19 0.23622482 <a title="81-lda-19" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>20 0.23590706 <a title="81-lda-20" href="./jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">71 jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
