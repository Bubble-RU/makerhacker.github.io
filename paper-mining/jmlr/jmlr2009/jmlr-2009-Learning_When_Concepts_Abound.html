<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 jmlr-2009-Learning When Concepts Abound</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="../home/jmlr2009_home.html">jmlr2009</a> <a title="jmlr-2009-50" href="#">jmlr2009-50</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>50 jmlr-2009-Learning When Concepts Abound</h1>
<br/><p>Source: <a title="jmlr-2009-50-pdf" href="http://jmlr.org/papers/volume10/madani09a/madani09a.pdf">pdf</a></p><p>Author: Omid Madani, Michael Connor, Wiley Greiner</p><p>Abstract: Many learning tasks, such as large-scale text categorization and word prediction, can beneﬁt from efﬁcient training and classiﬁcation when the number of classes, in addition to instances and features, is large, that is, in the thousands and beyond. We investigate the learning of sparse class indices to address this challenge. An index is a mapping from features to classes. We compare the index-learning methods against other techniques, including one-versus-rest and top-down classiﬁcation using perceptrons and support vector machines. We ﬁnd that index learning is highly advantageous for space and time efﬁciency, at both training and classiﬁcation times. Moreover, this approach yields similar and at times better accuracies. On problems with hundreds of thousands of instances and thousands of classes, the index is learned in minutes, while other methods can take hours or days. As we explain, the design of the learning update enables conveniently constraining each feature to connect to a small subset of the classes in the index. This constraint is crucial for scalability. Given an instance with l active (positive-valued) features, each feature on average connecting to d classes in the index (in the order of 10s in our experiments), update and classiﬁcation take O(dl log(dl)). Keywords: index learning, many-class learning, multiclass learning, online learning, text categorization</p><p>Reference: <a title="jmlr-2009-50-reference" href="../jmlr2009_reference/jmlr-2009-Learning_When_Concepts_Abound_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 As we explain, the design of the learning update enables conveniently constraining each feature to connect to a small subset of the classes in the index. [sent-13, score-0.315]
</p><p>2 Given an instance with l active (positive-valued) features, each feature on average connecting to d classes in the index (in the order of 10s in our experiments), update and classiﬁcation take O(dl log(dl)). [sent-15, score-0.537]
</p><p>3 Keywords: index learning, many-class learning, multiclass learning, online learning, text categorization  1. [sent-16, score-0.369]
</p><p>4 Figure 1: The problem of quick classiﬁcation in the presence of myriad classes: How can a system quickly classify a given instance, speciﬁed by a feature vector x ∈ Rn , into a small subset of classes from among possibly millions of candidate classes (shown by small circles)? [sent-25, score-0.345]
</p><p>5 During classiﬁcation, given an instance containing certain features, the index is used (“looked up”) much like a typical inverted index for document retrieval would be. [sent-55, score-0.431]
</p><p>6 In our indexing algorithms it is the features that update and normalize their connections to the classes. [sent-124, score-0.376]
</p><p>7 In a multiclass calendar scheduling task (Blum, 1997), Blum investigates an algorithm in which each feature votes for (connects to) the majority class in the past 5 classes seen for that feature (the classes of the most recent 5 instances in which the feature was active). [sent-148, score-0.798]
</p><p>8 However, in our case, the classes (to be indexed), unlike the documents, are implicit, indirectly speciﬁed by the training instances (the instances are not the “documents” to be indexed), and the index construction becomes 1. [sent-185, score-0.524]
</p><p>9 As one simple consequence, the presence of a feature in a training instance that belongs to class c does not imply that the feature will point to class c in the index learned. [sent-188, score-0.558]
</p><p>10 Indexing could be used to index already trained (say linear) classiﬁers, but the issues of space and time efﬁcient learning remain, and accuracy can suffer when using binary classiﬁers for class ranking (see Section 6. [sent-195, score-0.335]
</p><p>11 Each training instance is speciﬁed by a vector of feature values, vx , as well as a class (or assigned label) that the instance belongs to,2 cx . [sent-243, score-0.572]
</p><p>12 vx [ f ] denotes the value of feature f in the vector of features of instance x, where vx [ f ] ≥ 0. [sent-247, score-0.605]
</p><p>13 If vx [ f ] > 0, we say feature f is active (in instance x), and denote this aspect by f ∈ x. [sent-248, score-0.396]
</p><p>14 Thus the vector vx corresponding to a document lives in an |F | dimensional space, where vx [i] = k iff the word with id i (corresponding to dimension i) appears k times in the document, where k ≥ 0 (other possibilities for feature values include Boolean and TFIDF weighting). [sent-263, score-0.477]
</p><p>15 Therefore, in typical text categorization tasks, the number of active features in an instance is the number of unique words that appear in the corresponding document. [sent-264, score-0.317]
</p><p>16 The number of classes is so large that indexing them, not unlike the inverted index used for retrieval of documents and other object types, is a plausible approach. [sent-319, score-0.47]
</p><p>17 Figure 3 presents the basic cycle of categorization via index look up and learning via index updating (adjustments to connection weights). [sent-329, score-0.471]
</p><p>18 , vx [ f ] > 0): For the ﬁrst dmax classes with highest 2. [sent-348, score-0.463]
</p><p>19 Retrieve, score, and rank classes via connection weight to f : active features of x 1. [sent-349, score-0.409]
</p><p>20 The score that a class c receives, sc , can be written as sc =  ∑ r f × w f ,c × vx [ f ],  (1)  f ∈x  where r f is a measure of the predictiveness power or the rating of feature f , and we describe a method for computing it in Section 4. [sent-373, score-0.441]
</p><p>21 In a sense, each active feature casts votes for a subset of the classes, and those classes receive and tally their incoming votes (scores). [sent-379, score-0.364]
</p><p>22 The positive scoring classes can then be ranked by their score, or, if it sufﬁces, only the maximum scoring class can be kept track of and reported. [sent-381, score-0.32]
</p><p>23 On instance x, and with d connections per feature in the index, there can be at most |vx |d unique classes scored. [sent-386, score-0.407]
</p><p>24 In our implementation, for each active feature, only at most the dmax classes (25 in our experiments) with highest connection weights to the feature participate in scoring. [sent-388, score-0.569]
</p><p>25 An edge connecting feature f to class c has a positive weight denoted by w f ,c , or wi, j for feature i and class j, and corresponds to a list entry in the list for feature f . [sent-395, score-0.487]
</p><p>26 Let kx be the rank of the highest ranked true class after presenting instance x to the system. [sent-422, score-0.345]
</p><p>27 2581  M ADANI , C ONNOR AND G REINER  the highest ranked class is assigned to the instance, and R1 measures the proportions of instances to which the true class was assigned. [sent-433, score-0.452]
</p><p>28 kx MRR gives a reward of 1 if a correct class is ranked highest, the reward drops to 1/2 at rank 2, and slowly goes down the higher the k (the lower the rank). [sent-435, score-0.326]
</p><p>29 Our indexing techniques are more appropriate for the problem of obtaining good rankings per instance, similar to some other multiclass ranking algorithms (e. [sent-460, score-0.327]
</p><p>30 On a given instance, after the use of the index for scoring and ranking (an invocation of RankedRetrieval), if a measure of margin (to be described shortly) is not large enough, an update to the index is made. [sent-489, score-0.53]
</p><p>31 The margin is the score obtained by the true class, minus the highest scoring incorrect (negative) class (either of the two scores can be zero). [sent-490, score-0.312]
</p><p>32 2583  M ADANI , C ONNOR AND G REINER  /* The FF Algorithm */ Algorithm FeatureFocus(x, wmin , dmax , δm ) 1. [sent-506, score-0.407]
</p><p>33 , vx [ f ] > 0): For the ﬁrst dmax classes with highest connection weight to f : 1. [sent-518, score-0.576]
</p><p>34 (b)  /* Feature Streaming Update (allowing “leaks”) */ Algorithm FSU(x, f , wmin ) /* Single feature updating */ 1. [sent-522, score-0.472]
</p><p>35 If w f ,c < wmin , then /* drop tiny weights */ w f ,c ← 0, w′f ,c ← 0 (c)  Algorithm GenericWeightUpdate Each active feature: 1. [sent-527, score-0.441]
</p><p>36 Ignoring other features for now, and considering efﬁciency constraints, to which classes should this feature connect to, and with what weights? [sent-545, score-0.358]
</p><p>37 In this single feature case, classes are ranked by the weight assigned to them by the feature. [sent-547, score-0.352]
</p><p>38 2584  L EARNING W HEN C ONCEPTS A BOUND  It is not hard to verify that the best classes are the dmax classes with the highest proportions in the stream, or the highest P(c) if the distribution is ﬁxed and known (more precisely, P(c| f ), but f is ﬁxed here) and the ranking should also be by P(c). [sent-551, score-0.694]
</p><p>39 To maximize HR, when the feature can connect to at most k different classes, a k highest frequency set of classes should be picked, that is, choose S, such that |S| = k and S = {c|nc ≥ nc′ , ∀c′ ∈ S}), where nc denotes the number of times c occurs in the sequence. [sent-554, score-0.389]
</p><p>40 Since the weights are between 0 and 1 and approximate probabilities, it eases the decision of assessing importance of a connection: weights below wmin are dropped at the expense of some potential loss in accuracy. [sent-570, score-0.469]
</p><p>41 Note that wmin effec1 tively bounds the maximum outdegree during learning to be wmin . [sent-572, score-0.777]
</p><p>42 Given that FSU zeros some weights during its computation, it is instructive to look at how well it does in approximating proportions for the (sub)stream of classes that it processes for a single feature. [sent-576, score-0.313]
</p><p>43 This gives us an idea of how to set the wmin parameter and what to expect. [sent-577, score-0.317]
</p><p>44 To summarize, when the true probability (weight) w of interest is several multiples of wmin , with sufﬁcient sample size, the chance of dropping it is very low (the probability quickly goes down to 0 with increasing ww ), and moreover, the computed weight is also min close to the true conditional. [sent-579, score-0.375]
</p><p>45 In case of non-Boolean feature values, similar to perceptron and Winnow updates (Rosenblatt, 1958; Littlestone, 1988), the degree of activity of the feature, vx [ f ], affects how much the connection between the feature and the true class is strengthened. [sent-600, score-0.593]
</p><p>46 In particular, after w f > 1 9 wmin , a new class will be immediately dropped. [sent-604, score-0.36]
</p><p>47 2 Always Updating (the IND Algorithm) One method of index construction is to simply assign each edge the class conditional probabilities, P(c| f ) (the conditional probability that instance x ∈ c given that f ∈ x). [sent-617, score-0.314]
</p><p>48 At this point, updates can only affect classes already connected, and updates may improve the accuracy of their assigned weights, though there is a small chance that even classes with signiﬁcant weights may be eventually dropped (this has probability 1 over an inﬁnite sequence! [sent-628, score-0.462]
</p><p>49 3 Mistake-Driven Updating Using a Margin (the FF Algorithm) FF adds and drops edges and modiﬁes edge weights during learning by processing one instance at a time,12 and by invoking a feature updating algorithm, such as FSU. [sent-667, score-0.39]
</p><p>50 Unlike IND, FF addresses feature dependencies by not updating the index on every training instance. [sent-668, score-0.352]
</p><p>51 Equivalently, a feature updates its connection on only a fraction of the training instances in which it is active. [sent-669, score-0.347]
</p><p>52 It can, for example, avoid over counting the inﬂuence of features that are basically duplicates by learning relatively low connection weights for each such feature (similar to a rational for mistake driven updates in other learning algorithms such as the perceptron). [sent-684, score-0.357]
</p><p>53 The margin on the current instance is the score of the positive class minus the score of the highest scoring negative 2588  L EARNING W HEN C ONCEPTS A BOUND  class: δ = scx − s′ , where scx ≥ 0, s′ ≥ 0, s′ = max sc . [sent-729, score-0.415]
</p><p>54 Individual edge weights are in the [0, 1] range, and when the instances are l2 normalized, we have observed that on average top classes obtain scores in the [0, 1] range as well, irrespective of data sets or choice of margin threshold (Madani and Connor, 2007). [sent-736, score-0.51]
</p><p>55 During learning, FF may be viewed as directing a stream of classes to each feature, so that each feature can compute weights for a subset of the classes that it may connect to. [sent-753, score-0.526]
</p><p>56 Every active feature is updated for each true class for which its margin is below the margin threshold. [sent-760, score-0.316]
</p><p>57 In our case, it is the classes whose connections to a feature may be weakened due to one or more classes being strengthened. [sent-770, score-0.453]
</p><p>58 In the FSU update given in this paper, this weakening happens irrespective of whether a class was ranked high (this aspect is similar to Winnow, but again, for class weights instead of feature weights). [sent-771, score-0.375]
</p><p>59 It appeared harder to us to bound the number of features a class needs, and different classes may require widely varying number of features for adequate performance. [sent-778, score-0.336]
</p><p>60 Finally, we seek rapid categorization per instance, and constraining indegree of classes may not guarantee that 2590  L EARNING W HEN C ONCEPTS A BOUND  the outdegree of commonly occurring features would be small. [sent-791, score-0.48]
</p><p>61 False positive classes may obtain negative connections to features they weren’t connected to (when ranked higher than the true positive). [sent-796, score-0.388]
</p><p>62 Note that the use of index for classiﬁcation is one-versus-rest (or “ﬂat” classiﬁcation), but the index was not obtained by training binary classiﬁers. [sent-822, score-0.345]
</p><p>63 For the web data set, to obtain a sufﬁcient number of instances per class, we cut the taxonomy at depth 4, that is, we only considered the true classes up to depth 4. [sent-904, score-0.329]
</p><p>64 Note that dmax of 25 means a class is retrieved as along as it is within the ﬁrst 25 highest weight connections of some active feature, even if its weight is not much higher than wmin . [sent-933, score-0.855]
</p><p>65 5 basically means to update on most instances as index edge weights are less than 1, and thus class score differences tend not to be much higher than 1. [sent-943, score-0.49]
</p><p>66 The classes with the highest proportion in training are not the classes with the highest proportion on the test set. [sent-963, score-0.451]
</p><p>67 The scores of such a classiﬁer are more suitable for ranking the instances for the corresponding class than ranking classes for each instance. [sent-1156, score-0.498]
</p><p>68 274  Ttr (single pass) 0s vs 0s 3s vs 2s 9s vs 4s [40s − 50s] vs 6s 45m vs 27s 2h vs 64s 1h vs 41s  Figure 11: No constraints on dmax (maximum outdegree) nor wmin (wmin set to 0), compared to the default settings. [sent-1215, score-0.407]
</p><p>69 In these experiments, we set wmin to 0 and dmax to a large number (1000). [sent-1224, score-0.407]
</p><p>70 At outdegree constraint of 3 for RCV1, the number of edges in the learned index is around 80k instead of 180k (for the default dmax = 25), and the number of classes (connections) touched per feature is under 3, instead of 15 (Figure 10). [sent-1240, score-0.688]
</p><p>71 In general, it may be a better policy to use a weight threshold, greater than wmin , instead of a max outdegree constraint, for more efﬁcient retrieval, as well as more reduction in index size, without loss in ranking accuracy. [sent-1241, score-0.754]
</p><p>72 3 0  10  20  30  40  50  max outdegree allowed  Figure 12: Accuracy (R1 ) after one pass against the outdegree constraint. [sent-1253, score-0.342]
</p><p>73 However, since the outdegree is constrained for memory reasons, if we imposed a constraint that the connection weights of a feature should sum to 1, then “the” may give signiﬁcant but inaccurate weights to the classes that it happens to get connected with. [sent-1335, score-0.572]
</p><p>74 Thus IND is similar to Boolean FF with high margin and wmin = 0, but IND also has a post-improvement step of adjusting pind (using the training set), which we have observed can improve the test accuracy of IND signiﬁcantly (in addition to reducing index size). [sent-1389, score-0.705]
</p><p>75 Note that the scores that the classes receive during retrieval can increase signiﬁcantly with Boolean features (compared to using the feature values in l2 normalized vector representation). [sent-1413, score-0.419]
</p><p>76 (2007) uses a threshold ttol during training and updates the index only when more than ttol many false positive classes are retrieved on a training instance or when a false negative occurs. [sent-1442, score-0.675]
</p><p>77 2602  L EARNING W HEN C ONCEPTS A BOUND  Train and Test Accuracy versus Pass on the news group data set  Train and Test Accuracy versus Pass on the Web Dataset  1  1 train marg 0 test marg 0 train marg 0. [sent-1456, score-0.395]
</p><p>78 Let the indegree of a class, that is, the length of the prototype vector, be the number of features that have a signiﬁcant edge to the class (within the highest dmax edges for each feature). [sent-1498, score-0.519]
</p><p>79 Each feature computes that choice of classes it may connect to and the connection weights. [sent-1520, score-0.328]
</p><p>80 Theorem 4 The index learning problem with the objective of either maximizing accuracy (R1 ) or minimizing HR on a given set of instances, with the constraint of a constant upper bound, such as 1, on the outdegree of each feature is NP-Hard. [sent-1542, score-0.446]
</p><p>81 , those features whose corresponding sets are in the cover) to c1 , each with weight of |S |, and we connect all the other features to c2 with a relatively small weight of say 1. [sent-1564, score-0.337]
</p><p>82 1 wherein a feature wants to compute the proportions of the (sufﬁciently frequent) classes in the stream it observes. [sent-1583, score-0.39]
</p><p>83 9  1  Figure 21: The performance of FSU under different allowances wmin . [sent-1621, score-0.317]
</p><p>84 In the plot of part (b), the deviation is also compared to the case of 100 classes and wmin = 0. [sent-1632, score-0.44]
</p><p>85 • Setting small weights (below wmin ) to 0 (dropping edges) to save memory. [sent-1639, score-0.393]
</p><p>86 Intuitively, FSU should work well as long as the proportions we are interested in sufﬁciently exceed the wmin 1 threshold. [sent-1642, score-0.431]
</p><p>87 The probability that a class with say probability p is not seen in some wmin trials is p (1 − p)1/wmin , and as long the ratio wmin is high (several multiples), for example, p > 4wmin , this probability is relatively small. [sent-1643, score-0.711]
</p><p>88 More generally, the chance of being set to 0 (dropped) for a class with occurrence probability p quickly diminishes as we increase p the ratio wmin , and therefore the cause of inaccuracies due to ﬁnite memory (the outdegree constraint on features) is mitigated. [sent-1648, score-0.503]
</p><p>89 We conducted experiments to see how much the proportion estimation by FSU deviates from true proportions and in particular compared that deviation to the deviations when FSU is not memory constrained (when wmin is set to 0). [sent-1649, score-0.431]
</p><p>90 12  Figure 22: The performance of FSU, under different allowances wmin . [sent-1669, score-0.317]
</p><p>91 01 yields distances comparable to FSU with wmin = 0, but wmin =0. [sent-1675, score-0.634]
</p><p>92 We then generated a stream of 1000 class observations (1000 iid draws) from C |−1 such a distribution, and gave it to FSU with different values of wmin . [sent-1678, score-0.414]
</p><p>93 We plot the results for FSU under different wmin constraints. [sent-1693, score-0.317]
</p><p>94 We compared a number of other statistics, such as the maximum deviation from true probability, and the probability that the deviation is larger than a threshold, and FSU with wmin = 0. [sent-1699, score-0.317]
</p><p>95 01 performed similarly to wmin = 0 on the distributions tested. [sent-1700, score-0.317]
</p><p>96 The reason as alluded to earlier is that those classes with proportions 2608  L EARNING W HEN C ONCEPTS A BOUND  signiﬁcantly greater than wmin have a high chance of being seen early and frequently enough in the stream and not being dropped. [sent-1701, score-0.608]
</p><p>97 Thus, as long as we expect that the useful proportions are a few multiples away from the wmin we choose, FSU is expected to compute proportions that are close to ones computed by the FSU with wmin set to 0 (no space constraints). [sent-1702, score-0.862]
</p><p>98 Further, we expected that most often the important feature connection weights that determine the true classes during ranking have fairly high weight. [sent-1703, score-0.441]
</p><p>99 Finally, vector length is a factor: if there tend to exist strong features-class connections, the inﬂuence of the weaker connections on changing the ranking will be limited, in particular when the number of active features is adequately small. [sent-1705, score-0.329]
</p><p>100 In general however, some experimentation may be required to set the wmin parameter. [sent-1710, score-0.317]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wmin', 0.317), ('fsu', 0.294), ('ff', 0.268), ('madani', 0.265), ('ind', 0.194), ('vx', 0.172), ('adani', 0.151), ('oncepts', 0.151), ('onnor', 0.151), ('reiner', 0.151), ('index', 0.148), ('outdegree', 0.143), ('indexing', 0.141), ('classes', 0.123), ('marg', 0.115), ('proportions', 0.114), ('hr', 0.11), ('connections', 0.108), ('instances', 0.102), ('feature', 0.099), ('hen', 0.091), ('dmax', 0.09), ('ranking', 0.088), ('features', 0.085), ('perceptron', 0.083), ('highest', 0.078), ('instance', 0.077), ('ads', 0.076), ('prototype', 0.076), ('weights', 0.076), ('newsgroup', 0.073), ('reuters', 0.073), ('ranked', 0.072), ('mrr', 0.072), ('pind', 0.072), ('reward', 0.068), ('classi', 0.066), ('dumais', 0.065), ('indegree', 0.065), ('categorization', 0.064), ('web', 0.064), ('margin', 0.063), ('multiclass', 0.063), ('committee', 0.06), ('weight', 0.058), ('retrieval', 0.058), ('austen', 0.057), ('pass', 0.056), ('accuracy', 0.056), ('updating', 0.056), ('retrieved', 0.055), ('cx', 0.055), ('connection', 0.055), ('scores', 0.054), ('stream', 0.054), ('connect', 0.051), ('online', 0.051), ('connor', 0.05), ('freqbaseline', 0.05), ('ciency', 0.05), ('news', 0.05), ('training', 0.049), ('touched', 0.049), ('active', 0.048), ('ers', 0.047), ('sc', 0.047), ('votes', 0.047), ('threshold', 0.046), ('passes', 0.046), ('edge', 0.046), ('industry', 0.046), ('crammer', 0.046), ('boolean', 0.044), ('winnow', 0.044), ('jane', 0.044), ('pci', 0.043), ('rankedretrieval', 0.043), ('ttol', 0.043), ('ttr', 0.043), ('class', 0.043), ('text', 0.043), ('update', 0.042), ('perceptrons', 0.042), ('updates', 0.042), ('earning', 0.042), ('scoring', 0.041), ('rank', 0.04), ('taxonomy', 0.04), ('nc', 0.038), ('unweighted', 0.037), ('mesterharm', 0.036), ('edges', 0.036), ('kx', 0.035), ('rankings', 0.035), ('streaming', 0.035), ('inferior', 0.035), ('word', 0.034), ('trials', 0.034), ('score', 0.033), ('invoked', 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000019 <a title="50-tfidf-1" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>Author: Omid Madani, Michael Connor, Wiley Greiner</p><p>Abstract: Many learning tasks, such as large-scale text categorization and word prediction, can beneﬁt from efﬁcient training and classiﬁcation when the number of classes, in addition to instances and features, is large, that is, in the thousands and beyond. We investigate the learning of sparse class indices to address this challenge. An index is a mapping from features to classes. We compare the index-learning methods against other techniques, including one-versus-rest and top-down classiﬁcation using perceptrons and support vector machines. We ﬁnd that index learning is highly advantageous for space and time efﬁciency, at both training and classiﬁcation times. Moreover, this approach yields similar and at times better accuracies. On problems with hundreds of thousands of instances and thousands of classes, the index is learned in minutes, while other methods can take hours or days. As we explain, the design of the learning update enables conveniently constraining each feature to connect to a small subset of the classes in the index. This constraint is crucial for scalability. Given an instance with l active (positive-valued) features, each feature on average connecting to d classes in the index (in the order of 10s in our experiments), update and classiﬁcation take O(dl log(dl)). Keywords: index learning, many-class learning, multiclass learning, online learning, text categorization</p><p>2 0.078079008 <a title="50-tfidf-2" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>Author: Shivani Agarwal, Partha Niyogi</p><p>Abstract: The problem of ranking, in which the goal is to learn a real-valued ranking function that induces a ranking or ordering over an instance space, has recently gained much attention in machine learning. We study generalization properties of ranking algorithms using the notion of algorithmic stability; in particular, we derive generalization bounds for ranking algorithms that have good stability properties. We show that kernel-based ranking algorithms that perform regularization in a reproducing kernel Hilbert space have such stability properties, and therefore our bounds can be applied to these algorithms; this is in contrast with generalization bounds based on uniform convergence, which in many cases cannot be applied to these algorithms. Our results generalize earlier results that were derived in the special setting of bipartite ranking (Agarwal and Niyogi, 2005) to a more general setting of the ranking problem that arises frequently in applications. Keywords: ranking, generalization bounds, algorithmic stability</p><p>3 0.069748871 <a title="50-tfidf-3" href="./jmlr-2009-Estimating_Labels_from_Label_Proportions.html">29 jmlr-2009-Estimating Labels from Label Proportions</a></p>
<p>Author: Novi Quadrianto, Alex J. Smola, Tibério S. Caetano, Quoc V. Le</p><p>Abstract: Consider the following problem: given sets of unlabeled observations, each set with known label proportions, predict the labels of another set of observations, possibly with known label proportions. This problem occurs in areas like e-commerce, politics, spam ﬁltering and improper content detection. We present consistent estimators which can reconstruct the correct labels with high probability in a uniform convergence sense. Experiments show that our method works well in practice. Keywords: unsupervised learning, Gaussian processes, classiﬁcation and prediction, probabilistic models, missing variables</p><p>4 0.063121043 <a title="50-tfidf-4" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of onlinelearning algorithms with convex loss functions. This method has several essential properties: 1. The degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. 2. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove that small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. 3. The approach works well empirically. We apply the approach to several data sets and ﬁnd for data sets with large numbers of features, substantial sparsity is discoverable. Keywords: truncated gradient, stochastic gradient descent, online learning, sparsity, regularization, Lasso</p><p>5 0.061389901 <a title="50-tfidf-5" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>Author: Yihua Chen, Eric K. Garcia, Maya R. Gupta, Ali Rahimi, Luca Cazzanti</p><p>Abstract: This paper reviews and extends the ﬁeld of similarity-based classiﬁcation, presenting new analyses, algorithms, data sets, and a comprehensive set of experimental results for a rich collection of classiﬁcation problems. Speciﬁcally, the generalizability of using similarities as features is analyzed, design goals and methods for weighting nearest-neighbors for similarity-based learning are proposed, and different methods for consistently converting similarities into kernels are compared. Experiments on eight real data sets compare eight approaches and their variants to similarity-based learning. Keywords: similarity, dissimilarity, similarity-based learning, indeﬁnite kernels</p><p>6 0.061059795 <a title="50-tfidf-6" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>7 0.05796656 <a title="50-tfidf-7" href="./jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">9 jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>8 0.057611194 <a title="50-tfidf-8" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>9 0.056112591 <a title="50-tfidf-9" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>10 0.055381808 <a title="50-tfidf-10" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>11 0.052518725 <a title="50-tfidf-11" href="./jmlr-2009-Online_Learning_with_Sample_Path_Constraints.html">67 jmlr-2009-Online Learning with Sample Path Constraints</a></p>
<p>12 0.050312657 <a title="50-tfidf-12" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>13 0.049495783 <a title="50-tfidf-13" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>14 0.048467975 <a title="50-tfidf-14" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>15 0.047057275 <a title="50-tfidf-15" href="./jmlr-2009-Model_Monitor_%28M2%29%3A_Evaluating%2C_Comparing%2C_and_Monitoring_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">56 jmlr-2009-Model Monitor (M2): Evaluating, Comparing, and Monitoring Models    (Machine Learning Open Source Software Paper)</a></p>
<p>16 0.046863705 <a title="50-tfidf-16" href="./jmlr-2009-Bounded_Kernel-Based_Online_Learning.html">13 jmlr-2009-Bounded Kernel-Based Online Learning</a></p>
<p>17 0.046843253 <a title="50-tfidf-17" href="./jmlr-2009-The_P-Norm_Push%3A_A_Simple_Convex_Ranking_Algorithm_that_Concentrates_at_the_Top_of_the_List.html">95 jmlr-2009-The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List</a></p>
<p>18 0.046726447 <a title="50-tfidf-18" href="./jmlr-2009-Transfer_Learning_for_Reinforcement_Learning_Domains%3A_A_Survey.html">96 jmlr-2009-Transfer Learning for Reinforcement Learning Domains: A Survey</a></p>
<p>19 0.045690626 <a title="50-tfidf-19" href="./jmlr-2009-A_Survey_of_Accuracy_Evaluation_Metrics_of_Recommendation_Tasks.html">4 jmlr-2009-A Survey of Accuracy Evaluation Metrics of Recommendation Tasks</a></p>
<p>20 0.044426031 <a title="50-tfidf-20" href="./jmlr-2009-Learning_Linear_Ranking_Functions_for_Beam_Search_with_Application_to_Planning.html">47 jmlr-2009-Learning Linear Ranking Functions for Beam Search with Application to Planning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2009_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.23), (1, -0.075), (2, 0.053), (3, -0.036), (4, 0.028), (5, -0.019), (6, 0.022), (7, 0.207), (8, 0.017), (9, -0.024), (10, -0.03), (11, -0.074), (12, 0.072), (13, -0.111), (14, -0.109), (15, -0.128), (16, 0.106), (17, -0.1), (18, -0.06), (19, -0.05), (20, -0.057), (21, 0.168), (22, -0.013), (23, -0.097), (24, -0.044), (25, -0.07), (26, 0.179), (27, -0.058), (28, -0.168), (29, 0.004), (30, -0.009), (31, 0.144), (32, -0.074), (33, 0.007), (34, -0.017), (35, -0.169), (36, 0.122), (37, -0.173), (38, 0.145), (39, -0.041), (40, 0.012), (41, 0.068), (42, 0.036), (43, -0.015), (44, 0.032), (45, -0.023), (46, 0.122), (47, -0.006), (48, 0.042), (49, 0.034)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93709022 <a title="50-lsi-1" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>Author: Omid Madani, Michael Connor, Wiley Greiner</p><p>Abstract: Many learning tasks, such as large-scale text categorization and word prediction, can beneﬁt from efﬁcient training and classiﬁcation when the number of classes, in addition to instances and features, is large, that is, in the thousands and beyond. We investigate the learning of sparse class indices to address this challenge. An index is a mapping from features to classes. We compare the index-learning methods against other techniques, including one-versus-rest and top-down classiﬁcation using perceptrons and support vector machines. We ﬁnd that index learning is highly advantageous for space and time efﬁciency, at both training and classiﬁcation times. Moreover, this approach yields similar and at times better accuracies. On problems with hundreds of thousands of instances and thousands of classes, the index is learned in minutes, while other methods can take hours or days. As we explain, the design of the learning update enables conveniently constraining each feature to connect to a small subset of the classes in the index. This constraint is crucial for scalability. Given an instance with l active (positive-valued) features, each feature on average connecting to d classes in the index (in the order of 10s in our experiments), update and classiﬁcation take O(dl log(dl)). Keywords: index learning, many-class learning, multiclass learning, online learning, text categorization</p><p>2 0.47219467 <a title="50-lsi-2" href="./jmlr-2009-Estimating_Labels_from_Label_Proportions.html">29 jmlr-2009-Estimating Labels from Label Proportions</a></p>
<p>Author: Novi Quadrianto, Alex J. Smola, Tibério S. Caetano, Quoc V. Le</p><p>Abstract: Consider the following problem: given sets of unlabeled observations, each set with known label proportions, predict the labels of another set of observations, possibly with known label proportions. This problem occurs in areas like e-commerce, politics, spam ﬁltering and improper content detection. We present consistent estimators which can reconstruct the correct labels with high probability in a uniform convergence sense. Experiments show that our method works well in practice. Keywords: unsupervised learning, Gaussian processes, classiﬁcation and prediction, probabilistic models, missing variables</p><p>3 0.46791214 <a title="50-lsi-3" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>Author: Shivani Agarwal, Partha Niyogi</p><p>Abstract: The problem of ranking, in which the goal is to learn a real-valued ranking function that induces a ranking or ordering over an instance space, has recently gained much attention in machine learning. We study generalization properties of ranking algorithms using the notion of algorithmic stability; in particular, we derive generalization bounds for ranking algorithms that have good stability properties. We show that kernel-based ranking algorithms that perform regularization in a reproducing kernel Hilbert space have such stability properties, and therefore our bounds can be applied to these algorithms; this is in contrast with generalization bounds based on uniform convergence, which in many cases cannot be applied to these algorithms. Our results generalize earlier results that were derived in the special setting of bipartite ranking (Agarwal and Niyogi, 2005) to a more general setting of the ranking problem that arises frequently in applications. Keywords: ranking, generalization bounds, algorithmic stability</p><p>4 0.43687898 <a title="50-lsi-4" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of onlinelearning algorithms with convex loss functions. This method has several essential properties: 1. The degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. 2. The approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove that small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. 3. The approach works well empirically. We apply the approach to several data sets and ﬁnd for data sets with large numbers of features, substantial sparsity is discoverable. Keywords: truncated gradient, stochastic gradient descent, online learning, sparsity, regularization, Lasso</p><p>5 0.41773003 <a title="50-lsi-5" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>Author: Juan José del Coz, Jorge Díez, Antonio Bahamonde</p><p>Abstract: Nondeterministic classiﬁers are deﬁned as those allowed to predict more than one class for some entries from an input space. Given that the true class should be included in predictions and the number of classes predicted should be as small as possible, these kind of classiﬁers can be considered as Information Retrieval (IR) procedures. In this paper, we propose a family of IR loss functions to measure the performance of nondeterministic learners. After discussing such measures, we derive an algorithm for learning optimal nondeterministic hypotheses. Given an entry from the input space, the algorithm requires the posterior probabilities to compute the subset of classes with the lowest expected loss. From a general point of view, nondeterministic classiﬁers provide an improvement in the proportion of predictions that include the true class compared to their deterministic counterparts; the price to be paid for this increase is usually a tiny proportion of predictions with more than one class. The paper includes an extensive experimental study using three deterministic learners to estimate posterior probabilities: a multiclass Support Vector Machine (SVM), a Logistic Regression, and a Na¨ve Bayes. The data sets considered comprise both UCI ı multi-class learning tasks and microarray expressions of different kinds of cancer. We successfully compare nondeterministic classiﬁers with other alternative approaches. Additionally, we shall see how the quality of posterior probabilities (measured by the Brier score) determines the goodness of nondeterministic predictions. Keywords: nondeterministic, multiclassiﬁcation, reject option, multi-label classiﬁcation, posterior probabilities</p><p>6 0.40483567 <a title="50-lsi-6" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>7 0.38955188 <a title="50-lsi-7" href="./jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">35 jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>8 0.38174164 <a title="50-lsi-8" href="./jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">8 jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>9 0.35957685 <a title="50-lsi-9" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>10 0.35006875 <a title="50-lsi-10" href="./jmlr-2009-Learning_Linear_Ranking_Functions_for_Beam_Search_with_Application_to_Planning.html">47 jmlr-2009-Learning Linear Ranking Functions for Beam Search with Application to Planning</a></p>
<p>11 0.3309871 <a title="50-lsi-11" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>12 0.3282505 <a title="50-lsi-12" href="./jmlr-2009-Model_Monitor_%28M2%29%3A_Evaluating%2C_Comparing%2C_and_Monitoring_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">56 jmlr-2009-Model Monitor (M2): Evaluating, Comparing, and Monitoring Models    (Machine Learning Open Source Software Paper)</a></p>
<p>13 0.32768834 <a title="50-lsi-13" href="./jmlr-2009-A_Parameter-Free_Classification_Method_for_Large_Scale_Learning.html">3 jmlr-2009-A Parameter-Free Classification Method for Large Scale Learning</a></p>
<p>14 0.31359509 <a title="50-lsi-14" href="./jmlr-2009-Learning_Permutations_with_Exponential_Weights.html">49 jmlr-2009-Learning Permutations with Exponential Weights</a></p>
<p>15 0.30361509 <a title="50-lsi-15" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>16 0.30049071 <a title="50-lsi-16" href="./jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">52 jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>17 0.29647577 <a title="50-lsi-17" href="./jmlr-2009-Nieme%3A_Large-Scale_Energy-Based_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">60 jmlr-2009-Nieme: Large-Scale Energy-Based Models    (Machine Learning Open Source Software Paper)</a></p>
<p>18 0.2906644 <a title="50-lsi-18" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>19 0.28720802 <a title="50-lsi-19" href="./jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">98 jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>20 0.28252479 <a title="50-lsi-20" href="./jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">63 jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2009_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.024), (11, 0.031), (19, 0.019), (21, 0.011), (26, 0.018), (38, 0.036), (47, 0.013), (52, 0.553), (55, 0.031), (58, 0.023), (66, 0.086), (68, 0.017), (90, 0.051), (96, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98099738 <a title="50-lda-1" href="./jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">23 jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>Author: Steffen Bickel, Michael Brückner, Tobias Scheffer</p><p>Abstract: We address classiﬁcation problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution—problems also referred to as classiﬁcation under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. The problem of learning under covariate shift can be written as an integrated optimization problem. Instantiating the general optimization problem leads to a kernel logistic regression and an exponential model classiﬁer for covariate shift. The optimization problem is convex under certain conditions; our ﬁndings also clarify the relationship to the known kernel mean matching procedure. We report on experiments on problems of spam ﬁltering, text classiﬁcation, and landmine detection. Keywords: covariate shift, discriminative learning, transfer learning</p><p>same-paper 2 0.95796609 <a title="50-lda-2" href="./jmlr-2009-Learning_When_Concepts_Abound.html">50 jmlr-2009-Learning When Concepts Abound</a></p>
<p>Author: Omid Madani, Michael Connor, Wiley Greiner</p><p>Abstract: Many learning tasks, such as large-scale text categorization and word prediction, can beneﬁt from efﬁcient training and classiﬁcation when the number of classes, in addition to instances and features, is large, that is, in the thousands and beyond. We investigate the learning of sparse class indices to address this challenge. An index is a mapping from features to classes. We compare the index-learning methods against other techniques, including one-versus-rest and top-down classiﬁcation using perceptrons and support vector machines. We ﬁnd that index learning is highly advantageous for space and time efﬁciency, at both training and classiﬁcation times. Moreover, this approach yields similar and at times better accuracies. On problems with hundreds of thousands of instances and thousands of classes, the index is learned in minutes, while other methods can take hours or days. As we explain, the design of the learning update enables conveniently constraining each feature to connect to a small subset of the classes in the index. This constraint is crucial for scalability. Given an instance with l active (positive-valued) features, each feature on average connecting to d classes in the index (in the order of 10s in our experiments), update and classiﬁcation take O(dl log(dl)). Keywords: index learning, many-class learning, multiclass learning, online learning, text categorization</p><p>3 0.6904335 <a title="50-lda-3" href="./jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">62 jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>Author: Babak Shahbaba, Radford Neal</p><p>Abstract: We introduce a new nonlinear model for classiﬁcation, in which we model the joint distribution of response variable, y, and covariates, x, non-parametrically using Dirichlet process mixtures. We keep the relationship between y and x linear within each component of the mixture. The overall relationship becomes nonlinear if the mixture contains more than one component, with different regression coefﬁcients. We use simulated data to compare the performance of this new approach to alternative methods such as multinomial logit (MNL) models, decision trees, and support vector machines. We also evaluate our approach on two classiﬁcation problems: identifying the folding class of protein sequences and detecting Parkinson’s disease. Our model can sometimes improve predictive accuracy. Moreover, by grouping observations into sub-populations (i.e., mixture components), our model can sometimes provide insight into hidden structure in the data. Keywords: mixture models, Dirichlet process, classiﬁcation</p><p>4 0.67828554 <a title="50-lda-4" href="./jmlr-2009-Learning_Nondeterministic_Classifiers.html">48 jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>Author: Juan José del Coz, Jorge Díez, Antonio Bahamonde</p><p>Abstract: Nondeterministic classiﬁers are deﬁned as those allowed to predict more than one class for some entries from an input space. Given that the true class should be included in predictions and the number of classes predicted should be as small as possible, these kind of classiﬁers can be considered as Information Retrieval (IR) procedures. In this paper, we propose a family of IR loss functions to measure the performance of nondeterministic learners. After discussing such measures, we derive an algorithm for learning optimal nondeterministic hypotheses. Given an entry from the input space, the algorithm requires the posterior probabilities to compute the subset of classes with the lowest expected loss. From a general point of view, nondeterministic classiﬁers provide an improvement in the proportion of predictions that include the true class compared to their deterministic counterparts; the price to be paid for this increase is usually a tiny proportion of predictions with more than one class. The paper includes an extensive experimental study using three deterministic learners to estimate posterior probabilities: a multiclass Support Vector Machine (SVM), a Logistic Regression, and a Na¨ve Bayes. The data sets considered comprise both UCI ı multi-class learning tasks and microarray expressions of different kinds of cancer. We successfully compare nondeterministic classiﬁers with other alternative approaches. Additionally, we shall see how the quality of posterior probabilities (measured by the Brier score) determines the goodness of nondeterministic predictions. Keywords: nondeterministic, multiclassiﬁcation, reject option, multi-label classiﬁcation, posterior probabilities</p><p>5 0.65643466 <a title="50-lda-5" href="./jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">58 jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>Author: Pradip Ghanty, Samrat Paul, Nikhil R. Pal</p><p>Abstract: In this paper we propose a new multilayer classiﬁer architecture. The proposed hybrid architecture has two cascaded modules: feature extraction module and classiﬁcation module. In the feature extraction module we use the multilayered perceptron (MLP) neural networks, although other tools such as radial basis function (RBF) networks can be used. In the classiﬁcation module we use support vector machines (SVMs)—here also other tool such as MLP or RBF can be used. The feature extraction module has several sub-modules each of which is expected to extract features capturing the discriminating characteristics of different areas of the input space. The classiﬁcation module classiﬁes the data based on the extracted features. The resultant architecture with MLP in feature extraction module and SVM in classiﬁcation module is called NEUROSVM. The NEUROSVM is tested on twelve benchmark data sets and the performance of the NEUROSVM is found to be better than both MLP and SVM. We also compare the performance of proposed architecture with that of two ensemble methods: majority voting and averaging. Here also the NEUROSVM is found to perform better than these two ensemble methods. Further we explore the use of MLP and RBF in the classiﬁcation module of the proposed architecture. The most attractive feature of NEUROSVM is that it practically eliminates the severe dependency of SVM on the choice of kernel. This has been veriﬁed with respect to both linear and non-linear kernels. We have also demonstrated that for the feature extraction module, the full training of MLPs is not needed. Keywords: feature extraction, neural networks (NNs), support vector machines (SVMs), hybrid system, majority voting, averaging c 2009 Pradip Ghanty, Samrat Paul and Nikhil R. Pal. G HANTY, PAUL AND PAL</p><p>6 0.64206147 <a title="50-lda-6" href="./jmlr-2009-A_Parameter-Free_Classification_Method_for_Large_Scale_Learning.html">3 jmlr-2009-A Parameter-Free Classification Method for Large Scale Learning</a></p>
<p>7 0.62717676 <a title="50-lda-7" href="./jmlr-2009-Hash_Kernels_for_Structured_Data.html">38 jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>8 0.61697119 <a title="50-lda-8" href="./jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">87 jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>9 0.61029518 <a title="50-lda-9" href="./jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">97 jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>10 0.58695656 <a title="50-lda-10" href="./jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">99 jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<p>11 0.56723046 <a title="50-lda-11" href="./jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">24 jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>12 0.5584392 <a title="50-lda-12" href="./jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">10 jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>13 0.55455804 <a title="50-lda-13" href="./jmlr-2009-A_Least-squares_Approach_to_Direct_Importance_Estimation.html">1 jmlr-2009-A Least-squares Approach to Direct Importance Estimation</a></p>
<p>14 0.54758108 <a title="50-lda-14" href="./jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">70 jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>15 0.53346241 <a title="50-lda-15" href="./jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">86 jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>16 0.52400392 <a title="50-lda-16" href="./jmlr-2009-Distributed_Algorithms_for_Topic_Models.html">25 jmlr-2009-Distributed Algorithms for Topic Models</a></p>
<p>17 0.51795876 <a title="50-lda-17" href="./jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">82 jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>18 0.50046706 <a title="50-lda-18" href="./jmlr-2009-Cautious_Collective_Classification.html">15 jmlr-2009-Cautious Collective Classification</a></p>
<p>19 0.4901439 <a title="50-lda-19" href="./jmlr-2009-Estimating_Labels_from_Label_Proportions.html">29 jmlr-2009-Estimating Labels from Label Proportions</a></p>
<p>20 0.48421335 <a title="50-lda-20" href="./jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">37 jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
