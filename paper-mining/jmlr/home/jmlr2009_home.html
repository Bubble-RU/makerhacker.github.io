<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>jmlr 2009 knowledge graph</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2009" href="#">jmlr2009</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>jmlr 2009 knowledge graph</h1>
<br/><h3>similar papers computed by tfidf model</h3><br/><h3>similar papers computed by <a title="lsi-model" href="./jmlr2009_lsi.html">lsi model</a></h3><br/><h3>similar papers computed by <a title="lda-model" href="./jmlr2009_lda.html">lda model</a></h3><br/><h2>papers list:</h2><p>1 <a title="jmlr-2009-1" href="../jmlr2009/jmlr-2009-A_Least-squares_Approach_to_Direct_Importance_Estimation.html">jmlr-2009-A Least-squares Approach to Direct Importance Estimation</a></p>
<p>Author: Takafumi Kanamori, Shohei Hido, Masashi Sugiyama</p><p>Abstract: We address the problem of estimating the ratio of two probability density functions, which is often referred to as the importance. The importance values can be used for various succeeding tasks such as covariate shift adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally highly efﬁcient and simple to implement. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bounds. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efﬁcient than competing approaches. Keywords: importance sampling, covariate shift adaptation, novelty detection, regularization path, leave-one-out cross validation</p><p>2 <a title="jmlr-2009-2" href="../jmlr2009/jmlr-2009-A_New_Approach_to_Collaborative_Filtering%3A_Operator_Estimation_with_Spectral_Regularization.html">jmlr-2009-A New Approach to Collaborative Filtering: Operator Estimation with Spectral Regularization</a></p>
<p>Author: Jacob Abernethy, Francis Bach, Theodoros Evgeniou, Jean-Philippe Vert</p><p>Abstract: We present a general approach for collaborative ﬁltering (CF) using spectral regularization to learn linear operators mapping a set of “users” to a set of possibly desired “objects”. In particular, several recent low-rank type matrix-completion methods for CF are shown to be special cases of our proposed framework. Unlike existing regularization-based CF, our approach can be used to incorporate additional information such as attributes of the users/objects—a feature currently lacking in existing regularization-based CF approaches—using popular and well-known kernel methods. We provide novel representer theorems that we use to develop new estimation methods. We then provide learning algorithms based on low-rank decompositions and test them on a standard CF data set. The experiments indicate the advantages of generalizing the existing regularization-based CF methods to incorporate related information about users and objects. Finally, we show that certain multi-task learning methods can be also seen as special cases of our proposed approach. Keywords: collaborative ﬁltering, matrix completion, kernel methods, spectral regularization</p><p>3 <a title="jmlr-2009-3" href="../jmlr2009/jmlr-2009-A_Parameter-Free_Classification_Method_for_Large_Scale_Learning.html">jmlr-2009-A Parameter-Free Classification Method for Large Scale Learning</a></p>
<p>Author: Marc Boullé</p><p>Abstract: With the rapid growth of computer storage capacities, available data and demand for scoring models both follow an increasing trend, sharper than that of the processing power. However, the main limitation to a wide spread of data mining solutions is the non-increasing availability of skilled data analysts, which play a key role in data preparation and model selection. In this paper, we present a parameter-free scalable classiﬁcation method, which is a step towards fully automatic data mining. The method is based on Bayes optimal univariate conditional density estimators, naive Bayes classiﬁcation enhanced with a Bayesian variable selection scheme, and averaging of models using a logarithmic smoothing of the posterior distribution. We focus on the complexity of the algorithms and show how they can cope with data sets that are far larger than the available central memory. We ﬁnally report results on the Large Scale Learning challenge, where our method obtains state of the art performance within practicable computation time. Keywords: large scale learning, naive Bayes, Bayesianism, model selection, model averaging</p><p>4 <a title="jmlr-2009-4" href="../jmlr2009/jmlr-2009-A_Survey_of_Accuracy_Evaluation_Metrics_of_Recommendation_Tasks.html">jmlr-2009-A Survey of Accuracy Evaluation Metrics of Recommendation Tasks</a></p>
<p>Author: Asela Gunawardana, Guy Shani</p><p>Abstract: Recommender systems are now popular both commercially and in the research community, where many algorithms have been suggested for providing recommendations. These algorithms typically perform differently in various domains and tasks. Therefore, it is important from the research perspective, as well as from a practical view, to be able to decide on an algorithm that matches the domain and the task of interest. The standard way to make such decisions is by comparing a number of algorithms ofﬂine using some evaluation metric. Indeed, many evaluation metrics have been suggested for comparing recommendation algorithms. The decision on the proper evaluation metric is often critical, as each metric may favor a different algorithm. In this paper we review the proper construction of ofﬂine experiments for deciding on the most appropriate algorithm. We discuss three important tasks of recommender systems, and classify a set of appropriate well known evaluation metrics for each task. We demonstrate how using an improper evaluation metric can lead to the selection of an improper algorithm for the task of interest. We also discuss other important considerations when designing ofﬂine experiments. Keywords: recommender systems, collaborative ﬁltering, statistical analysis, comparative studies</p><p>5 <a title="jmlr-2009-5" href="../jmlr2009/jmlr-2009-Adaptive_False_Discovery_Rate_Control_under_Independence_and_Dependence.html">jmlr-2009-Adaptive False Discovery Rate Control under Independence and Dependence</a></p>
<p>Author: Gilles Blanchard,  Étienne Roquain</p><p>Abstract: In the context of multiple hypothesis testing, the proportion π0 of true null hypotheses in the pool of hypotheses to test often plays a crucial role, although it is generally unknown a priori. A testing procedure using an implicit or explicit estimate of this quantity in order to improve its efﬁcency is called adaptive. In this paper, we focus on the issue of false discovery rate (FDR) control and we present new adaptive multiple testing procedures with control of the FDR. In a ﬁrst part, assuming independence of the p-values, we present two new procedures and give a uniﬁed review of other existing adaptive procedures that have provably controlled FDR. We report extensive simulation results comparing these procedures and testing their robustness when the independence assumption is violated. The new proposed procedures appear competitive with existing ones. The overall best, though, is reported to be Storey’s estimator, albeit for a speciﬁc parameter setting that does not appear to have been considered before. In a second part, we propose adaptive versions of step-up procedures that have provably controlled FDR under positive dependence and unspeciﬁed dependence of the p-values, respectively. In the latter case, while simulations only show an improvement over non-adaptive procedures in limited situations, these are to our knowledge among the ﬁrst theoretically founded adaptive multiple testing procedures that control the FDR when the p-values are not independent. Keywords: multiple testing, false discovery rate, adaptive procedure, positive regression dependence, p-values</p><p>6 <a title="jmlr-2009-6" href="../jmlr2009/jmlr-2009-An_Algorithm_for_Reading_Dependencies_from_the_Minimal_Undirected_Independence_Map_of_a_Graphoid_that_Satisfies_Weak_Transitivity.html">jmlr-2009-An Algorithm for Reading Dependencies from the Minimal Undirected Independence Map of a Graphoid that Satisfies Weak Transitivity</a></p>
<p>Author: Jose M. Peña, Roland Nilsson, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We present a sound and complete graphical criterion for reading dependencies from the minimal undirected independence map G of a graphoid M that satisﬁes weak transitivity. Here, complete means that it is able to read all the dependencies in M that can be derived by applying the graphoid properties and weak transitivity to the dependencies used in the construction of G and the independencies obtained from G by vertex separation. We argue that assuming weak transitivity is not too restrictive. As an intermediate step in the derivation of the graphical criterion, we prove that for any undirected graph G there exists a strictly positive discrete probability distribution with the prescribed sample spaces that is faithful to G. We also report an algorithm that implements the graphical criterion and whose running time is considered to be at most O(n2 (e + n)) for n nodes and e edges. Finally, we illustrate how the graphical criterion can be used within bioinformatics to identify biologically meaningful gene dependencies. Keywords: graphical models, vertex separation, graphoids, weak transitivity, bioinformatics</p><p>7 <a title="jmlr-2009-7" href="../jmlr2009/jmlr-2009-An_Analysis_of_Convex_Relaxations_for_MAP_Estimation_of_Discrete_MRFs.html">jmlr-2009-An Analysis of Convex Relaxations for MAP Estimation of Discrete MRFs</a></p>
<p>Author: M. Pawan Kumar, Vladimir Kolmogorov, Philip H.S. Torr</p><p>Abstract: The problem of obtaining the maximum a posteriori estimate of a general discrete Markov random ﬁeld (i.e., a Markov random ﬁeld deﬁned using a discrete set of labels) is known to be NP-hard. However, due to its central importance in many applications, several approximation algorithms have been proposed in the literature. In this paper, we present an analysis of three such algorithms based on convex relaxations: (i) LP - S: the linear programming (LP) relaxation proposed by Schlesinger (1976) for a special case and independently in Chekuri et al. (2001), Koster et al. (1998), and Wainwright et al. (2005) for the general case; (ii) QP - RL: the quadratic programming (QP) relaxation of Ravikumar and Lafferty (2006); and (iii) SOCP - MS: the second order cone programming (SOCP) relaxation ﬁrst proposed by Muramatsu and Suzuki (2003) for two label problems and later extended by Kumar et al. (2006) for a general label set. We show that the SOCP - MS and the QP - RL relaxations are equivalent. Furthermore, we prove that despite the ﬂexibility in the form of the constraints/objective function offered by QP and SOCP, the LP - S relaxation strictly dominates (i.e., provides a better approximation than) QP - RL and SOCP MS . We generalize these results by deﬁning a large class of SOCP (and equivalent QP ) relaxations which is dominated by the LP - S relaxation. Based on these results we propose some novel SOCP relaxations which deﬁne constraints using random variables that form cycles or cliques in the graphical model representation of the random ﬁeld. Using some examples we show that the new SOCP relaxations strictly dominate the previous approaches. Keywords: probabilistic models, MAP estimation, discrete MRF, convex relaxations, linear programming, second-order cone programming, quadratic programming, dominating relaxations ∗. Work done while at the Dept. of Computing, Oxford Brookes University. c 2008 M. Pawan Kumar, Vladimir Kolmogorov, Philip H.S. Torr. K UMAR , KOLMOG</p><p>8 <a title="jmlr-2009-8" href="../jmlr2009/jmlr-2009-An_Anticorrelation_Kernel_for_Subsystem_Training_in_Multiple_Classifier_Systems.html">jmlr-2009-An Anticorrelation Kernel for Subsystem Training in Multiple Classifier Systems</a></p>
<p>Author: Luciana Ferrer, Kemal Sönmez, Elizabeth Shriberg</p><p>Abstract: We present a method for training support vector machine (SVM)-based classiﬁcation systems for combination with other classiﬁcation systems designed for the same task. Ideally, a new system should be designed such that, when combined with existing systems, the resulting performance is optimized. We present a simple model for this problem and use the understanding gained from this analysis to propose a method to achieve better combination performance when training SVM systems. We include a regularization term in the SVM objective function that aims to reduce the average class-conditional covariance between the resulting scores and the scores produced by the existing systems, introducing a trade-off between such covariance and the system’s individual performance. That is, the new system “takes one for the team”, falling somewhat short of its best possible performance in order to increase the diversity of the ensemble. We report results on the NIST 2005 and 2006 speaker recognition evaluations (SREs) for a variety of subsystems. We show a gain of 19% on the equal error rate (EER) of a combination of four systems when applying the proposed method with respect to the performance obtained when the four systems are trained independently of each other. Keywords: system combination, ensemble diversity, multiple classiﬁer systems, support vector machines, speaker recognition, kernel methods ∗. This author performed part of the work presented in this paper while at the Information Systems Laboratory, Department of Electrical Engineering, Stanford University. c 2009 Luciana Ferrer, Kemal S¨ nmez and Elizabeth Shriberg. o ¨ F ERRER , S ONMEZ AND S HRIBERG</p><p>9 <a title="jmlr-2009-9" href="../jmlr2009/jmlr-2009-Analysis_of_Perceptron-Based_Active_Learning.html">jmlr-2009-Analysis of Perceptron-Based Active Learning</a></p>
<p>Author: Sanjoy Dasgupta, Adam Tauman Kalai, Claire Monteleoni</p><p>Abstract: We start by showing that in an active learning setting, the Perceptron algorithm needs Ω( ε12 ) labels to learn linear separators within generalization error ε. We then present a simple active learning algorithm for this problem, which combines a modiﬁcation of the Perceptron update with an adaptive ﬁltering rule for deciding which points to query. For data distributed uniformly over the unit ˜ sphere, we show that our algorithm reaches generalization error ε after asking for just O(d log 1 ) ε labels. This exponential improvement over the usual sample complexity of supervised learning had previously been demonstrated only for the computationally more complex query-by-committee algorithm. Keywords: active learning, perceptron, label complexity bounds, online learning</p><p>10 <a title="jmlr-2009-10" href="../jmlr2009/jmlr-2009-Application_of_Non_Parametric_Empirical_Bayes_Estimation_to_High_Dimensional_Classification.html">jmlr-2009-Application of Non Parametric Empirical Bayes Estimation to High Dimensional Classification</a></p>
<p>Author: Eitan Greenshtein, Junyong Park</p><p>Abstract: We consider the problem of classiﬁcation using high dimensional features’ space. In a paper by Bickel and Levina (2004), it is recommended to use naive-Bayes classiﬁers, that is, to treat the features as if they are statistically independent. Consider now a sparse setup, where only a few of the features are informative for classiﬁcation. Fan and Fan (2008), suggested a variable selection and classiﬁcation method, called FAIR. The FAIR method improves the design of naive-Bayes classiﬁers in sparse setups. The improvement is due to reducing the noise in estimating the features’ means. This reduction is since that only the means of a few selected variables should be estimated. We also consider the design of naive Bayes classiﬁers. We show that a good alternative to variable selection is estimation of the means through a certain non parametric empirical Bayes procedure. In sparse setups the empirical Bayes implicitly performs an efﬁcient variable selection. It also adapts very well to non sparse setups, and has the advantage of making use of the information from many “weakly informative” variables, which variable selection type of classiﬁcation procedures give up on using. We compare our method with FAIR and other classiﬁcation methods in simulation for sparse and non sparse setups, and in real data examples involving classiﬁcation of normal versus malignant tissues based on microarray data. Keywords: non parametric empirical Bayes, high dimension, classiﬁcation</p><p>11 <a title="jmlr-2009-11" href="../jmlr2009/jmlr-2009-Bayesian_Network_Structure_Learning_by_Recursive_Autonomy_Identification.html">jmlr-2009-Bayesian Network Structure Learning by Recursive Autonomy Identification</a></p>
<p>Author: Raanan Yehezkel, Boaz Lerner</p><p>Abstract: We propose the recursive autonomy identiﬁcation (RAI) algorithm for constraint-based (CB) Bayesian network structure learning. The RAI algorithm learns the structure by sequential application of conditional independence (CI) tests, edge direction and structure decomposition into autonomous sub-structures. The sequence of operations is performed recursively for each autonomous substructure while simultaneously increasing the order of the CI test. While other CB algorithms d-separate structures and then direct the resulted undirected graph, the RAI algorithm combines the two processes from the outset and along the procedure. By this means and due to structure decomposition, learning a structure using RAI requires a smaller number of CI tests of high orders. This reduces the complexity and run-time of the algorithm and increases the accuracy by diminishing the curse-of-dimensionality. When the RAI algorithm learned structures from databases representing synthetic problems, known networks and natural problems, it demonstrated superiority with respect to computational complexity, run-time, structural correctness and classiﬁcation accuracy over the PC, Three Phase Dependency Analysis, Optimal Reinsertion, greedy search, Greedy Equivalence Search, Sparse Candidate, and Max-Min Hill-Climbing algorithms. Keywords: Bayesian networks, constraint-based structure learning</p><p>12 <a title="jmlr-2009-12" href="../jmlr2009/jmlr-2009-Bi-Level_Path_Following_for_Cross_Validated_Solution_of_Kernel_Quantile_Regression.html">jmlr-2009-Bi-Level Path Following for Cross Validated Solution of Kernel Quantile Regression</a></p>
<p>Author: Saharon Rosset</p><p>Abstract: We show how to follow the path of cross validated solutions to families of regularized optimization problems, deﬁned by a combination of a parameterized loss function and a regularization term. A primary example is kernel quantile regression, where the parameter of the loss function is the quantile being estimated. Even though the bi-level optimization problem we encounter for every quantile is non-convex, the manner in which the optimal cross-validated solution evolves with the parameter of the loss function allows tracking of this solution. We prove this property, construct the resulting algorithm, and demonstrate it on real and artiﬁcial data. This algorithm allows us to efﬁciently solve the whole family of bi-level problems. We show how it can be extended to cover other modeling problems, like support vector regression, and alternative in-sample model selection approaches.1</p><p>13 <a title="jmlr-2009-13" href="../jmlr2009/jmlr-2009-Bounded_Kernel-Based_Online_Learning.html">jmlr-2009-Bounded Kernel-Based Online Learning</a></p>
<p>Author: Francesco Orabona, Joseph Keshet, Barbara Caputo</p><p>Abstract: A common problem of kernel-based online algorithms, such as the kernel-based Perceptron algorithm, is the amount of memory required to store the online hypothesis, which may increase without bound as the algorithm progresses. Furthermore, the computational load of such algorithms grows linearly with the amount of memory used to store the hypothesis. To attack these problems, most previous work has focused on discarding some of the instances, in order to keep the memory bounded. In this paper we present a new algorithm, in which the instances are not discarded, but are instead projected onto the space spanned by the previous online hypothesis. We call this algorithm Projectron. While the memory size of the Projectron solution cannot be predicted before training, we prove that its solution is guaranteed to be bounded. We derive a relative mistake bound for the proposed algorithm, and deduce from it a slightly different algorithm which outperforms the Perceptron. We call this second algorithm Projectron++. We show that this algorithm can be extended to handle the multiclass and the structured output settings, resulting, as far as we know, in the ﬁrst online bounded algorithm that can learn complex classiﬁcation tasks. The method of bounding the hypothesis representation can be applied to any conservative online algorithm and to other online algorithms, as it is demonstrated for ALMA2 . Experimental results on various data sets show the empirical advantage of our technique compared to various bounded online algorithms, both in terms of memory and accuracy. Keywords: online learning, kernel methods, support vector machines, bounded support set</p><p>14 <a title="jmlr-2009-14" href="../jmlr2009/jmlr-2009-CarpeDiem%3A_Optimizing_the_Viterbi_Algorithm_and_Applications_to_Supervised_Sequential_Learning.html">jmlr-2009-CarpeDiem: Optimizing the Viterbi Algorithm and Applications to Supervised Sequential Learning</a></p>
<p>Author: Roberto Esposito, Daniele P. Radicioni</p><p>Abstract: The growth of information available to learning systems and the increasing complexity of learning tasks determine the need for devising algorithms that scale well with respect to all learning parameters. In the context of supervised sequential learning, the Viterbi algorithm plays a fundamental role, by allowing the evaluation of the best (most probable) sequence of labels with a time complexity linear in the number of time events, and quadratic in the number of labels. In this paper we propose CarpeDiem, a novel algorithm allowing the evaluation of the best possible sequence of labels with a sub-quadratic time complexity.1 We provide theoretical grounding together with solid empirical results supporting two chief facts. CarpeDiem always ﬁnds the optimal solution requiring, in most cases, only a small fraction of the time taken by the Viterbi algorithm; meantime, CarpeDiem is never asymptotically worse than the Viterbi algorithm, thus conﬁrming it as a sound replacement. Keywords: Viterbi algorithm, sequence labeling, conditional models, classiﬁers optimization, exact inference</p><p>15 <a title="jmlr-2009-15" href="../jmlr2009/jmlr-2009-Cautious_Collective_Classification.html">jmlr-2009-Cautious Collective Classification</a></p>
<p>Author: Luke K. McDowell, Kalyan Moy Gupta, David W. Aha</p><p>Abstract: Many collective classiﬁcation (CC) algorithms have been shown to increase accuracy when instances are interrelated. However, CC algorithms must be carefully applied because their use of estimated labels can in some cases decrease accuracy. In this article, we show that managing this label uncertainty through cautious algorithmic behavior is essential to achieving maximal, robust performance. First, we describe cautious inference and explain how four well-known families of CC algorithms can be parameterized to use varying degrees of such caution. Second, we introduce cautious learning and show how it can be used to improve the performance of almost any CC algorithm, with or without cautious inference. We then evaluate cautious inference and learning for the four collective inference families, with three local classiﬁers and a range of both synthetic and real-world data. We ﬁnd that cautious learning and cautious inference typically outperform less cautious approaches. In addition, we identify the data characteristics that predict more substantial performance differences. Our results reveal that the degree of caution used usually has a larger impact on performance than the choice of the underlying inference algorithm. Together, these results identify the most appropriate CC algorithms to use for particular task characteristics and explain multiple conﬂicting ﬁndings from prior CC research. Keywords: collective inference, statistical relational learning, approximate probabilistic inference, networked data, cautious inference</p><p>16 <a title="jmlr-2009-16" href="../jmlr2009/jmlr-2009-Classification_with_Gaussians_and_Convex_Loss.html">jmlr-2009-Classification with Gaussians and Convex Loss</a></p>
<p>Author: Dao-Hong Xiang, Ding-Xuan Zhou</p><p>Abstract: This paper considers binary classiﬁcation algorithms generated from Tikhonov regularization schemes associated with general convex loss functions and varying Gaussian kernels. Our main goal is to provide fast convergence rates for the excess misclassiﬁcation error. Allowing varying Gaussian kernels in the algorithms improves learning rates measured by regularization error and sample error. Special structures of Gaussian kernels enable us to construct, by a nice approximation scheme with a Fourier analysis technique, uniformly bounded regularizing functions achieving polynomial decays of the regularization error under a Sobolev smoothness condition. The sample error is estimated by using a projection operator and a tight bound for the covering numbers of reproducing kernel Hilbert spaces generated by Gaussian kernels. The convexity of the general loss function plays a very important role in our analysis. Keywords: reproducing kernel Hilbert space, binary classiﬁcation, general convex loss, varying Gaussian kernels, covering number, approximation</p><p>17 <a title="jmlr-2009-17" href="../jmlr2009/jmlr-2009-Computing_Maximum_Likelihood_Estimates_in_Recursive_Linear_Models_with_Correlated_Errors.html">jmlr-2009-Computing Maximum Likelihood Estimates in Recursive Linear Models with Correlated Errors</a></p>
<p>Author: Mathias Drton, Michael Eichler, Thomas S. Richardson</p><p>Abstract: In recursive linear models, the multivariate normal joint distribution of all variables exhibits a dependence structure induced by a recursive (or acyclic) system of linear structural equations. These linear models have a long tradition and appear in seemingly unrelated regressions, structural equation modelling, and approaches to causal inference. They are also related to Gaussian graphical models via a classical representation known as a path diagram. Despite the models’ long history, a number of problems remain open. In this paper, we address the problem of computing maximum likelihood estimates in the subclass of ‘bow-free’ recursive linear models. The term ‘bow-free’ refers to the condition that the errors for variables i and j be uncorrelated if variable i occurs in the structural equation for variable j. We introduce a new algorithm, termed Residual Iterative Conditional Fitting (RICF), that can be implemented using only least squares computations. In contrast to existing algorithms, RICF has clear convergence properties and yields exact maximum likelihood estimates after the ﬁrst iteration whenever the MLE is available in closed form. Keywords: linear regression, maximum likelihood estimation, path diagram, structural equation model, recursive semi-Markov model, residual iterative conditional ﬁtting</p><p>18 <a title="jmlr-2009-18" href="../jmlr2009/jmlr-2009-Consistency_and_Localizability.html">jmlr-2009-Consistency and Localizability</a></p>
<p>Author: Alon Zakai, Ya'acov Ritov</p><p>Abstract: We show that all consistent learning methodsĂ˘&euro;&rdquo;that is, that asymptotically achieve the lowest possible expected loss for any distribution on (X,Y )Ă˘&euro;&rdquo;are necessarily localizable, by which we mean that they do not signiÄ?Ĺš cantly change their response at a particular point when we show them only the part of the training set that is close to that point. This is true in particular for methods that appear to be deÄ?Ĺš ned in a non-local manner, such as support vector machines in classiÄ?Ĺš cation and least-squares estimators in regression. Aside from showing that consistency implies a speciÄ?Ĺš c form of localizability, we also show that consistency is logically equivalent to the combination of two properties: (1) a form of localizability, and (2) that the methodĂ˘&euro;&trade;s global mean (over the entire X distribution) correctly estimates the true mean. Consistency can therefore be seen as comprised of two aspects, one local and one global. Keywords: consistency, local learning, regression, classiÄ?Ĺš cation</p><p>19 <a title="jmlr-2009-19" href="../jmlr2009/jmlr-2009-Controlling_the_False_Discovery_Rate_of_the_Association_Causality_Structure_Learned_with_the_PC_Algorithm%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">jmlr-2009-Controlling the False Discovery Rate of the Association Causality Structure Learned with the PC Algorithm    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>Author: Junning Li, Z. Jane Wang</p><p>Abstract: In real world applications, graphical statistical models are not only a tool for operations such as classiﬁcation or prediction, but usually the network structures of the models themselves are also of great interest (e.g., in modeling brain connectivity). The false discovery rate (FDR), the expected ratio of falsely claimed connections to all those claimed, is often a reasonable error-rate criterion in these applications. However, current learning algorithms for graphical models have not been adequately adapted to the concerns of the FDR. The traditional practice of controlling the type I error rate and the type II error rate under a conventional level does not necessarily keep the FDR low, especially in the case of sparse networks. In this paper, we propose embedding an FDR-control procedure into the PC algorithm to curb the FDR of the skeleton of the learned graph. We prove that the proposed method can control the FDR under a user-speciﬁed level at the limit of large sample sizes. In the cases of moderate sample size (about several hundred), empirical experiments show that the method is still able to control the FDR under the user-speciﬁed level, and a heuristic modiﬁcation of the method is able to control the FDR more accurately around the user-speciﬁed level. The proposed method is applicable to any models for which statistical tests of conditional independence are available, such as discrete models and Gaussian models. Keywords: Bayesian networks, false discovery rate, PC algorithm, directed acyclic graph, skeleton</p><p>20 <a title="jmlr-2009-20" href="../jmlr2009/jmlr-2009-DL-Learner%3A_Learning_Concepts_in_Description_Logics.html">jmlr-2009-DL-Learner: Learning Concepts in Description Logics</a></p>
<p>Author: Jens Lehmann</p><p>Abstract: In this paper, we introduce DL-Learner, a framework for learning in description logics and OWL. OWL is the ofÄ?Ĺš cial W3C standard ontology language for the Semantic Web. Concepts in this language can be learned for constructing and maintaining OWL ontologies or for solving problems similar to those in Inductive Logic Programming. DL-Learner includes several learning algorithms, support for different OWL formats, reasoner interfaces, and learning problems. It is a cross-platform framework implemented in Java. The framework allows easy programmatic access and provides a command line interface, a graphical interface as well as a WSDL-based web service. Keywords: concept learning, description logics, OWL, classiÄ?Ĺš cation, open-source</p><p>21 <a title="jmlr-2009-21" href="../jmlr2009/jmlr-2009-Data-driven_Calibration_of_Penalties_for_Least-Squares_Regression.html">jmlr-2009-Data-driven Calibration of Penalties for Least-Squares Regression</a></p>
<p>22 <a title="jmlr-2009-22" href="../jmlr2009/jmlr-2009-Deterministic_Error_Analysis_of_Support_Vector_Regression_and_Related_Regularized_Kernel_Methods.html">jmlr-2009-Deterministic Error Analysis of Support Vector Regression and Related Regularized Kernel Methods</a></p>
<p>23 <a title="jmlr-2009-23" href="../jmlr2009/jmlr-2009-Discriminative_Learning_Under_Covariate_Shift.html">jmlr-2009-Discriminative Learning Under Covariate Shift</a></p>
<p>24 <a title="jmlr-2009-24" href="../jmlr2009/jmlr-2009-Distance_Metric_Learning_for_Large_Margin_Nearest_Neighbor_Classification.html">jmlr-2009-Distance Metric Learning for Large Margin Nearest Neighbor Classification</a></p>
<p>25 <a title="jmlr-2009-25" href="../jmlr2009/jmlr-2009-Distributed_Algorithms_for_Topic_Models.html">jmlr-2009-Distributed Algorithms for Topic Models</a></p>
<p>26 <a title="jmlr-2009-26" href="../jmlr2009/jmlr-2009-Dlib-ml%3A_A_Machine_Learning_Toolkit%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">jmlr-2009-Dlib-ml: A Machine Learning Toolkit    (Machine Learning Open Source Software Paper)</a></p>
<p>27 <a title="jmlr-2009-27" href="../jmlr2009/jmlr-2009-Efficient_Online_and_Batch_Learning_Using_Forward_Backward_Splitting.html">jmlr-2009-Efficient Online and Batch Learning Using Forward Backward Splitting</a></p>
<p>28 <a title="jmlr-2009-28" href="../jmlr2009/jmlr-2009-Entropy_Inference_and_the_James-Stein_Estimator%2C_with_Application_to_Nonlinear_Gene_Association_Networks.html">jmlr-2009-Entropy Inference and the James-Stein Estimator, with Application to Nonlinear Gene Association Networks</a></p>
<p>29 <a title="jmlr-2009-29" href="../jmlr2009/jmlr-2009-Estimating_Labels_from_Label_Proportions.html">jmlr-2009-Estimating Labels from Label Proportions</a></p>
<p>30 <a title="jmlr-2009-30" href="../jmlr2009/jmlr-2009-Estimation_of_Sparse_Binary_Pairwise_Markov_Networks_using_Pseudo-likelihoods.html">jmlr-2009-Estimation of Sparse Binary Pairwise Markov Networks using Pseudo-likelihoods</a></p>
<p>31 <a title="jmlr-2009-31" href="../jmlr2009/jmlr-2009-Evolutionary_Model_Type_Selection_for_Global_Surrogate_Modeling.html">jmlr-2009-Evolutionary Model Type Selection for Global Surrogate Modeling</a></p>
<p>32 <a title="jmlr-2009-32" href="../jmlr2009/jmlr-2009-Exploiting_Product_Distributions_to_Identify_Relevant_Variables_of_Correlation_Immune_Functions.html">jmlr-2009-Exploiting Product Distributions to Identify Relevant Variables of Correlation Immune Functions</a></p>
<p>33 <a title="jmlr-2009-33" href="../jmlr2009/jmlr-2009-Exploring_Strategies_for_Training_Deep_Neural_Networks.html">jmlr-2009-Exploring Strategies for Training Deep Neural Networks</a></p>
<p>34 <a title="jmlr-2009-34" href="../jmlr2009/jmlr-2009-Fast_ApproximatekNN_Graph_Construction_for_High_Dimensional_Data_via_Recursive_Lanczos_Bisection.html">jmlr-2009-Fast ApproximatekNN Graph Construction for High Dimensional Data via Recursive Lanczos Bisection</a></p>
<p>35 <a title="jmlr-2009-35" href="../jmlr2009/jmlr-2009-Feature_Selection_with_Ensembles%2C_Artificial_Variables%2C_and_Redundancy_Elimination%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr-2009-Feature Selection with Ensembles, Artificial Variables, and Redundancy Elimination    (Special Topic on Model Selection)</a></p>
<p>36 <a title="jmlr-2009-36" href="../jmlr2009/jmlr-2009-Fourier_Theoretic_Probabilistic_Inference_over_Permutations.html">jmlr-2009-Fourier Theoretic Probabilistic Inference over Permutations</a></p>
<p>37 <a title="jmlr-2009-37" href="../jmlr2009/jmlr-2009-Generalization_Bounds_for_Ranking_Algorithms_via_Algorithmic_Stability.html">jmlr-2009-Generalization Bounds for Ranking Algorithms via Algorithmic Stability</a></p>
<p>38 <a title="jmlr-2009-38" href="../jmlr2009/jmlr-2009-Hash_Kernels_for_Structured_Data.html">jmlr-2009-Hash Kernels for Structured Data</a></p>
<p>39 <a title="jmlr-2009-39" href="../jmlr2009/jmlr-2009-Hybrid_MPI_OpenMP_Parallel_Linear_Support_Vector_Machine_Training.html">jmlr-2009-Hybrid MPI OpenMP Parallel Linear Support Vector Machine Training</a></p>
<p>40 <a title="jmlr-2009-40" href="../jmlr2009/jmlr-2009-Identification_of_Recurrent_Neural_Networks_by_Bayesian_Interrogation_Techniques.html">jmlr-2009-Identification of Recurrent Neural Networks by Bayesian Interrogation Techniques</a></p>
<p>41 <a title="jmlr-2009-41" href="../jmlr2009/jmlr-2009-Improving_the_Reliability_of_Causal_Discovery_from_Small_Data_Sets_Using_Argumentation%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">jmlr-2009-Improving the Reliability of Causal Discovery from Small Data Sets Using Argumentation    (Special Topic on Causality)</a></p>
<p>42 <a title="jmlr-2009-42" href="../jmlr2009/jmlr-2009-Incorporating_Functional_Knowledge_in_Neural_Networks.html">jmlr-2009-Incorporating Functional Knowledge in Neural Networks</a></p>
<p>43 <a title="jmlr-2009-43" href="../jmlr2009/jmlr-2009-Java-ML%3A_A_Machine_Learning_Library%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">jmlr-2009-Java-ML: A Machine Learning Library    (Machine Learning Open Source Software Paper)</a></p>
<p>44 <a title="jmlr-2009-44" href="../jmlr2009/jmlr-2009-Learning_Acyclic_Probabilistic_Circuits_Using_Test_Paths.html">jmlr-2009-Learning Acyclic Probabilistic Circuits Using Test Paths</a></p>
<p>45 <a title="jmlr-2009-45" href="../jmlr2009/jmlr-2009-Learning_Approximate_Sequential_Patterns_for_Classification.html">jmlr-2009-Learning Approximate Sequential Patterns for Classification</a></p>
<p>46 <a title="jmlr-2009-46" href="../jmlr2009/jmlr-2009-Learning_Halfspaces_with_Malicious_Noise.html">jmlr-2009-Learning Halfspaces with Malicious Noise</a></p>
<p>47 <a title="jmlr-2009-47" href="../jmlr2009/jmlr-2009-Learning_Linear_Ranking_Functions_for_Beam_Search_with_Application_to_Planning.html">jmlr-2009-Learning Linear Ranking Functions for Beam Search with Application to Planning</a></p>
<p>48 <a title="jmlr-2009-48" href="../jmlr2009/jmlr-2009-Learning_Nondeterministic_Classifiers.html">jmlr-2009-Learning Nondeterministic Classifiers</a></p>
<p>49 <a title="jmlr-2009-49" href="../jmlr2009/jmlr-2009-Learning_Permutations_with_Exponential_Weights.html">jmlr-2009-Learning Permutations with Exponential Weights</a></p>
<p>50 <a title="jmlr-2009-50" href="../jmlr2009/jmlr-2009-Learning_When_Concepts_Abound.html">jmlr-2009-Learning When Concepts Abound</a></p>
<p>51 <a title="jmlr-2009-51" href="../jmlr2009/jmlr-2009-Low-Rank_Kernel_Learning_with_Bregman_Matrix_Divergences.html">jmlr-2009-Low-Rank Kernel Learning with Bregman Matrix Divergences</a></p>
<p>52 <a title="jmlr-2009-52" href="../jmlr2009/jmlr-2009-Margin-based_Ranking_and_an_Equivalence_between_AdaBoost_and_RankBoost.html">jmlr-2009-Margin-based Ranking and an Equivalence between AdaBoost and RankBoost</a></p>
<p>53 <a title="jmlr-2009-53" href="../jmlr2009/jmlr-2009-Marginal_Likelihood_Integrals_for_Mixtures_of_Independence_Models.html">jmlr-2009-Marginal Likelihood Integrals for Mixtures of Independence Models</a></p>
<p>54 <a title="jmlr-2009-54" href="../jmlr2009/jmlr-2009-Markov_Properties_for_Linear_Causal_Models_with_Correlated_Errors%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">jmlr-2009-Markov Properties for Linear Causal Models with Correlated Errors    (Special Topic on Causality)</a></p>
<p>55 <a title="jmlr-2009-55" href="../jmlr2009/jmlr-2009-Maximum_Entropy_Discrimination_Markov_Networks.html">jmlr-2009-Maximum Entropy Discrimination Markov Networks</a></p>
<p>56 <a title="jmlr-2009-56" href="../jmlr2009/jmlr-2009-Model_Monitor_%28M2%29%3A_Evaluating%2C_Comparing%2C_and_Monitoring_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">jmlr-2009-Model Monitor (M2): Evaluating, Comparing, and Monitoring Models    (Machine Learning Open Source Software Paper)</a></p>
<p>57 <a title="jmlr-2009-57" href="../jmlr2009/jmlr-2009-Multi-task_Reinforcement_Learning_in_Partially_Observable_Stochastic_Environments.html">jmlr-2009-Multi-task Reinforcement Learning in Partially Observable Stochastic Environments</a></p>
<p>58 <a title="jmlr-2009-58" href="../jmlr2009/jmlr-2009-NEUROSVM%3A_An_Architecture_to_Reduce_the_Effect_of_the_Choice_of_Kernel_on_the_Performance_of_SVM.html">jmlr-2009-NEUROSVM: An Architecture to Reduce the Effect of the Choice of Kernel on the Performance of SVM</a></p>
<p>59 <a title="jmlr-2009-59" href="../jmlr2009/jmlr-2009-Nearest_Neighbor_Clustering%3A_A_Baseline_Method_for_Consistent_Clustering_with_Arbitrary_Objective_Functions.html">jmlr-2009-Nearest Neighbor Clustering: A Baseline Method for Consistent Clustering with Arbitrary Objective Functions</a></p>
<p>60 <a title="jmlr-2009-60" href="../jmlr2009/jmlr-2009-Nieme%3A_Large-Scale_Energy-Based_Models%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">jmlr-2009-Nieme: Large-Scale Energy-Based Models    (Machine Learning Open Source Software Paper)</a></p>
<p>61 <a title="jmlr-2009-61" href="../jmlr2009/jmlr-2009-Nonextensive_Information_Theoretic_Kernels_on_Measures.html">jmlr-2009-Nonextensive Information Theoretic Kernels on Measures</a></p>
<p>62 <a title="jmlr-2009-62" href="../jmlr2009/jmlr-2009-Nonlinear_Models_Using_Dirichlet_Process_Mixtures.html">jmlr-2009-Nonlinear Models Using Dirichlet Process Mixtures</a></p>
<p>63 <a title="jmlr-2009-63" href="../jmlr2009/jmlr-2009-On_Efficient_Large_Margin_Semisupervised_Learning%3A_Method_and_Theory.html">jmlr-2009-On Efficient Large Margin Semisupervised Learning: Method and Theory</a></p>
<p>64 <a title="jmlr-2009-64" href="../jmlr2009/jmlr-2009-On_The_Power_of_Membership_Queries_in_Agnostic_Learning.html">jmlr-2009-On The Power of Membership Queries in Agnostic Learning</a></p>
<p>65 <a title="jmlr-2009-65" href="../jmlr2009/jmlr-2009-On_Uniform_Deviations_of_General_Empirical_Risks_with_Unboundedness%2C_Dependence%2C_and_High_Dimensionality.html">jmlr-2009-On Uniform Deviations of General Empirical Risks with Unboundedness, Dependence, and High Dimensionality</a></p>
<p>66 <a title="jmlr-2009-66" href="../jmlr2009/jmlr-2009-On_the_Consistency_of_Feature_Selection_using_Greedy_Least_Squares_Regression.html">jmlr-2009-On the Consistency of Feature Selection using Greedy Least Squares Regression</a></p>
<p>67 <a title="jmlr-2009-67" href="../jmlr2009/jmlr-2009-Online_Learning_with_Sample_Path_Constraints.html">jmlr-2009-Online Learning with Sample Path Constraints</a></p>
<p>68 <a title="jmlr-2009-68" href="../jmlr2009/jmlr-2009-Online_Learning_with_Samples_Drawn_from_Non-identical_Distributions.html">jmlr-2009-Online Learning with Samples Drawn from Non-identical Distributions</a></p>
<p>69 <a title="jmlr-2009-69" href="../jmlr2009/jmlr-2009-Optimized_Cutting_Plane_Algorithm_for_Large-Scale_Risk_Minimization.html">jmlr-2009-Optimized Cutting Plane Algorithm for Large-Scale Risk Minimization</a></p>
<p>70 <a title="jmlr-2009-70" href="../jmlr2009/jmlr-2009-Particle_Swarm_Model_Selection%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr-2009-Particle Swarm Model Selection    (Special Topic on Model Selection)</a></p>
<p>71 <a title="jmlr-2009-71" href="../jmlr2009/jmlr-2009-Perturbation_Corrections_in_Approximate_Inference%3A_Mixture_Modelling_Applications.html">jmlr-2009-Perturbation Corrections in Approximate Inference: Mixture Modelling Applications</a></p>
<p>72 <a title="jmlr-2009-72" href="../jmlr2009/jmlr-2009-Polynomial-Delay_Enumeration_of_Monotonic_Graph_Classes.html">jmlr-2009-Polynomial-Delay Enumeration of Monotonic Graph Classes</a></p>
<p>73 <a title="jmlr-2009-73" href="../jmlr2009/jmlr-2009-Prediction_With_Expert_Advice_For_The_Brier_Game.html">jmlr-2009-Prediction With Expert Advice For The Brier Game</a></p>
<p>74 <a title="jmlr-2009-74" href="../jmlr2009/jmlr-2009-Properties_of_Monotonic_Effects_on_Directed_Acyclic_Graphs.html">jmlr-2009-Properties of Monotonic Effects on Directed Acyclic Graphs</a></p>
<p>75 <a title="jmlr-2009-75" href="../jmlr2009/jmlr-2009-Provably_Efficient_Learning_with_Typed_Parametric_Models.html">jmlr-2009-Provably Efficient Learning with Typed Parametric Models</a></p>
<p>76 <a title="jmlr-2009-76" href="../jmlr2009/jmlr-2009-Python_Environment_for_Bayesian_Learning%3A_Inferring_the_Structure_of_Bayesian_Networks_from_Knowledge_and_Data%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">jmlr-2009-Python Environment for Bayesian Learning: Inferring the Structure of Bayesian Networks from Knowledge and Data    (Machine Learning Open Source Software Paper)</a></p>
<p>77 <a title="jmlr-2009-77" href="../jmlr2009/jmlr-2009-RL-Glue%3A_Language-Independent_Software_for_Reinforcement-Learning_Experiments%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">jmlr-2009-RL-Glue: Language-Independent Software for Reinforcement-Learning Experiments    (Machine Learning Open Source Software Paper)</a></p>
<p>78 <a title="jmlr-2009-78" href="../jmlr2009/jmlr-2009-Refinement_of_Reproducing_Kernels.html">jmlr-2009-Refinement of Reproducing Kernels</a></p>
<p>79 <a title="jmlr-2009-79" href="../jmlr2009/jmlr-2009-Reinforcement_Learning_in_Finite_MDPs%3A_PAC_Analysis.html">jmlr-2009-Reinforcement Learning in Finite MDPs: PAC Analysis</a></p>
<p>80 <a title="jmlr-2009-80" href="../jmlr2009/jmlr-2009-Reproducing_Kernel_Banach_Spaces_for_Machine_Learning.html">jmlr-2009-Reproducing Kernel Banach Spaces for Machine Learning</a></p>
<p>81 <a title="jmlr-2009-81" href="../jmlr2009/jmlr-2009-Robust_Process_Discovery_with_Artificial_Negative_Events%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">jmlr-2009-Robust Process Discovery with Artificial Negative Events    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>82 <a title="jmlr-2009-82" href="../jmlr2009/jmlr-2009-Robustness_and_Regularization_of_Support_Vector_Machines.html">jmlr-2009-Robustness and Regularization of Support Vector Machines</a></p>
<p>83 <a title="jmlr-2009-83" href="../jmlr2009/jmlr-2009-SGD-QN%3A_Careful_Quasi-Newton_Stochastic_Gradient_Descent.html">jmlr-2009-SGD-QN: Careful Quasi-Newton Stochastic Gradient Descent</a></p>
<p>84 <a title="jmlr-2009-84" href="../jmlr2009/jmlr-2009-Scalable_Collaborative_Filtering_Approaches_for_Large_Recommender_Systems%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">jmlr-2009-Scalable Collaborative Filtering Approaches for Large Recommender Systems    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>85 <a title="jmlr-2009-85" href="../jmlr2009/jmlr-2009-Settable_Systems%3A_An_Extension_of_Pearl%27s_Causal_Model_with_Optimization%2C_Equilibrium%2C_and_Learning.html">jmlr-2009-Settable Systems: An Extension of Pearl's Causal Model with Optimization, Equilibrium, and Learning</a></p>
<p>86 <a title="jmlr-2009-86" href="../jmlr2009/jmlr-2009-Similarity-based_Classification%3A_Concepts_and_Algorithms.html">jmlr-2009-Similarity-based Classification: Concepts and Algorithms</a></p>
<p>87 <a title="jmlr-2009-87" href="../jmlr2009/jmlr-2009-Sparse_Online_Learning_via_Truncated_Gradient.html">jmlr-2009-Sparse Online Learning via Truncated Gradient</a></p>
<p>88 <a title="jmlr-2009-88" href="../jmlr2009/jmlr-2009-Stable_and_Efficient_Gaussian_Process_Calculations.html">jmlr-2009-Stable and Efficient Gaussian Process Calculations</a></p>
<p>89 <a title="jmlr-2009-89" href="../jmlr2009/jmlr-2009-Strong_Limit_Theorems_for_the_Bayesian_Scoring_Criterion_in_Bayesian_Networks.html">jmlr-2009-Strong Limit Theorems for the Bayesian Scoring Criterion in Bayesian Networks</a></p>
<p>90 <a title="jmlr-2009-90" href="../jmlr2009/jmlr-2009-Structure_Spaces.html">jmlr-2009-Structure Spaces</a></p>
<p>91 <a title="jmlr-2009-91" href="../jmlr2009/jmlr-2009-Subgroup_Analysis_via_Recursive_Partitioning.html">jmlr-2009-Subgroup Analysis via Recursive Partitioning</a></p>
<p>92 <a title="jmlr-2009-92" href="../jmlr2009/jmlr-2009-Supervised_Descriptive_Rule_Discovery%3A_A_Unifying_Survey_of_Contrast_Set%2C_Emerging_Pattern_and_Subgroup_Mining.html">jmlr-2009-Supervised Descriptive Rule Discovery: A Unifying Survey of Contrast Set, Emerging Pattern and Subgroup Mining</a></p>
<p>93 <a title="jmlr-2009-93" href="../jmlr2009/jmlr-2009-The_Hidden_Life_of_Latent_Variables%3A_Bayesian_Learning_with_Mixed_Graph_Models.html">jmlr-2009-The Hidden Life of Latent Variables: Bayesian Learning with Mixed Graph Models</a></p>
<p>94 <a title="jmlr-2009-94" href="../jmlr2009/jmlr-2009-The_Nonparanormal%3A_Semiparametric_Estimation_of_High_Dimensional_Undirected_Graphs.html">jmlr-2009-The Nonparanormal: Semiparametric Estimation of High Dimensional Undirected Graphs</a></p>
<p>95 <a title="jmlr-2009-95" href="../jmlr2009/jmlr-2009-The_P-Norm_Push%3A_A_Simple_Convex_Ranking_Algorithm_that_Concentrates_at_the_Top_of_the_List.html">jmlr-2009-The P-Norm Push: A Simple Convex Ranking Algorithm that Concentrates at the Top of the List</a></p>
<p>96 <a title="jmlr-2009-96" href="../jmlr2009/jmlr-2009-Transfer_Learning_for_Reinforcement_Learning_Domains%3A_A_Survey.html">jmlr-2009-Transfer Learning for Reinforcement Learning Domains: A Survey</a></p>
<p>97 <a title="jmlr-2009-97" href="../jmlr2009/jmlr-2009-Ultrahigh_Dimensional_Feature_Selection%3A_Beyond_The_Linear_Model.html">jmlr-2009-Ultrahigh Dimensional Feature Selection: Beyond The Linear Model</a></p>
<p>98 <a title="jmlr-2009-98" href="../jmlr2009/jmlr-2009-Universal_Kernel-Based_Learning_with_Applications_to_Regular_Languages%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Mining_and_Learning_with_Graphs_and_Relations%29.html">jmlr-2009-Universal Kernel-Based Learning with Applications to Regular Languages    (Special Topic on Mining and Learning with Graphs and Relations)</a></p>
<p>99 <a title="jmlr-2009-99" href="../jmlr2009/jmlr-2009-Using_Local_Dependencies_within_Batches_to_Improve_Large_Margin_Classifiers.html">jmlr-2009-Using Local Dependencies within Batches to Improve Large Margin Classifiers</a></p>
<p>100 <a title="jmlr-2009-100" href="../jmlr2009/jmlr-2009-When_Is_There_a_Representer_Theorem%3F__Vector_Versus_Matrix_Regularizers.html">jmlr-2009-When Is There a Representer Theorem?  Vector Versus Matrix Regularizers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
