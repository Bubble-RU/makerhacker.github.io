<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>jmlr 2008 knowledge graph</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="#">jmlr2008</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>jmlr 2008 knowledge graph</h1>
<br/><h3>similar papers computed by tfidf model</h3><br/><h3>similar papers computed by <a title="lsi-model" href="./jmlr2008_lsi.html">lsi model</a></h3><br/><h3>similar papers computed by <a title="lda-model" href="./jmlr2008_lda.html">lda model</a></h3><br/><h2>papers list:</h2><p>1 <a title="jmlr-2008-1" href="../jmlr2008/jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>Author: Ja-Yong Koo, Yoonkyung Lee, Yuwon Kim, Changyi Park</p><p>Abstract: The support vector machine has been successful in a variety of applications. Also on the theoretical front, statistical properties of the support vector machine have been studied quite extensively with a particular attention to its Bayes risk consistency under some conditions. In this paper, we study somewhat basic statistical properties of the support vector machine yet to be investigated, namely the asymptotic behavior of the coefﬁcients of the linear support vector machine. A Bahadur type representation of the coefﬁcients is established under appropriate conditions, and their asymptotic normality and statistical variability are derived on the basis of the representation. These asymptotic results do not only help further our understanding of the support vector machine, but also they can be useful for related statistical inferences. Keywords: asymptotic normality, Bahadur representation, classiﬁcation, convexity lemma, Radon transform</p><p>2 <a title="jmlr-2008-2" href="../jmlr2008/jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Stefan Klanke, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages. Keywords: regression, local learning, online learning, C, C++, Matlab, Octave, Python</p><p>3 <a title="jmlr-2008-3" href="../jmlr2008/jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>Author: Bernadetta Tarigan, Sara A. van de Geer</p><p>Abstract: The success of support vector machines in binary classiﬁcation relies on the fact that hinge loss employed in the risk minimization targets the Bayes rule. Recent research explores some extensions of this large margin based method to the multicategory case. We show a moment bound for the socalled multi-hinge loss minimizers based on two kinds of complexity constraints: entropy with bracketing and empirical entropy. Obtaining such a result based on the latter is harder than ﬁnding one based on the former. We obtain fast rates of convergence that adapt to the unknown margin. Keywords: multi-hinge classiﬁcation, all-at-once, moment bound, fast rate, entropy</p><p>4 <a title="jmlr-2008-4" href="../jmlr2008/jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>Author: Zach Jorgensen, Yan Zhou, Meador Inge</p><p>Abstract: Statistical spam ﬁlters are known to be vulnerable to adversarial attacks. One of the more common adversarial attacks, known as the good word attack, thwarts spam ﬁlters by appending to spam messages sets of “good” words, which are words that are common in legitimate email but rare in spam. We present a counterattack strategy that attempts to differentiate spam from legitimate email in the input space by transforming each email into a bag of multiple segments, and subsequently applying multiple instance logistic regression on the bags. We treat each segment in the bag as an instance. An email is classiﬁed as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. We show that a classiﬁer using our multiple instance counterattack strategy is more robust to good word attacks than its single instance counterpart and other single instance learners commonly used in the spam ﬁltering domain. Keywords: spam ﬁltering, multiple instance learning, good word attack, adversarial learning</p><p>5 <a title="jmlr-2008-5" href="../jmlr2008/jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>Author: Arnak S. Dalalyan, Anatoly Juditsky, Vladimir Spokoiny</p><p>Abstract: The statistical problem of estimating the effective dimension-reduction (EDR) subspace in the multi-index regression model with deterministic design and additive noise is considered. A new procedure for recovering the directions of the EDR subspace is proposed. Many methods for estimating the EDR subspace perform principal component analysis on a family of vectors, say ˆ ˆ β1 , . . . , βL , nearly lying in the EDR subspace. This is in particular the case for the structure-adaptive approach proposed by Hristache et al. (2001a). In the present work, we propose to estimate the projector onto the EDR subspace by the solution to the optimization problem minimize ˆ ˆ max β (I − A)β =1,...,L subject to A ∈ Am∗ , where Am∗ is the set of all symmetric matrices with eigenvalues in [0, 1] and trace less than or equal √ to m∗ , with m∗ being the true structural dimension. Under mild assumptions, n-consistency of the proposed procedure is proved (up to a logarithmic factor) in the case when the structural dimension is not larger than 4. Moreover, the stochastic error of the estimator of the projector onto the EDR subspace is shown to depend on L logarithmically. This enables us to use a large number of vectors ˆ β for estimating the EDR subspace. The empirical behavior of the algorithm is studied through numerical simulations. Keywords: dimension-reduction, multi-index regression model, structure-adaptive approach, central subspace</p><p>6 <a title="jmlr-2008-6" href="../jmlr2008/jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>Author: Xianchao Xie, Zhi Geng</p><p>Abstract: In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is ﬁrst decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efﬁciency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method. Keywords: Bayesian network, conditional independence, decomposition, directed acyclic graph, structural learning</p><p>7 <a title="jmlr-2008-7" href="../jmlr2008/jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>Author: Glenn Shafer, Vladimir Vovk</p><p>Abstract: Conformal prediction uses past experience to determine precise levels of conÄ?Ĺš dence in new predictions. Given an error probability ĂŽÄž, together with a method that makes a prediction y of a label Ă&lsaquo;&dagger; y, it produces a set of labels, typically containing y, that also contains y with probability 1 Ă˘&circ;&rsquo; ĂŽÄž. Ă&lsaquo;&dagger; Conformal prediction can be applied to any method for producing y: a nearest-neighbor method, a Ă&lsaquo;&dagger; support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 Ă˘&circ;&rsquo; ĂŽÄž of the time, even though they are based on an accumulating data set rather than on independent data sets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005). Keywords: conÄ?Ĺš dence, on-line compression modeling, on-line learning, prediction regions</p><p>8 <a title="jmlr-2008-8" href="../jmlr2008/jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>Author: Faustino Gomez, Jürgen Schmidhuber, Risto Miikkulainen</p><p>Abstract: Many complex control problems require sophisticated solutions that are not amenable to traditional controller design. Not only is it difﬁcult to model real world systems, but often it is unclear what kind of behavior is required to solve the task. Reinforcement learning (RL) approaches have made progress by using direct interaction with the task environment, but have so far not scaled well to large state spaces and environments that are not fully observable. In recent years, neuroevolution, the artiﬁcial evolution of neural networks, has had remarkable success in tasks that exhibit these two properties. In this paper, we compare a neuroevolution method called Cooperative Synapse Neuroevolution (CoSyNE), that uses cooperative coevolution at the level of individual synaptic weights, to a broad range of reinforcement learning algorithms on very difﬁcult versions of the pole balancing problem that involve large (continuous) state spaces and hidden state. CoSyNE is shown to be signiﬁcantly more efﬁcient and powerful than the other methods on these tasks. Keywords: coevolution, recurrent neural networks, non-linear control, genetic algorithms, experimental comparison</p><p>9 <a title="jmlr-2008-9" href="../jmlr2008/jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>Author: Falk-Florian Henrich, Klaus Obermayer</p><p>Abstract: We introduce a computationally feasible, “constructive” active learning method for binary classiﬁcation. The learning algorithm is initially formulated for separable classiﬁcation problems, for a hyperspherical data space with constant data density, and for great spheres as classiﬁers. In order to reduce computational complexity the version space is restricted to spherical simplices and learning procedes by subdividing the edges of maximal length. We show that this procedure optimally reduces a tight upper bound on the generalization error. The method is then extended to other separable classiﬁcation problems using products of spheres as data spaces and isometries induced by charts of the sphere. An upper bound is provided for the probability of disagreement between classiﬁers (hence the generalization error) for non-constant data densities on the sphere. The emphasis of this work lies on providing mathematically exact performance estimates for active learning strategies. Keywords: active learning, spherical subdivision, error bounds, simplex halving</p><p>10 <a title="jmlr-2008-10" href="../jmlr2008/jmlr-2008-Active_Learning_of_Causal_Networks_with_Intervention_Experiments_and_Optimal_Designs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">jmlr-2008-Active Learning of Causal Networks with Intervention Experiments and Optimal Designs    (Special Topic on Causality)</a></p>
<p>Author: Yang-Bo He, Zhi Geng</p><p>Abstract: The causal discovery from data is important for various scientiﬁc investigations. Because we cannot distinguish the different directed acyclic graphs (DAGs) in a Markov equivalence class learned from observational data, we have to collect further information on causal structures from experiments with external interventions. In this paper, we propose an active learning approach for discovering causal structures in which we ﬁrst ﬁnd a Markov equivalence class from observational data, and then we orient undirected edges in every chain component via intervention experiments separately. In the experiments, some variables are manipulated through external interventions. We discuss two kinds of intervention experiments, randomized experiment and quasi-experiment. Furthermore, we give two optimal designs of experiments, a batch-intervention design and a sequential-intervention design, to minimize the number of manipulated variables and the set of candidate structures based on the minimax and the maximum entropy criteria. We show theoretically that structural learning can be done locally in subgraphs of chain components without need of checking illegal v-structures and cycles in the whole network and that a Markov equivalence subclass obtained after each intervention can still be depicted as a chain graph. Keywords: active learning, causal networks, directed acyclic graphs, intervention, Markov equivalence class, optimal design, structural learning</p><p>11 <a title="jmlr-2008-11" href="../jmlr2008/jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>Author: Sébastien Loustau</p><p>Abstract: This paper investigates statistical performances of Support Vector Machines (SVM) and considers the problem of adaptation to the margin parameter and to complexity. In particular we provide a classiﬁer with no tuning parameter. It is a combination of SVM classiﬁers. Our contribution is two-fold: (1) we propose learning rates for SVM using Sobolev spaces and build a numerically realizable aggregate that converges with same rate; (2) we present practical experiments of this method of aggregation for SVM using both Sobolev spaces and Gaussian kernels. Keywords: classiﬁcation, support vector machines, learning rates, approximation, aggregation of classiﬁers</p><p>12 <a title="jmlr-2008-12" href="../jmlr2008/jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>13 <a title="jmlr-2008-13" href="../jmlr2008/jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>Author: Eric Bax, Augusto Callejas</p><p>Abstract: This paper introduces a new PAC transductive error bound for classiﬁcation. The method uses information from the training examples and inputs of working examples to develop a set of likely assignments to outputs of the working examples. A likely assignment with maximum error determines the bound. The method is very effective for small data sets. Keywords: error bound, transduction, nearest neighbor, dynamic programming</p><p>14 <a title="jmlr-2008-14" href="../jmlr2008/jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>Author: Salvador García, Francisco Herrera</p><p>Abstract: In a recently published paper in JMLR, Demˇar (2006) recommends a set of non-parametric stas tistical tests and procedures which can be safely used for comparing the performance of classiﬁers over multiple data sets. After studying the paper, we realize that the paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method. However, it does not deal with some advanced topics in depth. Regarding these topics, we focus on more powerful proposals of statistical procedures for comparing n × n classiﬁers. Moreover, we illustrate an easy way of obtaining adjusted and comparable p-values in multiple comparison procedures. Keywords: statistical methods, non-parametric test, multiple comparisons tests, adjusted p-values, logically related hypotheses</p><p>15 <a title="jmlr-2008-15" href="../jmlr2008/jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>Author: Gerda Claeskens, Christophe Croux, Johan Van Kerckhoven</p><p>Abstract: Support vector machines for classiﬁcation have the advantage that the curse of dimensionality is circumvented. It has been shown that a reduction of the dimension of the input space leads to even better results. For this purpose, we propose two information criteria which can be computed directly from the deﬁnition of the support vector machine. We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. We arrive at the same ﬁndings for comparison on some real-world benchmark data sets. Keywords: information criterion, supervised classiﬁcation, support vector machine, variable selection</p><p>16 <a title="jmlr-2008-16" href="../jmlr2008/jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>17 <a title="jmlr-2008-17" href="../jmlr2008/jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><p>18 <a title="jmlr-2008-18" href="../jmlr2008/jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>19 <a title="jmlr-2008-19" href="../jmlr2008/jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>Author: Andreas Christmann, Arnout Van Messem</p><p>Abstract: We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand inﬂuence function (BIF) a modiﬁcation of F.R. Hampel’s inﬂuence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel’s inﬂuence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. Keywords: Bouligand derivatives, empirical risk minimization, inﬂuence function, robustness, support vector machines</p><p>20 <a title="jmlr-2008-20" href="../jmlr2008/jmlr-2008-Causal_Reasoning_with_Ancestral_Graphs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">jmlr-2008-Causal Reasoning with Ancestral Graphs    (Special Topic on Causality)</a></p>
<p>Author: Jiji Zhang</p><p>Abstract: Causal reasoning is primarily concerned with what would happen to a system under external interventions. In particular, we are often interested in predicting the probability distribution of some random variables that would result if some other variables were forced to take certain values. One prominent approach to tackling this problem is based on causal Bayesian networks, using directed acyclic graphs as causal diagrams to relate post-intervention probabilities to pre-intervention probabilities that are estimable from observational data. However, such causal diagrams are seldom fully testable given observational data. In consequence, many causal discovery algorithms based on data-mining can only output an equivalence class of causal diagrams (rather than a single one). This paper is concerned with causal reasoning given an equivalence class of causal diagrams, represented by a (partial) ancestral graph. We present two main results. The ﬁrst result extends Pearl (1995)’s celebrated do-calculus to the context of ancestral graphs. In the second result, we focus on a key component of Pearl’s calculus—the property of invariance under interventions, and give stronger graphical conditions for this property than those implied by the ﬁrst result. The second result also improves the earlier, similar results due to Spirtes et al. (1993). Keywords: ancestral graphs, causal Bayesian network, do-calculus, intervention</p><p>21 <a title="jmlr-2008-21" href="../jmlr2008/jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>22 <a title="jmlr-2008-22" href="../jmlr2008/jmlr-2008-Closed_Sets_for_Labeled_Data.html">jmlr-2008-Closed Sets for Labeled Data</a></p>
<p>23 <a title="jmlr-2008-23" href="../jmlr2008/jmlr-2008-Comments_on_the_Complete_Characterization_of_a_Family_of_Solutions_to_a_GeneralizedFisherCriterion.html">jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</a></p>
<p>24 <a title="jmlr-2008-24" href="../jmlr2008/jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>25 <a title="jmlr-2008-25" href="../jmlr2008/jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>26 <a title="jmlr-2008-26" href="../jmlr2008/jmlr-2008-Consistency_of_Trace_Norm_Minimization.html">jmlr-2008-Consistency of Trace Norm Minimization</a></p>
<p>27 <a title="jmlr-2008-27" href="../jmlr2008/jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>28 <a title="jmlr-2008-28" href="../jmlr2008/jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>29 <a title="jmlr-2008-29" href="../jmlr2008/jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>30 <a title="jmlr-2008-30" href="../jmlr2008/jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>31 <a title="jmlr-2008-31" href="../jmlr2008/jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>32 <a title="jmlr-2008-32" href="../jmlr2008/jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>33 <a title="jmlr-2008-33" href="../jmlr2008/jmlr-2008-Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">jmlr-2008-Evidence Contrary to the Statistical View of Boosting</a></p>
<p>34 <a title="jmlr-2008-34" href="../jmlr2008/jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>35 <a title="jmlr-2008-35" href="../jmlr2008/jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>36 <a title="jmlr-2008-36" href="../jmlr2008/jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>37 <a title="jmlr-2008-37" href="../jmlr2008/jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations.html">jmlr-2008-Forecasting Web Page Views: Methods and Observations</a></p>
<p>38 <a title="jmlr-2008-38" href="../jmlr2008/jmlr-2008-Generalization_from_Observed_to_Unobserved_Features_by_Clustering.html">jmlr-2008-Generalization from Observed to Unobserved Features by Clustering</a></p>
<p>39 <a title="jmlr-2008-39" href="../jmlr2008/jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>40 <a title="jmlr-2008-40" href="../jmlr2008/jmlr-2008-Graphical_Methods_for_Efficient_Likelihood_Inference_in_Gaussian_Covariance_Models.html">jmlr-2008-Graphical Methods for Efficient Likelihood Inference in Gaussian Covariance Models</a></p>
<p>41 <a title="jmlr-2008-41" href="../jmlr2008/jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>42 <a title="jmlr-2008-42" href="../jmlr2008/jmlr-2008-HPB%3A_A_Model_for_Handling_BN_Nodes_with_High_Cardinality_Parents.html">jmlr-2008-HPB: A Model for Handling BN Nodes with High Cardinality Parents</a></p>
<p>43 <a title="jmlr-2008-43" href="../jmlr2008/jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>44 <a title="jmlr-2008-44" href="../jmlr2008/jmlr-2008-Incremental_Identification_of_Qualitative_Models_of_Biological_Systems_using_Inductive_Logic_Programming.html">jmlr-2008-Incremental Identification of Qualitative Models of Biological Systems using Inductive Logic Programming</a></p>
<p>45 <a title="jmlr-2008-45" href="../jmlr2008/jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>46 <a title="jmlr-2008-46" href="../jmlr2008/jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>47 <a title="jmlr-2008-47" href="../jmlr2008/jmlr-2008-Learning_Balls_of_Strings_from_Edit_Corrections.html">jmlr-2008-Learning Balls of Strings from Edit Corrections</a></p>
<p>48 <a title="jmlr-2008-48" href="../jmlr2008/jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>49 <a title="jmlr-2008-49" href="../jmlr2008/jmlr-2008-Learning_Control_Knowledge_for_Forward_Search_Planning.html">jmlr-2008-Learning Control Knowledge for Forward Search Planning</a></p>
<p>50 <a title="jmlr-2008-50" href="../jmlr2008/jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>51 <a title="jmlr-2008-51" href="../jmlr2008/jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>52 <a title="jmlr-2008-52" href="../jmlr2008/jmlr-2008-Learning_from_Multiple_Sources.html">jmlr-2008-Learning from Multiple Sources</a></p>
<p>53 <a title="jmlr-2008-53" href="../jmlr2008/jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>54 <a title="jmlr-2008-54" href="../jmlr2008/jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>55 <a title="jmlr-2008-55" href="../jmlr2008/jmlr-2008-Linear-Time_Computation_of_Similarity_Measures_for_Sequential_Data.html">jmlr-2008-Linear-Time Computation of Similarity Measures for Sequential Data</a></p>
<p>56 <a title="jmlr-2008-56" href="../jmlr2008/jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>57 <a title="jmlr-2008-57" href="../jmlr2008/jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>58 <a title="jmlr-2008-58" href="../jmlr2008/jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>59 <a title="jmlr-2008-59" href="../jmlr2008/jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>60 <a title="jmlr-2008-60" href="../jmlr2008/jmlr-2008-Minimal_Nonlinear_Distortion_Principle_for_Nonlinear_Independent_Component_Analysis.html">jmlr-2008-Minimal Nonlinear Distortion Principle for Nonlinear Independent Component Analysis</a></p>
<p>61 <a title="jmlr-2008-61" href="../jmlr2008/jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>62 <a title="jmlr-2008-62" href="../jmlr2008/jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>63 <a title="jmlr-2008-63" href="../jmlr2008/jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>64 <a title="jmlr-2008-64" href="../jmlr2008/jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>65 <a title="jmlr-2008-65" href="../jmlr2008/jmlr-2008-Multi-Agent_Reinforcement_Learning_in_Common_Interest_and_Fixed_Sum_Stochastic_Games%3A_An_Experimental_Study.html">jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</a></p>
<p>66 <a title="jmlr-2008-66" href="../jmlr2008/jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>67 <a title="jmlr-2008-67" href="../jmlr2008/jmlr-2008-Near-Optimal_Sensor_Placements_in_Gaussian_Processes%3A_Theory%2C_Efficient_Algorithms_and_Empirical_Studies.html">jmlr-2008-Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies</a></p>
<p>68 <a title="jmlr-2008-68" href="../jmlr2008/jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>69 <a title="jmlr-2008-69" href="../jmlr2008/jmlr-2008-Non-Parametric_Modeling_of_Partially_Ranked_Data.html">jmlr-2008-Non-Parametric Modeling of Partially Ranked Data</a></p>
<p>70 <a title="jmlr-2008-70" href="../jmlr2008/jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>71 <a title="jmlr-2008-71" href="../jmlr2008/jmlr-2008-On_the_Equivalence_of_Linear_Dimensionality-Reducing_Transformations.html">jmlr-2008-On the Equivalence of Linear Dimensionality-Reducing Transformations</a></p>
<p>72 <a title="jmlr-2008-72" href="../jmlr2008/jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>73 <a title="jmlr-2008-73" href="../jmlr2008/jmlr-2008-On_the_Suitable_Domain_for_SVM_Training_in_Image_Coding.html">jmlr-2008-On the Suitable Domain for SVM Training in Image Coding</a></p>
<p>74 <a title="jmlr-2008-74" href="../jmlr2008/jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>75 <a title="jmlr-2008-75" href="../jmlr2008/jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>76 <a title="jmlr-2008-76" href="../jmlr2008/jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>77 <a title="jmlr-2008-77" href="../jmlr2008/jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>78 <a title="jmlr-2008-78" href="../jmlr2008/jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>79 <a title="jmlr-2008-79" href="../jmlr2008/jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>80 <a title="jmlr-2008-80" href="../jmlr2008/jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>81 <a title="jmlr-2008-81" href="../jmlr2008/jmlr-2008-Regularization_on_Graphs_with_Function-adapted_Diffusion_Processes.html">jmlr-2008-Regularization on Graphs with Function-adapted Diffusion Processes</a></p>
<p>82 <a title="jmlr-2008-82" href="../jmlr2008/jmlr-2008-Responses_to_Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">jmlr-2008-Responses to Evidence Contrary to the Statistical View of Boosting</a></p>
<p>83 <a title="jmlr-2008-83" href="../jmlr2008/jmlr-2008-Robust_Submodular_Observation_Selection.html">jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>84 <a title="jmlr-2008-84" href="../jmlr2008/jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>85 <a title="jmlr-2008-85" href="../jmlr2008/jmlr-2008-Shark%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">jmlr-2008-Shark    (Machine Learning Open Source Software Paper)</a></p>
<p>86 <a title="jmlr-2008-86" href="../jmlr2008/jmlr-2008-SimpleMKL.html">jmlr-2008-SimpleMKL</a></p>
<p>87 <a title="jmlr-2008-87" href="../jmlr2008/jmlr-2008-Stationary_Features_and_Cat_Detection.html">jmlr-2008-Stationary Features and Cat Detection</a></p>
<p>88 <a title="jmlr-2008-88" href="../jmlr2008/jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>89 <a title="jmlr-2008-89" href="../jmlr2008/jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>90 <a title="jmlr-2008-90" href="../jmlr2008/jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>91 <a title="jmlr-2008-91" href="../jmlr2008/jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>92 <a title="jmlr-2008-92" href="../jmlr2008/jmlr-2008-Universal_Multi-Task_Kernels.html">jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>93 <a title="jmlr-2008-93" href="../jmlr2008/jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>94 <a title="jmlr-2008-94" href="../jmlr2008/jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>95 <a title="jmlr-2008-95" href="../jmlr2008/jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>96 <a title="jmlr-2008-96" href="../jmlr2008/jmlr-2008-Visualizing_Data_using_t-SNE.html">jmlr-2008-Visualizing Data using t-SNE</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
