<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-41" href="#">jmlr2008-41</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</h1>
<br/><p>Source: <a title="jmlr-2008-41-pdf" href="http://jmlr.org/papers/volume9/chen08a/chen08a.pdf">pdf</a></p><p>Author: Shann-Ching Chen, Geoffrey J. Gordon, Robert F. Murphy</p><p>Abstract: In structured classiﬁcation problems, there is a direct conﬂict between expressive models and efﬁcient inference: while graphical models such as Markov random ﬁelds or factor graphs can represent arbitrary dependences among instance labels, the cost of inference via belief propagation in these models grows rapidly as the graph structure becomes more complicated. One important source of complexity in belief propagation is the need to marginalize large factors to compute messages. This operation takes time exponential in the number of variables in the factor, and can limit the expressiveness of the models we can use. In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efﬁcient algorithms for computing messages from these potentials during belief propagation. We believe these new potentials provide a good balance between expressive power and efﬁcient inference in practical structured classiﬁcation problems. We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. We use these potentials to classify images of protein subcellular location patterns in groups of cells. Classifying subcellular location patterns can help us answer many important questions in computational biology, including questions about how various treatments affect the synthesis and behavior of proteins and networks of proteins within a cell. Our new representation and algorithm lead to substantial improvements in both inference speed and classiﬁcation accuracy. Keywords: factor graphs, approximate inference algorithms, structured classiﬁcation, protein subcellular location patterns, location proteomics</p><p>Reference: <a title="jmlr-2008-41-reference" href="../jmlr2008_reference/jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efﬁcient algorithms for computing messages from these potentials during belief propagation. [sent-11, score-1.049]
</p><p>2 We believe these new potentials provide a good balance between expressive power and efﬁcient inference in practical structured classiﬁcation problems. [sent-12, score-0.59]
</p><p>3 We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. [sent-13, score-0.798]
</p><p>4 We use these potentials to classify images of protein subcellular location patterns in groups of cells. [sent-14, score-0.775]
</p><p>5 To speed up inference, we can move to approximate algorithms such as loopy belief propagation (see, e. [sent-30, score-0.584]
</p><p>6 Computing messages for these potentials is much more efﬁcient than for general potentials, even though the new potentials can express distributions that cannot be represented by groups of smaller potentials. [sent-36, score-1.125]
</p><p>7 Accordingly, we believe these new potentials provide a better balance between expressive power and efﬁcient inference than was previously available. [sent-37, score-0.566]
</p><p>8 We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. [sent-38, score-0.798]
</p><p>9 We use these potentials to classify images of protein subcellular location patterns in groups of cells. [sent-39, score-0.775]
</p><p>10 These papers describe applications of decomposable potentials and the corresponding fast inference algorithms for segmenting and classifying images of protein subcellular location patterns. [sent-44, score-0.973]
</p><p>11 But, none of these papers describe the idea of decomposable potentials of Section 4 or the nested inference algorithm of Section 5 in full generality. [sent-45, score-0.761]
</p><p>12 They also do not cover some of the instances of decomposable potentials described in Section 6; and, the experiments of Sections 8–9 have not been reported previously. [sent-46, score-0.583]
</p><p>13 , 2001) describes the relationships among a set of variables xi using local factors or potentials ϕ j . [sent-49, score-0.553]
</p><p>14 The advantage of collapsing variable nodes is that it can allow us to simplify the structure of our graph: if in the collapsed graph there are two factor nodes with the same set of arguments, then we can combine them by multiplying their potentials elementwise. [sent-114, score-0.839]
</p><p>15 The potentials ϕ23 and ϕ123 from the original graph have the same set of neighbors in the new graph, and so can be combined into one factor node. [sent-118, score-0.719]
</p><p>16 Similarly, the local potentials ϕloc and ϕloc can be combined with the factor ϕ12 to form 1 2 a new local potential at the collapsed node x12 . [sent-119, score-0.763]
</p><p>17 x2 x1  x1  x5 x3  x23  x24  x5  x4  Figure 3: A factor graph with multiple loops (left) and a tree derived from the factor graph by collapsing pairs of nodes (right). [sent-127, score-0.587]
</p><p>18 A junction tree is a connected acyclic graph whose nodes are labeled with sets of variables in a way that satisﬁes the running intersection property: if a variable is present at two nodes A and B, it is also present at all nodes along the (unique) path connecting A and B in the tree. [sent-136, score-0.546]
</p><p>19 The factor nodes in a treeshaped factor graph correspond to nodes of the junction tree, with labels equal to their argument sets. [sent-137, score-0.597]
</p><p>20 For either exact or loopy BP, the runtime for each pass over the factor graph is exponential in the number of distinct original variables included in the largest factor. [sent-157, score-0.508]
</p><p>21 Factor Graphs and Structured Classiﬁcation  To use belief propagation to solve structured classiﬁcation problems, we need two things: a local classiﬁer for individual instances, and a factor graph which encodes our prior beliefs about likely arrangements of instance labels. [sent-160, score-0.527]
</p><p>22 Then, we use this similarity graph to decide what potentials to add to our factor graph. [sent-165, score-0.705]
</p><p>23 ) Given the similarity graph, we compare factor graphs built from several different types of potentials; the following sections introduce these potentials and discuss their advantages and disadvantages. [sent-167, score-0.701]
</p><p>24 Our experimental results, below, will show that potentials that combine evidence additively can perform better than the Potts potential over a wider range of inference problems. [sent-192, score-0.717]
</p><p>25 In the factor graphs for our cell classiﬁcation experiments below, we use one voting potential per cell j; the center for the jth potential is cell j itself, and the voters are the cells that are adjacent to j in the similarity graph. [sent-197, score-0.966]
</p><p>26 The voting potential function combines the evidence from all of node x j ’s neighbors into a summary vote which then inﬂuences x’s classiﬁcation. [sent-205, score-0.557]
</p><p>27 The behavior of the voting potential contrasts with that of the Potts potential described in Section 3. [sent-209, score-0.502]
</p><p>28 So, we cannot expect to ﬁnd inference algorithms for general k-way potentials that take less than exponential time. [sent-234, score-0.566]
</p><p>29 Decomposable potentials include the special cases mentioned above, namely sparse potentials and potentials that are sums of low-arity terms, as well as a wide variety of other examples which we will describe in more detail below. [sent-288, score-1.383]
</p><p>30 Belief Propagation with Decomposable Potentials When we are running BP or loopy BP on a factor graph with decomposable potentials, we can accelerate the computation of the belief messages that would otherwise be slow to compute. [sent-291, score-0.951]
</p><p>31 In this section we will show that we can calculate these messages efﬁciently for decomposable potentials of the form shown in Equation 10. [sent-296, score-0.76]
</p><p>32 So, when we perform inference on a factor graph with decomposable potentials, we will be running two nested copies of belief propagation: the inner copy will act on a single decomposable potential, and will compute the messages which the outer copy needs to send from that potential. [sent-302, score-0.997]
</p><p>33 Pick a node of this junction tree whose label contains xk , and call this node the root. [sent-324, score-0.596]
</p><p>34 Because we have already computed the message from node 1 to node 4, we can now compute the message from 4 to par(4). [sent-347, score-0.578]
</p><p>35 It should be clear at this point that we can compute a message from each node in the junction tree to its parent, so long as we work from the leaves upward; the message from a node m to its parent par(m) will depend on the messages from m’s children to m. [sent-359, score-1.007]
</p><p>36 The above algorithm computes the message from a factor ϕ j to a single neighboring variable xk . [sent-364, score-0.551]
</p><p>37 Instead, as is usual for the belief propagation algorithm, we can combine all of the runs into a single computation which passes one message in each direction over each edge of the junction tree. [sent-366, score-0.738]
</p><p>38 First, if the argument sets of the incoming messages at a given factor node overlap, we may need to build different inner junction trees to compute different outgoing messages. [sent-368, score-0.573]
</p><p>39 Second, and more importantly, if the desired outer message depends on several original variables, then there may be no node of our inner junction tree that contains all the needed variables. [sent-371, score-0.57]
</p><p>40 In this section, we discuss the details and derivations of message passing with the voting potential, the associative Markov network potential, and the nested junction tree. [sent-379, score-0.807]
</p><p>41 There are two types of messages we need to think about: those from a factor ϕ j to the node xk that ϕ j is centered on, and those from a factor ϕ j to some non-centered variable node xi with i = k. [sent-383, score-0.748]
</p><p>42 With these assumptions, the message from a factor ϕ j to its center variable node xk can be computed as follows: (λ + |N( j)|) m j→k (xk ) =  ∑∑. [sent-388, score-0.591]
</p><p>43 ∑ I(xi , xk )mi→ j (xi ) ∏ mi → j (xi ) n i=1 x1 x2 xk−1 i =1,i =i  =  λ k−1 + ∑ I(xi , xk )mi→ j (xi ) n i=1 ∑ xi  =  λ k−1 + ∑ mi→ j (xk ). [sent-403, score-0.583]
</p><p>44 ∑ x2  xk  k−1  λ/n + ∑ I(xi , xk ) i=1  k  ∏ mi → j (xi )  i =2  =  k−1 k λ + ∑ . [sent-414, score-0.532]
</p><p>45 ∑ ∑ I(xi , xk ) ∏ mi → j (xi ) n x2 xk i=1 i =2  =  k−1 k k λ + ∑ . [sent-417, score-0.532]
</p><p>46 ∑ ∑ I(xi , xk ) ∏ mi → j (xi ) n x2 x2 xk i=2 xk i =2 i =2  =  k−1 k λ + ∑ I(x1 , xk )mk→ j (xk ) + ∑ . [sent-423, score-0.96]
</p><p>47 ∑ ∑ I(xi , xk ) ∏ mi → j (xi ) n xk x2 xk i=2 i =2  =  k−1 k λ + mk→ j (x1 ) + ∑ . [sent-426, score-0.746]
</p><p>48 ∑ ∑ I(xi , xk ) ∏ mi → j (xi ) n x2 xk i=2 i =2  663  C HEN , G ORDON AND M URPHY  =  k−1 k−1 λ + mk→ j (x1 ) + ∑ ∑ . [sent-429, score-0.532]
</p><p>49 ∑ I(xi , xk )mi→ j (xi )mk→ j (xk ) ∏ mi → j (xi ) n xk i=2 x2 i =2,i =i  =  k−1 λ + mk→ j (x1 ) + ∑ ∑ ∑ I(xi , xk )mi→ j (xi )mk→ j (xk ) n i=2 xi xk  =  k−1 λ + mk→ j (x1 ) + ∑ ∑ mi→ j (xk )mk→ j (xk ). [sent-432, score-1.011]
</p><p>50 2 Decomposing the AMN Potential It is even easier to derive an efﬁcient expression for the messages from an AMN potential than it was for the messages from a voting potential. [sent-450, score-0.734]
</p><p>51 3 The Nested Junction Tree The nested junction tree method (Kjærulff, 1998) can speed up message propagation in the junction trees that arise when we remove loops from a factor graph. [sent-485, score-0.987]
</p><p>52 Unlike the previous two examples (the voting and AMN potentials), the nested junction tree method does not attempt to look inside the factors of the original factor graph. [sent-488, score-0.712]
</p><p>53 The time and space costs of belief propagation on this junction tree are dominated by its largest potential, ϕ2345 . [sent-492, score-0.553]
</p><p>54 665  C HEN , G ORDON AND M URPHY  x2  x4  x1  x4 x1  x3  x5  x23  245 45  x5  345  Figure 4: A factor graph for which the nested junction tree method can speed up belief propagation. [sent-507, score-0.694]
</p><p>55 Right: inner junction tree for computing the message m2345→23 . [sent-510, score-0.505]
</p><p>56 This inner message passing algorithm works on an inner junction tree T built from the remaining edges of C. [sent-529, score-0.595]
</p><p>57 Recapping, we have 2|M\C | inner runs of message passing, each of which works with a junction tree containing a clique of size |C \ M| + |M ∩ C |. [sent-539, score-0.543]
</p><p>58 There are two ways it can become inapplicable: / ﬁrst, if M \ C = 0, then the argument above means that the nested junction tree has only a single clique of size |C|, and the nested junction tree method therefore offers no speedup. [sent-546, score-0.688]
</p><p>59 In this case, C gets incoming messages from all of its neighbors, and the nested junction tree method again offers no speedup: to perform any inference task on C’s fully-connected graph, we must again build a table of size 2|C| and take time at least k2|C| for some k > 0. [sent-551, score-0.634]
</p><p>60 In contrast, the examples of the previous two sections show that decomposable potentials in general can lead to far greater savings: in these examples, the message calculation time goes from exponential to polynomial in the size of the clique, and Fig. [sent-557, score-0.807]
</p><p>61 1, we can run loopy belief propagation on a factor graph that includes voting potentials. [sent-561, score-1.044]
</p><p>62 However, we might expect the messages from a factor ϕ j to a non-centered variable xi (where i = c j ) to be fairly weak: the overall vote of all of xc j ’s neighbors will not be inﬂuenced very much by xi ’s single vote, so there will not be a strong penalty if xi votes the wrong way. [sent-562, score-0.561]
</p><p>63 This observation suggests an even simpler algorithm for inference: we can run loopy BP but ignore all of the messages from factors to non-centered variables. [sent-563, score-0.501]
</p><p>64 Our group proposed a version of Prior Updating for inference on graphs (Chen and Murphy, 2006) before the work described in the current paper, which explores its relationship to potential functions in loopy belief propagation. [sent-566, score-0.787]
</p><p>65 We might also hope that PU could be more accurate than loopy BP, since it is less prone to double-count evidence: we have broken any loops which would allow a message that x c j sends to factor ϕ j to return back and inﬂuence the classiﬁcation of xc j . [sent-568, score-0.662]
</p><p>66 (As it turns out, we will see below that in our experiments PU is often slightly worse and sometimes slightly better than loopy BP on factor graphs with voting potentials. [sent-569, score-0.739]
</p><p>67 With a fully-connected similarity graph, all cells in the test image are considered equally similar to one another; such a graph expresses a prior belief that images containing a few groups of same-class cells are more likely than images containing cells of many different classes. [sent-603, score-0.658]
</p><p>68 But, these examples can be classiﬁed correctly by constructing a similarity graph and running belief propagation on the corresponding factor graph. [sent-610, score-0.545]
</p><p>69 In this graph there was one voting potential centered on each node; this potential covered the node and all of its similarity-graph neighbors, and had parameter λ = 1. [sent-641, score-0.681]
</p><p>70 • Of the k-way potentials that we tested, the voting potential is the best for a range of problem types. [sent-668, score-0.841]
</p><p>71 We used 50% graph edge density for the Potts and voting potentials and 40% graph edge density for the AMN potential, since these densities were approximately optimal according to our preliminary experiments. [sent-675, score-0.999]
</p><p>72 prior updating on the model with voting potentials (PU). [sent-683, score-0.74]
</p><p>73 6 illustrates, the methods using the voting potential (EIVP, LBVP, and PU) signiﬁcantly outperformed the other potentials when the graph has an equal number of nodes from each class. [sent-687, score-1.015]
</p><p>74 6%: loopy belief propagation on the model with voting potentials (LBVP, − −), prior updating on the model with voting potentials (PU, · · ·), loopy belief propagation on the model with AMN potentials (LBAMN, −−) and loopy belief propagation on the Potts model (LBP, − −). [sent-695, score-3.672]
</p><p>75 1%: loopy belief propagation with voting potentials (LBVP, − −), prior updating with voting potentials (PU, · · ·), loopy belief propagation with AMN potentials (LBAMN, − −) and loopy belief propagation on the Potts model (LBP, − −). [sent-716, score-3.672]
</p><p>76 ) On graphs with unequal class sizes, voting and Potts potentials achieved statistically comparable results when the graph complexity was tuned optimally. [sent-729, score-0.965]
</p><p>77 6%: loopy belief propagation on the model with voting potentials (LBVP, − −), prior updating on the model with voting potentials (PU, · · ·), loopy belief propagation on the model with AMN potentials (LBAMN, −−) and loopy belief propagation on the Potts model (LBP, − −). [sent-745, score-3.672]
</p><p>78 Related Work The problem of how to quickly compute belief messages (or other similar quantities in an inference algorithm) is an important one, and several special cases of our decomposable structure have been previously described in the literature. [sent-757, score-0.571]
</p><p>79 Since DMC potentials are decomposable, our message calculation algorithm allows us to run belief propagation quickly; doing so is essentially equivalent to Sidiqqi and Moore’s implementation of the forward-backward algorithm. [sent-771, score-0.986]
</p><p>80 Another interesting direction for future work would be to extend their parameter-learning algorithm to handle more general structures like the decomposable potentials studied here. [sent-777, score-0.583]
</p><p>81 In the context of a computer vision application, Felzenszwalb and Huttenlocher (2004) describe how to run loopy belief propagation quickly for a number of different pairwise potential functions. [sent-778, score-0.706]
</p><p>82 They also consider pairwise potentials based on distance functions, which do not in general appear to be examples of our class of decomposable potential functions. [sent-780, score-0.705]
</p><p>83 In addition to the potentials studied in this paper, multiple examples of speciﬁc decomposable potentials exist in the literature. [sent-782, score-1.044]
</p><p>84 All of these examples share a certain similarity to several of the special cases of decomposable potentials mentioned above, in that the speedup comes from accelerating the calculation of a single large sum within the message computation. [sent-788, score-0.849]
</p><p>85 So, a potential with 4 arguments, ϕ(x1 , x2 , x3 , x4 ) = f (x1 + x2 + x3 + x4 ), can be replaced by three smaller potentials and two new variables: I(y1 = x1 + x2 ) I(y2 = x3 + x4 ) f (y1 + y2 ). [sent-790, score-0.583]
</p><p>86 Similarly, a potential that depends on a sum of n terms can be replaced by n − 1 three-argument potentials and n − 2 additional variables. [sent-791, score-0.583]
</p><p>87 While this method works well for potentials that are functions of sums, it is not straightforward to extend it to more complicated potentials of the form we consider in this paper. [sent-794, score-0.922]
</p><p>88 This class of potentials is different from ours; perhaps an even larger class of potentials could be handled by combining the two techniques, arriving at a class of potentials that looked like ϕ j (x1 , . [sent-798, score-1.383]
</p><p>89 ) It does not appear to be easily possible to extend this group of algorithms to k-way potentials such as the voting potential. [sent-808, score-0.719]
</p><p>90 We have compared several different graphical models and inference algorithms designed to solve this sort of structured classiﬁcation problem, including one based on the novel voting potential function. [sent-816, score-0.509]
</p><p>91 Our experiments show that the voting potential often does a better job of encoding this intuition than the Potts and AMN potentials do. [sent-818, score-0.841]
</p><p>92 In addition to the new potential, we have presented new approximate inference algorithms: ﬁrst, we have shown how to implement loopy belief propagation efﬁciently for networks with decomposable potentials, including the AMN and voting potentials. [sent-819, score-1.069]
</p><p>93 And second, we have suggested ignoring certain belief messages during LBP for the voting potential, resulting in an algorithm called Prior Updating. [sent-820, score-0.602]
</p><p>94 These fast algorithms enable us to use potentials like the voting potential on real data, where they would otherwise be impractical. [sent-821, score-0.841]
</p><p>95 We believe that the voting potential function and the class of decomposable potentials will generalize to other graphs and other applications: for example, in image segmentation, it is common to use a potential which encourages nearby pixels to be part of the same object. [sent-822, score-1.195]
</p><p>96 With a potential similar to the voting potential described here, we could allow many neighboring pixels to vote on which object a particular pixel belongs to; and in fact, we have conducted initial experiments in this direction (Chen et al. [sent-823, score-0.554]
</p><p>97 In addition to the speciﬁc potentials we propose, our algorithms for calculating belief messages efﬁciently will also generalize to other domains. [sent-830, score-0.805]
</p><p>98 Whenever a graph contains a factor with a decomposable potential function, we can greatly reduce the time required to calculate belief messages from that factor. [sent-831, score-0.79]
</p><p>99 In such an algorithm, an inner loop of message passing would approximate the belief messages needed by the outer loop. [sent-834, score-0.626]
</p><p>100 On the uniqueness of loopy belief propagation ﬁxed points. [sent-919, score-0.584]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('potentials', 0.461), ('loopy', 0.283), ('voting', 0.258), ('message', 0.224), ('xk', 0.214), ('potts', 0.196), ('amn', 0.191), ('junction', 0.187), ('messages', 0.177), ('belief', 0.167), ('propagation', 0.134), ('subcellular', 0.129), ('potential', 0.122), ('lbp', 0.122), ('decomposable', 0.122), ('graph', 0.114), ('graphs', 0.11), ('inference', 0.105), ('mi', 0.104), ('ordon', 0.103), ('urphy', 0.103), ('ocation', 0.096), ('ubcellular', 0.096), ('factor', 0.088), ('pu', 0.087), ('hen', 0.087), ('lbvp', 0.084), ('bp', 0.083), ('jl', 0.081), ('jlm', 0.077), ('nested', 0.073), ('tructured', 0.073), ('murphy', 0.071), ('cells', 0.071), ('par', 0.07), ('node', 0.065), ('tree', 0.065), ('hela', 0.064), ('nodes', 0.06), ('lassification', 0.059), ('xv', 0.058), ('eivp', 0.058), ('neighbors', 0.056), ('lbamn', 0.051), ('cell', 0.051), ('protein', 0.051), ('xi', 0.051), ('images', 0.048), ('mk', 0.046), ('microscope', 0.045), ('chen', 0.042), ('similarity', 0.042), ('base', 0.041), ('factors', 0.041), ('ops', 0.039), ('clique', 0.038), ('xc', 0.038), ('location', 0.036), ('associative', 0.036), ('accuracy', 0.035), ('proximity', 0.035), ('dcutoff', 0.032), ('dmc', 0.032), ('loc', 0.032), ('uorescence', 0.032), ('edges', 0.032), ('accuracies', 0.031), ('xt', 0.029), ('inner', 0.029), ('passing', 0.029), ('loops', 0.029), ('slice', 0.029), ('collapsing', 0.029), ('evidence', 0.029), ('synthetic', 0.029), ('vote', 0.027), ('summations', 0.027), ('incoming', 0.027), ('collapsed', 0.027), ('classi', 0.026), ('boland', 0.026), ('eiamn', 0.026), ('eipp', 0.026), ('lbvpnc', 0.026), ('groups', 0.026), ('edge', 0.026), ('neighboring', 0.025), ('automated', 0.024), ('marginalize', 0.024), ('patterns', 0.024), ('equation', 0.024), ('structured', 0.024), ('runtime', 0.023), ('unequal', 0.022), ('gordon', 0.022), ('votes', 0.022), ('transition', 0.022), ('improvement', 0.022), ('updating', 0.021), ('classifying', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="41-tfidf-1" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>Author: Shann-Ching Chen, Geoffrey J. Gordon, Robert F. Murphy</p><p>Abstract: In structured classiﬁcation problems, there is a direct conﬂict between expressive models and efﬁcient inference: while graphical models such as Markov random ﬁelds or factor graphs can represent arbitrary dependences among instance labels, the cost of inference via belief propagation in these models grows rapidly as the graph structure becomes more complicated. One important source of complexity in belief propagation is the need to marginalize large factors to compute messages. This operation takes time exponential in the number of variables in the factor, and can limit the expressiveness of the models we can use. In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efﬁcient algorithms for computing messages from these potentials during belief propagation. We believe these new potentials provide a good balance between expressive power and efﬁcient inference in practical structured classiﬁcation problems. We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. We use these potentials to classify images of protein subcellular location patterns in groups of cells. Classifying subcellular location patterns can help us answer many important questions in computational biology, including questions about how various treatments affect the synthesis and behavior of proteins and networks of proteins within a cell. Our new representation and algorithm lead to substantial improvements in both inference speed and classiﬁcation accuracy. Keywords: factor graphs, approximate inference algorithms, structured classiﬁcation, protein subcellular location patterns, location proteomics</p><p>2 0.094643518 <a title="41-tfidf-2" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>Author: Zongming Ma, Xianchao Xie, Zhi Geng</p><p>Abstract: Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse. Keywords: chain graph, conditional independence, decomposition, graphical model, structural learning</p><p>3 0.074056409 <a title="41-tfidf-3" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>Author: Zach Jorgensen, Yan Zhou, Meador Inge</p><p>Abstract: Statistical spam ﬁlters are known to be vulnerable to adversarial attacks. One of the more common adversarial attacks, known as the good word attack, thwarts spam ﬁlters by appending to spam messages sets of “good” words, which are words that are common in legitimate email but rare in spam. We present a counterattack strategy that attempts to differentiate spam from legitimate email in the input space by transforming each email into a bag of multiple segments, and subsequently applying multiple instance logistic regression on the bags. We treat each segment in the bag as an instance. An email is classiﬁed as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. We show that a classiﬁer using our multiple instance counterattack strategy is more robust to good word attacks than its single instance counterpart and other single instance learners commonly used in the spam ﬁltering domain. Keywords: spam ﬁltering, multiple instance learning, good word attack, adversarial learning</p><p>4 0.065891437 <a title="41-tfidf-4" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>Author: Eric Perrier, Seiya Imoto, Satoru Miyano</p><p>Abstract: Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. However, a recent empirical study has shown that a hybrid algorithm improves sensitively accuracy and speed: it learns a skeleton with an independency test (IT) approach and constrains on the directed acyclic graphs (DAG) considered during the search-and-score phase. Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(γm n ), where γm < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree m and sparse structures ˜ allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even ﬁnds more accurate results when given a sound super-structure. Practically, S can be approximated by IT approaches; signiﬁcance level of the tests controls its sparseness, enabling to control the trade-off between speed and accuracy. For incomplete super-structures, a greedily post-processed version (COS+) still enables to signiﬁcantly outperform other heuristic searches. Keywords: subset Bayesian networks, structure learning, optimal search, super-structure, connected</p><p>5 0.061854716 <a title="41-tfidf-5" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>Author: Xianchao Xie, Zhi Geng</p><p>Abstract: In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is ﬁrst decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efﬁciency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method. Keywords: Bayesian network, conditional independence, decomposition, directed acyclic graph, structural learning</p><p>6 0.05726992 <a title="41-tfidf-6" href="./jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">30 jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>7 0.057106964 <a title="41-tfidf-7" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>8 0.056515411 <a title="41-tfidf-8" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>9 0.054481097 <a title="41-tfidf-9" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>10 0.052390266 <a title="41-tfidf-10" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>11 0.050294053 <a title="41-tfidf-11" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>12 0.048264857 <a title="41-tfidf-12" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>13 0.046321902 <a title="41-tfidf-13" href="./jmlr-2008-Graphical_Methods_for_Efficient_Likelihood_Inference_in_Gaussian_Covariance_Models.html">40 jmlr-2008-Graphical Methods for Efficient Likelihood Inference in Gaussian Covariance Models</a></p>
<p>14 0.046262622 <a title="41-tfidf-14" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>15 0.042225827 <a title="41-tfidf-15" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>16 0.041573316 <a title="41-tfidf-16" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>17 0.041298147 <a title="41-tfidf-17" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>18 0.040851988 <a title="41-tfidf-18" href="./jmlr-2008-Active_Learning_of_Causal_Networks_with_Intervention_Experiments_and_Optimal_Designs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">10 jmlr-2008-Active Learning of Causal Networks with Intervention Experiments and Optimal Designs    (Special Topic on Causality)</a></p>
<p>19 0.040216118 <a title="41-tfidf-19" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>20 0.040117528 <a title="41-tfidf-20" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, 0.095), (2, 0.028), (3, -0.007), (4, -0.088), (5, 0.085), (6, 0.024), (7, 0.034), (8, 0.021), (9, -0.04), (10, -0.115), (11, 0.06), (12, -0.063), (13, 0.147), (14, -0.068), (15, -0.096), (16, 0.013), (17, 0.114), (18, -0.046), (19, -0.051), (20, -0.235), (21, 0.043), (22, 0.142), (23, 0.003), (24, -0.069), (25, -0.128), (26, 0.275), (27, -0.011), (28, -0.018), (29, 0.029), (30, 0.02), (31, -0.056), (32, -0.057), (33, -0.26), (34, 0.158), (35, 0.124), (36, -0.053), (37, -0.175), (38, 0.124), (39, -0.131), (40, 0.124), (41, -0.099), (42, 0.028), (43, -0.025), (44, 0.076), (45, -0.084), (46, -0.087), (47, -0.079), (48, 0.059), (49, 0.15)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95448947 <a title="41-lsi-1" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>Author: Shann-Ching Chen, Geoffrey J. Gordon, Robert F. Murphy</p><p>Abstract: In structured classiﬁcation problems, there is a direct conﬂict between expressive models and efﬁcient inference: while graphical models such as Markov random ﬁelds or factor graphs can represent arbitrary dependences among instance labels, the cost of inference via belief propagation in these models grows rapidly as the graph structure becomes more complicated. One important source of complexity in belief propagation is the need to marginalize large factors to compute messages. This operation takes time exponential in the number of variables in the factor, and can limit the expressiveness of the models we can use. In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efﬁcient algorithms for computing messages from these potentials during belief propagation. We believe these new potentials provide a good balance between expressive power and efﬁcient inference in practical structured classiﬁcation problems. We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. We use these potentials to classify images of protein subcellular location patterns in groups of cells. Classifying subcellular location patterns can help us answer many important questions in computational biology, including questions about how various treatments affect the synthesis and behavior of proteins and networks of proteins within a cell. Our new representation and algorithm lead to substantial improvements in both inference speed and classiﬁcation accuracy. Keywords: factor graphs, approximate inference algorithms, structured classiﬁcation, protein subcellular location patterns, location proteomics</p><p>2 0.56169182 <a title="41-lsi-2" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>Author: Zach Jorgensen, Yan Zhou, Meador Inge</p><p>Abstract: Statistical spam ﬁlters are known to be vulnerable to adversarial attacks. One of the more common adversarial attacks, known as the good word attack, thwarts spam ﬁlters by appending to spam messages sets of “good” words, which are words that are common in legitimate email but rare in spam. We present a counterattack strategy that attempts to differentiate spam from legitimate email in the input space by transforming each email into a bag of multiple segments, and subsequently applying multiple instance logistic regression on the bags. We treat each segment in the bag as an instance. An email is classiﬁed as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. We show that a classiﬁer using our multiple instance counterattack strategy is more robust to good word attacks than its single instance counterpart and other single instance learners commonly used in the spam ﬁltering domain. Keywords: spam ﬁltering, multiple instance learning, good word attack, adversarial learning</p><p>3 0.37834698 <a title="41-lsi-3" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>Author: Eric Perrier, Seiya Imoto, Satoru Miyano</p><p>Abstract: Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. However, a recent empirical study has shown that a hybrid algorithm improves sensitively accuracy and speed: it learns a skeleton with an independency test (IT) approach and constrains on the directed acyclic graphs (DAG) considered during the search-and-score phase. Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(γm n ), where γm < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree m and sparse structures ˜ allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even ﬁnds more accurate results when given a sound super-structure. Practically, S can be approximated by IT approaches; signiﬁcance level of the tests controls its sparseness, enabling to control the trade-off between speed and accuracy. For incomplete super-structures, a greedily post-processed version (COS+) still enables to signiﬁcantly outperform other heuristic searches. Keywords: subset Bayesian networks, structure learning, optimal search, super-structure, connected</p><p>4 0.32927293 <a title="41-lsi-4" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>Author: Gal Elidan, Stephen Gould</p><p>Abstract: With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufﬁciently expressive for generalization while at the same time allow for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overﬁtting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modiﬁcations and that is polynomial both in the size of the graph and the treewidth bound. At the heart of our method is a dynamic triangulation that we update in a way that facilitates the addition of chain structures that increase the bound on the model’s treewidth by at most one. We demonstrate the effectiveness of our “treewidth-friendly” method on several real-life data sets and show that it is superior to the greedy approach as soon as the bound on the treewidth is nontrivial. Importantly, we also show that by making use of global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth. Keywords: Bayesian networks, structure learning, model selection, bounded treewidth</p><p>5 0.32652882 <a title="41-lsi-5" href="./jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">30 jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>Author: Vojtěch Franc, Bogdan Savchynskyy</p><p>Abstract: The max-sum classiﬁer predicts n-tuple of labels from n-tuple of observable variables by maximizing a sum of quality functions deﬁned over neighbouring pairs of labels and observable variables. Predicting labels as MAP assignments of a Random Markov Field is a particular example of the max-sum classiﬁer. Learning parameters of the max-sum classiﬁer is a challenging problem because even computing the response of such classiﬁer is NP-complete in general. Estimating parameters using the Maximum Likelihood approach is feasible only for a subclass of max-sum classiﬁers with an acyclic structure of neighbouring pairs. Recently, the discriminative methods represented by the perceptron and the Support Vector Machines, originally designed for binary linear classiﬁers, have been extended for learning some subclasses of the max-sum classiﬁer. Besides the max-sum classiﬁers with the acyclic neighbouring structure, it has been shown that the discriminative learning is possible even with arbitrary neighbouring structure provided the quality functions fulﬁll some additional constraints. In this article, we extend the discriminative approach to other three classes of max-sum classiﬁers with an arbitrary neighbourhood structure. We derive learning algorithms for two subclasses of max-sum classiﬁers whose response can be computed in polynomial time: (i) the max-sum classiﬁers with supermodular quality functions and (ii) the max-sum classiﬁers whose response can be computed exactly by a linear programming relaxation. Moreover, we show that the learning problem can be approximately solved even for a general max-sum classiﬁer. Keywords: max-xum classiﬁer, hidden Markov networks, support vector machines</p><p>6 0.32343772 <a title="41-lsi-6" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>7 0.26400492 <a title="41-lsi-7" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>8 0.25678587 <a title="41-lsi-8" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>9 0.24784657 <a title="41-lsi-9" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>10 0.23872073 <a title="41-lsi-10" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>11 0.21436378 <a title="41-lsi-11" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>12 0.21086864 <a title="41-lsi-12" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>13 0.20642273 <a title="41-lsi-13" href="./jmlr-2008-Regularization_on_Graphs_with_Function-adapted_Diffusion_Processes.html">81 jmlr-2008-Regularization on Graphs with Function-adapted Diffusion Processes</a></p>
<p>14 0.19969548 <a title="41-lsi-14" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>15 0.19489409 <a title="41-lsi-15" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>16 0.19450316 <a title="41-lsi-16" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>17 0.1929182 <a title="41-lsi-17" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>18 0.18975854 <a title="41-lsi-18" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>19 0.18215145 <a title="41-lsi-19" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>20 0.17488068 <a title="41-lsi-20" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.025), (5, 0.113), (21, 0.302), (31, 0.016), (40, 0.028), (54, 0.028), (58, 0.038), (66, 0.049), (76, 0.044), (88, 0.083), (92, 0.033), (94, 0.047), (97, 0.012), (99, 0.04)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79866391 <a title="41-lda-1" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>Author: Shann-Ching Chen, Geoffrey J. Gordon, Robert F. Murphy</p><p>Abstract: In structured classiﬁcation problems, there is a direct conﬂict between expressive models and efﬁcient inference: while graphical models such as Markov random ﬁelds or factor graphs can represent arbitrary dependences among instance labels, the cost of inference via belief propagation in these models grows rapidly as the graph structure becomes more complicated. One important source of complexity in belief propagation is the need to marginalize large factors to compute messages. This operation takes time exponential in the number of variables in the factor, and can limit the expressiveness of the models we can use. In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efﬁcient algorithms for computing messages from these potentials during belief propagation. We believe these new potentials provide a good balance between expressive power and efﬁcient inference in practical structured classiﬁcation problems. We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. We use these potentials to classify images of protein subcellular location patterns in groups of cells. Classifying subcellular location patterns can help us answer many important questions in computational biology, including questions about how various treatments affect the synthesis and behavior of proteins and networks of proteins within a cell. Our new representation and algorithm lead to substantial improvements in both inference speed and classiﬁcation accuracy. Keywords: factor graphs, approximate inference algorithms, structured classiﬁcation, protein subcellular location patterns, location proteomics</p><p>2 0.69158995 <a title="41-lda-2" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>Author: Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel, Daphne Koller</p><p>Abstract: We consider the problem of learning classiﬁers in structured domains, where some objects have a subset of features that are inherently absent due to complex relationships between the features. Unlike the case where a feature exists but its value is not observed, here we focus on the case where a feature may not even exist (structurally absent) for some of the samples. The common approach for handling missing features in discriminative models is to ﬁrst complete their unknown values, and then use a standard classiﬁcation procedure over the completed data. This paper focuses on features that are known to be non-existing, rather than have an unknown value. We show how incomplete data can be classiﬁed directly without any completion of the missing features using a max-margin learning framework. We formulate an objective function, based on the geometric interpretation of the margin, that aims to maximize the margin of each sample in its own relevant subspace. In this formulation, the linearly separable case can be transformed into a binary search over a series of second order cone programs (SOCP), a convex problem that can be solved efﬁciently. We also describe two approaches for optimizing the general case: an approximation that can be solved as a standard quadratic program (QP) and an iterative approach for solving the exact problem. By avoiding the pre-processing phase in which the data is completed, both of these approaches could offer considerable computational savings. More importantly, we show that the elegant handling of missing values by our approach allows it to both outperform other methods when the missing values have non-trivial structure, and be competitive with other methods when the values are missing at random. We demonstrate our results on several standard benchmarks and two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images. Keywords: max margin, missing features, network reconstruction, metabolic pa</p><p>3 0.49551046 <a title="41-lda-3" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Vikas Sindhwani, Sathiya S. Keerthi</p><p>Abstract: Due to its wide applicability, the problem of semi-supervised classiﬁcation is attracting increasing attention in machine learning. Semi-Supervised Support Vector Machines (S 3 VMs) are based on applying the margin maximization principle to both labeled and unlabeled examples. Unlike SVMs, their formulation leads to a non-convex optimization problem. A suite of algorithms have recently been proposed for solving S3 VMs. This paper reviews key ideas in this literature. The performance and behavior of various S3 VM algorithms is studied together, under a common experimental setting. Keywords: semi-supervised learning, support vector machines, non-convex optimization, transductive learning</p><p>4 0.4886781 <a title="41-lda-4" href="./jmlr-2008-Stationary_Features_and_Cat_Detection.html">87 jmlr-2008-Stationary Features and Cat Detection</a></p>
<p>Author: FranĂ§ois Fleuret, Donald Geman</p><p>Abstract: Most discriminative techniques for detecting instances from object categories in still images consist of looping over a partition of a pose space with dedicated binary classiÄ?Ĺš ers. The efÄ?Ĺš ciency of this strategy for a complex pose, that is, for Ä?Ĺš ne-grained descriptions, can be assessed by measuring the effect of sample size and pose resolution on accuracy and computation. Two conclusions emerge: (1) fragmenting the training data, which is inevitable in dealing with high in-class variation, severely reduces accuracy; (2) the computational cost at high resolution is prohibitive due to visiting a massive pose partition. To overcome data-fragmentation we propose a novel framework centered on pose-indexed features which assign a response to a pair consisting of an image and a pose, and are designed to be stationary: the probability distribution of the response is always the same if an object is actually present. Such features allow for efÄ?Ĺš cient, one-shot learning of pose-speciÄ?Ĺš c classiÄ?Ĺš ers. To avoid expensive scene processing, we arrange these classiÄ?Ĺš ers in a hierarchy based on nested partitions of the pose; as in previous work on coarse-to-Ä?Ĺš ne search, this allows for efÄ?Ĺš cient processing. The hierarchy is then Ă˘&euro;?foldedĂ˘&euro;? for training: all the classiÄ?Ĺš ers at each level are derived from one base predictor learned from all the data. The hierarchy is Ă˘&euro;?unfoldedĂ˘&euro;? for testing: parsing a scene amounts to examining increasingly Ä?Ĺš ner object descriptions only when there is sufÄ?Ĺš cient evidence for coarser ones. In this way, the detection results are equivalent to an exhaustive search at high resolution. We illustrate these ideas by detecting and localizing cats in highly cluttered greyscale scenes. Keywords: supervised learning, computer vision, image interpretation, cats, stationary features, hierarchical search</p><p>5 0.40374464 <a title="41-lda-5" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>6 0.3818762 <a title="41-lda-6" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>7 0.37280315 <a title="41-lda-7" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>8 0.37256905 <a title="41-lda-8" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>9 0.37255299 <a title="41-lda-9" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>10 0.37045011 <a title="41-lda-10" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>11 0.37036607 <a title="41-lda-11" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>12 0.36868149 <a title="41-lda-12" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>13 0.36827683 <a title="41-lda-13" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>14 0.36671406 <a title="41-lda-14" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>15 0.36547884 <a title="41-lda-15" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>16 0.3632212 <a title="41-lda-16" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>17 0.36217904 <a title="41-lda-17" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>18 0.36185813 <a title="41-lda-18" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>19 0.36160237 <a title="41-lda-19" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>20 0.36107659 <a title="41-lda-20" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
