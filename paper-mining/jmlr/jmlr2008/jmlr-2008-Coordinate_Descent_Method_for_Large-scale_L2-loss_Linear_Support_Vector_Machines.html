<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-28" href="#">jmlr2008-28</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</h1>
<br/><p>Source: <a title="jmlr-2008-28-pdf" href="http://jmlr.org/papers/volume9/chang08a/chang08a.pdf">pdf</a></p><p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>Reference: <a title="jmlr-2008-28-reference" href="../jmlr2008_reference/jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. [sent-12, score-0.277]
</p><p>2 Keywords: linear support vector machines, document classiﬁcation, coordinate descent  1. [sent-18, score-0.25]
</p><p>3 , 2007) extends Zhang’s work and develops an algorithm which alternates between stochastic gradient descent steps and projection steps. [sent-49, score-0.182]
</p><p>4 ): Low cost per iteration; High cost per iteration; ←→ slow convergence. [sent-54, score-0.12]
</p><p>5 Among methods discussed above, Pegasos randomly subsamples a few instances at a time, so the cost per iteration is low, but the number of iterations is high. [sent-56, score-0.179]
</p><p>6 Coordinate descent is a common unconstrained optimization technique, but its use for large linear SVM has not been exploited much. [sent-62, score-0.153]
</p><p>7 For SVM with kernels, decomposition methods are popular, and they are related to coordinate descent methods. [sent-65, score-0.229]
</p><p>8 1370  C OORDINATE D ESCENT M ETHOD FOR L ARGE - SCALE L2- LOSS L INEAR SVM  dinate descent method updates one component of w at a time by solving a one-variable sub-problem. [sent-67, score-0.2]
</p><p>9 An earlier paper using coordinate descents for L2-SVM is by Zhang and Oles (2001). [sent-70, score-0.121]
</p><p>10 Subsequent to this work, we and some collaborators propose a dual coordinate descent method for linear SVM (Hsieh et al. [sent-82, score-0.245]
</p><p>11 m=  #nz n  (5)  is the average number of nonzero values per feature, and P = max |x ji | ji  represents the upper bound of x ji . [sent-108, score-0.264]
</p><p>12 1371  (6)  C HANG , H SIEH AND L IN  Algorithm 1 Coordinate descent algorithm for L2-SVM 1. [sent-110, score-0.149]
</p><p>13 , wk and approximately solve the sub-problem (7) to n i+1 k+1 obtain wi . [sent-126, score-0.445]
</p><p>14 Solving Linear SVM via Coordinate Descent In this section, we describe our coordinate descent method for solving L2-SVM given in (3). [sent-128, score-0.262]
</p><p>15 , n, such that wk,1 = wk , wk,n+1 = wk+1 , and k+1 k+1 wk,i = [w1 , . [sent-134, score-0.408]
</p><p>16 , wk ) i i+1 n z  (7)  ≡ min f (wk,i + zei ), z  where ei = [0, . [sent-150, score-0.56]
</p><p>17 A description of the coordinate descent algorithm is in Algorithm i−1  1. [sent-157, score-0.245]
</p><p>18 The function in (7) can be rewritten as Di (z) = =  f (wk,i + zei ) 1 k,i (w + zei )T (wk,i + zei ) +C ∑ (b j (wk,i + zei ))2 , 2 j∈I(wk,i +ze )  (8)  i  where b j (w) = 1 − y j wT x j and I(w) = { j | b j (w) > 0}. [sent-158, score-0.608]
</p><p>19 In any interval of z where the set I(wk,i + zei ) does not change, Di (z) is quadratic. [sent-159, score-0.152]
</p><p>20 Di (¯) z The ﬁrst derivative of Di (z) is: k,i Di (z) = wi + z − 2C  ∑  j∈I(wk,i +ze  1372  y j x ji (b j (wk,i + zei )). [sent-163, score-0.267]
</p><p>21 i)  (9)  C OORDINATE D ESCENT M ETHOD FOR L ARGE - SCALE L2- LOSS L INEAR SVM  Unfortunately, Di (z) is not twice differentiable as the last term of Di (z) is not differentiable at {z | b j (wk,i + zei ) = 0 for some j}. [sent-164, score-0.202]
</p><p>22 ji  j∈I(wk,i +zei )  = 1 + 2C  (10)  j∈I(wk,i +zei )  A simple Newton method to solve (7) begins with z0 = 0 and iteratively updates z by the following way until Di (z) = 0: zt+1 = zt − Di (zt )/Di (zt ) for t = 0, 1, . [sent-166, score-0.129]
</p><p>23 Coordinate descent methods are known to converge if at each inner iteration we uniquely attain the minimum of the sub-problem (Bertsekas, 1999, Proposition 2. [sent-171, score-0.184]
</p><p>24 An earlier approach of using coordinate descents for L2-SVM without exactly solving the subproblem is by Zhang and Oles (2001). [sent-176, score-0.157]
</p><p>25 While coordinate descent methods have been well studied in optimization, most convergence analyses assume that the one-variable sub-problem is exactly solved. [sent-185, score-0.247]
</p><p>26 An excellent study on the convergence rate of coordinate descent methods is by Luo and Tseng (1992). [sent-240, score-0.247]
</p><p>27 Next, we investigate the computational complexity per outer iteration of Algorithm 1. [sent-250, score-0.119]
</p><p>28 However, one can use the following trick to save the time: b j (w + zei ) = b j (w) − zy j x ji , (17)  If b j (w), j = 1, . [sent-258, score-0.252]
</p><p>29 , l are available, then obtaining b j (w + zei ) involves only nonzero x ji ’s of the ith feature. [sent-261, score-0.23]
</p><p>30 Using (17), obtaining all b j (w + zei ) costs O(m), where m, the average number of nonzero values per feature, is deﬁned in (5). [sent-262, score-0.182]
</p><p>31 For each λ, the main cost is on calculating 1 k,i 1 k,i Di (λd) − Di (0) = (wi + λd)2 − (wi )2 2 2  ∑  +C  j∈I(wk,i +λdei )  Note that from (17), if x ji = 0,  (b j (wk,i + λdei ))2 −  ∑  (b j (wk,i ))2 . [sent-269, score-0.108]
</p><p>32 Therefore, in general the complexity per outer iteration is: O(nm) = O(#nz). [sent-275, score-0.119]
</p><p>33 Secondly, we show that the order of sub-problems at each iteration can be any permutation of {1, . [sent-279, score-0.083]
</p><p>34 2 Random Permutation of Sub-problems In Section 2, we propose a coordinate descent algorithm which solves the one-variable sub-problems in the order of w1 , . [sent-299, score-0.245]
</p><p>35 Similar to Algorithm 1, the algorithm generates a sequence {w k,i } such that wk,1 = wk , wk,n+1 = wk+1,1 and wtk+1 wtk  wtk,i =  if π−1 (t) < i, k −1 if πk (t) ≥ i. [sent-313, score-0.441]
</p><p>36 That is, from wk to wk+1 we only modify one component. [sent-325, score-0.408]
</p><p>37 The second one is a trust region Newton method (Lin et al. [sent-336, score-0.153]
</p><p>38 The last one is CMLS, which is a coordinate descent method proposed by Zhang and Oles (2001). [sent-339, score-0.245]
</p><p>39 1376  C OORDINATE D ESCENT M ETHOD FOR L ARGE - SCALE L2- LOSS L INEAR SVM  Algorithm 3 An online coordinate descent algorithm 1. [sent-340, score-0.261]
</p><p>40 , wk and approximately solve the sub-problem (7) to obtain n 1 i i k+1 wik . [sent-356, score-0.425]
</p><p>41 Coordinate descent methods have been used in other machine learning problems. [sent-357, score-0.133]
</p><p>42 (2002) discuss the connection between boosting/logistic regression and coordinate a descent methods. [sent-359, score-0.229]
</p><p>43 Their strategies for selecting coordinates at each outer iteration are different from ours. [sent-360, score-0.089]
</p><p>44 Here wk+1/2 is a vector obtained by the stochastic gradient descent step, and w k+1 is the projection √ of wk+1/2 to the set {w | w ≤ 1/ λ}. [sent-388, score-0.166]
</p><p>45 As the cost per iteration is O(#nz/l), the overall complexity is 2 2 2 ˜ C P (#nz) O δε  . [sent-410, score-0.111]
</p><p>46 Regarding the stopping condition, as at each iteration Pegasos only takes one sample for updating w, neither function nor gradient information is available. [sent-415, score-0.121]
</p><p>47 introduced a trust region Newton method for logistic regression. [sent-424, score-0.201]
</p><p>48 For convenience, in subsequent sections, we use TRON to indicate the trust region Newton method for L2-SVM, and TRON-LR for logistic regression The optimization procedure of TRON has two layers of iterations. [sent-427, score-0.221]
</p><p>49 For the computational complexity, the main cost per TRON iteration is O(#nz) × (# conjugate gradient iterations). [sent-439, score-0.144]
</p><p>50 (27)  Compared to our approach or Pegasos, the cost per TRON iteration is high. [sent-440, score-0.111]
</p><p>51 (a) Find an approximate solution sk of the trust region sub-problem (25). [sent-451, score-0.154]
</p><p>52 3 CMLS: A Coordinate Descent Method for L2-SVM In Sections 2 and 3, we introduced our coordinate descent method for solving L2-SVM. [sent-454, score-0.262]
</p><p>53 Here, we discuss the previous work (Zhang and Oles, 2001), which also applies the coordinate descent technique. [sent-455, score-0.229]
</p><p>54 At each outer iteration k, it sequentially minimizes sub-problems (8) by updating one variable of (3) at a time. [sent-457, score-0.113]
</p><p>55 Hence, CMLS applies a technique similar to the trust region method. [sent-459, score-0.137]
</p><p>56 It sets a size ∆ k,i of the trust region, evaluates the ﬁrst derivative (9) of (8), and calculates the upper bound of the generalized second derivative subject to |z| ≤ ∆k,i : l  Ui (z) = 1 + ∑ β j (wk,i + zei ), j=1  β j (w) =  2C 0  if y j wT x j ≤ 1 + |∆k,i xi j |, otherwise. [sent-460, score-0.248]
</p><p>57 In order to speed up the process, Zhang and Oles (2001) smooth the objective function of subproblems with a parameter ck ∈ [0, 1]: l 1 Di (z) = (wk,i + zei )T (wk,i + zei ) +C ∑ (b j (wk,i + zei ))2 , 2 j=1  where b j (w) =  1 − y j wT x j if 1 − y j wT x j > 0, ck (1 − y j wT x j ) otherwise. [sent-463, score-0.518]
</p><p>58 Experiments and Analysis In this section, we conduct two experiments to investigate the performance of our proposed coordinate descent algorithm. [sent-492, score-0.229]
</p><p>59 CD: the coordinate descent method described in Section 2. [sent-549, score-0.245]
</p><p>60 CMLS: a coordinate descent method for L2-SVM (Zhang and Oles, 2001, Algorithm 3). [sent-557, score-0.245]
</p><p>61 31  Table 3: The best parameter C and the corresponding testing accuracy of L1-SVM, L2-SVM and logistic regression (LR). [sent-607, score-0.107]
</p><p>62 TRON-LR: the trust region Newton method for logistic regression introduced by Lin et al. [sent-610, score-0.201]
</p><p>63 Since objective values are stable under such strict stopping conditions, these solutions are seen to be very close to the optima. [sent-633, score-0.081]
</p><p>64 The second experiment is to check the relationship between training time and testing accuracy using our implementation and other solvers: CMLS, TRON (L2-SVM and logistic regression), and Pegasos. [sent-644, score-0.156]
</p><p>65 In Figure 3, we present the testing accuracy along the training time. [sent-731, score-0.091]
</p><p>66 Both are coordinate descent methods, and the cost per iteration is similar. [sent-741, score-0.34]
</p><p>67 This slow convergence may make the selection of stopping conditions (maximal number of iterations for Pegasos) more difﬁcult. [sent-751, score-0.085]
</p><p>68 Unfortunately, since the cost per TRON iteration is high, and the Newton direction is not effective in the beginning, TRON is less efﬁcient in the early stage of the optimization procedure. [sent-755, score-0.131]
</p><p>69 However, as our coordinate descent method updates one component of w at each inner iteration, we have only Di (0) = ∇ f (wk,i )i , i = 1, . [sent-761, score-0.262]
</p><p>70 That is, the training procedure terminates after reaching a stable validation accuracy value. [sent-776, score-0.082]
</p><p>71 2, we show that a random order of the sub-problems helps our coordinate descent method to converge faster in most cases. [sent-781, score-0.245]
</p><p>72 With the permutation of features at each iteration, the cost per CDPER iteration is slightly higher than CD, but CDPER requires much fewer iterations to achieve a similar accuracy value. [sent-786, score-0.203]
</p><p>73 Then the cost per CD iteration is only 1/5 of CDPER, so CD is better in the beginning. [sent-791, score-0.111]
</p><p>74 This result seems to indicate that feature ordering affects the performance of the coordinate descent method. [sent-798, score-0.229]
</p><p>75 2 Coordinate Descents for Logistic Regression We can apply the proposed coordinate descent method to solve logistic regression, which is twice differentiable. [sent-801, score-0.293]
</p><p>76 An earlier study of using coordinate decent methods for logistic regression/maximum entropy is by Miroslav et al. [sent-802, score-0.144]
</p><p>77 Experiments show that for training rcv1, our coordinate descent method takes 93. [sent-806, score-0.277]
</p><p>78 Only for yahoo-japan and yahoo-korea, where TRON-LR is slow (see Figure 3), the coordinate descent method is competitive. [sent-809, score-0.245]
</p><p>79 This result is contrast to earlier experiments for L2-SVM, where the coordinate descent method more quickly obtains a useful model than TRON. [sent-810, score-0.245]
</p><p>80 Then |{ j | x ji = 0}| exponential operations are conducted. [sent-815, score-0.099]
</p><p>81 If we assume that λ = 1 satisﬁes the sufﬁcient decrease condition (12), then the cost per outer iteration is O(#nz) + (#nz exponential operations). [sent-816, score-0.165]
</p><p>82 For TRON-LR, a trust region Newton method, it calculates ∇ f (w) and ∇ 2 f (w) at the beginning of each iteration. [sent-819, score-0.137]
</p><p>83 From (27), the cost per iteration is O(#nz) × (# conjugate gradient iterations) + (l exponential operations). [sent-824, score-0.144]
</p><p>84 Therefore, the cost per iteration of applying trust region Newton methods to L2-SVM and logistic regression does not differ much. [sent-826, score-0.296]
</p><p>85 In contrast, (32) shows that coordinate descent methods are less suitable for logistic regression than L2SVM. [sent-827, score-0.277]
</p><p>86 , l, one updates exp(−y j (wk,i )T x j ) by multiplying it by exp(−zy j x ji ). [sent-832, score-0.095]
</p><p>87 Using y j = ±1, exp(−zy j x ji ) = (exp(−zx ji ))y j . [sent-833, score-0.156]
</p><p>88 As x ji is zero or a constant for all j, the number of exponential operations per inner iteration is reduced to one. [sent-834, score-0.18]
</p><p>89 In addition, applying fast approximations of exponential operations such as Schraudolph (1999) may speed up the coordinate descent method for logistic regression. [sent-835, score-0.314]
</p><p>90 3 Conclusions In summary, we propose and analyze a coordinate descent method for large-scale linear L2-SVM. [sent-837, score-0.245]
</p><p>91 If A is not bounded, there is a sub-sequence {wk } ⊂ A such that wk → ∞. [sent-880, score-0.408]
</p><p>92 Taking the summation of (43) from i = 1 to n, we have n  wk+1 − wk  1  ≥ γ ∑ |∇ f (wk,i )πk (i) | i=1 n  ≥ γ ∑ (|∇ f (wk,1 )πk (i) | − |∇ f (wk,i )πk (i) − ∇ f (wk,1 )πk (i) |)  (44)  i=1  =γ  ∇ f (wk,1 )  n  1−  ∑ |∇ f (wk,i )π (i) − ∇ f (wk,1 )π (i) | k  k  . [sent-905, score-0.408]
</p><p>93 From (44) and (45), we have wk+1 − wk  1  ≥  γ ∇ f (wk,1 ) 1 . [sent-908, score-0.408]
</p><p>94 1 + γ + 2γCP2 (#nz)  With (40), wk+1 − wk  2  1 ≥ √ wk+1 − wk n  1  γ ∇ f (wk ) ≥√ n(1 + γ + 2γCP2 (#nz))  1  γ ≥√ ∇ f (wk ) 2 . [sent-909, score-0.816]
</p><p>95 (47)  From (35) and (47), wk − w∗  2  ≤ ∇ f (wk ) − ∇ f (w∗ )  2  = ∇ f (wk ) 2 . [sent-912, score-0.408]
</p><p>96 n(1 + γ + 2γCP2 (#nz))  ≥ τ w k − w∗ 2 ,  (49)  From (12) and (49), n  f (wk ) − f (wk+1 ) = ∑ ( f (wk,i ) − f (wk,i+1 )) i=1  n  k,i+1 k,i ≥ ∑ σ(wπk (i) − wπk (i) )2 = σ wk+1 − wk i=1  2 2  ≥ στ2 wk − w∗ 2 . [sent-914, score-0.816]
</p><p>97 ) of Algorithm 3, we randomly choose one index i k and update wk to wk+1 . [sent-946, score-0.408]
</p><p>98 A dual coordinate descent method for large-scale linear SVM. [sent-1020, score-0.245]
</p><p>99 On the convergence of coordinate descent method for convex differentiable minimization. [sent-1063, score-0.288]
</p><p>100 Solving large scale linear prediction problems using stochastic gradient descent algorithms. [sent-1102, score-0.185]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wk', 0.408), ('cmls', 0.389), ('cdper', 0.363), ('tron', 0.351), ('di', 0.248), ('pegasos', 0.219), ('nz', 0.212), ('newton', 0.16), ('zei', 0.152), ('cd', 0.141), ('descent', 0.133), ('oordinate', 0.118), ('sieh', 0.108), ('escent', 0.1), ('trust', 0.096), ('coordinate', 0.096), ('arge', 0.09), ('wt', 0.082), ('ethod', 0.082), ('ji', 0.078), ('hang', 0.077), ('cdperone', 0.076), ('oles', 0.072), ('inear', 0.068), ('cmlsper', 0.068), ('svm', 0.064), ('hi', 0.052), ('iteration', 0.051), ('logistic', 0.048), ('mangasarian', 0.045), ('wq', 0.042), ('lr', 0.041), ('region', 0.041), ('outer', 0.038), ('stopping', 0.037), ('wi', 0.037), ('gradient', 0.033), ('permutation', 0.032), ('training', 0.032), ('per', 0.03), ('cost', 0.03), ('iterations', 0.03), ('accuracy', 0.03), ('testing', 0.029), ('zhang', 0.029), ('hsieh', 0.029), ('lin', 0.027), ('qk', 0.026), ('dei', 0.025), ('descents', 0.025), ('eik', 0.025), ('differentiable', 0.025), ('sequentially', 0.024), ('objective', 0.024), ('bottou', 0.022), ('leon', 0.022), ('wkk', 0.022), ('ze', 0.022), ('zy', 0.022), ('document', 0.021), ('relative', 0.021), ('converges', 0.021), ('operations', 0.021), ('stable', 0.02), ('piecewise', 0.02), ('optimization', 0.02), ('ck', 0.019), ('subproblem', 0.019), ('subsamples', 0.019), ('instances', 0.019), ('scale', 0.019), ('format', 0.019), ('zt', 0.018), ('permuting', 0.018), ('csie', 0.018), ('ntu', 0.018), ('convergence', 0.018), ('solvers', 0.018), ('updates', 0.017), ('software', 0.017), ('sk', 0.017), ('nk', 0.017), ('time', 0.017), ('accessing', 0.017), ('grippo', 0.017), ('jq', 0.017), ('miroslav', 0.017), ('wik', 0.017), ('wtk', 0.017), ('xli', 0.017), ('solving', 0.017), ('online', 0.016), ('sathiya', 0.016), ('tw', 0.016), ('log', 0.016), ('decrease', 0.016), ('algorithm', 0.016), ('theorem', 0.016), ('method', 0.016), ('keerthi', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="28-tfidf-1" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>2 0.44274959 <a title="28-tfidf-2" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>3 0.11696234 <a title="28-tfidf-3" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, Chih-Jen Lin</p><p>Abstract: LIBLINEAR is an open source library for large-scale linear classiﬁcation. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efﬁcient on large sparse data sets. Keywords: large-scale linear classiﬁcation, logistic regression, support vector machines, open source, machine learning</p><p>4 0.08114399 <a title="28-tfidf-4" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>5 0.059189789 <a title="28-tfidf-5" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Stefan Klanke, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages. Keywords: regression, local learning, online learning, C, C++, Matlab, Octave, Python</p><p>6 0.050639253 <a title="28-tfidf-6" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>7 0.041557841 <a title="28-tfidf-7" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>8 0.0397585 <a title="28-tfidf-8" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>9 0.039667137 <a title="28-tfidf-9" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>10 0.036767885 <a title="28-tfidf-10" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>11 0.034621403 <a title="28-tfidf-11" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>12 0.033067964 <a title="28-tfidf-12" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>13 0.03087418 <a title="28-tfidf-13" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>14 0.030739067 <a title="28-tfidf-14" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>15 0.028023442 <a title="28-tfidf-15" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>16 0.02797753 <a title="28-tfidf-16" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>17 0.027859772 <a title="28-tfidf-17" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>18 0.026645977 <a title="28-tfidf-18" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>19 0.025891772 <a title="28-tfidf-19" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>20 0.025153309 <a title="28-tfidf-20" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.172), (1, -0.103), (2, 0.095), (3, 0.707), (4, -0.006), (5, -0.074), (6, -0.089), (7, -0.165), (8, 0.137), (9, -0.034), (10, 0.116), (11, 0.016), (12, -0.021), (13, -0.012), (14, -0.006), (15, 0.073), (16, 0.009), (17, -0.016), (18, -0.06), (19, -0.026), (20, 0.056), (21, 0.001), (22, 0.073), (23, -0.065), (24, 0.015), (25, -0.047), (26, 0.051), (27, -0.011), (28, 0.046), (29, -0.007), (30, 0.016), (31, -0.023), (32, 0.053), (33, 0.021), (34, -0.057), (35, 0.008), (36, 0.024), (37, 0.009), (38, 0.03), (39, -0.03), (40, -0.042), (41, 0.095), (42, 0.011), (43, 0.049), (44, -0.031), (45, -0.022), (46, 0.076), (47, -0.004), (48, -0.04), (49, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.967538 <a title="28-lsi-1" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>2 0.93345934 <a title="28-lsi-2" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>3 0.34607205 <a title="28-lsi-3" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, Chih-Jen Lin</p><p>Abstract: LIBLINEAR is an open source library for large-scale linear classiﬁcation. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efﬁcient on large sparse data sets. Keywords: large-scale linear classiﬁcation, logistic regression, support vector machines, open source, machine learning</p><p>4 0.28878561 <a title="28-lsi-4" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>5 0.19820583 <a title="28-lsi-5" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>6 0.18262491 <a title="28-lsi-6" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>7 0.17827931 <a title="28-lsi-7" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>8 0.14194544 <a title="28-lsi-8" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>9 0.14160804 <a title="28-lsi-9" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>10 0.13063233 <a title="28-lsi-10" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>11 0.12827902 <a title="28-lsi-11" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>12 0.12523726 <a title="28-lsi-12" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>13 0.12331975 <a title="28-lsi-13" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>14 0.11113775 <a title="28-lsi-14" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>15 0.11112745 <a title="28-lsi-15" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>16 0.1109326 <a title="28-lsi-16" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>17 0.10826885 <a title="28-lsi-17" href="./jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">30 jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>18 0.10427597 <a title="28-lsi-18" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>19 0.10152192 <a title="28-lsi-19" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>20 0.098309286 <a title="28-lsi-20" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.023), (5, 0.018), (31, 0.044), (35, 0.277), (40, 0.025), (54, 0.057), (58, 0.047), (66, 0.061), (76, 0.016), (88, 0.051), (92, 0.022), (94, 0.234), (99, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83506888 <a title="28-lda-1" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>same-paper 2 0.83435631 <a title="28-lda-2" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>3 0.67979336 <a title="28-lda-3" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>4 0.67918104 <a title="28-lda-4" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>5 0.66071939 <a title="28-lda-5" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Stefan Klanke, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages. Keywords: regression, local learning, online learning, C, C++, Matlab, Octave, Python</p><p>6 0.54239821 <a title="28-lda-6" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>7 0.52286905 <a title="28-lda-7" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>8 0.51335299 <a title="28-lda-8" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>9 0.50825709 <a title="28-lda-9" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>10 0.49074736 <a title="28-lda-10" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>11 0.47669205 <a title="28-lda-11" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>12 0.46425271 <a title="28-lda-12" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>13 0.46281177 <a title="28-lda-13" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>14 0.46132442 <a title="28-lda-14" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>15 0.4583616 <a title="28-lda-15" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>16 0.45654228 <a title="28-lda-16" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>17 0.45636076 <a title="28-lda-17" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>18 0.45334905 <a title="28-lda-18" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>19 0.44825107 <a title="28-lda-19" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>20 0.44553688 <a title="28-lda-20" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
