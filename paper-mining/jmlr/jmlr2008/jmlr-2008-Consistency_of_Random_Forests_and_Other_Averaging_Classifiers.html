<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-25" href="#">jmlr2008-25</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</h1>
<br/><p>Source: <a title="jmlr-2008-25-pdf" href="http://jmlr.org/papers/volume9/biau08a/biau08a.pdf">pdf</a></p><p>Author: Gérard Biau, Luc Devroye, Gábor Lugosi</p><p>Abstract: In the last years of his life, Leo Breiman promoted random forests for use in classiﬁcation. He suggested using averaging as a means of obtaining good discrimination rules. The base classiﬁers used for averaging are simple and randomized, often based on random samples from the data. He left a few questions unanswered regarding the consistency of such rules. In this paper, we give a number of theorems that establish the universal consistency of averaging rules. We also show that some popular classiﬁers, including one suggested by Breiman, are not universally consistent. Keywords: random forests, classiﬁcation trees, consistency, bagging This paper is dedicated to the memory of Leo Breiman.</p><p>Reference: <a title="jmlr-2008-25-reference" href="../jmlr2008_reference/jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Journal of Machine Learning Research 9 (2008) 2015-2033  Submitted 1/08; Revised 5/08; Published 9/08  Consistency of Random Forests and Other Averaging Classiﬁers G´ rard Biau e  GERARD . [sent-1, score-0.046]
</p><p>2 CA  School of Computer Science McGill University Montreal, Canada H3A 2K6  G´ bor Lugosi a  LUGOSI @ UPF. [sent-5, score-0.037]
</p><p>3 ES  ICREA and Department of Economics Pompeu Fabra University Ramon Trias Fargas 25-27 08005 Barcelona, Spain  Editor: Peter Bartlett  Abstract In the last years of his life, Leo Breiman promoted random forests for use in classiﬁcation. [sent-6, score-0.398]
</p><p>4 He suggested using averaging as a means of obtaining good discrimination rules. [sent-7, score-0.061]
</p><p>5 The base classiﬁers used for averaging are simple and randomized, often based on random samples from the data. [sent-8, score-0.146]
</p><p>6 He left a few questions unanswered regarding the consistency of such rules. [sent-9, score-0.144]
</p><p>7 In this paper, we give a number of theorems that establish the universal consistency of averaging rules. [sent-10, score-0.182]
</p><p>8 We also show that some popular classiﬁers, including one suggested by Breiman, are not universally consistent. [sent-11, score-0.023]
</p><p>9 Keywords: random forests, classiﬁcation trees, consistency, bagging  This paper is dedicated to the memory of Leo Breiman. [sent-12, score-0.097]
</p><p>10 Introduction Ensemble methods, popular in machine learning, are learning algorithms that construct a set of many individual classiﬁers (called base learners) and combine them to classify new data points by taking a weighted or unweighted vote of their predictions. [sent-14, score-0.151]
</p><p>11 It is now well-known that ensembles are often much more accurate than the individual classiﬁers that make them up. [sent-15, score-0.035]
</p><p>12 The success of ensemble algorithms on many benchmark data sets has raised considerable interest in understanding why such methods succeed and identifying circumstances in which they can be expected to produce good results. [sent-16, score-0.07]
</p><p>13 These methods differ in the way the base learner is ﬁt and combined. [sent-17, score-0.053]
</p><p>14 For example, bagging (Breiman, 1996) proceeds by generating bootstrap samples from the original data set, constructing a classiﬁer from each bootstrap sample, and voting to combine. [sent-18, score-0.281]
</p><p>15 In boosting (Freund and Schapire, 1996) and arcing algorithms (Breiman, 1998) the successive classiﬁers are constructed by giving increased weight to those points that have been frequently misclassiﬁed, and the classiﬁers are combined using weighted voting. [sent-19, score-0.046]
</p><p>16 On the other hand, random split selection (Dietterich, 2000) c 2008 G´ rard Biau, Luc Devroye and G´ bor Lugosi. [sent-20, score-0.149]
</p><p>17 e a  B IAU , D EVROYE AND L UGOSI  grows trees on the original data set. [sent-21, score-0.037]
</p><p>18 For a ﬁxed number S, at each node, S best splits (in terms of minimizing deviance) are found and the actual split is randomly and uniformly selected from them. [sent-22, score-0.053]
</p><p>19 For a comprehensive review of ensemble methods, we refer the reader to Dietterich (2000a) and the references therein. [sent-23, score-0.051]
</p><p>20 Breiman (2001) provides a general framework for tree ensembles called “random forests”. [sent-24, score-0.136]
</p><p>21 Each tree depends on the values of a random vector sampled independently and with the same distribution for all trees. [sent-25, score-0.133]
</p><p>22 Thus, a random forest is a classiﬁer that consists of many decision trees and outputs the class that is the mode of the classes output by individual trees. [sent-26, score-0.379]
</p><p>23 Algorithms for inducing a random forest were ﬁrst developed by Breiman and Cutler, and “Random Forests” is their trademark. [sent-27, score-0.342]
</p><p>24 edu/users/breiman/RandomForests  provides a collection of downloadable technical reports, and gives an overview of random forests as well as comments on the features of the method. [sent-31, score-0.426]
</p><p>25 Random forests have been shown to give excellent performance on a number of practical problems. [sent-32, score-0.347]
</p><p>26 They work fast, generally exhibit a substantial performance improvement over single tree classiﬁers such as CART, and yield generalization error rates that compare favorably to the best statistical and machine learning methods. [sent-33, score-0.101]
</p><p>27 In fact, random forests are among the most accurate general-purpose classiﬁers available (see, for example, Breiman, 2001). [sent-34, score-0.379]
</p><p>28 Different random forests differ in how randomness is introduced in the tree building process, ranging from extreme random splitting strategies (Breiman, 2000; Cutler and Zhao, 2001) to more involved data-dependent strategies (Amit and Geman, 1997; Breiman, 2001; Dietterich, 2000). [sent-35, score-0.529]
</p><p>29 As a matter of fact, the statistical mechanism of random forests is not yet fully understood and is still under active investigation. [sent-36, score-0.379]
</p><p>30 Unlike single trees, where consistency is proved letting the number of ¨ observations in each terminal node become large (Devroye, Gy orﬁ, and Lugosi, 1996, Chapter 20), random forests are generally built to have a small number of cases in each terminal node. [sent-37, score-0.62]
</p><p>31 Although the mechanism of random forest algorithms appears simple, it is difﬁcult to analyze and remains largely unknown. [sent-38, score-0.342]
</p><p>32 Some attempts to investigate the driving force behind consistency of random forests are by Breiman (2000, 2004) and Lin and Jeon (2006), who establish a connection between random forests and adaptive nearest neighbor methods. [sent-39, score-0.922]
</p><p>33 Meinshausen (2006) proved consistency of certain random forests in the context of so-called quantile regression. [sent-40, score-0.539]
</p><p>34 In this paper we offer consistency theorems for various versions of random forests and other randomized ensemble classiﬁers. [sent-41, score-0.693]
</p><p>35 In Section 2 we introduce a general framework for studying classiﬁers based on averaging randomized base classiﬁers. [sent-42, score-0.256]
</p><p>36 We prove a simple but useful proposition showing that averaged classiﬁers are consistent whenever the base classiﬁers are. [sent-43, score-0.181]
</p><p>37 In Section 3 we prove consistency of two simple random forest classiﬁers, the purely random forest (suggested by Breiman as a starting point for study) and the scale-invariant random forest classiﬁers. [sent-44, score-1.192]
</p><p>38 In Section 4 it is shown that averaging may convert inconsistent rules into consistent ones. [sent-45, score-0.132]
</p><p>39 In Section 5 we brieﬂy investigate consistency of bagging rules. [sent-46, score-0.206]
</p><p>40 We show that, in general, bagging preserves consistency of the base rule and it may even create consistent rules from inconsistent ones. [sent-47, score-0.31]
</p><p>41 In particular, we show that if the bootstrap samples are sufﬁciently small, the bagged version of the 1-nearest neighbor classiﬁer is consistent. [sent-48, score-0.079]
</p><p>42 2016  C ONSISTENCY OF R ANDOM F ORESTS  Finally, in Section 6 we consider random forest classiﬁers based on randomized, greedily grown tree classiﬁers. [sent-49, score-0.443]
</p><p>43 We argue that some greedy random forest classiﬁers, including Breiman’s random forest classiﬁer, are inconsistent and suggest a consistent greedy random forest classiﬁer. [sent-50, score-1.097]
</p><p>44 pairs of random variables such that X (the so-called feature vector) takes its values in Rd while Y (the label) is a binary {0, 1}-valued random variable. [sent-58, score-0.064]
</p><p>45 A classiﬁer gn is a binary-valued function of X and Dn whose probability of error is deﬁned by L(gn ) = P(X,Y ) {gn (X, Dn ) = Y } where P(X,Y ) denotes probability with respect to the pair (X,Y ) (i. [sent-66, score-0.641]
</p><p>46 A sequence {gn } of classiﬁers is consistent for a certain distribution of (X,Y ) if L(gn ) → L∗ in probability. [sent-72, score-0.033]
</p><p>47 In this paper we investigate classiﬁers that calculate their decisions by taking a majority vote over randomized classiﬁers. [sent-73, score-0.328]
</p><p>48 A randomized classiﬁer may use a random variable Z to calculate its decision. [sent-74, score-0.217]
</p><p>49 A randomized classiﬁer is an arbitrary function of the form gn (X, Z, Dn ), which we abbreviate by gn (X, Z). [sent-76, score-1.424]
</p><p>50 The probability of error of gn becomes    L(gn ) = P(X,Y ),Z {gn (X, Z, Dn ) = Y } = P{gn (X, Z, Dn ) = Y |Dn } . [sent-77, score-0.641]
</p><p>51 The deﬁnition of consistency remains the same by augmenting the probability space appropriately to include the randomization. [sent-78, score-0.121]
</p><p>52 Given any randomized classiﬁer, one may calculate the classiﬁer for various draws of the randomizing variable Z. [sent-79, score-0.365]
</p><p>53 It is then a natural idea to deﬁne an averaged classiﬁer by taking a majority vote among the obtained random classiﬁers. [sent-80, score-0.244]
</p><p>54 , Zm are identically distributed draws of the randomizing variable, having the same distribution as Z. [sent-84, score-0.206]
</p><p>55 , Zm are independent, conditionally on X, Y , and Dn . [sent-88, score-0.032]
</p><p>56 , Zm ), one may deﬁne the corresponding voting classiﬁer by (m)  gn (x, Z m , Dn ) =  1 1 if m ∑m gn (x, Z j , Dn ) ≥ j=1 0 otherwise. [sent-92, score-1.386]
</p><p>57 (Here P Z and EZ denote probability and expectation with respect to the randomizing variable Z, that is, conditionally on X, Y , and Dn . [sent-94, score-0.186]
</p><p>58 ) (m) gn may be interpreted as an idealized version of the classiﬁer g n that draws many independent copies of the randomizing variable Z and takes a majority vote over the resulting classiﬁers. [sent-95, score-1.004]
</p><p>59 Our ﬁrst result states that consistency of a randomized classiﬁer is preserved by averaging. [sent-96, score-0.263]
</p><p>60 Proposition 1 Assume that the sequence {gn } of randomized classiﬁers is consistent for a certain (m) distribution of (X,Y ). [sent-97, score-0.175]
</p><p>61 Then the voting classiﬁer gn (for any value of m) and the averaged classiﬁer gn are also consistent. [sent-98, score-1.455]
</p><p>62 In fact, since P{gn (X, Z) = Y |X = x} ≥ P{g∗ (X) = Y |X = x} for all x ∈ Rd , consistency of {gn } means that for µ-almost all x, P{gn (X, Z) = Y |X = x} → P{g∗ (X) = Y |X = x} = min(η(x), 1 − η(x)) . [sent-100, score-0.121]
</p><p>63 ) Then P{g n (X, Z) = Y |X = x} = (2η(x) − 1)P{gn (x, Z) = 0} + 1 − η(x), and by consistency we have P{gn (x, Z) = 0} → 0. [sent-103, score-0.121]
</p><p>64 (m) (m) To prove consistency of the voting classiﬁer gn , it sufﬁces to show that P{gn (x, Z m ) = 0} → 0 for µ-almost all x for which η(x) > 1/2. [sent-104, score-0.866]
</p><p>65 Consistency of the averaged classiﬁer is proved by a similar argument. [sent-106, score-0.091]
</p><p>66 Random Forests Random forests, introduced by Breiman, are averaged classiﬁers in the sense deﬁned in Section 2. [sent-108, score-0.069]
</p><p>67 Formally, a random forest with m trees is a classiﬁer consisting of a collection of randomized base tree classiﬁers gn (x, Z1 ), . [sent-109, score-1.343]
</p><p>68 , Zm are identically distributed random vectors, independent conditionally on X, Y , and Dn . [sent-115, score-0.09]
</p><p>69 The randomizing variable is typically used to determine how the successive cuts are performed when building the tree such as selection of the node and the coordinate to split, as well as the position of the split. [sent-116, score-0.278]
</p><p>70 The random forest classiﬁer takes a majority vote among the random tree classiﬁers. [sent-117, score-0.618]
</p><p>71 If m is large, the random forest classiﬁer is well approximated by the averaged classiﬁer 2018  C ONSISTENCY OF R ANDOM F ORESTS     gn (x) =  {EZ gn (x,Z)≥1/2} . [sent-118, score-1.693]
</p><p>72 For brevity, we state most results of this paper for the averaged classiﬁer (m)  only, though by Proposition 1 various results remain true for the voting classiﬁer g n as well. [sent-119, score-0.173]
</p><p>73 In this section we analyze a simple random forest already considered by Breiman (2000), which we call the purely random forest. [sent-120, score-0.419]
</p><p>74 The random tree classiﬁer gn (x, Z) is constructed as follows. [sent-121, score-0.774]
</p><p>75 All nodes of the tree are associated with rectangular cells such that at each step of the construction of the tree, the collection of cells associated with the leaves of the tree (i. [sent-123, score-0.337]
</p><p>76 The split variable J is then selected uniformly at random from the d candidates x (1) , . [sent-128, score-0.086]
</p><p>77 Finally, the selected cell is split along the randomly chosen variable at a random location, chosen according to a uniform random variable on the length of the chosen side of the selected cell. [sent-132, score-0.192]
</p><p>78 The randomized classiﬁer gn (x, Z) takes a majority vote among all Yi for which the corresponding feature vector Xi falls in the same cell of the random partition as x. [sent-134, score-1.077]
</p><p>79 ) The purely random forest classiﬁer is a radically simpliﬁed version of random forest classiﬁers used in practice. [sent-136, score-0.729]
</p><p>80 The main simpliﬁcation lies in the fact that recursive cell splits do not depend on the labels Y1 , . [sent-137, score-0.073]
</p><p>81 The next theorem mainly serves as an illustration of how the consistency problem of random forest classiﬁers may be attacked. [sent-141, score-0.463]
</p><p>82 More involved versions of random forest classiﬁers are discussed in subsequent sections. [sent-142, score-0.342]
</p><p>83 Then the purely random forest classiﬁer gn is consistent whenever k → ∞ and k/n → 0 as k → ∞. [sent-144, score-1.061]
</p><p>84 Proof By Proposition 1 it sufﬁces to prove consistency of the randomized base tree classiﬁer g n . [sent-145, score-0.417]
</p><p>85 To this end, we recall a general consistency theorem for partitioning classiﬁers proved in (Devroye, Gy¨ rﬁ, and Lugosi, 1996, Theorem 6. [sent-146, score-0.143]
</p><p>86 Observe that the partition has k + 1 rectangular cells, say A 1 , . [sent-151, score-0.078]
</p><p>87 Since these points are independent and identically distributed, ﬁxing the set S (but not the order of the points) and Z, the conditional probability that X falls in the i-th cell equals Ni /(n + 1). [sent-166, score-0.109]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gn', 0.641), ('forests', 0.347), ('forest', 0.31), ('dn', 0.23), ('breiman', 0.195), ('randomized', 0.142), ('randomizing', 0.134), ('consistency', 0.121), ('er', 0.116), ('ers', 0.115), ('classi', 0.106), ('voting', 0.104), ('tree', 0.101), ('vote', 0.098), ('lugosi', 0.087), ('devroye', 0.082), ('biau', 0.081), ('zm', 0.077), ('nn', 0.073), ('averaged', 0.069), ('luc', 0.068), ('bagging', 0.065), ('averaging', 0.061), ('bootstrap', 0.056), ('evroye', 0.054), ('iau', 0.054), ('mcgill', 0.054), ('orests', 0.054), ('ugosi', 0.054), ('cell', 0.054), ('base', 0.053), ('ez', 0.052), ('ensemble', 0.051), ('draws', 0.046), ('cutler', 0.046), ('rard', 0.046), ('purely', 0.045), ('majority', 0.045), ('rectangular', 0.042), ('onsistency', 0.041), ('inconsistent', 0.038), ('bor', 0.037), ('terminal', 0.037), ('trees', 0.037), ('partition', 0.036), ('ensembles', 0.035), ('gy', 0.035), ('leo', 0.035), ('split', 0.034), ('cells', 0.033), ('consistent', 0.033), ('andom', 0.033), ('random', 0.032), ('rd', 0.032), ('conditionally', 0.032), ('falling', 0.031), ('paris', 0.031), ('brevity', 0.031), ('dietterich', 0.031), ('ni', 0.03), ('falls', 0.029), ('collection', 0.027), ('identically', 0.026), ('proposition', 0.026), ('letting', 0.024), ('xn', 0.024), ('calculate', 0.023), ('ramon', 0.023), ('pierre', 0.023), ('upmc', 0.023), ('montreal', 0.023), ('arcing', 0.023), ('bagged', 0.023), ('barcelona', 0.023), ('diam', 0.023), ('driving', 0.023), ('limm', 0.023), ('unanswered', 0.023), ('universally', 0.023), ('successive', 0.023), ('proved', 0.022), ('concreteness', 0.02), ('downloadable', 0.02), ('economics', 0.02), ('idealized', 0.02), ('investigate', 0.02), ('variable', 0.02), ('splits', 0.019), ('curie', 0.019), ('marie', 0.019), ('promoted', 0.019), ('cart', 0.019), ('bo', 0.019), ('borel', 0.019), ('deviance', 0.019), ('geman', 0.019), ('succeed', 0.019), ('quantile', 0.017), ('life', 0.017), ('randomness', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="25-tfidf-1" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>Author: Gérard Biau, Luc Devroye, Gábor Lugosi</p><p>Abstract: In the last years of his life, Leo Breiman promoted random forests for use in classiﬁcation. He suggested using averaging as a means of obtaining good discrimination rules. The base classiﬁers used for averaging are simple and randomized, often based on random samples from the data. He left a few questions unanswered regarding the consistency of such rules. In this paper, we give a number of theorems that establish the universal consistency of averaging rules. We also show that some popular classiﬁers, including one suggested by Breiman, are not universally consistent. Keywords: random forests, classiﬁcation trees, consistency, bagging This paper is dedicated to the memory of Leo Breiman.</p><p>2 0.15334232 <a title="25-tfidf-2" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>Author: Eric Bax</p><p>Abstract: This paper develops bounds on out-of-sample error rates for support vector machines (SVMs). The bounds are based on the numbers of support vectors in the SVMs rather than on VC dimension. The bounds developed here improve on support vector counting bounds derived using Littlestone and Warmuth’s compression-based bounding technique. Keywords: compression, error bound, support vector machine, nearly uniform</p><p>3 0.06794852 <a title="25-tfidf-3" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>Author: Amit Dhurandhar, Alin Dobra</p><p>Abstract: In this paper we use the methodology introduced by Dhurandhar and Dobra (2009) for analyzing the error of classiﬁers and the model selection measures, to analyze decision tree algorithms. The methodology consists of obtaining parametric expressions for the moments of the generalization error (GE) for the classiﬁcation model of interest, followed by plotting these expressions for interpretability. The major challenge in applying the methodology to decision trees, the main theme of this work, is customizing the generic expressions for the moments of GE to this particular classiﬁcation algorithm. The speciﬁc contributions we make in this paper are: (a) we primarily characterize a subclass of decision trees namely, Random decision trees, (b) we discuss how the analysis extends to other decision tree algorithms and (c) in order to extend the analysis to certain model selection measures, we generalize the relationships between the moments of GE and moments of the model selection measures given in (Dhurandhar and Dobra, 2009) to randomized classiﬁcation algorithms. An empirical comparison of the proposed method with Monte Carlo and distribution free bounds obtained using Breiman’s formula, depicts the advantages of the method in terms of running time and accuracy. It thus showcases the use of the deployed methodology as an exploratory tool to study learning algorithms. Keywords: moments, generalization error, decision trees</p><p>4 0.057420198 <a title="25-tfidf-4" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>Author: Sébastien Loustau</p><p>Abstract: This paper investigates statistical performances of Support Vector Machines (SVM) and considers the problem of adaptation to the margin parameter and to complexity. In particular we provide a classiﬁer with no tuning parameter. It is a combination of SVM classiﬁers. Our contribution is two-fold: (1) we propose learning rates for SVM using Sobolev spaces and build a numerically realizable aggregate that converges with same rate; (2) we present practical experiments of this method of aggregation for SVM using both Sobolev spaces and Gaussian kernels. Keywords: classiﬁcation, support vector machines, learning rates, approximation, aggregation of classiﬁers</p><p>5 0.057106964 <a title="25-tfidf-5" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>Author: Shann-Ching Chen, Geoffrey J. Gordon, Robert F. Murphy</p><p>Abstract: In structured classiﬁcation problems, there is a direct conﬂict between expressive models and efﬁcient inference: while graphical models such as Markov random ﬁelds or factor graphs can represent arbitrary dependences among instance labels, the cost of inference via belief propagation in these models grows rapidly as the graph structure becomes more complicated. One important source of complexity in belief propagation is the need to marginalize large factors to compute messages. This operation takes time exponential in the number of variables in the factor, and can limit the expressiveness of the models we can use. In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efﬁcient algorithms for computing messages from these potentials during belief propagation. We believe these new potentials provide a good balance between expressive power and efﬁcient inference in practical structured classiﬁcation problems. We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. We use these potentials to classify images of protein subcellular location patterns in groups of cells. Classifying subcellular location patterns can help us answer many important questions in computational biology, including questions about how various treatments affect the synthesis and behavior of proteins and networks of proteins within a cell. Our new representation and algorithm lead to substantial improvements in both inference speed and classiﬁcation accuracy. Keywords: factor graphs, approximate inference algorithms, structured classiﬁcation, protein subcellular location patterns, location proteomics</p><p>6 0.055232868 <a title="25-tfidf-6" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>7 0.04671799 <a title="25-tfidf-7" href="./jmlr-2008-Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">33 jmlr-2008-Evidence Contrary to the Statistical View of Boosting</a></p>
<p>8 0.042747982 <a title="25-tfidf-8" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>9 0.038887717 <a title="25-tfidf-9" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>10 0.038329139 <a title="25-tfidf-10" href="./jmlr-2008-Regularization_on_Graphs_with_Function-adapted_Diffusion_Processes.html">81 jmlr-2008-Regularization on Graphs with Function-adapted Diffusion Processes</a></p>
<p>11 0.037363492 <a title="25-tfidf-11" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>12 0.036348544 <a title="25-tfidf-12" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>13 0.035817966 <a title="25-tfidf-13" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>14 0.034862831 <a title="25-tfidf-14" href="./jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">30 jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>15 0.032908674 <a title="25-tfidf-15" href="./jmlr-2008-Stationary_Features_and_Cat_Detection.html">87 jmlr-2008-Stationary Features and Cat Detection</a></p>
<p>16 0.031330999 <a title="25-tfidf-16" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>17 0.028181577 <a title="25-tfidf-17" href="./jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>18 0.02801762 <a title="25-tfidf-18" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>19 0.027829239 <a title="25-tfidf-19" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>20 0.027420241 <a title="25-tfidf-20" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.14), (1, -0.029), (2, 0.128), (3, -0.113), (4, -0.033), (5, -0.013), (6, 0.083), (7, -0.118), (8, 0.003), (9, -0.192), (10, 0.054), (11, -0.005), (12, -0.087), (13, 0.091), (14, -0.214), (15, 0.022), (16, 0.294), (17, -0.005), (18, -0.176), (19, 0.04), (20, -0.078), (21, -0.073), (22, 0.186), (23, -0.151), (24, 0.12), (25, -0.106), (26, -0.017), (27, 0.031), (28, -0.043), (29, 0.003), (30, 0.036), (31, -0.222), (32, -0.109), (33, 0.076), (34, 0.226), (35, 0.096), (36, 0.001), (37, 0.104), (38, 0.017), (39, -0.044), (40, 0.025), (41, -0.103), (42, -0.06), (43, -0.159), (44, 0.071), (45, -0.167), (46, -0.102), (47, 0.017), (48, 0.031), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96751708 <a title="25-lsi-1" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>Author: Gérard Biau, Luc Devroye, Gábor Lugosi</p><p>Abstract: In the last years of his life, Leo Breiman promoted random forests for use in classiﬁcation. He suggested using averaging as a means of obtaining good discrimination rules. The base classiﬁers used for averaging are simple and randomized, often based on random samples from the data. He left a few questions unanswered regarding the consistency of such rules. In this paper, we give a number of theorems that establish the universal consistency of averaging rules. We also show that some popular classiﬁers, including one suggested by Breiman, are not universally consistent. Keywords: random forests, classiﬁcation trees, consistency, bagging This paper is dedicated to the memory of Leo Breiman.</p><p>2 0.63544101 <a title="25-lsi-2" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>Author: Eric Bax</p><p>Abstract: This paper develops bounds on out-of-sample error rates for support vector machines (SVMs). The bounds are based on the numbers of support vectors in the SVMs rather than on VC dimension. The bounds developed here improve on support vector counting bounds derived using Littlestone and Warmuth’s compression-based bounding technique. Keywords: compression, error bound, support vector machine, nearly uniform</p><p>3 0.33642811 <a title="25-lsi-3" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>Author: Shann-Ching Chen, Geoffrey J. Gordon, Robert F. Murphy</p><p>Abstract: In structured classiﬁcation problems, there is a direct conﬂict between expressive models and efﬁcient inference: while graphical models such as Markov random ﬁelds or factor graphs can represent arbitrary dependences among instance labels, the cost of inference via belief propagation in these models grows rapidly as the graph structure becomes more complicated. One important source of complexity in belief propagation is the need to marginalize large factors to compute messages. This operation takes time exponential in the number of variables in the factor, and can limit the expressiveness of the models we can use. In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efﬁcient algorithms for computing messages from these potentials during belief propagation. We believe these new potentials provide a good balance between expressive power and efﬁcient inference in practical structured classiﬁcation problems. We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. We use these potentials to classify images of protein subcellular location patterns in groups of cells. Classifying subcellular location patterns can help us answer many important questions in computational biology, including questions about how various treatments affect the synthesis and behavior of proteins and networks of proteins within a cell. Our new representation and algorithm lead to substantial improvements in both inference speed and classiﬁcation accuracy. Keywords: factor graphs, approximate inference algorithms, structured classiﬁcation, protein subcellular location patterns, location proteomics</p><p>4 0.32959464 <a title="25-lsi-4" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>Author: Amit Dhurandhar, Alin Dobra</p><p>Abstract: In this paper we use the methodology introduced by Dhurandhar and Dobra (2009) for analyzing the error of classiﬁers and the model selection measures, to analyze decision tree algorithms. The methodology consists of obtaining parametric expressions for the moments of the generalization error (GE) for the classiﬁcation model of interest, followed by plotting these expressions for interpretability. The major challenge in applying the methodology to decision trees, the main theme of this work, is customizing the generic expressions for the moments of GE to this particular classiﬁcation algorithm. The speciﬁc contributions we make in this paper are: (a) we primarily characterize a subclass of decision trees namely, Random decision trees, (b) we discuss how the analysis extends to other decision tree algorithms and (c) in order to extend the analysis to certain model selection measures, we generalize the relationships between the moments of GE and moments of the model selection measures given in (Dhurandhar and Dobra, 2009) to randomized classiﬁcation algorithms. An empirical comparison of the proposed method with Monte Carlo and distribution free bounds obtained using Breiman’s formula, depicts the advantages of the method in terms of running time and accuracy. It thus showcases the use of the deployed methodology as an exploratory tool to study learning algorithms. Keywords: moments, generalization error, decision trees</p><p>5 0.28881311 <a title="25-lsi-5" href="./jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">30 jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>Author: Vojtěch Franc, Bogdan Savchynskyy</p><p>Abstract: The max-sum classiﬁer predicts n-tuple of labels from n-tuple of observable variables by maximizing a sum of quality functions deﬁned over neighbouring pairs of labels and observable variables. Predicting labels as MAP assignments of a Random Markov Field is a particular example of the max-sum classiﬁer. Learning parameters of the max-sum classiﬁer is a challenging problem because even computing the response of such classiﬁer is NP-complete in general. Estimating parameters using the Maximum Likelihood approach is feasible only for a subclass of max-sum classiﬁers with an acyclic structure of neighbouring pairs. Recently, the discriminative methods represented by the perceptron and the Support Vector Machines, originally designed for binary linear classiﬁers, have been extended for learning some subclasses of the max-sum classiﬁer. Besides the max-sum classiﬁers with the acyclic neighbouring structure, it has been shown that the discriminative learning is possible even with arbitrary neighbouring structure provided the quality functions fulﬁll some additional constraints. In this article, we extend the discriminative approach to other three classes of max-sum classiﬁers with an arbitrary neighbourhood structure. We derive learning algorithms for two subclasses of max-sum classiﬁers whose response can be computed in polynomial time: (i) the max-sum classiﬁers with supermodular quality functions and (ii) the max-sum classiﬁers whose response can be computed exactly by a linear programming relaxation. Moreover, we show that the learning problem can be approximately solved even for a general max-sum classiﬁer. Keywords: max-xum classiﬁer, hidden Markov networks, support vector machines</p><p>6 0.27762103 <a title="25-lsi-6" href="./jmlr-2008-Regularization_on_Graphs_with_Function-adapted_Diffusion_Processes.html">81 jmlr-2008-Regularization on Graphs with Function-adapted Diffusion Processes</a></p>
<p>7 0.25722858 <a title="25-lsi-7" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>8 0.24352458 <a title="25-lsi-8" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>9 0.23641632 <a title="25-lsi-9" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>10 0.20154165 <a title="25-lsi-10" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>11 0.19729583 <a title="25-lsi-11" href="./jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>12 0.19062182 <a title="25-lsi-12" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>13 0.18060251 <a title="25-lsi-13" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>14 0.17458907 <a title="25-lsi-14" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>15 0.17386837 <a title="25-lsi-15" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>16 0.16876985 <a title="25-lsi-16" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>17 0.16847068 <a title="25-lsi-17" href="./jmlr-2008-Stationary_Features_and_Cat_Detection.html">87 jmlr-2008-Stationary Features and Cat Detection</a></p>
<p>18 0.15904523 <a title="25-lsi-18" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>19 0.15551782 <a title="25-lsi-19" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>20 0.1547721 <a title="25-lsi-20" href="./jmlr-2008-Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">33 jmlr-2008-Evidence Contrary to the Statistical View of Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.02), (5, 0.011), (40, 0.015), (54, 0.029), (58, 0.023), (66, 0.041), (76, 0.013), (88, 0.669), (92, 0.039), (94, 0.021), (99, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99415565 <a title="25-lda-1" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>Author: Gérard Biau, Luc Devroye, Gábor Lugosi</p><p>Abstract: In the last years of his life, Leo Breiman promoted random forests for use in classiﬁcation. He suggested using averaging as a means of obtaining good discrimination rules. The base classiﬁers used for averaging are simple and randomized, often based on random samples from the data. He left a few questions unanswered regarding the consistency of such rules. In this paper, we give a number of theorems that establish the universal consistency of averaging rules. We also show that some popular classiﬁers, including one suggested by Breiman, are not universally consistent. Keywords: random forests, classiﬁcation trees, consistency, bagging This paper is dedicated to the memory of Leo Breiman.</p><p>2 0.97859031 <a title="25-lda-2" href="./jmlr-2008-Learning_Control_Knowledge_for_Forward_Search_Planning.html">49 jmlr-2008-Learning Control Knowledge for Forward Search Planning</a></p>
<p>Author: Sungwook Yoon, Alan Fern, Robert Givan</p><p>Abstract: A number of today’s state-of-the-art planners are based on forward state-space search. The impressive performance can be attributed to progress in computing domain independent heuristics that perform well across many domains. However, it is easy to ﬁnd domains where such heuristics provide poor guidance, leading to planning failure. Motivated by such failures, the focus of this paper is to investigate mechanisms for learning domain-speciﬁc knowledge to better control forward search in a given domain. While there has been a large body of work on inductive learning of control knowledge for AI planning, there is a void of work aimed at forward-state-space search. One reason for this may be that it is challenging to specify a knowledge representation for compactly representing important concepts across a wide range of domains. One of the main contributions of this work is to introduce a novel feature space for representing such control knowledge. The key idea is to deﬁne features in terms of information computed via relaxed plan extraction, which has been a major source of success for non-learning planners. This gives a new way of leveraging relaxed planning techniques in the context of learning. Using this feature space, we describe three forms of control knowledge—reactive policies (decision list rules and measures of progress) and linear heuristics—and show how to learn them and incorporate them into forward state-space search. Our empirical results show that our approaches are able to surpass state-of-the-art nonlearning planners across a wide range of planning competition domains. Keywords: planning, machine learning, knowledge representation, search</p><p>3 0.95620722 <a title="25-lda-3" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>Author: Ja-Yong Koo, Yoonkyung Lee, Yuwon Kim, Changyi Park</p><p>Abstract: The support vector machine has been successful in a variety of applications. Also on the theoretical front, statistical properties of the support vector machine have been studied quite extensively with a particular attention to its Bayes risk consistency under some conditions. In this paper, we study somewhat basic statistical properties of the support vector machine yet to be investigated, namely the asymptotic behavior of the coefﬁcients of the linear support vector machine. A Bahadur type representation of the coefﬁcients is established under appropriate conditions, and their asymptotic normality and statistical variability are derived on the basis of the representation. These asymptotic results do not only help further our understanding of the support vector machine, but also they can be useful for related statistical inferences. Keywords: asymptotic normality, Bahadur representation, classiﬁcation, convexity lemma, Radon transform</p><p>4 0.91936934 <a title="25-lda-4" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>Author: Laurens van der Maaten, Geoffrey Hinton</p><p>Abstract: We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces signiﬁcantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to inﬂuence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are signiﬁcantly better than those produced by the other techniques on almost all of the data sets. Keywords: visualization, dimensionality reduction, manifold learning, embedding algorithms, multidimensional scaling</p><p>5 0.69871259 <a title="25-lda-5" href="./jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">53 jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>Author: Manu Chhabra, Robert A. Jacobs</p><p>Abstract: The computational complexities arising in motor control can be ameliorated through the use of a library of motor synergies. We present a new model, referred to as the Greedy Additive Regression (GAR) model, for learning a library of torque sequences, and for learning the coefﬁcients of a linear combination of sequences minimizing a cost function. From the perspective of numerical optimization, the GAR model is interesting because it creates a library of “local features”—each sequence in the library is a solution to a single training task—and learns to combine these sequences using a local optimization procedure, namely, additive regression. We speculate that learners with local representational primitives and local optimization procedures will show good performance on nonlinear tasks. The GAR model is also interesting from the perspective of motor control because it outperforms several competing models. Results using a simulated two-joint arm suggest that the GAR model consistently shows excellent performance in the sense that it rapidly learns to perform novel, complex motor tasks. Moreover, its library is overcomplete and sparse, meaning that only a small fraction of the stored torque sequences are used when learning a new movement. The library is also robust in the sense that, after an initial training period, nearly all novel movements can be learned as additive combinations of sequences in the library, and in the sense that it shows good generalization when an arm’s dynamics are altered between training and test conditions, such as when a payload is added to the arm. Lastly, the GAR model works well regardless of whether motor tasks are speciﬁed in joint space or Cartesian space. We conclude that learning techniques using local primitives and optimization procedures are viable and potentially important methods for motor control and possibly other domains, and that these techniques deserve further examination by the artiﬁcial intelligence and cognitive science</p><p>6 0.65951127 <a title="25-lda-6" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>7 0.64474571 <a title="25-lda-7" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>8 0.63724744 <a title="25-lda-8" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>9 0.63458079 <a title="25-lda-9" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>10 0.62933135 <a title="25-lda-10" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>11 0.6221056 <a title="25-lda-11" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>12 0.6082015 <a title="25-lda-12" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>13 0.59895766 <a title="25-lda-13" href="./jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>14 0.59244078 <a title="25-lda-14" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>15 0.58471495 <a title="25-lda-15" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>16 0.58063817 <a title="25-lda-16" href="./jmlr-2008-On_the_Equivalence_of_Linear_Dimensionality-Reducing_Transformations.html">71 jmlr-2008-On the Equivalence of Linear Dimensionality-Reducing Transformations</a></p>
<p>17 0.57608289 <a title="25-lda-17" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>18 0.57306075 <a title="25-lda-18" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>19 0.57175928 <a title="25-lda-19" href="./jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>20 0.57106167 <a title="25-lda-20" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
