<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-46" href="#">jmlr2008-46</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</h1>
<br/><p>Source: <a title="jmlr-2008-46-pdf" href="http://jmlr.org/papers/volume9/fan08a/fan08a.pdf">pdf</a></p><p>Author: Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, Chih-Jen Lin</p><p>Abstract: LIBLINEAR is an open source library for large-scale linear classiﬁcation. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efﬁcient on large sparse data sets. Keywords: large-scale linear classiﬁcation, logistic regression, support vector machines, open source, machine learning</p><p>Reference: <a title="jmlr-2008-46-reference" href="../jmlr2008_reference/jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 TW  Department of Computer Science National Taiwan University Taipei 106, Taiwan  Editor: Soeren Sonnenburg  Abstract LIBLINEAR is an open source library for large-scale linear classiﬁcation. [sent-16, score-0.197]
</p><p>2 It supports logistic regression and linear support vector machines. [sent-17, score-0.08]
</p><p>3 We provide easy-to-use command-line tools and library calls for users and developers. [sent-18, score-0.213]
</p><p>4 Comprehensive documents are available for both beginners and advanced users. [sent-19, score-0.024]
</p><p>5 Keywords: large-scale linear classiﬁcation, logistic regression, support vector machines, open source, machine learning  1. [sent-21, score-0.083]
</p><p>6 Linear classiﬁcation has become one of the most promising learning techniques for large sparse data with a huge number of instances and features. [sent-23, score-0.025]
</p><p>7 We develop LIBLINEAR as an easy-to-use tool to deal with such data. [sent-24, score-0.021]
</p><p>8 It supports L2-regularized logistic regression (LR), L2-loss and L1-loss linear support vector machines (SVMs) (Boser et al. [sent-25, score-0.097]
</p><p>9 It inherits many features of the popular SVM library LIBSVM (Chang and Lin, 2001) such as simple usage, rich documentation, and open source license (the BSD license1 ). [sent-27, score-0.26]
</p><p>10 For example, it takes only several seconds to train a text classiﬁcation problem from the Reuters Corpus Volume 1 (rcv1) that has more than 600,000 examples. [sent-29, score-0.087]
</p><p>11 For the same task, a general SVM solver such as LIBSVM would take several hours. [sent-30, score-0.056]
</p><p>12 Moreover, LIBLINEAR is competitive with or even faster than state of the art linear classiﬁers such as Pegasos (Shalev-Shwartz et al. [sent-31, score-0.026]
</p><p>13 In Sections 2 and 3, we discuss the design and implementation of LIBLINEAR. [sent-39, score-0.025]
</p><p>14 The New BSD license approved by the Open Source Initiative. [sent-43, score-0.066]
</p><p>15 Large Linear Classiﬁcation (Binary and Multi-class) LIBLINEAR supports two popular binary linear classiﬁers: LR and linear SVM. [sent-46, score-0.034]
</p><p>16 Given a set of  instance-label pairs (xi , yi ), i = 1, . [sent-47, score-0.028]
</p><p>17 , l, xi ∈ Rn , yi ∈ {−1, +1}, both methods solve the following unconstrained optimization problem with different loss functions ξ(w; x i , yi ): min w  1 T l w w +C ∑i=1 ξ(w; xi , yi ), 2  (1)  where C > 0 is a penalty parameter. [sent-50, score-0.084]
</p><p>18 For SVM, the two common loss functions are max(1 − yi wT xi , 0) and max(1 − yi wT xi , 0)2 . [sent-51, score-0.056]
</p><p>19 The approach for L1i i SVM and L2-SVM is a coordinate descent method (Hsieh et al. [sent-56, score-0.021]
</p><p>20 For LR and also L2-SVM, LIBLINEAR implements a trust region Newton method (Lin et al. [sent-58, score-0.062]
</p><p>21 In the testing phase, we predict a data point x as positive if wT x > 0, and negative otherwise. [sent-62, score-0.067]
</p><p>22 The Software Package The LIBLINEAR package includes a library and command-line tools for the learning task. [sent-67, score-0.197]
</p><p>23 The design is highly inspired by the LIBSVM package. [sent-68, score-0.025]
</p><p>24 They share similar usage as well as application program interfaces (APIs), so users/developers can easily use both packages. [sent-69, score-0.102]
</p><p>25 However, their models after training are quite different (in particular, LIBLINEAR stores w in the model, but LIBSVM does not. [sent-70, score-0.049]
</p><p>26 Because of such differences, we decide not to combine these two packages together. [sent-72, score-0.017]
</p><p>27 1 Practical Usage To illustrate the training and testing procedure, we take the data set news20, 3 which has more than one million features. [sent-75, score-0.092]
</p><p>28 575% (3863/4000) The whole procedure (training and testing) takes less than 15 seconds on a modern computer. [sent-85, score-0.045]
</p><p>29 The training time without including disk I/O is less than one second. [sent-86, score-0.05]
</p><p>30 Beyond this simple way of running LIBLINEAR, several parameters are available for advanced use. [sent-87, score-0.024]
</p><p>31 For example, one may specify a parameter to obtain probability outputs for logistic regression. [sent-88, score-0.046]
</p><p>32 2 Documentation The LIBLINEAR package comes with plenty of documentation. [sent-106, score-0.093]
</p><p>33 The README ﬁle describes the installation process, command-line usage, and the library calls. [sent-107, score-0.1]
</p><p>34 Users can read the “Quick Start” section, and begin within a few minutes. [sent-108, score-0.03]
</p><p>35 All the interface functions and related data structures are explained in detail. [sent-110, score-0.036]
</p><p>36 If the README ﬁle does not give the information users want, they can check the online FAQ page. [sent-114, score-0.062]
</p><p>37 4 In addition to software documentation, theoretical properties of the algorithms and comparisons to other methods are in Lin et al. [sent-115, score-0.043]
</p><p>38 The authors are also willing to answer any further questions. [sent-118, score-0.019]
</p><p>39 3 Design The main design principle is to keep the whole package as simple as possible while making the source codes easy to read and maintain. [sent-120, score-0.242]
</p><p>40 Files in LIBLINEAR can be separated into source ﬁles, prebuilt binaries, documentation, and language bindings. [sent-121, score-0.06]
</p><p>41 All source codes follow the C/C++ standard, and there is no dependency on external libraries. [sent-122, score-0.098]
</p><p>42 We provide a simple Makefile to compile the package from source codes. [sent-124, score-0.156]
</p><p>43 The train() function trains a classiﬁer on the given data and the predict() function predicts a given instance. [sent-128, score-0.019]
</p><p>44 To handle multi-class problems via the one-vs-the-rest strategy, train() conducts several binary classiﬁcations, each of which is by calling the train one() function. [sent-129, score-0.085]
</p><p>45 train one() then invokes the solver of users’ choice. [sent-130, score-0.141]
</p><p>46 As LIBLINEAR is written in a modular way, a new solver can be easily plugged in. [sent-134, score-0.074]
</p><p>47 This makes LIBLINEAR not only a machine learning tool but also an experimental platform. [sent-135, score-0.021]
</p><p>48 Making extensions of LIBLINEAR to languages other than C/C++ is easy. [sent-136, score-0.018]
</p><p>49 Many tools designed for LIBSVM can be reused with small modiﬁcations. [sent-138, score-0.044]
</p><p>50 Some examples are the parameter selection tool and the data format checking tool. [sent-139, score-0.021]
</p><p>51 Comparison Due to space limitation, we skip here the full details, which are in Lin et al. [sent-141, score-0.019]
</p><p>52 We only demonstrate that LIBLINEAR quickly reaches the testing accuracy corresponding to the optimal solution of (1). [sent-144, score-0.067]
</p><p>53 We conduct ﬁve-fold cross validation to select the best parameter C for each learning method (L1-SVM, L2-SVM, LR); then we train on the whole training set and predict the testing set. [sent-145, score-0.175]
</p><p>54 Figure 1 shows the comparison between LIBLINEAR and two state of the art L1-SVM solvers: Pegasos (Shalev-Shwartz et al. [sent-146, score-0.026]
</p><p>55 To make the comparison reproducible, codes used for experiments in Lin et al. [sent-149, score-0.038]
</p><p>56 978  Testing accuracy (%)  Testing accuracy (%)  0. [sent-162, score-0.044]
</p><p>57 97 0  5  10  15  20  25  30  Training Time (s)  (a) news20, l: 19,996, n: 1,355,191, #nz: 9,097,916  (b) rcv1, l: 677,399, n: 47,236, #nz: 156,436,656  Figure 1: Testing accuracy versus training time (in seconds). [sent-173, score-0.052]
</p><p>58 We split each set to 4/5 training and 1/5 testing. [sent-176, score-0.03]
</p><p>59 Conclusions LIBLINEAR is a simple and easy-to-use open source package for large linear classiﬁcation. [sent-178, score-0.168]
</p><p>60 (2008) conclude that solvers in LIBLINEAR perform well in practice and have good theoretical properties. [sent-182, score-0.03]
</p><p>61 The ultimate goal is to make easy learning with huge data possible. [sent-184, score-0.044]
</p><p>62 A dual coordinate descent method for large-scale linear SVM. [sent-213, score-0.021]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('liblinear', 0.844), ('hsieh', 0.248), ('lr', 0.111), ('libsvm', 0.111), ('csie', 0.102), ('ntu', 0.102), ('library', 0.1), ('svmperf', 0.099), ('pegasos', 0.095), ('tw', 0.095), ('chang', 0.087), ('lin', 0.084), ('usage', 0.08), ('documentation', 0.076), ('readme', 0.074), ('package', 0.071), ('fan', 0.064), ('users', 0.062), ('nz', 0.061), ('train', 0.06), ('source', 0.06), ('wt', 0.059), ('bsd', 0.059), ('keerthi', 0.056), ('solver', 0.056), ('sieh', 0.05), ('faq', 0.05), ('wang', 0.049), ('logistic', 0.046), ('testing', 0.045), ('trust', 0.044), ('software', 0.043), ('kdd', 0.041), ('license', 0.041), ('codes', 0.038), ('open', 0.037), ('interface', 0.036), ('hang', 0.036), ('taiwan', 0.036), ('supports', 0.034), ('boser', 0.032), ('svm', 0.031), ('le', 0.031), ('read', 0.03), ('solvers', 0.03), ('training', 0.03), ('yi', 0.028), ('newton', 0.028), ('seconds', 0.027), ('classi', 0.027), ('joachims', 0.027), ('tools', 0.026), ('art', 0.026), ('design', 0.025), ('compile', 0.025), ('ibrary', 0.025), ('sundararajan', 0.025), ('invokes', 0.025), ('conducts', 0.025), ('approved', 0.025), ('api', 0.025), ('closing', 0.025), ('skipped', 0.025), ('huge', 0.025), ('calls', 0.025), ('advanced', 0.024), ('accuracy', 0.022), ('soeren', 0.022), ('plenty', 0.022), ('interfaces', 0.022), ('inherits', 0.022), ('arge', 0.022), ('predict', 0.022), ('descent', 0.021), ('svms', 0.021), ('tool', 0.021), ('reproducible', 0.02), ('cjlin', 0.02), ('taipei', 0.02), ('weng', 0.02), ('disk', 0.02), ('edu', 0.019), ('corpus', 0.019), ('willing', 0.019), ('stores', 0.019), ('skip', 0.019), ('ultimate', 0.019), ('reuters', 0.019), ('trains', 0.019), ('icml', 0.019), ('whole', 0.018), ('region', 0.018), ('lassification', 0.018), ('plugged', 0.018), ('reused', 0.018), ('languages', 0.018), ('machines', 0.017), ('inear', 0.017), ('million', 0.017), ('packages', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="46-tfidf-1" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, Chih-Jen Lin</p><p>Abstract: LIBLINEAR is an open source library for large-scale linear classiﬁcation. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efﬁcient on large sparse data sets. Keywords: large-scale linear classiﬁcation, logistic regression, support vector machines, open source, machine learning</p><p>2 0.11696234 <a title="46-tfidf-2" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>3 0.071262084 <a title="46-tfidf-3" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>4 0.030280534 <a title="46-tfidf-4" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>Author: Bo Jiang, Xuegong Zhang, Tianxi Cai</p><p>Abstract: Support vector machine (SVM) is one of the most popular and promising classiﬁcation algorithms. After a classiﬁcation rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples. Keywords: k-fold cross-validation, model evaluation, perturbation-resampling, prediction errors, support vector machine</p><p>5 0.029818643 <a title="46-tfidf-5" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>6 0.029802375 <a title="46-tfidf-6" href="./jmlr-2008-Shark%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">85 jmlr-2008-Shark    (Machine Learning Open Source Software Paper)</a></p>
<p>7 0.026725074 <a title="46-tfidf-7" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>8 0.026560279 <a title="46-tfidf-8" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>9 0.025218744 <a title="46-tfidf-9" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>10 0.024078039 <a title="46-tfidf-10" href="./jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">53 jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>11 0.02276637 <a title="46-tfidf-11" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>12 0.022438947 <a title="46-tfidf-12" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>13 0.021037318 <a title="46-tfidf-13" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>14 0.020963157 <a title="46-tfidf-14" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>15 0.020923242 <a title="46-tfidf-15" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>16 0.020067114 <a title="46-tfidf-16" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>17 0.019435741 <a title="46-tfidf-17" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>18 0.018483566 <a title="46-tfidf-18" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>19 0.017402967 <a title="46-tfidf-19" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>20 0.0169743 <a title="46-tfidf-20" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.09), (1, -0.055), (2, 0.066), (3, 0.256), (4, -0.034), (5, -0.01), (6, -0.007), (7, -0.054), (8, 0.048), (9, -0.047), (10, 0.04), (11, -0.009), (12, 0.003), (13, -0.027), (14, -0.05), (15, 0.045), (16, 0.028), (17, 0.066), (18, -0.008), (19, -0.03), (20, -0.005), (21, 0.101), (22, -0.054), (23, -0.069), (24, 0.023), (25, -0.028), (26, 0.059), (27, 0.17), (28, 0.034), (29, 0.172), (30, 0.247), (31, 0.143), (32, -0.087), (33, 0.122), (34, -0.164), (35, 0.01), (36, -0.081), (37, -0.032), (38, -0.054), (39, -0.247), (40, -0.053), (41, 0.166), (42, 0.25), (43, -0.275), (44, 0.129), (45, 0.02), (46, -0.334), (47, 0.197), (48, 0.332), (49, -0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96887529 <a title="46-lsi-1" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, Chih-Jen Lin</p><p>Abstract: LIBLINEAR is an open source library for large-scale linear classiﬁcation. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efﬁcient on large sparse data sets. Keywords: large-scale linear classiﬁcation, logistic regression, support vector machines, open source, machine learning</p><p>2 0.27138346 <a title="46-lsi-2" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>3 0.21134819 <a title="46-lsi-3" href="./jmlr-2008-Shark%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">85 jmlr-2008-Shark    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Christian Igel, Verena Heidrich-Meisner, Tobias Glasmachers</p><p>Abstract: SHARK is an object-oriented library for the design of adaptive systems. It comprises methods for single- and multi-objective optimization (e.g., evolutionary and gradient-based algorithms) as well as kernel-based methods, neural networks, and other machine learning techniques. Keywords: machine learning software, neural networks, kernel-methods, evolutionary algorithms, optimization, multi-objective-optimization 1. Overview SHARK is a modular C++ library for the design and optimization of adaptive systems. It serves as a toolbox for real world applications and basic research in computational intelligence and machine learning. The library provides methods for single- and multi-objective optimization, in particular evolutionary and gradient-based algorithms, kernel-based learning methods, neural networks, and many other machine learning techniques. Its main design criteria are ﬂexibility and speed. Here we restrict the description of SHARK to its core components, albeit the library contains plenty of additional functionality. Further information can be obtained from the HTML documentation and tutorials. More than 60 illustrative example programs serve as starting points for using SHARK. 2. Basic Tools—Rng, Array, and LinAlg The library provides general auxiliary functions and data structures for the development of machine learning algorithms. The Rng module generates reproducible and platform independent sequences of pseudo random numbers, which can be drawn from 14 predeﬁned discrete and continuous parametric distributions. The Array class provides dynamical array templates of arbitrary type and dimension as well as basic operations acting on these templates. LinAlg implements linear algebra algorithms such as matrix inversion and singular value decomposition. 3. ReClaM—Regression and Classiﬁcation Methods The goal of the ReClaM module is to provide machine learning algorithms for supervised classiﬁcation and regression in a uniﬁed, modular framework. It is built like a construction kit, where the main building blocks are adaptive data processing models, error functions, and optimization c 2008 Christian Igel, Verena Heidrich-Meisner and Tobias Glasmachers. I GEL , H EIDRICH -M EISNER AND G LASMACHERS 8 90736D 3 ¨¥¨¥¥£ ¡ §§©§¦¤¢  init(...) optimize(...) E 8973 B@ 6 4C3 A 86 973 543 %$#¨!</p><p>4 0.15330724 <a title="46-lsi-4" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>Author: Gerda Claeskens, Christophe Croux, Johan Van Kerckhoven</p><p>Abstract: Support vector machines for classiﬁcation have the advantage that the curse of dimensionality is circumvented. It has been shown that a reduction of the dimension of the input space leads to even better results. For this purpose, we propose two information criteria which can be computed directly from the deﬁnition of the support vector machine. We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. We arrive at the same ﬁndings for comparison on some real-world benchmark data sets. Keywords: information criterion, supervised classiﬁcation, support vector machine, variable selection</p><p>5 0.14599688 <a title="46-lsi-5" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>6 0.1401249 <a title="46-lsi-6" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>7 0.13092546 <a title="46-lsi-7" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>8 0.12211606 <a title="46-lsi-8" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>9 0.10898714 <a title="46-lsi-9" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>10 0.10896215 <a title="46-lsi-10" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>11 0.10748169 <a title="46-lsi-11" href="./jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">53 jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>12 0.1058537 <a title="46-lsi-12" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>13 0.093836427 <a title="46-lsi-13" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>14 0.090999343 <a title="46-lsi-14" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>15 0.090773106 <a title="46-lsi-15" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>16 0.087708503 <a title="46-lsi-16" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>17 0.086225644 <a title="46-lsi-17" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>18 0.086174347 <a title="46-lsi-18" href="./jmlr-2008-Learning_Control_Knowledge_for_Forward_Search_Planning.html">49 jmlr-2008-Learning Control Knowledge for Forward Search Planning</a></p>
<p>19 0.082373857 <a title="46-lsi-19" href="./jmlr-2008-Multi-Agent_Reinforcement_Learning_in_Common_Interest_and_Fixed_Sum_Stochastic_Games%3A_An_Experimental_Study.html">65 jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</a></p>
<p>20 0.080330506 <a title="46-lsi-20" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.012), (5, 0.017), (31, 0.497), (35, 0.031), (40, 0.021), (54, 0.014), (58, 0.032), (66, 0.032), (88, 0.041), (92, 0.031), (94, 0.125), (99, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90456456 <a title="46-lda-1" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>Author: Eric Bax, Augusto Callejas</p><p>Abstract: This paper introduces a new PAC transductive error bound for classiﬁcation. The method uses information from the training examples and inputs of working examples to develop a set of likely assignments to outputs of the working examples. A likely assignment with maximum error determines the bound. The method is very effective for small data sets. Keywords: error bound, transduction, nearest neighbor, dynamic programming</p><p>same-paper 2 0.88207197 <a title="46-lda-2" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, Chih-Jen Lin</p><p>Abstract: LIBLINEAR is an open source library for large-scale linear classiﬁcation. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efﬁcient on large sparse data sets. Keywords: large-scale linear classiﬁcation, logistic regression, support vector machines, open source, machine learning</p><p>3 0.38459831 <a title="46-lda-3" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>Author: Eric Bax</p><p>Abstract: This paper develops bounds on out-of-sample error rates for support vector machines (SVMs). The bounds are based on the numbers of support vectors in the SVMs rather than on VC dimension. The bounds developed here improve on support vector counting bounds derived using Littlestone and Warmuth’s compression-based bounding technique. Keywords: compression, error bound, support vector machine, nearly uniform</p><p>4 0.36187389 <a title="46-lda-4" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>5 0.30614904 <a title="46-lda-5" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>6 0.30363193 <a title="46-lda-6" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>7 0.30248475 <a title="46-lda-7" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>8 0.29605752 <a title="46-lda-8" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>9 0.29513398 <a title="46-lda-9" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>10 0.28813288 <a title="46-lda-10" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>11 0.27970985 <a title="46-lda-11" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>12 0.27134132 <a title="46-lda-12" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>13 0.26865789 <a title="46-lda-13" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>14 0.26293686 <a title="46-lda-14" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>15 0.25644743 <a title="46-lda-15" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>16 0.25550851 <a title="46-lda-16" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>17 0.25543067 <a title="46-lda-17" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>18 0.24989662 <a title="46-lda-18" href="./jmlr-2008-Linear-Time_Computation_of_Similarity_Measures_for_Sequential_Data.html">55 jmlr-2008-Linear-Time Computation of Similarity Measures for Sequential Data</a></p>
<p>19 0.24874099 <a title="46-lda-19" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>20 0.24779123 <a title="46-lda-20" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
