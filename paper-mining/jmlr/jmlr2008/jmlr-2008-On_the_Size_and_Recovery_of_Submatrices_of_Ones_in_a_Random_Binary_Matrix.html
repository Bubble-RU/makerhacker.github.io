<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-72" href="#">jmlr2008-72</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</h1>
<br/><p>Source: <a title="jmlr-2008-72-pdf" href="http://jmlr.org/papers/volume9/sun08a/sun08a.pdf">pdf</a></p><p>Author: Xing Sun, Andrew B. Nobel</p><p>Abstract: Binary matrices, and their associated submatrices of 1s, play a central role in the study of random bipartite graphs and in core data mining problems such as frequent itemset mining (FIM). Motivated by these connections, this paper addresses several statistical questions regarding submatrices of 1s in a random binary matrix with independent Bernoulli entries. We establish a three-point concentration result, and a related probability bound, for the size of the largest square submatrix of 1s in a square Bernoulli matrix, and extend these results to non-square matrices and submatrices with ﬁxed aspect ratios. We then consider the noise sensitivity of frequent itemset mining under a simple binary additive noise model, and show that, even at small noise levels, large blocks of 1s leave behind fragments of only logarithmic size. As a result, standard FIM algorithms, which search only for submatrices of 1s, cannot directly recover such blocks when noise is present. On the positive side, we show that an error-tolerant frequent itemset criterion can recover a submatrix of 1s against a background of 0s plus noise, even when the size of the submatrix of 1s is very small. 1 Keywords: frequent itemset mining, bipartite graph, biclique, submatrix of 1s, statistical signiﬁcance</p><p>Reference: <a title="jmlr-2008-72-reference" href="../jmlr2008_reference/jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Motivated by these connections, this paper addresses several statistical questions regarding submatrices of 1s in a random binary matrix with independent Bernoulli entries. [sent-6, score-0.318]
</p><p>2 We establish a three-point concentration result, and a related probability bound, for the size of the largest square submatrix of 1s in a square Bernoulli matrix, and extend these results to non-square matrices and submatrices with ﬁxed aspect ratios. [sent-7, score-0.673]
</p><p>3 We then consider the noise sensitivity of frequent itemset mining under a simple binary additive noise model, and show that, even at small noise levels, large blocks of 1s leave behind fragments of only logarithmic size. [sent-8, score-0.383]
</p><p>4 As a result, standard FIM algorithms, which search only for submatrices of 1s, cannot directly recover such blocks when noise is present. [sent-9, score-0.304]
</p><p>5 On the positive side, we show that an error-tolerant frequent itemset criterion can recover a submatrix of 1s against a background of 0s plus noise, even when the size of the submatrix of 1s is very small. [sent-10, score-0.674]
</p><p>6 1 Keywords: frequent itemset mining, bipartite graph, biclique, submatrix of 1s, statistical signiﬁcance  1. [sent-11, score-0.538]
</p><p>7 In the case of binary matrices, the simplest submatrices of interest are constant, with all entries equal to 1. [sent-31, score-0.312]
</p><p>8 Motivated in part by these connections, this paper considers the extremal properties of submatrices of 1s in a random binary matrix, and considers the recovery of such submatrices in the presence of noise. [sent-33, score-0.576]
</p><p>9 We provide signiﬁcance bounds for the size of submatrices of 1s under the Bernoulli null hypothesis, and use these to establish limits on the performance of standard data mining methods in the presence of Bernoulli noise. [sent-35, score-0.343]
</p><p>10 In the same context, we establish several results on the precise asymptotic size of maximal submatrices of 1s, extending to the setting of bipartite graphs earlier work of Bollob´ s and Erd˝ s (1976) and Matula (1976) on the a o size of maximal cliques in random graphs. [sent-36, score-0.503]
</p><p>11 Lastly, we establish ﬁnite sample and asymptotic results concerning the recovery of all-1s submatrices in the presence of noise. [sent-37, score-0.333]
</p><p>12 1 Overview Connections between binary matrices, frequent itemset mining, and bipartite graphs are discussed in the next section. [sent-39, score-0.358]
</p><p>13 Section 3 is devoted to the size of the largest square submatrix of 1s in a random binary matrix. [sent-40, score-0.323]
</p><p>14 Section 6 is devoted to the noise sensitivity of frequent itemset mining and the recoverability of block structures in the presence of noise. [sent-43, score-0.29]
</p><p>15 If the threshold k is allowed to vary, then FIM algorithms essentially seek to ﬁnd every maximal submatrix of 1s in the data matrix X. [sent-72, score-0.295]
</p><p>16 The third question is how one can recover a submatrix of 1s embedded in a larger matrix of 0s when noise is present. [sent-76, score-0.305]
</p><p>17 Thus maximal submatrices of 1s in X correspond to bicliques in G. [sent-83, score-0.346]
</p><p>18 , Garey and Johnson, 1979; Hochbaum, 1998; Peeters, 2003) that the problem of ﬁnding a biclique with the largest number of edges in a given bipartite graph G is NP-complete, and thus the same is true of the general frequent itemset problem with no restriction on the threshold k. [sent-87, score-0.366]
</p><p>19 Largest Submatrices of 1s: Square Case In this section we study the size of the largest square submatrix of 1s in a square binary matrix whose entries are independent Bernoulli(p) random variables. [sent-96, score-0.41]
</p><p>20 Non-square matrices and submatrices are considered in Section 4. [sent-97, score-0.278]
</p><p>21 Deﬁnition: Given a binary matrix X, let M(X) be the largest k such that there exists a k ×k submatrix of 1s in X. [sent-102, score-0.317]
</p><p>22 When n is sufﬁciently large, the Equation (2) has a unique root s(n) satisfying log b n < s(n) < 2 logb n, where b = p−1 . [sent-120, score-0.773]
</p><p>23 The root s(n) deﬁned by (2) has the form s(n) = 2 logb n − 2 logb logb n +C + o(1) where b = p−1 and C = 2 logb e − 2 logb 2. [sent-123, score-3.865]
</p><p>24 One may obtain a cruder bound, on the probability that M(Z n ) is at least 2 logb n + r, in a simpler fashion by noting that EUk =  n k  2  p  k2  2 e2k ln n−k n2k ≤ 2 e−k log b ≤ k! [sent-132, score-0.84]
</p><p>25 It follows from Theorem 1 that for large n the size of the largest square submatrix of 1s in Z n can take one of at most two integer values in an interval of width 1 + 2ε containing the number s(n). [sent-141, score-0.306]
</p><p>26 (2001) used ﬁrst and second moment arguments to show (in our terminology) that P (logb n ≤ M(Zn ) ≤ 2 logb n) → 1 as n tends to inﬁnity. [sent-152, score-0.798]
</p><p>27 Improving these results, Park and Szpankowski (2005) showed that P ((1 + ε) logb n ≤ M(Zn ) ≤ (2 − ε) logb n) tends to 1 as n tends to ¨ inﬁnity for any ﬁxed 0 < ε < 1. [sent-153, score-1.546]
</p><p>28 1 Smallest Maximal Submatrix of 1s Square submatrices of 1s will occur by chance in a random binary matrix. [sent-162, score-0.289]
</p><p>29 The largest such submatrix has approximately 2 logb n − 2 logb logb n rows. [sent-163, score-2.574]
</p><p>30 Conversely, one may ask about the size of the smallest maximal square submatrix of 1s. [sent-164, score-0.301]
</p><p>31 (A square submatrix of 1s is maximal if there is no larger square submatrix of 1s that properly contains it. [sent-165, score-0.564]
</p><p>32 n→∞ logb n lim  Bollob´ s and Erd˝ s (1976) establish a related result on the size of the smallest clique in a random a o graph. [sent-172, score-0.818]
</p><p>33 Indeed, an extension of their argument provides a lower bound on the size of the smallest square submatrix of 1s that is not properly contained within a rectangular submatrix of 1s, and the resulting bound is necessarily larger than the one in Theorem 2. [sent-174, score-0.562]
</p><p>34 Non-Square Matrices In this section we consider the case where the primary matrix and the target submatrices of 1s may be rectangular, but maintain ﬁxed row/column aspect ratios as the size of the primary matrix grows. [sent-176, score-0.413]
</p><p>35 Investigating the value of k for which the expected number of βk × k submatrices of 1s in Z( αn , n) is equal to 1, we arrive at the function s(n, α, β) =  1+β 1+β logb n − logb β β  1+β logb n + logb α +C(β) + o(1), β  where b = p−1 and C(β) = β−1 ((1 + β) logb e − β logb β) depends only on β. [sent-183, score-4.894]
</p><p>36 In the case where α(n) = n γ for some γ > 0, the proof of Proposition 2 can be modiﬁed to show that P Mn (Z : nγ , β) ≥  γ+  β+1 logb n β  ≤ n−(β+1) r (logb n)(β+1+ε)r . [sent-193, score-0.795]
</p><p>37 5 2  when n  Theorem 3 implies that Z(αn, n) contains a submatrix of 1s having aspect ratio β and area (β + 1) log2 n, the latter increasing with β. [sent-198, score-0.287]
</p><p>38 Park and Szpankowski (2005) establish a related result, b showing that if we do not restrict β, the aspect ratio of the submatrices, then with high probability the submatrix of 1s in Z(m, n) with the largest area is of size O(n) × ln b or ln b × O(n). [sent-199, score-0.475]
</p><p>39 Thus M is the side length of the largest square submatrix of 1’s in 2437  S UN AND N OBEL  p  n  s(n)  40  3. [sent-210, score-0.29]
</p><p>40 3 used to evaluate the result for square submatrices above. [sent-234, score-0.291]
</p><p>41 For each such matrix, we identiﬁed all maximal rectangular submatrices of 1s, and recorded the length ˆ of both their longer and shorter sides. [sent-235, score-0.325]
</p><p>42 Fragmentation and Recovery in the Presence of Noise In this section we shift our attention from submatrices of 1s in Zn to a setting in which Zn plays the role of binary noise. [sent-244, score-0.289]
</p><p>43 Whatever its source, noise can potentially have serious consequences for frequent itemset methods if they are applied in a direct way to identify submatrices of 1s. [sent-270, score-0.486]
</p><p>44 If each entry of the target matrix Xn is zero, then Yn = Zn and the largest k × k submatrix of ones in Yn has k ≈ 2 logb n with b = p−1 . [sent-272, score-1.057]
</p><p>45 At the other extreme, if every entry of Xn is equal to one, then the entries of Yn are independent Bernoulli(1 − p) random variables, and in this case the largest square submatrix of ones in Y has side-length k ≈ 2 logb n with b = (1− p)−1 . [sent-273, score-1.086]
</p><p>46 For each ε > 0, eventually almost surely (2 − ε) logb n < M(Yn ) ≤ 2 logb n, where b = p−1 and b = (1 − p)−1 . [sent-278, score-1.564]
</p><p>47 It then follows from Theorem 1 that M(Yn ) ≤ 2 logb n eventually almost surely. [sent-286, score-0.791]
</p><p>48 Theorem 1 then implies that M(Yn ) ≥ (2 − ˆ M(Y ε) logb n eventually almost surely. [sent-292, score-0.791]
</p><p>49 Under the additive noise model (3), block structures in X cannot be recovered directly by methods such as frequent itemset mining that look for maximal submatrices of ones without errors. [sent-295, score-0.584]
</p><p>50 These submatrices correspond, in the data mining and bipartite graph settings, to approximate frequent itemsets and approximate bicliques, respectively. [sent-298, score-0.551]
</p><p>51 One can readily adapt the ﬁrst moment argument to obtain signiﬁcance bounds for submatrices with a large fraction of 1s; details can be found in Sun (2007). [sent-304, score-0.281]
</p><p>52 A submatrix U of a binary matrix Y is a τ-approximate frequent itemset (τ-AFI) if each of its rows satisﬁes F(ui∗ ) ≥ τ and each of its columns satisﬁes F(u∗ j ) ≥ τ. [sent-311, score-0.522]
</p><p>53 Let Xn be an n × n binary matrix that consists of an l × l submatrix of ones having index set C ∗ , with all other entries equal to 0. [sent-315, score-0.313]
</p><p>54 More precisely, let Cˆ be the family of index sets of square submatrices U ∈ AFIτ (Yn ), and let ˆ C = argmax |C| C∈Cˆ  be the index set of any maximal sized submatrix in Cˆ. [sent-321, score-0.557]
</p><p>55 and ∆2 (α, l) = 2n− 4 αl+2 logb n , with b = exp{3(1 − 2 p0 )2 /8p}. [sent-329, score-0.773]
</p><p>56 Suppose that ∗ {Xn : n ≥ 1} is a sequence of square binary matrices, and that Xn contains an ln × ln submatrix Cn of 1s with all other entries equal to 0. [sent-335, score-0.453]
</p><p>57 1 Proofs of Lemmas 1 and 2 Proof of Lemma 1: Differentiating logb (φn (s)) yields 1 1 ∂ logb (φn (s)) = + logb (n − s) − s − logb s − , ∂s 2(n − s) ln b 2s ln b which is negative when logb n < s < 2 logb n. [sent-348, score-4.772]
</p><p>58 Similarly, for 2 logb n ≤ s < n, s 1 1 s logb s logb φn (s) ≤ s logb (n − s) − − logb s − logb s − logb 2π + 2s + 2 2 2 2 1 1 logb s − logb s − logb 2π < 0 ≤ s 2− 2 2 2 when n is sufﬁciently large. [sent-350, score-7.73]
</p><p>59 Thus for sufﬁciently large n, there exists a unique solution s(n) of the equation φn (s) = 1 with s(n) ∈ (logb n, 2 logb n). [sent-351, score-0.773]
</p><p>60 Proof of Lemma 2: Taking logarithms of both sides of the equation φ n (s) = 1 and rearranging terms yields 1 s2 logb 2π 1 n n logb + n logb − (s + ) logb s + s logb (n − s) − = . [sent-352, score-3.865]
</p><p>61 2 n−s n−s 2 2 2 Lemma 1 implies that s(n) belongs to the interval (logb n, 2 logb n), so we consider the above equation in the case that n >> s. [sent-353, score-0.773]
</p><p>62 Dividing both sides of the equation by s yields log s s logb (n − s) − − logb s = − logb e + O( b ), 2 s which can be rewritten as log s s s n−s − logb − logb e + O( b ). [sent-354, score-3.865]
</p><p>63 logb n − − logb logb n = logb 2 logb n n s  (4)  For each n, deﬁne R(n) via the equation s(n) = 2 logb n − 2 logb logb n + R(n). [sent-355, score-6.184]
</p><p>64 Plugging this expression into (4), it follows that R(n) = 2 logb e − 2 logb 2 + o(1), and the result follows from the uniqueness of s(n). [sent-356, score-1.546]
</p><p>65 To begin, note that for any ﬁxed δ ∈ (0, 1/2), when n is sufﬁciently large, 2+δ k(n) 1 n − k(n) k(n) n n 2 logb n p 2 ≤ p 2 ≤ , Cn (r) r = r + k(n) k(n) (2 − δ) logb n n  which is less than one. [sent-364, score-1.546]
</p><p>66 Deﬁnition: For each k ≥ 1, let nk be the least integer n such that EUk (n) ≥ k3+ε , and let nk be the largest integer n such that EUk (n) ≤ k−3−ε . [sent-375, score-0.627]
</p><p>67 2  Note that nk and nk exist for sufﬁciently large k ≥ 1, as EUk (k) = pk ≤ k−3−ε , EUk (n) is monotone increasing in n, and EUk (n) → ∞ as n → ∞. [sent-376, score-0.601]
</p><p>68 When k is sufﬁciently large, nk − nk < C1 nk k k for some constant C1 > 2. [sent-382, score-0.852]
</p><p>69 Proof of (a): It follows from the deﬁnition of nk that (3+ε) nk k 2 p 2 ≤ k− 2 k  and 2444  (3+ε) nk + 1 k 2 p 2 ≥ k− 2 . [sent-385, score-0.852]
</p><p>70 and nk ≥ b 3+ε (n + 1)k k 2 Combining the two bounds on nk above, yields k  b 2 k! [sent-394, score-0.568]
</p><p>71 k−  (3+ε) 2  1 k  +k  (6)  and the asymptotic relation 1  k  k  nk = b 2 (k! [sent-396, score-0.303]
</p><p>72 (7)  From the deﬁnition of nk , one can establish in a similar fashion the inequalities b  k 2  k! [sent-398, score-0.311]
</p><p>73 (8)  and the asymptotic relation 1  k  k  nk = b 2 (k! [sent-401, score-0.303]
</p><p>74 (9)  The asymptotic expressions for nk and nk ensure that nk < nk+1 when k is sufﬁciently large. [sent-403, score-0.871]
</p><p>75 Proof of (b): It follows from inequalities (6) and (8) that, when k is sufﬁciently large, k  (3+ε) 2  k  3+ε 2  nk − nk ≤ b 2 k! [sent-404, score-0.568]
</p><p>76 k− (k  3+ε k  3+ε 2  1 k  −1  − 1) + k + 2  3+ε  ≤ (nk + 1)(k k − 1) + k + 2 log k < nk C1 . [sent-407, score-0.284]
</p><p>77 nk nk+1  Therefore, as k tends to inﬁnity, nk+2 −1 1 nk+2 − nk+1 n = k+1 nk → b 2 . [sent-411, score-0.568]
</p><p>78 nk+1 − nk 1 − nk+1  2445  S UN AND N OBEL  This completes the proof of Lemma 3. [sent-412, score-0.306]
</p><p>79 C,C ∈S  From the last display one may readily derive that EUk (n)2 =  k  ∑  l=1  n−k k−l  k l  n k  k  r=1  n−k k−r  k r  n k  ∑  · p2 k  2 −lr  ,  where the indices k and l indicate the number of rows and columns, respectively, that the submatrices 2 2 C and C have in common. [sent-416, score-0.273]
</p><p>80 There exists a constant C0 > 0 such that g(Uk (n)) ≤ C0 k−1−ε for every sufﬁciently large k and every nk ≤ n ≤ nk+1 . [sent-420, score-0.284]
</p><p>81 Similarly, If n ≥ nk , then by inequality (8), n ≥ b 2 k! [sent-424, score-0.314]
</p><p>82 k 2 inequality (6) implies that if n ≤ nk+1 then k ≥ (2 − η) logb n for some ﬁxed 0 < η < 1/2. [sent-425, score-0.803]
</p><p>83 Finally, 2446  O N S UBMATRICES OF 1 S IN A R ANDOM B INARY M ATRIX  k2  it follows from the assumption that n ≥ nk and the deﬁnition of nk that n p 2 = EUk (n) ≥ k EUk (nk ) ≥ k3/2+ε/2 . [sent-426, score-0.568]
</p><p>84 h(r) (r + 1) (n − 2k + r + 1) 1 When r ≤ 3 k, the inequality k ≤ 2 logb n implies that k  2  bk2 b 3 bk2 n 3 h(r + 1) ≤ ≤ < 1. [sent-433, score-0.803]
</p><p>85 h(r) n − 2k + r + 1 n − 2k + r + 1  When 2 k ≤ r < k − 1 the inequality k ≥ (2 − η) logb n with 0 < η < 1/2 implies that 3 2(2−η)  2k  h(r + 1) b3 n 3 ≥ ≥ > 1. [sent-434, score-0.803]
</p><p>86 With probability one, when k is sufﬁciently large, M(Z n ) = k whenever nk ≤ n ≤ nk+1 . [sent-441, score-0.284]
</p><p>87 It follows from their deﬁnition that nk < nk , and by Lemma 3 both integers tend to inﬁnity as k tends to inﬁnity. [sent-447, score-0.568]
</p><p>88 Thus for all n ≥ nk0 there exists a unique integer k (depending on n) such that n k ≤ n ≤ nk+1 or nk < n < nk . [sent-455, score-0.584]
</p><p>89 To begin, let n be such that nk ≤ n ≤ nk+1 for some k ≥ k0 . [sent-459, score-0.284]
</p><p>90 Thus, with probability one, when n is sufﬁciently large nk ≤ n ≤ nk+1 implies k < s(n) < k + 1 and M(Zn ) = k. [sent-463, score-0.284]
</p><p>91 2448  (12)  O N S UBMATRICES OF 1 S IN A R ANDOM B INARY M ATRIX  Suppose now that nk ≤ n ≤ nk . [sent-464, score-0.568]
</p><p>92 To this end, note that 0 < s(nk ) − s(nk ) = 2 logb  logb nk n nk − 2 logb + o(1) ≤ 2 logb k + o(1) nk logb nk nk n  log n  k k as logb nk > 1. [sent-467, score-6.342]
</p><p>93 It therefore sufﬁces to show that logb nk = o(1), but this follows from part (b) of b Lemma 3. [sent-468, score-1.057]
</p><p>94 Putting the bounds above together with Lemma 5, we ﬁnd that with probability one, when n is sufﬁciently large  nk ≤ n ≤ nk implies k − ε < s(n) < k + ε and M(Zn ) ∈ {k − 1, k}. [sent-469, score-0.568]
</p><p>95 Given 0 < τ0 < 1, if there exists a k × r binary matrix V such that F(V ) ≥ τ0 , then there exists a v × v submatrix U of V such that F(U) ≥ τ0 , where v = min{k, r}. [sent-486, score-0.29]
</p><p>96 Let W be a binary matrix, and let R1 and R2 be two square submatrices of W such that (i) |R2 | = k2 , (ii) |R1 \R2 | > kγ and (iii) R1 ∈ AFIτ (W ). [sent-492, score-0.324]
</p><p>97 / Proof of Lemma 8: The result is clearly true if R1 ∩ R2 = 0, so we assume that R1 and R2 overlap after suitable row and column permutations, R1 \R2 can be expressed either as a single maximal rectangular submatrix W1 , or as the union of two overlapping maximal rectangular W1 ∪ W2 . [sent-494, score-0.406]
</p><p>98 (A submatrix W of R1 \R2 is maximal if there is no other submatrix of R1 \R2 that contains it. [sent-495, score-0.494]
</p><p>99 It follows that 4 max M τ (C ∩C∗ c ) ≥ v ≥ c∈Cˆ  αl , 4  where τ = 1 − p0 and M τ (X) is size of the largest square submatrix with average greater than τ in a given matrix X. [sent-519, score-0.319]
</p><p>100 4  O N S UBMATRICES OF 1 S IN A R ANDOM B INARY M ATRIX  When n is sufﬁciently large and l ≥ 8α−1 (logb n + 2), we can bound P(A) as follows ≤  P(A)  P(max M τ (C ∩C∗ c ) ≥ c∈Cˆ  ≤ P(M τ (W ) ≥  αl ) 4  αl ) ≤ 2n−(αl/4−2 logb 4  n)  ,  (14)  3(1−p0 −p)2  where b = e 8p . [sent-522, score-0.793]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('logb', 0.773), ('nk', 0.284), ('submatrices', 0.256), ('submatrix', 0.228), ('zn', 0.16), ('euk', 0.155), ('bipartite', 0.11), ('frequent', 0.1), ('itemset', 0.1), ('inary', 0.081), ('obel', 0.081), ('ubmatrices', 0.081), ('fim', 0.074), ('atrix', 0.069), ('ln', 0.067), ('yn', 0.067), ('mining', 0.06), ('afi', 0.052), ('bicliques', 0.052), ('bollob', 0.052), ('andom', 0.049), ('uk', 0.048), ('lemma', 0.044), ('biclustering', 0.044), ('tanay', 0.044), ('aspect', 0.043), ('sun', 0.043), ('maximal', 0.038), ('un', 0.037), ('bern', 0.037), ('mn', 0.036), ('square', 0.035), ('binary', 0.033), ('recovery', 0.031), ('agrawal', 0.031), ('nobel', 0.031), ('rectangular', 0.031), ('inequality', 0.03), ('noise', 0.03), ('proposition', 0.03), ('biclique', 0.029), ('blr', 0.029), ('znk', 0.029), ('ciently', 0.029), ('matrix', 0.029), ('establish', 0.027), ('largest', 0.027), ('moment', 0.025), ('itemsets', 0.025), ('sigmod', 0.025), ('erd', 0.025), ('bernoulli', 0.025), ('cn', 0.024), ('suf', 0.023), ('theorem', 0.023), ('entries', 0.023), ('pei', 0.022), ('proof', 0.022), ('dawande', 0.022), ('szpankowski', 0.022), ('overlap', 0.022), ('matrices', 0.022), ('ul', 0.02), ('items', 0.02), ('xn', 0.02), ('bound', 0.02), ('cance', 0.02), ('primary', 0.019), ('fix', 0.019), ('asymptotic', 0.019), ('mannila', 0.019), ('mishra', 0.019), ('chapel', 0.019), ('clique', 0.018), ('ratios', 0.018), ('zi', 0.018), ('binomial', 0.018), ('recover', 0.018), ('union', 0.018), ('eventually', 0.018), ('dn', 0.017), ('rows', 0.017), ('exploratory', 0.017), ('monotone', 0.017), ('liu', 0.017), ('integer', 0.016), ('clustering', 0.016), ('pk', 0.016), ('bn', 0.016), ('ratio', 0.016), ('ces', 0.015), ('graphs', 0.015), ('nity', 0.015), ('columns', 0.015), ('biclusters', 0.015), ('cantelli', 0.015), ('madeira', 0.015), ('matula', 0.015), ('merck', 0.015), ('nen', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="72-tfidf-1" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>Author: Xing Sun, Andrew B. Nobel</p><p>Abstract: Binary matrices, and their associated submatrices of 1s, play a central role in the study of random bipartite graphs and in core data mining problems such as frequent itemset mining (FIM). Motivated by these connections, this paper addresses several statistical questions regarding submatrices of 1s in a random binary matrix with independent Bernoulli entries. We establish a three-point concentration result, and a related probability bound, for the size of the largest square submatrix of 1s in a square Bernoulli matrix, and extend these results to non-square matrices and submatrices with ﬁxed aspect ratios. We then consider the noise sensitivity of frequent itemset mining under a simple binary additive noise model, and show that, even at small noise levels, large blocks of 1s leave behind fragments of only logarithmic size. As a result, standard FIM algorithms, which search only for submatrices of 1s, cannot directly recover such blocks when noise is present. On the positive side, we show that an error-tolerant frequent itemset criterion can recover a submatrix of 1s against a background of 0s plus noise, even when the size of the submatrix of 1s is very small. 1 Keywords: frequent itemset mining, bipartite graph, biclique, submatrix of 1s, statistical signiﬁcance</p><p>2 0.12302469 <a title="72-tfidf-2" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>Author: Glenn Shafer, Vladimir Vovk</p><p>Abstract: Conformal prediction uses past experience to determine precise levels of conÄ?Ĺš dence in new predictions. Given an error probability ĂŽÄž, together with a method that makes a prediction y of a label Ă&lsaquo;&dagger; y, it produces a set of labels, typically containing y, that also contains y with probability 1 Ă˘&circ;&rsquo; ĂŽÄž. Ă&lsaquo;&dagger; Conformal prediction can be applied to any method for producing y: a nearest-neighbor method, a Ă&lsaquo;&dagger; support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 Ă˘&circ;&rsquo; ĂŽÄž of the time, even though they are based on an accumulating data set rather than on independent data sets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005). Keywords: conÄ?Ĺš dence, on-line compression modeling, on-line learning, prediction regions</p><p>3 0.07847622 <a title="72-tfidf-3" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields general results for classiﬁcation and regression. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. We discuss the related problem of learning parameters of a distribution from multiple data sources. Finally, we illustrate our theory through a series of synthetic simulations. Keywords: error bounds, multi-task learning</p><p>4 0.055150997 <a title="72-tfidf-4" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>Author: Bernadetta Tarigan, Sara A. van de Geer</p><p>Abstract: The success of support vector machines in binary classiﬁcation relies on the fact that hinge loss employed in the risk minimization targets the Bayes rule. Recent research explores some extensions of this large margin based method to the multicategory case. We show a moment bound for the socalled multi-hinge loss minimizers based on two kinds of complexity constraints: entropy with bracketing and empirical entropy. Obtaining such a result based on the latter is harder than ﬁnding one based on the former. We obtain fast rates of convergence that adapt to the unknown margin. Keywords: multi-hinge classiﬁcation, all-at-once, moment bound, fast rate, entropy</p><p>5 0.054135893 <a title="72-tfidf-5" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><p>6 0.042392511 <a title="72-tfidf-6" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>7 0.035697468 <a title="72-tfidf-7" href="./jmlr-2008-Closed_Sets_for_Labeled_Data.html">22 jmlr-2008-Closed Sets for Labeled Data</a></p>
<p>8 0.029477717 <a title="72-tfidf-8" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>9 0.026144974 <a title="72-tfidf-9" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>10 0.023692843 <a title="72-tfidf-10" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>11 0.023206361 <a title="72-tfidf-11" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>12 0.022253813 <a title="72-tfidf-12" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>13 0.020652359 <a title="72-tfidf-13" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>14 0.02039388 <a title="72-tfidf-14" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>15 0.02036736 <a title="72-tfidf-15" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>16 0.019660098 <a title="72-tfidf-16" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>17 0.019545553 <a title="72-tfidf-17" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>18 0.017764786 <a title="72-tfidf-18" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>19 0.017690936 <a title="72-tfidf-19" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>20 0.017265869 <a title="72-tfidf-20" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.111), (1, -0.024), (2, -0.074), (3, -0.068), (4, 0.065), (5, 0.026), (6, 0.063), (7, -0.197), (8, 0.03), (9, -0.08), (10, 0.105), (11, 0.014), (12, -0.087), (13, 0.016), (14, 0.163), (15, 0.008), (16, -0.063), (17, 0.205), (18, 0.215), (19, -0.241), (20, 0.147), (21, -0.093), (22, 0.18), (23, 0.027), (24, 0.11), (25, 0.057), (26, 0.038), (27, 0.092), (28, -0.124), (29, 0.008), (30, 0.097), (31, -0.057), (32, 0.133), (33, 0.02), (34, 0.125), (35, 0.087), (36, 0.085), (37, 0.099), (38, -0.096), (39, -0.029), (40, 0.112), (41, 0.164), (42, -0.183), (43, -0.09), (44, -0.02), (45, -0.039), (46, -0.126), (47, 0.041), (48, -0.178), (49, -0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96104723 <a title="72-lsi-1" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>Author: Xing Sun, Andrew B. Nobel</p><p>Abstract: Binary matrices, and their associated submatrices of 1s, play a central role in the study of random bipartite graphs and in core data mining problems such as frequent itemset mining (FIM). Motivated by these connections, this paper addresses several statistical questions regarding submatrices of 1s in a random binary matrix with independent Bernoulli entries. We establish a three-point concentration result, and a related probability bound, for the size of the largest square submatrix of 1s in a square Bernoulli matrix, and extend these results to non-square matrices and submatrices with ﬁxed aspect ratios. We then consider the noise sensitivity of frequent itemset mining under a simple binary additive noise model, and show that, even at small noise levels, large blocks of 1s leave behind fragments of only logarithmic size. As a result, standard FIM algorithms, which search only for submatrices of 1s, cannot directly recover such blocks when noise is present. On the positive side, we show that an error-tolerant frequent itemset criterion can recover a submatrix of 1s against a background of 0s plus noise, even when the size of the submatrix of 1s is very small. 1 Keywords: frequent itemset mining, bipartite graph, biclique, submatrix of 1s, statistical signiﬁcance</p><p>2 0.66810501 <a title="72-lsi-2" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>Author: Glenn Shafer, Vladimir Vovk</p><p>Abstract: Conformal prediction uses past experience to determine precise levels of conÄ?Ĺš dence in new predictions. Given an error probability ĂŽÄž, together with a method that makes a prediction y of a label Ă&lsaquo;&dagger; y, it produces a set of labels, typically containing y, that also contains y with probability 1 Ă˘&circ;&rsquo; ĂŽÄž. Ă&lsaquo;&dagger; Conformal prediction can be applied to any method for producing y: a nearest-neighbor method, a Ă&lsaquo;&dagger; support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 Ă˘&circ;&rsquo; ĂŽÄž of the time, even though they are based on an accumulating data set rather than on independent data sets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005). Keywords: conÄ?Ĺš dence, on-line compression modeling, on-line learning, prediction regions</p><p>3 0.26682442 <a title="72-lsi-3" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields general results for classiﬁcation and regression. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. We discuss the related problem of learning parameters of a distribution from multiple data sources. Finally, we illustrate our theory through a series of synthetic simulations. Keywords: error bounds, multi-task learning</p><p>4 0.20974243 <a title="72-lsi-4" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><p>5 0.20129704 <a title="72-lsi-5" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>Author: Bernadetta Tarigan, Sara A. van de Geer</p><p>Abstract: The success of support vector machines in binary classiﬁcation relies on the fact that hinge loss employed in the risk minimization targets the Bayes rule. Recent research explores some extensions of this large margin based method to the multicategory case. We show a moment bound for the socalled multi-hinge loss minimizers based on two kinds of complexity constraints: entropy with bracketing and empirical entropy. Obtaining such a result based on the latter is harder than ﬁnding one based on the former. We obtain fast rates of convergence that adapt to the unknown margin. Keywords: multi-hinge classiﬁcation, all-at-once, moment bound, fast rate, entropy</p><p>6 0.17101683 <a title="72-lsi-6" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>7 0.1610242 <a title="72-lsi-7" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>8 0.16063455 <a title="72-lsi-8" href="./jmlr-2008-Closed_Sets_for_Labeled_Data.html">22 jmlr-2008-Closed Sets for Labeled Data</a></p>
<p>9 0.14792848 <a title="72-lsi-9" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>10 0.1264476 <a title="72-lsi-10" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>11 0.12535688 <a title="72-lsi-11" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>12 0.12531771 <a title="72-lsi-12" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>13 0.11567322 <a title="72-lsi-13" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>14 0.11378577 <a title="72-lsi-14" href="./jmlr-2008-Regularization_on_Graphs_with_Function-adapted_Diffusion_Processes.html">81 jmlr-2008-Regularization on Graphs with Function-adapted Diffusion Processes</a></p>
<p>15 0.10613674 <a title="72-lsi-15" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>16 0.09975341 <a title="72-lsi-16" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>17 0.09582635 <a title="72-lsi-17" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>18 0.094513893 <a title="72-lsi-18" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>19 0.089259572 <a title="72-lsi-19" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>20 0.087569714 <a title="72-lsi-20" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.025), (9, 0.011), (40, 0.024), (54, 0.025), (58, 0.036), (66, 0.073), (76, 0.49), (78, 0.01), (88, 0.059), (92, 0.049), (94, 0.028), (99, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.92436421 <a title="72-lda-1" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>Author: Eric Perrier, Seiya Imoto, Satoru Miyano</p><p>Abstract: Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. However, a recent empirical study has shown that a hybrid algorithm improves sensitively accuracy and speed: it learns a skeleton with an independency test (IT) approach and constrains on the directed acyclic graphs (DAG) considered during the search-and-score phase. Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(γm n ), where γm < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree m and sparse structures ˜ allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even ﬁnds more accurate results when given a sound super-structure. Practically, S can be approximated by IT approaches; signiﬁcance level of the tests controls its sparseness, enabling to control the trade-off between speed and accuracy. For incomplete super-structures, a greedily post-processed version (COS+) still enables to signiﬁcantly outperform other heuristic searches. Keywords: subset Bayesian networks, structure learning, optimal search, super-structure, connected</p><p>same-paper 2 0.90064597 <a title="72-lda-2" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>Author: Xing Sun, Andrew B. Nobel</p><p>Abstract: Binary matrices, and their associated submatrices of 1s, play a central role in the study of random bipartite graphs and in core data mining problems such as frequent itemset mining (FIM). Motivated by these connections, this paper addresses several statistical questions regarding submatrices of 1s in a random binary matrix with independent Bernoulli entries. We establish a three-point concentration result, and a related probability bound, for the size of the largest square submatrix of 1s in a square Bernoulli matrix, and extend these results to non-square matrices and submatrices with ﬁxed aspect ratios. We then consider the noise sensitivity of frequent itemset mining under a simple binary additive noise model, and show that, even at small noise levels, large blocks of 1s leave behind fragments of only logarithmic size. As a result, standard FIM algorithms, which search only for submatrices of 1s, cannot directly recover such blocks when noise is present. On the positive side, we show that an error-tolerant frequent itemset criterion can recover a submatrix of 1s against a background of 0s plus noise, even when the size of the submatrix of 1s is very small. 1 Keywords: frequent itemset mining, bipartite graph, biclique, submatrix of 1s, statistical signiﬁcance</p><p>3 0.86427933 <a title="72-lda-3" href="./jmlr-2008-Generalization_from_Observed_to_Unobserved_Features_by_Clustering.html">38 jmlr-2008-Generalization from Observed to Unobserved Features by Clustering</a></p>
<p>Author: Eyal Krupka, Naftali Tishby</p><p>Abstract: We argue that when objects are characterized by many attributes, clustering them on the basis of a random subset of these attributes can capture information on the unobserved attributes as well. Moreover, we show that under mild technical conditions, clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set. We prove ﬁnite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting. We use our framework to analyze generalization to unobserved features of two well-known clustering algorithms: k-means and the maximum likelihood multinomial mixture model. The scheme is demonstrated for collaborative ﬁltering of users with movie ratings as attributes and document clustering with words as attributes. Keywords: clustering, unobserved features, learning theory, generalization in clustering, information bottleneck</p><p>4 0.57182449 <a title="72-lda-4" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>Author: Gal Elidan, Stephen Gould</p><p>Abstract: With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufﬁciently expressive for generalization while at the same time allow for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overﬁtting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modiﬁcations and that is polynomial both in the size of the graph and the treewidth bound. At the heart of our method is a dynamic triangulation that we update in a way that facilitates the addition of chain structures that increase the bound on the model’s treewidth by at most one. We demonstrate the effectiveness of our “treewidth-friendly” method on several real-life data sets and show that it is superior to the greedy approach as soon as the bound on the treewidth is nontrivial. Importantly, we also show that by making use of global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth. Keywords: Bayesian networks, structure learning, model selection, bounded treewidth</p><p>5 0.46250683 <a title="72-lda-5" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>Author: Zongming Ma, Xianchao Xie, Zhi Geng</p><p>Abstract: Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse. Keywords: chain graph, conditional independence, decomposition, graphical model, structural learning</p><p>6 0.42242849 <a title="72-lda-6" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>7 0.42102808 <a title="72-lda-7" href="./jmlr-2008-Causal_Reasoning_with_Ancestral_Graphs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">20 jmlr-2008-Causal Reasoning with Ancestral Graphs    (Special Topic on Causality)</a></p>
<p>8 0.42027095 <a title="72-lda-8" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>9 0.41389129 <a title="72-lda-9" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>10 0.41057494 <a title="72-lda-10" href="./jmlr-2008-HPB%3A_A_Model_for_Handling_BN_Nodes_with_High_Cardinality_Parents.html">42 jmlr-2008-HPB: A Model for Handling BN Nodes with High Cardinality Parents</a></p>
<p>11 0.39501375 <a title="72-lda-11" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>12 0.39438537 <a title="72-lda-12" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>13 0.38409641 <a title="72-lda-13" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>14 0.37272564 <a title="72-lda-14" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>15 0.37194088 <a title="72-lda-15" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>16 0.36692348 <a title="72-lda-16" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>17 0.36659512 <a title="72-lda-17" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>18 0.36474663 <a title="72-lda-18" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>19 0.36308336 <a title="72-lda-19" href="./jmlr-2008-Closed_Sets_for_Labeled_Data.html">22 jmlr-2008-Closed Sets for Labeled Data</a></p>
<p>20 0.35951126 <a title="72-lda-20" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
