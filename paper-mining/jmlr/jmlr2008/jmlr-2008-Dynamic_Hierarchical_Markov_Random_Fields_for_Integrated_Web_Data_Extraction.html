<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-31" href="#">jmlr2008-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</h1>
<br/><p>Source: <a title="jmlr-2008-31-pdf" href="http://jmlr.org/papers/volume9/zhu08a/zhu08a.pdf">pdf</a></p><p>Author: Jun Zhu, Zaiqing Nie, Bo Zhang, Ji-Rong Wen</p><p>Abstract: Existing template-independent web data extraction approaches adopt highly ineffective decoupled strategies—attempting to do data record detection and attribute labeling in two separate phases. In this paper, we propose an integrated web data extraction paradigm with hierarchical models. The proposed model is called Dynamic Hierarchical Markov Random Fields (DHMRFs). DHMRFs take structural uncertainty into consideration and deﬁne a joint distribution of both model structure and class labels. The joint distribution is an exponential family distribution. As a conditional model, DHMRFs relax the independence assumption as made in directed models. Since exact inference is intractable, a variational method is developed to learn the model’s parameters and to ﬁnd the MAP model structure and label assignments. We apply DHMRFs to a real-world web data extraction task. Experimental results show that: (1) integrated web data extraction models can achieve signiﬁcant improvements on both record detection and attribute labeling compared to decoupled models; (2) in diverse web data extraction DHMRFs can potentially address the blocky artifact issue which is suffered by ﬁxed-structured hierarchical models. Keywords: conditional random ﬁelds, dynamic hierarchical Markov random ﬁelds, integrated web data extraction, statistical hierarchical modeling, blocky artifact issue</p><p>Reference: <a title="jmlr-2008-31-reference" href="../jmlr2008_reference/jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we propose an integrated web data extraction paradigm with hierarchical models. [sent-9, score-0.81]
</p><p>2 We apply DHMRFs to a real-world web data extraction task. [sent-15, score-0.508]
</p><p>3 Keywords: conditional random ﬁelds, dynamic hierarchical Markov random ﬁelds, integrated web data extraction, statistical hierarchical modeling, blocky artifact issue  1. [sent-17, score-1.012]
</p><p>4 However, existing approaches use highly ineffective decoupled strategies—attempting to do data record detection and attribute labeling in two separate phases. [sent-26, score-0.629]
</p><p>5 This paper is to ﬁrst propose an integrated web data extraction paradigm with hierarchical Markov Random Fields, and then address the blocky artifact issue (Irving et al. [sent-27, score-0.933]
</p><p>6 However, how to extract product information from webpages generated by many (maybe tens of thousands of) different templates is non-trivial. [sent-38, score-0.376]
</p><p>7 List pages are webpages containing several structured data records, and detail pages are webpages only containing detailed information about a single object. [sent-48, score-0.369]
</p><p>8 However, it is highly ineffective to use decoupled strategies—attempting to do data record detection and attribute labeling in two separate phases. [sent-56, score-0.629]
</p><p>9 The reasons for this are: Error Propagation: as record detection and attribute labeling are two separate phases, the errors in record detection will be propagated to attribute labeling. [sent-57, score-1.01]
</p><p>10 more effective record detection algorithms should take into account the semantic labels of the text, but existing methods (Zhai and Liu, 2005; Lerman et al. [sent-68, score-0.403]
</p><p>11 To address the above problems, the ﬁrst part of this paper is to propose an integrated web data extraction paradigm. [sent-80, score-0.63]
</p><p>12 Speciﬁcally, we take a vision-tree representation of webpages and deﬁne both record detection and attribute labeling as assigning semantic labels to the nodes on the trees. [sent-81, score-0.906]
</p><p>13 Then, we can deﬁne the integrated web data extraction that performs record detection and attribute labeling simultaneously. [sent-82, score-1.201]
</p><p>14 Based on the tree representation, we deﬁne a simple integrated web data extraction model—Hierarchical Conditional Random Fields (HCRFs), whose structures are determined by vision-trees. [sent-83, score-0.671]
</p><p>15 Thus, effective web data extraction models should have the capability to adapt their structures during the inference process. [sent-94, score-0.539]
</p><p>16 But in the scenario of web data, sharing CPTs can be difﬁcult because the hierarchical structures are not as regular as the dyadic or quad trees in image processing. [sent-103, score-0.552]
</p><p>17 The performance of our models is demonstrated on a web data extraction task—production information extraction. [sent-117, score-0.539]
</p><p>18 Section 3 presents an integrated web data extraction paradigm and ﬁxed-structured Hierarchical Conditional Random Fields. [sent-123, score-0.63]
</p><p>19 Preliminary Background Knowledge The background knowledge, on which the following work is based, is from web data extraction and statistical hierarchical modeling. [sent-130, score-0.688]
</p><p>20 1 Web Data Extraction Web data extraction is an information extraction (IE) task that identiﬁes information of interest from webpages. [sent-133, score-0.476]
</p><p>21 The difference of web data extraction from traditional IE is that various types of 1587  Z HU , N IE , Z HANG AND W EN  structural dependencies between HTML elements exist. [sent-134, score-0.553]
</p><p>22 For example, the HTML tag tree is itself hierarchical and each webpage is displayed as a two-dimensional image to readers. [sent-135, score-0.434]
</p><p>23 This paper is to explore both hierarchical and two-dimensional spatial information for more effective web data extraction. [sent-139, score-0.45]
</p><p>24 They take in some manually labeled webpages and learn some extraction rules (or wrappers). [sent-142, score-0.393]
</p><p>25 Since the learned wrappers can only be used to extract data from similar pages, maintaining the wrappers as web sites change will require substantial efforts. [sent-143, score-0.348]
</p><p>26 2 Statistical Hierarchical Modeling Multi-scale or hierarchical statistical modeling has shown great promise in image labeling (Kato et al. [sent-167, score-0.382]
</p><p>27 For example, ﬁxed models in image processing often lead to the blocky artifact issue, and the similar problem arises in web data extraction due to the diversity of web data. [sent-181, score-1.002]
</p><p>28 Integrated Web Data Extraction In this section, we formally deﬁne the integrated web data extraction and also propose Hierarchical Conditional Random Fields (HCRFs) to perform that task. [sent-201, score-0.63]
</p><p>29 Good representation can make the extraction task easier and improve extraction accuracy. [sent-204, score-0.476]
</p><p>30 2 Record Detection and Attribute Labeling Based on the deﬁnition of vision-tree, we now formally deﬁne the concepts of record detection and attribute labeling. [sent-227, score-0.439]
</p><p>31 1589  Z HU , N IE , Z HANG AND W EN  (a)  (b)  (c)  Figure 3: (a) Partial vision-tree of the webpage in Figure 1(a); (b) An HCRF model with linearchain neighborhood between sibling nodes; (c) Another HCRF model with 2D neighborhood between sibling nodes and between nodes that share a grand-parent. [sent-228, score-0.369]
</p><p>32 1 (Record detection): Given a vision-tree, record detection is the task of locating the root of a minimal subtree that contains the content of a record. [sent-234, score-0.331]
</p><p>33 2 (Attribute labeling): For each identiﬁed record, attribute labeling is the task of assigning attribute labels to the leaf blocks (elements) within the record. [sent-240, score-0.603]
</p><p>34 We can build a complete model to extract both records and attributes by sequentially combining existing record detection and attribute labeling algorithms. [sent-241, score-0.772]
</p><p>35 Therefore, we propose an integrated approach that conducts simultaneous record extraction and attribute labeling. [sent-243, score-0.699]
</p><p>36 3 Integrated Web Data Extraction Based on the above deﬁnitions, both record detection and attribute labeling are the task of assigning labels to blocks of the vision-tree for a webpage. [sent-245, score-0.696]
</p><p>37 Formally, we deﬁne the integrated web data extraction as: Deﬁnition 3. [sent-247, score-0.63]
</p><p>38 The goal of web data extraction is to ﬁnd a label assignment y that has the maximum posterior probability y = arg maxy p(y|x), and extract data from this assignment. [sent-255, score-0.601]
</p><p>39 4 Hierarchical Conditional Random Fields In this section, we ﬁrst introduce some basics of Conditional Random Fields and then propose Hierarchical Conditional Random Fields for integrated web data extraction. [sent-257, score-0.392]
</p><p>40 Then, T = ∪L−1 {(i, j, l) : 0 ≤ i, j < Nd , 0 ≤ l < Nd−1 , ni j = d=1 1, sil = 1, and s jl = 1} is the set of triangles in the graph G. [sent-288, score-0.356]
</p><p>41 Then, hierarchical statistical modeling is a task to construct an appropriate hierarchical model structure and carry out inference about the labels of given observations. [sent-316, score-0.411]
</p><p>42 We will give an example of web data extraction in the experiment section. [sent-318, score-0.508]
</p><p>43 This assumption holds in applications such as web data extraction and image processing. [sent-348, score-0.578]
</p><p>44 With the structure model, the ﬁrst part of the energy when the system is at the state (s, y) is E1 (s, y, x) = ∑ µk ∑ sil s jl ni j gk (i, j, l, x), k  i jl  where a triple (i, j, l) denotes a particular position in the dynamic model. [sent-353, score-0.668]
</p><p>45 For example, in web data extraction only the labels of leaf nodes are observable and both the hierarchical structures and the labels of inner nodes are hidden. [sent-386, score-1.041]
</p><p>46 Let q(s, yh |ye , x) be an approximation of the true distribution p(s, yh |ye , x). [sent-391, score-0.416]
</p><p>47 With a little abuse of notations, we will use q(s, yh ) to denote q(s, yh |ye , x). [sent-392, score-0.416]
</p><p>48 Take p(s, yh |ye , x) = p(s, yh , ye |x)/p(ye |x) into the above equation and use the non-negativity of KL divergence, we get a lower bound of the log-likelihood log p(ye |x) ≥ ∑ q(s, yh )[log p(s, yh , ye |x) − log q(s, yh )]. [sent-396, score-1.228]
</p><p>49 s,yh  Equivalently, L (Θ) ∑s,yh q(s, yh )[log q(s, yh ) − log p(s, yh , ye |x)] is an upper bound of the negative log-likelihood −L(Θ). [sent-397, score-0.718]
</p><p>50 1595  Z HU , N IE , Z HANG AND W EN  Similarly, the derivatives of L (Θ) with respect to µk are ∂L (Θ) = − ∑ ni j sil s jl ∂µk i jl  q(s,yh ) gk (i,  j, l, x, ζ) −  ∂F∞ . [sent-403, score-0.531]
</p><p>51 Now, the upper bound is approximated by  L (Θ)=F0 − F∞ CFiApp ,  ≈F0 − Fi = KL(q0 ||p) − KL(qi ||p)  where q0 = q(s, yh ) is optimized with observable labels clamped to their values, and q i (s, yh , ye ) is optimized with all variables free starting with q0 . [sent-407, score-0.561]
</p><p>52 Assume q0 can be factorized as q0 = q(s, yh ) = q(s)q(yh ), and we get KL(q0 ||p) = − log p(s, yh , ye |x)  q0  − H(q(s)) − H(q(yh )),  (3)  where H(p) = − log p p is the entropy of distribution p. [sent-419, score-0.51]
</p><p>53 i iy  il  Substitute the above distributions into (3) and keep q(yh ) ﬁxed, then we get KL(q0 ||p) = − log p(s, yh , ye |x)  q0  − H(q(s)) + c,  where c is a constant. [sent-428, score-0.345]
</p><p>54 Let the derivative over µil equal zero, and we get µil ∝ exp  ∑k λk sil ∑ j s jl  ∑k µk sil ∑ j s jl q(s) ni j gk (i, j, l, x)+ y1 y2 y3 q(s) ni j ∑y1 ,y2 ,y3 αi α j αl q(yh ) f k (y1 , y2 , y3 , x, ζ)  . [sent-429, score-0.749]
</p><p>55 Let the derivative over my equal zero, and we get i   y1 y2  ni j sil s jl q(s) α j αl q(yh ) fk (y, y1 , y2 , x, ζ)+  ni j s jl sil q(s) αy1 αy2 q(yh ) fk (y1 , y, y2 , x, ζ)+ . [sent-432, score-0.712]
</p><p>56 The ﬁrst part is to evaluate the performance of integrated web data extraction models compared with existing decoupled methods. [sent-461, score-0.719]
</p><p>57 All the experiments are carried out on a real-world web data extraction task—production information extraction. [sent-463, score-0.508]
</p><p>58 Based on work by Zhai and Liu (2005), we try to align the elements of two adjacent blocks in the same page and extract some global features to help attribute labeling. [sent-519, score-0.36]
</p><p>59 2 Label Spaces For variables at leaf nodes, we are interested in deciding whether a leaf block (an element) is an attribute value of the object we want to extract. [sent-529, score-0.348]
</p><p>60 So, we have two types of label spaces—leaf label space for variables at leaf nodes and inner label space for variables at inner nodes. [sent-531, score-0.366]
</p><p>61 All these labels are general to any web data extraction problem, and they are independent of any speciﬁc object type. [sent-539, score-0.559]
</p><p>62 Object-type Dependent Labels: Between data record blocks and leaf blocks, there are intermediate blocks on a vision-tree. [sent-540, score-0.41]
</p><p>63 The object-type dependent labels in product information extraction are listed in Table 2 with the format Con *. [sent-546, score-0.335]
</p><p>64 For the training data, the detail pages are from 61 web sites and the list pages are from 81 web sites. [sent-554, score-0.692]
</p><p>65 The number of web sites that are found in both list and detail training data is 39. [sent-555, score-0.422]
</p><p>66 Totally, 58 unique templates are presented in the list training pages and 61 unique templates are presented in the detail training pages. [sent-557, score-0.382]
</p><p>67 For testing data, Table 3 shows the number of unique web sites where the pages come from and the number of different templates presented in these data. [sent-558, score-0.439]
</p><p>68 For example, the pages in LDST are from 167 web sites, of which 78 are found in list training data and 52 are found in detail training data. [sent-559, score-0.386]
</p><p>69 The number of web sites that are found in both list and detail training data is 34. [sent-560, score-0.422]
</p><p>70 Thus, totally 71 list page web sites and 263 detail page web sites are not seen in the training data. [sent-562, score-0.868]
</p><p>71 For templates, 83 list page templates and 208 detail page templates are not seen the training data. [sent-563, score-0.522]
</p><p>72 In DDST, pages from different web sites typically have different templates and thus most templates have 1 document. [sent-566, score-0.572]
</p><p>73 A block is considered as a correctly detected data record if it contains all the appeared 1601  Z HU , N IE , Z HANG AND W EN  Data Sets LDST DDST #Web Site 167 (78/52/34) 268 (2/3/0) #Template 140 (57/0/0) 212 (0/4/0) Table 3: Statistics of the data sets. [sent-569, score-0.32]
</p><p>74 Evaluation of Integrated Web Data Extraction Models In this section, we report the evaluation results of integrated web data extraction models compared with decoupled models. [sent-576, score-0.719]
</p><p>75 The results demonstrate that integrated extraction models can achieve signiﬁcant improvements over decoupled models in both record detection and attribute labeling. [sent-577, score-0.919]
</p><p>76 We also show the generalization ability of the integrated extraction models. [sent-578, score-0.36]
</p><p>77 For the integrated extraction model, a webpage is ﬁrst segmented by VIPS to construct a vision-tree and then HCRFs are used to detect both records and attributes on the vision-tree. [sent-584, score-0.616]
</p><p>78 To see the separate effect of our approach on record detection and attribute labeling, we ﬁrst detect data records on the parsed vision-trees using the content features, tree distance, shape distance, and type distance features. [sent-598, score-0.65]
</p><p>79 We use the same 200 list pages to train a 2D CRF model for extraction on list pages, and use the same 150 detail pages to train another 2D CRF model for extraction on detail pages. [sent-603, score-0.708]
</p><p>80 In contrast, our approach integrates attribute labeling into block detection and can consider semantics during detecting data records. [sent-628, score-0.452]
</p><p>81 When not considering the semantics of the elements, H SNG and H S extract more noise blocks compared with H NG or HCRF, so the precisions of record detection decrease by 5. [sent-632, score-0.414]
</p><p>82 Attribute labeling beneﬁts from good quality records: one reason for this better performance is that attribute labeling can beneﬁt from the good results of record detection. [sent-771, score-0.603]
</p><p>83 For example, if a detected record is not a data record or misses some important information such as Name, attribute labeling will fail to ﬁnd the missed information or will ﬁnd some incorrect information. [sent-772, score-0.712]
</p><p>84 Global features help attribute labeling: another reason for the improvements in attribute labeling is the incorporation of the global features as in Section 5. [sent-775, score-0.48]
</p><p>85 3 Generalization Ability We report some empirical results to show the generalization ability of the integrated web data extraction models. [sent-795, score-0.63]
</p><p>86 We randomly pick 37 templates from LDST and for each template we collect 5 webpages for training and 10 webpages for testing. [sent-796, score-0.443]
</p><p>87 We can see that the integrated extraction models converge very quickly. [sent-800, score-0.391]
</p><p>88 As the number of templates increase in the training data, the extraction accuracy becomes higher and the variances become smaller. [sent-801, score-0.371]
</p><p>89 Results show that DHMRFs can (at least partially) overcome the blocky artifact issue in diverse web data extraction. [sent-809, score-0.393]
</p><p>90 Instead, as structure selection is integrated with labeling in DHMRFs, the dynamic model can properly group the attributes of the same object, and at the same time separate the attributes of different objects with the help of semantic labels. [sent-879, score-0.551]
</p><p>91 The less discriminative power of D-Trees causes additional errors in constructing model structures even for the non-intertwined cases, and thus hurts the accuracy of record detection and attribute labeling. [sent-900, score-0.439]
</p><p>92 , 4) of templates in the testing data are seen in the training data, the results on webpages generated from unseen templates do not change much. [sent-1047, score-0.455]
</p><p>93 The integrated HCRFs outperform the sequential HCRFs, which take record detection and attribute labeling as two separate steps as described in Section 6. [sent-1054, score-0.693]
</p><p>94 Conclusions and Future Work In this paper, we proposed an integrated web data extraction paradigm with hierarchical models. [sent-1095, score-0.81]
</p><p>95 1610  DYNAMIC H IERARCHICAL M ARKOV R ANDOM F IELDS  between variables, DHMRFs can potentially address the blocky artifact issue in diverse web data extraction. [sent-1100, score-0.393]
</p><p>96 We apply the models to a real-world web data extraction task. [sent-1104, score-0.539]
</p><p>97 Finally, extensive studies of the integrated extraction models in other complicated domains, like extracting researchers’ information (Zhu et al. [sent-1110, score-0.391]
</p><p>98 ROADRUNNER: Towards automatic data extraction from large web sites. [sent-1152, score-0.508]
</p><p>99 Simultaneous record detection and attribute labeling in web data extraction. [sent-1299, score-0.841]
</p><p>100 Dynamic hierarchical Markov random ﬁelds and their application to web data extraction. [sent-1307, score-0.45]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dhmrfs', 0.297), ('web', 0.27), ('hcrf', 0.252), ('extraction', 0.238), ('yh', 0.208), ('record', 0.198), ('hierarchical', 0.18), ('hcrfs', 0.168), ('sil', 0.168), ('webpages', 0.155), ('attribute', 0.141), ('jl', 0.138), ('dynamic', 0.137), ('templates', 0.133), ('labeling', 0.132), ('integrated', 0.122), ('contrastive', 0.114), ('fields', 0.111), ('ields', 0.11), ('ldst', 0.107), ('records', 0.106), ('ierarchical', 0.103), ('detection', 0.1), ('zhai', 0.099), ('webpage', 0.097), ('ye', 0.094), ('arkov', 0.087), ('ddst', 0.084), ('zhu', 0.084), ('ie', 0.083), ('hu', 0.079), ('block', 0.079), ('andom', 0.079), ('depta', 0.076), ('desc', 0.076), ('nodes', 0.075), ('hang', 0.074), ('crf', 0.074), ('blocks', 0.074), ('image', 0.07), ('price', 0.07), ('page', 0.07), ('sng', 0.069), ('blocky', 0.065), ('leaf', 0.064), ('intertwined', 0.061), ('sibling', 0.061), ('divergence', 0.06), ('en', 0.06), ('detail', 0.059), ('crfs', 0.058), ('artifact', 0.058), ('decoupled', 0.058), ('list', 0.057), ('semantic', 0.054), ('name', 0.054), ('lerman', 0.053), ('attributes', 0.053), ('liu', 0.052), ('wake', 0.052), ('jun', 0.052), ('labels', 0.051), ('label', 0.051), ('ni', 0.05), ('tag', 0.046), ('gatterbauer', 0.046), ('nie', 0.046), ('wen', 0.046), ('zaiqing', 0.046), ('product', 0.046), ('markov', 0.045), ('dependencies', 0.045), ('yl', 0.044), ('detected', 0.043), ('il', 0.043), ('extract', 0.042), ('tree', 0.041), ('cpts', 0.039), ('font', 0.038), ('sleep', 0.038), ('welling', 0.038), ('gk', 0.037), ('bo', 0.037), ('inner', 0.037), ('sites', 0.036), ('node', 0.036), ('html', 0.035), ('unseen', 0.034), ('element', 0.034), ('content', 0.033), ('kl', 0.033), ('features', 0.033), ('sharing', 0.032), ('eld', 0.032), ('incorporate', 0.031), ('models', 0.031), ('china', 0.031), ('distance', 0.031), ('crawled', 0.03), ('irving', 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="31-tfidf-1" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>Author: Jun Zhu, Zaiqing Nie, Bo Zhang, Ji-Rong Wen</p><p>Abstract: Existing template-independent web data extraction approaches adopt highly ineffective decoupled strategies—attempting to do data record detection and attribute labeling in two separate phases. In this paper, we propose an integrated web data extraction paradigm with hierarchical models. The proposed model is called Dynamic Hierarchical Markov Random Fields (DHMRFs). DHMRFs take structural uncertainty into consideration and deﬁne a joint distribution of both model structure and class labels. The joint distribution is an exponential family distribution. As a conditional model, DHMRFs relax the independence assumption as made in directed models. Since exact inference is intractable, a variational method is developed to learn the model’s parameters and to ﬁnd the MAP model structure and label assignments. We apply DHMRFs to a real-world web data extraction task. Experimental results show that: (1) integrated web data extraction models can achieve signiﬁcant improvements on both record detection and attribute labeling compared to decoupled models; (2) in diverse web data extraction DHMRFs can potentially address the blocky artifact issue which is suffered by ﬁxed-structured hierarchical models. Keywords: conditional random ﬁelds, dynamic hierarchical Markov random ﬁelds, integrated web data extraction, statistical hierarchical modeling, blocky artifact issue</p><p>2 0.095295817 <a title="31-tfidf-2" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>Author: Abraham George, Warren B. Powell, Sanjeev R. Kulkarni</p><p>Abstract: We consider the problem of estimating the value of a multiattribute resource, where the attributes are categorical or discrete in nature and the number of potential attribute vectors is very large. The problem arises in approximate dynamic programming when we need to estimate the value of a multiattribute resource from estimates based on Monte-Carlo simulation. These problems have been traditionally solved using aggregation, but choosing the right level of aggregation requires resolving the classic tradeoff between aggregation error and sampling error. We propose a method that estimates the value of a resource at different levels of aggregation simultaneously, and then uses a weighted combination of the estimates. Using the optimal weights, which minimizes the variance of the estimate while accounting for correlations between the estimates, is computationally too expensive for practical applications. We have found that a simple inverse variance formula (adjusted for bias), which effectively assumes the estimates are independent, produces near-optimal estimates. We use the setting of two levels of aggregation to explain why this approximation works so well. Keywords: hierarchical statistics, approximate dynamic programming, mixture models, adaptive learning, multiattribute resources</p><p>3 0.08171311 <a title="31-tfidf-3" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>Author: Thomas G. Dietterich, Guohua Hao, Adam Ashenfelter</p><p>Abstract: Conditional random ﬁelds (CRFs) provide a ﬂexible and powerful model for sequence labeling problems. However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features and feature combinations. This paper describes a new algorithm for training CRFs via gradient tree boosting. In tree boosting, the CRF potential functions are represented as weighted sums of regression trees, which provide compact representations of feature interactions. So the algorithm does not explicitly consider the potentially large parameter space. As a result, gradient tree boosting scales linearly in the order of the Markov model and in the order of the feature interactions, rather than exponentially as in previous algorithms based on iterative scaling and gradient descent. Gradient tree boosting also makes it possible to use instance weighting (as in C4.5) and surrogate splitting (as in CART) to handle missing values. Experimental studies of the effectiveness of these two methods (as well as standard imputation and indicator feature methods) show that instance weighting is the best method in most cases when feature values are missing at random. Keywords: sequential supervised learning, conditional random ﬁelds, functional gradient, gradient tree boosting, missing values</p><p>4 0.078332029 <a title="31-tfidf-4" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>5 0.069945417 <a title="31-tfidf-5" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>Author: Amit Dhurandhar, Alin Dobra</p><p>Abstract: In this paper we use the methodology introduced by Dhurandhar and Dobra (2009) for analyzing the error of classiﬁers and the model selection measures, to analyze decision tree algorithms. The methodology consists of obtaining parametric expressions for the moments of the generalization error (GE) for the classiﬁcation model of interest, followed by plotting these expressions for interpretability. The major challenge in applying the methodology to decision trees, the main theme of this work, is customizing the generic expressions for the moments of GE to this particular classiﬁcation algorithm. The speciﬁc contributions we make in this paper are: (a) we primarily characterize a subclass of decision trees namely, Random decision trees, (b) we discuss how the analysis extends to other decision tree algorithms and (c) in order to extend the analysis to certain model selection measures, we generalize the relationships between the moments of GE and moments of the model selection measures given in (Dhurandhar and Dobra, 2009) to randomized classiﬁcation algorithms. An empirical comparison of the proposed method with Monte Carlo and distribution free bounds obtained using Breiman’s formula, depicts the advantages of the method in terms of running time and accuracy. It thus showcases the use of the deployed methodology as an exploratory tool to study learning algorithms. Keywords: moments, generalization error, decision trees</p><p>6 0.068297789 <a title="31-tfidf-6" href="./jmlr-2008-Stationary_Features_and_Cat_Detection.html">87 jmlr-2008-Stationary Features and Cat Detection</a></p>
<p>7 0.067318074 <a title="31-tfidf-7" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>8 0.059676509 <a title="31-tfidf-8" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>9 0.058759212 <a title="31-tfidf-9" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>10 0.050598327 <a title="31-tfidf-10" href="./jmlr-2008-On_the_Equivalence_of_Linear_Dimensionality-Reducing_Transformations.html">71 jmlr-2008-On the Equivalence of Linear Dimensionality-Reducing Transformations</a></p>
<p>11 0.050294053 <a title="31-tfidf-11" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>12 0.049586419 <a title="31-tfidf-12" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>13 0.048694253 <a title="31-tfidf-13" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>14 0.04847889 <a title="31-tfidf-14" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>15 0.04615961 <a title="31-tfidf-15" href="./jmlr-2008-HPB%3A_A_Model_for_Handling_BN_Nodes_with_High_Cardinality_Parents.html">42 jmlr-2008-HPB: A Model for Handling BN Nodes with High Cardinality Parents</a></p>
<p>16 0.046010077 <a title="31-tfidf-16" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>17 0.043731894 <a title="31-tfidf-17" href="./jmlr-2008-Linear-Time_Computation_of_Similarity_Measures_for_Sequential_Data.html">55 jmlr-2008-Linear-Time Computation of Similarity Measures for Sequential Data</a></p>
<p>18 0.043601841 <a title="31-tfidf-18" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>19 0.0435686 <a title="31-tfidf-19" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>20 0.042663451 <a title="31-tfidf-20" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.204), (1, 0.026), (2, 0.051), (3, 0.013), (4, -0.194), (5, 0.081), (6, 0.087), (7, 0.035), (8, 0.012), (9, 0.012), (10, -0.092), (11, 0.201), (12, -0.173), (13, -0.014), (14, -0.087), (15, -0.285), (16, -0.08), (17, -0.127), (18, 0.094), (19, -0.068), (20, 0.091), (21, -0.016), (22, -0.025), (23, -0.067), (24, 0.153), (25, 0.07), (26, 0.023), (27, -0.093), (28, 0.061), (29, -0.027), (30, -0.023), (31, 0.177), (32, 0.035), (33, -0.078), (34, 0.003), (35, 0.07), (36, 0.104), (37, 0.096), (38, 0.043), (39, -0.011), (40, 0.053), (41, -0.063), (42, 0.081), (43, -0.043), (44, 0.018), (45, -0.048), (46, -0.138), (47, 0.02), (48, 0.04), (49, -0.16)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95211679 <a title="31-lsi-1" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>Author: Jun Zhu, Zaiqing Nie, Bo Zhang, Ji-Rong Wen</p><p>Abstract: Existing template-independent web data extraction approaches adopt highly ineffective decoupled strategies—attempting to do data record detection and attribute labeling in two separate phases. In this paper, we propose an integrated web data extraction paradigm with hierarchical models. The proposed model is called Dynamic Hierarchical Markov Random Fields (DHMRFs). DHMRFs take structural uncertainty into consideration and deﬁne a joint distribution of both model structure and class labels. The joint distribution is an exponential family distribution. As a conditional model, DHMRFs relax the independence assumption as made in directed models. Since exact inference is intractable, a variational method is developed to learn the model’s parameters and to ﬁnd the MAP model structure and label assignments. We apply DHMRFs to a real-world web data extraction task. Experimental results show that: (1) integrated web data extraction models can achieve signiﬁcant improvements on both record detection and attribute labeling compared to decoupled models; (2) in diverse web data extraction DHMRFs can potentially address the blocky artifact issue which is suffered by ﬁxed-structured hierarchical models. Keywords: conditional random ﬁelds, dynamic hierarchical Markov random ﬁelds, integrated web data extraction, statistical hierarchical modeling, blocky artifact issue</p><p>2 0.44048318 <a title="31-lsi-2" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>Author: Abraham George, Warren B. Powell, Sanjeev R. Kulkarni</p><p>Abstract: We consider the problem of estimating the value of a multiattribute resource, where the attributes are categorical or discrete in nature and the number of potential attribute vectors is very large. The problem arises in approximate dynamic programming when we need to estimate the value of a multiattribute resource from estimates based on Monte-Carlo simulation. These problems have been traditionally solved using aggregation, but choosing the right level of aggregation requires resolving the classic tradeoff between aggregation error and sampling error. We propose a method that estimates the value of a resource at different levels of aggregation simultaneously, and then uses a weighted combination of the estimates. Using the optimal weights, which minimizes the variance of the estimate while accounting for correlations between the estimates, is computationally too expensive for practical applications. We have found that a simple inverse variance formula (adjusted for bias), which effectively assumes the estimates are independent, produces near-optimal estimates. We use the setting of two levels of aggregation to explain why this approximation works so well. Keywords: hierarchical statistics, approximate dynamic programming, mixture models, adaptive learning, multiattribute resources</p><p>3 0.41998214 <a title="31-lsi-3" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>Author: Amit Dhurandhar, Alin Dobra</p><p>Abstract: In this paper we use the methodology introduced by Dhurandhar and Dobra (2009) for analyzing the error of classiﬁers and the model selection measures, to analyze decision tree algorithms. The methodology consists of obtaining parametric expressions for the moments of the generalization error (GE) for the classiﬁcation model of interest, followed by plotting these expressions for interpretability. The major challenge in applying the methodology to decision trees, the main theme of this work, is customizing the generic expressions for the moments of GE to this particular classiﬁcation algorithm. The speciﬁc contributions we make in this paper are: (a) we primarily characterize a subclass of decision trees namely, Random decision trees, (b) we discuss how the analysis extends to other decision tree algorithms and (c) in order to extend the analysis to certain model selection measures, we generalize the relationships between the moments of GE and moments of the model selection measures given in (Dhurandhar and Dobra, 2009) to randomized classiﬁcation algorithms. An empirical comparison of the proposed method with Monte Carlo and distribution free bounds obtained using Breiman’s formula, depicts the advantages of the method in terms of running time and accuracy. It thus showcases the use of the deployed methodology as an exploratory tool to study learning algorithms. Keywords: moments, generalization error, decision trees</p><p>4 0.39674997 <a title="31-lsi-4" href="./jmlr-2008-Stationary_Features_and_Cat_Detection.html">87 jmlr-2008-Stationary Features and Cat Detection</a></p>
<p>Author: FranĂ§ois Fleuret, Donald Geman</p><p>Abstract: Most discriminative techniques for detecting instances from object categories in still images consist of looping over a partition of a pose space with dedicated binary classiÄ?Ĺš ers. The efÄ?Ĺš ciency of this strategy for a complex pose, that is, for Ä?Ĺš ne-grained descriptions, can be assessed by measuring the effect of sample size and pose resolution on accuracy and computation. Two conclusions emerge: (1) fragmenting the training data, which is inevitable in dealing with high in-class variation, severely reduces accuracy; (2) the computational cost at high resolution is prohibitive due to visiting a massive pose partition. To overcome data-fragmentation we propose a novel framework centered on pose-indexed features which assign a response to a pair consisting of an image and a pose, and are designed to be stationary: the probability distribution of the response is always the same if an object is actually present. Such features allow for efÄ?Ĺš cient, one-shot learning of pose-speciÄ?Ĺš c classiÄ?Ĺš ers. To avoid expensive scene processing, we arrange these classiÄ?Ĺš ers in a hierarchy based on nested partitions of the pose; as in previous work on coarse-to-Ä?Ĺš ne search, this allows for efÄ?Ĺš cient processing. The hierarchy is then Ă˘&euro;?foldedĂ˘&euro;? for training: all the classiÄ?Ĺš ers at each level are derived from one base predictor learned from all the data. The hierarchy is Ă˘&euro;?unfoldedĂ˘&euro;? for testing: parsing a scene amounts to examining increasingly Ä?Ĺš ner object descriptions only when there is sufÄ?Ĺš cient evidence for coarser ones. In this way, the detection results are equivalent to an exhaustive search at high resolution. We illustrate these ideas by detecting and localizing cats in highly cluttered greyscale scenes. Keywords: supervised learning, computer vision, image interpretation, cats, stationary features, hierarchical search</p><p>5 0.39482537 <a title="31-lsi-5" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>Author: Thomas G. Dietterich, Guohua Hao, Adam Ashenfelter</p><p>Abstract: Conditional random ﬁelds (CRFs) provide a ﬂexible and powerful model for sequence labeling problems. However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features and feature combinations. This paper describes a new algorithm for training CRFs via gradient tree boosting. In tree boosting, the CRF potential functions are represented as weighted sums of regression trees, which provide compact representations of feature interactions. So the algorithm does not explicitly consider the potentially large parameter space. As a result, gradient tree boosting scales linearly in the order of the Markov model and in the order of the feature interactions, rather than exponentially as in previous algorithms based on iterative scaling and gradient descent. Gradient tree boosting also makes it possible to use instance weighting (as in C4.5) and surrogate splitting (as in CART) to handle missing values. Experimental studies of the effectiveness of these two methods (as well as standard imputation and indicator feature methods) show that instance weighting is the best method in most cases when feature values are missing at random. Keywords: sequential supervised learning, conditional random ﬁelds, functional gradient, gradient tree boosting, missing values</p><p>6 0.36268571 <a title="31-lsi-6" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>7 0.34316263 <a title="31-lsi-7" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>8 0.30118582 <a title="31-lsi-8" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>9 0.28549618 <a title="31-lsi-9" href="./jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations.html">37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</a></p>
<p>10 0.26152593 <a title="31-lsi-10" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>11 0.26077962 <a title="31-lsi-11" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>12 0.25309238 <a title="31-lsi-12" href="./jmlr-2008-HPB%3A_A_Model_for_Handling_BN_Nodes_with_High_Cardinality_Parents.html">42 jmlr-2008-HPB: A Model for Handling BN Nodes with High Cardinality Parents</a></p>
<p>13 0.24948195 <a title="31-lsi-13" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>14 0.23821518 <a title="31-lsi-14" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>15 0.23187506 <a title="31-lsi-15" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>16 0.21355884 <a title="31-lsi-16" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>17 0.21067883 <a title="31-lsi-17" href="./jmlr-2008-Linear-Time_Computation_of_Similarity_Measures_for_Sequential_Data.html">55 jmlr-2008-Linear-Time Computation of Similarity Measures for Sequential Data</a></p>
<p>18 0.20856014 <a title="31-lsi-18" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>19 0.19880751 <a title="31-lsi-19" href="./jmlr-2008-Regularization_on_Graphs_with_Function-adapted_Diffusion_Processes.html">81 jmlr-2008-Regularization on Graphs with Function-adapted Diffusion Processes</a></p>
<p>20 0.18812586 <a title="31-lsi-20" href="./jmlr-2008-On_the_Equivalence_of_Linear_Dimensionality-Reducing_Transformations.html">71 jmlr-2008-On the Equivalence of Linear Dimensionality-Reducing Transformations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.018), (5, 0.032), (9, 0.014), (10, 0.378), (24, 0.01), (31, 0.017), (40, 0.042), (54, 0.036), (58, 0.035), (66, 0.046), (76, 0.031), (78, 0.011), (88, 0.079), (92, 0.055), (94, 0.049), (99, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77205694 <a title="31-lda-1" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>Author: Jun Zhu, Zaiqing Nie, Bo Zhang, Ji-Rong Wen</p><p>Abstract: Existing template-independent web data extraction approaches adopt highly ineffective decoupled strategies—attempting to do data record detection and attribute labeling in two separate phases. In this paper, we propose an integrated web data extraction paradigm with hierarchical models. The proposed model is called Dynamic Hierarchical Markov Random Fields (DHMRFs). DHMRFs take structural uncertainty into consideration and deﬁne a joint distribution of both model structure and class labels. The joint distribution is an exponential family distribution. As a conditional model, DHMRFs relax the independence assumption as made in directed models. Since exact inference is intractable, a variational method is developed to learn the model’s parameters and to ﬁnd the MAP model structure and label assignments. We apply DHMRFs to a real-world web data extraction task. Experimental results show that: (1) integrated web data extraction models can achieve signiﬁcant improvements on both record detection and attribute labeling compared to decoupled models; (2) in diverse web data extraction DHMRFs can potentially address the blocky artifact issue which is suffered by ﬁxed-structured hierarchical models. Keywords: conditional random ﬁelds, dynamic hierarchical Markov random ﬁelds, integrated web data extraction, statistical hierarchical modeling, blocky artifact issue</p><p>2 0.34501642 <a title="31-lda-2" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>3 0.32710937 <a title="31-lda-3" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>Author: Elisa Ricci, Tijl De Bie, Nello Cristianini</p><p>Abstract: Most approaches to structured output prediction rely on a hypothesis space of prediction functions that compute their output by maximizing a linear scoring function. In this paper we present two novel learning algorithms for this hypothesis class, and a statistical analysis of their performance. The methods rely on efﬁciently computing the ﬁrst two moments of the scoring function over the output space, and using them to create convex objective functions for training. We report extensive experimental results for sequence alignment, named entity recognition, and RNA secondary structure prediction. Keywords: structured output prediction, discriminative learning, Z-score, discriminant analysis, PAC bound</p><p>4 0.32692236 <a title="31-lda-4" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>Author: Shann-Ching Chen, Geoffrey J. Gordon, Robert F. Murphy</p><p>Abstract: In structured classiﬁcation problems, there is a direct conﬂict between expressive models and efﬁcient inference: while graphical models such as Markov random ﬁelds or factor graphs can represent arbitrary dependences among instance labels, the cost of inference via belief propagation in these models grows rapidly as the graph structure becomes more complicated. One important source of complexity in belief propagation is the need to marginalize large factors to compute messages. This operation takes time exponential in the number of variables in the factor, and can limit the expressiveness of the models we can use. In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efﬁcient algorithms for computing messages from these potentials during belief propagation. We believe these new potentials provide a good balance between expressive power and efﬁcient inference in practical structured classiﬁcation problems. We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. We use these potentials to classify images of protein subcellular location patterns in groups of cells. Classifying subcellular location patterns can help us answer many important questions in computational biology, including questions about how various treatments affect the synthesis and behavior of proteins and networks of proteins within a cell. Our new representation and algorithm lead to substantial improvements in both inference speed and classiﬁcation accuracy. Keywords: factor graphs, approximate inference algorithms, structured classiﬁcation, protein subcellular location patterns, location proteomics</p><p>5 0.3268787 <a title="31-lda-5" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>Author: Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel, Daphne Koller</p><p>Abstract: We consider the problem of learning classiﬁers in structured domains, where some objects have a subset of features that are inherently absent due to complex relationships between the features. Unlike the case where a feature exists but its value is not observed, here we focus on the case where a feature may not even exist (structurally absent) for some of the samples. The common approach for handling missing features in discriminative models is to ﬁrst complete their unknown values, and then use a standard classiﬁcation procedure over the completed data. This paper focuses on features that are known to be non-existing, rather than have an unknown value. We show how incomplete data can be classiﬁed directly without any completion of the missing features using a max-margin learning framework. We formulate an objective function, based on the geometric interpretation of the margin, that aims to maximize the margin of each sample in its own relevant subspace. In this formulation, the linearly separable case can be transformed into a binary search over a series of second order cone programs (SOCP), a convex problem that can be solved efﬁciently. We also describe two approaches for optimizing the general case: an approximation that can be solved as a standard quadratic program (QP) and an iterative approach for solving the exact problem. By avoiding the pre-processing phase in which the data is completed, both of these approaches could offer considerable computational savings. More importantly, we show that the elegant handling of missing values by our approach allows it to both outperform other methods when the missing values have non-trivial structure, and be competitive with other methods when the values are missing at random. We demonstrate our results on several standard benchmarks and two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images. Keywords: max margin, missing features, network reconstruction, metabolic pa</p><p>6 0.32669464 <a title="31-lda-6" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>7 0.32633689 <a title="31-lda-7" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>8 0.32569316 <a title="31-lda-8" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>9 0.32566634 <a title="31-lda-9" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>10 0.32500219 <a title="31-lda-10" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>11 0.32471794 <a title="31-lda-11" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>12 0.32182589 <a title="31-lda-12" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>13 0.32152739 <a title="31-lda-13" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>14 0.32145101 <a title="31-lda-14" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>15 0.31887811 <a title="31-lda-15" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>16 0.31850201 <a title="31-lda-16" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>17 0.31769311 <a title="31-lda-17" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>18 0.31735003 <a title="31-lda-18" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>19 0.31700492 <a title="31-lda-19" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>20 0.3145 <a title="31-lda-20" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
