<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-12" href="#">jmlr2008-12</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</h1>
<br/><p>Source: <a title="jmlr-2008-12-pdf" href="http://jmlr.org/papers/volume9/balakrishnan08a/balakrishnan08a.pdf">pdf</a></p><p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>Reference: <a title="jmlr-2008-12-reference" href="../jmlr2008_reference/jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. [sent-8, score-0.339]
</p><p>2 These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. [sent-9, score-0.224]
</p><p>3 We concern ourselves speciﬁcally with binary classiﬁcation and consider L1 -regularized logistic and probit regression models. [sent-14, score-0.279]
</p><p>4 Due to matrix multiplications on t × t or d × d matrices, typical computational time requirements are O(t 3 + d 3 ), with memory requirements that are O(td + d 2 ). [sent-23, score-0.213]
</p><p>5 BALAKRISHNAN AND M ADIGAN  This paper presents two basic algorithms for learning L1 -logistic and/or probit regression models. [sent-26, score-0.174]
</p><p>6 The ﬁrst algorithm we present is an online algorithm which sequentially processes each observation only once. [sent-28, score-0.374]
</p><p>7 This multi-pass algorithm (the MP algorithm) also processes data sequentially but makes a small constant number of extra passes over the data set. [sent-32, score-0.199]
</p><p>8 Hence, this sequential algorithm provides results similar to those of batch algorithms for this problem. [sent-33, score-0.268]
</p><p>9 The MP algorithm’s computational cost is a constant factor higher and memory costs are essentially the same as those of the online algorithm. [sent-34, score-0.439]
</p><p>10 Finally, we propose the RMMP (Reduced Memory MP) algorithm that has signiﬁcantly lower worst case memory costs, O(d + k 2 ) (where k d) and the same computational costs as the MP algorithm (thus both computational and memory costs are essentially linear in t and d). [sent-35, score-0.492]
</p><p>11 We will comment on the similarities and differences of our technique to other learning algorithms, in particular other online algorithms, in the following sections. [sent-36, score-0.238]
</p><p>12 We restrict our Rz 2 analytical results to the two most commonly used link functions, the probit Φ(z) = −∞ √1 e−x /2 dx 2π z  e and logistic Φ(z) = 1+ez link functions. [sent-47, score-0.403]
</p><p>13 The ﬁrst term on the right hand side is the likelihood: t  t  i=1  i=1  ∏ p(yi |β) = ∏  yi Φ(βT xi ) + (1 − yi )(1 − Φ(βT xi ) . [sent-52, score-0.172]
</p><p>14 314  S PARSE C LASSIFIERS FOR M ASSIVE DATA  Finding the MAP β leads to the optimization problem we wish to solve (now on the log scale): max (log p(β|Dt )) β t  ≡  max β  ∑ log  i=1  yi Φ(βT xi ) + (1 − yi )(1 − Φ(βT xi ) − log p(β) . [sent-53, score-0.253]
</p><p>15 , 1999): max (log p(β|Dt )) β t  ≡  max β  ∑ log  i=1  yi Φ(βT xi ) + (1 − yi )(1 − Φ(βT xi ) − γ β  1  . [sent-63, score-0.199]
</p><p>16 Methods such as cross validation can be used to pick its value and algorithms also exist to ﬁnd solutions for all values of the regularization parameter (commonly called regularization path algorithms). [sent-69, score-0.196]
</p><p>17 To the best of our knowledge, all existing algorithms solve the above convex optimization problem in the batch setting, that is, by storing the data set Dt in memory and iterating over it (Fu, 1998; Osborne et al. [sent-84, score-0.377]
</p><p>18 Consequently, these algorithms cannot be used in the massive data/online scenario, where memory costs dependent on t represent a signiﬁcant practical impediment. [sent-88, score-0.227]
</p><p>19 Approximating the Likelihood for Online Learning The Bayesian paradigm supports online learning in a natural fashion; starting from the prior, the ﬁrst training example produces a posterior distribution incorporating the evidence from the ﬁrst example. [sent-91, score-0.328]
</p><p>20 We want to avoid algorithms that begin with a “load data into memory” step and also avoid memory costs that increase with increasing amounts of data. [sent-94, score-0.208]
</p><p>21 In other words, we want memory costs independent of t. [sent-95, score-0.174]
</p><p>22 In either case the approximation uses a simple Taylor expansion around βT xi , where βi−1 estimates the posterior mode given the ﬁrst i − 1 examples, D i−1 i−1 (Appendix A provides expressions for ai , bi for the probit and logistic link functions). [sent-102, score-0.652]
</p><p>23 We then have: t  ∑ log(p(yi |β))  i=1  t  ∑  ≈  ai (βT xi )2 + bi (βT xi ) + ci  i=1 t  t  t  i=1  i=1  ∑ ai (βT xi )(xT β) + ∑ bi (βT xi ) + ∑ ci i  =  i=1  t  = βT Ψt β + βT θt + ∑ ci i=1  where: t  t  i=1  i=1  Ψt = ∑ ai xi xT , and θt = ∑ bi xi . [sent-103, score-0.612]
</p><p>24 Further, the ﬁxed size d × d matrix Ψ and the d × 1 vector θ can be updated in an online fashion as data accumulate: T Ψt+1 = Ψt + at+1 xt+1 xt+1 , and θt+1 = θt + bt+1 xt+1 . [sent-106, score-0.238]
</p><p>25 Also, the scheme as set up requires O(d 2 ) memory in the worst case. [sent-113, score-0.177]
</p><p>26 An Online Algorithm The quadratic approximation and the Shooting algorithm lead straightforwardly to an online algorithm. [sent-151, score-0.353]
</p><p>27 Calculate the quadratic Taylor series approximation to each observation’s log-likelihood at the current estimate of the posterior mode, β i−1 , thus ﬁnding parameters ai , bi . [sent-153, score-0.241]
</p><p>28 We show the performance of the online algorithm on a low dimensional simulated data set in Figure 3 (the data generating mechanism is a logistic regression model with d = 11, and t = 100, 000. [sent-156, score-0.465]
</p><p>29 As we process greater numbers of observations, the online estimates (the solid lines) improve, that is, get closer to the batch 319  BALAKRISHNAN AND M ADIGAN  Algorithm 2: The Online algorithm. [sent-158, score-0.539]
</p><p>30 Obtain quadratic approximation to term likelihood at βi−1 , that is, obtain ai , bi . [sent-166, score-0.213]
</p><p>31 2007, publicly available software for batch L1 penalized logistic regression). [sent-171, score-0.325]
</p><p>32 Figure 4 shows individual plots of the online and batch estimates for four representative components of MAP βi in blue. [sent-173, score-0.591]
</p><p>33 We also plot the absolute difference between the batch and online estimates in green (dotted line) on the same plot on the right (green) axis. [sent-174, score-0.567]
</p><p>34 016  Table 1: Table with columns showing values of βtrue , and the MAP estimates of β obtained by the batch algorithm and the online algorithm, for increasing amounts of data on the simulated data set. [sent-249, score-0.659]
</p><p>35 To aid assessing convergence of the online to the batch estimates, we show the value of the L1 norm of the adjacent vectors (batch vs. [sent-250, score-0.488]
</p><p>36 In the worst case, the online algorithm requires O(d 2 ) space and O(d 2 ) computational time to compute the MAP β for each new observation. [sent-253, score-0.334]
</p><p>37 5  0  2  4  6  8  10 4  x 10  Figure 3: Performance of the online algorithm on a simulated data set, with regularization parameter γ = 100 (see text for details). [sent-256, score-0.469]
</p><p>38 Although the practical memory costs of the algorithm will likely be less than O(d 2 ), exactly how much less depends heavily on the data, since Ψ (the part of the sketch dominating the memory requirements) is a weighted sum of outer products of the x i ’s. [sent-262, score-0.385]
</p><p>39 Here, we highlight the fact that the online algorithm is accurate and practical if the problem is of low to medium input dimension, but massive in terms of the number of observations. [sent-264, score-0.339]
</p><p>40 1 Heuristics for Improvement/Issues While one can also obtain parameter estimates for ﬁxed t ( batch problems) using the online algorithm, multiple passes typically provide better estimates, albeit with increased computational cost. [sent-267, score-0.682]
</p><p>41 Since the online algorithm typically initializes itself far from β∗ , it is only after processing a sufﬁcient number of examples that the online algorithm’s term approximations will start being taken closer to β ∗ . [sent-269, score-0.553]
</p><p>42 The panels show four representative parameters from that ﬁgure, also showing tapering L1 loss (dotted green line) between the online and batch algorithm estimates on the right axis (in green). [sent-286, score-0.615]
</p><p>43 Hence, if the online algorithm is initialized at β0 = 0, for any i < t, the output MAP estimate βi will be more shrunk towards zero than β∗ . [sent-291, score-0.286]
</p><p>44 Figure 3 illustrates this for smaller values of t where the solid lines (approximate MAP estimates) are closer to zero than the dashed lines (exact batch estimates). [sent-292, score-0.22]
</p><p>45 This suggests the following two heuristics to improve the quality of estimates from the online algorithm. [sent-293, score-0.319]
</p><p>46 The second heuristic is for the online algorithm to keep a block of observations in memory temporarily instead of immediately discarding each observation after processing it. [sent-296, score-0.415]
</p><p>47 322  S PARSE C LASSIFIERS FOR M ASSIVE DATA  then uses the value of the parameter estimates after having seen/processed all the observations in a block to update the sketches for the whole block. [sent-300, score-0.229]
</p><p>48 In experiments not reported here, both of these heuristics improve the ﬁnal online estimates somewhat. [sent-302, score-0.319]
</p><p>49 One possibility for improving upon the O(d 2 ) worst case computational requirement of the online algorithm is as follows. [sent-303, score-0.365]
</p><p>50 In the inﬁnite data case, in order to obtain sparsity in parameter estimates, the amount of regularization must be allowed to increase as observations accumulate— an increasingly weighty likelihood term will inundate any ﬁxed amount of regularization. [sent-304, score-0.18]
</p><p>51 The worst case O(d 2 ) memory requirement of the online algorithm, however, presents a greater challenge. [sent-308, score-0.446]
</p><p>52 In the next section we outline a multi-pass algorithm based on the same sequential quadratic approximation that improves the accuracy of estimates when applied to ﬁnite data sets and also uses less memory than the online algorithm. [sent-309, score-0.563]
</p><p>53 A Multi-pass Algorithm The block heuristic of the previous section implies that taking all term approximations at the ﬁnal online algorithm MAP βt value would certainly produce better estimates of Ψt , θt . [sent-311, score-0.396]
</p><p>54 Note that consequently there is no need for the shooting algorithm during the pass through the data set. [sent-316, score-0.554]
</p><p>55 For a constant number of passes, the MP algorithm has the worst case computational time requirement of O(td 2 ) to do an equivalent batch MAP β estimation. [sent-319, score-0.347]
</p><p>56 Once again, if the data set is sparse, this cost is closer in practice to O(t f 2 + md) (the ﬁrst term is the cost of updating the sketches and the second md term is the cost of the Shooting algorithm). [sent-320, score-0.21]
</p><p>57 The worst case memory requirement of the MP algorithm is O(d 2 ), which is just a constant with respect to t. [sent-321, score-0.256]
</p><p>58 Expectation Propagation (Minka, 2001b) by contrast requires explicitly storing term approximations and thus has memory costs that scale linearly with t, that is, O(t). [sent-322, score-0.203]
</p><p>59 The next subsection presents a modiﬁcation of the MP algorithm that reduces this worst case memory requirement. [sent-323, score-0.225]
</p><p>60 1 A Reduced Memory Multi-pass Algorithm The key to reducing the memory requirements of the algorithm in the previous subsection is exploiting the sparsity of β∗ . [sent-325, score-0.253]
</p><p>61 That is, the active set is the set of variables that are either nonzero and optimal or variables that violate optimality at the start of a pass (the corresponding nonzero elements of the vectors/matrices are denoted by their previous symbols but with a ˜ above ˜ them). [sent-333, score-0.241]
</p><p>62 The search for the optimal parameter values is slightly more involved though, now proceeding iteratively by ﬁrst identifying candidate nonzero components of βMAP , and then reﬁning the estimates for these components. [sent-338, score-0.208]
</p><p>63 Obtain quadratic approximation to term likelihood at βz−1 , that is, obtain ai , bi . [sent-349, score-0.213]
</p><p>64 end Note that memory requirements are now O(d + k 2 ), where k is the number of variables in the largest active set. [sent-356, score-0.235]
</p><p>65 The worst case computational time requirements for a constant number of passes, are still O(td 2 ) to do an equivalent batch MAP β estimation. [sent-359, score-0.31]
</p><p>66 Under the same sparsity assumptions as in previous sections, in practice this cost is better quantiﬁed as O(t(k 2 + f 2 ) + kd) (again, the ﬁrst term is the cost associated with updating the sketches and the second term is the cost of Shooting). [sent-360, score-0.185]
</p><p>67 This is because no outer products are computed, since the active set is initialized as the empty set; the ﬁrst pass is used simply to determine the size and components of the active set and the parameter estimates for the next iteration are still zero, β1 = 0. [sent-364, score-0.35]
</p><p>68 Typically, setting the reduced memory parameter k to be larger than this ﬁrst active set size results in further RMMP iterations mimicking iterations of the MP algorithm. [sent-365, score-0.225]
</p><p>69 Both of these observations together imply that if we start the RMMP algorithm with enough memory allotted to look at all possibly relevant β components, we will follow the MP search path (as a motivating example, consider that setting k = d results in the MP algorithm exactly). [sent-369, score-0.225]
</p><p>70 We make probit regression comparisons to results obtained using a batch EM algorithm for Laplacian prior based probit regression (we implemented a slightly modiﬁed version of the algorithm in Figueiredo and Jain, 2001). [sent-389, score-0.692]
</p><p>71 In so doing and by obtaining essentially identical parameter estimates to batch algorithms, our predictive performance will mirror those of the batch algorithms. [sent-392, score-0.596]
</p><p>72 The data generating mechanism is either a probit or logistic regression model with one intercept term and 10 model coefﬁcients, for a total of 11 ﬁxed parameters. [sent-399, score-0.279]
</p><p>73 For the experiments with the online algorithm (Figure 3, Table 1), we used the same model parameters as above, but with t = 100, 000 and only a logistic regression model. [sent-405, score-0.427]
</p><p>74 1 Results The low dimensional simulated data set highlights typical results we obtain with the Online algorithm and the MP algorithm (the RMMP algorithm is not of practical signiﬁcance in this case). [sent-416, score-0.182]
</p><p>75 The parameter estimates from the Online algorithm are quite close to batch estimates, likely due to the relatively large data set size (t being large relative to d). [sent-419, score-0.381]
</p><p>76 Also, with very few passes over the data set, denoted as before by the variable z, we obtain parameter estimates practically identical to those obtained by the batch algorithm. [sent-420, score-0.444]
</p><p>77 To show this, the tables report results for both too little regularization (γ = 10, probit link) and too much regularization (γ = 100, logistic link) for this particular data set. [sent-425, score-0.407]
</p><p>78 As a guide to assessing convergence in this and other tables that follow, we show the L1 norm of the difference between the batch algorithm estimates (EM or BBR as appropriate) and the Online, MP or RMMP algorithm iterates (also as appropriate). [sent-426, score-0.427]
</p><p>79 The batch EM algorithm for probit regression is prohibitively expensive on this data set as it involves inverting a high dimensional matrix, but we can run BBR to obtain batch logistic regression results. [sent-432, score-0.803]
</p><p>80 For both amounts of regularization the Online parameter estimates aren’t particularly good (although between the two settings, the parameter estimates with the higher amount of regularization are better). [sent-437, score-0.424]
</p><p>81 The ﬁnal row displays the L1 norm of the difference between the batch algorithm estimates (EM or BBR as appropriate) and the Online/MP algorithm estimates. [sent-531, score-0.427]
</p><p>82 We point out that this is a huge reduction in the worst case memory required, an approximately 98% reduction (k = 3000 vs. [sent-537, score-0.177]
</p><p>83 The RMMP algorithm performs very well, requiring about z = 7 passes (only two more than the MP algorithm) to converge to correct parameter values. [sent-541, score-0.191]
</p><p>84 For γ = 10, where k = 300 is small (only twice the number of non-zero components in the MAP β), once again the same kind of results hold, with the MP algorithm needing about 7 passes over the data set and the RMMP algorithm needing about 15 passes to converge to the batch β. [sent-542, score-0.646]
</p><p>85 For the RCV1-v2 training data (d = 47, 236, t = 23, 149), sparsity again enables application of BBR to obtain the batch MAP β parameter values, as well as the Online and MP algorithms, although this is quite cumbersome. [sent-544, score-0.316]
</p><p>86 In table are the indices of β (and word stem features they correspond to in brackets), coefﬁcients from BBR, and the Online algorithm, those obtained after a particular number of passes over the data using the MP algorithm (full memory) and parameters from the RMMP algorithm with k = 300. [sent-650, score-0.236]
</p><p>87 For the BIG-RCV data set (d = 288, 062, t = 421, 816) however, computational and memory limitations made it impossible to run the batch algorithms on this data set (also the Online and MP algorithm). [sent-821, score-0.349]
</p><p>88 It is precisely for cases like this that the RMMP algorithm is useful, and we were able to obtain parameter estimates for reasonable settings of regularization—see for example, the right portion of Table 5. [sent-822, score-0.206]
</p><p>89 We obtained the best possible predictive parameters using 10-fold cross-validation on the RCV1-v2 training data set with a batch algorithm. [sent-825, score-0.293]
</p><p>90 We then trained a separate sparse logistic classiﬁer on the BIG-RCV data set using the RMMP algorithm with k = 3000 and γ = 40. [sent-830, score-0.196]
</p><p>91 Conclusions In this paper we presented an asymptotically convergent online algorithm that builds sparse generalized linear models for massive data sets. [sent-835, score-0.382]
</p><p>92 of-the-art batch algorithms are impractical/cumbersome, and our results show that examining such data sets in their entirety can lead to better classiﬁer performance. [sent-846, score-0.22]
</p><p>93 The link function (we will restrict analytical ˆ i−1 results to the logistic and probit link functions) is Φ(z) as before and we denote its ﬁrst and second derivative, with respect to z, by Φ (z) and Φ (z) respectively. [sent-854, score-0.403]
</p><p>94 For the probit link function: Φ(z) = Φ (z) = Φ (z) =  Z z  2 1 √ e−x /2 dx −∞ 2π 1 −z2 /2 √ e 2π −z −z2 /2 √ e , 2π  whereas for the logistic link function: Φ(z) = Φ (z) = Φ (z) =  ez 1 + ez ez (1 + ez )2 (ez )(1 − ez ) . [sent-857, score-0.768]
</p><p>95 (1 + ez )3  These expressions then allow us to compute the ai , bi in the cases needed. [sent-858, score-0.187]
</p><p>96 Indeed, after going through all the observations in the data set (pass z, say): Ω = 2Ψ βz−1 + θ = 2  t  ∑ ai  i=1  xi xT − diag(x2 ) i i  t  βz−1 + ∑ bi xi , i=1  which follows from the deﬁnitions of Ω, θ and Ψ . [sent-891, score-0.204]
</p><p>97 This leads to the following equation for Ω: t  t  t  i=1  i=1  i=1  Ω = 2 ∑ ai (βT xi )xi − 2 ∑ ai (x2 βz−1 ) + ∑ bi xi , i z−1 where (x2 βz−1 ) is a vector whose entries are x2 multiplied by βz−1 component-wise. [sent-893, score-0.252]
</p><p>98 We present a proof sketch for the convergence behavior of the online algorithm in the inﬁnite data limit. [sent-898, score-0.32]
</p><p>99 Suppose now that the online algorithm converges to a particular ﬁxed point. [sent-902, score-0.286]
</p><p>100 A simple and efﬁcient algorithm for gene selection using sparse logistic regression. [sent-1079, score-0.196]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('shooting', 0.449), ('rmmp', 0.364), ('bbr', 0.266), ('online', 0.238), ('mp', 0.233), ('batch', 0.22), ('adigan', 0.168), ('assive', 0.168), ('balakrishnan', 0.149), ('probit', 0.138), ('memory', 0.129), ('map', 0.128), ('lassifiers', 0.117), ('passes', 0.111), ('subdifferential', 0.107), ('logistic', 0.105), ('genkin', 0.098), ('parse', 0.096), ('modapte', 0.084), ('regularization', 0.082), ('estimates', 0.081), ('link', 0.08), ('ez', 0.073), ('shevade', 0.071), ('sketches', 0.07), ('bi', 0.066), ('active', 0.064), ('dt', 0.06), ('posterior', 0.06), ('figueiredo', 0.059), ('md', 0.059), ('pass', 0.057), ('massive', 0.053), ('laplacian', 0.053), ('components', 0.052), ('bayesian', 0.051), ('ai', 0.048), ('worst', 0.048), ('algorithm', 0.048), ('update', 0.046), ('lewis', 0.046), ('xi', 0.045), ('costs', 0.045), ('portion', 0.045), ('minka', 0.044), ('predictive', 0.043), ('sparse', 0.043), ('nonzero', 0.043), ('requirements', 0.042), ('th', 0.041), ('yi', 0.041), ('sequentially', 0.04), ('simulated', 0.038), ('quadratic', 0.038), ('opper', 0.036), ('regression', 0.036), ('jain', 0.036), ('suhrid', 0.036), ('em', 0.035), ('optimality', 0.034), ('sketch', 0.034), ('amounts', 0.034), ('sparsity', 0.034), ('modi', 0.033), ('keerthi', 0.033), ('likelihood', 0.032), ('manuscript', 0.032), ('parameter', 0.032), ('bernardo', 0.032), ('td', 0.032), ('text', 0.031), ('requirement', 0.031), ('training', 0.03), ('norm', 0.03), ('approximations', 0.029), ('koh', 0.029), ('approximation', 0.029), ('ers', 0.029), ('table', 0.029), ('prior', 0.028), ('acquisit', 0.028), ('bordered', 0.028), ('econom', 0.028), ('kass', 0.028), ('krishnapuram', 0.028), ('moody', 0.028), ('multipass', 0.028), ('needing', 0.028), ('obligat', 0.028), ('rcv', 0.028), ('rockafellar', 0.028), ('shar', 0.028), ('streaming', 0.028), ('unused', 0.028), ('lasso', 0.028), ('green', 0.028), ('convex', 0.028), ('propagation', 0.028), ('log', 0.027), ('cost', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999928 <a title="12-tfidf-1" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>2 0.14780867 <a title="12-tfidf-2" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>Author: Manfred K. Warmuth, Dima Kuzmin</p><p>Abstract: We design an online algorithm for Principal Component Analysis. In each trial the current instance is centered and projected into a probabilistically chosen low dimensional subspace. The regret of our online algorithm, that is, the total expected quadratic compression loss of the online algorithm minus the total quadratic compression loss of the batch algorithm, is bounded by a term whose dependence on the dimension of the instances is only logarithmic. We ﬁrst develop our methodology in the expert setting of online learning by giving an algorithm for learning as well as the best subset of experts of a certain size. This algorithm is then lifted to the matrix setting where the subsets of experts correspond to subspaces. The algorithm represents the uncertainty over the best subspace as a density matrix whose eigenvalues are bounded. The running time is O(n2 ) per trial, where n is the dimension of the instances. Keywords: principal component analysis, online learning, density matrix, expert setting, quantum Bayes rule</p><p>3 0.10751488 <a title="12-tfidf-3" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>4 0.078096196 <a title="12-tfidf-4" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>Author: Onureena Banerjee, Laurent El Ghaoui, Alexandre d'Aspremont</p><p>Abstract: We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1 -norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our ﬁrst algorithm uses block coordinate descent, and can be interpreted as recursive 1 -norm penalized regression. Our second algorithm, based on Nesterov’s ﬁrst order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data. Keywords: model selection, maximum likelihood estimation, convex optimization, Gaussian graphical model, binary data</p><p>5 0.076862782 <a title="12-tfidf-5" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>Author: Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui</p><p>Abstract: Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefﬁcients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semideﬁnite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefﬁcients, with total complexity O(n3 ), where n is the number of variables. We then use the same relaxation to derive sufﬁcient conditions for global optimality of a solution, which can be tested in O(n3 ) per pattern. We discuss applications in subset selection and sparse recovery and show on artiﬁcial examples and biological data that our algorithm does provide globally optimal solutions in many cases. Keywords: PCA, subset selection, sparse eigenvalues, sparse recovery, lasso</p><p>6 0.074800909 <a title="12-tfidf-6" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>7 0.065049231 <a title="12-tfidf-7" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>8 0.063177243 <a title="12-tfidf-8" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>9 0.056674913 <a title="12-tfidf-9" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>10 0.0533554 <a title="12-tfidf-10" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>11 0.049399406 <a title="12-tfidf-11" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>12 0.04895895 <a title="12-tfidf-12" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>13 0.048307952 <a title="12-tfidf-13" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>14 0.046580367 <a title="12-tfidf-14" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>15 0.044500437 <a title="12-tfidf-15" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>16 0.043762896 <a title="12-tfidf-16" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>17 0.041554898 <a title="12-tfidf-17" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>18 0.038994554 <a title="12-tfidf-18" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>19 0.038948279 <a title="12-tfidf-19" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>20 0.038218565 <a title="12-tfidf-20" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.232), (1, -0.081), (2, -0.077), (3, 0.106), (4, -0.039), (5, 0.079), (6, -0.073), (7, 0.019), (8, -0.246), (9, -0.007), (10, -0.052), (11, -0.08), (12, -0.026), (13, 0.035), (14, -0.043), (15, 0.102), (16, 0.089), (17, -0.13), (18, 0.163), (19, -0.032), (20, -0.067), (21, 0.024), (22, -0.001), (23, 0.031), (24, -0.143), (25, -0.197), (26, -0.198), (27, 0.002), (28, 0.193), (29, 0.042), (30, -0.015), (31, 0.146), (32, 0.014), (33, 0.05), (34, 0.125), (35, 0.017), (36, 0.029), (37, 0.043), (38, 0.183), (39, 0.044), (40, 0.036), (41, 0.069), (42, -0.092), (43, 0.039), (44, -0.071), (45, 0.159), (46, -0.022), (47, 0.083), (48, 0.022), (49, 0.273)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93215126 <a title="12-lsi-1" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>2 0.62406772 <a title="12-lsi-2" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>Author: Manfred K. Warmuth, Dima Kuzmin</p><p>Abstract: We design an online algorithm for Principal Component Analysis. In each trial the current instance is centered and projected into a probabilistically chosen low dimensional subspace. The regret of our online algorithm, that is, the total expected quadratic compression loss of the online algorithm minus the total quadratic compression loss of the batch algorithm, is bounded by a term whose dependence on the dimension of the instances is only logarithmic. We ﬁrst develop our methodology in the expert setting of online learning by giving an algorithm for learning as well as the best subset of experts of a certain size. This algorithm is then lifted to the matrix setting where the subsets of experts correspond to subspaces. The algorithm represents the uncertainty over the best subspace as a density matrix whose eigenvalues are bounded. The running time is O(n2 ) per trial, where n is the dimension of the instances. Keywords: principal component analysis, online learning, density matrix, expert setting, quantum Bayes rule</p><p>3 0.40966263 <a title="12-lsi-3" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>4 0.36488163 <a title="12-lsi-4" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>Author: Elisa Ricci, Tijl De Bie, Nello Cristianini</p><p>Abstract: Most approaches to structured output prediction rely on a hypothesis space of prediction functions that compute their output by maximizing a linear scoring function. In this paper we present two novel learning algorithms for this hypothesis class, and a statistical analysis of their performance. The methods rely on efﬁciently computing the ﬁrst two moments of the scoring function over the output space, and using them to create convex objective functions for training. We report extensive experimental results for sequence alignment, named entity recognition, and RNA secondary structure prediction. Keywords: structured output prediction, discriminative learning, Z-score, discriminant analysis, PAC bound</p><p>5 0.36010829 <a title="12-lsi-5" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>Author: Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, Peter L. Bartlett</p><p>Abstract: Log-linear and maximum-margin models are two commonly-used methods in supervised machine learning, and are frequently used in structured prediction problems. Efﬁcient learning of parameters in these models is therefore an important problem, and becomes a key factor when learning from very large data sets. This paper describes exponentiated gradient (EG) algorithms for training such models, where EG updates are applied to the convex dual of either the log-linear or maxmargin objective function; the dual in both the log-linear and max-margin cases corresponds to minimizing a convex function with simplex constraints. We study both batch and online variants of the algorithm, and provide rates of convergence for both cases. In the max-margin case, O( 1 ) EG ε updates are required to reach a given accuracy ε in the dual; in contrast, for log-linear models only O(log( 1 )) updates are required. For both the max-margin and log-linear cases, our bounds suggest ε that the online EG algorithm requires a factor of n less computation to reach a desired accuracy than the batch EG algorithm, where n is the number of training examples. Our experiments conﬁrm that the online algorithms are much faster than the batch algorithms in practice. We describe how the EG updates factor in a convenient way for structured prediction problems, allowing the algorithms to be efﬁciently applied to problems such as sequence learning or natural language parsing. We perform extensive evaluation of the algorithms, comparing them to L-BFGS and stochastic gradient descent for log-linear models, and to SVM-Struct for max-margin models. The algorithms are applied to a multi-class problem as well as to a more complex large-scale parsing task. In all these settings, the EG algorithms presented here outperform the other methods. Keywords: exponentiated gradient, log-linear models, maximum-margin models, structured prediction, conditional random ﬁelds ∗. These authors contributed equally. c 2008 Michael Col</p><p>6 0.32890308 <a title="12-lsi-6" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>7 0.3002722 <a title="12-lsi-7" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>8 0.27550191 <a title="12-lsi-8" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>9 0.27129304 <a title="12-lsi-9" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>10 0.24166325 <a title="12-lsi-10" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>11 0.24068148 <a title="12-lsi-11" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>12 0.23693825 <a title="12-lsi-12" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>13 0.23162837 <a title="12-lsi-13" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>14 0.23113465 <a title="12-lsi-14" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>15 0.2277237 <a title="12-lsi-15" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>16 0.21362898 <a title="12-lsi-16" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>17 0.21155564 <a title="12-lsi-17" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>18 0.21083023 <a title="12-lsi-18" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>19 0.21043098 <a title="12-lsi-19" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>20 0.20695469 <a title="12-lsi-20" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.017), (5, 0.015), (9, 0.017), (40, 0.026), (54, 0.026), (58, 0.051), (66, 0.064), (76, 0.021), (88, 0.053), (92, 0.035), (94, 0.57), (99, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99429321 <a title="12-lda-1" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Stefan Klanke, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages. Keywords: regression, local learning, online learning, C, C++, Matlab, Octave, Python</p><p>2 0.98110497 <a title="12-lda-2" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>same-paper 3 0.96842694 <a title="12-lda-3" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>4 0.85701025 <a title="12-lda-4" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>5 0.59211069 <a title="12-lda-5" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>6 0.59164584 <a title="12-lda-6" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>7 0.58874965 <a title="12-lda-7" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>8 0.58111256 <a title="12-lda-8" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>9 0.57460481 <a title="12-lda-9" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>10 0.56735879 <a title="12-lda-10" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>11 0.53989726 <a title="12-lda-11" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>12 0.52850491 <a title="12-lda-12" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>13 0.52340567 <a title="12-lda-13" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>14 0.51818407 <a title="12-lda-14" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>15 0.5143292 <a title="12-lda-15" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>16 0.50057483 <a title="12-lda-16" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>17 0.49744892 <a title="12-lda-17" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>18 0.48780882 <a title="12-lda-18" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>19 0.47952145 <a title="12-lda-19" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>20 0.47720045 <a title="12-lda-20" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
