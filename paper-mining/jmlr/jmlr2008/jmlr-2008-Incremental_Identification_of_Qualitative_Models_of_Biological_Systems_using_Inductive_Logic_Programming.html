<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>44 jmlr-2008-Incremental Identification of Qualitative Models of Biological Systems using Inductive Logic Programming</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-44" href="#">jmlr2008-44</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>44 jmlr-2008-Incremental Identification of Qualitative Models of Biological Systems using Inductive Logic Programming</h1>
<br/><p>Source: <a title="jmlr-2008-44-pdf" href="http://jmlr.org/papers/volume9/srinivasan08a/srinivasan08a.pdf">pdf</a></p><p>Author: Ashwin Srinivasan, Ross D. King</p><p>Abstract: The use of computational models is increasingly expected to play an important role in predicting the behaviour of biological systems. Models are being sought at different scales of biological organisation namely: sub-cellular, cellular, tissue, organ, organism and ecosystem; with a view of identifying how different components are connected together, how they are controlled and how they behave when functioning as a system. Except for very simple biological processes, system identiﬁcation from ﬁrst principles can be extremely difﬁcult. This has brought into focus automated techniques for constructing models using data of system behaviour. Such techniques face three principal issues: (1) The model representation language must be rich enough to capture system behaviour; (2) The system identiﬁcation technique must be powerful enough to identify substantially complex models; and (3) There may not be sufﬁcient data to obtain both the model’s structure and precise estimates of all of its parameters. In this paper, we address these issues in the following ways: (1) Models are represented in an expressive subset of ﬁrst-order logic. Speciﬁcally, they are expressed as logic programs; (2) System identiﬁcation is done using techniques developed in Inductive Logic Programming (ILP). This allows the identiﬁcation of ﬁrst-order logic models from data. Speciﬁcally, we employ an incremental approach in which increasingly complex models are constructed from simpler ones using snapshots of system behaviour; and (3) We restrict ourselves to “qualitative” models. These are non-parametric: thus, usually less data are required than for identifying parametric quantitative models. A further advantage is that the data need not be precise numerical observations (instead, they are abstractions like positive, negative, zero, increasing, decreasing and so on). We describe incremental construction of qualitative models using a simple physical system and demonstrate its application to identiﬁcatio</p><p>Reference: <a title="jmlr-2008-44-reference" href="../jmlr2008_reference/jmlr-2008-Incremental_Identification_of_Qualitative_Models_of_Biological_Systems_using_Inductive_Logic_Programming_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we will be concerned exclusively with system identiﬁcation from such qualitative data. [sent-91, score-0.465]
</p><p>2 Clearly, these qualitative models cannot be treated as being equivalent to their quantitative counterparts. [sent-93, score-0.492]
</p><p>3 This involves the use of qualitative constraints which form the building blocks of qualitative models (these models include abstractions of ordinary differential equations). [sent-97, score-0.999]
</p><p>4 Section 3 describes informally the the basics of an ILP system used to identify qualitative models. [sent-98, score-0.465]
</p><p>5 Reasoning with qualitative abstractions requires a calculus: we propose to use the constraintbased formulation used in the qualitative simulation program QSIM (Kuipers, 1994) (here we provide an informal description along the lines described by Bratko 2001). [sent-109, score-0.814]
</p><p>6 The qualitative state of a system is simply a list of the qualitative states of the system’s variables and a qualitative behaviour is a list of consecutive qualitative states. [sent-153, score-1.685]
</p><p>7 Figure 2 shows a qualitative model for a simple physical system, expressed in terms of the QSIM constraints. [sent-159, score-0.452]
</p><p>8 A qualitative behaviour of this system—that is, a sequence of qualitative states of the system variables La, Lb and Fab that satisfy the model’s constraints—is shown in Fig. [sent-160, score-0.903]
</p><p>9 Since qualitative models are non-parametric all computational effort is focussed on identifying the model structure. [sent-167, score-0.508]
</p><p>10 It should also be apparent, that while quantitative addition is functional (the sum of a pair of numbers is a unique number) , qualitative addition ones is relational (that is, for a pair of qualitative states for A and B, ADD may be true for more than one qualitative state for C). [sent-208, score-1.218]
</p><p>11 The qualitative model expresses the same information as a conjunction of constraints (here, we have used the QSIM constraints described in the paper). [sent-213, score-0.574]
</p><p>12 0/inc  Figure 3: A qualitative behaviour of the U-tube that is consistent with the qualitative model in Fig. [sent-250, score-0.866]
</p><p>13 Model Identiﬁcation using Inductive Logic Programming Given correct deﬁnitions for the QSIM constraints, it is our aim in this paper to identify qualitative models such as that shown in Fig. [sent-254, score-0.447]
</p><p>14 Figure 4 shows some parts of a graph for the U-tube in which a model is reﬁned to another by the addition of a qualitative constraint. [sent-263, score-0.428]
</p><p>15 Examples of these for qualitative model identiﬁcation are: (a) Deﬁnitions for qualitative constraints like DERIV, MPLUS, ADD and so on, along with appropriate dimensionality checks etc. [sent-331, score-0.892]
</p><p>16 For qualitative model identiﬁcation, these would be qualitative observations of system behaviour of the form shown in Fig. [sent-341, score-0.94]
</p><p>17 For example, given correct deﬁnitions for the qualitative constraints DERIV, MPLUS, ADD and MINUS as background knowledge, the qualitative model described by the conjunction DERIV(La,Fab) ∧ DERIV(Lb,Fba) ∧ ADD(Lb,Diff,La) ∧ MPLUS(Diff,Fab) ∧ MINUS(Fab,Fba) is an explanation of the examples in Fig. [sent-346, score-0.925]
</p><p>18 Roughly speaking, for qualitative models, generalisations correspond either: to removing one or more qualitative components from the diagrammatic model; or to “disconnecting” qualitative components from each other. [sent-352, score-1.229]
</p><p>19 , 1989; Mozetic, 1987) appear to have the been the ﬁrst to use qualitative reasoning to build a static model for the electric activity of the heart. [sent-369, score-0.428]
</p><p>20 Coiera’s G ENMODEL (Coiera, 1989a,b) was the ﬁrst machine learning system that constructed qualitative models for dynamic systems. [sent-370, score-0.521]
</p><p>21 More recently, the QOPH system for identifying qualitative models exploited the possibility of providing the ILP system A LEPH with a special-purpose reﬁnement operator (Coghill et al. [sent-377, score-0.645]
</p><p>22 This operator, with certain “built-in” constraints on acceptable qualitative models, is used by A LEPH to search the space of possible models. [sent-379, score-0.55]
</p><p>23 Although not using a general-purpose ILP system, Suc and colleagues have proposed a hybrid approach of combining a logic-based qualitative learner followed by numeric modelling to construct quantitative models of systems (Suc et al. [sent-385, score-0.521]
</p><p>24 The advantages of using of a purely qualitative representation for modelling metabolic pathways has been recently advocated (King et al. [sent-391, score-0.42]
</p><p>25 While this approach has been reasonably successful in the identiﬁcation of small to medium-sized models (for example, qualitative models containing no more than 4 to 5 constraints), it is unclear whether the approach can scale up to the identiﬁcation of substantially complex models. [sent-401, score-0.503]
</p><p>26 2 grows exponentially in the number of qualitative constraints in the model. [sent-403, score-0.464]
</p><p>27 Instead, we use a reﬁnement operator ρA that is restricted to performing specialisations only (for qualitative models, this amounts to adding qualitative constraints and connecting existing qualitative components). [sent-421, score-1.272]
</p><p>28 For qualitative models, this performs a trade-off between the likelihood of a qualitative model and its complexity (a quantity related to the number of constraints in the model); and 4. [sent-427, score-0.892]
</p><p>29 With these implementation choices it can be shown that, for identiﬁcation of qualitative models, the size of the search space of such an incremental procedure is dominated by maximum number of additional qualitative constraints that need to be identiﬁed at any one of the stages (see Section A. [sent-437, score-1.019]
</p><p>30 We can only guarantee correctness of the incremental approach to the extent that any model identiﬁed for a stage will logically entail the observations for that stage (given the background knowledge). [sent-441, score-0.44]
</p><p>31 1 Incremental Qualitative Model Identiﬁcation: An Example We consider identifying the qualitative model of the coupled-tanks system shown in Fig. [sent-447, score-0.526]
</p><p>32 InflowA A  B  Qualitative Model:  Fab La  Lb  DERIV(La,NetflowA) DERIV(Lb,NetflowB) ADD(Lb,Diff,La) , MPLUS(Diff,Fab) MPLUS(Lb,OutflowB) ADD(NetflowB,OutflowB,Fab) ADD(Fab,NetflowA,InflowA)  OutflowB  Figure 8: A system comprised of two coupled tanks and its qualitative model. [sent-450, score-0.607]
</p><p>33 In the ﬁrst stage, we focus on identifying a model for tank B, using the single tank system in Fig. [sent-453, score-0.437]
</p><p>34 10 (often called the “bathtub” system in qualitative modelling literature). [sent-454, score-0.494]
</p><p>35 Any consistent models identiﬁed for the single tank system are then extended to return ﬁnal models for the coupled tanks system (see Fig. [sent-455, score-0.553]
</p><p>36 The qualitative model is simply a conjunction of two single tank models. [sent-468, score-0.579]
</p><p>37 Our deﬁnitions are based on those in Bratko (2001); (b) A set of general constraints on “well-posed” qualitative models. [sent-473, score-0.464]
</p><p>38 This consists of specifying the number of qualitative constraints in the ﬁnal model for each stage. [sent-475, score-0.501]
</p><p>39 This is 3 for Stage 1 (the single tank model) and 7 for Stage 2 (the coupled tanks model); and (d) Stage-speciﬁc “mode” declarations similar to the description in Muggleton (1995) that provide domain and connectivity information for the qualitative variables (see Fig. [sent-476, score-0.783]
</p><p>40 These are in the form of qualitative states for the system variables. [sent-479, score-0.465]
</p><p>41 The level of water La or Lb for the system can similarly assume any of the following qualitative states: level : 0/std, level : 0/inc, level : 0. [sent-497, score-0.577]
</p><p>42 Examples for Stage 2 contain the qualitative states of all the system variables. [sent-510, score-0.465]
</p><p>43 (2005), the term “well-posed” qualitative models is used to denote those models that satisfy a number of domain-independent constraints. [sent-516, score-0.503]
</p><p>44 A well-posed model must be of a particular size (measured by the number of qualitative constraints). [sent-519, score-0.428]
</p><p>45 The number of instances of any qualitative constraint in a well-posed model should be below some prescribed number. [sent-525, score-0.428]
</p><p>46 The “Connected” constraint that requires all intermediate variables should appear in at least two qualitative constraints is enforced by the more general “Irrelevant variables” constraint here. [sent-537, score-0.464]
</p><p>47 The following constraints on the qualitative variables were also used. [sent-593, score-0.464]
</p><p>48 37  Figure 14: Well-posed models for the single tank system identiﬁed by the ﬁrst stage of incremental learning. [sent-623, score-0.508]
</p><p>49 Model 4 is the target model  The coupled tank system identiﬁcation task can clearly be decomposed into two stages: the identiﬁcation of the single tank system consisting of 3 qualitative constraints, followed by its extension by 4 further constraints. [sent-635, score-0.929]
</p><p>50 The single tank model is identiﬁed in Stage 1, and then extended to the coupled tank model in Stage 2. [sent-639, score-0.427]
</p><p>51 Finally, while most of the constraints 1–9 on well-posed models are motivated by some wellunderstood principles underlying qualitative reasoning, the constraints 10–13 on qualitative variables are not. [sent-652, score-0.984]
</p><p>52 First, they are intended to illustrate the ability of a general-purpose ILP system to identify qualitative models for biological systems at signiﬁcantly different scales of organisation. [sent-658, score-0.553]
</p><p>53 The background knowledge will also largely be the same, consisting of deﬁnitions for qualitative constraints. [sent-662, score-0.424]
</p><p>54 00 %  Figure 18: Estimates of the reduction in the search space by the constraints introduced on qualitative variables. [sent-675, score-0.496]
</p><p>55 Under certain simplifying assumptions described, the qualitative model is in Fig. [sent-695, score-0.428]
</p><p>56 All models were constructed in a single-stage containing 6 qualitative constraints, in approximately 65 seconds of processor time. [sent-707, score-0.447]
</p><p>57 Under the simple assumptions of g(N) ∝ N and d(P) ∝ P, the corresponding qualitative model is in (c). [sent-714, score-0.428]
</p><p>58 1495  S RINIVASAN AND K ING  {φ}  Predator−prey Models  L  B  E  ρA f Bayes Predator−prey observations QSIM constraints General model constraints Predator−prey model constraints Mode declarations  Figure 20: Incremental model identiﬁcation of predator prey models. [sent-715, score-0.539]
</p><p>59 2 Organ-Level System Identiﬁcation In this section we consider identiﬁcation of a qualitative model for the human lung. [sent-722, score-0.428]
</p><p>60 Our goal is to reconstruct the qualitative model in Fig. [sent-763, score-0.428]
</p><p>61 We examine identiﬁcation of the full model in two stages: the ﬁrst stage being concerned with identifying the insulin component (the ﬁrst three constraints in the qualitative model) and the second, the glucose component (the remaining six constraints in the model). [sent-765, score-0.953]
</p><p>62 The resulting quantiative model is in (b) and the corresponding qualitative model is in (c). [sent-782, score-0.465]
</p><p>63 1499  S RINIVASAN AND K ING  Lung Models  L  Approximate Lung Model  B  E  ρA f Bayes Lung observations QSIM constraints General model constraints Lung model constraints Mode declarations  Figure 24: Incremental model identiﬁcation of lung models. [sent-783, score-0.516]
</p><p>64 The qualitative model in Clancy and Kuipers (1994) utilises a sigmoid function SPLUS. [sent-803, score-0.428]
</p><p>65 We will use the following simple qualitative model for enzymes and metabolites. [sent-827, score-0.428]
</p><p>66 A “qualitative cell-state” is given by the qualitative states of the metabolites of interest in the cell. [sent-839, score-0.424]
</p><p>67 We are interested here in ﬁnding a sequence of qualitative reactions that are consistent with the qualitative cell-states before and after glycolysis. [sent-851, score-0.859]
</p><p>68 For this, we introduce a PATHWAY relation which, for a given sequence of qualitative reactions, holds for pairs of qualitative cell-states Be f ore, A f ter such that the qualitative state of each metabolite in Be f ore can be transformed into its state in A f ter by the qualitative reactions. [sent-852, score-1.69]
</p><p>69 Glycolsis, as the quote above makes clear, and indeed most other pathways have been uncovered by ﬁrst experimentally separating them into constituent parts (the qualitative modelling of pathways in (King et al. [sent-859, score-0.42]
</p><p>70 in f /std} A qualitative reaction glc + at p g6p + ad p Some cell-states consistent with glc + at p g6p + ad p Before: {ad p : 0/std, at p : 0. [sent-900, score-0.571]
</p><p>71 In this, a qualitative reaction causes a qualitative decrease in the reactants and a qualitative increase in the products. [sent-928, score-1.173]
</p><p>72 n QREACTION(Si−1 , Ri , Si ) F = Sn QREACTION(State, R, NewState) QDECREASE(State, Reactants(R), S) QINCREASE(S, Products(R), NewState)  Figure 31: A qualitative model for glycolysis. [sent-943, score-0.428]
</p><p>73 Pathways consist of qualitative reactions, each of which result in a qualitative decrease in the reactants and a qualitative increase in the products. [sent-944, score-1.173]
</p><p>74 The non-determinacy of qualitative arithmetic means that a qualitative reaction acting on a cell-state could result in one of several new cell-states (since there would be several ways to decrease or increase the qualitative values of metabolites). [sent-945, score-1.173]
</p><p>75 The automatic decomposition task is as follows: given values for the 5 system variables, identify the single tank “subsystem” speciﬁed by OutflowB, Fab and Lb and then identify the the ﬁnal model using the single tank submodel and the remaining system variables. [sent-991, score-0.54]
</p><p>76 In this paper, we argue that a qualitative representation of values, along with a powerful machine-learning approach like ILP, provides a useful tool for system-identiﬁcation at different levels of biological organisation. [sent-1028, score-0.423]
</p><p>77 In these circumstances, the data can be converted to a qualitative representation (in the manner described by Hau and Coiera, 1997) and one or more qualitative models identiﬁed. [sent-1033, score-0.838]
</p><p>78 This may be sufﬁcient for common-sense reasoning about a system, but is clearly insufﬁcient for a complete understanding; (2) simulations with qualitative models can contain spurious behavious; and (3) abstractions appear to be largely restricted to ODE models. [sent-1044, score-0.479]
</p><p>79 Acknowledgments The authors would like to thank George Coghill for helping us understand the constraints deﬁning well-posed qualitative models and pointing out an important practical ﬂaw in an early attempt at constructing the lung model. [sent-1062, score-0.607]
</p><p>80 Binesh Mangar attempted to construct some of the qualitative models for the coupled tanks and the lung during the course of his M. [sent-1064, score-0.676]
</p><p>81 Given a deﬁnite clause C, the qualitative constraints in the model (the size of the model) are obtained by counting the number of qualitative constraints in C. [sent-1174, score-0.965]
</p><p>82 • Constraints, such as the restrictions to well-posed models, are assumed to be encoded in the background knowledge; • acceptable(C, B, E) is T RUE for any qualitative model C that is consistent with the constraints in B, given E. [sent-1176, score-0.534]
</p><p>83 Let S be the size of an acceptable model and C be a qualitative model of size S with n = S − S . [sent-1201, score-0.519]
</p><p>84 In addition, it is clear by deﬁnition that given a qualitative model C, accep table(C , B, E) is T RUE for any model C ∈ ρ1 (C, B, E). [sent-1250, score-0.465]
</p><p>85 Although a general statements about search complexity can be made, the following remarks refer speciﬁcally to the search for qualitative models. [sent-1275, score-0.455]
</p><p>86 Let the number of qualitative constraints in acceptable models be restricted to some size d. [sent-1277, score-0.574]
</p><p>87 We now consider an incremental procedure that simply selects some of the best qualitative models found at a stage for reﬁnement at the next stage. [sent-1283, score-0.674]
</p><p>88 It follows that the size of the search space depends principally on the maximum number of qualitative constraints added at any stage. [sent-1284, score-0.496]
</p><p>89 1516  I DENTIFYING Q UALITATIVE M ODELS OF B IOLOGICAL S YSTEMS  Remark 3 Incremental search space for qualitative models. [sent-1285, score-0.423]
</p><p>90 At each stage i, a model is constructed by addition of di+ = di − di−1 constraints to a model selected at stage i − 1. [sent-1292, score-0.433]
</p><p>91 For qualitative models, this translates to retaining the qualitative components found at the previous stage but disconnecting connections between some or all pairs whose outputs are connected together. [sent-1302, score-0.925]
</p><p>92 While a general analysis will require a detailed description of the variable splitting procedure, a less detailed calculation is possible for the kinds of qualitative models sought here. [sent-1303, score-0.471]
</p><p>93 Remark 4 Incremental search space for qualitative models (limited generalisation). [sent-1305, score-0.479]
</p><p>94 If the maximum number of variables in any qualitative constraint is bounded by A, then there can be at most si = Adi splittable variables in any model M from stage i. [sent-1318, score-0.571]
</p><p>95 Therefore the number of models after generalisation of any M from stage i is at most G(i) = Bsi (ni ) Since there are no more than m models at any stage i, the total number of models after generalisation is no more than mG(i). [sent-1319, score-0.538]
</p><p>96 Remark 5 Incremental search space for qualitative models (general case). [sent-1328, score-0.479]
</p><p>97 In the worst case, all the constraints found in each of the models at stage i are removed by the generalisation step and the specialisation step at stage i + 1 has to construct models with i + 1 constraints in each case. [sent-1337, score-0.586]
</p><p>98 The mode declaration for the glycolysis/2 predicate is: GLYCOLYSIS(+cellstate,-cellstate)  Although QSIM constraints form the basis of qualitative reactions, the models constructed use a pathway/3 predicate. [sent-1715, score-0.645]
</p><p>99 The mode declaration for this predicate is simply: PATHWAY(+cellstate,#qreactions,-cellstate)  (Here the ”#” indicates that a corresponding argument is a ground term: in this case a sequence of qualitative reactions). [sent-1716, score-0.516]
</p><p>100 1 Speciﬁcation We will assume that we are looking to decompose a system speciﬁed by a set of qualitative system variables. [sent-1722, score-0.539]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('deriv', 0.446), ('qualitative', 0.391), ('mplus', 0.383), ('ilp', 0.229), ('tank', 0.151), ('stage', 0.143), ('mminus', 0.142), ('glucose', 0.138), ('sub', 0.134), ('dentifying', 0.114), ('glycolysis', 0.114), ('iological', 0.114), ('rinivasan', 0.114), ('ualitative', 0.114), ('ystems', 0.114), ('mult', 0.106), ('odels', 0.102), ('declarations', 0.099), ('qsim', 0.095), ('tanks', 0.091), ('lung', 0.087), ('incremental', 0.084), ('pathway', 0.08), ('reactions', 0.077), ('mode', 0.074), ('insulin', 0.074), ('system', 0.074), ('constraints', 0.073), ('identi', 0.07), ('nement', 0.067), ('splus', 0.063), ('minus', 0.061), ('prey', 0.059), ('models', 0.056), ('flow', 0.055), ('qval', 0.055), ('acceptable', 0.054), ('decomposition', 0.053), ('coupled', 0.051), ('declaration', 0.051), ('fab', 0.051), ('predator', 0.051), ('add', 0.051), ('stages', 0.048), ('glc', 0.047), ('muggleton', 0.047), ('randomised', 0.047), ('behaviour', 0.047), ('ing', 0.047), ('quantitative', 0.045), ('hi', 0.045), ('ter', 0.043), ('blood', 0.043), ('ad', 0.043), ('generalisation', 0.042), ('ore', 0.04), ('dhap', 0.039), ('ilevel', 0.039), ('lb', 0.039), ('logic', 0.038), ('adp', 0.038), ('bratko', 0.037), ('gas', 0.037), ('model', 0.037), ('cost', 0.036), ('coghill', 0.035), ('inflowa', 0.035), ('outflowb', 0.035), ('vid', 0.035), ('metabolites', 0.033), ('background', 0.033), ('biological', 0.032), ('search', 0.032), ('abstractions', 0.032), ('diagrammatic', 0.032), ('glevel', 0.032), ('la', 0.031), ('modelling', 0.029), ('level', 0.028), ('costk', 0.028), ('descendents', 0.028), ('ecosystem', 0.028), ('incsearch', 0.028), ('nad', 0.028), ('organ', 0.028), ('pep', 0.028), ('priming', 0.028), ('marker', 0.027), ('gin', 0.027), ('srinivasan', 0.027), ('operator', 0.026), ('physical', 0.024), ('splitting', 0.024), ('identifying', 0.024), ('fructose', 0.024), ('generalisations', 0.024), ('nadh', 0.024), ('organisation', 0.024), ('pulmonary', 0.024), ('tabulation', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="44-tfidf-1" href="./jmlr-2008-Incremental_Identification_of_Qualitative_Models_of_Biological_Systems_using_Inductive_Logic_Programming.html">44 jmlr-2008-Incremental Identification of Qualitative Models of Biological Systems using Inductive Logic Programming</a></p>
<p>Author: Ashwin Srinivasan, Ross D. King</p><p>Abstract: The use of computational models is increasingly expected to play an important role in predicting the behaviour of biological systems. Models are being sought at different scales of biological organisation namely: sub-cellular, cellular, tissue, organ, organism and ecosystem; with a view of identifying how different components are connected together, how they are controlled and how they behave when functioning as a system. Except for very simple biological processes, system identiﬁcation from ﬁrst principles can be extremely difﬁcult. This has brought into focus automated techniques for constructing models using data of system behaviour. Such techniques face three principal issues: (1) The model representation language must be rich enough to capture system behaviour; (2) The system identiﬁcation technique must be powerful enough to identify substantially complex models; and (3) There may not be sufﬁcient data to obtain both the model’s structure and precise estimates of all of its parameters. In this paper, we address these issues in the following ways: (1) Models are represented in an expressive subset of ﬁrst-order logic. Speciﬁcally, they are expressed as logic programs; (2) System identiﬁcation is done using techniques developed in Inductive Logic Programming (ILP). This allows the identiﬁcation of ﬁrst-order logic models from data. Speciﬁcally, we employ an incremental approach in which increasingly complex models are constructed from simpler ones using snapshots of system behaviour; and (3) We restrict ourselves to “qualitative” models. These are non-parametric: thus, usually less data are required than for identifying parametric quantitative models. A further advantage is that the data need not be precise numerical observations (instead, they are abstractions like positive, negative, zero, increasing, decreasing and so on). We describe incremental construction of qualitative models using a simple physical system and demonstrate its application to identiﬁcatio</p><p>2 0.038367357 <a title="44-tfidf-2" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>Author: Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel, Daphne Koller</p><p>Abstract: We consider the problem of learning classiﬁers in structured domains, where some objects have a subset of features that are inherently absent due to complex relationships between the features. Unlike the case where a feature exists but its value is not observed, here we focus on the case where a feature may not even exist (structurally absent) for some of the samples. The common approach for handling missing features in discriminative models is to ﬁrst complete their unknown values, and then use a standard classiﬁcation procedure over the completed data. This paper focuses on features that are known to be non-existing, rather than have an unknown value. We show how incomplete data can be classiﬁed directly without any completion of the missing features using a max-margin learning framework. We formulate an objective function, based on the geometric interpretation of the margin, that aims to maximize the margin of each sample in its own relevant subspace. In this formulation, the linearly separable case can be transformed into a binary search over a series of second order cone programs (SOCP), a convex problem that can be solved efﬁciently. We also describe two approaches for optimizing the general case: an approximation that can be solved as a standard quadratic program (QP) and an iterative approach for solving the exact problem. By avoiding the pre-processing phase in which the data is completed, both of these approaches could offer considerable computational savings. More importantly, we show that the elegant handling of missing values by our approach allows it to both outperform other methods when the missing values have non-trivial structure, and be competitive with other methods when the values are missing at random. We demonstrate our results on several standard benchmarks and two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images. Keywords: max margin, missing features, network reconstruction, metabolic pa</p><p>3 0.026780231 <a title="44-tfidf-3" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>Author: Ilya Shpitser, Judea Pearl</p><p>Abstract: We consider a hierarchy of queries about causal relationships in graphical models, where each level in the hierarchy requires more detailed information than the one below. The hierarchy consists of three levels: associative relationships, derived from a joint distribution over the observable variables; cause-effect relationships, derived from distributions resulting from external interventions; and counterfactuals, derived from distributions that span multiple “parallel worlds” and resulting from simultaneous, possibly conﬂicting observations and interventions. We completely characterize cases where a given causal query can be computed from information lower in the hierarchy, and provide algorithms that accomplish this computation. Speciﬁcally, we show when effects of interventions can be computed from observational studies, and when probabilities of counterfactuals can be computed from experimental studies. We also provide a graphical characterization of those queries which cannot be computed (by any method) from queries at a lower layer of the hierarchy. Keywords: causality, graphical causal models, identiﬁcation</p><p>4 0.025495909 <a title="44-tfidf-4" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><p>5 0.025401264 <a title="44-tfidf-5" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>6 0.025281306 <a title="44-tfidf-6" href="./jmlr-2008-Learning_Control_Knowledge_for_Forward_Search_Planning.html">49 jmlr-2008-Learning Control Knowledge for Forward Search Planning</a></p>
<p>7 0.024777278 <a title="44-tfidf-7" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>8 0.024145288 <a title="44-tfidf-8" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>9 0.022933275 <a title="44-tfidf-9" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>10 0.022712715 <a title="44-tfidf-10" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>11 0.022675425 <a title="44-tfidf-11" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>12 0.02176109 <a title="44-tfidf-12" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>13 0.020519957 <a title="44-tfidf-13" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>14 0.020420041 <a title="44-tfidf-14" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>15 0.02041349 <a title="44-tfidf-15" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>16 0.018626815 <a title="44-tfidf-16" href="./jmlr-2008-Non-Parametric_Modeling_of_Partially_Ranked_Data.html">69 jmlr-2008-Non-Parametric Modeling of Partially Ranked Data</a></p>
<p>17 0.018320598 <a title="44-tfidf-17" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>18 0.018259507 <a title="44-tfidf-18" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>19 0.017996611 <a title="44-tfidf-19" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>20 0.017794944 <a title="44-tfidf-20" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.102), (1, 0.004), (2, -0.01), (3, 0.001), (4, -0.054), (5, 0.007), (6, 0.005), (7, -0.0), (8, 0.012), (9, 0.028), (10, -0.034), (11, 0.03), (12, 0.014), (13, -0.013), (14, 0.045), (15, -0.029), (16, 0.027), (17, 0.006), (18, 0.003), (19, -0.011), (20, 0.039), (21, -0.028), (22, -0.084), (23, -0.078), (24, -0.039), (25, -0.055), (26, -0.024), (27, -0.078), (28, 0.132), (29, -0.087), (30, 0.04), (31, -0.104), (32, 0.331), (33, 0.107), (34, 0.336), (35, 0.108), (36, -0.443), (37, -0.303), (38, -0.093), (39, 0.34), (40, -0.31), (41, 0.113), (42, 0.156), (43, 0.055), (44, 0.037), (45, 0.068), (46, -0.284), (47, 0.028), (48, -0.078), (49, -0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97642934 <a title="44-lsi-1" href="./jmlr-2008-Incremental_Identification_of_Qualitative_Models_of_Biological_Systems_using_Inductive_Logic_Programming.html">44 jmlr-2008-Incremental Identification of Qualitative Models of Biological Systems using Inductive Logic Programming</a></p>
<p>Author: Ashwin Srinivasan, Ross D. King</p><p>Abstract: The use of computational models is increasingly expected to play an important role in predicting the behaviour of biological systems. Models are being sought at different scales of biological organisation namely: sub-cellular, cellular, tissue, organ, organism and ecosystem; with a view of identifying how different components are connected together, how they are controlled and how they behave when functioning as a system. Except for very simple biological processes, system identiﬁcation from ﬁrst principles can be extremely difﬁcult. This has brought into focus automated techniques for constructing models using data of system behaviour. Such techniques face three principal issues: (1) The model representation language must be rich enough to capture system behaviour; (2) The system identiﬁcation technique must be powerful enough to identify substantially complex models; and (3) There may not be sufﬁcient data to obtain both the model’s structure and precise estimates of all of its parameters. In this paper, we address these issues in the following ways: (1) Models are represented in an expressive subset of ﬁrst-order logic. Speciﬁcally, they are expressed as logic programs; (2) System identiﬁcation is done using techniques developed in Inductive Logic Programming (ILP). This allows the identiﬁcation of ﬁrst-order logic models from data. Speciﬁcally, we employ an incremental approach in which increasingly complex models are constructed from simpler ones using snapshots of system behaviour; and (3) We restrict ourselves to “qualitative” models. These are non-parametric: thus, usually less data are required than for identifying parametric quantitative models. A further advantage is that the data need not be precise numerical observations (instead, they are abstractions like positive, negative, zero, increasing, decreasing and so on). We describe incremental construction of qualitative models using a simple physical system and demonstrate its application to identiﬁcatio</p><p>2 0.14239815 <a title="44-lsi-2" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>Author: Ilya Shpitser, Judea Pearl</p><p>Abstract: We consider a hierarchy of queries about causal relationships in graphical models, where each level in the hierarchy requires more detailed information than the one below. The hierarchy consists of three levels: associative relationships, derived from a joint distribution over the observable variables; cause-effect relationships, derived from distributions resulting from external interventions; and counterfactuals, derived from distributions that span multiple “parallel worlds” and resulting from simultaneous, possibly conﬂicting observations and interventions. We completely characterize cases where a given causal query can be computed from information lower in the hierarchy, and provide algorithms that accomplish this computation. Speciﬁcally, we show when effects of interventions can be computed from observational studies, and when probabilities of counterfactuals can be computed from experimental studies. We also provide a graphical characterization of those queries which cannot be computed (by any method) from queries at a lower layer of the hierarchy. Keywords: causality, graphical causal models, identiﬁcation</p><p>3 0.11718144 <a title="44-lsi-3" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>Author: Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel, Daphne Koller</p><p>Abstract: We consider the problem of learning classiﬁers in structured domains, where some objects have a subset of features that are inherently absent due to complex relationships between the features. Unlike the case where a feature exists but its value is not observed, here we focus on the case where a feature may not even exist (structurally absent) for some of the samples. The common approach for handling missing features in discriminative models is to ﬁrst complete their unknown values, and then use a standard classiﬁcation procedure over the completed data. This paper focuses on features that are known to be non-existing, rather than have an unknown value. We show how incomplete data can be classiﬁed directly without any completion of the missing features using a max-margin learning framework. We formulate an objective function, based on the geometric interpretation of the margin, that aims to maximize the margin of each sample in its own relevant subspace. In this formulation, the linearly separable case can be transformed into a binary search over a series of second order cone programs (SOCP), a convex problem that can be solved efﬁciently. We also describe two approaches for optimizing the general case: an approximation that can be solved as a standard quadratic program (QP) and an iterative approach for solving the exact problem. By avoiding the pre-processing phase in which the data is completed, both of these approaches could offer considerable computational savings. More importantly, we show that the elegant handling of missing values by our approach allows it to both outperform other methods when the missing values have non-trivial structure, and be competitive with other methods when the values are missing at random. We demonstrate our results on several standard benchmarks and two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images. Keywords: max margin, missing features, network reconstruction, metabolic pa</p><p>4 0.1077138 <a title="44-lsi-4" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>Author: Abraham George, Warren B. Powell, Sanjeev R. Kulkarni</p><p>Abstract: We consider the problem of estimating the value of a multiattribute resource, where the attributes are categorical or discrete in nature and the number of potential attribute vectors is very large. The problem arises in approximate dynamic programming when we need to estimate the value of a multiattribute resource from estimates based on Monte-Carlo simulation. These problems have been traditionally solved using aggregation, but choosing the right level of aggregation requires resolving the classic tradeoff between aggregation error and sampling error. We propose a method that estimates the value of a resource at different levels of aggregation simultaneously, and then uses a weighted combination of the estimates. Using the optimal weights, which minimizes the variance of the estimate while accounting for correlations between the estimates, is computationally too expensive for practical applications. We have found that a simple inverse variance formula (adjusted for bias), which effectively assumes the estimates are independent, produces near-optimal estimates. We use the setting of two levels of aggregation to explain why this approximation works so well. Keywords: hierarchical statistics, approximate dynamic programming, mixture models, adaptive learning, multiattribute resources</p><p>5 0.10184778 <a title="44-lsi-5" href="./jmlr-2008-Learning_Control_Knowledge_for_Forward_Search_Planning.html">49 jmlr-2008-Learning Control Knowledge for Forward Search Planning</a></p>
<p>Author: Sungwook Yoon, Alan Fern, Robert Givan</p><p>Abstract: A number of today’s state-of-the-art planners are based on forward state-space search. The impressive performance can be attributed to progress in computing domain independent heuristics that perform well across many domains. However, it is easy to ﬁnd domains where such heuristics provide poor guidance, leading to planning failure. Motivated by such failures, the focus of this paper is to investigate mechanisms for learning domain-speciﬁc knowledge to better control forward search in a given domain. While there has been a large body of work on inductive learning of control knowledge for AI planning, there is a void of work aimed at forward-state-space search. One reason for this may be that it is challenging to specify a knowledge representation for compactly representing important concepts across a wide range of domains. One of the main contributions of this work is to introduce a novel feature space for representing such control knowledge. The key idea is to deﬁne features in terms of information computed via relaxed plan extraction, which has been a major source of success for non-learning planners. This gives a new way of leveraging relaxed planning techniques in the context of learning. Using this feature space, we describe three forms of control knowledge—reactive policies (decision list rules and measures of progress) and linear heuristics—and show how to learn them and incorporate them into forward state-space search. Our empirical results show that our approaches are able to surpass state-of-the-art nonlearning planners across a wide range of planning competition domains. Keywords: planning, machine learning, knowledge representation, search</p><p>6 0.095414139 <a title="44-lsi-6" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>7 0.083854258 <a title="44-lsi-7" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>8 0.080600336 <a title="44-lsi-8" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>9 0.080599226 <a title="44-lsi-9" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>10 0.075467154 <a title="44-lsi-10" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>11 0.074134462 <a title="44-lsi-11" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>12 0.074115075 <a title="44-lsi-12" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>13 0.072604813 <a title="44-lsi-13" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>14 0.071463861 <a title="44-lsi-14" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>15 0.07127966 <a title="44-lsi-15" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>16 0.070502497 <a title="44-lsi-16" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>17 0.069668435 <a title="44-lsi-17" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>18 0.067315161 <a title="44-lsi-18" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>19 0.066779889 <a title="44-lsi-19" href="./jmlr-2008-Non-Parametric_Modeling_of_Partially_Ranked_Data.html">69 jmlr-2008-Non-Parametric Modeling of Partially Ranked Data</a></p>
<p>20 0.066767626 <a title="44-lsi-20" href="./jmlr-2008-Closed_Sets_for_Labeled_Data.html">22 jmlr-2008-Closed Sets for Labeled Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.016), (5, 0.019), (9, 0.015), (19, 0.403), (21, 0.014), (40, 0.044), (54, 0.037), (58, 0.032), (66, 0.035), (76, 0.021), (78, 0.014), (88, 0.068), (92, 0.025), (94, 0.041), (95, 0.071), (99, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76539183 <a title="44-lda-1" href="./jmlr-2008-Incremental_Identification_of_Qualitative_Models_of_Biological_Systems_using_Inductive_Logic_Programming.html">44 jmlr-2008-Incremental Identification of Qualitative Models of Biological Systems using Inductive Logic Programming</a></p>
<p>Author: Ashwin Srinivasan, Ross D. King</p><p>Abstract: The use of computational models is increasingly expected to play an important role in predicting the behaviour of biological systems. Models are being sought at different scales of biological organisation namely: sub-cellular, cellular, tissue, organ, organism and ecosystem; with a view of identifying how different components are connected together, how they are controlled and how they behave when functioning as a system. Except for very simple biological processes, system identiﬁcation from ﬁrst principles can be extremely difﬁcult. This has brought into focus automated techniques for constructing models using data of system behaviour. Such techniques face three principal issues: (1) The model representation language must be rich enough to capture system behaviour; (2) The system identiﬁcation technique must be powerful enough to identify substantially complex models; and (3) There may not be sufﬁcient data to obtain both the model’s structure and precise estimates of all of its parameters. In this paper, we address these issues in the following ways: (1) Models are represented in an expressive subset of ﬁrst-order logic. Speciﬁcally, they are expressed as logic programs; (2) System identiﬁcation is done using techniques developed in Inductive Logic Programming (ILP). This allows the identiﬁcation of ﬁrst-order logic models from data. Speciﬁcally, we employ an incremental approach in which increasingly complex models are constructed from simpler ones using snapshots of system behaviour; and (3) We restrict ourselves to “qualitative” models. These are non-parametric: thus, usually less data are required than for identifying parametric quantitative models. A further advantage is that the data need not be precise numerical observations (instead, they are abstractions like positive, negative, zero, increasing, decreasing and so on). We describe incremental construction of qualitative models using a simple physical system and demonstrate its application to identiﬁcatio</p><p>2 0.31497389 <a title="44-lda-2" href="./jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">53 jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>Author: Manu Chhabra, Robert A. Jacobs</p><p>Abstract: The computational complexities arising in motor control can be ameliorated through the use of a library of motor synergies. We present a new model, referred to as the Greedy Additive Regression (GAR) model, for learning a library of torque sequences, and for learning the coefﬁcients of a linear combination of sequences minimizing a cost function. From the perspective of numerical optimization, the GAR model is interesting because it creates a library of “local features”—each sequence in the library is a solution to a single training task—and learns to combine these sequences using a local optimization procedure, namely, additive regression. We speculate that learners with local representational primitives and local optimization procedures will show good performance on nonlinear tasks. The GAR model is also interesting from the perspective of motor control because it outperforms several competing models. Results using a simulated two-joint arm suggest that the GAR model consistently shows excellent performance in the sense that it rapidly learns to perform novel, complex motor tasks. Moreover, its library is overcomplete and sparse, meaning that only a small fraction of the stored torque sequences are used when learning a new movement. The library is also robust in the sense that, after an initial training period, nearly all novel movements can be learned as additive combinations of sequences in the library, and in the sense that it shows good generalization when an arm’s dynamics are altered between training and test conditions, such as when a payload is added to the arm. Lastly, the GAR model works well regardless of whether motor tasks are speciﬁed in joint space or Cartesian space. We conclude that learning techniques using local primitives and optimization procedures are viable and potentially important methods for motor control and possibly other domains, and that these techniques deserve further examination by the artiﬁcial intelligence and cognitive science</p><p>3 0.2665107 <a title="44-lda-3" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>Author: Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel, Daphne Koller</p><p>Abstract: We consider the problem of learning classiﬁers in structured domains, where some objects have a subset of features that are inherently absent due to complex relationships between the features. Unlike the case where a feature exists but its value is not observed, here we focus on the case where a feature may not even exist (structurally absent) for some of the samples. The common approach for handling missing features in discriminative models is to ﬁrst complete their unknown values, and then use a standard classiﬁcation procedure over the completed data. This paper focuses on features that are known to be non-existing, rather than have an unknown value. We show how incomplete data can be classiﬁed directly without any completion of the missing features using a max-margin learning framework. We formulate an objective function, based on the geometric interpretation of the margin, that aims to maximize the margin of each sample in its own relevant subspace. In this formulation, the linearly separable case can be transformed into a binary search over a series of second order cone programs (SOCP), a convex problem that can be solved efﬁciently. We also describe two approaches for optimizing the general case: an approximation that can be solved as a standard quadratic program (QP) and an iterative approach for solving the exact problem. By avoiding the pre-processing phase in which the data is completed, both of these approaches could offer considerable computational savings. More importantly, we show that the elegant handling of missing values by our approach allows it to both outperform other methods when the missing values have non-trivial structure, and be competitive with other methods when the values are missing at random. We demonstrate our results on several standard benchmarks and two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images. Keywords: max margin, missing features, network reconstruction, metabolic pa</p><p>4 0.25843897 <a title="44-lda-4" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>Author: Hsuan-Tien Lin, Ling Li</p><p>Abstract: Ensemble learning algorithms such as boosting can achieve better performance by averaging over the predictions of some base hypotheses. Nevertheless, most existing algorithms are limited to combining only a ﬁnite number of hypotheses, and the generated ensemble is usually sparse. Thus, it is not clear whether we should construct an ensemble classiﬁer with a larger or even an inﬁnite number of hypotheses. In addition, constructing an inﬁnite ensemble itself is a challenging task. In this paper, we formulate an inﬁnite ensemble learning framework based on the support vector machine (SVM). The framework can output an inﬁnite and nonsparse ensemble through embedding inﬁnitely many hypotheses into an SVM kernel. We use the framework to derive two novel kernels, the stump kernel and the perceptron kernel. The stump kernel embodies inﬁnitely many decision stumps, and the perceptron kernel embodies inﬁnitely many perceptrons. We also show that the Laplacian radial basis function kernel embodies inﬁnitely many decision trees, and can thus be explained through inﬁnite ensemble learning. Experimental results show that SVM with these kernels is superior to boosting with the same base hypothesis set. In addition, SVM with the stump kernel or the perceptron kernel performs similarly to SVM with the Gaussian radial basis function kernel, but enjoys the beneﬁt of faster parameter selection. These properties make the novel kernels favorable choices in practice. Keywords: ensemble learning, boosting, support vector machine, kernel</p><p>5 0.25408319 <a title="44-lda-5" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>6 0.25278938 <a title="44-lda-6" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>7 0.25095528 <a title="44-lda-7" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>8 0.24891838 <a title="44-lda-8" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>9 0.24870223 <a title="44-lda-9" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>10 0.24853913 <a title="44-lda-10" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>11 0.24808936 <a title="44-lda-11" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>12 0.24796087 <a title="44-lda-12" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>13 0.24740349 <a title="44-lda-13" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>14 0.24585485 <a title="44-lda-14" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>15 0.24577332 <a title="44-lda-15" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>16 0.24502079 <a title="44-lda-16" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>17 0.2448445 <a title="44-lda-17" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>18 0.24475536 <a title="44-lda-18" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>19 0.23913379 <a title="44-lda-19" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>20 0.23836029 <a title="44-lda-20" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
