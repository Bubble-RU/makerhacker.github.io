<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>91 jmlr-2008-Trust Region Newton Method for Logistic Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-91" href="#">jmlr2008-91</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>91 jmlr-2008-Trust Region Newton Method for Logistic Regression</h1>
<br/><p>Source: <a title="jmlr-2008-91-pdf" href="http://jmlr.org/papers/volume9/lin08b/lin08b.pdf">pdf</a></p><p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>Reference: <a title="jmlr-2008-91-reference" href="../jmlr2008_reference/jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Research Santa Clara, CA 95054, USA  Editor: Alexander Smola  Abstract Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. [sent-10, score-0.179]
</p><p>2 In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. [sent-11, score-0.721]
</p><p>3 Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. [sent-13, score-0.218]
</p><p>4 Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines  1. [sent-15, score-1.059]
</p><p>5 Introduction The logistic regression model is useful for two-class classiﬁcation. [sent-16, score-0.153]
</p><p>6 There are many methods for training logistic regression models. [sent-30, score-0.196]
</p><p>7 , 2003), nonlinear conjugate gradient, quasi Newton (in particular, limited memory BFGS) (Liu and Nocedal, 1989; Benson and Mor´ , 2001), and truncated Newton (Komarek and Moore, e 2005). [sent-34, score-0.432]
</p><p>8 Truncated Newton methods have been an effective approach for large-scale unconstrained optimization, but their use for logistic regression has not been fully exploited. [sent-53, score-0.182]
</p><p>9 In Section 2, we discuss an efﬁcient and robust truncated Newton method for logistic regression. [sent-55, score-0.247]
</p><p>10 This approach, called trust region Newton method, uses only approximate Newton steps in the beginning, but takes full Newton directions in the end for fast convergence. [sent-56, score-0.547]
</p><p>11 In Sections 3 and 4, we discuss some existing optimization methods for logistic regression and conduct comparisons. [sent-57, score-0.181]
</p><p>12 Section 5 investigates a variant of our proposed method by using preconditioned conjugate gradients in the trust region framework. [sent-60, score-0.778]
</p><p>13 In Section 6, we extend the proposed trust region method to solve L2-loss support vector machines. [sent-61, score-0.568]
</p><p>14 For large-scale logistic regression, we then propose a trust region Newton method, which is a type of truncated Newton approach. [sent-72, score-0.773]
</p><p>15 1 Newton and Truncated Newton Methods To discuss Newton methods, we need the gradient and Hessian of f (w): l  ∇ f (w) = w +C ∑ (σ(yi wT xi ) − 1)yi xi , i=1 T  ∇2 f (w) = I +CX DX,  (3) (4)  where I is the identity matrix, Tx  σ(yi wT xi ) = (1 + e−yi w  i  )−1 . [sent-74, score-0.179]
</p><p>16 Since ∇2 f (wk ) is invertible, the simplest Newton method updates w by the following way wk+1 = wk + sk ,  (5)  where k is the iteration index and sk , the Newton direction, is the solution of the following linear system: ∇2 f (wk )sk = −∇ f (wk ). [sent-83, score-0.649]
</p><p>17 Two techniques are often used: line search and trust region. [sent-92, score-0.396]
</p><p>18 Therefore, for large logistic regression, iterative methods are more suitable than direct methods, which require the whole Hessian matrix. [sent-100, score-0.155]
</p><p>19 Among iterative methods, currently conjugate gradients are the most used ones in Newton methods. [sent-101, score-0.199]
</p><p>20 The optimization procedure then has two layers of iterations: at each outer iteration an inner conjugate gradient procedure ﬁnds the Newton direction. [sent-102, score-0.352]
</p><p>21 Unfortunately, conjugate gradient methods may suffer from lengthy iterations in certain situations. [sent-103, score-0.352]
</p><p>22 Komarek and Moore (2005) are among the ﬁrst to apply truncated Newton methods for logistic regression. [sent-106, score-0.226]
</p><p>23 1 They approximately solve (6) by conjugate gradient procedures and use (5) to update wk . [sent-107, score-0.751]
</p><p>24 They terminate the conjugate gradient procedure if the relative difference of log likelihoods between two consecutive conjugate gradient iterations is smaller than a threshold. [sent-108, score-0.603]
</p><p>25 Some comparisons between limited memory quasi Newton and truncated Newton are by Nocedal and Nash (1991) and Zou et al. [sent-116, score-0.263]
</p><p>26 2 A Trust Region Newton Method We consider the trust region method (Lin and Mor´ , 1999), which is a truncated Newton method to e deal with general bound-constrained optimization problems (i. [sent-119, score-0.718]
</p><p>27 At each iteration of a trust region Newton method for minimizing f (w), we have an iterate wk , a size ∆k of the trust region, and a quadratic model 1 qk (s) = ∇ f (wk )T s + sT ∇2 f (wk )s 2 1. [sent-124, score-1.517]
</p><p>28 630  T RUST R EGION N EWTON M ETHOD FOR L OGISTIC R EGRESSION  Algorithm 1 A trust region algorithm for logistic regression 1. [sent-126, score-0.7]
</p><p>29 • Find an approximate solution sk of the trust region sub-problem min qk (s) s  subject to s ≤ ∆k . [sent-133, score-0.709]
</p><p>30 Next, we ﬁnd a step sk to approximately minimize qk (s) subject to the constraint s ≤ ∆k . [sent-138, score-0.162]
</p><p>31 We then update wk and ∆k by checking the ratio ρk =  f (wk + sk ) − f (wk ) qk (sk )  (8)  of the actual reduction in the function to the predicted reduction in the quadratic model. [sent-139, score-0.634]
</p><p>32 The direction is accepted if ρk is large enough: wk+1 =  wk + sk wk  if ρk > η0 , if ρk ≤ η0 ,  (9)  where η0 > 0 is a pre-speciﬁed value. [sent-140, score-0.975]
</p><p>33 The trust region bound ∆k is updated by the rules ∆k+1 ∈ [σ1 min{ sk , ∆k }, σ2 ∆k ] if ρk ≤ η1 , ∆k+1 ∈ [σ1 ∆k , σ3 ∆k ] if ρk ∈ (η1 , η2 ), ∆k+1 ∈ [∆k , σ3 ∆k ] if ρk ≥ η2 . [sent-142, score-0.626]
</p><p>34 (10)  Similar rules are used in most modern trust region methods. [sent-143, score-0.547]
</p><p>35 A description of our trust region algorithm is given in Algorithm 1. [sent-144, score-0.547]
</p><p>36 The conjugate gradient method to approximately solve the trust region sub-problem (11) is given in Algorithm 2. [sent-147, score-0.847]
</p><p>37 Note that only one Hessian-vector product is needed at each conjugate gradient iteration. [sent-150, score-0.279]
</p><p>38 Since ri = −∇ f (wk ) − ∇2 f (wk )¯i , s the stopping condition (12) is the same as − ∇ f (wk ) − ∇2 f (wk )¯i ≤ ξk ∇ f (wk ) , s 631  L IN , W ENG AND K EERTHI  Algorithm 2 Conjugate gradient procedure for approximately solving the trust region sub-problem (11) ¯ 1. [sent-151, score-0.77]
</p><p>39 (inner iterations) • If  ri ≤ ξk ∇ f (wk ) ,  (12)  ¯ then output sk = si and stop. [sent-158, score-0.188]
</p><p>40 ¯ • If si+1 ≥ ∆k , compute τ such that ¯ si + τdi = ∆k ,  (13)  ¯ then output sk = si + τdi and stop. [sent-161, score-0.169]
</p><p>41 However, Algorithm 2 is different from standard conjugate gradient methods for linear systems as the constraint s ≤ ∆ ¯ must be taken care of. [sent-166, score-0.279]
</p><p>42 1) with s0 = 0, we have ¯ ¯ si < si+1 , ∀i, ¯ so in a ﬁnite number of conjugate gradient iterations, either (12) is satisﬁed or si+1 violates the trust region constraint. [sent-168, score-0.871]
</p><p>43 In the latter situation, (13) ﬁnds a point on the trust region boundary as qk (¯i + τdi ) < qk (¯i ). [sent-169, score-0.713]
</p><p>44 s s The whole procedure is a careful design to make sure that the approximate Newton direction is good enough and the trust region method converges. [sent-170, score-0.568]
</p><p>45 Next, we discuss convergence properties of the trust region Newton method. [sent-171, score-0.574]
</p><p>46 If ξk < 1, then the trust region method Q-linearly converges: lim  k→∞  wk+1 − w∗ < 1, wk − w∗  (14)  where w∗ is the unique optimal solution of (2). [sent-186, score-1.016]
</p><p>47 wk − w∗ 2  Regarding the computational complexity, the cost per iteration is O(nnz) for 1 function and 0/1 gradient evaluations + O(nnz) × number of conjugate gradient iterations,  (15)  where nnz is the number of nonzero elements in the sparse matrix X. [sent-192, score-1.01]
</p><p>48 Note that if wk is not updated in (9), then the gradient is the same for the next iteration. [sent-193, score-0.558]
</p><p>49 Related Methods and Implementation Issues In this section, we discuss a general limited memory quasi Newton implementation (Liu and Nocedal, 1989). [sent-195, score-0.162]
</p><p>50 Many consider it to be very efﬁcient for training logistic regression. [sent-196, score-0.168]
</p><p>51 We also discuss implementation issues of the proposed trust region Newton method. [sent-197, score-0.547]
</p><p>52 This property is useful for large logistic regression as we cannot afford to store Hk . [sent-204, score-0.153]
</p><p>53 In contrast, limited memory quasi Newton methods consider only approximate Hessian matrices, so we can expect that it has slower convergence. [sent-234, score-0.162]
</p><p>54 Moreover, compared to (15), the cost per iteration is less than that for our trust region method. [sent-237, score-0.592]
</p><p>55 We set the initial ∆0 = ∇ f (w0 ) and take η0 = 10−4 in (9) to update wk . [sent-245, score-0.472]
</p><p>56 In the conjugate gradient e procedure, we use ξk = 0. [sent-254, score-0.279]
</p><p>57 These choices are considered appropriate following the research on trust region methods in the past several decades. [sent-257, score-0.547]
</p><p>58 It is unclear yet if they are the best for logistic regression problems, but certainly we would like to try custom settings ﬁrst. [sent-258, score-0.153]
</p><p>59 Experiments In this section, we compare our approach with a quasi Newton implementation for logistic regression. [sent-260, score-0.218]
</p><p>60 2 Comparisons We compare two logistic regression implementations: • TRON: the trust region Newton method discussed in Section 2. [sent-370, score-0.721]
</p><p>61 • LBFGS: the limited memory quasi Newton implementation (Liu and Nocedal, 1989). [sent-372, score-0.162]
</p><p>62 For the ﬁrst one, we simulate the practical use of logistic regression by setting a stopping condition and checking the prediction ability. [sent-395, score-0.202]
</p><p>63 Most unconstrained optimization software use gradient information to terminate the iterative procedure, so we use ∇ f (wk )  ∞  ≤ 10−3  (18)  as the stopping condition. [sent-396, score-0.267]
</p><p>64 On training time, TRON is better than LBFGS, so truncated Newton methods are effective for training logistic regression. [sent-406, score-0.312]
</p><p>65 For example, if X=  −10 0 −20 0 , 30 0 0 10  then its compressed column format is by three arrays: X val = [−10, 30, −20, 10],  X rowind = [1, 2, 1, 2],  X colptr = [1, 3, 3, 4, 5],  where rowind means row indices and colptr means column pointers. [sent-449, score-0.275]
</p><p>66 The main conjugate gradient operation (7) involves two matrix-vector products—one is with X T , and the other is with X. [sent-454, score-0.279]
</p><p>67 If the number of nonzeros in a column is signiﬁcantly larger than those in a row, very likely a column cannot be ﬁt into the same memory level as that for a row. [sent-462, score-0.152]
</p><p>68 Preconditioned Conjugate Gradient Methods To reduce the number of conjugate gradient iterations, in the truncated Newton method one often uses preconditioned conjugate gradient procedures. [sent-475, score-0.721]
</p><p>69 If the approximate factorization (19) is good enough, P−1 ∇2 f (wk )P−T is close to the identity and less conjugate gradient iterations are needed. [sent-477, score-0.324]
</p><p>70 However, as we need extra efforts to ﬁnd P and the cost per conjugate iteration is higher, a smaller number of conjugate gradient iterations may not lead to shorter training time. [sent-478, score-0.581]
</p><p>71 First, lengthy conjugate gradient iterations often occur at ﬁnal outer steps, but for machine learning applications the algorithm may stop before reaching such a stage. [sent-482, score-0.375]
</p><p>72 The cost of obtaining the preconditioner is thus no more than that of one conjugate gradient iteration. [sent-488, score-0.31]
</p><p>73 642  (20)  T RUST R EGION N EWTON M ETHOD FOR L OGISTIC R EGRESSION  Algorithm 4 Preconditioned conjugate gradient procedure for approximately solving the trust region sub-problem (21) ˆ 1. [sent-491, score-0.826]
</p><p>74 (inner iterations) • If  ˆ ri ≤ ξk g ,  ˆ then output sk = P−T si and stop. [sent-498, score-0.188]
</p><p>75 ˆ With s = PT s, we transform (20) to ˆ s min qk (ˆ) ˆ s  where  ˆ subject to s ≤ ∆k ,  (21)  1 ˆ ˆ s ˆ ˆ qk (ˆ) = gT s + sT Hˆ, ˆ s 2  and ˆ g = P−1 ∇ f (wk ),  ˆ H = P−1 ∇2 f (wk )P−T . [sent-505, score-0.166]
</p><p>76 Note that in practical implementations we calculate Hdi by a way similar to (7) P−1 (P−T di +C(X T (D(X(P−T di ))))). [sent-509, score-0.16]
</p><p>77 In Table 4, we present the average number of conjugate gradient iterations per fold in the CV procedure. [sent-510, score-0.347]
</p><p>78 643  L IN , W ENG AND K EERTHI  Problem a9a real-sim news20 citeseer yahoo-japan rcv1 yahoo-korea  CG 567 104 71 113 278 225 779  PCG 263 160 155 115 326 280 736  Table 4: Average number of conjugate gradient iterations per fold in the CV procedure. [sent-515, score-0.347]
</p><p>79 Trust Region Method for L2-SVM The second term in (2) can be considered as a loss function, so regularized logistic regression is related to other learning approaches such as Support Vector Machines (SVM) (Boser et al. [sent-521, score-0.153]
</p><p>80 In this section, we extend our trust region method for L2-SVM. [sent-535, score-0.568]
</p><p>81 The gradient ∇ f2 (w) is Lipschitz continuous, so one can deﬁne the generalized Hessian matrix B(w) = I + 2CX T DX, where   1 if 1 − yi wT xi > 0,  Dii = any element in [0, 1] if 1 − yi wT xi = 0,   0 if 1 − yi wT xi < 0. [sent-543, score-0.303]
</p><p>82 Then the trust region method (Lin and Mor´ , 1999) can be applied by replacing ∇2 f (w) in Section e 2. [sent-544, score-0.568]
</p><p>83 The Hessian-vector product in the conjugate gradient procedure is then T B(w)s = s + 2C · XI,: (DI,I (XI,: s)). [sent-553, score-0.279]
</p><p>84 Keerthi and DeCoste (2005) use conjugate gradient methods to solve (26), and the procedure is described in Algorithm 5. [sent-563, score-0.279]
</p><p>85 In contrast, the convergence of our trust region method holds when the conjugate gradient procedure only approximately minimizes the trust-region sub-problem. [sent-567, score-0.874]
</p><p>86 ¯ Solve (26) by the conjugate gradient procedure and obtain wk . [sent-576, score-0.727]
</p><p>87 Find αk = arg min f (wk + αsk ), α≥0  and set wk+1 = wk + αk sk . [sent-578, score-0.527]
</p><p>88 3 Comparisons We compare our proposed trust region implementation (TRON) in Section 6. [sent-580, score-0.547]
</p><p>89 To solve (26), SVMlin considers a relative stopping condition for the conjugate gradient procedure. [sent-586, score-0.328]
</p><p>90 Following their convergence result, we modify SVMlin to quite accurately solve the linear system (26): Recall in Algorithm 5 that we sequentially obtain the following items: ¯ wk → Ik → wk . [sent-587, score-0.923]
</p><p>91 We then use ¯ (I + 2CXIT,: XIk ,: )wk − 2CXIT,: yIk k k  ∞  ≤ 10−3  as the stopping condition of the conjugate gradient procedure in SVMlin. [sent-588, score-0.328]
</p><p>92 Both approaches spend most of their time on the operation (24) in the conjugate gradient procedure. [sent-590, score-0.305]
</p><p>93 In contrast, trust region methods are effective on using only approximate directions in the early stage of the procedure. [sent-594, score-0.547]
</p><p>94 Discussion and Conclusions As logistic regression is a special case of maximum entropy models and conditional random ﬁelds, it is possible to extend the proposed approach for them. [sent-596, score-0.153]
</p><p>95 In summary, we have shown that a trust region Newton method is effective for training largescale logistic regression problems as well as L2-SVM. [sent-612, score-0.764]
</p><p>96 It is interesting that we do not need many special settings for logistic regression; a rather direct use of modern trust region techniques already yields excellent performances. [sent-614, score-0.672]
</p><p>97 If this property is wrong, there is a sequence {wk } in the set (29) satisfying wk → ∞. [sent-625, score-0.448]
</p><p>98 However, 1 f (wk ) ≥ wk 2 → ∞ 2 contradicts the fact that f (wk ) ≤ f (w0 ), ∀k. [sent-626, score-0.448]
</p><p>99 Making logistic regression a core data mining tool: A practical investigation of accuracy, speed, and simplicity. [sent-699, score-0.153]
</p><p>100 The conjugate gradient method and trust regions in large scale optimization. [sent-771, score-0.696]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('wk', 0.448), ('trust', 0.396), ('newton', 0.369), ('mor', 0.206), ('tron', 0.185), ('conjugate', 0.169), ('lbfgs', 0.154), ('region', 0.151), ('egion', 0.134), ('ogistic', 0.134), ('cv', 0.132), ('logistic', 0.125), ('eerthi', 0.124), ('ewton', 0.113), ('rust', 0.113), ('gradient', 0.11), ('truncated', 0.101), ('eng', 0.094), ('wt', 0.094), ('ethod', 0.093), ('quasi', 0.093), ('egression', 0.086), ('hessian', 0.085), ('qk', 0.083), ('nocedal', 0.083), ('di', 0.08), ('sk', 0.079), ('preconditioners', 0.073), ('hk', 0.072), ('ri', 0.064), ('lin', 0.063), ('nnz', 0.061), ('svmlin', 0.061), ('komarek', 0.051), ('yahoo', 0.051), ('stopping', 0.049), ('argonne', 0.049), ('steihaug', 0.049), ('bfgs', 0.047), ('decoste', 0.047), ('keerthi', 0.046), ('iterations', 0.045), ('si', 0.045), ('liu', 0.044), ('training', 0.043), ('memory', 0.043), ('format', 0.043), ('jorge', 0.042), ('formats', 0.041), ('preconditioned', 0.041), ('nonzeros', 0.041), ('row', 0.04), ('bouaricha', 0.036), ('hdi', 0.036), ('column', 0.034), ('stephen', 0.034), ('yi', 0.034), ('sathiya', 0.031), ('differentiable', 0.031), ('preconditioner', 0.031), ('iterative', 0.03), ('unconstrained', 0.029), ('regression', 0.028), ('optimization', 0.028), ('compressed', 0.028), ('lengthy', 0.028), ('mangasarian', 0.028), ('ruby', 0.028), ('pietra', 0.028), ('dii', 0.028), ('convergence', 0.027), ('time', 0.026), ('limited', 0.026), ('document', 0.026), ('weng', 0.025), ('pt', 0.025), ('continuously', 0.025), ('benson', 0.024), ('colptr', 0.024), ('malouf', 0.024), ('normalizations', 0.024), ('pcg', 0.024), ('rong', 0.024), ('rowind', 0.024), ('update', 0.024), ('nonzero', 0.023), ('outer', 0.023), ('xi', 0.023), ('per', 0.023), ('dx', 0.022), ('iteration', 0.022), ('taiwan', 0.022), ('matrix', 0.022), ('sparse', 0.022), ('software', 0.021), ('instances', 0.021), ('moore', 0.021), ('method', 0.021), ('twice', 0.021), ('asuncion', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="91-tfidf-1" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>2 0.44274959 <a title="91-tfidf-2" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>3 0.15626225 <a title="91-tfidf-3" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>4 0.078567103 <a title="91-tfidf-4" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Vikas Sindhwani, Sathiya S. Keerthi</p><p>Abstract: Due to its wide applicability, the problem of semi-supervised classiﬁcation is attracting increasing attention in machine learning. Semi-Supervised Support Vector Machines (S 3 VMs) are based on applying the margin maximization principle to both labeled and unlabeled examples. Unlike SVMs, their formulation leads to a non-convex optimization problem. A suite of algorithms have recently been proposed for solving S3 VMs. This paper reviews key ideas in this literature. The performance and behavior of various S3 VM algorithms is studied together, under a common experimental setting. Keywords: semi-supervised learning, support vector machines, non-convex optimization, transductive learning</p><p>5 0.076479286 <a title="91-tfidf-5" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Stefan Klanke, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages. Keywords: regression, local learning, online learning, C, C++, Matlab, Octave, Python</p><p>6 0.071262084 <a title="91-tfidf-6" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>7 0.063177243 <a title="91-tfidf-7" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>8 0.055624332 <a title="91-tfidf-8" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>9 0.049620785 <a title="91-tfidf-9" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>10 0.042886458 <a title="91-tfidf-10" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>11 0.041004416 <a title="91-tfidf-11" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>12 0.037598375 <a title="91-tfidf-12" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>13 0.035268597 <a title="91-tfidf-13" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>14 0.034373932 <a title="91-tfidf-14" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>15 0.034198642 <a title="91-tfidf-15" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>16 0.034170199 <a title="91-tfidf-16" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>17 0.033850849 <a title="91-tfidf-17" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>18 0.03021778 <a title="91-tfidf-18" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>19 0.029578194 <a title="91-tfidf-19" href="./jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">80 jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>20 0.029110109 <a title="91-tfidf-20" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.202), (1, -0.108), (2, 0.09), (3, 0.719), (4, 0.002), (5, -0.057), (6, -0.1), (7, -0.174), (8, 0.136), (9, -0.008), (10, 0.096), (11, 0.035), (12, -0.009), (13, -0.002), (14, -0.006), (15, 0.053), (16, 0.004), (17, -0.028), (18, -0.055), (19, -0.04), (20, 0.05), (21, -0.027), (22, 0.037), (23, -0.013), (24, 0.034), (25, -0.015), (26, 0.037), (27, -0.041), (28, 0.007), (29, -0.024), (30, -0.065), (31, -0.039), (32, 0.03), (33, -0.015), (34, 0.003), (35, -0.024), (36, 0.034), (37, 0.007), (38, 0.057), (39, 0.053), (40, -0.004), (41, 0.026), (42, -0.027), (43, 0.052), (44, -0.036), (45, -0.021), (46, 0.098), (47, -0.031), (48, -0.084), (49, 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96918297 <a title="91-lsi-1" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>2 0.94962102 <a title="91-lsi-2" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>3 0.38716203 <a title="91-lsi-3" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>4 0.2429688 <a title="91-lsi-4" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>Author: Olivier Chapelle, Vikas Sindhwani, Sathiya S. Keerthi</p><p>Abstract: Due to its wide applicability, the problem of semi-supervised classiﬁcation is attracting increasing attention in machine learning. Semi-Supervised Support Vector Machines (S 3 VMs) are based on applying the margin maximization principle to both labeled and unlabeled examples. Unlike SVMs, their formulation leads to a non-convex optimization problem. A suite of algorithms have recently been proposed for solving S3 VMs. This paper reviews key ideas in this literature. The performance and behavior of various S3 VM algorithms is studied together, under a common experimental setting. Keywords: semi-supervised learning, support vector machines, non-convex optimization, transductive learning</p><p>5 0.22063243 <a title="91-lsi-5" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Stefan Klanke, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages. Keywords: regression, local learning, online learning, C, C++, Matlab, Octave, Python</p><p>6 0.21768695 <a title="91-lsi-6" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>7 0.19988556 <a title="91-lsi-7" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>8 0.18907945 <a title="91-lsi-8" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>9 0.17786573 <a title="91-lsi-9" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>10 0.15376261 <a title="91-lsi-10" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>11 0.15337054 <a title="91-lsi-11" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>12 0.14320731 <a title="91-lsi-12" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>13 0.14263691 <a title="91-lsi-13" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>14 0.13139725 <a title="91-lsi-14" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>15 0.13060397 <a title="91-lsi-15" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>16 0.12878579 <a title="91-lsi-16" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>17 0.12062351 <a title="91-lsi-17" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>18 0.1185094 <a title="91-lsi-18" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>19 0.11286525 <a title="91-lsi-19" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>20 0.11090921 <a title="91-lsi-20" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.015), (5, 0.015), (31, 0.018), (40, 0.023), (54, 0.036), (58, 0.038), (66, 0.037), (76, 0.016), (88, 0.039), (92, 0.021), (94, 0.625), (99, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99126619 <a title="91-lda-1" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Stefan Klanke, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages. Keywords: regression, local learning, online learning, C, C++, Matlab, Octave, Python</p><p>same-paper 2 0.97212887 <a title="91-lda-2" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>3 0.95116192 <a title="91-lda-3" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>4 0.83593619 <a title="91-lda-4" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>5 0.5664978 <a title="91-lda-5" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, Chih-Jen Lin</p><p>Abstract: LIBLINEAR is an open source library for large-scale linear classiﬁcation. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efﬁcient on large sparse data sets. Keywords: large-scale linear classiﬁcation, logistic regression, support vector machines, open source, machine learning</p><p>6 0.55793184 <a title="91-lda-6" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>7 0.55520445 <a title="91-lda-7" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>8 0.55426919 <a title="91-lda-8" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>9 0.54699439 <a title="91-lda-9" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>10 0.53770059 <a title="91-lda-10" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>11 0.49934024 <a title="91-lda-11" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>12 0.48989332 <a title="91-lda-12" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>13 0.48966777 <a title="91-lda-13" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>14 0.48015493 <a title="91-lda-14" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>15 0.47804642 <a title="91-lda-15" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>16 0.46110252 <a title="91-lda-16" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>17 0.45586991 <a title="91-lda-17" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>18 0.44632116 <a title="91-lda-18" href="./jmlr-2008-Shark%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">85 jmlr-2008-Shark    (Machine Learning Open Source Software Paper)</a></p>
<p>19 0.44569519 <a title="91-lda-19" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>20 0.43675679 <a title="91-lda-20" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
