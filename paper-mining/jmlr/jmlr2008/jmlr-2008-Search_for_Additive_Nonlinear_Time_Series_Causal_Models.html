<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-84" href="#">jmlr2008-84</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</h1>
<br/><p>Source: <a title="jmlr-2008-84-pdf" href="http://jmlr.org/papers/volume9/chu08a/chu08a.pdf">pdf</a></p><p>Author: Tianjiao Chu, Clark Glymour</p><p>Abstract: Pointwise consistent, feasible procedures for estimating contemporaneous linear causal structure from time series data have been developed using multiple conditional independence tests, but no such procedures are available for non-linear systems. We describe a feasible procedure for learning a class of non-linear time series structures, which we call additive non-linear time series. We show that for data generated from stationary models of this type, two classes of conditional independence relations among time series variables and their lags can be tested efﬁciently and consistently using tests based on additive model regression. Combining results of statistical tests for these two classes of conditional independence relations and the temporal structure of time series data, a new consistent model speciﬁcation procedure is able to extract relatively detailed causal information. We investigate the ﬁnite sample behavior of the procedure through simulation, and illustrate the application of this method through analysis of the possible causal connections among four ocean indices. Several variants of the procedure are also discussed. Keywords: conditional independence test, contemporaneous causation, additive model regression, Granger causality, ocean indices</p><p>Reference: <a title="jmlr-2008-84-reference" href="../jmlr2008_reference/jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lag', 0.54), ('contemp', 0.516), ('xt', 0.313), ('caus', 0.191), ('xtd', 0.139), ('arrowhead', 0.111), ('clim', 0.109), ('ddit', 0.099), ('lymo', 0.099), ('commit', 0.098), ('oc', 0.096), ('yt', 0.095), ('sery', 0.093), ('soi', 0.089), ('tmax', 0.089), ('nfer', 0.084), ('ery', 0.084), ('edg', 0.08), ('nao', 0.079), ('wp', 0.079)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="84-tfidf-1" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>Author: Tianjiao Chu, Clark Glymour</p><p>Abstract: Pointwise consistent, feasible procedures for estimating contemporaneous linear causal structure from time series data have been developed using multiple conditional independence tests, but no such procedures are available for non-linear systems. We describe a feasible procedure for learning a class of non-linear time series structures, which we call additive non-linear time series. We show that for data generated from stationary models of this type, two classes of conditional independence relations among time series variables and their lags can be tested efﬁciently and consistently using tests based on additive model regression. Combining results of statistical tests for these two classes of conditional independence relations and the temporal structure of time series data, a new consistent model speciﬁcation procedure is able to extract relatively detailed causal information. We investigate the ﬁnite sample behavior of the procedure through simulation, and illustrate the application of this method through analysis of the possible causal connections among four ocean indices. Several variants of the procedure are also discussed. Keywords: conditional independence test, contemporaneous causation, additive model regression, Granger causality, ocean indices</p><p>2 0.1057931 <a title="84-tfidf-2" href="./jmlr-2008-Causal_Reasoning_with_Ancestral_Graphs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">20 jmlr-2008-Causal Reasoning with Ancestral Graphs    (Special Topic on Causality)</a></p>
<p>Author: Jiji Zhang</p><p>Abstract: Causal reasoning is primarily concerned with what would happen to a system under external interventions. In particular, we are often interested in predicting the probability distribution of some random variables that would result if some other variables were forced to take certain values. One prominent approach to tackling this problem is based on causal Bayesian networks, using directed acyclic graphs as causal diagrams to relate post-intervention probabilities to pre-intervention probabilities that are estimable from observational data. However, such causal diagrams are seldom fully testable given observational data. In consequence, many causal discovery algorithms based on data-mining can only output an equivalence class of causal diagrams (rather than a single one). This paper is concerned with causal reasoning given an equivalence class of causal diagrams, represented by a (partial) ancestral graph. We present two main results. The ﬁrst result extends Pearl (1995)’s celebrated do-calculus to the context of ancestral graphs. In the second result, we focus on a key component of Pearl’s calculus—the property of invariance under interventions, and give stronger graphical conditions for this property than those implied by the ﬁrst result. The second result also improves the earlier, similar results due to Spirtes et al. (1993). Keywords: ancestral graphs, causal Bayesian network, do-calculus, intervention</p><p>3 0.073035218 <a title="84-tfidf-3" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>Author: Jean-Philippe Pellet, André Elisseeff</p><p>Abstract: We show how a generic feature-selection algorithm returning strongly relevant variables can be turned into a causal structure-learning algorithm. We prove this under the Faithfulness assumption for the data distribution. In a causal graph, the strongly relevant variables for a node X are its parents, children, and children’s parents (or spouses), also known as the Markov blanket of X. Identifying the spouses leads to the detection of the V-structure patterns and thus to causal orientations. Repeating the task for all variables yields a valid partially oriented causal graph. We ﬁrst show an efﬁcient way to identify the spouse links. We then perform several experiments in the continuous domain using the Recursive Feature Elimination feature-selection algorithm with Support Vector Regression and empirically verify the intuition of this direct (but computationally expensive) approach. Within the same framework, we then devise a fast and consistent algorithm, Total Conditioning (TC), and a variant, TCbw , with an explicit backward feature-selection heuristics, for Gaussian data. After running a series of comparative experiments on ﬁve artiﬁcial networks, we argue that Markov blanket algorithms such as TC/TCbw or Grow-Shrink scale better than the reference PC algorithm and provides higher structural accuracy. Keywords: causal structure learning, feature selection, Markov blanket, partial correlation, statistical test of conditional independence</p><p>4 0.070886679 <a title="84-tfidf-4" href="./jmlr-2008-Active_Learning_of_Causal_Networks_with_Intervention_Experiments_and_Optimal_Designs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">10 jmlr-2008-Active Learning of Causal Networks with Intervention Experiments and Optimal Designs    (Special Topic on Causality)</a></p>
<p>Author: Yang-Bo He, Zhi Geng</p><p>Abstract: The causal discovery from data is important for various scientiﬁc investigations. Because we cannot distinguish the different directed acyclic graphs (DAGs) in a Markov equivalence class learned from observational data, we have to collect further information on causal structures from experiments with external interventions. In this paper, we propose an active learning approach for discovering causal structures in which we ﬁrst ﬁnd a Markov equivalence class from observational data, and then we orient undirected edges in every chain component via intervention experiments separately. In the experiments, some variables are manipulated through external interventions. We discuss two kinds of intervention experiments, randomized experiment and quasi-experiment. Furthermore, we give two optimal designs of experiments, a batch-intervention design and a sequential-intervention design, to minimize the number of manipulated variables and the set of candidate structures based on the minimax and the maximum entropy criteria. We show theoretically that structural learning can be done locally in subgraphs of chain components without need of checking illegal v-structures and cycles in the whole network and that a Markov equivalence subclass obtained after each intervention can still be depicted as a chain graph. Keywords: active learning, causal networks, directed acyclic graphs, intervention, Markov equivalence class, optimal design, structural learning</p><p>5 0.069081038 <a title="84-tfidf-5" href="./jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">30 jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>Author: Vojtěch Franc, Bogdan Savchynskyy</p><p>Abstract: The max-sum classiﬁer predicts n-tuple of labels from n-tuple of observable variables by maximizing a sum of quality functions deﬁned over neighbouring pairs of labels and observable variables. Predicting labels as MAP assignments of a Random Markov Field is a particular example of the max-sum classiﬁer. Learning parameters of the max-sum classiﬁer is a challenging problem because even computing the response of such classiﬁer is NP-complete in general. Estimating parameters using the Maximum Likelihood approach is feasible only for a subclass of max-sum classiﬁers with an acyclic structure of neighbouring pairs. Recently, the discriminative methods represented by the perceptron and the Support Vector Machines, originally designed for binary linear classiﬁers, have been extended for learning some subclasses of the max-sum classiﬁer. Besides the max-sum classiﬁers with the acyclic neighbouring structure, it has been shown that the discriminative learning is possible even with arbitrary neighbouring structure provided the quality functions fulﬁll some additional constraints. In this article, we extend the discriminative approach to other three classes of max-sum classiﬁers with an arbitrary neighbourhood structure. We derive learning algorithms for two subclasses of max-sum classiﬁers whose response can be computed in polynomial time: (i) the max-sum classiﬁers with supermodular quality functions and (ii) the max-sum classiﬁers whose response can be computed exactly by a linear programming relaxation. Moreover, we show that the learning problem can be approximately solved even for a general max-sum classiﬁer. Keywords: max-xum classiﬁer, hidden Markov networks, support vector machines</p><p>6 0.068383321 <a title="84-tfidf-6" href="./jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations.html">37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</a></p>
<p>7 0.062208347 <a title="84-tfidf-7" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>8 0.062092334 <a title="84-tfidf-8" href="./jmlr-2008-Graphical_Methods_for_Efficient_Likelihood_Inference_in_Gaussian_Covariance_Models.html">40 jmlr-2008-Graphical Methods for Efficient Likelihood Inference in Gaussian Covariance Models</a></p>
<p>9 0.06033976 <a title="84-tfidf-9" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>10 0.059788845 <a title="84-tfidf-10" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>11 0.057907157 <a title="84-tfidf-11" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>12 0.05649811 <a title="84-tfidf-12" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>13 0.052849405 <a title="84-tfidf-13" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>14 0.05200142 <a title="84-tfidf-14" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>15 0.047565605 <a title="84-tfidf-15" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>16 0.045800362 <a title="84-tfidf-16" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>17 0.045581877 <a title="84-tfidf-17" href="./jmlr-2008-Minimal_Nonlinear_Distortion_Principle_for_Nonlinear_Independent_Component_Analysis.html">60 jmlr-2008-Minimal Nonlinear Distortion Principle for Nonlinear Independent Component Analysis</a></p>
<p>18 0.041226376 <a title="84-tfidf-18" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>19 0.040600929 <a title="84-tfidf-19" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>20 0.038600378 <a title="84-tfidf-20" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.159), (1, -0.129), (2, -0.02), (3, -0.047), (4, -0.053), (5, 0.009), (6, -0.015), (7, 0.163), (8, -0.048), (9, -0.113), (10, -0.043), (11, -0.013), (12, 0.171), (13, -0.058), (14, 0.039), (15, 0.132), (16, -0.039), (17, 0.005), (18, -0.194), (19, -0.283), (20, 0.076), (21, 0.049), (22, 0.297), (23, -0.016), (24, -0.026), (25, -0.151), (26, -0.051), (27, 0.141), (28, -0.002), (29, 0.091), (30, -0.066), (31, -0.092), (32, 0.087), (33, 0.022), (34, -0.055), (35, 0.101), (36, 0.074), (37, -0.032), (38, 0.063), (39, 0.017), (40, 0.165), (41, -0.081), (42, 0.07), (43, 0.031), (44, 0.068), (45, 0.125), (46, -0.081), (47, -0.019), (48, 0.122), (49, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95050764 <a title="84-lsi-1" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>Author: Tianjiao Chu, Clark Glymour</p><p>Abstract: Pointwise consistent, feasible procedures for estimating contemporaneous linear causal structure from time series data have been developed using multiple conditional independence tests, but no such procedures are available for non-linear systems. We describe a feasible procedure for learning a class of non-linear time series structures, which we call additive non-linear time series. We show that for data generated from stationary models of this type, two classes of conditional independence relations among time series variables and their lags can be tested efﬁciently and consistently using tests based on additive model regression. Combining results of statistical tests for these two classes of conditional independence relations and the temporal structure of time series data, a new consistent model speciﬁcation procedure is able to extract relatively detailed causal information. We investigate the ﬁnite sample behavior of the procedure through simulation, and illustrate the application of this method through analysis of the possible causal connections among four ocean indices. Several variants of the procedure are also discussed. Keywords: conditional independence test, contemporaneous causation, additive model regression, Granger causality, ocean indices</p><p>2 0.43995678 <a title="84-lsi-2" href="./jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations.html">37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</a></p>
<p>Author: Jia Li, Andrew W. Moore</p><p>Abstract: Web sites must forecast Web page views in order to plan computer resource allocation and estimate upcoming revenue and advertising growth. In this paper, we focus on extracting trends and seasonal patterns from page view series, two dominant factors in the variation of such series. We investigate the Holt-Winters procedure and a state space model for making relatively short-term prediction. It is found that Web page views exhibit strong impulsive changes occasionally. The impulses cause large prediction errors long after their occurrences. A method is developed to identify impulses and to alleviate their damage on prediction. We also develop a long-range trend and season extraction method, namely the Elastic Smooth Season Fitting (ESSF) algorithm, to compute scalable and smooth yearly seasons. ESSF derives the yearly season by minimizing the residual sum of squares under smoothness regularization, a quadratic optimization problem. It is shown that for longterm prediction, ESSF improves accuracy signiﬁcantly over other methods that ignore the yearly seasonality. Keywords: web page views, forecast, Holt-Winters, Kalman ﬁltering, elastic smooth season ﬁtting</p><p>3 0.42875043 <a title="84-lsi-3" href="./jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">30 jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>Author: Vojtěch Franc, Bogdan Savchynskyy</p><p>Abstract: The max-sum classiﬁer predicts n-tuple of labels from n-tuple of observable variables by maximizing a sum of quality functions deﬁned over neighbouring pairs of labels and observable variables. Predicting labels as MAP assignments of a Random Markov Field is a particular example of the max-sum classiﬁer. Learning parameters of the max-sum classiﬁer is a challenging problem because even computing the response of such classiﬁer is NP-complete in general. Estimating parameters using the Maximum Likelihood approach is feasible only for a subclass of max-sum classiﬁers with an acyclic structure of neighbouring pairs. Recently, the discriminative methods represented by the perceptron and the Support Vector Machines, originally designed for binary linear classiﬁers, have been extended for learning some subclasses of the max-sum classiﬁer. Besides the max-sum classiﬁers with the acyclic neighbouring structure, it has been shown that the discriminative learning is possible even with arbitrary neighbouring structure provided the quality functions fulﬁll some additional constraints. In this article, we extend the discriminative approach to other three classes of max-sum classiﬁers with an arbitrary neighbourhood structure. We derive learning algorithms for two subclasses of max-sum classiﬁers whose response can be computed in polynomial time: (i) the max-sum classiﬁers with supermodular quality functions and (ii) the max-sum classiﬁers whose response can be computed exactly by a linear programming relaxation. Moreover, we show that the learning problem can be approximately solved even for a general max-sum classiﬁer. Keywords: max-xum classiﬁer, hidden Markov networks, support vector machines</p><p>4 0.41762871 <a title="84-lsi-4" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>Author: Jean-Philippe Pellet, André Elisseeff</p><p>Abstract: We show how a generic feature-selection algorithm returning strongly relevant variables can be turned into a causal structure-learning algorithm. We prove this under the Faithfulness assumption for the data distribution. In a causal graph, the strongly relevant variables for a node X are its parents, children, and children’s parents (or spouses), also known as the Markov blanket of X. Identifying the spouses leads to the detection of the V-structure patterns and thus to causal orientations. Repeating the task for all variables yields a valid partially oriented causal graph. We ﬁrst show an efﬁcient way to identify the spouse links. We then perform several experiments in the continuous domain using the Recursive Feature Elimination feature-selection algorithm with Support Vector Regression and empirically verify the intuition of this direct (but computationally expensive) approach. Within the same framework, we then devise a fast and consistent algorithm, Total Conditioning (TC), and a variant, TCbw , with an explicit backward feature-selection heuristics, for Gaussian data. After running a series of comparative experiments on ﬁve artiﬁcial networks, we argue that Markov blanket algorithms such as TC/TCbw or Grow-Shrink scale better than the reference PC algorithm and provides higher structural accuracy. Keywords: causal structure learning, feature selection, Markov blanket, partial correlation, statistical test of conditional independence</p><p>5 0.39944646 <a title="84-lsi-5" href="./jmlr-2008-Causal_Reasoning_with_Ancestral_Graphs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">20 jmlr-2008-Causal Reasoning with Ancestral Graphs    (Special Topic on Causality)</a></p>
<p>Author: Jiji Zhang</p><p>Abstract: Causal reasoning is primarily concerned with what would happen to a system under external interventions. In particular, we are often interested in predicting the probability distribution of some random variables that would result if some other variables were forced to take certain values. One prominent approach to tackling this problem is based on causal Bayesian networks, using directed acyclic graphs as causal diagrams to relate post-intervention probabilities to pre-intervention probabilities that are estimable from observational data. However, such causal diagrams are seldom fully testable given observational data. In consequence, many causal discovery algorithms based on data-mining can only output an equivalence class of causal diagrams (rather than a single one). This paper is concerned with causal reasoning given an equivalence class of causal diagrams, represented by a (partial) ancestral graph. We present two main results. The ﬁrst result extends Pearl (1995)’s celebrated do-calculus to the context of ancestral graphs. In the second result, we focus on a key component of Pearl’s calculus—the property of invariance under interventions, and give stronger graphical conditions for this property than those implied by the ﬁrst result. The second result also improves the earlier, similar results due to Spirtes et al. (1993). Keywords: ancestral graphs, causal Bayesian network, do-calculus, intervention</p><p>6 0.37719965 <a title="84-lsi-6" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>7 0.30476037 <a title="84-lsi-7" href="./jmlr-2008-Graphical_Methods_for_Efficient_Likelihood_Inference_in_Gaussian_Covariance_Models.html">40 jmlr-2008-Graphical Methods for Efficient Likelihood Inference in Gaussian Covariance Models</a></p>
<p>8 0.28945208 <a title="84-lsi-8" href="./jmlr-2008-Minimal_Nonlinear_Distortion_Principle_for_Nonlinear_Independent_Component_Analysis.html">60 jmlr-2008-Minimal Nonlinear Distortion Principle for Nonlinear Independent Component Analysis</a></p>
<p>9 0.26010883 <a title="84-lsi-9" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>10 0.22161075 <a title="84-lsi-10" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>11 0.21561387 <a title="84-lsi-11" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>12 0.21018629 <a title="84-lsi-12" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>13 0.1905738 <a title="84-lsi-13" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>14 0.18035257 <a title="84-lsi-14" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>15 0.1744795 <a title="84-lsi-15" href="./jmlr-2008-Active_Learning_of_Causal_Networks_with_Intervention_Experiments_and_Optimal_Designs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">10 jmlr-2008-Active Learning of Causal Networks with Intervention Experiments and Optimal Designs    (Special Topic on Causality)</a></p>
<p>16 0.17172837 <a title="84-lsi-16" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>17 0.16079859 <a title="84-lsi-17" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>18 0.1591386 <a title="84-lsi-18" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>19 0.15729181 <a title="84-lsi-19" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>20 0.14829548 <a title="84-lsi-20" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.017), (4, 0.072), (25, 0.064), (26, 0.033), (42, 0.018), (43, 0.013), (66, 0.088), (67, 0.466), (86, 0.025), (91, 0.043), (96, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.54563743 <a title="84-lda-1" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>Author: Tianjiao Chu, Clark Glymour</p><p>Abstract: Pointwise consistent, feasible procedures for estimating contemporaneous linear causal structure from time series data have been developed using multiple conditional independence tests, but no such procedures are available for non-linear systems. We describe a feasible procedure for learning a class of non-linear time series structures, which we call additive non-linear time series. We show that for data generated from stationary models of this type, two classes of conditional independence relations among time series variables and their lags can be tested efﬁciently and consistently using tests based on additive model regression. Combining results of statistical tests for these two classes of conditional independence relations and the temporal structure of time series data, a new consistent model speciﬁcation procedure is able to extract relatively detailed causal information. We investigate the ﬁnite sample behavior of the procedure through simulation, and illustrate the application of this method through analysis of the possible causal connections among four ocean indices. Several variants of the procedure are also discussed. Keywords: conditional independence test, contemporaneous causation, additive model regression, Granger causality, ocean indices</p><p>2 0.27167124 <a title="84-lda-2" href="./jmlr-2008-Learning_Balls_of_Strings_from_Edit_Corrections.html">47 jmlr-2008-Learning Balls of Strings from Edit Corrections</a></p>
<p>Author: Leonor Becerra-Bonache, Colin de la Higuera, Jean-Christophe Janodet, Frédéric Tantini</p><p>Abstract: When facing the question of learning languages in realistic settings, one has to tackle several problems that do not admit simple solutions. On the one hand, languages are usually deﬁned by complex grammatical mechanisms for which the learning results are predominantly negative, as the few algorithms are not really able to cope with noise. On the other hand, the learning settings themselves rely either on too simple information (text) or on unattainable one (query systems that do not exist in practice, nor can be simulated). We consider simple but sound classes of languages deﬁned via the widely used edit distance: the balls of strings. We propose to learn them with the help of a new sort of queries, called the correction queries: when a string is submitted to the Oracle, either she accepts it if it belongs to the target language, or she proposes a correction, that is, a string of the language close to the query with respect to the edit distance. We show that even if the good balls are not learnable in Angluin’s M AT model, they can be learned from a polynomial number of correction queries. Moreover, experimental evidence simulating a human Expert shows that this algorithm is resistant to approximate answers. Keywords: grammatical inference, oracle learning, correction queries, edit distance, balls of strings</p><p>3 0.27113593 <a title="84-lda-3" href="./jmlr-2008-Causal_Reasoning_with_Ancestral_Graphs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">20 jmlr-2008-Causal Reasoning with Ancestral Graphs    (Special Topic on Causality)</a></p>
<p>Author: Jiji Zhang</p><p>Abstract: Causal reasoning is primarily concerned with what would happen to a system under external interventions. In particular, we are often interested in predicting the probability distribution of some random variables that would result if some other variables were forced to take certain values. One prominent approach to tackling this problem is based on causal Bayesian networks, using directed acyclic graphs as causal diagrams to relate post-intervention probabilities to pre-intervention probabilities that are estimable from observational data. However, such causal diagrams are seldom fully testable given observational data. In consequence, many causal discovery algorithms based on data-mining can only output an equivalence class of causal diagrams (rather than a single one). This paper is concerned with causal reasoning given an equivalence class of causal diagrams, represented by a (partial) ancestral graph. We present two main results. The ﬁrst result extends Pearl (1995)’s celebrated do-calculus to the context of ancestral graphs. In the second result, we focus on a key component of Pearl’s calculus—the property of invariance under interventions, and give stronger graphical conditions for this property than those implied by the ﬁrst result. The second result also improves the earlier, similar results due to Spirtes et al. (1993). Keywords: ancestral graphs, causal Bayesian network, do-calculus, intervention</p><p>4 0.26687548 <a title="84-lda-4" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>Author: Xianchao Xie, Zhi Geng</p><p>Abstract: In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is ﬁrst decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efﬁciency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method. Keywords: Bayesian network, conditional independence, decomposition, directed acyclic graph, structural learning</p><p>5 0.26659006 <a title="84-lda-5" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>Author: Eric Perrier, Seiya Imoto, Satoru Miyano</p><p>Abstract: Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. However, a recent empirical study has shown that a hybrid algorithm improves sensitively accuracy and speed: it learns a skeleton with an independency test (IT) approach and constrains on the directed acyclic graphs (DAG) considered during the search-and-score phase. Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(γm n ), where γm < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree m and sparse structures ˜ allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even ﬁnds more accurate results when given a sound super-structure. Practically, S can be approximated by IT approaches; signiﬁcance level of the tests controls its sparseness, enabling to control the trade-off between speed and accuracy. For incomplete super-structures, a greedily post-processed version (COS+) still enables to signiﬁcantly outperform other heuristic searches. Keywords: subset Bayesian networks, structure learning, optimal search, super-structure, connected</p><p>6 0.26586342 <a title="84-lda-6" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>7 0.26367059 <a title="84-lda-7" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>8 0.26221782 <a title="84-lda-8" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>9 0.26141942 <a title="84-lda-9" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>10 0.25950238 <a title="84-lda-10" href="./jmlr-2008-Graphical_Methods_for_Efficient_Likelihood_Inference_in_Gaussian_Covariance_Models.html">40 jmlr-2008-Graphical Methods for Efficient Likelihood Inference in Gaussian Covariance Models</a></p>
<p>11 0.25915441 <a title="84-lda-11" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>12 0.25607541 <a title="84-lda-12" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>13 0.25509253 <a title="84-lda-13" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>14 0.25329927 <a title="84-lda-14" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>15 0.25198022 <a title="84-lda-15" href="./jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">53 jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>16 0.25191751 <a title="84-lda-16" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>17 0.24890001 <a title="84-lda-17" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>18 0.24648398 <a title="84-lda-18" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>19 0.24623391 <a title="84-lda-19" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>20 0.24611484 <a title="84-lda-20" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
