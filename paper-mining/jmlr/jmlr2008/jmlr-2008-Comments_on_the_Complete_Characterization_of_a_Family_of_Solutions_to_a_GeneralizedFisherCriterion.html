<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>23 jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-23" href="#">jmlr2008-23</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>23 jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</h1>
<br/><p>Source: <a title="jmlr-2008-23-pdf" href="http://jmlr.org/papers/volume9/ye08a/ye08a.pdf">pdf</a></p><p>Author: Jieping Ye</p><p>Abstract: Loog (2007) provided a complete characterization of the family of solutions to a generalized Fisher criterion. We show that this characterization is essentially equivalent to the original characterization proposed in Ye (2005). The computational advantage of the original characterization over the new one is discussed, which justiﬁes its practical use. Keywords: linear discriminant analysis, dimension reduction, linear transformation 1. Generalized Fisher Criterion For a given data set consisting of n data points {ai }n in IRd , a linear transformation G ∈ IRd× i=1 ( < d) maps each ai for 1 ≤ i ≤ n in the d-dimensional space to a vector ai in the -dimensional ˜ space as follows: G : ai ∈ IRd → ai = GT ai ∈ IR . ˜ Assume that there are k classes in the data set. The within-class scatter matrix S w , the betweenclass scatter matrix Sb , and the total scatter matrix St involved in linear discriminant analysis are deﬁned as follows (Fukunaga, 1990): k Sw = ∑ (Ai − ci eT )(Ai − ci eT )T , i=1 k Sb = ∑ ni (ci − c)(ci − c)T , i=1 k St = ∑ (Ai − ceT )(Ai − ceT )T , i=1 where Ai denotes the data matrix of the i-th class, ci = Ai e/ni is the centroid of the i-th class, ni is the sample size of the i-th class, c = Ae/n is the global centroid, and e is the vector of all ones with an appropriate length. It is easy to verify that St = Sb + Sw . In Ye (2005), the optimal transformation G is computed by maximizing a generalized Fisher criterion as follows: + G = arg max trace GT St G GT Sb G , (1) m× G∈IR c 2008 Jieping Ye. YE where M + denotes the pseudo-inverse (Golub and Van Loan, 1996) of M and it is introduced to overcome the singularity problem when dealing with high-dimensional low-sample-size data. 1.1 Equivalent Transformation Two linear transformations G1 and G2 can be considered equivalent if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. Indeed, in this case, the difference between the projections 1 2 by G1 and G2 is a mere shift. Deﬁnition 1.1 For a given data set {a1 , · · · , an }, two transformations G1 and G2 are equivalent, if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. 1 2 2. Characterization of Solutions to the Generalized Fisher Criterion Let St = UΣU T be the orthogonal eigendecomposition of St (note that St is symmetric and positive semi-deﬁnite), where U ∈ IRd×d is orthogonal and Σ ∈ IRd×d is diagonal with nonnegative diagonal entries sorted in nonincreasing order. Denote Σr as the r-th principal submatrix of Σ, where r = rank(St ). Partition U into two components as U = [U1 ,U2 ], where U1 ∈ IRd×r and U2 ∈ IRd×(d−r) . Note that r ≤ n, and for high-dimensional low-sample-size data, U1 is much smaller than U2 . In Loog (2007), a complete family of solutions S to the maximization problem in Eq. (1) is given as (We correct the error in Loog (2007) by using U instead of U T .) S= U ΛZ Y ∈ IRd× Z ∈ IR × is nonsingular , Y ∈ IR(n−r)× , where Λ ∈ IRr× maximizes the following objective function: F0 (X) = trace −1 X T Σr X T X T (U1 SbU1 )X . ˜ In Ye (2005), a family of solutions S is given as ˜ S= U ΛZ 0 ∈ IRd× Z ∈ IR × is nonsingular . The only difference between these two characterizations of solutions is the matrix Y in S , which is ˜ replaced by the zero matrix in S . We show in the next section the equivalence relationship between these two characterizations. 3. Equivalent Solution Characterizations ˜ Consider the following two transformations G1 and G2 from S and S respectively: G1 = U ΛZ Y ∈ S, G2 = U 518 ΛZ 0 ˜ ∈ S. O N THE C OMPLETE C HARACTERIZATION OF S OLUTIONS TO A G ENERALIZED F ISHER C RITERION Recall that U = [U1 ,U2 ], where the columns of U2 span the null space of St . Hence, n T T T 0 = U2 St U2 = ∑ U2 (ai − c) · (U2 (ai − c))T , i=1 T and U2 (ai − c) = 0, for i = 1, · · · , n, where c is the global centroid. It follows that T T T GT (ai − c) = Z T ΛT U1 (ai − c) +Y T U2 (ai − c) = Z T ΛT U1 (ai − c) = GT (ai − c), 1 2 for i = 1, · · · , n. That is, G1 and G2 are equivalent transformations. Hence, the two solution charac˜ terizations S and S are essentially equivalent. Remark 3.1 The analysis above shows that the additional information contained in S is the null ˜ space, U2 , of St , which leads to an equivalent transformation. In S , the null space U2 is removed, which can be further justiﬁed as follows. Since St = Sb + Sw , we have T T T 0 = U2 St U2 = U2 SbU2 +U2 SwU2 . T It follows that U2 SbU2 = 0, as both Sb and Sw are positive semi-deﬁnite. Thus, the null space U2 does not contain any discriminant information. This explains why the null space of St is removed in most discriminant analysis based algorithms proposed in the past. 4. Efﬁciency Comparison In S , the full matrix U is involved, whose computation may be expensive, especially for high˜ dimensional data. In contrast, only the ﬁrst component U1 ∈ IRd×r of U is involved in S , which can be computed efﬁciently for high-dimensional low-sample-size problem by directly working on the Gram matrix instead of the covariance matrix. ˜ In summary, we show that S and S are equivalent characterizations of the solutions to the generalized Fisher criterion in Eq. (1). However, the latter one is preferred in practice due to its relative efﬁciency for high-dimensional low-sample-size data. References K. Fukunaga. Introduction to Statistical Pattern Classiﬁcation. Academic Press, San Diego, California, USA, 1990. G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, MD, USA, third edition, 1996. M. Loog. A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion. Journal of Machine Learning Research, 8:2121–2123, 2007. J. Ye. Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems. Journal of Machine Learning Research, 6:483–502, 2005. 519</p><p>Reference: <a title="jmlr-2008-23-reference" href="../jmlr2008_reference/jmlr-2008-Comments_on_the_Complete_Characterization_of_a_Family_of_Solutions_to_a_GeneralizedFisherCriterion_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Department of Computer Science and Engineering Arizona State University Tempe, AZ 85287, USA  Editor: Xiaotong Shen  Abstract Loog (2007) provided a complete characterization of the family of solutions to a generalized Fisher criterion. [sent-3, score-0.446]
</p><p>2 We show that this characterization is essentially equivalent to the original characterization proposed in Ye (2005). [sent-4, score-0.397]
</p><p>3 The computational advantage of the original characterization over the new one is discussed, which justiﬁes its practical use. [sent-5, score-0.173]
</p><p>4 Keywords: linear discriminant analysis, dimension reduction, linear transformation  1. [sent-6, score-0.175]
</p><p>5 Generalized Fisher Criterion For a given data set consisting of n data points {ai }n in IRd , a linear transformation G ∈ IRd× i=1 ( < d) maps each ai for 1 ≤ i ≤ n in the d-dimensional space to a vector ai in the -dimensional ˜ space as follows: G : ai ∈ IRd → ai = GT ai ∈ IR . [sent-7, score-1.5]
</p><p>6 In Ye (2005), the optimal transformation G is computed by maximizing a generalized Fisher criterion as follows: + G = arg max trace GT St G GT Sb G , (1) m× G∈IR c 2008 Jieping Ye. [sent-11, score-0.279]
</p><p>7 YE  where M + denotes the pseudo-inverse (Golub and Van Loan, 1996) of M and it is introduced to overcome the singularity problem when dealing with high-dimensional low-sample-size data. [sent-12, score-0.044]
</p><p>8 1 Equivalent Transformation Two linear transformations G1 and G2 can be considered equivalent if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. [sent-14, score-0.118]
</p><p>9 Indeed, in this case, the difference between the projections 1 2 by G1 and G2 is a mere shift. [sent-15, score-0.068]
</p><p>10 1 For a given data set {a1 , · · · , an }, two transformations G1 and G2 are equivalent, if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. [sent-17, score-0.092]
</p><p>11 Denote Σr as the r-th principal submatrix of Σ, where r = rank(St ). [sent-20, score-0.053]
</p><p>12 In Loog (2007), a complete family of solutions S to the maximization problem in Eq. [sent-23, score-0.208]
</p><p>13 )  S= U  ΛZ Y  ∈ IRd×  Z ∈ IR  ×  is nonsingular , Y ∈ IR(n−r)×  ,  where Λ ∈ IRr× maximizes the following objective function: F0 (X) = trace  −1  X T Σr X  T X T (U1 SbU1 )X . [sent-25, score-0.14]
</p><p>14 ˜ In Ye (2005), a family of solutions S is given as ˜ S= U  ΛZ 0  ∈ IRd×  Z ∈ IR  ×  is nonsingular  . [sent-26, score-0.237]
</p><p>15 The only difference between these two characterizations of solutions is the matrix Y in S , which is ˜ replaced by the zero matrix in S . [sent-27, score-0.299]
</p><p>16 We show in the next section the equivalence relationship between these two characterizations. [sent-28, score-0.027]
</p><p>17 Equivalent Solution Characterizations ˜ Consider the following two transformations G1 and G2 from S and S respectively: G1 = U  ΛZ Y  ∈ S,  G2 = U  518  ΛZ 0  ˜ ∈ S. [sent-30, score-0.092]
</p><p>18 O N THE C OMPLETE C HARACTERIZATION OF S OLUTIONS TO A G ENERALIZED F ISHER C RITERION  Recall that U = [U1 ,U2 ], where the columns of U2 span the null space of St . [sent-31, score-0.137]
</p><p>19 Hence, n  T T T 0 = U2 St U2 = ∑ U2 (ai − c) · (U2 (ai − c))T , i=1  T and U2 (ai − c) = 0, for i = 1, · · · , n, where c is the global centroid. [sent-32, score-0.017]
</p><p>20 It follows that T T T GT (ai − c) = Z T ΛT U1 (ai − c) +Y T U2 (ai − c) = Z T ΛT U1 (ai − c) = GT (ai − c), 1 2  for i = 1, · · · , n. [sent-33, score-0.013]
</p><p>21 Hence, the two solution charac˜ terizations S and S are essentially equivalent. [sent-35, score-0.025]
</p><p>22 1 The analysis above shows that the additional information contained in S is the null ˜ space, U2 , of St , which leads to an equivalent transformation. [sent-37, score-0.14]
</p><p>23 In S , the null space U2 is removed, which can be further justiﬁed as follows. [sent-38, score-0.112]
</p><p>24 T It follows that U2 SbU2 = 0, as both Sb and Sw are positive semi-deﬁnite. [sent-40, score-0.013]
</p><p>25 Thus, the null space U2 does not contain any discriminant information. [sent-41, score-0.217]
</p><p>26 This explains why the null space of St is removed in most discriminant analysis based algorithms proposed in the past. [sent-42, score-0.278]
</p><p>27 Efﬁciency Comparison In S , the full matrix U is involved, whose computation may be expensive, especially for high˜ dimensional data. [sent-44, score-0.045]
</p><p>28 In contrast, only the ﬁrst component U1 ∈ IRd×r of U is involved in S , which can be computed efﬁciently for high-dimensional low-sample-size problem by directly working on the Gram matrix instead of the covariance matrix. [sent-45, score-0.113]
</p><p>29 ˜ In summary, we show that S and S are equivalent characterizations of the solutions to the generalized Fisher criterion in Eq. [sent-46, score-0.366]
</p><p>30 However, the latter one is preferred in practice due to its relative efﬁciency for high-dimensional low-sample-size data. [sent-48, score-0.022]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ird', 0.501), ('st', 0.325), ('gt', 0.29), ('ai', 0.272), ('sb', 0.269), ('ye', 0.23), ('ir', 0.206), ('sw', 0.179), ('characterization', 0.173), ('fisher', 0.157), ('jieping', 0.15), ('loog', 0.15), ('characterizations', 0.123), ('scatter', 0.108), ('discriminant', 0.105), ('cet', 0.1), ('null', 0.095), ('transformations', 0.092), ('solutions', 0.086), ('generalized', 0.081), ('ci', 0.077), ('centroid', 0.077), ('nonsingular', 0.077), ('family', 0.074), ('transformation', 0.07), ('golub', 0.068), ('criterion', 0.05), ('isher', 0.05), ('baltimore', 0.05), ('olutions', 0.05), ('matrix', 0.045), ('xiaotong', 0.045), ('mere', 0.045), ('asu', 0.045), ('fukunaga', 0.045), ('haracterization', 0.045), ('tempe', 0.045), ('undersampled', 0.045), ('ni', 0.043), ('trace', 0.043), ('omplete', 0.041), ('md', 0.041), ('arizona', 0.041), ('eigendecomposition', 0.041), ('orthogonal', 0.039), ('diego', 0.038), ('hopkins', 0.038), ('johns', 0.038), ('van', 0.038), ('involved', 0.038), ('justi', 0.036), ('loan', 0.036), ('az', 0.036), ('submatrix', 0.036), ('removed', 0.033), ('gram', 0.032), ('complete', 0.032), ('shen', 0.031), ('diagonal', 0.028), ('explains', 0.028), ('usa', 0.027), ('equivalent', 0.026), ('essentially', 0.025), ('span', 0.025), ('sorted', 0.024), ('projections', 0.023), ('nonnegative', 0.023), ('preferred', 0.022), ('ciency', 0.022), ('overcome', 0.022), ('dealing', 0.022), ('maximizes', 0.02), ('california', 0.02), ('arg', 0.019), ('maps', 0.019), ('contained', 0.019), ('remark', 0.018), ('summary', 0.017), ('working', 0.017), ('consisting', 0.017), ('principal', 0.017), ('global', 0.017), ('space', 0.017), ('edition', 0.016), ('san', 0.016), ('academic', 0.016), ('maximization', 0.016), ('maximizing', 0.016), ('partition', 0.016), ('entries', 0.015), ('rank', 0.015), ('equivalence', 0.015), ('expensive', 0.015), ('comments', 0.014), ('covariance', 0.013), ('verify', 0.013), ('follows', 0.013), ('relationship', 0.012), ('reduction', 0.012), ('engineering', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="23-tfidf-1" href="./jmlr-2008-Comments_on_the_Complete_Characterization_of_a_Family_of_Solutions_to_a_GeneralizedFisherCriterion.html">23 jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</a></p>
<p>Author: Jieping Ye</p><p>Abstract: Loog (2007) provided a complete characterization of the family of solutions to a generalized Fisher criterion. We show that this characterization is essentially equivalent to the original characterization proposed in Ye (2005). The computational advantage of the original characterization over the new one is discussed, which justiﬁes its practical use. Keywords: linear discriminant analysis, dimension reduction, linear transformation 1. Generalized Fisher Criterion For a given data set consisting of n data points {ai }n in IRd , a linear transformation G ∈ IRd× i=1 ( < d) maps each ai for 1 ≤ i ≤ n in the d-dimensional space to a vector ai in the -dimensional ˜ space as follows: G : ai ∈ IRd → ai = GT ai ∈ IR . ˜ Assume that there are k classes in the data set. The within-class scatter matrix S w , the betweenclass scatter matrix Sb , and the total scatter matrix St involved in linear discriminant analysis are deﬁned as follows (Fukunaga, 1990): k Sw = ∑ (Ai − ci eT )(Ai − ci eT )T , i=1 k Sb = ∑ ni (ci − c)(ci − c)T , i=1 k St = ∑ (Ai − ceT )(Ai − ceT )T , i=1 where Ai denotes the data matrix of the i-th class, ci = Ai e/ni is the centroid of the i-th class, ni is the sample size of the i-th class, c = Ae/n is the global centroid, and e is the vector of all ones with an appropriate length. It is easy to verify that St = Sb + Sw . In Ye (2005), the optimal transformation G is computed by maximizing a generalized Fisher criterion as follows: + G = arg max trace GT St G GT Sb G , (1) m× G∈IR c 2008 Jieping Ye. YE where M + denotes the pseudo-inverse (Golub and Van Loan, 1996) of M and it is introduced to overcome the singularity problem when dealing with high-dimensional low-sample-size data. 1.1 Equivalent Transformation Two linear transformations G1 and G2 can be considered equivalent if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. Indeed, in this case, the difference between the projections 1 2 by G1 and G2 is a mere shift. Deﬁnition 1.1 For a given data set {a1 , · · · , an }, two transformations G1 and G2 are equivalent, if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. 1 2 2. Characterization of Solutions to the Generalized Fisher Criterion Let St = UΣU T be the orthogonal eigendecomposition of St (note that St is symmetric and positive semi-deﬁnite), where U ∈ IRd×d is orthogonal and Σ ∈ IRd×d is diagonal with nonnegative diagonal entries sorted in nonincreasing order. Denote Σr as the r-th principal submatrix of Σ, where r = rank(St ). Partition U into two components as U = [U1 ,U2 ], where U1 ∈ IRd×r and U2 ∈ IRd×(d−r) . Note that r ≤ n, and for high-dimensional low-sample-size data, U1 is much smaller than U2 . In Loog (2007), a complete family of solutions S to the maximization problem in Eq. (1) is given as (We correct the error in Loog (2007) by using U instead of U T .) S= U ΛZ Y ∈ IRd× Z ∈ IR × is nonsingular , Y ∈ IR(n−r)× , where Λ ∈ IRr× maximizes the following objective function: F0 (X) = trace −1 X T Σr X T X T (U1 SbU1 )X . ˜ In Ye (2005), a family of solutions S is given as ˜ S= U ΛZ 0 ∈ IRd× Z ∈ IR × is nonsingular . The only difference between these two characterizations of solutions is the matrix Y in S , which is ˜ replaced by the zero matrix in S . We show in the next section the equivalence relationship between these two characterizations. 3. Equivalent Solution Characterizations ˜ Consider the following two transformations G1 and G2 from S and S respectively: G1 = U ΛZ Y ∈ S, G2 = U 518 ΛZ 0 ˜ ∈ S. O N THE C OMPLETE C HARACTERIZATION OF S OLUTIONS TO A G ENERALIZED F ISHER C RITERION Recall that U = [U1 ,U2 ], where the columns of U2 span the null space of St . Hence, n T T T 0 = U2 St U2 = ∑ U2 (ai − c) · (U2 (ai − c))T , i=1 T and U2 (ai − c) = 0, for i = 1, · · · , n, where c is the global centroid. It follows that T T T GT (ai − c) = Z T ΛT U1 (ai − c) +Y T U2 (ai − c) = Z T ΛT U1 (ai − c) = GT (ai − c), 1 2 for i = 1, · · · , n. That is, G1 and G2 are equivalent transformations. Hence, the two solution charac˜ terizations S and S are essentially equivalent. Remark 3.1 The analysis above shows that the additional information contained in S is the null ˜ space, U2 , of St , which leads to an equivalent transformation. In S , the null space U2 is removed, which can be further justiﬁed as follows. Since St = Sb + Sw , we have T T T 0 = U2 St U2 = U2 SbU2 +U2 SwU2 . T It follows that U2 SbU2 = 0, as both Sb and Sw are positive semi-deﬁnite. Thus, the null space U2 does not contain any discriminant information. This explains why the null space of St is removed in most discriminant analysis based algorithms proposed in the past. 4. Efﬁciency Comparison In S , the full matrix U is involved, whose computation may be expensive, especially for high˜ dimensional data. In contrast, only the ﬁrst component U1 ∈ IRd×r of U is involved in S , which can be computed efﬁciently for high-dimensional low-sample-size problem by directly working on the Gram matrix instead of the covariance matrix. ˜ In summary, we show that S and S are equivalent characterizations of the solutions to the generalized Fisher criterion in Eq. (1). However, the latter one is preferred in practice due to its relative efﬁciency for high-dimensional low-sample-size data. References K. Fukunaga. Introduction to Statistical Pattern Classiﬁcation. Academic Press, San Diego, California, USA, 1990. G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, MD, USA, third edition, 1996. M. Loog. A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion. Journal of Machine Learning Research, 8:2121–2123, 2007. J. Ye. Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems. Journal of Machine Learning Research, 6:483–502, 2005. 519</p><p>2 0.27541953 <a title="23-tfidf-2" href="./jmlr-2008-On_the_Equivalence_of_Linear_Dimensionality-Reducing_Transformations.html">71 jmlr-2008-On the Equivalence of Linear Dimensionality-Reducing Transformations</a></p>
<p>Author: Marco Loog</p><p>Abstract: In this JMLR volume, Ye (2008) demonstrates the essential equivalence of two sets of solutions to a generalized Fisher criterion used for linear dimensionality reduction (see Ye, 2005; Loog, 2007). Here, I point out the basic ﬂaw in this new contribution. Keywords: linear discriminant analysis, equivalence relation, linear subspaces, Bayes error</p><p>3 0.062678784 <a title="23-tfidf-3" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>Author: Liviu Panait, Karl Tuyls, Sean Luke</p><p>Abstract: This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the beneﬁts of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction</p><p>4 0.061208148 <a title="23-tfidf-4" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>Author: Jieping Ye, Shuiwang Ji, Jianhui Chen</p><p>Abstract: Regularized kernel discriminant analysis (RKDA) performs linear discriminant analysis in the feature space via the kernel trick. Its performance depends on the selection of kernels. In this paper, we consider the problem of multiple kernel learning (MKL) for RKDA, in which the optimal kernel matrix is obtained as a linear combination of pre-speciﬁed kernel matrices. We show that the kernel learning problem in RKDA can be formulated as convex programs. First, we show that this problem can be formulated as a semideﬁnite program (SDP). Based on the equivalence relationship between RKDA and least square problems in the binary-class case, we propose a convex quadratically constrained quadratic programming (QCQP) formulation for kernel learning in RKDA. A semi-inﬁnite linear programming (SILP) formulation is derived to further improve the efﬁciency. We extend these formulations to the multi-class case based on a key result established in this paper. That is, the multi-class RKDA kernel learning problem can be decomposed into a set of binary-class kernel learning problems which are constrained to share a common kernel. Based on this decomposition property, SDP formulations are proposed for the multi-class case. Furthermore, it leads naturally to QCQP and SILP formulations. As the performance of RKDA depends on the regularization parameter, we show that this parameter can also be optimized in a joint framework with the kernel. Extensive experiments have been conducted and analyzed, and connections to other algorithms are discussed. Keywords: model selection, kernel discriminant analysis, semideﬁnite programming, quadratically constrained quadratic programming, semi-inﬁnite linear programming</p><p>5 0.051028091 <a title="23-tfidf-5" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: In this paper, the naive credal classiﬁer, which is a set-valued counterpart of naive Bayes, is extended to a general and ﬂexible treatment of incomplete data, yielding a new classiﬁer called naive credal classiﬁer 2 (NCC2). The new classiﬁer delivers classiﬁcations that are reliable even in the presence of small sample sizes and missing values. Extensive empirical evaluations show that, by issuing set-valued classiﬁcations, NCC2 is able to isolate and properly deal with instances that are hard to classify (on which naive Bayes accuracy drops considerably), and to perform as well as naive Bayes on the other instances. The experiments point to a general problem: they show that with missing values, empirical evaluations may not reliably estimate the accuracy of a traditional classiﬁer, such as naive Bayes. This phenomenon adds even more value to the robust approach to classiﬁcation implemented by NCC2. Keywords: naive Bayes, naive credal classiﬁer, imprecise probabilities, missing values, conservative inference rule, missing at random</p><p>6 0.049202815 <a title="23-tfidf-6" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>7 0.045314323 <a title="23-tfidf-7" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>8 0.03243231 <a title="23-tfidf-8" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>9 0.03130785 <a title="23-tfidf-9" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>10 0.027717628 <a title="23-tfidf-10" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>11 0.027674936 <a title="23-tfidf-11" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>12 0.025355175 <a title="23-tfidf-12" href="./jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">80 jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>13 0.024704158 <a title="23-tfidf-13" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>14 0.021036705 <a title="23-tfidf-14" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>15 0.020297494 <a title="23-tfidf-15" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>16 0.020063663 <a title="23-tfidf-16" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>17 0.019797247 <a title="23-tfidf-17" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>18 0.0191062 <a title="23-tfidf-18" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>19 0.018467942 <a title="23-tfidf-19" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>20 0.017195249 <a title="23-tfidf-20" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.106), (1, -0.042), (2, -0.068), (3, -0.016), (4, -0.047), (5, -0.061), (6, 0.035), (7, 0.177), (8, -0.18), (9, 0.275), (10, 0.449), (11, 0.215), (12, -0.412), (13, -0.106), (14, -0.117), (15, 0.048), (16, -0.086), (17, 0.048), (18, -0.04), (19, 0.043), (20, -0.035), (21, 0.016), (22, 0.003), (23, 0.022), (24, 0.004), (25, -0.049), (26, 0.0), (27, 0.009), (28, -0.047), (29, 0.007), (30, 0.015), (31, -0.014), (32, -0.002), (33, 0.033), (34, 0.021), (35, 0.011), (36, 0.012), (37, -0.042), (38, -0.019), (39, -0.022), (40, -0.045), (41, 0.016), (42, -0.059), (43, -0.0), (44, -0.02), (45, 0.004), (46, 0.032), (47, 0.017), (48, -0.01), (49, -0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98642313 <a title="23-lsi-1" href="./jmlr-2008-Comments_on_the_Complete_Characterization_of_a_Family_of_Solutions_to_a_GeneralizedFisherCriterion.html">23 jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</a></p>
<p>Author: Jieping Ye</p><p>Abstract: Loog (2007) provided a complete characterization of the family of solutions to a generalized Fisher criterion. We show that this characterization is essentially equivalent to the original characterization proposed in Ye (2005). The computational advantage of the original characterization over the new one is discussed, which justiﬁes its practical use. Keywords: linear discriminant analysis, dimension reduction, linear transformation 1. Generalized Fisher Criterion For a given data set consisting of n data points {ai }n in IRd , a linear transformation G ∈ IRd× i=1 ( < d) maps each ai for 1 ≤ i ≤ n in the d-dimensional space to a vector ai in the -dimensional ˜ space as follows: G : ai ∈ IRd → ai = GT ai ∈ IR . ˜ Assume that there are k classes in the data set. The within-class scatter matrix S w , the betweenclass scatter matrix Sb , and the total scatter matrix St involved in linear discriminant analysis are deﬁned as follows (Fukunaga, 1990): k Sw = ∑ (Ai − ci eT )(Ai − ci eT )T , i=1 k Sb = ∑ ni (ci − c)(ci − c)T , i=1 k St = ∑ (Ai − ceT )(Ai − ceT )T , i=1 where Ai denotes the data matrix of the i-th class, ci = Ai e/ni is the centroid of the i-th class, ni is the sample size of the i-th class, c = Ae/n is the global centroid, and e is the vector of all ones with an appropriate length. It is easy to verify that St = Sb + Sw . In Ye (2005), the optimal transformation G is computed by maximizing a generalized Fisher criterion as follows: + G = arg max trace GT St G GT Sb G , (1) m× G∈IR c 2008 Jieping Ye. YE where M + denotes the pseudo-inverse (Golub and Van Loan, 1996) of M and it is introduced to overcome the singularity problem when dealing with high-dimensional low-sample-size data. 1.1 Equivalent Transformation Two linear transformations G1 and G2 can be considered equivalent if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. Indeed, in this case, the difference between the projections 1 2 by G1 and G2 is a mere shift. Deﬁnition 1.1 For a given data set {a1 , · · · , an }, two transformations G1 and G2 are equivalent, if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. 1 2 2. Characterization of Solutions to the Generalized Fisher Criterion Let St = UΣU T be the orthogonal eigendecomposition of St (note that St is symmetric and positive semi-deﬁnite), where U ∈ IRd×d is orthogonal and Σ ∈ IRd×d is diagonal with nonnegative diagonal entries sorted in nonincreasing order. Denote Σr as the r-th principal submatrix of Σ, where r = rank(St ). Partition U into two components as U = [U1 ,U2 ], where U1 ∈ IRd×r and U2 ∈ IRd×(d−r) . Note that r ≤ n, and for high-dimensional low-sample-size data, U1 is much smaller than U2 . In Loog (2007), a complete family of solutions S to the maximization problem in Eq. (1) is given as (We correct the error in Loog (2007) by using U instead of U T .) S= U ΛZ Y ∈ IRd× Z ∈ IR × is nonsingular , Y ∈ IR(n−r)× , where Λ ∈ IRr× maximizes the following objective function: F0 (X) = trace −1 X T Σr X T X T (U1 SbU1 )X . ˜ In Ye (2005), a family of solutions S is given as ˜ S= U ΛZ 0 ∈ IRd× Z ∈ IR × is nonsingular . The only difference between these two characterizations of solutions is the matrix Y in S , which is ˜ replaced by the zero matrix in S . We show in the next section the equivalence relationship between these two characterizations. 3. Equivalent Solution Characterizations ˜ Consider the following two transformations G1 and G2 from S and S respectively: G1 = U ΛZ Y ∈ S, G2 = U 518 ΛZ 0 ˜ ∈ S. O N THE C OMPLETE C HARACTERIZATION OF S OLUTIONS TO A G ENERALIZED F ISHER C RITERION Recall that U = [U1 ,U2 ], where the columns of U2 span the null space of St . Hence, n T T T 0 = U2 St U2 = ∑ U2 (ai − c) · (U2 (ai − c))T , i=1 T and U2 (ai − c) = 0, for i = 1, · · · , n, where c is the global centroid. It follows that T T T GT (ai − c) = Z T ΛT U1 (ai − c) +Y T U2 (ai − c) = Z T ΛT U1 (ai − c) = GT (ai − c), 1 2 for i = 1, · · · , n. That is, G1 and G2 are equivalent transformations. Hence, the two solution charac˜ terizations S and S are essentially equivalent. Remark 3.1 The analysis above shows that the additional information contained in S is the null ˜ space, U2 , of St , which leads to an equivalent transformation. In S , the null space U2 is removed, which can be further justiﬁed as follows. Since St = Sb + Sw , we have T T T 0 = U2 St U2 = U2 SbU2 +U2 SwU2 . T It follows that U2 SbU2 = 0, as both Sb and Sw are positive semi-deﬁnite. Thus, the null space U2 does not contain any discriminant information. This explains why the null space of St is removed in most discriminant analysis based algorithms proposed in the past. 4. Efﬁciency Comparison In S , the full matrix U is involved, whose computation may be expensive, especially for high˜ dimensional data. In contrast, only the ﬁrst component U1 ∈ IRd×r of U is involved in S , which can be computed efﬁciently for high-dimensional low-sample-size problem by directly working on the Gram matrix instead of the covariance matrix. ˜ In summary, we show that S and S are equivalent characterizations of the solutions to the generalized Fisher criterion in Eq. (1). However, the latter one is preferred in practice due to its relative efﬁciency for high-dimensional low-sample-size data. References K. Fukunaga. Introduction to Statistical Pattern Classiﬁcation. Academic Press, San Diego, California, USA, 1990. G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, MD, USA, third edition, 1996. M. Loog. A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion. Journal of Machine Learning Research, 8:2121–2123, 2007. J. Ye. Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems. Journal of Machine Learning Research, 6:483–502, 2005. 519</p><p>2 0.94454485 <a title="23-lsi-2" href="./jmlr-2008-On_the_Equivalence_of_Linear_Dimensionality-Reducing_Transformations.html">71 jmlr-2008-On the Equivalence of Linear Dimensionality-Reducing Transformations</a></p>
<p>Author: Marco Loog</p><p>Abstract: In this JMLR volume, Ye (2008) demonstrates the essential equivalence of two sets of solutions to a generalized Fisher criterion used for linear dimensionality reduction (see Ye, 2005; Loog, 2007). Here, I point out the basic ﬂaw in this new contribution. Keywords: linear discriminant analysis, equivalence relation, linear subspaces, Bayes error</p><p>3 0.24920566 <a title="23-lsi-3" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>Author: Liviu Panait, Karl Tuyls, Sean Luke</p><p>Abstract: This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the beneﬁts of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction</p><p>4 0.16934223 <a title="23-lsi-4" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>Author: Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui</p><p>Abstract: Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefﬁcients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semideﬁnite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefﬁcients, with total complexity O(n3 ), where n is the number of variables. We then use the same relaxation to derive sufﬁcient conditions for global optimality of a solution, which can be tested in O(n3 ) per pattern. We discuss applications in subset selection and sparse recovery and show on artiﬁcial examples and biological data that our algorithm does provide globally optimal solutions in many cases. Keywords: PCA, subset selection, sparse eigenvalues, sparse recovery, lasso</p><p>5 0.1681993 <a title="23-lsi-5" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>6 0.14999859 <a title="23-lsi-6" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>7 0.14074336 <a title="23-lsi-7" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>8 0.10590224 <a title="23-lsi-8" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>9 0.10387183 <a title="23-lsi-9" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>10 0.098608583 <a title="23-lsi-10" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>11 0.092515007 <a title="23-lsi-11" href="./jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">80 jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>12 0.09175764 <a title="23-lsi-12" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>13 0.084355667 <a title="23-lsi-13" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>14 0.084055722 <a title="23-lsi-14" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>15 0.079759769 <a title="23-lsi-15" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>16 0.07419724 <a title="23-lsi-16" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>17 0.07206247 <a title="23-lsi-17" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>18 0.071376175 <a title="23-lsi-18" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>19 0.068868183 <a title="23-lsi-19" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>20 0.066614166 <a title="23-lsi-20" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.027), (31, 0.01), (40, 0.019), (55, 0.085), (58, 0.075), (66, 0.045), (74, 0.468), (76, 0.017), (88, 0.047), (92, 0.038), (94, 0.022), (99, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83898729 <a title="23-lda-1" href="./jmlr-2008-Comments_on_the_Complete_Characterization_of_a_Family_of_Solutions_to_a_GeneralizedFisherCriterion.html">23 jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</a></p>
<p>Author: Jieping Ye</p><p>Abstract: Loog (2007) provided a complete characterization of the family of solutions to a generalized Fisher criterion. We show that this characterization is essentially equivalent to the original characterization proposed in Ye (2005). The computational advantage of the original characterization over the new one is discussed, which justiﬁes its practical use. Keywords: linear discriminant analysis, dimension reduction, linear transformation 1. Generalized Fisher Criterion For a given data set consisting of n data points {ai }n in IRd , a linear transformation G ∈ IRd× i=1 ( < d) maps each ai for 1 ≤ i ≤ n in the d-dimensional space to a vector ai in the -dimensional ˜ space as follows: G : ai ∈ IRd → ai = GT ai ∈ IR . ˜ Assume that there are k classes in the data set. The within-class scatter matrix S w , the betweenclass scatter matrix Sb , and the total scatter matrix St involved in linear discriminant analysis are deﬁned as follows (Fukunaga, 1990): k Sw = ∑ (Ai − ci eT )(Ai − ci eT )T , i=1 k Sb = ∑ ni (ci − c)(ci − c)T , i=1 k St = ∑ (Ai − ceT )(Ai − ceT )T , i=1 where Ai denotes the data matrix of the i-th class, ci = Ai e/ni is the centroid of the i-th class, ni is the sample size of the i-th class, c = Ae/n is the global centroid, and e is the vector of all ones with an appropriate length. It is easy to verify that St = Sb + Sw . In Ye (2005), the optimal transformation G is computed by maximizing a generalized Fisher criterion as follows: + G = arg max trace GT St G GT Sb G , (1) m× G∈IR c 2008 Jieping Ye. YE where M + denotes the pseudo-inverse (Golub and Van Loan, 1996) of M and it is introduced to overcome the singularity problem when dealing with high-dimensional low-sample-size data. 1.1 Equivalent Transformation Two linear transformations G1 and G2 can be considered equivalent if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. Indeed, in this case, the difference between the projections 1 2 by G1 and G2 is a mere shift. Deﬁnition 1.1 For a given data set {a1 , · · · , an }, two transformations G1 and G2 are equivalent, if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. 1 2 2. Characterization of Solutions to the Generalized Fisher Criterion Let St = UΣU T be the orthogonal eigendecomposition of St (note that St is symmetric and positive semi-deﬁnite), where U ∈ IRd×d is orthogonal and Σ ∈ IRd×d is diagonal with nonnegative diagonal entries sorted in nonincreasing order. Denote Σr as the r-th principal submatrix of Σ, where r = rank(St ). Partition U into two components as U = [U1 ,U2 ], where U1 ∈ IRd×r and U2 ∈ IRd×(d−r) . Note that r ≤ n, and for high-dimensional low-sample-size data, U1 is much smaller than U2 . In Loog (2007), a complete family of solutions S to the maximization problem in Eq. (1) is given as (We correct the error in Loog (2007) by using U instead of U T .) S= U ΛZ Y ∈ IRd× Z ∈ IR × is nonsingular , Y ∈ IR(n−r)× , where Λ ∈ IRr× maximizes the following objective function: F0 (X) = trace −1 X T Σr X T X T (U1 SbU1 )X . ˜ In Ye (2005), a family of solutions S is given as ˜ S= U ΛZ 0 ∈ IRd× Z ∈ IR × is nonsingular . The only difference between these two characterizations of solutions is the matrix Y in S , which is ˜ replaced by the zero matrix in S . We show in the next section the equivalence relationship between these two characterizations. 3. Equivalent Solution Characterizations ˜ Consider the following two transformations G1 and G2 from S and S respectively: G1 = U ΛZ Y ∈ S, G2 = U 518 ΛZ 0 ˜ ∈ S. O N THE C OMPLETE C HARACTERIZATION OF S OLUTIONS TO A G ENERALIZED F ISHER C RITERION Recall that U = [U1 ,U2 ], where the columns of U2 span the null space of St . Hence, n T T T 0 = U2 St U2 = ∑ U2 (ai − c) · (U2 (ai − c))T , i=1 T and U2 (ai − c) = 0, for i = 1, · · · , n, where c is the global centroid. It follows that T T T GT (ai − c) = Z T ΛT U1 (ai − c) +Y T U2 (ai − c) = Z T ΛT U1 (ai − c) = GT (ai − c), 1 2 for i = 1, · · · , n. That is, G1 and G2 are equivalent transformations. Hence, the two solution charac˜ terizations S and S are essentially equivalent. Remark 3.1 The analysis above shows that the additional information contained in S is the null ˜ space, U2 , of St , which leads to an equivalent transformation. In S , the null space U2 is removed, which can be further justiﬁed as follows. Since St = Sb + Sw , we have T T T 0 = U2 St U2 = U2 SbU2 +U2 SwU2 . T It follows that U2 SbU2 = 0, as both Sb and Sw are positive semi-deﬁnite. Thus, the null space U2 does not contain any discriminant information. This explains why the null space of St is removed in most discriminant analysis based algorithms proposed in the past. 4. Efﬁciency Comparison In S , the full matrix U is involved, whose computation may be expensive, especially for high˜ dimensional data. In contrast, only the ﬁrst component U1 ∈ IRd×r of U is involved in S , which can be computed efﬁciently for high-dimensional low-sample-size problem by directly working on the Gram matrix instead of the covariance matrix. ˜ In summary, we show that S and S are equivalent characterizations of the solutions to the generalized Fisher criterion in Eq. (1). However, the latter one is preferred in practice due to its relative efﬁciency for high-dimensional low-sample-size data. References K. Fukunaga. Introduction to Statistical Pattern Classiﬁcation. Academic Press, San Diego, California, USA, 1990. G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, MD, USA, third edition, 1996. M. Loog. A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion. Journal of Machine Learning Research, 8:2121–2123, 2007. J. Ye. Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems. Journal of Machine Learning Research, 6:483–502, 2005. 519</p><p>2 0.28512123 <a title="23-lda-2" href="./jmlr-2008-On_the_Equivalence_of_Linear_Dimensionality-Reducing_Transformations.html">71 jmlr-2008-On the Equivalence of Linear Dimensionality-Reducing Transformations</a></p>
<p>Author: Marco Loog</p><p>Abstract: In this JMLR volume, Ye (2008) demonstrates the essential equivalence of two sets of solutions to a generalized Fisher criterion used for linear dimensionality reduction (see Ye, 2005; Loog, 2007). Here, I point out the basic ﬂaw in this new contribution. Keywords: linear discriminant analysis, equivalence relation, linear subspaces, Bayes error</p><p>3 0.21539669 <a title="23-lda-3" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>Author: Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui</p><p>Abstract: Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefﬁcients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semideﬁnite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefﬁcients, with total complexity O(n3 ), where n is the number of variables. We then use the same relaxation to derive sufﬁcient conditions for global optimality of a solution, which can be tested in O(n3 ) per pattern. We discuss applications in subset selection and sparse recovery and show on artiﬁcial examples and biological data that our algorithm does provide globally optimal solutions in many cases. Keywords: PCA, subset selection, sparse eigenvalues, sparse recovery, lasso</p><p>4 0.21480684 <a title="23-lda-4" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>Author: Bo Jiang, Xuegong Zhang, Tianxi Cai</p><p>Abstract: Support vector machine (SVM) is one of the most popular and promising classiﬁcation algorithms. After a classiﬁcation rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples. Keywords: k-fold cross-validation, model evaluation, perturbation-resampling, prediction errors, support vector machine</p><p>5 0.19332249 <a title="23-lda-5" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>6 0.19271815 <a title="23-lda-6" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>7 0.19093594 <a title="23-lda-7" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>8 0.18883105 <a title="23-lda-8" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>9 0.18545499 <a title="23-lda-9" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>10 0.18389566 <a title="23-lda-10" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>11 0.18379094 <a title="23-lda-11" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>12 0.18342406 <a title="23-lda-12" href="./jmlr-2008-Consistency_of_Trace_Norm_Minimization.html">26 jmlr-2008-Consistency of Trace Norm Minimization</a></p>
<p>13 0.17998469 <a title="23-lda-13" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>14 0.17939356 <a title="23-lda-14" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>15 0.17931063 <a title="23-lda-15" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>16 0.17916687 <a title="23-lda-16" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>17 0.17716697 <a title="23-lda-17" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>18 0.17703685 <a title="23-lda-18" href="./jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>19 0.17660709 <a title="23-lda-19" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>20 0.17634863 <a title="23-lda-20" href="./jmlr-2008-Causal_Reasoning_with_Ancestral_Graphs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">20 jmlr-2008-Causal Reasoning with Ancestral Graphs    (Special Topic on Causality)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
