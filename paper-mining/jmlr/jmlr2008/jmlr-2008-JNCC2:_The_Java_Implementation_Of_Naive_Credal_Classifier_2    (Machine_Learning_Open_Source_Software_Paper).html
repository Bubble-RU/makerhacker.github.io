<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-45" href="#">jmlr2008-45</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</h1>
<br/><p>Source: <a title="jmlr-2008-45-pdf" href="http://jmlr.org/papers/volume9/corani08b/corani08b.pdf">pdf</a></p><p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: JNCC2 implements the naive credal classiﬁer 2 (NCC2). This is an extension of naive Bayes to imprecise probabilities that aims at delivering robust classiﬁcations also when dealing with small or incomplete data sets. Robustness is achieved by delivering set-valued classiﬁcations (that is, returning multiple classes) on the instances for which (i) the learning set is not informative enough to smooth the effect of choice of the prior density or (ii) the uncertainty arising from missing data prevents the reliable indication of a single class. JNCC2 is released under the GNU GPL license. Keywords: imprecise probabilities, missing data, naive Bayes, naive credal classiﬁer 2, Java</p><p>Reference: <a title="jmlr-2008-45-reference" href="../jmlr2008_reference/jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 CH  IDSIA Istituto Dalle Molle di Studi sull’Intelligenza Artiﬁciale CH-6928 Manno (Lugano), Switzerland  Editor: Mikio Braun  Abstract JNCC2 implements the naive credal classiﬁer 2 (NCC2). [sent-3, score-0.505]
</p><p>2 This is an extension of naive Bayes to imprecise probabilities that aims at delivering robust classiﬁcations also when dealing with small or incomplete data sets. [sent-4, score-0.554]
</p><p>3 Keywords: imprecise probabilities, missing data, naive Bayes, naive credal classiﬁer 2, Java  1. [sent-7, score-1.118]
</p><p>4 Introduction JNCC2 is the Java implementation of naive credal classiﬁer 2 (NCC2) (Corani and Zaffalon, 2008). [sent-8, score-0.505]
</p><p>5 NCC2 extends naive Bayes (NBC) to imprecise probabilities (Walley, 1991) in order to deliver reliable classiﬁcations even on small or incomplete data sets. [sent-9, score-0.59]
</p><p>6 To deal with this problem, NCC2 speciﬁes a set of prior densities, referred to as prior credal set; the credal set is then turned into a set of posteriors via element-wise application of Bayes’ rule. [sent-12, score-0.662]
</p><p>7 Eventually, NCC2 returns the classes that are non-dominated with respect to the set of posterior densities (class c 1 dominates class c2 if the probability of c1 is larger than the probability of c2 for all the posteriors). [sent-13, score-0.089]
</p><p>8 When faced with an instance that would be classiﬁed in a prior-dependent way by naive Bayes, NCC2 will detect multiple non-dominated classes and will then return multiple classes; this is an indeterminate classiﬁcation. [sent-14, score-0.495]
</p><p>9 As for missing data, NCC2 assumes that the missingness process (MP) which generates missing data can be either MAR (that is, missing at random), or unknown; in the latter case, it is referred to as non-MAR. [sent-15, score-0.731]
</p><p>10 As MAR missing data can be safely ignored (Little and Rubin, 1987), NCC2 ignores them. [sent-16, score-0.217]
</p><p>11 On the other hand, NCC2 deals conservatively with non-MAR missing data, that is, it considers all the possible replacements for non-MAR missing data. [sent-17, score-0.473]
</p><p>12 NCC2 can handle mixed situations where some features are subject to a MAR MP and some others to a non-MAR MP; moreover, the list of features subject to the MAR and to the unknown MP can be different between training and test set. [sent-18, score-0.159]
</p><p>13 The conservative treatment of non-MAR missing data generates additional c 2008 Giorgio Corani and Marco Zaffalon. [sent-19, score-0.258]
</p><p>14 C ORANI AND Z AFFALON  indeterminacy of NCC2, as a way to preserve reliability despite the information hidden by missing values and by the fact that the MP is unknown. [sent-20, score-0.293]
</p><p>15 NCC2 can hence be seen as separating “easy” instances, over which it returns a single class, from “hard” instances, over which it returns an indeterminate classiﬁcation. [sent-21, score-0.373]
</p><p>16 Experimental evaluations have shown that the accuracy of naive Bayes sharply drops on the hard instances, while on the same instances NCC2 remains reliable thanks to the indeterminate classiﬁcations. [sent-22, score-0.756]
</p><p>17 The zip ﬁle downloadable from the JNCC2 website contains executables, sources, examples, user manual and tutorial. [sent-35, score-0.205]
</p><p>18 A GUI version of the software will be released in the near future and will be published on the same website. [sent-36, score-0.086]
</p><p>19 Indicators of Performance The performance of NBC is measured by the accuracy, that is, the percentage of correct classiﬁcations. [sent-38, score-0.054]
</p><p>20 To assess the effectiveness of the approach based on imprecise probabilities, the accuracy of naive Bayes is moreover measured separately on the instances recognized as hard and easy by NCC2. [sent-40, score-0.616]
</p><p>21 If NCC2 is effective at separating easy from hard instances, a signiﬁcant difference will be found between the two measures. [sent-41, score-0.068]
</p><p>22 Moreover, JNCC2 computes the confusion matrices of NBC and NCC2; in case of NCC2 the confusion matrix refers to the determinate classiﬁcations only. [sent-42, score-0.181]
</p><p>23 Some Implementation Details JNCC2 loads data from ARFF ﬁles; this is a plain text format, originally developed for WEKA (Witten and Frank, 2005). [sent-44, score-0.086]
</p><p>24 2636  JNCC2, THE I MPLEMENTATION OF NAIVE C REDAL C LASSIFIER 2  As a pre-processing step, JNCC2 discretizes all the numerical features, using the supervised discretization algorithm of Fayyad and Irani (1993). [sent-51, score-0.181]
</p><p>25 The discretization intervals are computed on the training set, and then applied unchanged on the test set. [sent-52, score-0.115]
</p><p>26 validateTestFile() /*loads training and test ﬁle; reads list of non-Mar features; discretizes features*/ parseArffFile(); parseArffTestingFile(); parseNonMar(); discretizeNumFeatures(); /*learns and validates NBC*/ nbc = new NaiveBayes(trainingSet); nbc. [sent-55, score-0.63]
</p><p>27 classifyInstances(testSet); /*learns and validates NCC2; the list of non-Mar features in training and testing is required*/ ncc2 = new NaiveCredalClassiﬁer2(trainingSet, nonMarTraining, nonMarTesting); ncc2. [sent-56, score-0.255]
</p><p>28 classifyInstances(testingSet); /*writes output ﬁles*/ writePerfIndicators(); writePredictions(); JNCC2 can perform three kinds of experiments: training and testing, cross-validation, and classiﬁcation of instances of the test set whose class is unknown. [sent-57, score-0.148]
</p><p>29 The pseudo code of the experiment with training and testing is described by Algorithm 1. [sent-58, score-0.104]
</p><p>30 Examples To run the following examples, move to the directory examples/completeData, generated under the JNCC2 directory after unzipping the package. [sent-60, score-0.408]
</p><p>31 To perform a training and testing experiment, type for instance: “java jncc20. [sent-61, score-0.104]
</p><p>32 As a consequence, JNCC2 will load the training and test set, discretize the numerical features, learn both NBC and NCC2, and use them to predict the instances of the test set. [sent-68, score-0.191]
</p><p>33 Similar experiments can be performed also with the glass and contact-lenses data sets, provided in the same directory. [sent-70, score-0.035]
</p><p>34 JNCC2 will report the performance indicators to ﬁle, together with their observed standard deviations, but it will not write the predictions. [sent-77, score-0.106]
</p><p>35 (As a side remark, if one wants to run crossvalidation, there is no need of splitting the original data set into a training and a testing ﬁle, as it is has been done in this directory. [sent-78, score-0.104]
</p><p>36 ) The directory examples/missingData contains two examples of data sets containing missing data; a look at the provided ﬁles NonMar. [sent-79, score-0.421]
</p><p>37 txt should make it clear how to declare the non-MAR features. [sent-80, score-0.033]
</p><p>38 The directory examples/unkClasses contains two examples in which the class of the instances of the testing set is not available. [sent-81, score-0.384]
</p><p>39 For the iris data set, the experiment is for instance started as follows: “java jncc20. [sent-82, score-0.033]
</p><p>40 Learning reliable classiﬁers from small or incomplete data sets: the naive credal classiﬁer 2. [sent-94, score-0.629]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nbc', 0.345), ('credal', 0.302), ('corani', 0.259), ('indeterminate', 0.259), ('missing', 0.217), ('java', 0.204), ('directory', 0.204), ('naive', 0.203), ('imprecise', 0.193), ('zaffalon', 0.173), ('idsia', 0.155), ('giorgio', 0.155), ('mp', 0.146), ('mar', 0.124), ('instances', 0.112), ('indicators', 0.106), ('arff', 0.102), ('discretizes', 0.102), ('hasler', 0.102), ('trainingset', 0.102), ('cations', 0.091), ('affalon', 0.086), ('gnu', 0.086), ('loads', 0.086), ('orani', 0.086), ('released', 0.086), ('les', 0.083), ('discretization', 0.079), ('bayes', 0.077), ('le', 0.072), ('reliable', 0.072), ('determinate', 0.071), ('delivering', 0.071), ('validates', 0.071), ('testing', 0.068), ('witten', 0.066), ('fayyad', 0.066), ('marco', 0.066), ('website', 0.062), ('classi', 0.061), ('posteriors', 0.058), ('switzerland', 0.055), ('confusion', 0.055), ('percentage', 0.054), ('incomplete', 0.052), ('densities', 0.048), ('features', 0.043), ('irani', 0.043), ('naivebayes', 0.043), ('ciale', 0.043), ('discretize', 0.043), ('indeterminacy', 0.043), ('intelligenza', 0.043), ('istituto', 0.043), ('lassifier', 0.043), ('redal', 0.043), ('studi', 0.043), ('sull', 0.043), ('walley', 0.043), ('generates', 0.041), ('returns', 0.041), ('ch', 0.04), ('accuracy', 0.039), ('weka', 0.039), ('writes', 0.039), ('dalle', 0.039), ('lugano', 0.039), ('manno', 0.039), ('molle', 0.039), ('downloadable', 0.039), ('zip', 0.039), ('reads', 0.039), ('conservatively', 0.039), ('gui', 0.039), ('mikio', 0.039), ('missingness', 0.039), ('learns', 0.039), ('list', 0.037), ('training', 0.036), ('hard', 0.036), ('glass', 0.035), ('deliver', 0.035), ('strati', 0.035), ('sharply', 0.035), ('braun', 0.035), ('license', 0.035), ('probabilities', 0.035), ('er', 0.034), ('swiss', 0.033), ('recognized', 0.033), ('declare', 0.033), ('iris', 0.033), ('faced', 0.033), ('manual', 0.033), ('os', 0.033), ('rubin', 0.033), ('pseudocode', 0.033), ('reliability', 0.033), ('separating', 0.032), ('user', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="45-tfidf-1" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: JNCC2 implements the naive credal classiﬁer 2 (NCC2). This is an extension of naive Bayes to imprecise probabilities that aims at delivering robust classiﬁcations also when dealing with small or incomplete data sets. Robustness is achieved by delivering set-valued classiﬁcations (that is, returning multiple classes) on the instances for which (i) the learning set is not informative enough to smooth the effect of choice of the prior density or (ii) the uncertainty arising from missing data prevents the reliable indication of a single class. JNCC2 is released under the GNU GPL license. Keywords: imprecise probabilities, missing data, naive Bayes, naive credal classiﬁer 2, Java</p><p>2 0.28927562 <a title="45-tfidf-2" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: In this paper, the naive credal classiﬁer, which is a set-valued counterpart of naive Bayes, is extended to a general and ﬂexible treatment of incomplete data, yielding a new classiﬁer called naive credal classiﬁer 2 (NCC2). The new classiﬁer delivers classiﬁcations that are reliable even in the presence of small sample sizes and missing values. Extensive empirical evaluations show that, by issuing set-valued classiﬁcations, NCC2 is able to isolate and properly deal with instances that are hard to classify (on which naive Bayes accuracy drops considerably), and to perform as well as naive Bayes on the other instances. The experiments point to a general problem: they show that with missing values, empirical evaluations may not reliably estimate the accuracy of a traditional classiﬁer, such as naive Bayes. This phenomenon adds even more value to the robust approach to classiﬁcation implemented by NCC2. Keywords: naive Bayes, naive credal classiﬁer, imprecise probabilities, missing values, conservative inference rule, missing at random</p><p>3 0.1419796 <a title="45-tfidf-3" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>Author: Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel, Daphne Koller</p><p>Abstract: We consider the problem of learning classiﬁers in structured domains, where some objects have a subset of features that are inherently absent due to complex relationships between the features. Unlike the case where a feature exists but its value is not observed, here we focus on the case where a feature may not even exist (structurally absent) for some of the samples. The common approach for handling missing features in discriminative models is to ﬁrst complete their unknown values, and then use a standard classiﬁcation procedure over the completed data. This paper focuses on features that are known to be non-existing, rather than have an unknown value. We show how incomplete data can be classiﬁed directly without any completion of the missing features using a max-margin learning framework. We formulate an objective function, based on the geometric interpretation of the margin, that aims to maximize the margin of each sample in its own relevant subspace. In this formulation, the linearly separable case can be transformed into a binary search over a series of second order cone programs (SOCP), a convex problem that can be solved efﬁciently. We also describe two approaches for optimizing the general case: an approximation that can be solved as a standard quadratic program (QP) and an iterative approach for solving the exact problem. By avoiding the pre-processing phase in which the data is completed, both of these approaches could offer considerable computational savings. More importantly, we show that the elegant handling of missing values by our approach allows it to both outperform other methods when the missing values have non-trivial structure, and be competitive with other methods when the values are missing at random. We demonstrate our results on several standard benchmarks and two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images. Keywords: max margin, missing features, network reconstruction, metabolic pa</p><p>4 0.061896104 <a title="45-tfidf-4" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>Author: Thomas G. Dietterich, Guohua Hao, Adam Ashenfelter</p><p>Abstract: Conditional random ﬁelds (CRFs) provide a ﬂexible and powerful model for sequence labeling problems. However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features and feature combinations. This paper describes a new algorithm for training CRFs via gradient tree boosting. In tree boosting, the CRF potential functions are represented as weighted sums of regression trees, which provide compact representations of feature interactions. So the algorithm does not explicitly consider the potentially large parameter space. As a result, gradient tree boosting scales linearly in the order of the Markov model and in the order of the feature interactions, rather than exponentially as in previous algorithms based on iterative scaling and gradient descent. Gradient tree boosting also makes it possible to use instance weighting (as in C4.5) and surrogate splitting (as in CART) to handle missing values. Experimental studies of the effectiveness of these two methods (as well as standard imputation and indicator feature methods) show that instance weighting is the best method in most cases when feature values are missing at random. Keywords: sequential supervised learning, conditional random ﬁelds, functional gradient, gradient tree boosting, missing values</p><p>5 0.049399406 <a title="45-tfidf-5" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>6 0.033801157 <a title="45-tfidf-6" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>7 0.031721957 <a title="45-tfidf-7" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>8 0.030043708 <a title="45-tfidf-8" href="./jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>9 0.029627955 <a title="45-tfidf-9" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>10 0.026520913 <a title="45-tfidf-10" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>11 0.02523965 <a title="45-tfidf-11" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>12 0.024547359 <a title="45-tfidf-12" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>13 0.024260592 <a title="45-tfidf-13" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>14 0.022692688 <a title="45-tfidf-14" href="./jmlr-2008-Generalization_from_Observed_to_Unobserved_Features_by_Clustering.html">38 jmlr-2008-Generalization from Observed to Unobserved Features by Clustering</a></p>
<p>15 0.022418462 <a title="45-tfidf-15" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>16 0.022109147 <a title="45-tfidf-16" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>17 0.021585297 <a title="45-tfidf-17" href="./jmlr-2008-Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">33 jmlr-2008-Evidence Contrary to the Statistical View of Boosting</a></p>
<p>18 0.021463227 <a title="45-tfidf-18" href="./jmlr-2008-HPB%3A_A_Model_for_Handling_BN_Nodes_with_High_Cardinality_Parents.html">42 jmlr-2008-HPB: A Model for Handling BN Nodes with High Cardinality Parents</a></p>
<p>19 0.021037318 <a title="45-tfidf-19" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>20 0.020445643 <a title="45-tfidf-20" href="./jmlr-2008-On_the_Equivalence_of_Linear_Dimensionality-Reducing_Transformations.html">71 jmlr-2008-On the Equivalence of Linear Dimensionality-Reducing Transformations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.127), (1, -0.028), (2, 0.077), (3, 0.048), (4, -0.301), (5, 0.245), (6, 0.296), (7, 0.037), (8, -0.13), (9, 0.177), (10, 0.259), (11, -0.33), (12, 0.219), (13, 0.023), (14, 0.134), (15, 0.106), (16, 0.115), (17, -0.098), (18, -0.031), (19, -0.081), (20, -0.06), (21, -0.039), (22, -0.006), (23, 0.003), (24, -0.024), (25, 0.099), (26, 0.03), (27, 0.037), (28, -0.037), (29, -0.005), (30, -0.013), (31, 0.047), (32, -0.018), (33, -0.023), (34, 0.002), (35, 0.01), (36, -0.01), (37, 0.02), (38, 0.01), (39, 0.021), (40, 0.023), (41, 0.001), (42, -0.026), (43, 0.022), (44, 0.056), (45, -0.033), (46, -0.017), (47, 0.032), (48, 0.016), (49, -0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95271814 <a title="45-lsi-1" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: JNCC2 implements the naive credal classiﬁer 2 (NCC2). This is an extension of naive Bayes to imprecise probabilities that aims at delivering robust classiﬁcations also when dealing with small or incomplete data sets. Robustness is achieved by delivering set-valued classiﬁcations (that is, returning multiple classes) on the instances for which (i) the learning set is not informative enough to smooth the effect of choice of the prior density or (ii) the uncertainty arising from missing data prevents the reliable indication of a single class. JNCC2 is released under the GNU GPL license. Keywords: imprecise probabilities, missing data, naive Bayes, naive credal classiﬁer 2, Java</p><p>2 0.95220286 <a title="45-lsi-2" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: In this paper, the naive credal classiﬁer, which is a set-valued counterpart of naive Bayes, is extended to a general and ﬂexible treatment of incomplete data, yielding a new classiﬁer called naive credal classiﬁer 2 (NCC2). The new classiﬁer delivers classiﬁcations that are reliable even in the presence of small sample sizes and missing values. Extensive empirical evaluations show that, by issuing set-valued classiﬁcations, NCC2 is able to isolate and properly deal with instances that are hard to classify (on which naive Bayes accuracy drops considerably), and to perform as well as naive Bayes on the other instances. The experiments point to a general problem: they show that with missing values, empirical evaluations may not reliably estimate the accuracy of a traditional classiﬁer, such as naive Bayes. This phenomenon adds even more value to the robust approach to classiﬁcation implemented by NCC2. Keywords: naive Bayes, naive credal classiﬁer, imprecise probabilities, missing values, conservative inference rule, missing at random</p><p>3 0.42327222 <a title="45-lsi-3" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>Author: Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel, Daphne Koller</p><p>Abstract: We consider the problem of learning classiﬁers in structured domains, where some objects have a subset of features that are inherently absent due to complex relationships between the features. Unlike the case where a feature exists but its value is not observed, here we focus on the case where a feature may not even exist (structurally absent) for some of the samples. The common approach for handling missing features in discriminative models is to ﬁrst complete their unknown values, and then use a standard classiﬁcation procedure over the completed data. This paper focuses on features that are known to be non-existing, rather than have an unknown value. We show how incomplete data can be classiﬁed directly without any completion of the missing features using a max-margin learning framework. We formulate an objective function, based on the geometric interpretation of the margin, that aims to maximize the margin of each sample in its own relevant subspace. In this formulation, the linearly separable case can be transformed into a binary search over a series of second order cone programs (SOCP), a convex problem that can be solved efﬁciently. We also describe two approaches for optimizing the general case: an approximation that can be solved as a standard quadratic program (QP) and an iterative approach for solving the exact problem. By avoiding the pre-processing phase in which the data is completed, both of these approaches could offer considerable computational savings. More importantly, we show that the elegant handling of missing values by our approach allows it to both outperform other methods when the missing values have non-trivial structure, and be competitive with other methods when the values are missing at random. We demonstrate our results on several standard benchmarks and two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images. Keywords: max margin, missing features, network reconstruction, metabolic pa</p><p>4 0.23668446 <a title="45-lsi-4" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>Author: Thomas G. Dietterich, Guohua Hao, Adam Ashenfelter</p><p>Abstract: Conditional random ﬁelds (CRFs) provide a ﬂexible and powerful model for sequence labeling problems. However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features and feature combinations. This paper describes a new algorithm for training CRFs via gradient tree boosting. In tree boosting, the CRF potential functions are represented as weighted sums of regression trees, which provide compact representations of feature interactions. So the algorithm does not explicitly consider the potentially large parameter space. As a result, gradient tree boosting scales linearly in the order of the Markov model and in the order of the feature interactions, rather than exponentially as in previous algorithms based on iterative scaling and gradient descent. Gradient tree boosting also makes it possible to use instance weighting (as in C4.5) and surrogate splitting (as in CART) to handle missing values. Experimental studies of the effectiveness of these two methods (as well as standard imputation and indicator feature methods) show that instance weighting is the best method in most cases when feature values are missing at random. Keywords: sequential supervised learning, conditional random ﬁelds, functional gradient, gradient tree boosting, missing values</p><p>5 0.17743087 <a title="45-lsi-5" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>Author: Elena Marchiori</p><p>Abstract: In supervised learning, a training set consisting of labeled instances is used by a learning algorithm for generating a model (classiﬁer) that is subsequently employed for deciding the class label of new instances (for generalization). Characteristics of the training set, such as presence of noisy instances and size, inﬂuence the learning algorithm and affect generalization performance. This paper introduces a new network-based representation of a training set, called hit miss network (HMN), which provides a compact description of the nearest neighbor relation over pairs of instances from each pair of classes. We show that structural properties of HMN’s correspond to properties of training points related to the one nearest neighbor (1-NN) decision rule, such as being border or central point. This motivates us to use HMN’s for improving the performance of a 1-NN classiﬁer by removing instances from the training set (instance selection). We introduce three new HMN-based algorithms for instance selection. HMN-C, which removes instances without affecting accuracy of 1-NN on the original training set, HMN-E, based on a more aggressive storage reduction, and HMN-EI, which applies iteratively HMN-E. Their performance is assessed on 22 data sets with different characteristics, such as input dimension, cardinality, class balance, number of classes, noise content, and presence of redundant variables. Results of experiments on these data sets show that accuracy of 1-NN classiﬁer increases signiﬁcantly when HMN-EI is applied. Comparison with state-of-the-art editing algorithms for instance selection on these data sets indicates best generalization performance of HMN-EI and no signiﬁcant difference in storage requirements. In general, these results indicate that HMN’s provide a powerful graph-based representation of a training set, which can be successfully applied for performing noise and redundance reduction in instance-based learning. Keywords: graph-based training set repre</p><p>6 0.16105849 <a title="45-lsi-6" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>7 0.14575855 <a title="45-lsi-7" href="./jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>8 0.13624568 <a title="45-lsi-8" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>9 0.13433883 <a title="45-lsi-9" href="./jmlr-2008-HPB%3A_A_Model_for_Handling_BN_Nodes_with_High_Cardinality_Parents.html">42 jmlr-2008-HPB: A Model for Handling BN Nodes with High Cardinality Parents</a></p>
<p>10 0.13293345 <a title="45-lsi-10" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>11 0.12935171 <a title="45-lsi-11" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>12 0.12166019 <a title="45-lsi-12" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>13 0.1176993 <a title="45-lsi-13" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>14 0.11318883 <a title="45-lsi-14" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>15 0.10795701 <a title="45-lsi-15" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>16 0.10599533 <a title="45-lsi-16" href="./jmlr-2008-Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">33 jmlr-2008-Evidence Contrary to the Statistical View of Boosting</a></p>
<p>17 0.10471086 <a title="45-lsi-17" href="./jmlr-2008-Stationary_Features_and_Cat_Detection.html">87 jmlr-2008-Stationary Features and Cat Detection</a></p>
<p>18 0.1045607 <a title="45-lsi-18" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>19 0.10427278 <a title="45-lsi-19" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>20 0.10141224 <a title="45-lsi-20" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.017), (16, 0.606), (31, 0.022), (40, 0.028), (54, 0.032), (58, 0.026), (66, 0.029), (76, 0.027), (88, 0.043), (92, 0.027), (94, 0.036), (99, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88309789 <a title="45-lda-1" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: JNCC2 implements the naive credal classiﬁer 2 (NCC2). This is an extension of naive Bayes to imprecise probabilities that aims at delivering robust classiﬁcations also when dealing with small or incomplete data sets. Robustness is achieved by delivering set-valued classiﬁcations (that is, returning multiple classes) on the instances for which (i) the learning set is not informative enough to smooth the effect of choice of the prior density or (ii) the uncertainty arising from missing data prevents the reliable indication of a single class. JNCC2 is released under the GNU GPL license. Keywords: imprecise probabilities, missing data, naive Bayes, naive credal classiﬁer 2, Java</p><p>2 0.43547907 <a title="45-lda-2" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: In this paper, the naive credal classiﬁer, which is a set-valued counterpart of naive Bayes, is extended to a general and ﬂexible treatment of incomplete data, yielding a new classiﬁer called naive credal classiﬁer 2 (NCC2). The new classiﬁer delivers classiﬁcations that are reliable even in the presence of small sample sizes and missing values. Extensive empirical evaluations show that, by issuing set-valued classiﬁcations, NCC2 is able to isolate and properly deal with instances that are hard to classify (on which naive Bayes accuracy drops considerably), and to perform as well as naive Bayes on the other instances. The experiments point to a general problem: they show that with missing values, empirical evaluations may not reliably estimate the accuracy of a traditional classiﬁer, such as naive Bayes. This phenomenon adds even more value to the robust approach to classiﬁcation implemented by NCC2. Keywords: naive Bayes, naive credal classiﬁer, imprecise probabilities, missing values, conservative inference rule, missing at random</p><p>3 0.14065777 <a title="45-lda-3" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>4 0.13903239 <a title="45-lda-4" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>Author: Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, Peter L. Bartlett</p><p>Abstract: Log-linear and maximum-margin models are two commonly-used methods in supervised machine learning, and are frequently used in structured prediction problems. Efﬁcient learning of parameters in these models is therefore an important problem, and becomes a key factor when learning from very large data sets. This paper describes exponentiated gradient (EG) algorithms for training such models, where EG updates are applied to the convex dual of either the log-linear or maxmargin objective function; the dual in both the log-linear and max-margin cases corresponds to minimizing a convex function with simplex constraints. We study both batch and online variants of the algorithm, and provide rates of convergence for both cases. In the max-margin case, O( 1 ) EG ε updates are required to reach a given accuracy ε in the dual; in contrast, for log-linear models only O(log( 1 )) updates are required. For both the max-margin and log-linear cases, our bounds suggest ε that the online EG algorithm requires a factor of n less computation to reach a desired accuracy than the batch EG algorithm, where n is the number of training examples. Our experiments conﬁrm that the online algorithms are much faster than the batch algorithms in practice. We describe how the EG updates factor in a convenient way for structured prediction problems, allowing the algorithms to be efﬁciently applied to problems such as sequence learning or natural language parsing. We perform extensive evaluation of the algorithms, comparing them to L-BFGS and stochastic gradient descent for log-linear models, and to SVM-Struct for max-margin models. The algorithms are applied to a multi-class problem as well as to a more complex large-scale parsing task. In all these settings, the EG algorithms presented here outperform the other methods. Keywords: exponentiated gradient, log-linear models, maximum-margin models, structured prediction, conditional random ﬁelds ∗. These authors contributed equally. c 2008 Michael Col</p><p>5 0.13716823 <a title="45-lda-5" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>Author: Elisa Ricci, Tijl De Bie, Nello Cristianini</p><p>Abstract: Most approaches to structured output prediction rely on a hypothesis space of prediction functions that compute their output by maximizing a linear scoring function. In this paper we present two novel learning algorithms for this hypothesis class, and a statistical analysis of their performance. The methods rely on efﬁciently computing the ﬁrst two moments of the scoring function over the output space, and using them to create convex objective functions for training. We report extensive experimental results for sequence alignment, named entity recognition, and RNA secondary structure prediction. Keywords: structured output prediction, discriminative learning, Z-score, discriminant analysis, PAC bound</p><p>6 0.13678549 <a title="45-lda-6" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>7 0.13647145 <a title="45-lda-7" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>8 0.13620278 <a title="45-lda-8" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>9 0.13619611 <a title="45-lda-9" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>10 0.13594763 <a title="45-lda-10" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>11 0.135316 <a title="45-lda-11" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>12 0.1348867 <a title="45-lda-12" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>13 0.13452047 <a title="45-lda-13" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>14 0.1340632 <a title="45-lda-14" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>15 0.13312633 <a title="45-lda-15" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>16 0.13258474 <a title="45-lda-16" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>17 0.13258156 <a title="45-lda-17" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>18 0.13039601 <a title="45-lda-18" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>19 0.12957244 <a title="45-lda-19" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>20 0.12955253 <a title="45-lda-20" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
