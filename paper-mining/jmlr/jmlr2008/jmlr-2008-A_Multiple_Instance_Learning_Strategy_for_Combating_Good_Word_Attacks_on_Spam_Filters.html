<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-4" href="#">jmlr2008-4</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</h1>
<br/><p>Source: <a title="jmlr-2008-4-pdf" href="http://jmlr.org/papers/volume9/jorgensen08a/jorgensen08a.pdf">pdf</a></p><p>Author: Zach Jorgensen, Yan Zhou, Meador Inge</p><p>Abstract: Statistical spam ﬁlters are known to be vulnerable to adversarial attacks. One of the more common adversarial attacks, known as the good word attack, thwarts spam ﬁlters by appending to spam messages sets of “good” words, which are words that are common in legitimate email but rare in spam. We present a counterattack strategy that attempts to differentiate spam from legitimate email in the input space by transforming each email into a bag of multiple segments, and subsequently applying multiple instance logistic regression on the bags. We treat each segment in the bag as an instance. An email is classiﬁed as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. We show that a classiﬁer using our multiple instance counterattack strategy is more robust to good word attacks than its single instance counterpart and other single instance learners commonly used in the spam ﬁltering domain. Keywords: spam ﬁltering, multiple instance learning, good word attack, adversarial learning</p><p>Reference: <a title="jmlr-2008-4-reference" href="../jmlr2008_reference/jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 One of the more common adversarial attacks, known as the good word attack, thwarts spam ﬁlters by appending to spam messages sets of “good” words, which are words that are common in legitimate email but rare in spam. [sent-8, score-2.205]
</p><p>2 We present a counterattack strategy that attempts to differentiate spam from legitimate email in the input space by transforming each email into a bag of multiple segments, and subsequently applying multiple instance logistic regression on the bags. [sent-9, score-1.337]
</p><p>3 An email is classiﬁed as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. [sent-11, score-1.099]
</p><p>4 We show that a classiﬁer using our multiple instance counterattack strategy is more robust to good word attacks than its single instance counterpart and other single instance learners commonly used in the spam ﬁltering domain. [sent-12, score-1.336]
</p><p>5 Keywords: spam ﬁltering, multiple instance learning, good word attack, adversarial learning  1. [sent-13, score-1.03]
</p><p>6 Introduction It has been nearly thirty years since the ﬁrst email spam appeared on the Arpanet. [sent-14, score-0.775]
</p><p>7 Today, to most end users, spam does not seem to be a serious threat due to the apparent effectiveness of current spam ﬁlters. [sent-15, score-1.312]
</p><p>8 In this paper, we target one of the adversarial techniques spammers often use to circumvent existing spam ﬁlters. [sent-22, score-0.781]
</p><p>9 Adversarial attacks on spam ﬁlters have become an increasing challenge to the anti-spam community. [sent-23, score-0.868]
</p><p>10 Spam messages injected with such words are more likely to appear legitimate and bypass spam ﬁlters. [sent-27, score-1.345]
</p><p>11 Our spam ﬁltering strategy adopts the classical MI assumption, which states that a bag is positive if at least one of its instances is positive, and negative if all instances are negative. [sent-51, score-0.889]
</p><p>12 Thus, an email is classiﬁed as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. [sent-53, score-1.099]
</p><p>13 The idea is that by splitting an email into multiple instances, a multiple instance learner will be able to recognize the spam part of the message even if the message has been injected with good words. [sent-54, score-1.408]
</p><p>14 Next, we formalize the spam ﬁltering problem as a multiple instance learning problem and explain our proposed counterattack strategy in more detail. [sent-58, score-0.815]
</p><p>15 In their adversarial classiﬁer reverse engineer (ACRE) framework, the adversary aims to identify difﬁcult spam instances (the ones that are hard to detect by the classiﬁer) through membership queries. [sent-71, score-0.826]
</p><p>16 Lowd and Meek (2005b) present and evaluate several variations of this type of attack on spam ﬁlters. [sent-86, score-0.843]
</p><p>17 Active good word attacks use feedback obtained by sending test messages to a spam ﬁlter in order to determine which words are “good”. [sent-88, score-1.477]
</p><p>18 The active attacks were found to be more effective than the passive attacks; however, active attacks are generally more difﬁcult to perform than passive attacks because they require user-level access to the spam ﬁlter, which is not always possible. [sent-89, score-1.292]
</p><p>19 Passive good word attacks, on the other hand, do not involve any feedback from the spam ﬁlter, but rather, guesses are made as to which words are considered good. [sent-90, score-1.004]
</p><p>20 Finally, frequency ratio attacks involve the selection of words that occur very often in legitimate messages but not in spam messages. [sent-96, score-1.345]
</p><p>21 The authors’ tests showed that this technique was quite effective, resulting in the average spam message being passed off as legitimate by adding as few as 150 good words to it. [sent-97, score-1.058]
</p><p>22 Preliminary results were also presented that suggested that frequent retraining on attacked messages may help reduce the effect of good word attacks on spam ﬁlters. [sent-98, score-1.391]
</p><p>23 (2005) also examined the effectiveness of good word attacks on statistical spam ﬁlters. [sent-100, score-1.077]
</p><p>24 They present a “large-scale evaluation” of the effectiveness of the attack on four spam ﬁlters: na¨ve Bayes, support vector machine (SVM), LogitBoost, and SpamProbe. [sent-101, score-0.843]
</p><p>25 Their experiments were ı performed on a large email corpus consisting of around a million spam and ham messages, which they formed by combining several public and private corpora. [sent-102, score-0.927]
</p><p>26 In their experiments, spam emails were camouﬂaged by combining them with portions of legitimate messages. [sent-106, score-0.903]
</p><p>27 They experimented with camouﬂaged messages containing twice as much spam content as legitimate content, and vice versa. [sent-107, score-0.994]
</p><p>28 In the case of spam ﬁltering, an adversary can modify spam emails by injecting them with good words. [sent-143, score-1.61]
</p><p>29 So, ∆Xi represents a set of good words added to a spam message by the spammer. [sent-144, score-0.995]
</p><p>30 the ﬁlter is trained on normal emails, that is, emails that have not been injected with good words, and tested on emails which have been injected with good words; 2. [sent-146, score-0.848]
</p><p>31 Multiple Instance Bag Creation We now formulate the spam ﬁltering problem as a multiple instance binary classiﬁcation problem in the context of adversarial attacks. [sent-156, score-0.821]
</p><p>32 This splitting approach is reasonable in practice because spammers usually append a section of good words to either the beginning or the end of an email to ensure the legibility of the spam message. [sent-171, score-1.084]
</p><p>33 These weights are calculated using word frequencies obtained from the spam and legitimate messages in the training corpus. [sent-177, score-1.172]
</p><p>34 More speciﬁcally, the weight of a term W is given as follows: weight(W ) =  p(W | Ds ) , p(W | Ds ) + p(W | Dh )  where Ds and Dh are the spam and ham emails in the training set respectively. [sent-178, score-0.937]
</p><p>35 3 Split-P The third splitting method, split-projection (split-P), transforms each message into a bag of two instances by projecting the message vector onto the spam and ham prototype vectors. [sent-186, score-1.18]
</p><p>36 The prototype vectors are computed using all the spam and ham messages in the training set. [sent-187, score-1.032]
</p><p>37 If we view the spam and ham messages in the training set as two clusters, then the prototypes are essentially the centroid of the two clusters. [sent-188, score-1.012]
</p><p>38 More speciﬁcally, let Cs be the set of emails that are spam and C be the set of emails that are legitimate. [sent-189, score-0.944]
</p><p>39 The rationale of this splitting approach rests on the assumption that a message is close to the spam prototype in terms of cosine similarity if it is indeed spam, and a ham message is close to the ham prototype. [sent-193, score-1.093]
</p><p>40 In this method, however, the ham and spam prototypes are calculated by averaging the corresponding attribute values of all of the ham and spam emails, respectively: |Cs |  Ps = 1/|Cs | · ∑ Csi , i=1 |C |  P = 1/|C | · ∑ C i . [sent-196, score-1.48]
</p><p>41 i=1  ith  where Cs is a set of spam and Csi is the spam message in Cs ; C is a set of ham, and C i is the ith ham message in C . [sent-197, score-1.618]
</p><p>42 Now that we have devised several techniques for creating multiple instance bags from email messages, we can transform the standard supervised learning problem of spam ﬁltering into a multiple instance learning problem under the standard MI assumption. [sent-199, score-1.005]
</p><p>43 In this paper, we adopt the multiple instance logistic regression (MILR) model to train a spam ﬁlter that is more robust to adversarial good word attacks than traditional spam ﬁlters based on single instance models. [sent-200, score-1.969]
</p><p>44 Here Yi is a dichotomous outcome of the ith bag (for example, spam or legitimate). [sent-212, score-0.808]
</p><p>45 Good word attacks were simulated by generating a list of good words from the corpus and injecting them into spam messages in the training and/or test data sets. [sent-224, score-1.651]
</p><p>46 Additionally, we tested a relatively new compression-based ı spam ﬁlter (Bratko and Filipiˇ , 2005) against the good word attack. [sent-226, score-0.865]
</p><p>47 1 Experimental Data Our experimental data consists of 36,674 spam and legitimate email messages from the 2006 TREC spam corpus. [sent-230, score-1.769]
</p><p>48 We did not take any measures to counter obfuscated words in the spam messages, as that is out of the scope of this paper. [sent-235, score-0.795]
</p><p>49 The percentage of spam messages in each subset varies as in the operational setting (see Figure 1). [sent-246, score-0.891]
</p><p>50 When generating the global good word list, we ranked every unique word in the corpus according to the ratio of its frequency in the legitimate messages over its frequency in the spam messages. [sent-279, score-1.412]
</p><p>51 In the ﬁrst experiment, we train all of the classiﬁers on normal email (that is, email that has not been injected with good words) and then test them on email that has been injected with good words. [sent-297, score-0.943]
</p><p>52 Half of the spam messages (selected at random) in each of the remaining 14 variations of each test set were injected with some quantity of random good words from our global good word list, beginning with 10 words. [sent-311, score-1.545]
</p><p>53 With each successive version of the test set, the quantity of good words injected into half of the spam messages was increased: ﬁrst in increments of 10 words, up to 50, and then in increments of 50 words up to 500. [sent-312, score-1.52]
</p><p>54 The injected words were randomly selected, without replacement, from our global good word list on a message by message basis. [sent-313, score-0.771]
</p><p>55 We chose to inject good words into only half of the messages in each test set because, in practice, spam messages injected with good words account for only a subset of the spam emails encountered by a given ﬁlter. [sent-314, score-2.623]
</p><p>56 Figure 3 shows how the average recall of each classiﬁer is affected as the good word attack increases in strength (that is, the quantity of good words injected into the spam emails increases). [sent-318, score-1.656]
</p><p>57 The AUC of a spam classiﬁer can be interpreted as the probability that the classiﬁer will rate a randomly chosen spam email as more spammy than a randomly chosen legitimate email. [sent-325, score-1.62]
</p><p>58 From the results we can see that, with the exception of MILRT, the good word attack signiﬁcantly affected the ability of each classiﬁer to identify spam emails. [sent-328, score-1.052]
</p><p>59 927) in average recall after 500 good words had been added to the spam messages. [sent-332, score-0.944]
</p><p>60 9, as the quantity of good words injected into half of the spam messages in the test set increases; no good words were injected into the training set. [sent-351, score-1.837]
</p><p>61 When good words are injected into a spam message they end up in the legitimate instance of the bag and have no effect on the spammy instance; thus the bag still contains a spammy instance and is classiﬁed correctly as spam. [sent-530, score-1.774]
</p><p>62 For each corresponding test set, the entire contents of the local good word list were added to all of the spam messages in the set. [sent-558, score-1.193]
</p><p>63 000 MILRH  MILRP MILRS MILRT  LR  MNB  MILRH  SVM  MILRP MILRS MILRT  LR  MNB  SVM  Figure 9: The average precision (left) and recall (right) of each classiﬁer when injecting the entire local good word list into all of the spam messages in the test set. [sent-590, score-1.279]
</p><p>64 2 Experiment 2: Training on Attacked Spam Messages In the second experiment, our goal was to observe the effect that training on messages injected with good words has on the susceptibility of the classiﬁers to attacks on the test set. [sent-592, score-0.929]
</p><p>65 We injected 10 good words into half of the spam messages (selected at random) in the ﬁrst version of the training set and then increased the number of injected good words by 10 for each subsequent version, up to 50 good words for the ﬁfth version. [sent-595, score-2.018]
</p><p>66 Figure 15 shows two graphs containing the ROC curves of all the classiﬁers when 0 good words and 500 good words are added to the test set respectively and 10 good words are added to the training set. [sent-602, score-0.787]
</p><p>67 Injecting just 10 good words into half of the spam messages in the training set appeared to lessen the effect of the good word attack for almost all of the classiﬁers. [sent-604, score-1.576]
</p><p>68 In particular, the average recall of LR with 500 good words injected into half of the spam messages in the test set was 32. [sent-605, score-1.422]
</p><p>69 1% higher after 10 good words had been injected into the training set compared to when no good words had been injected into the training set (comparing Figures 3 and 10). [sent-606, score-0.912]
</p><p>70 After 30 good words had been injected into the training set, the presence of good words in the test messages actually began to increase the likelihood that such messages would be correctly classiﬁed as spam. [sent-616, score-1.159]
</p><p>71 9, as the number of good words injected into half of the spam messages in the test set increases; 10 good words were also injected into half of the spam messages in the training set. [sent-626, score-2.773]
</p><p>72 9, as the number of good words injected into half of the spam messages in the test set increases; 20 good words were also injected into half of the spam messages in the training set. [sent-636, score-2.773]
</p><p>73 However, it is important to realize that this would only work in cases where the attacked messages being classiﬁed contained the same good words as the attacked messages that the spam ﬁlter was trained on. [sent-639, score-1.491]
</p><p>74 9, as the number of good words injected into half of the spam messages in the test set increases; 30 good words were also injected into half of the spam messages in the training set. [sent-650, score-2.773]
</p><p>75 The entire contents of the local good word list were added to all of the spam messages in the corresponding training and test sets. [sent-653, score-1.23]
</p><p>76 3 Attacking the Compression-Based Filter Relatively new to the spam ﬁltering scene is the idea of using statistical data compression algorithms for spam ﬁltering. [sent-658, score-1.329]
</p><p>77 The general idea is to construct two compression models, one from a collection of spam emails and one from a collection of legitimate emails, and 1135  J ORGENSEN , Z HOU AND I NGE  Recall with 40 Words Added to the Training Set MILRH  LR  MILRT  MNB  MILRP  SVM  MILRS  1. [sent-660, score-0.92]
</p><p>78 9, as the number of good words injected into half of the spam messages in the test set increases; 40 good words were also injected into half of the the spam messages in the training set. [sent-669, score-2.773]
</p><p>79 9, as the number of good words injected into half of the spam messages in the test set increases; 50 good words were also injected into half of the the spam messages in the training set. [sent-679, score-2.773]
</p><p>80 8  1  False Positive Rate  Figure 15: ROC curves for all classiﬁers when 0 good words (top) and 500 good words (bottom) have been injected into the test set and 10 good words have been injected into the training set. [sent-696, score-1.131]
</p><p>81 8  1  False Positive Rate  Figure 16: ROC curves for all classiﬁers when 0 good words (top) and 500 good words (bottom) have been injected into the test set and 50 good words have been injected into the training set. [sent-720, score-1.131]
</p><p>82 a word-level spam ﬁlter to misclassify emails containing such words unless extra care and effort are expended to detect and deal with these obfuscations. [sent-721, score-0.939]
</p><p>83 642  MILRH MILRP MILRS MILRT  LR  MNB  SVM  Figure 17: The average precision (left) and recall (right) of each classiﬁer when injecting the entire contents of each local good word list into all of the spam messages in the corresponding training and test sets. [sent-765, score-1.316]
</p><p>84 However, since our simulated attack appends good words to the bottom of the spam messages, it is possible that truncating the messages could result in some or all of the added good words being removed from spam messages that are longer than 2500 bytes. [sent-775, score-2.449]
</p><p>85 500  Number of Good Words Added to Spam in Test Set  Figure 18: Effect of the good word attack on the PPMD algorithm as the number of good words added into half of the spam messages in the test set increases; no good words were added to the spam in the training set. [sent-796, score-2.545]
</p><p>86 450  Number of Good Words Added to Spam in Test Set  Figure 19: Effect of the good word attack on the PPMD algorithm as the number of good words added into half of the spam messages in the test set increases; no good words were added to the spam in the training set. [sent-805, score-2.545]
</p><p>87 We recognized two possible ways for a spammer to attack a spam ﬁlter equipped with splitting methods like split-H. [sent-809, score-0.929]
</p><p>88 One way to attack it is to create a visual pattern with good words so that the original spam message is still legible after the attack, but the spam is fragmented in such a way that “spammy” words are well separated. [sent-811, score-1.937]
</p><p>89 450  Number of Good Words Added to Spam in Test Set  Figure 20: Effect of the good word attack on the PPMD algorithm when half of the spam messages in the test set were altered. [sent-818, score-1.358]
</p><p>90 940 0  10 20 30 40 50 100 150 200 250 300 350 400 450 500 Number of Good Words Added to Spam in Test Set  Figure 21: Effect of the good word attack on the PPMD algorithm when half of the spam in the training/test sets were altered; 10 words were added to the training spam. [sent-827, score-1.313]
</p><p>91 940 0  10 20 30 40 50 100 150 200 250 300 350 400 450 500 Number of Good Words Added to Spam in Test Set  Figure 22: Effect of the good word attack on the PPMD algorithm when half of the spam in the training/test sets were altered; 50 words were added to the training spam. [sent-835, score-1.313]
</p><p>92 good words good words good words  Table 3: Attacking split-H by fragmenting spam with injected good words. [sent-858, score-1.557]
</p><p>93 50% of the spam messages in each test set were selected at random to be attacked by inserting 3 random good words before and after every 6 words in the message. [sent-860, score-1.342]
</p><p>94 A second way to defeat the split-H method is to append a very large block of good words to the spam messages, so that after the split, good words would still outweigh spammy words in both instances in the bag. [sent-873, score-1.316]
</p><p>95 Observe in Figure 3 that the average recall of MILRH did not really begin to drop signiﬁcantly until after 50 good words had been injected into the spam messages in the test set. [sent-875, score-1.377]
</p><p>96 As even more good words were injected into the spam messages, the average recall continued to drop as the longer messages began to accumulate enough good words to outweigh the spammy words in both instances. [sent-876, score-1.766]
</p><p>97 In practice, depending on the length of the spam message, coming up with a large enough block of good words might prove difﬁcult. [sent-877, score-0.863]
</p><p>98 Conclusions and Future Work A multiple instance learning counterattack strategy for combating adversarial good word attacks on statistical spam ﬁlters has been proposed. [sent-879, score-1.324]
</p><p>99 Additionally, we have conﬁrmed earlier reports that re-training on attacked as well as normal emails may strengthen a spam ﬁlter against good word attacks. [sent-895, score-1.088]
</p><p>100 Since it is an arms race between spammers and ﬁlter designers, we also plan to make our MI strategy adaptive as new spam techniques are devised, and on-line as the concept of spam drifts over time. [sent-901, score-1.373]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spam', 0.656), ('messages', 0.235), ('attacks', 0.212), ('injected', 0.212), ('attack', 0.187), ('milrh', 0.148), ('milrt', 0.148), ('emails', 0.144), ('word', 0.141), ('words', 0.139), ('bag', 0.133), ('milrp', 0.126), ('milrs', 0.126), ('email', 0.119), ('mnb', 0.105), ('legitimate', 0.103), ('message', 0.092), ('adversarial', 0.088), ('ham', 0.084), ('hou', 0.084), ('nge', 0.084), ('orgensen', 0.084), ('lr', 0.083), ('attacked', 0.079), ('ilters', 0.079), ('ombating', 0.079), ('ord', 0.079), ('pam', 0.079), ('bags', 0.076), ('lter', 0.074), ('ppmd', 0.074), ('spammy', 0.069), ('good', 0.068), ('corpus', 0.068), ('ood', 0.067), ('splitting', 0.065), ('counterattack', 0.058), ('milr', 0.058), ('instance', 0.05), ('thresh', 0.047), ('half', 0.045), ('adversary', 0.044), ('precision', 0.043), ('roc', 0.043), ('injecting', 0.042), ('lowd', 0.042), ('recall', 0.041), ('added', 0.04), ('instances', 0.038), ('lters', 0.038), ('filipi', 0.037), ('spammers', 0.037), ('training', 0.037), ('bratko', 0.036), ('false', 0.034), ('ltering', 0.033), ('aged', 0.032), ('camou', 0.032), ('ers', 0.029), ('bi', 0.028), ('multiple', 0.027), ('bytes', 0.027), ('list', 0.027), ('csi', 0.026), ('test', 0.026), ('pr', 0.026), ('truncating', 0.026), ('mi', 0.025), ('strategy', 0.024), ('altered', 0.024), ('classi', 0.024), ('svm', 0.023), ('curves', 0.023), ('ps', 0.022), ('attacking', 0.022), ('trec', 0.022), ('logistic', 0.021), ('chronologically', 0.021), ('nbm', 0.021), ('spammer', 0.021), ('prototype', 0.02), ('zhou', 0.019), ('ith', 0.019), ('neutral', 0.018), ('retained', 0.018), ('smo', 0.018), ('rate', 0.017), ('compression', 0.017), ('er', 0.016), ('preprocessed', 0.016), ('resilient', 0.016), ('webb', 0.016), ('weight', 0.016), ('carpinter', 0.016), ('conformations', 0.016), ('dalvi', 0.016), ('dd', 0.016), ('molecule', 0.016), ('rocchio', 0.016), ('threshs', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="4-tfidf-1" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>Author: Zach Jorgensen, Yan Zhou, Meador Inge</p><p>Abstract: Statistical spam ﬁlters are known to be vulnerable to adversarial attacks. One of the more common adversarial attacks, known as the good word attack, thwarts spam ﬁlters by appending to spam messages sets of “good” words, which are words that are common in legitimate email but rare in spam. We present a counterattack strategy that attempts to differentiate spam from legitimate email in the input space by transforming each email into a bag of multiple segments, and subsequently applying multiple instance logistic regression on the bags. We treat each segment in the bag as an instance. An email is classiﬁed as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. We show that a classiﬁer using our multiple instance counterattack strategy is more robust to good word attacks than its single instance counterpart and other single instance learners commonly used in the spam ﬁltering domain. Keywords: spam ﬁltering, multiple instance learning, good word attack, adversarial learning</p><p>2 0.077973478 <a title="4-tfidf-2" href="./jmlr-2008-Linear-Time_Computation_of_Similarity_Measures_for_Sequential_Data.html">55 jmlr-2008-Linear-Time Computation of Similarity Measures for Sequential Data</a></p>
<p>Author: Konrad Rieck, Pavel Laskov</p><p>Abstract: Efﬁcient and expressive comparison of sequences is an essential procedure for learning with sequential data. In this article we propose a generic framework for computation of similarity measures for sequences, covering various kernel, distance and non-metric similarity functions. The basis for comparison is embedding of sequences using a formal language, such as a set of natural words, k-grams or all contiguous subsequences. As realizations of the framework we provide linear-time algorithms of different complexity and capabilities using sorted arrays, tries and sufﬁx trees as underlying data structures. Experiments on data sets from bioinformatics, text processing and computer security illustrate the efﬁciency of the proposed algorithms—enabling peak performances of up to 106 pairwise comparisons per second. The utility of distances and non-metric similarity measures for sequences as alternatives to string kernels is demonstrated in applications of text categorization, network intrusion detection and transcription site recognition in DNA. Keywords: string kernels, string distances, learning with sequential data</p><p>3 0.074056409 <a title="4-tfidf-3" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>Author: Shann-Ching Chen, Geoffrey J. Gordon, Robert F. Murphy</p><p>Abstract: In structured classiﬁcation problems, there is a direct conﬂict between expressive models and efﬁcient inference: while graphical models such as Markov random ﬁelds or factor graphs can represent arbitrary dependences among instance labels, the cost of inference via belief propagation in these models grows rapidly as the graph structure becomes more complicated. One important source of complexity in belief propagation is the need to marginalize large factors to compute messages. This operation takes time exponential in the number of variables in the factor, and can limit the expressiveness of the models we can use. In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efﬁcient algorithms for computing messages from these potentials during belief propagation. We believe these new potentials provide a good balance between expressive power and efﬁcient inference in practical structured classiﬁcation problems. We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. We use these potentials to classify images of protein subcellular location patterns in groups of cells. Classifying subcellular location patterns can help us answer many important questions in computational biology, including questions about how various treatments affect the synthesis and behavior of proteins and networks of proteins within a cell. Our new representation and algorithm lead to substantial improvements in both inference speed and classiﬁcation accuracy. Keywords: factor graphs, approximate inference algorithms, structured classiﬁcation, protein subcellular location patterns, location proteomics</p><p>4 0.032847118 <a title="4-tfidf-4" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>Author: Glenn Shafer, Vladimir Vovk</p><p>Abstract: Conformal prediction uses past experience to determine precise levels of conÄ?Ĺš dence in new predictions. Given an error probability ĂŽÄž, together with a method that makes a prediction y of a label Ă&lsaquo;&dagger; y, it produces a set of labels, typically containing y, that also contains y with probability 1 Ă˘&circ;&rsquo; ĂŽÄž. Ă&lsaquo;&dagger; Conformal prediction can be applied to any method for producing y: a nearest-neighbor method, a Ă&lsaquo;&dagger; support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 Ă˘&circ;&rsquo; ĂŽÄž of the time, even though they are based on an accumulating data set rather than on independent data sets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005). Keywords: conÄ?Ĺš dence, on-line compression modeling, on-line learning, prediction regions</p><p>5 0.03045203 <a title="4-tfidf-5" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>Author: Eric Bax, Augusto Callejas</p><p>Abstract: This paper introduces a new PAC transductive error bound for classiﬁcation. The method uses information from the training examples and inputs of working examples to develop a set of likely assignments to outputs of the working examples. A likely assignment with maximum error determines the bound. The method is very effective for small data sets. Keywords: error bound, transduction, nearest neighbor, dynamic programming</p><p>6 0.029820126 <a title="4-tfidf-6" href="./jmlr-2008-Multi-Agent_Reinforcement_Learning_in_Common_Interest_and_Fixed_Sum_Stochastic_Games%3A_An_Experimental_Study.html">65 jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</a></p>
<p>7 0.02944825 <a title="4-tfidf-7" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>8 0.024075661 <a title="4-tfidf-8" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>9 0.023577217 <a title="4-tfidf-9" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>10 0.022917418 <a title="4-tfidf-10" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>11 0.021872321 <a title="4-tfidf-11" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>12 0.021848293 <a title="4-tfidf-12" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>13 0.020405645 <a title="4-tfidf-13" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>14 0.02032535 <a title="4-tfidf-14" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>15 0.020258659 <a title="4-tfidf-15" href="./jmlr-2008-Generalization_from_Observed_to_Unobserved_Features_by_Clustering.html">38 jmlr-2008-Generalization from Observed to Unobserved Features by Clustering</a></p>
<p>16 0.019861404 <a title="4-tfidf-16" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>17 0.019435741 <a title="4-tfidf-17" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>18 0.018873991 <a title="4-tfidf-18" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>19 0.018031655 <a title="4-tfidf-19" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>20 0.017150329 <a title="4-tfidf-20" href="./jmlr-2008-Stationary_Features_and_Cat_Detection.html">87 jmlr-2008-Stationary Features and Cat Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.096), (1, -0.024), (2, 0.031), (3, -0.002), (4, -0.102), (5, 0.029), (6, 0.072), (7, 0.001), (8, 0.024), (9, -0.049), (10, -0.083), (11, 0.071), (12, -0.037), (13, 0.049), (14, -0.012), (15, 0.029), (16, 0.034), (17, 0.141), (18, -0.052), (19, -0.2), (20, -0.113), (21, 0.083), (22, 0.047), (23, 0.094), (24, -0.274), (25, -0.179), (26, 0.408), (27, 0.195), (28, 0.134), (29, 0.141), (30, 0.045), (31, 0.079), (32, -0.181), (33, -0.272), (34, 0.039), (35, 0.047), (36, -0.04), (37, -0.111), (38, -0.014), (39, 0.121), (40, -0.095), (41, 0.02), (42, -0.162), (43, 0.066), (44, -0.054), (45, 0.024), (46, 0.053), (47, -0.066), (48, 0.062), (49, -0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96800119 <a title="4-lsi-1" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>Author: Zach Jorgensen, Yan Zhou, Meador Inge</p><p>Abstract: Statistical spam ﬁlters are known to be vulnerable to adversarial attacks. One of the more common adversarial attacks, known as the good word attack, thwarts spam ﬁlters by appending to spam messages sets of “good” words, which are words that are common in legitimate email but rare in spam. We present a counterattack strategy that attempts to differentiate spam from legitimate email in the input space by transforming each email into a bag of multiple segments, and subsequently applying multiple instance logistic regression on the bags. We treat each segment in the bag as an instance. An email is classiﬁed as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. We show that a classiﬁer using our multiple instance counterattack strategy is more robust to good word attacks than its single instance counterpart and other single instance learners commonly used in the spam ﬁltering domain. Keywords: spam ﬁltering, multiple instance learning, good word attack, adversarial learning</p><p>2 0.48191467 <a title="4-lsi-2" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>Author: Shann-Ching Chen, Geoffrey J. Gordon, Robert F. Murphy</p><p>Abstract: In structured classiﬁcation problems, there is a direct conﬂict between expressive models and efﬁcient inference: while graphical models such as Markov random ﬁelds or factor graphs can represent arbitrary dependences among instance labels, the cost of inference via belief propagation in these models grows rapidly as the graph structure becomes more complicated. One important source of complexity in belief propagation is the need to marginalize large factors to compute messages. This operation takes time exponential in the number of variables in the factor, and can limit the expressiveness of the models we can use. In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efﬁcient algorithms for computing messages from these potentials during belief propagation. We believe these new potentials provide a good balance between expressive power and efﬁcient inference in practical structured classiﬁcation problems. We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. We use these potentials to classify images of protein subcellular location patterns in groups of cells. Classifying subcellular location patterns can help us answer many important questions in computational biology, including questions about how various treatments affect the synthesis and behavior of proteins and networks of proteins within a cell. Our new representation and algorithm lead to substantial improvements in both inference speed and classiﬁcation accuracy. Keywords: factor graphs, approximate inference algorithms, structured classiﬁcation, protein subcellular location patterns, location proteomics</p><p>3 0.42591181 <a title="4-lsi-3" href="./jmlr-2008-Linear-Time_Computation_of_Similarity_Measures_for_Sequential_Data.html">55 jmlr-2008-Linear-Time Computation of Similarity Measures for Sequential Data</a></p>
<p>Author: Konrad Rieck, Pavel Laskov</p><p>Abstract: Efﬁcient and expressive comparison of sequences is an essential procedure for learning with sequential data. In this article we propose a generic framework for computation of similarity measures for sequences, covering various kernel, distance and non-metric similarity functions. The basis for comparison is embedding of sequences using a formal language, such as a set of natural words, k-grams or all contiguous subsequences. As realizations of the framework we provide linear-time algorithms of different complexity and capabilities using sorted arrays, tries and sufﬁx trees as underlying data structures. Experiments on data sets from bioinformatics, text processing and computer security illustrate the efﬁciency of the proposed algorithms—enabling peak performances of up to 106 pairwise comparisons per second. The utility of distances and non-metric similarity measures for sequences as alternatives to string kernels is demonstrated in applications of text categorization, network intrusion detection and transcription site recognition in DNA. Keywords: string kernels, string distances, learning with sequential data</p><p>4 0.15144268 <a title="4-lsi-4" href="./jmlr-2008-Multi-Agent_Reinforcement_Learning_in_Common_Interest_and_Fixed_Sum_Stochastic_Games%3A_An_Experimental_Study.html">65 jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</a></p>
<p>Author: Avraham Bab, Ronen I. Brafman</p><p>Abstract: Multi Agent Reinforcement Learning (MARL) has received continually growing attention in the past decade. Many algorithms that vary in their approaches to the different subtasks of MARL have been developed. However, the theoretical convergence results for these algorithms do not give a clue as to their practical performance nor supply insights to the dynamics of the learning process itself. This work is a comprehensive empirical study conducted on MGS, a simulation system developed for this purpose. It surveys the important algorithms in the ﬁeld, demonstrates the strengths and weaknesses of the different approaches to MARL through application of FriendQ, OAL, WoLF, FoeQ, Rmax, and other algorithms to a variety of fully cooperative and fully competitive domains in self and heterogeneous play, and supplies an informal analysis of the resulting learning processes. The results can aid in the design of new learning algorithms, in matching existing algorithms to speciﬁc tasks, and may guide further research and formal analysis of the learning processes. Keywords: reinforcement learning, multi-agent reinforcement learning, stochastic games</p><p>5 0.14046092 <a title="4-lsi-5" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>Author: Glenn Shafer, Vladimir Vovk</p><p>Abstract: Conformal prediction uses past experience to determine precise levels of conÄ?Ĺš dence in new predictions. Given an error probability ĂŽÄž, together with a method that makes a prediction y of a label Ă&lsaquo;&dagger; y, it produces a set of labels, typically containing y, that also contains y with probability 1 Ă˘&circ;&rsquo; ĂŽÄž. Ă&lsaquo;&dagger; Conformal prediction can be applied to any method for producing y: a nearest-neighbor method, a Ă&lsaquo;&dagger; support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 Ă˘&circ;&rsquo; ĂŽÄž of the time, even though they are based on an accumulating data set rather than on independent data sets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005). Keywords: conÄ?Ĺš dence, on-line compression modeling, on-line learning, prediction regions</p><p>6 0.13000603 <a title="4-lsi-6" href="./jmlr-2008-Generalization_from_Observed_to_Unobserved_Features_by_Clustering.html">38 jmlr-2008-Generalization from Observed to Unobserved Features by Clustering</a></p>
<p>7 0.11154186 <a title="4-lsi-7" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>8 0.11086398 <a title="4-lsi-8" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>9 0.11083198 <a title="4-lsi-9" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>10 0.10903297 <a title="4-lsi-10" href="./jmlr-2008-On_the_Suitable_Domain_for_SVM_Training_in_Image_Coding.html">73 jmlr-2008-On the Suitable Domain for SVM Training in Image Coding</a></p>
<p>11 0.1062017 <a title="4-lsi-11" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>12 0.099110603 <a title="4-lsi-12" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>13 0.098003767 <a title="4-lsi-13" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>14 0.097770803 <a title="4-lsi-14" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>15 0.092698403 <a title="4-lsi-15" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>16 0.085909858 <a title="4-lsi-16" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>17 0.084416322 <a title="4-lsi-17" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>18 0.080941528 <a title="4-lsi-18" href="./jmlr-2008-Closed_Sets_for_Labeled_Data.html">22 jmlr-2008-Closed Sets for Labeled Data</a></p>
<p>19 0.079377636 <a title="4-lsi-19" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>20 0.077957727 <a title="4-lsi-20" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.012), (5, 0.026), (31, 0.028), (40, 0.033), (51, 0.526), (54, 0.026), (58, 0.029), (60, 0.014), (66, 0.031), (76, 0.03), (88, 0.054), (92, 0.029), (94, 0.041), (99, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82473695 <a title="4-lda-1" href="./jmlr-2008-Multi-Agent_Reinforcement_Learning_in_Common_Interest_and_Fixed_Sum_Stochastic_Games%3A_An_Experimental_Study.html">65 jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</a></p>
<p>Author: Avraham Bab, Ronen I. Brafman</p><p>Abstract: Multi Agent Reinforcement Learning (MARL) has received continually growing attention in the past decade. Many algorithms that vary in their approaches to the different subtasks of MARL have been developed. However, the theoretical convergence results for these algorithms do not give a clue as to their practical performance nor supply insights to the dynamics of the learning process itself. This work is a comprehensive empirical study conducted on MGS, a simulation system developed for this purpose. It surveys the important algorithms in the ﬁeld, demonstrates the strengths and weaknesses of the different approaches to MARL through application of FriendQ, OAL, WoLF, FoeQ, Rmax, and other algorithms to a variety of fully cooperative and fully competitive domains in self and heterogeneous play, and supplies an informal analysis of the resulting learning processes. The results can aid in the design of new learning algorithms, in matching existing algorithms to speciﬁc tasks, and may guide further research and formal analysis of the learning processes. Keywords: reinforcement learning, multi-agent reinforcement learning, stochastic games</p><p>same-paper 2 0.81889272 <a title="4-lda-2" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>Author: Zach Jorgensen, Yan Zhou, Meador Inge</p><p>Abstract: Statistical spam ﬁlters are known to be vulnerable to adversarial attacks. One of the more common adversarial attacks, known as the good word attack, thwarts spam ﬁlters by appending to spam messages sets of “good” words, which are words that are common in legitimate email but rare in spam. We present a counterattack strategy that attempts to differentiate spam from legitimate email in the input space by transforming each email into a bag of multiple segments, and subsequently applying multiple instance logistic regression on the bags. We treat each segment in the bag as an instance. An email is classiﬁed as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. We show that a classiﬁer using our multiple instance counterattack strategy is more robust to good word attacks than its single instance counterpart and other single instance learners commonly used in the spam ﬁltering domain. Keywords: spam ﬁltering, multiple instance learning, good word attack, adversarial learning</p><p>3 0.17901616 <a title="4-lda-3" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>4 0.17572403 <a title="4-lda-4" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>Author: Liviu Panait, Karl Tuyls, Sean Luke</p><p>Abstract: This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the beneﬁts of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction</p><p>5 0.17498519 <a title="4-lda-5" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>Author: Shann-Ching Chen, Geoffrey J. Gordon, Robert F. Murphy</p><p>Abstract: In structured classiﬁcation problems, there is a direct conﬂict between expressive models and efﬁcient inference: while graphical models such as Markov random ﬁelds or factor graphs can represent arbitrary dependences among instance labels, the cost of inference via belief propagation in these models grows rapidly as the graph structure becomes more complicated. One important source of complexity in belief propagation is the need to marginalize large factors to compute messages. This operation takes time exponential in the number of variables in the factor, and can limit the expressiveness of the models we can use. In this paper, we study a new class of potential functions, which we call decomposable k-way potentials, and provide efﬁcient algorithms for computing messages from these potentials during belief propagation. We believe these new potentials provide a good balance between expressive power and efﬁcient inference in practical structured classiﬁcation problems. We discuss three instances of decomposable potentials: the associative Markov network potential, the nested junction tree, and a new type of potential which we call the voting potential. We use these potentials to classify images of protein subcellular location patterns in groups of cells. Classifying subcellular location patterns can help us answer many important questions in computational biology, including questions about how various treatments affect the synthesis and behavior of proteins and networks of proteins within a cell. Our new representation and algorithm lead to substantial improvements in both inference speed and classiﬁcation accuracy. Keywords: factor graphs, approximate inference algorithms, structured classiﬁcation, protein subcellular location patterns, location proteomics</p><p>6 0.17494537 <a title="4-lda-6" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>7 0.17485482 <a title="4-lda-7" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>8 0.17370467 <a title="4-lda-8" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>9 0.17363873 <a title="4-lda-9" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>10 0.17323349 <a title="4-lda-10" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>11 0.17311163 <a title="4-lda-11" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>12 0.17271705 <a title="4-lda-12" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>13 0.17249332 <a title="4-lda-13" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>14 0.17244968 <a title="4-lda-14" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>15 0.17231052 <a title="4-lda-15" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>16 0.17120241 <a title="4-lda-16" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>17 0.17045489 <a title="4-lda-17" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>18 0.1700692 <a title="4-lda-18" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>19 0.16835746 <a title="4-lda-19" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>20 0.16792953 <a title="4-lda-20" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
