<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-17" href="#">jmlr2008-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</h1>
<br/><p>Source: <a title="jmlr-2008-17-pdf" href="http://jmlr.org/papers/volume9/hoyle08a/hoyle08a.pdf">pdf</a></p><p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><p>Reference: <a title="jmlr-2008-17-reference" href="../jmlr2008_reference/jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. [sent-10, score-0.759]
</p><p>2 The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. [sent-13, score-0.455]
</p><p>3 In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. [sent-14, score-0.459]
</p><p>4 Its utility and success stems from the simplicity of the method - one simply calculates the eigenvectors and eigenvalues of the sample ˆ ˆ covariance matrix C of the data set. [sent-23, score-0.632]
</p><p>5 Identiﬁcation of the appropriate signal dimensionality is just a model selection process to which the techniques of Bayesian model selection can be applied via a suitable approximation of the Bayesian evidence (MacKay, 1992). [sent-30, score-0.798]
</p><p>6 • In Section 3 we summarize the behaviour of the eigenvectors and eigenvalues of sample covariance matrices formed from high-dimensional small sample size data sets. [sent-33, score-0.835]
</p><p>7 • In Section 6 the model selection performance of the improved approximation to the model evidence is compared with a frequentist hypothesis testing approach to model selection. [sent-39, score-0.483]
</p><p>8 However, though retaining only a small number of terms from the asymptotic expansion of the evidence would be increasingly accurate as N → ∞, individual expansion coefﬁcients may be signiﬁcant due to the large data dimensionality d. [sent-52, score-0.464]
</p><p>9 For high dimensional data, rather than considering the evidence to be close to its value obtained in the asymptotic limit N → ∞ at ﬁxed d, it may be more appropriate to consider the evidence as being close to its value in the distinguished limit d, N → ∞ at ﬁxed α = N/d. [sent-56, score-0.664]
</p><p>10 The distorting effects of high dimensionality upon covariance matrix eigenvalue spectra and eigenvectors are well known from random matrix theory (RMT) studies (Johnstone, 2006). [sent-69, score-0.608]
</p><p>11 The RMT studies inform us about the expected sample covariance eigenvalue spectrum in the limit d → ∞ (at ﬁxed α), and consequently the limits of any model selection procedure based upon the ˆ observed eigenvalue spectra. [sent-70, score-0.952]
</p><p>12 As PCA is based upon the eigenvalues and eigenvectors of C, understanding their behaviour for small sample sizes and high data dimensions is key to understanding the behaviour of the existing model selection criterion, including the Bayesian model selection approach of Minka. [sent-71, score-0.99]
</p><p>13 Such models have been termed “spiked” covariance models within the statistics research literature (Johnstone, 2001), due to the small number of δ-function spikes in the population covariance eigenspectrum. [sent-84, score-0.622]
</p><p>14 The signal strengths σ2 Am merely determine the population covariance eigenvalues corresponding to signal directions, and so the number of signal components S is commonly estimated by some process of inspection of the ordered eigenvalues ¯ ¯ ˆ λi , i = 1, . [sent-86, score-1.767]
</p><p>15 The empirical eigenvalue density is considered to be a self-averaging quantity, such that as N → ∞ the eigenvalue density from any individual sample covariance matrix is well represented by the ensemble average. [sent-98, score-0.608]
</p><p>16 Hoyle and Rattray (2004a) studied the expected behaviour of the sample covariance eigenvalue spectrum for “spiked” covariance models in the asymptotic limit d → ∞ at ﬁxed α, by using techniques from statistical physics. [sent-118, score-1.102]
</p><p>17 As the addition of a small number, S, of signal directions provides a relatively small perturbation to an isotropic population covariance, the majority, or bulk of eigenvalues are still distributed according to the Marˇ enko-Pastur law. [sent-120, score-0.929]
</p><p>18 For the “spiked” covariance models of c Equation (1) the expected eigenvalue distribution ρ(λ) is modiﬁed from ρ bulk (λ). [sent-122, score-0.612]
</p><p>19 A transition occurs at α = A−2 , such that for α > A−2 a sample eigenvalue located at m m λ = λu (Am ) can be resolved separately from the remaining Marˇ enko-Pastur bulk of eigenvalues. [sent-125, score-0.49]
</p><p>20 c Thus for S signal components within the “spiked” covariance model we can observe up to S transitions in the sample covariance eigenspectrum, on increasing α. [sent-126, score-0.917]
</p><p>21 (1996), who 2737  H OYLE  considered the behaviour (as d → ∞ at ﬁxed α) of the expectation value of R 2 , where R1 = B1 · J1 is 1 the overlap between the ﬁrst principal component J1 of the sample covariance and B1 . [sent-129, score-0.752]
</p><p>22 That the ability to detect the signal components is reﬂected in the sample covariance eigenvalue structure (with retarded learning transitions coinciding with transitions in the eigenspectrum) demonstrates the utility of the sample covariance eigenspectrum for model selection. [sent-132, score-1.289]
</p><p>23 It also highlights that if the true signal dimensionality is S then asymptotically we have at most only S sample covariance eigenvalues separated from the Marˇ enko-Pastur bulk c ˆ eigenvalues sepadistribution, dependent on the value of α. [sent-133, score-1.488]
</p><p>24 Equally, for sufﬁciently small α it is impossible, asymptotically, to distinguish the sample spectrum from one which has been generated from a model containing no signal structure, that is, from a population covariance C = σ2 I. [sent-135, score-0.808]
</p><p>25 Within these constraints placed by the expected behaviour of the observed eigenspectra we now attempt to derive a suitable Bayesian model selection procedure that performs well in the distinguished asymptotic limit N, d → ∞ at ﬁxed α. [sent-136, score-0.628]
</p><p>26 The matrix H represents the signal considered present in the data and so is modelled as being due to a small number, k, of orthogonal signal components u i , i = 1, . [sent-146, score-0.606]
</p><p>27 , k, which represent estimators of the population covariance eigenvalues Λ i . [sent-155, score-0.678]
</p><p>28 Given a prior p(U , W , L, v) the evidence for a signal dimensionality k is then, p(D|k) =  Z  dU dW dLdv p(D|U , W , L, v)p(U , W , L, v) . [sent-160, score-0.482]
</p><p>29 Area(Vk (Rd )) i=1  The dependence of Nk (d) upon k is relatively weak compared to other factors contributing to ln p(D|k), and so Minka drops Nk (d) from further consideration in approximating p(D|k). [sent-170, score-0.487]
</p><p>30 (6)  H OYLE  Within this approximation lˆi provides a point estimate of the ith population covariance eigenvalue Λi . [sent-174, score-0.594]
</p><p>31 We have sampled data vectors ξ µ from a population covariance C containing three signal components. [sent-179, score-0.638]
</p><p>32 2a, that even with transposing the centred data matrix, the model selection accuracy decreases with increasing data dimensionality d, at ﬁxed sample size N. [sent-198, score-0.483]
</p><p>33 Consequently the accuracy of model selection based upon the sample covariance eigenspectrum will always decrease with increasing d, at ﬁxed N, due to the distorting effects of high data dimensionality. [sent-200, score-0.676]
</p><p>34 The data is generated with a population covariance C containing three signal components -see main text for details. [sent-217, score-0.691]
</p><p>35 Solid symbols represent simulation results from the model selection procedure applied to the mean centred data matrix, whilst open symbols represent simulation results from the model selection procedure applied to the transpose of the mean centred data matrix. [sent-218, score-0.827]
</p><p>36 One ﬁnds, N ˆ |HH T + vI|−(N+1+η)/2 exp(− tr((HH T + vI)−1 (C + N −1 ηI))) 2 N +1+η k N N−1 = exp − ln li + (d − k) ln v − ∑ ∑ λj 2 2v j=1 i=1 +  ηd N k −1 −1 N−1 η k ∑ (v − li ) ∑ λ j R2j − 2v + 2 ∑ (v−1 − li−1 ) . [sent-228, score-0.958]
</p><p>37 i 2v j=1 j=1 i=1 i=1  (8)  Approximations to the model evidence can now be made by approximating this integration over the overlap variables {Ri j }, and consequently this approach is termed the “overlap” method. [sent-234, score-0.465]
</p><p>38 ,  The dominant stationary point solution has the overlap between the i th signal direction estimate, ui ˆ ii ˆ and the ith sample covariance eigenvector, vi , being non-zero, that is, R2 > 0, R2 = 0, ∀i = i , ii 2 = 0. [sent-237, score-0.879]
</p><p>39 b i = 1 − R2 − N(v−1 − lˆi−1 )λi ˆ  (11) (12) (13)  (14) (15)  Again the saddle-point solution values v and lˆi provide us with point estimates for the populaˆ tion noise level σ2 and population signal eigenvalue Λi respectively. [sent-248, score-0.559]
</p><p>40 Obtainˆ ing real-valued estimates, lˆi , for the population covariance eigenvalues is clearly dependent upon the quadratic equation in (12) having a non-negative discriminant. [sent-250, score-0.79]
</p><p>41 The “overlap” approximation to the log-evidence, given in Equation (16), can be used for model selection by selecting the value of k that has the highest value of ln p(D|k). [sent-273, score-0.594]
</p><p>42 3(b) are the simulation estimates of model selection accuracy for Minka’s approximation to the model evidence applied to the transposed mean centred data. [sent-282, score-0.665]
</p><p>43 The “overlap” approximation essentially contains the leading order term of an asymptotic expansion of the evidence in that distinguished limit. [sent-289, score-0.459]
</p><p>44 It has been argued that since for many real high-dimensional data sets α 1, one would expect that approximations to the model evidence that are accurate in the distinguished limit will have superior model selection accuracy at ﬁnite values of d, N. [sent-292, score-0.541]
</p><p>45 Studying the ensemble expectation, in the asymptotic limit of d → ∞ at ﬁxed α, of the “overlap” approximation to the model evidence provides us with insight into its accuracy as a model selection procedure for high dimensional data. [sent-299, score-0.676]
</p><p>46 Consequently, due ˆ j=1 to the self-averaging nature of the sample covariance eigenvalue spectrum, we have that v → E ξ (λ) ˆ as N → ∞, where we have used Eξ (·) to denote expectation over the ensemble of sample data sets. [sent-301, score-0.551]
</p><p>47 We already commented in Section 3 that Eξ (λ) = σ2 in the asymptotic limit N → ∞ at ﬁxed α, and so v provides an asymptotically unbiased estimate of the population noise level. [sent-302, score-0.463]
</p><p>48 Estimates of ˆ the population signal eigenvalues are given by {lˆi }k , and in the distinguished asymptotic limit i=1 solutions to Equation (12) for lˆi are given by, v ˆ lˆi = (1 + λi v−1 − α−1 ) ± ˆ 2  (1 + λi v−1 − α−1 )2 − 4λi v−1 . [sent-303, score-0.993]
</p><p>49 ˆ ˆ  (17)  If we consider a “spiked” population covariance model of the form in Equation (1) the population covariance eigenvalues correspond to signal eigenvalues Λ i = σ2 (1 + Ai ), i ≤ S and noise eigenvalues Λi = σ2 , i > S. [sent-304, score-1.85]
</p><p>50 For sample covariance eigenvalues that are below the edge of the Marˇ enko-Pastur bulk distribution, c 1 2 (1 + α− 2 )2 , we obtain only complex solutions from Equation (17). [sent-307, score-0.817]
</p><p>51 Speciﬁcally we have for α > A −2 , i  ∑ ln(λi − λ j )  lim N −1 Eξ  N,d→∞  = ln σ2 − (α−1 − 1) ln(1 + Ai ) + α−1 ln Ai +  j>k  1 , αAi  (18)  whilst for α < A−2 we have, i lim N −1 Eξ  N,d→∞  ∑ ln(λi − λ j ) j>k  1  1  1  = ln σ2 − (α−1 − 1) ln(1 + α− 2 ) + α−1 ln α− 2 + α− 2 . [sent-313, score-1.703]
</p><p>52 (19)  The asymptotic behaviour of the ratio Area(Vk (Rd−N+1 ))/Area(Vk (Rd )) is easily evaluated to give, ln  Area(Vk (Rd−N+1 )) Area(Vk (Rd ))  =  Nk d − ln π + ln − (α−1 − 1) ln(1 − α) − 1 + O(ln N) . [sent-314, score-1.502]
</p><p>53 (20) 2 2  Substituting Equations (18),(19),(20) and the asymptotic values for v and lˆi into Equation (16), we ˆ obtain after some straight-forward algebra, Eξ (ln p(D|k)) = −  1 + Ai N k 1 ∑ Θ(α − A−2 ) Ai − αAi + (α−1 − 1) ln 1 + (1/αAi ) i 2 i=1  + α−1 ln  1 αA2 i  Nd Nd ln σ2 − + O(ln N) . [sent-315, score-1.371]
</p><p>54 2 2  (21)  If we set x = α−1 we can write the summand in Equation (21) as Θ(α − A−2 ) f (x, Ai ) where, i f (x, A) = A + x ln x + (x − 1) ln(1 + A) − 2x ln A − (x − 1) ln(1 + xA −1 ) − xA−1 . [sent-316, score-0.816]
</p><p>55 Consequently if α > A −2 , so that the i i sample covariance eigenvalue spectrum reﬂects the presence of the signal B i , then the addition of 2746  AUTOMATIC PCA D IMENSIONALITY S ELECTION  ith principal component results in an increase in the asymptotic approximation to the log-evidence. [sent-318, score-1.117]
</p><p>56 Ultimately this is due to the fact that we are considering models with a ﬁnite number, k, of signal components, and so in the asymptotic limit we are considering a vanishingly small proportion of sample covariance eigenvalues as representing signal components. [sent-321, score-1.328]
</p><p>57 For ﬁnite sample sizes we would expect the higher order terms in the expansion of the log-evidence to lead to a decrease in the log-evidence on inclusion of principal components that correspond to sample covariance eigenvalues that are below the bulk edge. [sent-323, score-1.067]
</p><p>58 The limiting model selection estimate, S, for the true signal dimensionality, S, then simply corresponds to counting the number of sample covariance eigenvalues that are beyond the upper edge of the Marˇ enko-Pastur bulk distribution. [sent-325, score-1.243]
</p><p>59 j=1  The asymptotic analysis of the “overlap” method reveals that unbiased estimates of the population signal eigenvalues can be recovered and that, asymptotically, model selection based upon the “overlap” approximation to the log-evidence performs optimally. [sent-327, score-1.15]
</p><p>60 Speciﬁcally f M (x, A) is given as, fM (x, A) = A + x ln x + (x − 1) ln(1 + A) − 2x ln A − x ln 1 + xA−1 + xA−2 . [sent-336, score-1.224]
</p><p>61 (23)  The transition point at which a signal component is strong enough to be distinguishable from the 1 Marˇ enko-Pastur bulk distribution in Equation (2) is given by a signal strength A = α − 2 . [sent-337, score-0.878]
</p><p>62 Thus, even though a detectable signal is present model selection based upon Equation (22) would not include that signal component. [sent-343, score-0.727]
</p><p>63 2b it is only at the largest value of α shown that we have f M > 0 for all three signal components, and thus that all three signal components are guaranteed to be detectable in the asymptotic limit. [sent-353, score-0.718]
</p><p>64 N f M /2 represents the incremental change (to leading order) in the log-evidence on retaining a 1 principal component corresponding to a signal component of strength A = yα − 2 . [sent-369, score-0.46]
</p><p>65 Whilst this result appears intuitive from the viewpoint of the behaviour of eigenspectra of large sample covariance matrices presented in Section 3, we have also shown that not all approximations to the Bayesian evidence reduce in the asymptotic limit to this optimal choice for model selection. [sent-372, score-0.878]
</p><p>66 One of the most commonly applied techniques for dimensionality selection for PCA is to select sample covariance eigenvalues (and corresponding eigenvectors) that account for a ﬁxed percentage of the total variance, for example, 90%. [sent-375, score-0.762]
</p><p>67 However, with sample covariance eigenvalues potentially being highly biased even when the population covariance is isotropic this is not always a reliable or easily implemented technique. [sent-378, score-0.988]
</p><p>68 Sample covariance eigenvalues above c 2749  H OYLE  the 45 degree line in these Wachter plots indicate potentially signal containing principal components. [sent-383, score-0.857]
</p><p>69 Comparison of the (k + 1)th sample covariance eigenvalue, λk+1 , against the sampling distribution of λk+1 under H0 would allow for potential rejection of the null-hypothesis and inclusion of the (k + 1)th principal component as representing genuine signal in the data. [sent-385, score-0.704]
</p><p>70 Then for data drawn from an isotropic population covariance, C = σ 2 I, the largest sample covariance eigenvalue λ1 (suitably centred and scaled) converges in distribution to the Tracy-Widom distribution W1 . [sent-399, score-0.737]
</p><p>71 2  200  400  600  800  1000  Dimension d  Figure 5: Comparison of the model selection accuracy for the “overlap” method (solid black symbols) with a null hypothesis test based upon the Tracy-Widom distribution for the largest eigenvalue of a sample covariance matrix (open symbols). [sent-412, score-0.736]
</p><p>72 Although in the distinguished asymptotic limit the Bayesian and frequentist approaches to model selection agree, it is interesting to compare model selection accuracies for ﬁnite values of N and d. [sent-423, score-0.649]
</p><p>73 For real data sets the sampling distribution of the individually ranked eigenvalues will have an effect upon the performance of the hypothesis test approach and likewise accuracy of point estimates for model parameters will impact upon the performance of the Bayesian methods. [sent-424, score-0.585]
</p><p>74 By deﬁnition, the hypothesis test only considers a sample covariance eigenvalue to represent a signal if it exceeds that expected from the null model by more than reasonable sampling variation. [sent-437, score-0.779]
</p><p>75 For PCA the appropriate distinguished asymptotic limit is d, N → ∞, with α = N/d ﬁxed, though for other models different distinguished limits may need to be considered in order to observe meaningful non-trivial behaviour that is distinct from the large sample limit, N → ∞. [sent-443, score-0.642]
</p><p>76 Ultimately this is due to the biased sample covariance eigenvalues and the poor accuracy of the sample covariance eigenvectors in representing the signal directions when α < 1. [sent-449, score-1.26]
</p><p>77 Of greater interest perhaps is the fact that we have been able to demonstrate the asymptotic equivalence of the Bayesian evidence based model selection criterion and the frequentist hypothesis testing approach to model selection. [sent-457, score-0.527]
</p><p>78 Furthermore, analysis of the asymptotic behaviour of the “overlap” approximation to the log-evidence reveals that the estimators of the population signal eigenvalues are unbiased, at least for the “spiked” covariance models considered here. [sent-458, score-1.271]
</p><p>79 Although Minka’s derivation provides a poorer approximation to the model evidence, in the distinguished limit N, d → ∞ at ﬁxed α, in comparison to the “overlap” approximation, it is still the correct leading order approximation in the asymptotic limit N → ∞ at arbitrary ﬁxed values of d. [sent-460, score-0.581]
</p><p>80 This suggests that Minka’s Laplace approximation to the model evidence could be re-used to develop improved point estimates of population covariance eigenvalues {Λ i }. [sent-462, score-0.9]
</p><p>81 One proceeds by noting that ˆ the eigenvectors of C are the maximum posterior estimates of U for arbitrary choices of {l i } and v, since projection of the sample data onto the sample covariance eigenvectors retains the greatest variance. [sent-463, score-0.532]
</p><p>82 j=1  2753  H OYLE  The equation above, determining the asymptotic behaviour of the estimator lˆi is asymptotically identical to that given in Equation (12) for the “overlap” method and so, as already noted, gives asymptotically unbiased estimates for the population signal eigenvalue. [sent-469, score-0.92]
</p><p>83 However, as we have demonstrated with i=1 the increased model selection accuracy of the “overlap” method, it is important to explicitly reformulate the integration in terms of variables that are ﬁnite in number even in the asymptotic limit N, d → ∞ at ﬁxed α. [sent-474, score-0.505]
</p><p>84 If we consider a distinguished limit characterised by N/d → α < 1 and k/N → β < 1 as N, d, k → ∞, then any asymptotic analysis will need to take account of the effect a non-vanishing proportion of signal population eigenvalues has upon the distribution of sample covariance eigenvalues. [sent-481, score-1.387]
</p><p>85 The signal directions would no longer represent a small number of rank one perturbations of the identity matrix, with the consequence that the limiting sample covariance eigenvalue distribution would no longer correspond to the Marˇ enko-Pastur distribution given in Equation (2). [sent-482, score-0.74]
</p><p>86 Thus, the degeneracy of the model likelihood is due to a combination of small sample size, N < d, and that the likelihood is expressed in terms of projections of the sample data onto the model signal vectors. [sent-491, score-0.541]
</p><p>87 Since we are interested in the leading order behaviour of this expectation value, that is, the scaling with N, we can change the summation over j to include only those eigenvalues in the bulk distribution given in Equation (2). [sent-497, score-0.633]
</p><p>88 Even if some c sample covariance eigenvalues λ j lie outside the Marˇ enko-Pastur bulk for j > k the asymptotic c result given in (25) is still valid since N −1 ln(λi − λ) ∼ O(N −1 ) for λi > λ > λmax . [sent-499, score-0.964]
</p><p>89 Thus contributions to N −1 Eξ (∑ j>k ln(λi − λ j )) from a small number of sample eigenvalues outside of the bulk distribution are vanishingly small in the asymptotic limit. [sent-500, score-0.758]
</p><p>90 Since we are restricting the summation over j to eigenvalues in the bulk then if we denote the interval [λ min , λmax ] ≡ Ibulk , we can write, lim N −1 Eξ  N,d→∞  ∑  λ j ∈Ibulk  ln(λi − λ j )  = lim N −1 Eξ tr ln(λi I − N −1 G) . [sent-502, score-0.502]
</p><p>91 (27)  Differentiating with respect to x and q1 the expression in (27) is easily maximized to give (for λi = σ2 (1 + Ai )(1 + α−1 A−1 )), i ln σ2 − (α−1 − 1) ln(1 + Ai ) + α−1 ln Ai +  1 . [sent-510, score-0.816]
</p><p>92 αAi  (28)  Here we have assumed the sample covariance eigenvalue λ i will correspond to that from a “spiked” population covariance and that we are above the retarded learning transition for the ith signal component. [sent-511, score-1.234]
</p><p>93 For α < A−2 we are below the retarded learning transition for the ith signal and we expect i 1 λi to be located approximately at the upper edge of the bulk distribution so that λ i σ2 (1 + α− 2 )2 . [sent-512, score-0.651]
</p><p>94 Setting λi = λmax in the previous replica calculation still yields a well-behaved estimate for limN→∞ N −1 Eξ ∑ j>k ln(λi − λ j ) , namely, 1  1 −2  1  1  ln σ2 − (α−1 − 1) ln(1 + α− 2 ) + α−1 ln α− 2 + α− 2 . [sent-514, score-0.858]
</p><p>95 Asymptotically, in the limit N → ∞ and for the population covariance signal strengths chosen, three sample covariance eigenvalues are expected to be separated from the bulk distribution over the entire range of α plotted. [sent-524, score-1.575]
</p><p>96 In this case the population covariance contains a single signal component, of strength A, whilst we have ﬁxed α = 0. [sent-528, score-0.743]
</p><p>97 The population covariance contains three signal components with A1 = 50, A2 = 30, A3 = 20. [sent-555, score-0.691]
</p><p>98 The population covariance contains a single signal component with signal strength A. [sent-559, score-0.968]
</p><p>99 Eigenvalues of large sample covariance matrices of spiked population models. [sent-565, score-0.556]
</p><p>100 Phase transition of the largest eigenvalue for non-null complex sample covariance matrices. [sent-571, score-0.493]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ln', 0.408), ('eigenvalues', 0.262), ('signal', 0.259), ('covariance', 0.243), ('bulk', 0.24), ('overlap', 0.176), ('mar', 0.165), ('minka', 0.152), ('asymptotic', 0.147), ('imensionality', 0.136), ('oyle', 0.136), ('population', 0.136), ('pca', 0.133), ('behaviour', 0.131), ('eigenvalue', 0.129), ('centred', 0.125), ('evidence', 0.121), ('laplace', 0.109), ('hoyle', 0.105), ('spiked', 0.105), ('distinguished', 0.103), ('dimensionality', 0.102), ('eigenspectrum', 0.098), ('principal', 0.093), ('bayesian', 0.089), ('integration', 0.088), ('vk', 0.088), ('limit', 0.086), ('selection', 0.083), ('upon', 0.079), ('retarded', 0.073), ('integrand', 0.073), ('sample', 0.072), ('hh', 0.071), ('whilst', 0.071), ('li', 0.071), ('election', 0.07), ('equation', 0.07), ('simulation', 0.065), ('dri', 0.063), ('automatic', 0.059), ('rd', 0.059), ('bishop', 0.058), ('ai', 0.056), ('approximation', 0.056), ('ri', 0.056), ('eigenvectors', 0.055), ('accuracy', 0.054), ('components', 0.053), ('frequentist', 0.053), ('tipping', 0.053), ('rattray', 0.052), ('spectrum', 0.051), ('transition', 0.049), ('asymptotically', 0.048), ('model', 0.047), ('unbiased', 0.046), ('degeneracy', 0.044), ('uctuations', 0.044), ('area', 0.044), ('symbols', 0.043), ('baik', 0.042), ('replica', 0.042), ('rotational', 0.042), ('wachter', 0.042), ('averages', 0.038), ('stationary', 0.038), ('component', 0.037), ('contributions', 0.037), ('estimators', 0.037), ('limiting', 0.037), ('orthogonal', 0.035), ('ensemble', 0.035), ('estimates', 0.035), ('strengths', 0.034), ('strength', 0.034), ('nk', 0.034), ('consequently', 0.033), ('expansion', 0.032), ('ui', 0.032), ('isotropic', 0.032), ('az', 0.032), ('transposed', 0.032), ('dbi', 0.031), ('eigenspectra', 0.031), ('enko', 0.031), ('manchester', 0.031), ('rmt', 0.031), ('fm', 0.031), ('ith', 0.03), ('transpose', 0.03), ('johnstone', 0.03), ('increasingly', 0.03), ('hypothesis', 0.029), ('dimension', 0.029), ('bm', 0.029), ('reformulation', 0.029), ('vi', 0.029), ('integral', 0.029), ('physics', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999952 <a title="17-tfidf-1" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><p>2 0.27378064 <a title="17-tfidf-2" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>3 0.17148563 <a title="17-tfidf-3" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>Author: Mikio L. Braun, Joachim M. Buhmann, Klaus-Robert Müller</p><p>Abstract: We show that the relevant information of a supervised learning problem is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem in the sense that it can asymptotically represent the function to be learned and is sufﬁciently smooth. Thus, kernels do not only transform data sets such that good generalization can be achieved using only linear discriminant functions, but this transformation is also performed in a manner which makes economical use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data for supervised learning problems. Practically, we propose an algorithm which enables us to recover the number of leading kernel PCA components relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to aid in model selection, and (3) to denoise in feature space in order to yield better classiﬁcation results. Keywords: kernel methods, feature space, dimension reduction, effective dimensionality</p><p>4 0.11801022 <a title="17-tfidf-4" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>Author: Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui</p><p>Abstract: Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefﬁcients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semideﬁnite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefﬁcients, with total complexity O(n3 ), where n is the number of variables. We then use the same relaxation to derive sufﬁcient conditions for global optimality of a solution, which can be tested in O(n3 ) per pattern. We discuss applications in subset selection and sparse recovery and show on artiﬁcial examples and biological data that our algorithm does provide globally optimal solutions in many cases. Keywords: PCA, subset selection, sparse eigenvalues, sparse recovery, lasso</p><p>5 0.11148667 <a title="17-tfidf-5" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields general results for classiﬁcation and regression. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. We discuss the related problem of learning parameters of a distribution from multiple data sources. Finally, we illustrate our theory through a series of synthetic simulations. Keywords: error bounds, multi-task learning</p><p>6 0.1003158 <a title="17-tfidf-6" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>7 0.099375024 <a title="17-tfidf-7" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>8 0.091872379 <a title="17-tfidf-8" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>9 0.078589246 <a title="17-tfidf-9" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>10 0.065753758 <a title="17-tfidf-10" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>11 0.063348696 <a title="17-tfidf-11" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>12 0.061689418 <a title="17-tfidf-12" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>13 0.061541904 <a title="17-tfidf-13" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>14 0.058880899 <a title="17-tfidf-14" href="./jmlr-2008-Active_Learning_of_Causal_Networks_with_Intervention_Experiments_and_Optimal_Designs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">10 jmlr-2008-Active Learning of Causal Networks with Intervention Experiments and Optimal Designs    (Special Topic on Causality)</a></p>
<p>15 0.057240933 <a title="17-tfidf-15" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>16 0.054268073 <a title="17-tfidf-16" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>17 0.054135893 <a title="17-tfidf-17" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>18 0.0533554 <a title="17-tfidf-18" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>19 0.05011956 <a title="17-tfidf-19" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>20 0.048590079 <a title="17-tfidf-20" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.29), (1, -0.107), (2, -0.283), (3, -0.104), (4, 0.158), (5, 0.163), (6, -0.107), (7, -0.144), (8, -0.165), (9, 0.167), (10, 0.077), (11, 0.136), (12, 0.299), (13, 0.021), (14, -0.098), (15, 0.027), (16, -0.019), (17, 0.042), (18, 0.026), (19, -0.002), (20, 0.1), (21, 0.033), (22, -0.018), (23, -0.016), (24, 0.139), (25, -0.017), (26, 0.128), (27, 0.052), (28, 0.046), (29, -0.015), (30, 0.118), (31, 0.018), (32, 0.135), (33, -0.1), (34, -0.042), (35, 0.03), (36, 0.08), (37, -0.014), (38, -0.034), (39, -0.031), (40, -0.008), (41, 0.038), (42, 0.06), (43, 0.031), (44, 0.013), (45, -0.074), (46, -0.0), (47, -0.024), (48, 0.003), (49, -0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9797672 <a title="17-lsi-1" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><p>2 0.77195317 <a title="17-lsi-2" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>3 0.45553815 <a title="17-lsi-3" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>Author: Mikio L. Braun, Joachim M. Buhmann, Klaus-Robert Müller</p><p>Abstract: We show that the relevant information of a supervised learning problem is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem in the sense that it can asymptotically represent the function to be learned and is sufﬁciently smooth. Thus, kernels do not only transform data sets such that good generalization can be achieved using only linear discriminant functions, but this transformation is also performed in a manner which makes economical use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data for supervised learning problems. Practically, we propose an algorithm which enables us to recover the number of leading kernel PCA components relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to aid in model selection, and (3) to denoise in feature space in order to yield better classiﬁcation results. Keywords: kernel methods, feature space, dimension reduction, effective dimensionality</p><p>4 0.39926791 <a title="17-lsi-4" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>Author: Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui</p><p>Abstract: Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefﬁcients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semideﬁnite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefﬁcients, with total complexity O(n3 ), where n is the number of variables. We then use the same relaxation to derive sufﬁcient conditions for global optimality of a solution, which can be tested in O(n3 ) per pattern. We discuss applications in subset selection and sparse recovery and show on artiﬁcial examples and biological data that our algorithm does provide globally optimal solutions in many cases. Keywords: PCA, subset selection, sparse eigenvalues, sparse recovery, lasso</p><p>5 0.38866037 <a title="17-lsi-5" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields general results for classiﬁcation and regression. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. We discuss the related problem of learning parameters of a distribution from multiple data sources. Finally, we illustrate our theory through a series of synthetic simulations. Keywords: error bounds, multi-task learning</p><p>6 0.32549602 <a title="17-lsi-6" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>7 0.31886593 <a title="17-lsi-7" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>8 0.30375782 <a title="17-lsi-8" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>9 0.29860398 <a title="17-lsi-9" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>10 0.2803266 <a title="17-lsi-10" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>11 0.27218771 <a title="17-lsi-11" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>12 0.26678556 <a title="17-lsi-12" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>13 0.24627607 <a title="17-lsi-13" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>14 0.24622817 <a title="17-lsi-14" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>15 0.24134722 <a title="17-lsi-15" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>16 0.22251664 <a title="17-lsi-16" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>17 0.21917121 <a title="17-lsi-17" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>18 0.21123466 <a title="17-lsi-18" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>19 0.20127167 <a title="17-lsi-19" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>20 0.19348639 <a title="17-lsi-20" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.027), (5, 0.011), (9, 0.428), (31, 0.011), (40, 0.067), (54, 0.033), (58, 0.084), (66, 0.071), (76, 0.028), (88, 0.064), (92, 0.04), (94, 0.044), (99, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94656748 <a title="17-lda-1" href="./jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations.html">37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</a></p>
<p>Author: Jia Li, Andrew W. Moore</p><p>Abstract: Web sites must forecast Web page views in order to plan computer resource allocation and estimate upcoming revenue and advertising growth. In this paper, we focus on extracting trends and seasonal patterns from page view series, two dominant factors in the variation of such series. We investigate the Holt-Winters procedure and a state space model for making relatively short-term prediction. It is found that Web page views exhibit strong impulsive changes occasionally. The impulses cause large prediction errors long after their occurrences. A method is developed to identify impulses and to alleviate their damage on prediction. We also develop a long-range trend and season extraction method, namely the Elastic Smooth Season Fitting (ESSF) algorithm, to compute scalable and smooth yearly seasons. ESSF derives the yearly season by minimizing the residual sum of squares under smoothness regularization, a quadratic optimization problem. It is shown that for longterm prediction, ESSF improves accuracy signiﬁcantly over other methods that ignore the yearly seasonality. Keywords: web page views, forecast, Holt-Winters, Kalman ﬁltering, elastic smooth season ﬁtting</p><p>same-paper 2 0.83724183 <a title="17-lda-2" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><p>3 0.35212514 <a title="17-lda-3" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>Author: Tianjiao Chu, Clark Glymour</p><p>Abstract: Pointwise consistent, feasible procedures for estimating contemporaneous linear causal structure from time series data have been developed using multiple conditional independence tests, but no such procedures are available for non-linear systems. We describe a feasible procedure for learning a class of non-linear time series structures, which we call additive non-linear time series. We show that for data generated from stationary models of this type, two classes of conditional independence relations among time series variables and their lags can be tested efﬁciently and consistently using tests based on additive model regression. Combining results of statistical tests for these two classes of conditional independence relations and the temporal structure of time series data, a new consistent model speciﬁcation procedure is able to extract relatively detailed causal information. We investigate the ﬁnite sample behavior of the procedure through simulation, and illustrate the application of this method through analysis of the possible causal connections among four ocean indices. Several variants of the procedure are also discussed. Keywords: conditional independence test, contemporaneous causation, additive model regression, Granger causality, ocean indices</p><p>4 0.34059522 <a title="17-lda-4" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>5 0.31424004 <a title="17-lda-5" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>Author: Jörg Lücke,  Maneesh Sahani</p><p>Abstract: We study a generative model in which hidden causes combine competitively to produce observations. Multiple active causes combine to determine the value of an observed variable through a max function, in the place where algorithms such as sparse coding, independent component analysis, or non-negative matrix factorization would use a sum. This max rule can represent a more realistic model of non-linear interaction between basic components in many settings, including acoustic and image data. While exact maximum-likelihood learning of the parameters of this model proves to be intractable, we show that efﬁcient approximations to expectation-maximization (EM) can be found in the case of sparsely active hidden causes. One of these approximations can be formulated as a neural network model with a generalized softmax activation function and Hebbian learning. Thus, we show that learning in recent softmax-like neural networks may be interpreted as approximate maximization of a data likelihood. We use the bars benchmark test to numerically verify our analytical results and to demonstrate the competitiveness of the resulting algorithms. Finally, we show results of learning model parameters to ﬁt acoustic and visual data sets in which max-like component combinations arise naturally. Keywords: component extraction, maximum likelihood, approximate EM, competitive learning, neural networks</p><p>6 0.31395516 <a title="17-lda-6" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>7 0.31356317 <a title="17-lda-7" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>8 0.31339672 <a title="17-lda-8" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>9 0.31251952 <a title="17-lda-9" href="./jmlr-2008-Incremental_Identification_of_Qualitative_Models_of_Biological_Systems_using_Inductive_Logic_Programming.html">44 jmlr-2008-Incremental Identification of Qualitative Models of Biological Systems using Inductive Logic Programming</a></p>
<p>10 0.31077817 <a title="17-lda-10" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>11 0.30948406 <a title="17-lda-11" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>12 0.30906263 <a title="17-lda-12" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>13 0.30805275 <a title="17-lda-13" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>14 0.30690381 <a title="17-lda-14" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>15 0.30437002 <a title="17-lda-15" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>16 0.30246371 <a title="17-lda-16" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>17 0.30242169 <a title="17-lda-17" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>18 0.30172506 <a title="17-lda-18" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>19 0.30072835 <a title="17-lda-19" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>20 0.30031702 <a title="17-lda-20" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
