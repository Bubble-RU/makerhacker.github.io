<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-14" href="#">jmlr2008-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</h1>
<br/><p>Source: <a title="jmlr-2008-14-pdf" href="http://jmlr.org/papers/volume9/garcia08a/garcia08a.pdf">pdf</a></p><p>Author: Salvador García, Francisco Herrera</p><p>Abstract: In a recently published paper in JMLR, Demˇar (2006) recommends a set of non-parametric stas tistical tests and procedures which can be safely used for comparing the performance of classiﬁers over multiple data sets. After studying the paper, we realize that the paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method. However, it does not deal with some advanced topics in depth. Regarding these topics, we focus on more powerful proposals of statistical procedures for comparing n × n classiﬁers. Moreover, we illustrate an easy way of obtaining adjusted and comparable p-values in multiple comparison procedures. Keywords: statistical methods, non-parametric test, multiple comparisons tests, adjusted p-values, logically related hypotheses</p><p>Reference: <a title="jmlr-2008-14-reference" href="../jmlr2008_reference/jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 After studying the paper, we realize that the paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method. [sent-6, score-0.263]
</p><p>2 Regarding these topics, we focus on more powerful proposals of statistical procedures for comparing n × n classiﬁers. [sent-8, score-0.229]
</p><p>3 Moreover, we illustrate an easy way of obtaining adjusted and comparable p-values in multiple comparison procedures. [sent-9, score-0.17]
</p><p>4 Keywords: statistical methods, non-parametric test, multiple comparisons tests, adjusted p-values, logically related hypotheses  1. [sent-10, score-0.626]
</p><p>5 In them, a new proposal or methodology is offered and it is compared with other methods by means of pairwise comparisons. [sent-18, score-0.116]
</p><p>6 However, only the rankings computed by Friedman’s method (Friedman, 1937) are stipulated and authors establish comparisons based on them, without taking into account signiﬁcance levels. [sent-25, score-0.166]
</p><p>7 Demˇar focused his work in the analysis of new proposals, and he introduced s the Nemenyi test for making all pairwise comparisons (Nemenyi, 1963). [sent-26, score-0.316]
</p><p>8 We have based on the necessity to apply more powerful procedures in empirical studies in which no new method is proposed and the beneﬁt consists of obtaining more statistical differences among the classiﬁers compared. [sent-36, score-0.263]
</p><p>9 Thus, in this paper we describe these procedures and we analyze their behavior by means of the analysis of multiple repetitions of experiments with randomly selected data sets. [sent-37, score-0.205]
</p><p>10 However, advanced procedures such as Holm (1979), Hochberg (1988), Hommel (1988) and the ones described in this paper are usually not incorporated in statistical packages. [sent-41, score-0.209]
</p><p>11 The computation of the correct p-value, or Adjusted P-Value (APV) (Westfall and Young, 2004), in a comparison using any of these procedures is not very difﬁcult and, in this paper, we show how to include it with an illustrative example. [sent-42, score-0.189]
</p><p>12 Section 2 presents more powerful procedures for comparing all the classiﬁers among them in a n×n comparison of multiple classiﬁers and a case study. [sent-44, score-0.301]
</p><p>13 In Section 3 we describe the procedures for obtaining the APV by considering the post-hoc procedures explained by Demˇar and the ones explained in this paper. [sent-45, score-0.352]
</p><p>14 In Section 4, we perform an experimental study s of the behavior of the statistical procedures and we discuss the results obtained. [sent-46, score-0.161]
</p><p>15 Once Friedman’s test rejects the null hypothesis, we can proceed with a post-hoc test in order to ﬁnd the concrete pairwise comparisons which produce differences. [sent-52, score-0.438]
</p><p>16 For this reason, he described and studied in depth more powerful and sophisticated procedures derived from Bonferroni-Dunn such as Holm’s, Hochberg’s and Hommel’s methods. [sent-55, score-0.203]
</p><p>17 Nevertheless, we think that performing all pairwise comparisons in an experimental analysis may be useful and interesting in different cases when proposing a new method. [sent-56, score-0.282]
</p><p>18 In this case, the repetition of comparisons choosing different control classiﬁers may lose the control of the family-wise error. [sent-58, score-0.222]
</p><p>19 Our intention in this section is to give a detailed description of more powerful and advanced procedures derived from the Nemenyi test and to show a case study that uses these procedures. [sent-59, score-0.285]
</p><p>20 1 Advanced Procedures for Performing All Pairwise Comparisons A set of pairwise comparisons can be associated with a set or family of hypotheses. [sent-61, score-0.313]
</p><p>21 Any of the posthoc tests which can be applied to non-parametric tests (that is, those derived from the Bonferroni correction or similar procedures) work over a family of hypotheses. [sent-62, score-0.139]
</p><p>22 Two basic procedures are: • Nemenyi (1963) procedure: it adjusts the value of α in a single step by dividing the value of α by the number of comparisons performed, m = k(k − 1)/2. [sent-65, score-0.352]
</p><p>23 Holm’s procedure rejects H1 to H(i−1) if i is the smallest integer such that pi > α/(m − i + 1). [sent-76, score-0.187]
</p><p>24 They are easy to perform, but they often have a similar power to Holm’s procedure (they have more power than Holm’s procedure, but the difference between them is not very notable) when considering all pairwise comparisons. [sent-78, score-0.263]
</p><p>25 The hypotheses being tested belonging to a family of all pairwise comparisons are logically interrelated so that not all combinations of true and false hypotheses are possible. [sent-79, score-0.941]
</p><p>26 As a simple example of such a situation suppose that we want to test the three hypotheses of pairwise equality associated with the pairwise comparisons of three classiﬁers Ci , i = 1, 2, 3. [sent-80, score-0.712]
</p><p>27 It is easily seen from the relations among the hypotheses that if any one of them is false, at least one other must be false. [sent-81, score-0.28]
</p><p>28 Thus, there cannot be one false and two true hypotheses among these three. [sent-84, score-0.28]
</p><p>29 2679  G ARC´A AND H ERRERA I  Based on this argument, Shaffer proposed two procedures which make use of the logical relation among the family of hypotheses for adjusting the value of α (Shaffer, 1986). [sent-85, score-0.507]
</p><p>30 • Shaffer’s static procedure: following Holm’s step down method, at stage j, instead of rejecting Hi if pi ≤ α/(m − i + 1), reject Hi if pi ≤ α/ti , where ti is the maximum number of hypotheses which can be true given that any (i − 1) hypotheses are false. [sent-86, score-0.774]
</p><p>31 ,tm are fully determined for the given hypotheses H1 , . [sent-90, score-0.28]
</p><p>32 The possible numbers of true hypotheses, and thus the values of t i can be obtained from the recursive formula S(k) =  k [  {  j=1  j + x : x ∈ S(k − j)}, 2  where S(k) is the set of possible numbers of true hypotheses with k classiﬁers being compared, k ≥ 2, and S(0) = S(1) = {0}. [sent-94, score-0.28]
</p><p>33 • Shaffer’s dynamic procedure: it increases the power of the ﬁrst by substituting α/t i at stage i by the value α/ti∗ , where ti∗ is the maximum number of hypotheses that could be true, given that the previous hypotheses are false. [sent-95, score-0.609]
</p><p>34 It is a dynamic procedure since t i∗ depends not only on the logical structure of the hypotheses, but also on the hypotheses already rejected at step i. [sent-96, score-0.454]
</p><p>35 In Bergmann and Hommel (1988) was proposed a procedure based on the idea of ﬁnding all elementary hypotheses which cannot be rejected. [sent-99, score-0.329]
</p><p>36 , m} is called exhaustive if exactly all H j , j ∈ I, could be true. [sent-104, score-0.103]
</p><p>37 Sets S1 , S5 , S6 , S7 and S8 can be possible, because their hypotheses can be true at the same time, so they are exhaustive sets. [sent-117, score-0.383]
</p><p>38 Set S2 , basing on logically related hypotheses principles, is not possible because the performance of C1 cannot be equal to C2 and C3 , whereas C2 has different performance than C3 . [sent-118, score-0.348]
</p><p>39 The same consideration can be done to S3 and S4 , which are not exhaustive sets. [sent-119, score-0.103]
</p><p>40 • Bergmann and Hommel (1988) procedure: Reject all H j with j ∈ A, where the acceptance set / A=  {I : I exhaustive, min{Pi : i ∈ I} > α/|I|}  [  is the index set of null hypotheses which are retained. [sent-121, score-0.31]
</p><p>41 Due to this fact, we will obtain a set, named E, which will contain all the possible exhaustive sets of hypotheses for a certain comparison. [sent-126, score-0.383]
</p><p>42 Once the E set is obtained, the hypotheses that do not belong to the A set are rejected. [sent-128, score-0.28]
</p><p>43 Figure 1 shows a valid algorithm for obtaining all the exhaustive sets of hypotheses, using as input a list of classiﬁers C. [sent-129, score-0.133]
</p><p>44 E is a set of families of hypotheses; likewise, a family of hypotheses is a set of hypotheses. [sent-130, score-0.311]
</p><p>45 The computational complexity of the algorithm for obtaining 2 exhaustive sets is O(2n ). [sent-140, score-0.133]
</p><p>46 2681  G ARC´A AND H ERRERA I  An example illustrating the algorithm for obtaining all exhaustive sets is drawn in Figure 2. [sent-144, score-0.133]
</p><p>47 The comparisons or hypotheses are denoted by pairs of numbers without a separation character between them. [sent-146, score-0.446]
</p><p>48 When |Ci | < 2, no comparisons can be performed, so the obtainExhaustive function returns an empty set E. [sent-148, score-0.166]
</p><p>49 Table 1 gives the number of hypotheses (m), the number (2 n ) of index sets I and the number of exhaustive index sets (ne ) for k classiﬁers being compared. [sent-155, score-0.383]
</p><p>50 E = E ∪ {set of all possible and distinct pairwise comparisons using C} / 3. [sent-161, score-0.282]
</p><p>51 Return E Figure 1: Algorithm for obtaining all exhaustive sets The following subsections present a case study of a n × n comparison of some well-known classiﬁers over thirty data sets. [sent-176, score-0.161]
</p><p>52 In it, the four procedures explained above are employed. [sent-177, score-0.161]
</p><p>53 2 Performing All Pairwise Comparisons: A Case Study In the following, we show an example involving the four procedures described with a comparison of ﬁve classiﬁers: C4. [sent-179, score-0.189]
</p><p>54 7 · 1010  ne 14 51 202 876 4139 21146  Table 1: All pairwise comparisons of k classiﬁers  2683  G ARC´A AND H ERRERA I  NaiveBayes, Kernel (McLachlan, 2004)1 and, ﬁnally, CN2 (Clark and Niblett, 1989). [sent-182, score-0.282]
</p><p>55 F Due to the fact that the critical values are lower than the respective statistics, we can proceed with the post-hoc tests in order to detect signiﬁcant pairwise differences among all the classiﬁers. [sent-198, score-0.2]
</p><p>56 The standard 5·6 error in the pairwise comparison between two classiﬁers is SE = k(k+1) = 6·30 = 0. [sent-200, score-0.144]
</p><p>57 Table 6N 3 presents the family of hypotheses ordered by their p-value and the adjustment of α by Nemenyi’s, Holm’s and Shaffer’s static procedures. [sent-202, score-0.459]
</p><p>58 • Nemenyi’s test rejects the hypotheses [1–4] since the corresponding p-values are smaller than the adjusted α’s. [sent-203, score-0.47]
</p><p>59 • Bergmann-Hommel’s dynamic procedure ﬁrst obtains the exhaustive index set of hypotheses. [sent-206, score-0.152]
</p><p>60 4 It rejects all hypotheses H j with j ∈ A, so it rejects the hypotheses [1–8]. [sent-210, score-0.736]
</p><p>61 05  Table 3: Family of hypotheses ordered by p-value and adjusting of α by Nemenyi (NM), Holm (HM) and Shaffer (SH) procedures, considering an initial α = 0. [sent-456, score-0.28]
</p><p>62 The most powerful procedures detailed in this paper, Shaffer’s and Bergmann-Hommel’s, work following the same method of Holm’s procedure, so it is possible to hybridize them with other types of step up procedures, such as Hochberg’s, Hommel’s and Rom’s methods. [sent-461, score-0.203]
</p><p>63 When we apply these methods by using the logical relationships among hypothesis in a static way, they do not control the family-wise error (Hochberg and Rom, 1995). [sent-462, score-0.221]
</p><p>64 Furthermore, a small improvement of power in the Bergmann-Hommel procedure described here can be achieved when using Simes conjecture (Simes, 1986) in the obtaining of A set (see Hommel and Bernhard, 1999, for more details). [sent-465, score-0.128]
</p><p>65 When a p-value is within a multiple comparison, as in the example in Table 3, it reﬂects the probability error of a certain comparison, but it does not take into account the remaining comparisons belonging to the family. [sent-470, score-0.21]
</p><p>66 We also include the post-hoc tests explained in Demˇar (2006) and other for comparisons s with a control classiﬁer. [sent-475, score-0.248]
</p><p>67 • m is the number of possible comparisons in an all pairwise comparisons design; that is, m = k·(k−1) . [sent-480, score-0.448]
</p><p>68 2 • t j is the maximum number of hypotheses which can be true given that any ( j − 1) hypotheses are false (see the description of Shaffer’s static procedure in Section 2. [sent-481, score-0.723]
</p><p>69 2687  G ARC´A AND H ERRERA I  The procedures of p-value adjustment can be classiﬁed into: • one-step. [sent-483, score-0.195]
</p><p>70 – Holm APVi (using it in all pairwise comparisons): min{v; 1}, where v = max{(m − j + 1)p j : 1 ≤ j ≤ i}. [sent-492, score-0.116]
</p><p>71 – Shaffer static APVi : min{v; 1}, where v = max{t j p j : 1 ≤ j ≤ i}. [sent-493, score-0.114]
</p><p>72 0  Table 5: APVs obtained in the example by Nemenyi (NM), Holm (HM), Shaffer’s static (SH) and Bergmann-Hommel’s dynamic (BH)  4. [sent-586, score-0.114]
</p><p>73 Experimental Framework In this section, we want to determine the power and behavior of the studied procedures through the experiments in which we repeatedly compared the classiﬁers on sets of ten randomly chosen data sets, recording the number of equivalence hypothesis rejected and APVs. [sent-587, score-0.344]
</p><p>74 In comparisons of multiple classiﬁers, samples of data sets have to be selected with the probabilities computed from the differences in accuracy of two classiﬁers. [sent-598, score-0.24]
</p><p>75 Note that the repeated comparisons done here only involve ten data sets each time, so the rejection of equivalence of two classiﬁers is more difﬁcult at the beginning of the process. [sent-602, score-0.192]
</p><p>76 2689  G ARC´A AND H ERRERA I  Figure 4 shows the results of this study considering the pairwise comparison between C4. [sent-609, score-0.144]
</p><p>77 It gives an approximation of the power of the statistical procedures considered in this paper. [sent-611, score-0.21]
</p><p>78 Obviously, the Bergmann-Hommel procedure is the most powerful, followed by Shaffer’s static procedure. [sent-614, score-0.163]
</p><p>79 The graphic also informs us about the use of logically related hypothesis, given that the procedures that use this information have a bias towards the same point and those which do not use this information, tend to a lower point than the ﬁrst. [sent-615, score-0.254]
</p><p>80 Again, the beneﬁt of using more sophisticated testing procedures is easily noticeable. [sent-619, score-0.161]
</p><p>81 3  700 average p-value  rejected hypotheses  Holm  0. [sent-621, score-0.37]
</p><p>82 05  100 0  0  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20  0  1  k  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 k  (a) Number of hypotheses rejected in pairwise comparisons  (b) Average APV in pairwise comparisons  Figure 4: C4. [sent-627, score-0.934]
</p><p>83 1-NN Figure 5 shows the results of this study considering all possible pairwise comparisons in the set of classiﬁers. [sent-629, score-0.282]
</p><p>84 Figure 5(a) presents the number of times they rejected any comparison belonging to the family. [sent-631, score-0.118]
</p><p>85 In Figure 5(b) we illustrate the average APV for all the comparisons of classiﬁers. [sent-634, score-0.166]
</p><p>86 5  average p-value  3000 rejected hypotheses  Holm  0. [sent-641, score-0.37]
</p><p>87 Moreover, the beneﬁt of using information about logically related hypothesis is noticeable, thus we strongly encourage the use of this procedure. [sent-647, score-0.112]
</p><p>88 Demˇar does not deal in depth with some s s topics related to multiple comparisons involving all the algorithms and computations of adjusted p-values. [sent-654, score-0.304]
</p><p>89 In this paper, we describe other advanced testing procedures for conducting all pairwise comparisons in a multiple comparisons analysis: Shaffer’s static and Bergmann-Hommel’s procedures. [sent-655, score-0.815]
</p><p>90 The advantage that they obtain is produced due to the incorporation of more information about the hypotheses to be tested: in n × n comparisons, a logical relationship among them exists. [sent-656, score-0.315]
</p><p>91 As a general rule, the Bergmann-Hommel procedure is the most powerful one but it requires intensive computation in comparisons involving numerous classiﬁers. [sent-657, score-0.257]
</p><p>92 Moreover, we present the methods for obtaining the adjusted p-values, which are valid p-values associated to each comparison useful to be compared with any level of signiﬁcance without restrictions and they also provide more information. [sent-659, score-0.126]
</p><p>93 Source Code of the Procedures The source code, written in JAVA, that implements all the procedures described in this paper, is available at http://sci2s. [sent-668, score-0.161]
</p><p>94 Improvements of general multiple test procedures for redundant systems of hypotheses. [sent-719, score-0.239]
</p><p>95 A sharper bonferroni procedure for multiple tests of signiﬁcance. [sent-758, score-0.223]
</p><p>96 Extensions of multiple testing procedures based on Simes’ test. [sent-763, score-0.205]
</p><p>97 A rapid algorithm and a computer program for multiple test procedures using procedures using logical structures of hypotheses. [sent-776, score-0.435]
</p><p>98 Maximizing the area under the ROC curve by pairwise feature combination. [sent-796, score-0.116]
</p><p>99 A sequentially rejective test procedure based on a modiﬁed bonferroni inequality. [sent-835, score-0.218]
</p><p>100 An improved Bonferroni procedure for multiple tests of signiﬁcance. [sent-854, score-0.147]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('holm', 0.356), ('nemenyi', 0.326), ('shaffer', 0.297), ('hypotheses', 0.28), ('dem', 0.227), ('hommel', 0.223), ('apvi', 0.208), ('comparisons', 0.166), ('procedures', 0.161), ('ar', 0.145), ('errera', 0.134), ('hochberg', 0.134), ('naivebayes', 0.126), ('ers', 0.122), ('pairwise', 0.116), ('static', 0.114), ('apv', 0.113), ('apvs', 0.104), ('bergmann', 0.104), ('exhaustive', 0.103), ('xtension', 0.101), ('arc', 0.093), ('ets', 0.09), ('omparisons', 0.09), ('rejected', 0.09), ('tatistical', 0.09), ('divisions', 0.089), ('garc', 0.089), ('rejects', 0.088), ('lassifiers', 0.083), ('vs', 0.081), ('bonferroni', 0.076), ('ultiple', 0.072), ('adjusted', 0.068), ('logically', 0.068), ('classi', 0.066), ('obtainexhaustive', 0.059), ('rejective', 0.059), ('tests', 0.054), ('rom', 0.05), ('pi', 0.05), ('power', 0.049), ('procedure', 0.049), ('advanced', 0.048), ('cmin', 0.045), ('hm', 0.045), ('sheskin', 0.045), ('simes', 0.045), ('friedman', 0.044), ('hypothesis', 0.044), ('multiple', 0.044), ('powerful', 0.042), ('logical', 0.035), ('ci', 0.035), ('test', 0.034), ('adjustment', 0.034), ('family', 0.031), ('sh', 0.031), ('obtaining', 0.03), ('acceptance', 0.03), ('alcal', 0.03), ('ban', 0.03), ('decsai', 0.03), ('dermatology', 0.03), ('discretizer', 0.03), ('esmeir', 0.03), ('granada', 0.03), ('herrera', 0.03), ('iman', 0.03), ('keel', 0.03), ('korb', 0.03), ('marrocco', 0.03), ('mushrooms', 0.03), ('salvador', 0.03), ('ugr', 0.03), ('vowel', 0.03), ('westfall', 0.03), ('cance', 0.03), ('differences', 0.03), ('fayyad', 0.029), ('comparison', 0.028), ('control', 0.028), ('guidelines', 0.027), ('yang', 0.027), ('comparing', 0.026), ('topics', 0.026), ('rejection', 0.026), ('abalone', 0.025), ('adjusts', 0.025), ('adult', 0.025), ('asuncion', 0.025), ('bupa', 0.025), ('cleveland', 0.025), ('graphic', 0.025), ('imbalanced', 0.025), ('irani', 0.025), ('murray', 0.025), ('spambase', 0.025), ('wine', 0.025), ('kernel', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="14-tfidf-1" href="./jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>Author: Salvador García, Francisco Herrera</p><p>Abstract: In a recently published paper in JMLR, Demˇar (2006) recommends a set of non-parametric stas tistical tests and procedures which can be safely used for comparing the performance of classiﬁers over multiple data sets. After studying the paper, we realize that the paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method. However, it does not deal with some advanced topics in depth. Regarding these topics, we focus on more powerful proposals of statistical procedures for comparing n × n classiﬁers. Moreover, we illustrate an easy way of obtaining adjusted and comparable p-values in multiple comparison procedures. Keywords: statistical methods, non-parametric test, multiple comparisons tests, adjusted p-values, logically related hypotheses</p><p>2 0.062363684 <a title="14-tfidf-2" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>Author: Hsuan-Tien Lin, Ling Li</p><p>Abstract: Ensemble learning algorithms such as boosting can achieve better performance by averaging over the predictions of some base hypotheses. Nevertheless, most existing algorithms are limited to combining only a ﬁnite number of hypotheses, and the generated ensemble is usually sparse. Thus, it is not clear whether we should construct an ensemble classiﬁer with a larger or even an inﬁnite number of hypotheses. In addition, constructing an inﬁnite ensemble itself is a challenging task. In this paper, we formulate an inﬁnite ensemble learning framework based on the support vector machine (SVM). The framework can output an inﬁnite and nonsparse ensemble through embedding inﬁnitely many hypotheses into an SVM kernel. We use the framework to derive two novel kernels, the stump kernel and the perceptron kernel. The stump kernel embodies inﬁnitely many decision stumps, and the perceptron kernel embodies inﬁnitely many perceptrons. We also show that the Laplacian radial basis function kernel embodies inﬁnitely many decision trees, and can thus be explained through inﬁnite ensemble learning. Experimental results show that SVM with these kernels is superior to boosting with the same base hypothesis set. In addition, SVM with the stump kernel or the perceptron kernel performs similarly to SVM with the Gaussian radial basis function kernel, but enjoys the beneﬁt of faster parameter selection. These properties make the novel kernels favorable choices in practice. Keywords: ensemble learning, boosting, support vector machine, kernel</p><p>3 0.05218336 <a title="14-tfidf-3" href="./jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">80 jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>Author: Tzu-Kuo Huang, Chih-Jen Lin, Ruby C. Weng</p><p>Abstract: This paper proposes new approaches to rank individuals from their group comparison results. Many real-world problems are of this type. For example, ranking players from team comparisons is important in some sports. In machine learning, a closely related application is classiﬁcation using coding matrices. Group comparison results are usually in two types: binary indicator outcomes (wins/losses) or measured outcomes (scores). For each type of results, we propose new models for estimating individuals’ abilities, and hence a ranking of individuals. The estimation is carried out by solving convex minimization problems, for which we develop easy and efﬁcient solution procedures. Experiments on real bridge records and multi-class classiﬁcation demonstrate the viability of the proposed models. Keywords: ranking, group comparison, binary/scored outcomes, Bradley-Terry model, multiclass classiﬁcation</p><p>4 0.045945376 <a title="14-tfidf-4" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>Author: Bo Jiang, Xuegong Zhang, Tianxi Cai</p><p>Abstract: Support vector machine (SVM) is one of the most popular and promising classiﬁcation algorithms. After a classiﬁcation rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples. Keywords: k-fold cross-validation, model evaluation, perturbation-resampling, prediction errors, support vector machine</p><p>5 0.036755372 <a title="14-tfidf-5" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>Author: Eyal Krupka, Amir Navot, Naftali Tishby</p><p>Abstract: Feature selection is the task of choosing a small subset of features that is sufﬁcient to predict the target labels well. Here, instead of trying to directly determine which features are better, we attempt to learn the properties of good features. For this purpose we assume that each feature is represented by a set of properties, referred to as meta-features. This approach enables prediction of the quality of features without measuring their value on the training instances. We use this ability to devise new selection algorithms that can efﬁciently search for new good features in the presence of a huge number of features, and to dramatically reduce the number of feature measurements needed. We demonstrate our algorithms on a handwritten digit recognition problem and a visual object category recognition problem. In addition, we show how this novel viewpoint enables derivation of better generalization bounds for the joint learning problem of selection and classiﬁcation, and how it contributes to a better understanding of the problem. Speciﬁcally, in the context of object recognition, previous works showed that it is possible to ﬁnd one set of features which ﬁts most object categories (aka a universal dictionary). Here we use our framework to analyze one such universal dictionary and ﬁnd that the quality of features in this dictionary can be predicted accurately by its meta-features. Keywords: feature selection, unobserved features, meta-features</p><p>6 0.035076961 <a title="14-tfidf-6" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>7 0.030697081 <a title="14-tfidf-7" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>8 0.030689353 <a title="14-tfidf-8" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>9 0.030043708 <a title="14-tfidf-9" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>10 0.02821132 <a title="14-tfidf-10" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>11 0.028181577 <a title="14-tfidf-11" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>12 0.02754901 <a title="14-tfidf-12" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>13 0.027060796 <a title="14-tfidf-13" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>14 0.02690988 <a title="14-tfidf-14" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>15 0.026904445 <a title="14-tfidf-15" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>16 0.026889684 <a title="14-tfidf-16" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>17 0.026514011 <a title="14-tfidf-17" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>18 0.025903005 <a title="14-tfidf-18" href="./jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">30 jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>19 0.025421135 <a title="14-tfidf-19" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>20 0.024965798 <a title="14-tfidf-20" href="./jmlr-2008-Learning_Control_Knowledge_for_Forward_Search_Planning.html">49 jmlr-2008-Learning Control Knowledge for Forward Search Planning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.129), (1, -0.031), (2, 0.091), (3, -0.029), (4, -0.038), (5, 0.005), (6, 0.035), (7, 0.003), (8, 0.022), (9, -0.002), (10, 0.013), (11, -0.001), (12, -0.019), (13, 0.021), (14, -0.017), (15, 0.018), (16, 0.13), (17, 0.011), (18, -0.016), (19, 0.008), (20, 0.02), (21, -0.125), (22, -0.208), (23, -0.113), (24, -0.067), (25, -0.101), (26, -0.101), (27, 0.17), (28, -0.079), (29, 0.427), (30, 0.067), (31, -0.081), (32, 0.272), (33, -0.04), (34, 0.087), (35, -0.197), (36, -0.077), (37, 0.048), (38, 0.11), (39, -0.142), (40, 0.182), (41, -0.192), (42, 0.099), (43, 0.347), (44, -0.18), (45, 0.046), (46, 0.024), (47, 0.126), (48, -0.071), (49, -0.206)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95648652 <a title="14-lsi-1" href="./jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>Author: Salvador García, Francisco Herrera</p><p>Abstract: In a recently published paper in JMLR, Demˇar (2006) recommends a set of non-parametric stas tistical tests and procedures which can be safely used for comparing the performance of classiﬁers over multiple data sets. After studying the paper, we realize that the paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method. However, it does not deal with some advanced topics in depth. Regarding these topics, we focus on more powerful proposals of statistical procedures for comparing n × n classiﬁers. Moreover, we illustrate an easy way of obtaining adjusted and comparable p-values in multiple comparison procedures. Keywords: statistical methods, non-parametric test, multiple comparisons tests, adjusted p-values, logically related hypotheses</p><p>2 0.26801935 <a title="14-lsi-2" href="./jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">80 jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>Author: Tzu-Kuo Huang, Chih-Jen Lin, Ruby C. Weng</p><p>Abstract: This paper proposes new approaches to rank individuals from their group comparison results. Many real-world problems are of this type. For example, ranking players from team comparisons is important in some sports. In machine learning, a closely related application is classiﬁcation using coding matrices. Group comparison results are usually in two types: binary indicator outcomes (wins/losses) or measured outcomes (scores). For each type of results, we propose new models for estimating individuals’ abilities, and hence a ranking of individuals. The estimation is carried out by solving convex minimization problems, for which we develop easy and efﬁcient solution procedures. Experiments on real bridge records and multi-class classiﬁcation demonstrate the viability of the proposed models. Keywords: ranking, group comparison, binary/scored outcomes, Bradley-Terry model, multiclass classiﬁcation</p><p>3 0.2593503 <a title="14-lsi-3" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>Author: Bo Jiang, Xuegong Zhang, Tianxi Cai</p><p>Abstract: Support vector machine (SVM) is one of the most popular and promising classiﬁcation algorithms. After a classiﬁcation rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples. Keywords: k-fold cross-validation, model evaluation, perturbation-resampling, prediction errors, support vector machine</p><p>4 0.24923928 <a title="14-lsi-4" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>Author: Hsuan-Tien Lin, Ling Li</p><p>Abstract: Ensemble learning algorithms such as boosting can achieve better performance by averaging over the predictions of some base hypotheses. Nevertheless, most existing algorithms are limited to combining only a ﬁnite number of hypotheses, and the generated ensemble is usually sparse. Thus, it is not clear whether we should construct an ensemble classiﬁer with a larger or even an inﬁnite number of hypotheses. In addition, constructing an inﬁnite ensemble itself is a challenging task. In this paper, we formulate an inﬁnite ensemble learning framework based on the support vector machine (SVM). The framework can output an inﬁnite and nonsparse ensemble through embedding inﬁnitely many hypotheses into an SVM kernel. We use the framework to derive two novel kernels, the stump kernel and the perceptron kernel. The stump kernel embodies inﬁnitely many decision stumps, and the perceptron kernel embodies inﬁnitely many perceptrons. We also show that the Laplacian radial basis function kernel embodies inﬁnitely many decision trees, and can thus be explained through inﬁnite ensemble learning. Experimental results show that SVM with these kernels is superior to boosting with the same base hypothesis set. In addition, SVM with the stump kernel or the perceptron kernel performs similarly to SVM with the Gaussian radial basis function kernel, but enjoys the beneﬁt of faster parameter selection. These properties make the novel kernels favorable choices in practice. Keywords: ensemble learning, boosting, support vector machine, kernel</p><p>5 0.24671087 <a title="14-lsi-5" href="./jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">30 jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>Author: Vojtěch Franc, Bogdan Savchynskyy</p><p>Abstract: The max-sum classiﬁer predicts n-tuple of labels from n-tuple of observable variables by maximizing a sum of quality functions deﬁned over neighbouring pairs of labels and observable variables. Predicting labels as MAP assignments of a Random Markov Field is a particular example of the max-sum classiﬁer. Learning parameters of the max-sum classiﬁer is a challenging problem because even computing the response of such classiﬁer is NP-complete in general. Estimating parameters using the Maximum Likelihood approach is feasible only for a subclass of max-sum classiﬁers with an acyclic structure of neighbouring pairs. Recently, the discriminative methods represented by the perceptron and the Support Vector Machines, originally designed for binary linear classiﬁers, have been extended for learning some subclasses of the max-sum classiﬁer. Besides the max-sum classiﬁers with the acyclic neighbouring structure, it has been shown that the discriminative learning is possible even with arbitrary neighbouring structure provided the quality functions fulﬁll some additional constraints. In this article, we extend the discriminative approach to other three classes of max-sum classiﬁers with an arbitrary neighbourhood structure. We derive learning algorithms for two subclasses of max-sum classiﬁers whose response can be computed in polynomial time: (i) the max-sum classiﬁers with supermodular quality functions and (ii) the max-sum classiﬁers whose response can be computed exactly by a linear programming relaxation. Moreover, we show that the learning problem can be approximately solved even for a general max-sum classiﬁer. Keywords: max-xum classiﬁer, hidden Markov networks, support vector machines</p><p>6 0.20515485 <a title="14-lsi-6" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>7 0.16272464 <a title="14-lsi-7" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>8 0.16175307 <a title="14-lsi-8" href="./jmlr-2008-Learning_Control_Knowledge_for_Forward_Search_Planning.html">49 jmlr-2008-Learning Control Knowledge for Forward Search Planning</a></p>
<p>9 0.15900917 <a title="14-lsi-9" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>10 0.15469113 <a title="14-lsi-10" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>11 0.14591332 <a title="14-lsi-11" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>12 0.13881984 <a title="14-lsi-12" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>13 0.13342217 <a title="14-lsi-13" href="./jmlr-2008-Linear-Time_Computation_of_Similarity_Measures_for_Sequential_Data.html">55 jmlr-2008-Linear-Time Computation of Similarity Measures for Sequential Data</a></p>
<p>14 0.13338806 <a title="14-lsi-14" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>15 0.13125832 <a title="14-lsi-15" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>16 0.12499931 <a title="14-lsi-16" href="./jmlr-2008-Stationary_Features_and_Cat_Detection.html">87 jmlr-2008-Stationary Features and Cat Detection</a></p>
<p>17 0.12179946 <a title="14-lsi-17" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>18 0.12089242 <a title="14-lsi-18" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>19 0.11830493 <a title="14-lsi-19" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>20 0.11724679 <a title="14-lsi-20" href="./jmlr-2008-Closed_Sets_for_Labeled_Data.html">22 jmlr-2008-Closed Sets for Labeled Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.017), (5, 0.017), (16, 0.011), (31, 0.017), (40, 0.038), (54, 0.026), (58, 0.043), (66, 0.055), (76, 0.029), (82, 0.481), (88, 0.076), (92, 0.034), (94, 0.048), (99, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72692549 <a title="14-lda-1" href="./jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>Author: Salvador García, Francisco Herrera</p><p>Abstract: In a recently published paper in JMLR, Demˇar (2006) recommends a set of non-parametric stas tistical tests and procedures which can be safely used for comparing the performance of classiﬁers over multiple data sets. After studying the paper, we realize that the paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method. However, it does not deal with some advanced topics in depth. Regarding these topics, we focus on more powerful proposals of statistical procedures for comparing n × n classiﬁers. Moreover, we illustrate an easy way of obtaining adjusted and comparable p-values in multiple comparison procedures. Keywords: statistical methods, non-parametric test, multiple comparisons tests, adjusted p-values, logically related hypotheses</p><p>2 0.27448988 <a title="14-lda-2" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>Author: Thomas G. Dietterich, Guohua Hao, Adam Ashenfelter</p><p>Abstract: Conditional random ﬁelds (CRFs) provide a ﬂexible and powerful model for sequence labeling problems. However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features and feature combinations. This paper describes a new algorithm for training CRFs via gradient tree boosting. In tree boosting, the CRF potential functions are represented as weighted sums of regression trees, which provide compact representations of feature interactions. So the algorithm does not explicitly consider the potentially large parameter space. As a result, gradient tree boosting scales linearly in the order of the Markov model and in the order of the feature interactions, rather than exponentially as in previous algorithms based on iterative scaling and gradient descent. Gradient tree boosting also makes it possible to use instance weighting (as in C4.5) and surrogate splitting (as in CART) to handle missing values. Experimental studies of the effectiveness of these two methods (as well as standard imputation and indicator feature methods) show that instance weighting is the best method in most cases when feature values are missing at random. Keywords: sequential supervised learning, conditional random ﬁelds, functional gradient, gradient tree boosting, missing values</p><p>3 0.2500796 <a title="14-lda-3" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>4 0.24623205 <a title="14-lda-4" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>Author: Yair Goldberg, Alon Zakai, Dan Kushnir, Ya'acov Ritov</p><p>Abstract: We analyze the performance of a class of manifold-learning algorithms that ﬁnd their output by minimizing a quadratic form under some normalization constraints. This class consists of Locally Linear Embedding (LLE), Laplacian Eigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and Diffusion maps. We present and prove conditions on the manifold that are necessary for the success of the algorithms. Both the ﬁnite sample case and the limit case are analyzed. We show that there are simple manifolds in which the necessary conditions are violated, and hence the algorithms cannot recover the underlying manifolds. Finally, we present numerical results that demonstrate our claims. Keywords: dimensionality reduction, manifold learning, Laplacian eigenmap, diffusion maps, locally linear embedding, local tangent space alignment, Hessian eigenmap</p><p>5 0.24394055 <a title="14-lda-5" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>Author: Hsuan-Tien Lin, Ling Li</p><p>Abstract: Ensemble learning algorithms such as boosting can achieve better performance by averaging over the predictions of some base hypotheses. Nevertheless, most existing algorithms are limited to combining only a ﬁnite number of hypotheses, and the generated ensemble is usually sparse. Thus, it is not clear whether we should construct an ensemble classiﬁer with a larger or even an inﬁnite number of hypotheses. In addition, constructing an inﬁnite ensemble itself is a challenging task. In this paper, we formulate an inﬁnite ensemble learning framework based on the support vector machine (SVM). The framework can output an inﬁnite and nonsparse ensemble through embedding inﬁnitely many hypotheses into an SVM kernel. We use the framework to derive two novel kernels, the stump kernel and the perceptron kernel. The stump kernel embodies inﬁnitely many decision stumps, and the perceptron kernel embodies inﬁnitely many perceptrons. We also show that the Laplacian radial basis function kernel embodies inﬁnitely many decision trees, and can thus be explained through inﬁnite ensemble learning. Experimental results show that SVM with these kernels is superior to boosting with the same base hypothesis set. In addition, SVM with the stump kernel or the perceptron kernel performs similarly to SVM with the Gaussian radial basis function kernel, but enjoys the beneﬁt of faster parameter selection. These properties make the novel kernels favorable choices in practice. Keywords: ensemble learning, boosting, support vector machine, kernel</p><p>6 0.24332039 <a title="14-lda-6" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>7 0.24093133 <a title="14-lda-7" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>8 0.23960333 <a title="14-lda-8" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>9 0.23940279 <a title="14-lda-9" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>10 0.23879878 <a title="14-lda-10" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>11 0.23857734 <a title="14-lda-11" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>12 0.23705243 <a title="14-lda-12" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>13 0.23622595 <a title="14-lda-13" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>14 0.23569137 <a title="14-lda-14" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>15 0.23550716 <a title="14-lda-15" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>16 0.23514938 <a title="14-lda-16" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>17 0.23505516 <a title="14-lda-17" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>18 0.2327373 <a title="14-lda-18" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>19 0.23174644 <a title="14-lda-19" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>20 0.23167238 <a title="14-lda-20" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
