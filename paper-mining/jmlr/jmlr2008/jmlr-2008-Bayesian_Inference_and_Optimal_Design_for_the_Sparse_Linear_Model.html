<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-18" href="#">jmlr2008-18</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</h1>
<br/><p>Source: <a title="jmlr-2008-18-pdf" href="http://jmlr.org/papers/volume9/seeger08a/seeger08a.pdf">pdf</a></p><p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>Reference: <a title="jmlr-2008-18-reference" href="../jmlr2008_reference/jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. [sent-10, score-0.26]
</p><p>2 We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. [sent-11, score-0.423]
</p><p>3 Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing  1. [sent-16, score-0.487]
</p><p>4 We also address the problem of sparse linear coding of natural images, optimising the codebook by empirical Bayesian marginal likelihood maximisation. [sent-78, score-0.336]
</p><p>5 Optimal design is discussed in Section 4, and an approximation to the marginal likelihood is given in Section 5. [sent-92, score-0.274]
</p><p>6 We also introduce the applications of interest in our work here: identiﬁcation of gene networks, and sparse coding of natural images, and we give remarks about applications to compressive sensing, which are subject to work in progress (Seeger and Nickisch, 2008). [sent-101, score-0.377]
</p><p>7 A sparsity prior (or regulariser) on the coefﬁcients, for example in the sparse linear model (1) with Laplace prior (2), leads to just that. [sent-111, score-0.294]
</p><p>8 The posterior for the “very sparse” prior shows shrinkage towards the axes even more strongly, and in terms of enforcing sparsity, this prior is preferable to the Laplacian. [sent-134, score-0.268]
</p><p>9 , 1999; Peeters and Westra, 2004), the mode a of the posterior P(a|X , u) is found through ˆ convex optimisation (recall that the log posterior is concave), and a is treated as posterior estimate ˆ of a. [sent-164, score-0.597]
</p><p>10 In contrast, in the Bayesian case, the posterior mass of all exactly sparse a (at least one component exactly zero) is zero, because the posterior has a density w. [sent-167, score-0.397]
</p><p>11 The Bayesian approach via the sparse linear model (1) has been suggested by Lewicki and Olshausen (1999), where the average coding cost of images under the model is put forward as criterion for ranking different code matrices X . [sent-224, score-0.261]
</p><p>12 In a Bayesian nomenclature, the averageRcoding cost is the negative log marginal likelihood − log P(D), where P(D) = ∏ j P(u j ), P(u j ) = P(u j |a j )P(a j ) da j , and differences of these for different X are log Bayes factors. [sent-226, score-0.345]
</p><p>13 An approximate Bayesian variant of compressive sensing has been proposed by Ji and Carin (2007), using sparse Bayesian learning (Tipping, 2001) to approximate the inference. [sent-261, score-0.406]
</p><p>14 Furthermore, the sparsity of a is encoded via a Laplace prior, motivating the sparse linear model for compressive sensing. [sent-269, score-0.319]
</p><p>15 Another point in which our approach differs from much of the existing work on compressive sensing, has to do with the sparsity prior we employ. [sent-278, score-0.281]
</p><p>16 Our sparsity prior concentrates on the bi-separation characteristic, without enforcing exact sparseness, thus may be better suited to many compressive sensing applications than the requirement of exact sparsity. [sent-283, score-0.381]
</p><p>17 Since the likelihood is a Gaussian function of a, the true log posterior log P(a|D) is concave as well, thus has a single mode only. [sent-295, score-0.365]
</p><p>18 If P(0) (a) := N(u|X a, σ2 I) is the Gaussian likelihood (1), the true posterior is P(a|D) ∝ P(0) (a) ∏ ti (ai ), i  ˜ τ τ ti (ai ) = e−˜ |ai | . [sent-296, score-0.417]
</p><p>19 In order to motivate EP, note that an optimal Gaussian posterior approximation Q(a) (at least in our context here) would be obtained by setting its mean and covariance to the true posterior 768  BAYESIAN I NFERENCE FOR S PARSE L INEAR M ODEL  statistics. [sent-299, score-0.335]
</p><p>20 However, we are able to compute one-dimensional integrals involving a single non-Gaussian site ti (ai ). [sent-301, score-0.285]
</p><p>21 The EP posterior approximation has the form ˜ Q(a) ∝ P(0) (a) ∏ ti (ai ), i  ˜ where ti (ai |bi , πi ) are Gaussian factors. [sent-303, score-0.401]
</p><p>22 We will see that a good representation has to allow for the rapid “random-access” extraction of marginals Q(ai ), and we have to be able to efﬁciently and robustly update it after a ˆ change of bi , πi . [sent-313, score-0.262]
</p><p>23 The site approximations are ti (ai ) = NU (ai |σ−2 bi , σ−2 πi ), so that Q is a Gaussian. [sent-318, score-0.355]
</p><p>24 Also, a careful reader may have noted that we are only concerned general site approximations t ˆ about marginal distributions Q(ai ) and Pi (ai ) during an EP update at ti . [sent-323, score-0.469]
</p><p>25 ˜ It is important to note that EP is not merely a local approximation, in that ti is somehow ﬁtted to 11 because posterior mean and covariance are shaped jointly ti locally. [sent-339, score-0.366]
</p><p>26 Loosely speaking, the likelihood couples coefﬁcients ai , so that the intentions of the prior factors ti (ai ), namely to force their respective arguments towards zero, have to be weighted against each other in a very non-local procedure. [sent-341, score-0.358]
</p><p>27 In fact, non-locality is a central aspect of Bayesian inference which makes it so hard to compute, and inference is particularly hard to do in models where strong long-range posterior dependencies are present which cannot easily be predicted from local interactions only. [sent-343, score-0.284]
</p><p>28 Finally, would it not be much simpler and more efﬁcient to locate the true posterior mode through convex optimisation (recall that the posterior is log-concave), then do a Laplace approximation there, which amounts to expanding the log posterior density to second order around the mode? [sent-344, score-0.632]
</p><p>29 Therefore, an EP only, P ˜ update automatically results in the site approximation ti being a (Gaussian) function of ai only. [sent-359, score-0.539]
</p><p>30 An EP update is local, in that its input is a marginal Q(ai ) and it affects single site parameters bi , πi only. [sent-365, score-0.431]
</p><p>31 However, we are really interested in the marginal means and variances, which in some cases are only weakly dependent on certain site parameters. [sent-395, score-0.282]
</p><p>32 Let d(a, b) := |a − b|/ max{|a|, |b|, 10 −3 } and ∆i = max{d(hi , hi ), √ σd( ρi , ρi )}, where Q(ui ) = N(hi , σ2 ρi ) and Q (ui ) = N(hi , σ2 ρi ) are the posterior marginals before and after an update at site i. [sent-397, score-0.483]
</p><p>33 2 Posterior Representation In this section, we develop a representation of the posterior approximation Q(a) = N(h, σ 2 Σ) which allows efﬁcient access to entries of h, diag Σ (marginal moments), and which can be updated robustly and efﬁciently for single site parameter changes (after EP updates). [sent-400, score-0.46]
</p><p>34 After an EP update bi → bi , πi → πi , we have that L (L )T = LLT + (πi − πi )δi δT , i  L γ = Lγ + (bi − bi )δi . [sent-411, score-0.289]
</p><p>35 After an EP update bi → bi , πi → πi , the representation is modiﬁed as (0) (0) follows. [sent-427, score-0.264]
</p><p>36 3 The EP Update An EP update works by matching moments between a tilted and the new posterior distribution. [sent-441, score-0.256]
</p><p>37 For an update at site i, we require the marginal Q(ai ) = N(hi , σ2 ρi ) only, which is obtained from the Q representation. [sent-442, score-0.361]
</p><p>38 While we cannot offer a ﬁrm explanation for this yet, our intuition is that the effect of Laplace prior sites on the posterior is much stronger, trying to emulate the essentially discrete feature selection process in a “unimodal” manner. [sent-459, score-0.319]
</p><p>39 In the gene network identiﬁcation application, we ran into problems of numerical instability coming from the combination of Laplace sites with very underdetermined coupling factors P(0) . [sent-461, score-0.255]
</p><p>40 This is ˜ because we divide through the site approximation ti (ai ), whose πi ≥ κ keeps the variance small. [sent-483, score-0.32]
</p><p>41 We then try to do an EP update based on a very wide cavity distribution Q\i (ai ) and a quite narrow site ti (ai ) (enforcing a strong sparsity constraint requires a rather large τ). [sent-485, score-0.443]
</p><p>42 The only difference to standard EP is that we tie the parameters of the corresponding fractional site approximation replicas. [sent-490, score-0.287]
</p><p>43 We choose the new site ˆ parameters bi , πi such that the moments of Pi and Q match. [sent-495, score-0.274]
</p><p>44 Fractional updates are easily implemented for sites t (a |τ) ˜ ˜ ∝ Q ˜i i i i i with some hyperparameter τ, such that ti (ai |τ)η = ti (ai |ητ). [sent-500, score-0.391]
</p><p>45 Finally, the site parameters are updated as ˜ bi = (1 − η)bi + bi ,  ˜ πi = (1 − η)πi + πi ,  ˆ upon which Pi and the new Q have the same moments. [sent-508, score-0.343]
</p><p>46 There, the full standard EP update is computed, but the site parameters are updated to a convex combination of old and proposed new values. [sent-510, score-0.282]
</p><p>47 Theorem 1 Let EP be applied to a model with true posterior of the form P(a|D) ∝ P(0) (a) ∏ ti (ai ), i  where P(0) (a) is a joint unnormalised Gaussian factor, and the sites ti (ai ) are log-concave. [sent-554, score-0.476]
</p><p>48 The theorem holds just as well for general sites t i (a) ˜ with corresponding site approximations ti (a) = NU (σ−2 bi , σ−2 Πi ), if “πi ≥ 0” is replaced by “Πi positive semideﬁnite”. [sent-559, score-0.465]
</p><p>49 Here, the posterior is a product of independent factors for the rows of A, so that for given (x ∗ , u∗ ), the information gain is the sum of D[Q Q] over the posterior factors, where (x∗ , u∗, j ) is appended to D for the j-th factor. [sent-635, score-0.329]
</p><p>50 However, inference schemes such as expectation propagation (and other variational ones) are designed to approximate the posterior marginals well. [sent-673, score-0.404]
</p><p>51 (5)  i=1  ˜ Recall that in EP, the sites ti are replaced by approximations ti of Gaussian form. [sent-703, score-0.326]
</p><p>52 Earlier on, we did not bother with the normalisation constants of these approximations, but now we have to make them ˜ ˜ explicit: ti (ai ) → Citi (ai ), ti (ai ) = NU (ai |σ−2 bi , σ−2 πi ). [sent-704, score-0.286]
</p><p>53 Furthermore, if α is a parameter of the site ti independent of P(0) , then ∂L ∂ log Zi ∂ = = EPi logti (ai ) . [sent-715, score-0.348]
</p><p>54 It is well known that ∇θ(0) log P(u) = EP(a|D) [∇θ(0) log P(0) (a)], from which (7) is obtained by replacing the true posterior by the EP approximation Q(a): the true gradient of the approximate criterion is the approximate gradient of the true criterion. [sent-719, score-0.377]
</p><p>55 The dominant computation within our framework, both for experimental design and marginal likelihood maximisation, is spent performing a sweep of EP updates. [sent-739, score-0.272]
</p><p>56 For each site i, the marginal Q(a i ) has to be determined, and the representation for doing so has to be updated afterwards. [sent-741, score-0.353]
</p><p>57 A key problem is how to efﬁciently detect the sites whose update would change the current posterior the most. [sent-744, score-0.339]
</p><p>58 For an update at site i, we require Q(ai ) = N(hi , σ2 ρi ). [sent-773, score-0.256]
</p><p>59 For example, in the context of experimental design, it is not affordable to update each site after each new data point inclusion. [sent-787, score-0.256]
</p><p>60 It is reasonable to assume that a small impact of an EP update on the marginal Q(ai ) implies that the whole posterior Q changes little, so site i need not be updated at the moment. [sent-793, score-0.537]
</p><p>61 In order to direct EP updates towards sites with maximum marginal impact, it is necessary to keep all marginals Q(ai ) up-to-date at all times. [sent-794, score-0.321]
</p><p>62 Experiments In this section, we present experiments for gene regulatory network identiﬁcation, for sparse coding of natural images, and for compressive sensing. [sent-843, score-0.441]
</p><p>63 1 Regulatory Network Identiﬁcation Our application of experimental design to gene network identiﬁcation, using the sparse linear model, has been described in Section 2. [sent-845, score-0.26]
</p><p>64 In contrast to this, we follow the hypothesis of Lewicki and Olshausen (1999) and learn X by maximising the EP approximation to the log marginal likelihood log P(D|X ) (see Section 5). [sent-968, score-0.317]
</p><p>65 Let φ := − ∑ j∈B L j be the EP approximation to this criterion, evaluated over a batch of size |B| (here, L j is the EP log marginal likelihood approximation on image j). [sent-978, score-0.323]
</p><p>66 It is easy to see that the gradient of the exact log marginal likelihood is ∇X log P(u j |X ) = σ−2 EP(a j |u j ,X ) [e j aT ] = σ−2 (u − X E[a j ])E[a j ]T − X Cov[a j ] , j where e j := u j − X a j . [sent-989, score-0.282]
</p><p>67 In other words, the posterior mean E[a j ] is replaced with the mode, and the second term depending on the posterior covariance Cov[a j ] is neglected altogether. [sent-991, score-0.3]
</p><p>68 Since the Laplace sparsity prior leads to a posterior which is signiﬁcantly skewed (towards coordinate axes, see Figure 1), mean and mode tend to be quite different. [sent-992, score-0.326]
</p><p>69 28 In EP, the posterior expectations are replaced by EQ [·], where Q = N(h, σ2 Σ) is the EP posterior approximation (see Appendix C). [sent-993, score-0.335]
</p><p>70 80  Table 1: EP negative log marginal likelihood (EP average coding cost) per image, evaluated on the full test set (50000 cases), for different methods after 10000 batch updates of learning X . [sent-1057, score-0.367]
</p><p>71 We compare methods in general by evaluating the EP negative log marginal likelihood approximation on the test set, normalised by the number of images. [sent-1066, score-0.254]
</p><p>72 learning curves along 10000 batch updates are shown in Figure 3 (using the test subset), and ﬁnal EP negative log marginal likelihoods per image on the full test are given in Table 1. [sent-1077, score-0.267]
</p><p>73 While X could still be learned with τ = 1, the test set log marginal likelihood evaluations for these codes could not be computed for many patches (using EP with η = 1). [sent-1106, score-0.27]
</p><p>74 80  Table 2: EP negative log marginal likelihood (EP average coding cost) per image, evaluated on the full test set (50000 cases), where X has been learned by EP. [sent-1189, score-0.302]
</p><p>75 Apart from learning codes with EP, we can also use the log marginal likelihood approximation of EP in order to compare codes obtained by other methods. [sent-1221, score-0.356]
</p><p>76 The latter is certainly signiﬁcantly faster than any of the other methods here, but its suboptimal performance on the same ﬁxed data, and more importantly the lack of an experimental design framework, clearly motivates considering approximate Bayesian inference for compressive sensing as well. [sent-1286, score-0.426]
</p><p>77 Before we start, we remind the reader that by “approximate inference” method, we mean a technique which delivers a useful approximation of marginal (or even joint) posteriors, and ideally can also be used in order to approximate the marginal likelihood. [sent-1298, score-0.278]
</p><p>78 ARD works by placing a prior N(a i |0, σ2 π−1 ) on ai , where πi is a i scale parameter, then maximising the marginal likelihood P(u, π) w. [sent-1306, score-0.355]
</p><p>79 Variational Mean Field Bayes A direct approach for obtaining an easily computable lower bound on the log marginal likelihood log P(u) works by lower-bounding the sites ti (ai ) by terms of Gaussian form. [sent-1377, score-0.5]
</p><p>80 (2006) give the precise relationship between SBL and direct site bounding (called “integral case” and “convex case” there), showing that if ti admits a scale mixture decomposition, it can also be bounded via Legendre duality. [sent-1399, score-0.285]
</p><p>81 797  S EEGER  Note that for the same (β, τ), πi is smaller for the SBL update than for the direct site bounding one. [sent-1400, score-0.256]
</p><p>82 A comparison between approximate inference techniques would be incomplete without including variational mean ﬁeld Bayes (VMFB) (Attias, 2000; Ghahramani and Beal, 2001), maybe the most well known variational technique in the moment. [sent-1407, score-0.266]
</p><p>83 org), although we understand this term as encompassing other variational methods for Bayesian inference as well, such as EP, SBL, direct site bounding, and others more. [sent-1410, score-0.327]
</p><p>84 VMFB for the sparse linear model is equivalent to direct site bounding, as has been shown in Palmer et al. [sent-1413, score-0.274]
</p><p>85 Another drawback of MCMC is that while samples of the posterior are obtained, these cannot be used in a simple way in order to obtain a good estimate of the log marginal likelihood log P(u) (see Section 5). [sent-1452, score-0.432]
</p><p>86 The optimal design capability has been demonstrated for the application of gene regulatory network identiﬁcation, where the sparsity prior was found to be essential in order to realise very signiﬁcant gains. [sent-1459, score-0.339]
</p><p>87 They address the sparse image coding problem with the sparse linear model, but do not consider optimal design applications. [sent-1496, score-0.394]
</p><p>88 On the other hand, they do not employ the natural EP marginal likelihood approximation we use here, but rather a variational bound. [sent-1501, score-0.274]
</p><p>89 In this context, the sparse linear model has been proposed as a useful setup, in which codes can be learned by maximising the marginal likelihood. [sent-1508, score-0.253]
</p><p>90 3 indicate that approximate Bayesian inference and experimental design hold signiﬁcant promises for compressive sensing, where so far approaches based on L 1 penalised estimation and random designs seem to predominate. [sent-1517, score-0.326]
</p><p>91 Our experiences with the sparse linear model on the image coding problem (or with very underdetermined gene network identiﬁcation settings) suggest that in some relevant cases, numerical stability issues seem to be inherently present in EP (i. [sent-1521, score-0.359]
</p><p>92 The preliminary experiments with compressive sensing have been done in joint work with Hannes Nickisch. [sent-1561, score-0.275]
</p><p>93 802  BAYESIAN I NFERENCE FOR S PARSE L INEAR M ODEL  ˜ ˜ The Laplace site is ti (a) = exp(−˜ |a|), τ = τ/σ > 0. [sent-1568, score-0.285]
</p><p>94 40 We need to compute moments Ik = EN(h,ρ) [ak ti (a)], k = 0, 1, 2, where we write a = ai , h = h\i , ρ = ρ\i ˜ ˜ ˜ for simplicity. [sent-1570, score-0.275]
</p><p>95 Note that Zi = I0 = EQ\i [ti (ai )] is not required for the EP update itself, but has to be evaluated if an approximation to the marginal likelihood P(D) is ˜ sought (see Section 5; recall that Zi as computed here has to be multiplied with the prefactor τ/2 of ti which we omitted). [sent-1606, score-0.378]
</p><p>96 For an update at site i, we can assume e \i (a ) is a proper Gaussian. [sent-1611, score-0.256]
</p><p>97 Namely, log Q\i is jointly concave in (ai , h\i ) (being a negative quadratic in ai − h\i ), so that ti (ai )Q\i (ai |h\i ) is log-concave in (ai , h\i ). [sent-1613, score-0.311]
</p><p>98 Namely, what is referred to as site parameters in Seeger (2005), are in fact the σ−2 bi , σ−2 πi here, not bi , πi . [sent-1683, score-0.317]
</p><p>99 One could possibly choose another representation of the Laplace sites, whence the equivalence of VMFB and direct site bounding would not hold, but this is not done here. [sent-1707, score-0.254]
</p><p>100 Experimental design for efﬁcient identiﬁcation of gene regulatory networks using sparse Bayesian models. [sent-2199, score-0.272]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ep', 0.651), ('site', 0.177), ('olshausen', 0.176), ('laplace', 0.172), ('sbl', 0.154), ('posterior', 0.15), ('compressive', 0.143), ('ai', 0.14), ('bayesian', 0.127), ('eeger', 0.126), ('nference', 0.126), ('sites', 0.11), ('lewicki', 0.109), ('ti', 0.108), ('marginal', 0.105), ('sensing', 0.1), ('sparse', 0.097), ('inear', 0.085), ('parse', 0.085), ('design', 0.083), ('coding', 0.083), ('variational', 0.083), ('sparsity', 0.079), ('update', 0.079), ('seeger', 0.076), ('fractional', 0.075), ('odel', 0.074), ('eq', 0.071), ('bi', 0.07), ('inference', 0.067), ('underdetermined', 0.065), ('updates', 0.065), ('log', 0.063), ('prior', 0.059), ('gaussian', 0.058), ('field', 0.057), ('code', 0.055), ('gene', 0.054), ('tipping', 0.051), ('likelihood', 0.051), ('codes', 0.051), ('mcmc', 0.05), ('rvm', 0.05), ('priors', 0.049), ('degenerate', 0.048), ('palmer', 0.047), ('opper', 0.046), ('optimisation', 0.046), ('representation', 0.045), ('casella', 0.044), ('steinke', 0.044), ('vmfb', 0.044), ('quadrature', 0.042), ('mvms', 0.042), ('winther', 0.042), ('marginals', 0.041), ('maximisation', 0.039), ('wipf', 0.039), ('woodbury', 0.039), ('regulatory', 0.038), ('mode', 0.038), ('hi', 0.036), ('hyperparameters', 0.036), ('approximation', 0.035), ('lters', 0.034), ('minka', 0.034), ('image', 0.034), ('pi', 0.033), ('sweep', 0.033), ('approximate', 0.033), ('llt', 0.033), ('optimised', 0.033), ('cholesky', 0.033), ('mvm', 0.033), ('zi', 0.032), ('done', 0.032), ('propagation', 0.03), ('student', 0.03), ('sequential', 0.03), ('gain', 0.029), ('nu', 0.029), ('park', 0.028), ('carin', 0.028), ('girolami', 0.028), ('nickisch', 0.028), ('candidates', 0.028), ('marginalisation', 0.028), ('renormalisation', 0.028), ('inclusion', 0.027), ('ndings', 0.027), ('robustly', 0.027), ('moments', 0.027), ('updated', 0.026), ('images', 0.026), ('network', 0.026), ('vt', 0.026), ('xt', 0.025), ('numerically', 0.025), ('regularisation', 0.025), ('stable', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000012 <a title="18-tfidf-1" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>2 0.20990983 <a title="18-tfidf-2" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>3 0.13720095 <a title="18-tfidf-3" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>4 0.12529896 <a title="18-tfidf-4" href="./jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>Author: Andreas Christmann, Arnout Van Messem</p><p>Abstract: We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand inﬂuence function (BIF) a modiﬁcation of F.R. Hampel’s inﬂuence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel’s inﬂuence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. Keywords: Bouligand derivatives, empirical risk minimization, inﬂuence function, robustness, support vector machines</p><p>5 0.112396 <a title="18-tfidf-5" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>Author: Michiel Debruyne, Mia Hubert, Johan A.K. Suykens</p><p>Abstract: Recent results about the robustness of kernel methods involve the analysis of inﬂuence functions. By deﬁnition the inﬂuence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the inﬂuence function is used in a similar way to analyze the statistical efﬁciency of a method. Links between both worlds are explored. The inﬂuence function is related to the ﬁrst term of a Taylor expansion. Higher order inﬂuence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate inﬂuence functions at a speciﬁc sample distribution to obtain an approximation of the leave-one-out error. A speciﬁc implementation is proposed using a L1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data. Keywords: kernel based regression, robustness, stability, inﬂuence function, model selection</p><p>6 0.10751488 <a title="18-tfidf-6" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>7 0.10106616 <a title="18-tfidf-7" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>8 0.092963107 <a title="18-tfidf-8" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>9 0.091872379 <a title="18-tfidf-9" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>10 0.081841528 <a title="18-tfidf-10" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>11 0.053887513 <a title="18-tfidf-11" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>12 0.052876301 <a title="18-tfidf-12" href="./jmlr-2008-Near-Optimal_Sensor_Placements_in_Gaussian_Processes%3A_Theory%2C_Efficient_Algorithms_and_Empirical_Studies.html">67 jmlr-2008-Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies</a></p>
<p>13 0.051188257 <a title="18-tfidf-13" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>14 0.048866708 <a title="18-tfidf-14" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>15 0.048827346 <a title="18-tfidf-15" href="./jmlr-2008-On_the_Suitable_Domain_for_SVM_Training_in_Image_Coding.html">73 jmlr-2008-On the Suitable Domain for SVM Training in Image Coding</a></p>
<p>16 0.047912236 <a title="18-tfidf-16" href="./jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">80 jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>17 0.04581615 <a title="18-tfidf-17" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>18 0.045314323 <a title="18-tfidf-18" href="./jmlr-2008-Comments_on_the_Complete_Characterization_of_a_Family_of_Solutions_to_a_GeneralizedFisherCriterion.html">23 jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</a></p>
<p>19 0.045102231 <a title="18-tfidf-19" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>20 0.042916734 <a title="18-tfidf-20" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.272), (1, -0.081), (2, -0.221), (3, 0.031), (4, 0.021), (5, 0.202), (6, -0.14), (7, -0.095), (8, -0.085), (9, 0.294), (10, -0.206), (11, -0.031), (12, -0.088), (13, -0.113), (14, -0.047), (15, -0.119), (16, 0.077), (17, -0.008), (18, 0.084), (19, 0.051), (20, -0.264), (21, -0.04), (22, 0.081), (23, -0.075), (24, -0.053), (25, -0.125), (26, -0.068), (27, -0.082), (28, -0.062), (29, 0.068), (30, 0.02), (31, -0.006), (32, -0.101), (33, 0.124), (34, -0.039), (35, -0.084), (36, -0.075), (37, 0.059), (38, -0.102), (39, 0.002), (40, -0.031), (41, 0.054), (42, -0.092), (43, 0.022), (44, 0.034), (45, 0.048), (46, 0.038), (47, 0.002), (48, -0.008), (49, -0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96906817 <a title="18-lsi-1" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>2 0.60262865 <a title="18-lsi-2" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>3 0.47566625 <a title="18-lsi-3" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>Author: Michiel Debruyne, Mia Hubert, Johan A.K. Suykens</p><p>Abstract: Recent results about the robustness of kernel methods involve the analysis of inﬂuence functions. By deﬁnition the inﬂuence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the inﬂuence function is used in a similar way to analyze the statistical efﬁciency of a method. Links between both worlds are explored. The inﬂuence function is related to the ﬁrst term of a Taylor expansion. Higher order inﬂuence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate inﬂuence functions at a speciﬁc sample distribution to obtain an approximation of the leave-one-out error. A speciﬁc implementation is proposed using a L1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data. Keywords: kernel based regression, robustness, stability, inﬂuence function, model selection</p><p>4 0.43306825 <a title="18-lsi-4" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>5 0.42497891 <a title="18-lsi-5" href="./jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>Author: Andreas Christmann, Arnout Van Messem</p><p>Abstract: We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand inﬂuence function (BIF) a modiﬁcation of F.R. Hampel’s inﬂuence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel’s inﬂuence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. Keywords: Bouligand derivatives, empirical risk minimization, inﬂuence function, robustness, support vector machines</p><p>6 0.4127315 <a title="18-lsi-6" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>7 0.40969786 <a title="18-lsi-7" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>8 0.40259463 <a title="18-lsi-8" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>9 0.32313663 <a title="18-lsi-9" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>10 0.30833852 <a title="18-lsi-10" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>11 0.23420496 <a title="18-lsi-11" href="./jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">80 jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>12 0.22004759 <a title="18-lsi-12" href="./jmlr-2008-On_the_Suitable_Domain_for_SVM_Training_in_Image_Coding.html">73 jmlr-2008-On the Suitable Domain for SVM Training in Image Coding</a></p>
<p>13 0.21976161 <a title="18-lsi-13" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>14 0.20016129 <a title="18-lsi-14" href="./jmlr-2008-Comments_on_the_Complete_Characterization_of_a_Family_of_Solutions_to_a_GeneralizedFisherCriterion.html">23 jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</a></p>
<p>15 0.19707139 <a title="18-lsi-15" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>16 0.19343005 <a title="18-lsi-16" href="./jmlr-2008-Near-Optimal_Sensor_Placements_in_Gaussian_Processes%3A_Theory%2C_Efficient_Algorithms_and_Empirical_Studies.html">67 jmlr-2008-Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies</a></p>
<p>17 0.19108741 <a title="18-lsi-17" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>18 0.18690723 <a title="18-lsi-18" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>19 0.18217796 <a title="18-lsi-19" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>20 0.17436416 <a title="18-lsi-20" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.032), (1, 0.013), (5, 0.035), (9, 0.016), (31, 0.015), (40, 0.037), (53, 0.273), (54, 0.039), (58, 0.058), (66, 0.077), (72, 0.015), (76, 0.037), (88, 0.072), (92, 0.044), (94, 0.081), (99, 0.078)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79592413 <a title="18-lda-1" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>2 0.53968155 <a title="18-lda-2" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>3 0.50528085 <a title="18-lda-3" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>4 0.49206495 <a title="18-lda-4" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>Author: Andreas Krause, H. Brendan McMahan, Carlos Guestrin, Anupam Gupta</p><p>Abstract: In many applications, one has to actively select among a set of expensive observations before making an informed decision. For example, in environmental monitoring, we want to select locations to measure in order to most effectively predict spatial phenomena. Often, we want to select observations which are robust against a number of possible objective functions. Examples include minimizing the maximum posterior variance in Gaussian Process regression, robust experimental design, and sensor placement for outbreak detection. In this paper, we present the Submodular Saturation algorithm, a simple and efﬁcient algorithm with strong theoretical approximation guarantees for cases where the possible objective functions exhibit submodularity, an intuitive diminishing returns property. Moreover, we prove that better approximation algorithms do not exist unless NP-complete problems admit efﬁcient algorithms. We show how our algorithm can be extended to handle complex cost functions (incorporating non-unit observation cost or communication and path costs). We also show how the algorithm can be used to near-optimally trade off expected-case (e.g., the Mean Square Prediction Error in Gaussian Process regression) and worst-case (e.g., maximum predictive variance) performance. We show that many important machine learning problems ﬁt our robust submodular observation selection formalism, and provide extensive empirical evaluation on several real-world problems. For Gaussian Process regression, our algorithm compares favorably with state-of-the-art heuristics described in the geostatistics literature, while being simpler, faster and providing theoretical guarantees. For robust experimental design, our algorithm performs favorably compared to SDP-based algorithms. c 2008 Andreas Krause, H. Brendan McMahan, Carlos Guestrin and Anupam Gupta. K RAUSE , M C M AHAN , G UESTRIN AND G UPTA Keywords: observation selection, experimental design, active learning, submodular functions, Gaussi</p><p>5 0.48859447 <a title="18-lda-5" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>Author: Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, Peter L. Bartlett</p><p>Abstract: Log-linear and maximum-margin models are two commonly-used methods in supervised machine learning, and are frequently used in structured prediction problems. Efﬁcient learning of parameters in these models is therefore an important problem, and becomes a key factor when learning from very large data sets. This paper describes exponentiated gradient (EG) algorithms for training such models, where EG updates are applied to the convex dual of either the log-linear or maxmargin objective function; the dual in both the log-linear and max-margin cases corresponds to minimizing a convex function with simplex constraints. We study both batch and online variants of the algorithm, and provide rates of convergence for both cases. In the max-margin case, O( 1 ) EG ε updates are required to reach a given accuracy ε in the dual; in contrast, for log-linear models only O(log( 1 )) updates are required. For both the max-margin and log-linear cases, our bounds suggest ε that the online EG algorithm requires a factor of n less computation to reach a desired accuracy than the batch EG algorithm, where n is the number of training examples. Our experiments conﬁrm that the online algorithms are much faster than the batch algorithms in practice. We describe how the EG updates factor in a convenient way for structured prediction problems, allowing the algorithms to be efﬁciently applied to problems such as sequence learning or natural language parsing. We perform extensive evaluation of the algorithms, comparing them to L-BFGS and stochastic gradient descent for log-linear models, and to SVM-Struct for max-margin models. The algorithms are applied to a multi-class problem as well as to a more complex large-scale parsing task. In all these settings, the EG algorithms presented here outperform the other methods. Keywords: exponentiated gradient, log-linear models, maximum-margin models, structured prediction, conditional random ﬁelds ∗. These authors contributed equally. c 2008 Michael Col</p><p>6 0.4872874 <a title="18-lda-6" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>7 0.48715147 <a title="18-lda-7" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>8 0.47976956 <a title="18-lda-8" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>9 0.47765476 <a title="18-lda-9" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>10 0.47623542 <a title="18-lda-10" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>11 0.47605088 <a title="18-lda-11" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>12 0.47360477 <a title="18-lda-12" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>13 0.47346678 <a title="18-lda-13" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>14 0.47254521 <a title="18-lda-14" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>15 0.47048014 <a title="18-lda-15" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>16 0.47004893 <a title="18-lda-16" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>17 0.46980321 <a title="18-lda-17" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>18 0.46697229 <a title="18-lda-18" href="./jmlr-2008-Near-Optimal_Sensor_Placements_in_Gaussian_Processes%3A_Theory%2C_Efficient_Algorithms_and_Empirical_Studies.html">67 jmlr-2008-Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies</a></p>
<p>19 0.46669284 <a title="18-lda-19" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>20 0.46309105 <a title="18-lda-20" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
