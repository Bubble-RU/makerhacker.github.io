<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-27" href="#">jmlr2008-27</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</h1>
<br/><p>Source: <a title="jmlr-2008-27-pdf" href="http://jmlr.org/papers/volume9/bach08b/bach08b.pdf">pdf</a></p><p>Author: Francis R. Bach</p><p>Abstract: We consider the least-square regression problem with regularization by a block 1 -norm, that is, a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1 -norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic group selection consistency of the group Lasso. We derive necessary and sufﬁcient conditions for the consistency of group Lasso under practical assumptions, such as model misspeciﬁcation. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this inﬁnite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisﬁed. Keywords: sparsity, regularization, consistency, convex optimization, covariance operators</p><p>Reference: <a title="jmlr-2008-27-reference" href="../jmlr2008_reference/jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we study the asymptotic group selection consistency of the group Lasso. [sent-6, score-0.15]
</p><p>2 When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. [sent-8, score-0.124]
</p><p>3 In the case of a ﬁxed number of covariates, the Lasso does recover the sparsity pattern if and only if a certain simple condition on the generating covariance matrices is veriﬁed (Yuan and Lin, 2007). [sent-26, score-0.116]
</p><p>4 A related Lasso-type procedure is the group Lasso, where the covariates are assumed to be clustered in groups, and instead of summing the absolute values of each individual loading, the sum of Euclidean norms of the loadings in each group is used. [sent-30, score-0.146]
</p><p>5 Intuitively, this should drive all the weights in one group to zero together, and thus lead to group selection (Yuan and Lin, 2006). [sent-31, score-0.114]
</p><p>6 In Section 2, we extend the consistency results of the Lasso to the group Lasso, showing that similar correlation conditions are necessary and sufﬁcient conditions for consistency. [sent-32, score-0.114]
</p><p>7 The group Lasso essentially replaces groups of size one by groups of size larger than one. [sent-39, score-0.103]
</p><p>8 We extend the consistency results of the group Lasso to this nonparametric case, by using covariance operators and appropriate notions of functional analysis. [sent-47, score-0.2]
</p><p>9 In this paper, we study path consistency of the group Lasso and of multiple kernel learning, and in simulations we use the publicly available code for the algorithm of Bach et al. [sent-139, score-0.124]
</p><p>10 Note that the conditions involve the covariance between all active groups X j , j ∈ J and all non active groups Xi , i ∈ Jc . [sent-143, score-0.133]
</p><p>11 In the case of the group Lasso, even with a ﬁnite ﬁxed number of groups, our results are not as strong, as we can only get the strict condition as sufﬁcient and the weak condition as necessary. [sent-148, score-0.113]
</p><p>12 1, shows that if the condition (4) is satisﬁed, any regularization parameter that satisﬁes a certain decay conditions will lead to a consistent estimator; thus the strong condition (4) is sufﬁcient for path consistency: Theorem 2 Assume (A1-3). [sent-152, score-0.106]
</p><p>13 (1) converges in probability to w and ˆ the group sparsity pattern J(w) = { j, w j = 0} converges in probability to J (i. [sent-154, score-0.146]
</p><p>14 On the other hand, the result (and the similar one for the Lasso) is rather disappointing regarding the applicability of the group Lasso as a practical group selection method, as Theorem 3 states that if the weak correlation condition (5) is not satisﬁed, we cannot have consistency. [sent-162, score-0.148]
</p><p>15 6 Reﬁnements of Consistency Conditions Our current results state that the strict condition (4) is sufﬁcient for joint consistency of the group Lasso, while the weak condition (5) is only necessary. [sent-169, score-0.161]
</p><p>16 The main technical reason for those differences is that in dimension one, the set of vectors of unit norm is ﬁnite (two possible values), and thus regular squared norm consistency leads to estimates of the signs of the loadings (i. [sent-171, score-0.105]
</p><p>17 Assume the weak condition (5) is satisﬁed and that for all i ∈ J c such 1 that di ΣXi XJ Σ−1XJ Diag(d j / w j )wJ = 1, we have XJ ∆ ΣXJ Xi ΣXi XJ Σ−1XJ Diag d j / w j XJ  Ip j −  w jw j wj wj  ∆ > 0,  (6)  with ∆ = −Σ−1XJ Diag(d j / w j )wJ . [sent-176, score-0.81]
</p><p>18 (1) converges in probability to w and the group sparsity ˆ pattern J(w) = { j, w j = 0} converges in probability to J. [sent-178, score-0.146]
</p><p>19 The previous theorem does not contradict the fact that condition (4) is necessary for w jw j j wj  path-consistency in the Lasso case: indeed, if w j has dimension one (i. [sent-182, score-0.417]
</p><p>20 We can also further reﬁned the necessary condition results in Theorem 3: as stated in Theorem 3, the group Lasso estimator may be both consistent in terms of norm and sparsity patterns only if the condition (5) is satisﬁed. [sent-188, score-0.185]
</p><p>21 However, if we require only the consistent sparsity pattern estimation, then we may allow the convergence of the regularization parameter λ n to a strictly positive limit λ0 . [sent-189, score-0.109]
</p><p>22 w∈R 2 j=1  (7)  If there exists λ0 > 0 such that the solution has the correct sparsity pattern, then the group Lasso estimate with λn → λ0 , will have a consistent sparsity pattern. [sent-191, score-0.147]
</p><p>23 Thus, even when condition (5) is not satisﬁed, we may have consistent estimation of the sparsity pattern but inconsistent estimation of the loading vectors. [sent-195, score-0.163]
</p><p>24 In this section, we consider sufﬁcient conditions that do not depend on the loading vector, but only on the sparsity pattern J and of course on the covariance matrices. [sent-217, score-0.127]
</p><p>25 The following condition is sufﬁcient for consistency of the group Lasso, for all possible loading vectors w with sparsity pattern J: C(ΣXX , d, J) = max c i∈J  1 ΣX X Σ−1 Diag(d j )uJ < 1. [sent-218, score-0.217]
</p><p>26 This leads to the following sufﬁcient condition for consistency of the group Lasso (which extends the condition of Yuan and Lin, 2007): max c i∈J  1 di  ∑ d j ∑ ΣX X  i k  j∈J  k∈J  Σ−1XJ XJ  kj  < 1. [sent-220, score-0.216]
</p><p>27 (2004a), we can instead consider regularization by the square of the block 1 -norm: min  w∈R p , b∈R  1 ¯ ¯ Y − Xw − b1n 2n  2  1 + µn 2  2  m  ∑ dj  wj  . [sent-238, score-0.458]
</p><p>28 (1), by minimizing in closed form with respect to b, to obtain: 1 1 1ˆ ˆ ˆ minp ΣYY − ΣY X w + w ΣXX w + µn w∈R 2 2 2  2  m  ∑ dj  wj  . [sent-243, score-0.422]
</p><p>29 2): Proposition 8 A vector w ∈ R p with sparsity pattern J = { j, w j = 0} is optimal for problem (12) if and only if ∀ j ∈ Jc, ∀ j ∈ J,  ˆ ˆ ΣX j X w − ΣX j Y ˆ ˆ ΣX j X w − ΣX j Y  µn d j (∑n di wi ) , i=1 d jw j = −µn (∑n di wi ) . [sent-246, score-0.175]
</p><p>30 i=1 wj  Note the correspondence at the optimum between optimal solutions of the two optimization problems in Eq. [sent-247, score-0.352]
</p><p>31 But since the relationship between λ n ˆ and µn at optimum is λn = µn (∑n di wi ) and that ∑n di wi converges to a constant whenever i=1 i=1 w is consistent, it does apply as well with minor modiﬁcations (in particular, to deal with the case ˆ where J is empty, which requires µn = ∞). [sent-252, score-0.135]
</p><p>32 Covariance Operators and Multiple Kernel Learning We now extend the previous consistency results to the case of nonparametric estimation, where each group is a potentially inﬁnite dimensional space of functions. [sent-254, score-0.133]
</p><p>33 In this nonparametric context, covariance operators constitute appropriate tools for the statistical analysis and are becoming standard in the theoretical analysis of kernel methods (Fukumizu et al. [sent-262, score-0.126]
</p><p>34 1188  C ONSISTENCY OF THE G ROUP L ASSO AND M ULTIPLE K ERNEL L EARNING  kernel is linear, the covariance operator is exactly the covariance matrix, and the Hilbert-Schmidt norm is the Frobenius norm, while the operator norm is the maximum singular value (also referred to as the spectral norm). [sent-285, score-0.205]
</p><p>35 , m, then we can naturally deﬁne the crosscovariance operators ΣXi X j from F j to Fi such that ∀( f i , f j ) ∈ Fi × F j , fi , ΣXi X j f j Fi = cov( fi (Xi ), f j (X j )) = E( fi (Xi ) f j (X j )) − (E fi (Xi ))(E f j (X j )). [sent-318, score-0.295]
</p><p>36 We can now deﬁne a joint block covariance operator on F = F1 × · · · × Fm following the block structure of covariance matrices in Section 2. [sent-323, score-0.11]
</p><p>37 Throughout this paper we will make the assumption that those operators CXi X j are compact for i = j: compact operators can be characterized as limits of ﬁnite rank operators or as operators that can be diagonalized on a countable basis with spectrum composed of a sequence tending to zero (see, e. [sent-328, score-0.209]
</p><p>38 (2008) requires to consider consistency for norms weaker than the RKHS norms (Caponnetto and de Vito, 2005; Steinwart, 2001), and is left for future research. [sent-403, score-0.114]
</p><p>39 (12), where empirical covariance matrices are replaced by empirical covariance operators: 1 µn 1ˆ ˆ ˆ min ΣYY − f , ΣXY F + f , ΣXX f F + f ∈F 2 2 2  2  m  ∑ dj  f j Fj  . [sent-447, score-0.15]
</p><p>40 The main reason of rewriting the conditions in terms of correlation operators rather than covariance operators is that correlation operators are invertible by assumption, while covariance operators are not as soon as the Hilbert spaces have inﬁnite dimensions. [sent-473, score-0.283]
</p><p>41 The following theorems give necessary and sufﬁcient conditions for the path consistency of the nonparametric group Lasso (see proofs in Appendix C. [sent-474, score-0.121]
</p><p>42 If condition (16) is satisﬁed, then for any sequence µn such that µn → 0 and µn n1/2 → +∞, any sequence of nonparametric group Lasso estimates fˆ converges in probability to f and the sparsity pattern J( fˆ) = { j, fˆj = 0} converges in probability to J. [sent-477, score-0.199]
</p><p>43 ,m dj di2  The optimal function may then be written as f j = η j ∑n αi k j (·, xi j ). [sent-506, score-0.122]
</p><p>44 Finally, we state a corollary of Proposition 13 that shows that under our assumptions regarding the correlation operator, we have a unique solution to the nonparametric groups Lasso problem with probability tending to one (see proof in Appendix A. [sent-518, score-0.128]
</p><p>45 Adaptive Group Lasso and Multiple Kernel Learning In previous sections, we have shown that speciﬁc necessary and sufﬁcient conditions are needed for path consistency of the group Lasso and multiple kernel learning. [sent-531, score-0.124]
</p><p>46 We denote by w ˆ 1ˆ 1 µn ˆ ˆ ΣYY − ΣY X w + w ΣXX w + 2 2 2 1195  m  ∑  j=1  2  wLS −γ ˆj  wj  . [sent-539, score-0.352]
</p><p>47 The unique minimizer fˆκn of  1ˆ 1 κn ˆ ˆ ΣYY − ΣXY , f F + f , ΣXX f F + 2 2 2  m  ∑  fj 2 j, F  j=1  1/2 LS converges in probability to f if κn → 0 and κn n1/2 → 0. [sent-547, score-0.247]
</p><p>48 n  Since the least-square estimate is consistent and we have an upper bound on its convergence rate, we follow Zou (2006) and use it to deﬁned adaptive weights d j for which we get both sparsity and regular consistency without any conditions on the value of the correlation operators. [sent-549, score-0.187]
</p><p>49 1 Groups of Finite Sizes In the ﬁnite dimensional group case, we sampled X ∈ R p from a normal distribution with zero mean vector and a covariance matrix of size p = 8 for m = 4 groups of size p j = 2, j = 1, . [sent-574, score-0.129]
</p><p>50 We selected Card(J) = 2 groups at random and sampled non zero loading vectors as follows: (a) sample each loading from from independent standard normal distributions, (b) rescale those to unit norm, (c) rescale those 1 by a scaling which is uniform at random between 3 and 1. [sent-581, score-0.151]
</p><p>51 This allows 1197  BACH  consistent − adaptive (γ = 1)  consistent − non adaptive  0. [sent-599, score-0.167]
</p><p>52 2  0  0  5  10  0  15  −log(µ)  10 −log(µ)  inconsistent − non adaptive  inconsistent − adaptive (γ = 1)  0. [sent-607, score-0.177]
</p><p>53 2  0  2  4  6 8 −log(µ)  10  0  12  5  10 −log(µ)  15  inconsistent − adaptive (γ = 1)  inconsistent − non adaptive 1  0. [sent-614, score-0.177]
</p><p>54 4  4 6 −log(µ)  8  consistent − adaptive (γ=1) 2  consistent − adaptive (γ=1) 1 P(correct pattern)  0  −4  0. [sent-633, score-0.126]
</p><p>55 When it is less than one, then we get with 1199  BACH  inconsistent − non adaptive  inconsistent − non adaptive  n=101 n=102 n=103 n=104 n=105  0  0. [sent-648, score-0.218]
</p><p>56 2  −2 −4  0  2  4 6 −log(µ)  8  2  4 6 −log(µ)  8  inconsistent − adaptive (γ=1)  inconsistent − adaptive (γ=1) 0. [sent-652, score-0.136]
</p><p>57 We compare them to the population values η j : both in terms of values, j=1 1200  C ONSISTENCY OF THE G ROUP L ASSO AND M ULTIPLE K ERNEL L EARNING  inconsistent − non adaptive 2  inconsistent − non adaptive 0. [sent-683, score-0.218]
</p><p>58 2 0 0  4 6 −log(µ)  inconsistent − adaptive (γ=1) 2  inconsistent − adaptive (γ=1) 1 P(correct pattern)  0  n=101 n=102 n=103 n=104 n=105  2  4 6 −log(µ)  8  0  2  4 6 −log(µ)  8  Figure 4: Synthetic example where consistency condition in Eq. [sent-691, score-0.215]
</p><p>59 In particular, under practical assumptions regarding the distributions the data are sampled from, we have provided necessary and sufﬁcient conditions for model consistency of the group Lasso and its nonparametric version, multiple kernel learning. [sent-699, score-0.146]
</p><p>60 1201  BACH  correct sparsity, regular consistency correct sparsity, no regular consistency incorrect sparsity 1 0. [sent-700, score-0.186]
</p><p>61 4 −5  5  0  5  −1 −5  0  5  Figure 6: Functions to be estimated in the synthetic nonparametric group Lasso experiments (left: consistent case, right: inconsistent case). [sent-721, score-0.118]
</p><p>62 The current work could be extended in several ways: ﬁrst, a more detailed study of the limiting distributions of the group Lasso and adaptive group Lasso estimators could be carried and then extend the analysis of Zou (2006) or Juditsky and Nemirovski (2000) and Wu et al. [sent-722, score-0.145]
</p><p>63 1202  C ONSISTENCY OF THE G ROUP L ASSO AND M ULTIPLE K ERNEL L EARNING  consistent − adaptive (γ = 2)  consistent − non adaptive  0. [sent-728, score-0.167]
</p><p>64 2  0  0  5  10 −log(µ)  0  15  10 −log(µ)  15  inconsistent − adaptive (γ = 2)  inconsistent − non adaptive 1  0. [sent-736, score-0.177]
</p><p>65 2  0  0  5  10 −log(µ)  0  15  10  15 −log(µ)  20  Figure 7: Regularization paths for the group Lasso for two weighting schemes (left: non adaptive, right: adaptive) and two different population densities (top: strict consistency condition satisﬁed, bottom: weak condition not satisﬁed. [sent-744, score-0.214]
</p><p>66 We consider the Lagrangian with dual variables (β j , γ j ) ∈ R p j × R such that β j γ j: 1 2  m  1 2  ˆ ˆ ˆ L (w, v, β, γ) = ΣYY − ΣY X w + w ΣXX w + λn d v − ∑  j=1  wj vj  βj . [sent-754, score-0.386]
</p><p>67 Since the dual and the primal problems are strictly feasible, strong duality holds and the KKT conditions for reduced primal/dual variables (w, β) are ˆ ˆ ∀ j, β j = ΣX j X w − ΣX jY m  ∀ j, ∑ d j w j = j=1  ∀ j,  βj dj  wj + wj  (stationarity - 1) ,  βi 1 max µn i=1,. [sent-772, score-0.822]
</p><p>68 Complementary slackness for the second order cone implies that: βj dj  wj + wj  i=1,. [sent-779, score-0.787]
</p><p>69 ,m  βi = 0, di  βj dj  = max  max  if and only if, either (a) w j = 0, or (b) w j = 0 and  i=1,. [sent-782, score-0.125]
</p><p>70 ,m di j=1 α 1n = 0 ∀ j,  ¯ −X j α dj  (stationarity - 1) ,  (stationarity - 3) ,  wj + wj  (α Ki α)1/2 =0 i=1,. [sent-833, score-0.829]
</p><p>71 Complementary slackness for the second order cone goes leads to: ¯ −X j α dj  wj + wj  (α Ki α)1/2 = 0, i=1,. [sent-837, score-0.787]
</p><p>72 5 Proof of Proposition 14 What makes this proposition non obvious is the fact that the covariance operator Σ XX is not invertible in general. [sent-854, score-0.167]
</p><p>73 Thus, non unicity implies that the empirical covariance matrix of the random variables g j (X j ), j ∈ J, is non invertible. [sent-863, score-0.122]
</p><p>74 XJ Since w is consistent, and λn n1/2 → +∞, then for each i ∈ J c , ˜ 1 ˆ ˆ ΣXiY − ΣXi XJ wJ ˜ d i λn 1 converges in probability to di ΣXi XJ Σ−1XJ Diag(d j / w j )wJ which is of norm strictly smaller than XJ one because condition (4) is satisﬁed. [sent-893, score-0.151]
</p><p>75 Thus the probability that w is indeed optimal, which is equal ˜ to 1 1 ˆ ˆ ˆ ˆ 1 ΣXiY − ΣXi XJ wJ ˜ ˜ P ∀i ∈ Jc , ∏c P di λn ΣXiY − ΣXi XJ wJ 1 , d i λn i∈J  is tending to 1, which implies the theorem. [sent-894, score-0.108]
</p><p>76 The ﬁrst term is O p (n−1/2 λn ) = o p (λ2 ), while the last ones are n equal to w j + λn ∆ j − w j = λn  wj wj  ∆ j + o p (λn ). [sent-920, score-0.704]
</p><p>77 Given w deﬁned ˜ c , with probability through Lemma 20 and 21, we need to satisfy optimality condition (2) for all i ∈ J 1 tending to one. [sent-925, score-0.104]
</p><p>78 1, that the optimality condition is indeed satisﬁed with probability tending to one. [sent-927, score-0.104]
</p><p>79 (25), we get: 1 ˆ ˆ (ΣXiY − ΣXi XJ wJ ) = O p (n−1/2 λ−1 ) + ΣXi XJ Σ−1XJ ˜ n XJ λn Diag(d j / w j )wJ + λn ΣXi XJ Σ−1XJ Diag d j / w j XJ  Ip j −  w jw j  ∆ + o p (λn )  wj wj  = A + λn B + o p (λn ) + O p (n−1/2 λ−1 ). [sent-934, score-0.724]
</p><p>80 Moreover, if E 1 is true, then the group Lasso estimate has the correct sparsity pattern if and only if for all i ∈ Jc , ˆ ˆ ΣXi XJ (wJ − wJ ) − ΣXi ε ˜  λn di = λ0 n−1/2 di . [sent-943, score-0.218]
</p><p>81 Thus, the probability P(maxi∈Jc si /di λ0 ) converges to P max c i∈J  1 σ(ΣXi XJ Σ−1XJ vJ − vi ) − λ0 ΣXi XJ Σ−1XJ Diag(d j / w j )wJ XJ XJ di  λ0 . [sent-952, score-0.11]
</p><p>82 Under the event E1 which has probability tending to one, we have correct pattern selection if and only if maxi∈Jc si /di λ0 , which leads to P max c i∈J  1 σti − λ0 ΣXi XJ Σ−1XJ Diag(d j / w j )wJ XJ di  λ0 ,  where ti = ΣXi XJ Σ−1XJ vJ − vi . [sent-953, score-0.163]
</p><p>83 (2007), which states that the empirical covariance estimator converges in probability at rate O p (n−1/2 ) to the population covariance operators: ˆ Lemma 22 Assume (A4) and (A6). [sent-959, score-0.105]
</p><p>84 2 Proof of Theorem 11 We now extend Lemma 20 to covariance operators, which requires to use the alternative formulation and a slower rate of decrease for the regularization parameter: Lemma 25 Let f˜J be any minimizer of 1 µn 1ˆ ˆ ˆ ΣYY − ΣXJY , fJ FJ + fJ , ΣXJ XJ fJ FJ + 2 2 2  2  ∑ dj  f j Fj  . [sent-972, score-0.149]
</p><p>85 n Proof Note that from Cauchy-Schwarz inequality, we have:  2  ∑ dj j∈J  f j Fj  1/2 1/2 = ∑ d j f j Fj × j∈J  ∑ dj j∈J  f j Fj  ∑  j∈J  1/2  dj  2 f j Fj  1/2  f j Fj dj fj 2 j F f j Fj  ,  5. [sent-975, score-0.487]
</p><p>86 We consider the unique minimizer f¯J of the following cost function, built by replacing the regularization by its upperbound, 1ˆ 1 µn ˆ ˆ F( fJ ) = ΣYY − ΣXJY , fJ FJ + fJ , ΣXJ XJ fJ FJ + 2 2 2  ∑ dj  f j Fj  j∈J  dj fj 2 j F ∑ fj F . [sent-978, score-0.593]
</p><p>87 Note that D is upperbounded and lowerbounded, as an auto-adjoint operator, by strictly positive constants times the identity operator (with probability tending to one), that is, Dmax IFJ D Dmin IFJ with Dmin , Dmax > 0. [sent-980, score-0.103]
</p><p>88 fi F j  Since the right hand side of the previous equation corresponds to a continuously differentiable function of fJ around fJ (with upper-bounded derivatives around fJ ), we have: ∇ fi Fn ( f¯J ) − 0 Fi  1/2 Cµn fJ − f¯J FJ = µn O p (µn + n−1/2 µ−1 ). [sent-990, score-0.128]
</p><p>89 Since by Proposition 14, the solution is unique with probability tending to one, we need to prove that with probability tending to one f˜ is optimal for problem in Eq. [sent-999, score-0.106]
</p><p>90 Thus 1 ˆ ˆ ΣXiY − ΣXi XJ f˜J Fi 1 P di µn f d is tending to 1, which implies the theorem (using the same arguments than in the proof of Theorem 2 in Appendix B. [sent-1025, score-0.134]
</p><p>91 With probability tending to one, we have the optimality condition (15): ˆ ˆ ˆ ˆ ΣXJ ε + ΣXJ XJ fJ = ΣXJY = ΣXJ XJ fˆJ + µn fˆ If we denote Dn = n1/2 µn fˆ  d Diag(d j /  d Diag(d j /  fˆj F j ) fˆJ . [sent-1035, score-0.104]
</p><p>92 We have: ˆ ˆ ˆ n1/2 vi , ΣXiY − ΣXi XJ fˆJ Fi = n1/2 vi , ΣXi ε Fi + wJ , ΣXJ XJ n1/2 (fJ − fˆJ ) FJ + o p (1) ˆ ˆ = n1/2 vi , ΣXi ε Fi + wJ , µ0 D fJ − n1/2 ΣXJ ε FJ + o p (1) ˆ ˆ = wJ , µ0 D fJ FJ +n1/2 vi , ΣXi ε Fi −n1/2 wJ , ΣXJ ε FJ + o p (1). [sent-1041, score-0.12]
</p><p>93 2 shows that we have: 2 EEn  (1 − 1/n)σ2 vi , ΣXi Xi vi Fi + σ2 wJ , ΣXJ XJ wJ FJ − 2σ2 vi , ΣXi XJ wJ Fi min min min 2 2 = (1 − 1/n)(σmin vi , ΣXi Xi vi Fi − σmin vi , ΣXi XJ wJ Fi ). [sent-1047, score-0.18]
</p><p>94 Thus the ˆ ˆ probability P n1/2 vi , ΣXiY − ΣXi XJ fˆJ Fi di fˆ d vi Fi + 1 converges to a strictly positive limit (note that fˆ d can be replaced by f d without changing the result). [sent-1054, score-0.16]
</p><p>95 Since µn n1/2 → µ0 < ∞, this implies that ˆ ˆ P µ−1 vi , ΣXiY − ΣXi XJ fˆJ Fi > di fˆ d vi Fi n  is asymptotically strictly positive (i. [sent-1055, score-0.135]
</p><p>96 Σ CX X C−1 Diag(d j / f j F j )gJ di Xi Xi i J XJ XJ Fi Since with probability tending to one J( fˆ) = J, with probability tending to one, we have from optimality condition (15), and the usual line of arguments (see Eq. [sent-1061, score-0.212]
</p><p>97 Following the same argument as in the proof of Theorem 11, (and because µn n1/2 → +∞ as a consequence of Proposition 26), the ﬁrst term in the last expression (divided by µn ) converges to 1/2  −1 vi = ΣXi Xi CXi XJ CXJ XJ f  d Diag(d j /  f j F j )gJ  By assumption vi > di f d . [sent-1064, score-0.152]
</p><p>98 µn fˆ d Thus there is a non vanishing probability of being strictly larger than d i , which implies that with non vanishing probability, the optimality condition (14) is not satisﬁed, which is a contradiction. [sent-1068, score-0.153]
</p><p>99 Proof of Results on Adaptive Group Lasso In this appendix, we give proofs of the consistency of the adaptive group Lasso procedures. [sent-1089, score-0.142]
</p><p>100 We consider the eigenbasis of the non centered covariance operators on each F j , j = 1, . [sent-1123, score-0.12]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('xj', 0.822), ('wj', 0.352), ('fj', 0.207), ('xx', 0.153), ('lasso', 0.114), ('cxj', 0.099), ('diag', 0.094), ('dj', 0.07), ('bach', 0.07), ('asso', 0.067), ('fi', 0.064), ('xiy', 0.064), ('roup', 0.057), ('di', 0.055), ('tending', 0.053), ('xi', 0.052), ('group', 0.051), ('onsistency', 0.051), ('consistency', 0.048), ('cxi', 0.047), ('jc', 0.044), ('xjc', 0.044), ('ernel', 0.043), ('adaptive', 0.043), ('loading', 0.042), ('non', 0.041), ('ultiple', 0.041), ('covariance', 0.04), ('operators', 0.039), ('proposition', 0.039), ('yy', 0.037), ('px', 0.035), ('norms', 0.033), ('fukumizu', 0.032), ('sparsity', 0.032), ('condition', 0.031), ('dn', 0.03), ('vi', 0.03), ('operator', 0.03), ('xjy', 0.029), ('xy', 0.029), ('yuan', 0.026), ('groups', 0.026), ('converges', 0.025), ('kernel', 0.025), ('inconsistent', 0.025), ('regularization', 0.024), ('gj', 0.023), ('earning', 0.022), ('nonparametric', 0.022), ('ls', 0.022), ('jw', 0.02), ('optimality', 0.02), ('lanckriet', 0.02), ('strictly', 0.02), ('hilbert', 0.02), ('consistent', 0.02), ('rkhs', 0.02), ('norm', 0.02), ('ek', 0.019), ('xw', 0.018), ('baker', 0.018), ('cxx', 0.018), ('dgj', 0.018), ('hilbertian', 0.018), ('uj', 0.018), ('vj', 0.018), ('invertible', 0.017), ('ki', 0.017), ('wls', 0.017), ('vaart', 0.017), ('regular', 0.017), ('dual', 0.016), ('integrable', 0.016), ('correlation', 0.015), ('der', 0.015), ('minimizer', 0.015), ('fm', 0.014), ('dk', 0.014), ('bn', 0.014), ('theorem', 0.014), ('lemma', 0.013), ('slackness', 0.013), ('pattern', 0.013), ('dx', 0.013), ('weighting', 0.012), ('dmin', 0.012), ('fn', 0.012), ('weights', 0.012), ('stationarity', 0.012), ('correct', 0.012), ('proof', 0.012), ('primal', 0.012), ('dimensional', 0.012), ('zou', 0.012), ('dfj', 0.012), ('jy', 0.012), ('kernels', 0.012), ('square', 0.012), ('covariates', 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999899 <a title="27-tfidf-1" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: We consider the least-square regression problem with regularization by a block 1 -norm, that is, a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1 -norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic group selection consistency of the group Lasso. We derive necessary and sufﬁcient conditions for the consistency of group Lasso under practical assumptions, such as model misspeciﬁcation. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this inﬁnite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisﬁed. Keywords: sparsity, regularization, consistency, convex optimization, covariance operators</p><p>2 0.11238918 <a title="27-tfidf-2" href="./jmlr-2008-Consistency_of_Trace_Norm_Minimization.html">26 jmlr-2008-Consistency of Trace Norm Minimization</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufﬁcient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulﬁlled. Keywords: convex optimization, singular value decomposition, trace norm, consistency</p><p>3 0.068077974 <a title="27-tfidf-3" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>Author: Onureena Banerjee, Laurent El Ghaoui, Alexandre d'Aspremont</p><p>Abstract: We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1 -norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our ﬁrst algorithm uses block coordinate descent, and can be interpreted as recursive 1 -norm penalized regression. Our second algorithm, based on Nesterov’s ﬁrst order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data. Keywords: model selection, maximum likelihood estimation, convex optimization, Gaussian graphical model, binary data</p><p>4 0.066013388 <a title="27-tfidf-4" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>Author: Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui</p><p>Abstract: Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefﬁcients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semideﬁnite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefﬁcients, with total complexity O(n3 ), where n is the number of variables. We then use the same relaxation to derive sufﬁcient conditions for global optimality of a solution, which can be tested in O(n3 ) per pattern. We discuss applications in subset selection and sparse recovery and show on artiﬁcial examples and biological data that our algorithm does provide globally optimal solutions in many cases. Keywords: PCA, subset selection, sparse eigenvalues, sparse recovery, lasso</p><p>5 0.060655516 <a title="27-tfidf-5" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>Author: Liviu Panait, Karl Tuyls, Sean Luke</p><p>Abstract: This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the beneﬁts of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction</p><p>6 0.051370114 <a title="27-tfidf-6" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>7 0.04008558 <a title="27-tfidf-7" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>8 0.038603973 <a title="27-tfidf-8" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>9 0.036374837 <a title="27-tfidf-9" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>10 0.034278046 <a title="27-tfidf-10" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>11 0.032715946 <a title="27-tfidf-11" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>12 0.032194566 <a title="27-tfidf-12" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>13 0.031106317 <a title="27-tfidf-13" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>14 0.02982525 <a title="27-tfidf-14" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>15 0.027522493 <a title="27-tfidf-15" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>16 0.026812937 <a title="27-tfidf-16" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>17 0.025808163 <a title="27-tfidf-17" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>18 0.024439165 <a title="27-tfidf-18" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>19 0.023791395 <a title="27-tfidf-19" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>20 0.022571769 <a title="27-tfidf-20" href="./jmlr-2008-Non-Parametric_Modeling_of_Partially_Ranked_Data.html">69 jmlr-2008-Non-Parametric Modeling of Partially Ranked Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.131), (1, -0.055), (2, -0.073), (3, -0.001), (4, 0.094), (5, -0.061), (6, -0.063), (7, 0.066), (8, -0.053), (9, -0.042), (10, 0.052), (11, -0.099), (12, -0.033), (13, 0.122), (14, 0.213), (15, -0.196), (16, 0.036), (17, 0.001), (18, -0.278), (19, 0.075), (20, 0.082), (21, 0.011), (22, 0.146), (23, -0.029), (24, 0.031), (25, -0.065), (26, -0.074), (27, 0.246), (28, -0.02), (29, -0.093), (30, 0.008), (31, 0.046), (32, -0.025), (33, -0.168), (34, 0.006), (35, 0.051), (36, -0.067), (37, 0.234), (38, -0.093), (39, -0.01), (40, -0.227), (41, -0.082), (42, 0.216), (43, -0.028), (44, -0.297), (45, -0.113), (46, 0.009), (47, 0.006), (48, -0.079), (49, 0.166)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97136635 <a title="27-lsi-1" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: We consider the least-square regression problem with regularization by a block 1 -norm, that is, a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1 -norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic group selection consistency of the group Lasso. We derive necessary and sufﬁcient conditions for the consistency of group Lasso under practical assumptions, such as model misspeciﬁcation. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this inﬁnite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisﬁed. Keywords: sparsity, regularization, consistency, convex optimization, covariance operators</p><p>2 0.6641922 <a title="27-lsi-2" href="./jmlr-2008-Consistency_of_Trace_Norm_Minimization.html">26 jmlr-2008-Consistency of Trace Norm Minimization</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufﬁcient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulﬁlled. Keywords: convex optimization, singular value decomposition, trace norm, consistency</p><p>3 0.23248044 <a title="27-lsi-3" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>Author: Liviu Panait, Karl Tuyls, Sean Luke</p><p>Abstract: This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the beneﬁts of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction</p><p>4 0.21765471 <a title="27-lsi-4" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>Author: Andreas Maurer</p><p>Abstract: A method is introduced to learn and represent similarity with linear operators in kernel induced Hilbert spaces. Transferring error bounds for vector valued large-margin classiﬁers to the setting of Hilbert-Schmidt operators leads to dimension free bounds on a risk functional for linear representations and motivates a regularized objective functional. Minimization of this objective is effected by a simple technique of stochastic gradient descent. The resulting representations are tested on transfer problems in image processing, involving plane and spatial geometric invariants, handwritten characters and face recognition. Keywords: learning similarity, similarity, transfer learning</p><p>5 0.15947337 <a title="27-lsi-5" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>Author: Ja-Yong Koo, Yoonkyung Lee, Yuwon Kim, Changyi Park</p><p>Abstract: The support vector machine has been successful in a variety of applications. Also on the theoretical front, statistical properties of the support vector machine have been studied quite extensively with a particular attention to its Bayes risk consistency under some conditions. In this paper, we study somewhat basic statistical properties of the support vector machine yet to be investigated, namely the asymptotic behavior of the coefﬁcients of the linear support vector machine. A Bahadur type representation of the coefﬁcients is established under appropriate conditions, and their asymptotic normality and statistical variability are derived on the basis of the representation. These asymptotic results do not only help further our understanding of the support vector machine, but also they can be useful for related statistical inferences. Keywords: asymptotic normality, Bahadur representation, classiﬁcation, convexity lemma, Radon transform</p><p>6 0.1415835 <a title="27-lsi-6" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>7 0.12864302 <a title="27-lsi-7" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>8 0.12413236 <a title="27-lsi-8" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>9 0.123236 <a title="27-lsi-9" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>10 0.12145147 <a title="27-lsi-10" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>11 0.11217836 <a title="27-lsi-11" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>12 0.11028875 <a title="27-lsi-12" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>13 0.10705356 <a title="27-lsi-13" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>14 0.10615925 <a title="27-lsi-14" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>15 0.099568948 <a title="27-lsi-15" href="./jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">80 jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>16 0.093217552 <a title="27-lsi-16" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>17 0.09199129 <a title="27-lsi-17" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>18 0.090998553 <a title="27-lsi-18" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>19 0.089203194 <a title="27-lsi-19" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>20 0.085351907 <a title="27-lsi-20" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.034), (5, 0.011), (12, 0.307), (40, 0.032), (54, 0.04), (58, 0.069), (61, 0.065), (66, 0.097), (76, 0.016), (78, 0.03), (88, 0.062), (92, 0.039), (94, 0.036), (99, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.78044617 <a title="27-lda-1" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: We consider the least-square regression problem with regularization by a block 1 -norm, that is, a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1 -norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic group selection consistency of the group Lasso. We derive necessary and sufﬁcient conditions for the consistency of group Lasso under practical assumptions, such as model misspeciﬁcation. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this inﬁnite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisﬁed. Keywords: sparsity, regularization, consistency, convex optimization, covariance operators</p><p>2 0.74814188 <a title="27-lda-2" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>Author: Imhoi Koo, Rhee Man Kil</p><p>Abstract: This paper presents a new method of model selection for regression problems using the modulus of continuity. For this purpose, we suggest the prediction risk bounds of regression models using the modulus of continuity which can be interpreted as the complexity of functions. We also present the model selection criterion referred to as the modulus of continuity information criterion (MCIC) which is derived from the suggested prediction risk bounds. The suggested MCIC provides a risk estimate using the modulus of continuity for a trained regression model (or an estimation function) while other model selection criteria such as the AIC and BIC use structural information such as the number of training parameters. As a result, the suggested MCIC is able to discriminate the performances of trained regression models, even with the same structure of training models. To show the effectiveness of the proposed method, the simulation for function approximation using the multilayer perceptrons (MLPs) was conducted. Through the simulation for function approximation, it was demonstrated that the suggested MCIC provides a good selection tool for nonlinear regression models, even with the limited size of data. Keywords: regression models, multilayer perceptrons, model selection, information criteria, modulus of continuity</p><p>3 0.477694 <a title="27-lda-3" href="./jmlr-2008-Consistency_of_Trace_Norm_Minimization.html">26 jmlr-2008-Consistency of Trace Norm Minimization</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufﬁcient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulﬁlled. Keywords: convex optimization, singular value decomposition, trace norm, consistency</p><p>4 0.42538434 <a title="27-lda-4" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>Author: Xianchao Xie, Zhi Geng</p><p>Abstract: In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is ﬁrst decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efﬁciency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method. Keywords: Bayesian network, conditional independence, decomposition, directed acyclic graph, structural learning</p><p>5 0.40920717 <a title="27-lda-5" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>Author: Zongming Ma, Xianchao Xie, Zhi Geng</p><p>Abstract: Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse. Keywords: chain graph, conditional independence, decomposition, graphical model, structural learning</p><p>6 0.39734334 <a title="27-lda-6" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>7 0.38941634 <a title="27-lda-7" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>8 0.38815385 <a title="27-lda-8" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>9 0.38592035 <a title="27-lda-9" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>10 0.38348848 <a title="27-lda-10" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>11 0.38337076 <a title="27-lda-11" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>12 0.38332674 <a title="27-lda-12" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>13 0.38145611 <a title="27-lda-13" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>14 0.38039276 <a title="27-lda-14" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>15 0.37977767 <a title="27-lda-15" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>16 0.379426 <a title="27-lda-16" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>17 0.37911761 <a title="27-lda-17" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>18 0.37450743 <a title="27-lda-18" href="./jmlr-2008-Causal_Reasoning_with_Ancestral_Graphs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">20 jmlr-2008-Causal Reasoning with Ancestral Graphs    (Special Topic on Causality)</a></p>
<p>19 0.37327582 <a title="27-lda-19" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>20 0.37240112 <a title="27-lda-20" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
