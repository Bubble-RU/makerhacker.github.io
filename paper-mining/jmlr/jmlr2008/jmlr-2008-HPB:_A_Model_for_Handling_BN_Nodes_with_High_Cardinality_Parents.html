<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 jmlr-2008-HPB: A Model for Handling BN Nodes with High Cardinality Parents</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-42" href="#">jmlr2008-42</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>42 jmlr-2008-HPB: A Model for Handling BN Nodes with High Cardinality Parents</h1>
<br/><p>Source: <a title="jmlr-2008-42-pdf" href="http://jmlr.org/papers/volume9/jambeiro08a/jambeiro08a.pdf">pdf</a></p><p>Author: Jorge Jambeiro Filho, Jacques Wainer</p><p>Abstract: We replaced the conditional probability tables of Bayesian network nodes whose parents have high cardinality with a multilevel empirical hierarchical Bayesian model called hierarchical pattern Bayes (HPB).1 The resulting Bayesian networks achieved signiÄ?Ĺš cant performance improvements over Bayesian networks with the same structure and traditional conditional probability tables, over Bayesian networks with simpler structures like naĂ&sbquo;Â¨ve Bayes and tree augmented naĂ&sbquo;Â¨ve Bayes, over Ă&bdquo;Ä&hellip; Ă&bdquo;Ä&hellip; Bayesian networks where traditional conditional probability tables were substituted by noisy-OR gates, default tables, decision trees and decision graphs and over Bayesian networks constructed after a cardinality reduction preprocessing phase using the agglomerative information bottleneck method. Our main tests took place in important fraud detection domains, which are characterized by the presence of high cardinality attributes and by the existence of relevant interactions among them. Other tests, over UCI data sets, show that HPB may have a quite wide applicability. Keywords: probabilistic reasoning, Bayesian networks, smoothing, hierarchical Bayes, empirical Bayes</p><p>Reference: <a title="jmlr-2008-42-reference" href="../jmlr2008_reference/jmlr-2008-HPB%3A_A_Model_for_Handling_BN_Nodes_with_High_Cardinality_Parents_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our main tests took place in important fraud detection domains, which are characterized by the presence of high cardinality attributes and by the existence of relevant interactions among them. [sent-9, score-0.243]
</p><p>2 Thus, the domain is characterized by the presence of high cardinality attributes. [sent-26, score-0.097]
</p><p>3 Ĺš t from critical patterns we would like to use the Bayesian network (BN) (Pearl, 1988) presented in Figure 1, where all explanatory attributes are parents of the class attribute. [sent-48, score-0.281]
</p><p>4      dcc  imp  cp  epr     Ă&sbquo;    Ă&sbquo;Ä&bdquo;  Ă&sbquo;  e Ă&sbquo;    se Ă&sbquo;Ä&bdquo;C Ă&sbquo; Ă&sbquo;&hellip;   class    Figure 1: Direct BN structure for misclassiÄ? [sent-50, score-0.109]
</p><p>5 The distributions of X j given any two different combinations of values for its parents are assumed to be independent and a Dirichlet prior probability distribution for Ă&#x17D;Â¸ jk is usually adopted. [sent-52, score-0.169]
</p><p>6 It is clear that for rarely seen combinations of attributes the choice of such structure and Equation (2) tends to produce unreliable probabilities whose calculation is dominated by the noninformative prior probability distribution. [sent-59, score-0.138]
</p><p>7 This can be achieved limiting the number of parents for a network node. [sent-61, score-0.097]
</p><p>8 NaĂ&sbquo;Â¨ve Ă&bdquo;Ä&hellip; Bayes(Duda and Hart, 1973) is an extreme example where the maximum number of parents is limited to one (the class node is the only parent of any other node). [sent-62, score-0.17]
</p><p>9 , 1997) adds a tree to the structure of naĂ&sbquo;Â¨ve Bayes connecting the explanatory Ă&bdquo;Ä&hellip; attributes and limits the maximum number of parent nodes to two. [sent-64, score-0.136]
</p><p>10 However, limiting the maximum Ă&sbquo;Â´ number of parents also limits the representational power of the Bayesian network(Boull e, 2005) and, thus, limits our ability to capture interactions among attributes and beneÄ? [sent-65, score-0.212]
</p><p>11 Since the high cardinality of our attributes is creating trouble, it is a reasonable idea to preprocess the data, reducing the cardinality of the attributes. [sent-68, score-0.283]
</p><p>12 However, the process of reducing the cardinality of one attribute is blind with respect to the others (except for the class attribute) (Slonim and Tishby, 1999; BoullĂ&sbquo;Â´ , 2005; Micci-Barreca, 2001), and thus it is une likely that cardinality reduction will result in any signiÄ? [sent-70, score-0.247]
</p><p>13 Using noisy-OR, the number of parameters required to represent the conditional probability distribution (CPD) of a node given its parents, instead of being proportional to the product of the cardinality of all parents attributes, becomes proportional to the sum of their cardinality. [sent-74, score-0.227]
</p><p>14 Ĺš&sbquo;exible representations for the conditional probability distributions of a node given its parents, like default tables (DFs) (Friedman and Goldszmidt, 1996b), decision trees (DTs) (Friedman and Goldszmidt, 1996b) and decision graphs (DGs) (Chickering et al. [sent-77, score-0.129]
</p><p>15 2143  JAMBEIRO AND WAINER  Using traditional CPTs, we assume that the probability distributions for a node given any two combinations of values for the parents are independent. [sent-81, score-0.13]
</p><p>16 On the other hand, using DTs, DFs or DGs to represent the conditional probability distributions of a node given its parents, we assume that the probability distribution of the node given two different combinations of values for the parents may be either identical or completely independent. [sent-84, score-0.163]
</p><p>17 Ĺš nition of a smoothing schema for TAN shows that we can treat the data that is used to estimate a CPT as hierarchical: P(x ji |Ä&#x17D;&euro; jk ) =  N jki + S Ă&sbquo;Ë&Dagger; P(x ji ) , N jk + S  where S is a constant that deÄ? [sent-102, score-0.268]
</p><p>18 ADE is the consequence of adopting an informative Dirichlet prior probability distribution where Ă&#x17D;Ä&hellip; jki Ă˘&circ;? [sent-105, score-0.1]
</p><p>19 P(x ji ), where P(x ji ) is the unconditional probability of x ji (for the meaning of Ă&#x17D;Ä&hellip; jki , see Equation 1). [sent-106, score-0.173]
</p><p>20 ADE uses the probability distribution assessed in a wider population (the whole training set) to build an informative prior probability distribution for a narrower population and so it has a hierarchical nature. [sent-107, score-0.086]
</p><p>21 (2003) ADE is an empirical hierarchical Bayesian model, not a full hierarchical Bayesian model. [sent-109, score-0.12]
</p><p>22 It is a linear combination of two factors N jki /N jk and P(x ji ). [sent-114, score-0.153]
</p><p>23 In contrast, we present a model, that we call hierarchical pattern Bayes (HPB), which moves slowly from smaller populations to larger ones beneÄ? [sent-118, score-0.082]
</p><p>24 Its name comes from the fact that it explores an hierarchy of patterns intensively, though it is not a full hierarchical Bayesian model. [sent-122, score-0.123]
</p><p>25 Ĺš nition 4 The level of a pattern W , level(W ), is the number of attributes deÄ? [sent-140, score-0.111]
</p><p>26 Consequently, P(Cr |W ) =  Nwr + S Ă&sbquo;Ë&Dagger; P(Cr |g(W )) , Nw + S  (3)  where Nw is the number of instances in the training set satisfying the pattern W and Nwr is the number of instances in the training set satisfying the pattern W whose class label is Cr . [sent-149, score-0.086]
</p><p>27 Each pattern is represented by a node and the set of parents of a pattern W in the DAG presented in Figure 2 is g(W ). [sent-153, score-0.174]
</p><p>28 Assuming independence among overlapping patterns, as Equation (4) does, is equivalent to assuming independence among attributes which are known to be highly correlated, what may appear to be strange. [sent-172, score-0.089]
</p><p>29 However, naĂ&sbquo;Â¨ve Bayes has been reported to Ă&bdquo;Ä&hellip; perform better when attributes are highly correlated than when correlation is moderate (Rish et al. [sent-173, score-0.089]
</p><p>30 If the class variable is binary, this strategy works well, but if the class node has high cardinality it is better to employ a noninformative prior probability distribution: P(Cr ) =  Nr + SNI /Mc , N + SNI  where Mc is the number of classes and SNI is the smoothing constant that deÄ? [sent-185, score-0.215]
</p><p>31 The reason why naĂ&sbquo;Â¨ve Bayes produces extreme probabilities is that it treats each attribute value Ă&bdquo;Ä&hellip; in a pattern as if it were new information. [sent-202, score-0.096]
</p><p>32 Since attributes are not really independent, a new attribute 2147  JAMBEIRO AND WAINER  value is not 100% new information, treating it as if it were completely new reinforces the previous beliefs of naĂ&sbquo;Â¨ve Bayes towards either zero or one. [sent-203, score-0.142]
</p><p>33 In order to obtain better posterior probability distributions, calibration mechanisms which try to compensate the overly conÄ? [sent-206, score-0.08]
</p><p>34 NaĂ&sbquo;Â¨ve Bayes assumes that attributes are independent given the class. [sent-208, score-0.089]
</p><p>35 Equation (4) assumes that Ă&bdquo;Ä&hellip; some aggregations of attributes are independent given the class. [sent-209, score-0.089]
</p><p>36 Since many of these aggregations have attributes in common, the use of Equation (4) is equivalent to assuming independence among attributes which are known to be highly correlated. [sent-210, score-0.178]
</p><p>37 NaĂ&sbquo;Â¨ve Bayes has been reported to perform Ă&bdquo;Ä&hellip; better when attributes are highly correlated than when correlation is moderate (Rish et al. [sent-211, score-0.089]
</p><p>38 Our calibration mechanism attenuates the probabilities when they are extreme without affecting them in the point P(Cr ), where, we believe, they are already correct. [sent-223, score-0.101]
</p><p>39 In Figure 3 we show the effect of the calibration mechanism. [sent-224, score-0.08]
</p><p>40 The other oblique straight line is the result of our calibration mechanism, P (Cr |g(W )). [sent-233, score-0.08]
</p><p>41 At this level, all interactions among attributes may be captured as long as there are enough training instances. [sent-239, score-0.115]
</p><p>42 Actually, if there are high cardinality attributes, it is more likely that only a minority of them are represented well. [sent-241, score-0.097]
</p><p>43 On the other hand, prior probabilities are critical for the vast majority of cases where level L patterns are not well represented in the training set. [sent-243, score-0.093]
</p><p>44 At this level, a greater fraction of patterns are well represented and it is still possible to capture the majority of attribute interactions. [sent-245, score-0.095]
</p><p>45 This results in extreme probability estimates that are attenuated by the calibration mechanism in Equation (5). [sent-252, score-0.101]
</p><p>46 However, since the number of parents of any node in a Bayesian network is usually small because the size of a CPT is exponential in the number 2149  JAMBEIRO AND WAINER  of parent nodes, HPB may be used as a replacement for Bayesian networks conditional probability tables in almost any domain. [sent-267, score-0.209]
</p><p>47 Space and time are frequently not the limiting factor for the number of parents of a BN node. [sent-268, score-0.097]
</p><p>48 More parents usually mean less reliable probabilities (Keogh and Pazzani, 1999) and it is not uncommon to limit their number to two (Friedman and Goldszmidt, 1996a; Keogh and Pazzani, 1999; Hamine and Helman, 2004). [sent-269, score-0.097]
</p><p>49 To calculate, P(x jk |Ä&#x17D;&euro; ji ) it is just a matter of acting as if Cr = x jk and W = Ä&#x17D;&euro; ji , ignoring all other attributes and using HPB to calculate P(Cr |W ). [sent-272, score-0.247]
</p><p>50 The higher such level is, the more attributes in common the aggregations have, the more extreme probability estimates are and the stronger must be the effect of the calibration mechanism. [sent-287, score-0.19]
</p><p>51 2150  HPB: A M ODEL FOR H ANDLING BN N ODES WITH H IGH C ARDINALITY PARENTS  We employed hit curves, instead of the more popular Receiver Operating Characteristic Curves (ROC) (Egan, 1975), because they match the interests of the user of a fraud detection system directly. [sent-300, score-0.08]
</p><p>52 Ĺš ne exactly the same attributes (possibly with different values). [sent-306, score-0.089]
</p><p>53 Note that, in both cases, HPB running time is exponential in the number of parent attributes, linear in the number of instances and independent of the cardinality of the parent attributes. [sent-324, score-0.156]
</p><p>54 Considering four explanatory attributes: declared custom code (DCC), importer (IMP), country of production (CP) and entry point in the receiving country (EPR), we need to estimate, for each new example, the probability that it involves a misclassiÄ? [sent-344, score-0.146]
</p><p>55 We found that the construction of a DG becomes very slow when the BN node in question has high cardinality and its parents also have high cardinality. [sent-371, score-0.227]
</p><p>56 High cardinality parents imply many possible split/merge operations to compare in each step of the learning algorithm and a high cardinality child implies that each comparison requires a lot of calculation. [sent-372, score-0.291]
</p><p>57 (1997), all four explanatory attributes were parents of the class attribute. [sent-374, score-0.214]
</p><p>58 Build a search enumeration SE containing all powers of 10, all halves of powers of 10 and quarters of powers of 10 within SI; 3. [sent-389, score-0.078]
</p><p>59 Ĺš cation detection - hit curves (to avoid pollution we only present curves related to a subset of the tested methods)  3. [sent-471, score-0.08]
</p><p>60 The reason is that critical patterns involving all attributes are decisive in the very beginning of the curves. [sent-769, score-0.156]
</p><p>61 ADE treats all attributes at once and thus can beneÄ? [sent-770, score-0.089]
</p><p>62 Using Decision Graphs (with binary splits enabled), the most critical patterns were separated from the others and that resulted in a signiÄ? [sent-773, score-0.089]
</p><p>63 Ĺš ts from critical patterns involving many or even all attributes, but also considers the inÄ? [sent-780, score-0.102]
</p><p>64 Since the cardinality of the attributes is a problem in this domain, we decided to also test all classiÄ? [sent-791, score-0.186]
</p><p>65 Ĺš cation methods on a transformed data set where the cardinality of all attributes were reduced by the agglomerative information bottleneck method (AIBN). [sent-792, score-0.215]
</p><p>66 AIBN reduces the cardinality of an attribute by successively executing the merge of two values that results in minimum mutual information lost. [sent-797, score-0.15]
</p><p>67 In spite of this, the cardinality reduction was accentuated. [sent-800, score-0.097]
</p><p>68 Table 3 shows the cardinality of the attributes before and after reduction. [sent-801, score-0.186]
</p><p>69 Attribute DCC IMP CP EPR  Original Cardinality 7608 18846 161 80  Final Cardinality 101 84 50 28  Table 3: Cardinality reduction using AIBN 2157  JAMBEIRO AND WAINER  Because of the lower cardinality of the resulting attributes, it was possible to test BNs with DGs instead of standalone DGs. [sent-802, score-0.126]
</p><p>70 The reason is that AIBN joins attribute values looking at each attribute separately and thus ignoring any interaction among them. [sent-916, score-0.106]
</p><p>71 BNs with DGs lost much of their ability to explore critical patterns too, which also resulted in a much worse performance at a selection rate of 1%. [sent-918, score-0.089]
</p><p>72 Ĺš cation detection with cardinality reduction - hit curves (to avoid pollution we only present curves related to a subset of the tested methods)  Actor in Role 1  Actor in Role 2 . [sent-933, score-0.177]
</p><p>73 Ĺš cation detection with cardinality reduction - other measures Since the number of possible actors can be very big, but the number of roles is usually small, it seems reasonable to replace the CPT of the Action node in Figure 6 with HPB. [sent-1111, score-0.161]
</p><p>74 We used two high cardinality explanatory attributes: the importer (IMP) and the exporter (EXP)4 to predict another high cardinality variable, the declared custom code (DCC). [sent-1117, score-0.325]
</p><p>75 The importer attribute can assume 18846 values, the exporter attribute can assume 43880 values and the declared custom code can assume 7608 values. [sent-1120, score-0.209]
</p><p>76 We did not test DGs, DFs and DTs because the combination of high cardinality parents and a high cardinality child makes them too slow. [sent-1129, score-0.291]
</p><p>77 For comparison we also evaluated BNs using other representations for the conditional probability distribution (CPD) of a node given its parents . [sent-1245, score-0.13]
</p><p>78 To guarantee that we would not have excessively long execution times we limited the maximum number of parents to 10 and because HPB does not handle continuous attributes we removed them all. [sent-1256, score-0.186]
</p><p>79 1, HPB is much faster than DGs, DTs and DFs in the task of handling a small set of high cardinality explanatory attributes. [sent-1305, score-0.125]
</p><p>80 However, in UCI tests, many BN structures involved sets of low cardinality parents. [sent-1306, score-0.097]
</p><p>81 However, the vast majority of variables in the tested data sets have low cardinality (the highest cardinality variable among all data sets is the audiology class variable with 24 possible values) and many of them are binary. [sent-1312, score-0.194]
</p><p>82 Ĺš cation predictions, even when the characteristics of the attributes are opposite to the ones that inspired HPB. [sent-1317, score-0.089]
</p><p>83 Conclusions We presented HPB a novel multilevel empirical hierarchical Bayesian model, which is intended to replace conditional probability tables of Bayesian network nodes whose parents have high cardinality. [sent-1322, score-0.195]
</p><p>84 In this domain, interactions among attributes have a great inÄ? [sent-1332, score-0.115]
</p><p>85 Ĺš cation, but due to the high cardinality of the attributes in this domain, exploiting such interactions is challenging. [sent-1335, score-0.212]
</p><p>86 HPBĂ˘&euro;&trade;s execution time is exponential in the number of parents of a BN node but independent of their cardinality. [sent-1338, score-0.13]
</p><p>87 Since the number of parents of a BN node is almost always small, for nodes whose parents have high cardinality, HPB, at least when its smoothing coefÄ? [sent-1339, score-0.263]
</p><p>88 Ĺš xed, is much faster than default tables, decision trees or decision graphs when employed to represent the conditional probability distribution of the node given its parents. [sent-1341, score-0.091]
</p><p>89 Despite the fact that these data sets do not include high cardinality attributes, HPB was the representation that resulted in more winnings than any other representation in three comparison measures. [sent-1349, score-0.119]
</p><p>90 However, we can still conclude that BN nodes whose CPDs given their parents are better represented by HPB than by other methods are not rare. [sent-1352, score-0.097]
</p><p>91 If this results in a node with high cardinality parents, they can just use HPB as a plug-in replacement for the CPT of the node and keep the structure they want. [sent-1357, score-0.185]
</p><p>92 Without a method like HPB the high cardinality parents could easily result in unreliable probability estimates that could compromise the whole model. [sent-1358, score-0.194]
</p><p>93 Ĺš&sbquo;ect the target problem as closely as the original one, but which would avoid the use of high cardinality nodes as parents of the same node. [sent-1360, score-0.194]
</p><p>94 However, we are not aware of any hierarchical Bayesian model that can replace conditional probability tables of Bayesian network nodes whose parents have high cardinality. [sent-1367, score-0.195]
</p><p>95 Ĺš xed, symmetrical (all attributes are treated the same way) and complete (all subsets of each pattern of interest are considered in the calculation of the probability of a class given the pattern). [sent-1377, score-0.111]
</p><p>96 If instead of this we employed mixtures of probability tables (Fujimoto and Murata, 2006) where default tables, decision trees or decision graphs were used as category integration tables, results could be better. [sent-1381, score-0.096]
</p><p>97 A bayesian approach to learning bayesian networks with local structure. [sent-3291, score-0.112]
</p><p>98 Using a hierarchical bayesian model to handle high cardinality attributes with relevant interactions in a classiÄ? [sent-3344, score-0.328]
</p><p>99 A preprocessing scheme for high-cardinality categorical attributes in classiÄ? [sent-3362, score-0.089]
</p><p>100 The revisiting problem in mobile robot map building: A hierarchical bayesian approach. [sent-3381, score-0.116]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hpb', 0.725), ('cr', 0.288), ('mce', 0.24), ('bn', 0.202), ('rmse', 0.159), ('ade', 0.137), ('nb', 0.118), ('jambeiro', 0.114), ('nc', 0.105), ('wainer', 0.103), ('cardinality', 0.097), ('parents', 0.097), ('attributes', 0.089), ('andling', 0.08), ('ardinality', 0.08), ('calibration', 0.08), ('dgs', 0.08), ('igh', 0.08), ('jki', 0.074), ('na', 0.062), ('odes', 0.061), ('hierarchical', 0.06), ('cpts', 0.058), ('bns', 0.057), ('goldszmidt', 0.057), ('bayesian', 0.056), ('attribute', 0.053), ('jt', 0.052), ('hit', 0.049), ('ut', 0.049), ('pc', 0.049), ('tr', 0.047), ('bayes', 0.047), ('jk', 0.046), ('imp', 0.046), ('cpt', 0.044), ('dts', 0.044), ('tan', 0.042), ('patterns', 0.042), ('aibn', 0.04), ('dcc', 0.04), ('friedman', 0.04), ('odel', 0.04), ('dfs', 0.039), ('cpd', 0.039), ('dg', 0.038), ('tables', 0.038), ('smoothing', 0.036), ('ts', 0.035), ('misclassi', 0.035), ('importer', 0.034), ('ntr', 0.034), ('nw', 0.034), ('nwr', 0.034), ('zadrozny', 0.034), ('node', 0.033), ('ji', 0.033), ('detection', 0.031), ('dirichlet', 0.03), ('brazil', 0.029), ('ve', 0.029), ('agglomerative', 0.029), ('filho', 0.029), ('standalone', 0.029), ('explanatory', 0.028), ('uci', 0.026), ('prior', 0.026), ('powers', 0.026), ('interactions', 0.026), ('si', 0.025), ('critical', 0.025), ('declared', 0.024), ('pat', 0.024), ('coef', 0.024), ('mdl', 0.024), ('chickering', 0.024), ('cpds', 0.023), ('crt', 0.023), ('customs', 0.023), ('epr', 0.023), ('exporter', 0.023), ('gelman', 0.023), ('noninformative', 0.023), ('sni', 0.023), ('pattern', 0.022), ('resulted', 0.022), ('custom', 0.022), ('replacement', 0.022), ('instances', 0.021), ('extreme', 0.021), ('hierarchy', 0.021), ('cients', 0.021), ('default', 0.02), ('domingos', 0.019), ('bianca', 0.019), ('country', 0.019), ('enabled', 0.019), ('equation', 0.019), ('decision', 0.019), ('parent', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="42-tfidf-1" href="./jmlr-2008-HPB%3A_A_Model_for_Handling_BN_Nodes_with_High_Cardinality_Parents.html">42 jmlr-2008-HPB: A Model for Handling BN Nodes with High Cardinality Parents</a></p>
<p>Author: Jorge Jambeiro Filho, Jacques Wainer</p><p>Abstract: We replaced the conditional probability tables of Bayesian network nodes whose parents have high cardinality with a multilevel empirical hierarchical Bayesian model called hierarchical pattern Bayes (HPB).1 The resulting Bayesian networks achieved signiÄ?Ĺš cant performance improvements over Bayesian networks with the same structure and traditional conditional probability tables, over Bayesian networks with simpler structures like naĂ&sbquo;Â¨ve Bayes and tree augmented naĂ&sbquo;Â¨ve Bayes, over Ă&bdquo;Ä&hellip; Ă&bdquo;Ä&hellip; Bayesian networks where traditional conditional probability tables were substituted by noisy-OR gates, default tables, decision trees and decision graphs and over Bayesian networks constructed after a cardinality reduction preprocessing phase using the agglomerative information bottleneck method. Our main tests took place in important fraud detection domains, which are characterized by the presence of high cardinality attributes and by the existence of relevant interactions among them. Other tests, over UCI data sets, show that HPB may have a quite wide applicability. Keywords: probabilistic reasoning, Bayesian networks, smoothing, hierarchical Bayes, empirical Bayes</p><p>2 0.055232011 <a title="42-tfidf-2" href="./jmlr-2008-Generalization_from_Observed_to_Unobserved_Features_by_Clustering.html">38 jmlr-2008-Generalization from Observed to Unobserved Features by Clustering</a></p>
<p>Author: Eyal Krupka, Naftali Tishby</p><p>Abstract: We argue that when objects are characterized by many attributes, clustering them on the basis of a random subset of these attributes can capture information on the unobserved attributes as well. Moreover, we show that under mild technical conditions, clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set. We prove ﬁnite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting. We use our framework to analyze generalization to unobserved features of two well-known clustering algorithms: k-means and the maximum likelihood multinomial mixture model. The scheme is demonstrated for collaborative ﬁltering of users with movie ratings as attributes and document clustering with words as attributes. Keywords: clustering, unobserved features, learning theory, generalization in clustering, information bottleneck</p><p>3 0.05340888 <a title="42-tfidf-3" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>Author: Abraham George, Warren B. Powell, Sanjeev R. Kulkarni</p><p>Abstract: We consider the problem of estimating the value of a multiattribute resource, where the attributes are categorical or discrete in nature and the number of potential attribute vectors is very large. The problem arises in approximate dynamic programming when we need to estimate the value of a multiattribute resource from estimates based on Monte-Carlo simulation. These problems have been traditionally solved using aggregation, but choosing the right level of aggregation requires resolving the classic tradeoff between aggregation error and sampling error. We propose a method that estimates the value of a resource at different levels of aggregation simultaneously, and then uses a weighted combination of the estimates. Using the optimal weights, which minimizes the variance of the estimate while accounting for correlations between the estimates, is computationally too expensive for practical applications. We have found that a simple inverse variance formula (adjusted for bias), which effectively assumes the estimates are independent, produces near-optimal estimates. We use the setting of two levels of aggregation to explain why this approximation works so well. Keywords: hierarchical statistics, approximate dynamic programming, mixture models, adaptive learning, multiattribute resources</p><p>4 0.053122595 <a title="42-tfidf-4" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>Author: Amit Dhurandhar, Alin Dobra</p><p>Abstract: In this paper we use the methodology introduced by Dhurandhar and Dobra (2009) for analyzing the error of classiﬁers and the model selection measures, to analyze decision tree algorithms. The methodology consists of obtaining parametric expressions for the moments of the generalization error (GE) for the classiﬁcation model of interest, followed by plotting these expressions for interpretability. The major challenge in applying the methodology to decision trees, the main theme of this work, is customizing the generic expressions for the moments of GE to this particular classiﬁcation algorithm. The speciﬁc contributions we make in this paper are: (a) we primarily characterize a subclass of decision trees namely, Random decision trees, (b) we discuss how the analysis extends to other decision tree algorithms and (c) in order to extend the analysis to certain model selection measures, we generalize the relationships between the moments of GE and moments of the model selection measures given in (Dhurandhar and Dobra, 2009) to randomized classiﬁcation algorithms. An empirical comparison of the proposed method with Monte Carlo and distribution free bounds obtained using Breiman’s formula, depicts the advantages of the method in terms of running time and accuracy. It thus showcases the use of the deployed methodology as an exploratory tool to study learning algorithms. Keywords: moments, generalization error, decision trees</p><p>5 0.049636342 <a title="42-tfidf-5" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>Author: Eric Perrier, Seiya Imoto, Satoru Miyano</p><p>Abstract: Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. However, a recent empirical study has shown that a hybrid algorithm improves sensitively accuracy and speed: it learns a skeleton with an independency test (IT) approach and constrains on the directed acyclic graphs (DAG) considered during the search-and-score phase. Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(γm n ), where γm < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree m and sparse structures ˜ allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even ﬁnds more accurate results when given a sound super-structure. Practically, S can be approximated by IT approaches; signiﬁcance level of the tests controls its sparseness, enabling to control the trade-off between speed and accuracy. For incomplete super-structures, a greedily post-processed version (COS+) still enables to signiﬁcantly outperform other heuristic searches. Keywords: subset Bayesian networks, structure learning, optimal search, super-structure, connected</p><p>6 0.046686091 <a title="42-tfidf-6" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>7 0.04615961 <a title="42-tfidf-7" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>8 0.032835126 <a title="42-tfidf-8" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>9 0.03237924 <a title="42-tfidf-9" href="./jmlr-2008-Minimal_Nonlinear_Distortion_Principle_for_Nonlinear_Independent_Component_Analysis.html">60 jmlr-2008-Minimal Nonlinear Distortion Principle for Nonlinear Independent Component Analysis</a></p>
<p>10 0.030822387 <a title="42-tfidf-10" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>11 0.028677246 <a title="42-tfidf-11" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>12 0.028423861 <a title="42-tfidf-12" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>13 0.027199335 <a title="42-tfidf-13" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>14 0.027124798 <a title="42-tfidf-14" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>15 0.02711506 <a title="42-tfidf-15" href="./jmlr-2008-On_the_Suitable_Domain_for_SVM_Training_in_Image_Coding.html">73 jmlr-2008-On the Suitable Domain for SVM Training in Image Coding</a></p>
<p>16 0.026482258 <a title="42-tfidf-16" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>17 0.026206851 <a title="42-tfidf-17" href="./jmlr-2008-Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">33 jmlr-2008-Evidence Contrary to the Statistical View of Boosting</a></p>
<p>18 0.0260939 <a title="42-tfidf-18" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>19 0.025665941 <a title="42-tfidf-19" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>20 0.023570515 <a title="42-tfidf-20" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.126), (1, 0.018), (2, 0.03), (3, -0.011), (4, -0.088), (5, 0.07), (6, 0.027), (7, -0.002), (8, -0.011), (9, 0.021), (10, -0.067), (11, 0.061), (12, -0.073), (13, -0.014), (14, -0.018), (15, -0.147), (16, 0.105), (17, -0.172), (18, 0.128), (19, -0.037), (20, 0.266), (21, 0.061), (22, 0.032), (23, 0.116), (24, -0.226), (25, 0.048), (26, 0.027), (27, 0.087), (28, -0.183), (29, -0.114), (30, -0.009), (31, -0.244), (32, -0.06), (33, 0.032), (34, -0.157), (35, -0.084), (36, 0.045), (37, -0.016), (38, -0.031), (39, -0.068), (40, -0.061), (41, -0.018), (42, 0.164), (43, 0.326), (44, 0.279), (45, -0.252), (46, -0.19), (47, -0.049), (48, -0.007), (49, 0.116)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95604926 <a title="42-lsi-1" href="./jmlr-2008-HPB%3A_A_Model_for_Handling_BN_Nodes_with_High_Cardinality_Parents.html">42 jmlr-2008-HPB: A Model for Handling BN Nodes with High Cardinality Parents</a></p>
<p>Author: Jorge Jambeiro Filho, Jacques Wainer</p><p>Abstract: We replaced the conditional probability tables of Bayesian network nodes whose parents have high cardinality with a multilevel empirical hierarchical Bayesian model called hierarchical pattern Bayes (HPB).1 The resulting Bayesian networks achieved signiÄ?Ĺš cant performance improvements over Bayesian networks with the same structure and traditional conditional probability tables, over Bayesian networks with simpler structures like naĂ&sbquo;Â¨ve Bayes and tree augmented naĂ&sbquo;Â¨ve Bayes, over Ă&bdquo;Ä&hellip; Ă&bdquo;Ä&hellip; Bayesian networks where traditional conditional probability tables were substituted by noisy-OR gates, default tables, decision trees and decision graphs and over Bayesian networks constructed after a cardinality reduction preprocessing phase using the agglomerative information bottleneck method. Our main tests took place in important fraud detection domains, which are characterized by the presence of high cardinality attributes and by the existence of relevant interactions among them. Other tests, over UCI data sets, show that HPB may have a quite wide applicability. Keywords: probabilistic reasoning, Bayesian networks, smoothing, hierarchical Bayes, empirical Bayes</p><p>2 0.27085683 <a title="42-lsi-2" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>Author: Eric Perrier, Seiya Imoto, Satoru Miyano</p><p>Abstract: Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. However, a recent empirical study has shown that a hybrid algorithm improves sensitively accuracy and speed: it learns a skeleton with an independency test (IT) approach and constrains on the directed acyclic graphs (DAG) considered during the search-and-score phase. Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(γm n ), where γm < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree m and sparse structures ˜ allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even ﬁnds more accurate results when given a sound super-structure. Practically, S can be approximated by IT approaches; signiﬁcance level of the tests controls its sparseness, enabling to control the trade-off between speed and accuracy. For incomplete super-structures, a greedily post-processed version (COS+) still enables to signiﬁcantly outperform other heuristic searches. Keywords: subset Bayesian networks, structure learning, optimal search, super-structure, connected</p><p>3 0.25193766 <a title="42-lsi-3" href="./jmlr-2008-Generalization_from_Observed_to_Unobserved_Features_by_Clustering.html">38 jmlr-2008-Generalization from Observed to Unobserved Features by Clustering</a></p>
<p>Author: Eyal Krupka, Naftali Tishby</p><p>Abstract: We argue that when objects are characterized by many attributes, clustering them on the basis of a random subset of these attributes can capture information on the unobserved attributes as well. Moreover, we show that under mild technical conditions, clustering the objects on the basis of such a random subset performs almost as well as clustering with the full attribute set. We prove ﬁnite sample generalization theorems for this novel learning scheme that extends analogous results from the supervised learning setting. We use our framework to analyze generalization to unobserved features of two well-known clustering algorithms: k-means and the maximum likelihood multinomial mixture model. The scheme is demonstrated for collaborative ﬁltering of users with movie ratings as attributes and document clustering with words as attributes. Keywords: clustering, unobserved features, learning theory, generalization in clustering, information bottleneck</p><p>4 0.21392503 <a title="42-lsi-4" href="./jmlr-2008-Minimal_Nonlinear_Distortion_Principle_for_Nonlinear_Independent_Component_Analysis.html">60 jmlr-2008-Minimal Nonlinear Distortion Principle for Nonlinear Independent Component Analysis</a></p>
<p>Author: Kun Zhang, Laiwan Chan</p><p>Abstract: It is well known that solutions to the nonlinear independent component analysis (ICA) problem are highly non-unique. In this paper we propose the “minimal nonlinear distortion” (MND) principle for tackling the ill-posedness of nonlinear ICA problems. MND prefers the nonlinear ICA solution with the estimated mixing procedure as close as possible to linear, among all possible solutions. It also helps to avoid local optima in the solutions. To achieve MND, we exploit a regularization term to minimize the mean square error between the nonlinear mixing mapping and the best-ﬁtting linear one. The effect of MND on the inherent trivial and non-trivial indeterminacies in nonlinear ICA solutions is investigated. Moreover, we show that local MND is closely related to the smoothness regularizer penalizing large curvature, which provides another useful regularization condition for nonlinear ICA. Experiments on synthetic data show the usefulness of the MND principle for separating various nonlinear mixtures. Finally, as an application, we use nonlinear ICA with MND to separate daily returns of a set of stocks in Hong Kong, and the linear causal relations among them are successfully discovered. The resulting causal relations give some interesting insights into the stock market. Such a result can not be achieved by linear ICA. Simulation studies also verify that when doing causality discovery, sometimes one should not ignore the nonlinear distortion in the data generation procedure, even if it is weak. Keywords: nonlinear ICA, regularization, minimal nonlinear distortion, mean square error, best linear reconstruction</p><p>5 0.21324719 <a title="42-lsi-5" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>Author: Amit Dhurandhar, Alin Dobra</p><p>Abstract: In this paper we use the methodology introduced by Dhurandhar and Dobra (2009) for analyzing the error of classiﬁers and the model selection measures, to analyze decision tree algorithms. The methodology consists of obtaining parametric expressions for the moments of the generalization error (GE) for the classiﬁcation model of interest, followed by plotting these expressions for interpretability. The major challenge in applying the methodology to decision trees, the main theme of this work, is customizing the generic expressions for the moments of GE to this particular classiﬁcation algorithm. The speciﬁc contributions we make in this paper are: (a) we primarily characterize a subclass of decision trees namely, Random decision trees, (b) we discuss how the analysis extends to other decision tree algorithms and (c) in order to extend the analysis to certain model selection measures, we generalize the relationships between the moments of GE and moments of the model selection measures given in (Dhurandhar and Dobra, 2009) to randomized classiﬁcation algorithms. An empirical comparison of the proposed method with Monte Carlo and distribution free bounds obtained using Breiman’s formula, depicts the advantages of the method in terms of running time and accuracy. It thus showcases the use of the deployed methodology as an exploratory tool to study learning algorithms. Keywords: moments, generalization error, decision trees</p><p>6 0.21244206 <a title="42-lsi-6" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>7 0.1958013 <a title="42-lsi-7" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>8 0.17495799 <a title="42-lsi-8" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>9 0.16364193 <a title="42-lsi-9" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>10 0.14461295 <a title="42-lsi-10" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>11 0.12588936 <a title="42-lsi-11" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>12 0.12214751 <a title="42-lsi-12" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>13 0.11526916 <a title="42-lsi-13" href="./jmlr-2008-Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">33 jmlr-2008-Evidence Contrary to the Statistical View of Boosting</a></p>
<p>14 0.10524912 <a title="42-lsi-14" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>15 0.10116523 <a title="42-lsi-15" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>16 0.091800787 <a title="42-lsi-16" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>17 0.086584941 <a title="42-lsi-17" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>18 0.086515129 <a title="42-lsi-18" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>19 0.085913911 <a title="42-lsi-19" href="./jmlr-2008-Stationary_Features_and_Cat_Detection.html">87 jmlr-2008-Stationary Features and Cat Detection</a></p>
<p>20 0.082533412 <a title="42-lsi-20" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.017), (5, 0.025), (31, 0.016), (40, 0.027), (54, 0.038), (58, 0.026), (66, 0.043), (69, 0.451), (76, 0.048), (88, 0.062), (92, 0.039), (94, 0.051), (99, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76221496 <a title="42-lda-1" href="./jmlr-2008-HPB%3A_A_Model_for_Handling_BN_Nodes_with_High_Cardinality_Parents.html">42 jmlr-2008-HPB: A Model for Handling BN Nodes with High Cardinality Parents</a></p>
<p>Author: Jorge Jambeiro Filho, Jacques Wainer</p><p>Abstract: We replaced the conditional probability tables of Bayesian network nodes whose parents have high cardinality with a multilevel empirical hierarchical Bayesian model called hierarchical pattern Bayes (HPB).1 The resulting Bayesian networks achieved signiÄ?Ĺš cant performance improvements over Bayesian networks with the same structure and traditional conditional probability tables, over Bayesian networks with simpler structures like naĂ&sbquo;Â¨ve Bayes and tree augmented naĂ&sbquo;Â¨ve Bayes, over Ă&bdquo;Ä&hellip; Ă&bdquo;Ä&hellip; Bayesian networks where traditional conditional probability tables were substituted by noisy-OR gates, default tables, decision trees and decision graphs and over Bayesian networks constructed after a cardinality reduction preprocessing phase using the agglomerative information bottleneck method. Our main tests took place in important fraud detection domains, which are characterized by the presence of high cardinality attributes and by the existence of relevant interactions among them. Other tests, over UCI data sets, show that HPB may have a quite wide applicability. Keywords: probabilistic reasoning, Bayesian networks, smoothing, hierarchical Bayes, empirical Bayes</p><p>2 0.72708875 <a title="42-lda-2" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>Author: Abraham George, Warren B. Powell, Sanjeev R. Kulkarni</p><p>Abstract: We consider the problem of estimating the value of a multiattribute resource, where the attributes are categorical or discrete in nature and the number of potential attribute vectors is very large. The problem arises in approximate dynamic programming when we need to estimate the value of a multiattribute resource from estimates based on Monte-Carlo simulation. These problems have been traditionally solved using aggregation, but choosing the right level of aggregation requires resolving the classic tradeoff between aggregation error and sampling error. We propose a method that estimates the value of a resource at different levels of aggregation simultaneously, and then uses a weighted combination of the estimates. Using the optimal weights, which minimizes the variance of the estimate while accounting for correlations between the estimates, is computationally too expensive for practical applications. We have found that a simple inverse variance formula (adjusted for bias), which effectively assumes the estimates are independent, produces near-optimal estimates. We use the setting of two levels of aggregation to explain why this approximation works so well. Keywords: hierarchical statistics, approximate dynamic programming, mixture models, adaptive learning, multiattribute resources</p><p>3 0.2510483 <a title="42-lda-3" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>4 0.2460217 <a title="42-lda-4" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>Author: Gal Elidan, Stephen Gould</p><p>Abstract: With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufﬁciently expressive for generalization while at the same time allow for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overﬁtting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modiﬁcations and that is polynomial both in the size of the graph and the treewidth bound. At the heart of our method is a dynamic triangulation that we update in a way that facilitates the addition of chain structures that increase the bound on the model’s treewidth by at most one. We demonstrate the effectiveness of our “treewidth-friendly” method on several real-life data sets and show that it is superior to the greedy approach as soon as the bound on the treewidth is nontrivial. Importantly, we also show that by making use of global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth. Keywords: Bayesian networks, structure learning, model selection, bounded treewidth</p><p>5 0.24411917 <a title="42-lda-5" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>Author: Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, Peter L. Bartlett</p><p>Abstract: Log-linear and maximum-margin models are two commonly-used methods in supervised machine learning, and are frequently used in structured prediction problems. Efﬁcient learning of parameters in these models is therefore an important problem, and becomes a key factor when learning from very large data sets. This paper describes exponentiated gradient (EG) algorithms for training such models, where EG updates are applied to the convex dual of either the log-linear or maxmargin objective function; the dual in both the log-linear and max-margin cases corresponds to minimizing a convex function with simplex constraints. We study both batch and online variants of the algorithm, and provide rates of convergence for both cases. In the max-margin case, O( 1 ) EG ε updates are required to reach a given accuracy ε in the dual; in contrast, for log-linear models only O(log( 1 )) updates are required. For both the max-margin and log-linear cases, our bounds suggest ε that the online EG algorithm requires a factor of n less computation to reach a desired accuracy than the batch EG algorithm, where n is the number of training examples. Our experiments conﬁrm that the online algorithms are much faster than the batch algorithms in practice. We describe how the EG updates factor in a convenient way for structured prediction problems, allowing the algorithms to be efﬁciently applied to problems such as sequence learning or natural language parsing. We perform extensive evaluation of the algorithms, comparing them to L-BFGS and stochastic gradient descent for log-linear models, and to SVM-Struct for max-margin models. The algorithms are applied to a multi-class problem as well as to a more complex large-scale parsing task. In all these settings, the EG algorithms presented here outperform the other methods. Keywords: exponentiated gradient, log-linear models, maximum-margin models, structured prediction, conditional random ﬁelds ∗. These authors contributed equally. c 2008 Michael Col</p><p>6 0.24062017 <a title="42-lda-6" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>7 0.24025431 <a title="42-lda-7" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>8 0.23935178 <a title="42-lda-8" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>9 0.23842603 <a title="42-lda-9" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>10 0.2382379 <a title="42-lda-10" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>11 0.23791376 <a title="42-lda-11" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>12 0.23661894 <a title="42-lda-12" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>13 0.23606233 <a title="42-lda-13" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>14 0.23589984 <a title="42-lda-14" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>15 0.23539375 <a title="42-lda-15" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>16 0.23479716 <a title="42-lda-16" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>17 0.23477501 <a title="42-lda-17" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>18 0.23232116 <a title="42-lda-18" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>19 0.23219544 <a title="42-lda-19" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>20 0.23178987 <a title="42-lda-20" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
