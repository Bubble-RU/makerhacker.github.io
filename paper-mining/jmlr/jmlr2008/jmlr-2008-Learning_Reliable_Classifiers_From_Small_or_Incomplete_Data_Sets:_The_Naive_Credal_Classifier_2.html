<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-50" href="#">jmlr2008-50</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</h1>
<br/><p>Source: <a title="jmlr-2008-50-pdf" href="http://jmlr.org/papers/volume9/corani08a/corani08a.pdf">pdf</a></p><p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: In this paper, the naive credal classiﬁer, which is a set-valued counterpart of naive Bayes, is extended to a general and ﬂexible treatment of incomplete data, yielding a new classiﬁer called naive credal classiﬁer 2 (NCC2). The new classiﬁer delivers classiﬁcations that are reliable even in the presence of small sample sizes and missing values. Extensive empirical evaluations show that, by issuing set-valued classiﬁcations, NCC2 is able to isolate and properly deal with instances that are hard to classify (on which naive Bayes accuracy drops considerably), and to perform as well as naive Bayes on the other instances. The experiments point to a general problem: they show that with missing values, empirical evaluations may not reliably estimate the accuracy of a traditional classiﬁer, such as naive Bayes. This phenomenon adds even more value to the robust approach to classiﬁcation implemented by NCC2. Keywords: naive Bayes, naive credal classiﬁer, imprecise probabilities, missing values, conservative inference rule, missing at random</p><p>Reference: <a title="jmlr-2008-50-reference" href="../jmlr2008_reference/jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The new classiﬁer delivers classiﬁcations that are reliable even in the presence of small sample sizes and missing values. [sent-4, score-0.114]
</p><p>2 Extensive empirical evaluations show that, by issuing set-valued classiﬁcations, NCC2 is able to isolate and properly deal with instances that are hard to classify (on which naive Bayes accuracy drops considerably), and to perform as well as naive Bayes on the other instances. [sent-5, score-0.297]
</p><p>3 The experiments point to a general problem: they show that with missing values, empirical evaluations may not reliably estimate the accuracy of a traditional classiﬁer, such as naive Bayes. [sent-6, score-0.248]
</p><p>4 Keywords: naive Bayes, naive credal classiﬁer, imprecise probabilities, missing values, conservative inference rule, missing at random  1. [sent-8, score-0.741]
</p><p>5 1 In fact, there are at least two kinds of ignorance involved in the process of learning from data. [sent-11, score-0.129]
</p><p>6 The ﬁrst is prior ignorance about the domain, as we are assuming that data are our only source of information. [sent-12, score-0.172]
</p><p>7 The second is ignorance arising from missing values, as data are often incomplete; in this case, ignorance is about the process that originates the missing values: that is, the missingness process. [sent-13, score-0.539]
</p><p>8 Considering Bayesian classiﬁers, we see that prior ignorance is modeled in common practice by so-called non-informative prior densities (or just priors, for short). [sent-16, score-0.247]
</p><p>9 Indeed, full ignorance is not compatible with learning, as it is well known (e. [sent-19, score-0.129]
</p><p>10 This appears to indicate that non-informative priors do not model prior ignorance satisfactorily. [sent-27, score-0.207]
</p><p>11 A more objective-minded2 model of prior ignorance has been proposed through a classiﬁer called naive credal classiﬁer (NCC), see Zaffalon (2001), which is an extension of naive Bayes classiﬁer (NBC) to imprecise probabilities (Walley, 1991). [sent-28, score-0.671]
</p><p>12 NCC models prior ignorance by a set of prior densities (also called prior credal set), which is turned into a set of posteriors by elementwise application of Bayes’ rule. [sent-29, score-0.477]
</p><p>13 The classiﬁcation is eventually issued by returning all the classes that are non-dominated by any other class according to the posterior credal set, where class c i is said to dominate c j if for all the posteriors it holds that the probability of ci is larger than that of c j . [sent-30, score-0.26]
</p><p>14 , classiﬁcations made by more than one class) when faced with instances that are hard to classify, due to a combination of prior ignorance and poor information about those speciﬁc instances in the learning set. [sent-33, score-0.24]
</p><p>15 , 2003), thus demonstrating the usefulness, for classiﬁcation purposes, of modeling prior ignorance via a credal set. [sent-36, score-0.337]
</p><p>16 Similarly, we say that a classiﬁer is determinate when it outputs a single class and indeterminate otherwise. [sent-39, score-0.087]
</p><p>17 As for the ignorance arising from missing data, we can think of the missingness process (MP) as a process that takes in input the complete data, which we cannot usually observe, and outputs the incomplete data, which we do observe. [sent-40, score-0.339]
</p><p>18 If data are our only source of information, we are ignorant about the MP because it is usually not possible to learn how it operates, from the observed, incomplete data. [sent-41, score-0.059]
</p><p>19 In common practice, missing values are often ignored; this entails the idea that the MP is nonselective in producing them, or, in other words, that it is a missing at random (MAR) process (Little and Rubin, 1987; Jaeger, 2005). [sent-42, score-0.228]
</p><p>20 In its original formulation, NCC introduced also an initial attempt to deal with ignorance about the MP. [sent-44, score-0.129]
</p><p>21 The idea was to model ignorance about it by using a set of likelihoods: a likelihood per each complete learning set consistent with the incomplete one. [sent-45, score-0.172]
</p><p>22 Furthermore, their treatment of missing values rests on intuitive arguments rather than on a principled derivation. [sent-48, score-0.114]
</p><p>23 ” We do so to stress that using (very) weak assumptions leads to results that are much more determined by the data than by our prior beliefs (intended in a loose way, not only as prior probabilities), and in this sense are more objective. [sent-51, score-0.105]
</p><p>24 We call the resulting classiﬁer naive credal classiﬁer 2 (NCC2), in order to emphasize the advancement made to deal with incomplete data, while keeping the original beneﬁts of NCC on the front of prior ignorance. [sent-56, score-0.351]
</p><p>25 This is a key characteristic of NCC2: in fact, if the MP is unknown, it may well change its behavior from unit to unit for all we know (i. [sent-59, score-0.058]
</p><p>26 The development of NCC2 is based on a recently derived so-called conservative inference rule (CIR) to compute (imprecise) conditional expectations with incomplete data (Zaffalon, 2005b). [sent-62, score-0.08]
</p><p>27 After giving some notation and brieﬂy recalling CIR in Section 2, we derive NCC2 in Section 3 by specializing CIR to the case of naive classiﬁcation. [sent-63, score-0.1]
</p><p>28 The analysis turns out to be particularly meaningful when we compare NCC2 with its precise-probability counterpart, that is, naive Bayes. [sent-68, score-0.1]
</p><p>29 We do this by evaluating the accuracy of NBC on the instances of the test set where NCC2 issues a determinate classiﬁcation separately from those where it does not. [sent-69, score-0.104]
</p><p>30 In fact, NCC2 is indeterminate on an instance when it deems that there is not enough knowledge in the learning set to make a determinate classiﬁcation reliably; NBC, on the other hand, issues a determinate classiﬁcation on such an instance (as well as on any other). [sent-70, score-0.157]
</p><p>31 And this is indeed the case: the experiments show that NBC undergoes a major drop in accuracy moving from the instances classiﬁed in a determinate way by NCC2 to the indeterminate ones. [sent-72, score-0.159]
</p><p>32 These instances are precisely those that are hard to classify and that NCC2 isolates by delivering set-valued classiﬁcations. [sent-75, score-0.063]
</p><p>33 , the proportion of times the true class is contained in the output set) is often similar to the accuracy obtained on the instances classiﬁed in a determinate way. [sent-78, score-0.104]
</p><p>34 To make our results stronger, we have also investigated whether NBC could take advantage of the posterior probabilities it computes in order to deal more successfully with the hard instances. [sent-80, score-0.053]
</p><p>35 583  C ORANI AND Z AFFALON  We have considered such posterior probabilities again by separating the cases where NCC2 is determinate from the others, and by comparing those probabilities with the measured accuracy that NBC actually achieves on the data. [sent-81, score-0.146]
</p><p>36 What we show is that the NBC probabilities are (also very) unreliable on the instances that are hard to classify, as isolated by NCC2, and deﬁnitely more unreliable than on the remaining instances. [sent-82, score-0.117]
</p><p>37 In other words, we observe another kind of drop that now is related to the quality of the posterior probabilities computed by NBC. [sent-83, score-0.075]
</p><p>38 Overall, we show that NBC may well be too optimistic in dealing with small data sets and missing data, thus yielding unreliable predictions. [sent-84, score-0.144]
</p><p>39 It is useful to recall that NBC is known to be very robust to missing data. [sent-85, score-0.114]
</p><p>40 Therefore, it is not unlikely that the optimism on the front of missing data is even greater with more complex classiﬁers. [sent-86, score-0.114]
</p><p>41 At the same time, and in contrast with naive Bayes, our experiments show that NCC2 may sometimes be too pessimistic (i. [sent-88, score-0.1]
</p><p>42 This happens because by construction NCC2 implicitly considers the worst possible MP to have acted on the non-MAR part of the data, and in some cases this hypothesis may be too far from the MP that has actually produced the missing values. [sent-91, score-0.114]
</p><p>43 Even more important, we show that such an unreliability cannot be uncovered by making empirical evaluations: despite the predictive accuracy of NBC on a certain instance is measured properly by cross-validation, the actual accuracy on new instances of the same type can be signiﬁcantly worse. [sent-96, score-0.066]
</p><p>44 This highlights the fact that modeling ignorance properly is important, even if there are data available for empirical evaluations. [sent-97, score-0.129]
</p><p>45 This is not to say that one should abuse assuming ignorance about the MP: when there are many missing values, especially in the instance to classify, one would obtain conclusions much too weak. [sent-99, score-0.243]
</p><p>46 The conclusion we achieve in this case is that the use of coarsened rather than missing observations is another very effective means to incorporate knowledge, in case there is no support for declaring some variables as subject to a MAR MP. [sent-107, score-0.129]
</p><p>47 A given manifest value is identical to the corresponding latent one, unless the 584  T HE NAIVE C REDAL C LASSIFIER 2  latent value has been turned into missing by the MP; in this case, the manifest value is actually the symbol of missing value. [sent-110, score-0.448]
</p><p>48 Therefore, in a case where the data at hand are complete, the instances of the latent and the manifest variables coincide. [sent-111, score-0.144]
</p><p>49 , aMr ˆ ˆ x+ ˆ  Figure 1: Graphical representation of some vectors of latent variables. [sent-202, score-0.066]
</p><p>50 , a certain row) of the data set: the learning set (or training set) is made up of the units for which 1 ≤ i ≤ N, while the unit to classify (not belonging to the learning set) is indexed by M := N + 1. [sent-209, score-0.099]
</p><p>51 A set of units to classify is referred to as test set. [sent-210, score-0.07]
</p><p>52 We denote: (i) the latent class variable as Ci , and we assume that it is always observed; (ii) the latent attribute variables affected by an unknown MP (i. [sent-212, score-0.17]
</p><p>53 ,Aik ; (iii) the latent attributes affected by a MAR MP as Ai1 , . [sent-217, score-0.084]
</p><p>54 4 ˆ For all i, Ci takes generic value ci in the ﬁnite set C, called set of latent classes, while Ai j (Ail ) ˆ l ), called sets of latent attributes. [sent-222, score-0.175]
</p><p>55 take generic values a j (al ) in the ﬁnite sets A j (A ˆ ˆ We deﬁne the following groups of latent variables: Xi := (Ai1 , . [sent-223, score-0.066]
</p><p>56 plete learning sets, while those of (XM , X To complete the notation regarding latent variables, we assume that the generic latent unit ˆ (d, x) ∈ D × X is generated in independently and identically distributed (IID) way according to ˆ the aleatory probability (or chance) ϑ(d,x) . [sent-262, score-0.161]
</p><p>57 Knowledge about θ is expressed by p(θ), which denotes an imprecise prior density for θ. [sent-265, score-0.154]
</p><p>58 This means that p(θ) is known to belong to a non-empty set P(θ) of precise prior densities for θ. [sent-266, score-0.103]
</p><p>59 As for the manifest variables, we assume that we either observe a precise value, or we do not observe it at all. [sent-268, score-0.072]
</p><p>60 Manifest variables are denoted by the letter O followed by the latent variable they refer to, written as a subscript. [sent-269, score-0.066]
</p><p>61 We deﬁne hence the following manifest variables: O := O D = ˆ ˆ ˆ (O1 , . [sent-270, score-0.044]
</p><p>62 To this extent, a traditional probabilistic classiﬁer outputs what it deems to be the optimal prediction: that is, the class with the highest probability (in the case of 0-1 loss function) on the basis of a uniquely computed posterior density. [sent-291, score-0.046]
</p><p>63 In the imprecise setting, however, the optimality criterion has to be extended to manage a set of posterior densities (derived from a set of priors and a set of likelihoods), instead of a single posterior; in particular, according to Section 3. [sent-292, score-0.225]
</p><p>64 2 of Walley (1991), the optimality criterion in the imprecise setting prescribes to return the non-dominated classes. [sent-294, score-0.14]
</p><p>65 The deﬁnition of dominance is as follows: class ci dominates c j if for all the computed posteriors densities, the posterior probability of ci is greater than that of c j ; clearly, c j is non-dominated if no class dominates c j . [sent-295, score-0.179]
</p><p>66 Observe that, as a result of the uncertainty arising from both prior speciﬁcation and non-MAR missing values, there can be several non-dominated classes; in this case, the classiﬁer returns an indeterminate (or set-valued) classiﬁcation. [sent-297, score-0.206]
</p><p>67 Classiﬁers that issue set-valued classiﬁcations are called credal classiﬁers by Zaffalon (2002). [sent-298, score-0.165]
</p><p>68 In other words, credal classiﬁers are models that allow us to drop the dominated classes, as sub-optimal, and to express our indecision about the optimal class by yielding the remaining set of non-dominated classes. [sent-300, score-0.187]
</p><p>69 If we exclude the classes that are non-dominated due to indifference rather than incomparability, and that constitute a very special case in the imprecise setting. [sent-305, score-0.111]
</p><p>70 - + ˆ ˆ+ p(θ)∈P(θ) p(cM |d , x ∈ o ) inf  (1)  Actually, Equation (1) is the general form of the test of dominance for any classiﬁer based on the conservative inference rule presented in Zaffalon (2005b); CIR is a conditioning rule (i. [sent-307, score-0.107]
</p><p>71 CIR can be regarded as unifying two rules (Zaffalon, 2005b): a conservative learning rule, which prescribes how to learn the classiﬁer from an incomplete training set, and a conservative updating rule, which prescribes how to classify a novel instance that contains missing values. [sent-312, score-0.336]
</p><p>72 The inner loop, which minimizes over the prior credal set, is common to both learning and updating rules. [sent-314, score-0.208]
</p><p>73 NCC2 specializes the test of Equation (1) to the case of naive classiﬁcation. [sent-315, score-0.119]
</p><p>74 In the following, we will move from the precise setting (corresponding in fact to naive Bayes) to NCC2 in four steps: in Section 3. [sent-316, score-0.128]
</p><p>75 4 we ﬁnally relax the assumptions of completeness about non-MAR data in learning and testing respectively, thus managing a set of likelihoods and instances to classify. [sent-320, score-0.051]
</p><p>76 In practice, this corresponds to the naive Bayes setting, with the difference that we explicitly separate variables affected by the MAR MP and the unknown MP. [sent-327, score-0.118]
</p><p>77 e, the assumption of mutual independence ˆ ˆ of the latent attribute variables Ai1 , . [sent-329, score-0.086]
</p><p>78 ˆ ˆ By sticking to the naive hypothesis, the likelihood can be expressed as follows: Lemma 1 n(a ,c) n(a ,c) ˆ n(c) p(d, x ∈ o|ϑ) = ∏c∈C {ϑc [∏k ∏a j ∈A j ϑa j |cj ][∏r ∏al ∈Al ϑal |cl ]}. [sent-342, score-0.1]
</p><p>79 of joint occurrences of (a j , c) in d, and n(al , c) denotes the number of joint occurrences of (al , c) in the learning set after dropping ˆ ˆ ˆ the units with missing values of Al . [sent-345, score-0.155]
</p><p>80 With similar arguments to those used with Lemma 1, and assuming that the MAR attribute variables have been re-ordered so as to index the non-missing ones in the instance to classify from 1 to r ≤ r, we obtain: Lemma 2 p(cM , xM , xM ∈ oM |ϑ) = ϑcM ∏k ϑaM j |cM ∏r ϑaMl |cM . [sent-347, score-0.049]
</p><p>81 ˆ ˆ j=1 l=1 ˆ Note that restricting the second product between l = 1 and l = r prevents the inclusion in the expression of the attributes that are missing in the unit to classify. [sent-348, score-0.143]
</p><p>82 The real hyperparameter t(·) can instead be regarded as the proportion of units in which the variables in question take certain values (e. [sent-356, score-0.059]
</p><p>83 The credal set ˆ ˆ P(θ) is deﬁned as the set of all the precise priors that satisfy these constraints. [sent-362, score-0.228]
</p><p>84 The construction of P(θ) is similar to the approach implemented by Walleys’ imprecise Dirichlet model (Walley, 1996). [sent-363, score-0.111]
</p><p>85 Then, our ignorance about this process is modeled by considering the set of all the states of information about the process, that is, by considering that all of them are actually possible. [sent-365, score-0.129]
</p><p>86 , r ); ˆ ˆ ˆ ˆ ˆ • nl (cM ) := ∑al ∈Al n(al , cM ). [sent-376, score-0.058]
</p><p>87 ˆ ˆ ˆ Note that MAR attributes that are missing in the unit to be classiﬁed do not affect the posterior probabilities of the class. [sent-377, score-0.196]
</p><p>88 2 Credal Dominance Tests with an Imprecise Prior Let us now address the computation of the inner optimization problem in (1), under the choice of the imprecise prior made in Section 3. [sent-381, score-0.154]
</p><p>89 Lemma 4 Consider the problem inf p(θ)∈P(θ) p(cM |d - , x+ ∈ o+ )/p(cM |d - , x+ ∈ o+ ), with the set of ˆ ˆ ˆ ˆ prior densities described in Section 3. [sent-384, score-0.104]
</p><p>90 Such a problem is equivalent to the following: inf  {[  0 <1 r  · ∏[ l=1  =:  n(aM j , cM ) n(cM ) + st(cM ) k−1 k ] ∏ n(cM ) + s − t(cM ) j=1 n(aM j , cM ) + st(cM )  nl (cM ) + st(cM ) n(aMl , cM ) ˆ · ]} nl (cM ) + s − t(cM ) n(aMl , cM ) + st(cM ) ˆ inf  0 <1 h(t(cM )) := h(0). [sent-387, score-0.174]
</p><p>91 If (ln h(t(cM ))) |t(cM )=1 ≤ 0, let inf0 <1 r  · ∏[ l=1  n(aM j , cM ) n(cM ) + st(cM ) k−1 k ] ∏ · n(cM ) + s − st(cM ) j=1 n(aM j , cM ) + st(cM )  nl (cM ) + st(cM ) n(aMl , cM ) ˆ · ]}. [sent-390, score-0.058]
</p><p>92 nl (cM ) + s − st(cM ) n(aMl , cM ) + st(cM ) ˆ  The problem can be then re-written as follows: inf  min{[  0 <1 d∈o r  · ∏[ l=1  n(aM j , cM ) n(cM ) + st(cM ) k−1 k ] ∏ · n(cM ) + s − st(cM ) n(aM j , cM ) + st(cM ) j=1  n(aMl , cM ) ˆ nl (cM ) + st(cM ) · ]}. [sent-391, score-0.145]
</p><p>93 nl (cM ) + s − st(cM ) n(aMl , cM ) + st(cM ) ˆ  (14)  This is obtained by inverting the order of the optimizations and using Expression (6). [sent-392, score-0.058]
</p><p>94 Consider a value this is the lower envelope of the set of functions x such that the lower envelope coincides with two different functions in F e before and after x. [sent-400, score-0.082]
</p><p>95 In other words, the set of points where any two different functions in Fe cross (let us call them partition points), contains the points where the function matched by the lower envelope changes. [sent-402, score-0.093]
</p><p>96 20 The crucial observation here is that the lower envelope matches a single function of F e in any sub-interval of (0, s) that does not contain partition points; and we are always able to select an element of Ae that gives rise to the matched function. [sent-409, score-0.093]
</p><p>97 In other words, the partition points can be used to deﬁne a partition of (0, s), in the sub-intervals of which the lower envelop matches a single function that we are able to characterize by an element of A e . [sent-410, score-0.068]
</p><p>98 Note that some partition points may be such that the lower envelope does not change the matched function in F e , and so they could be discarded. [sent-419, score-0.093]
</p><p>99 The ﬁrst step of the procedure builds the set of partition points for the functions in F e , and the next one deﬁnes the partition of (0, s). [sent-426, score-0.068]
</p><p>100 Reliable diagnoses of dementia by the naive credal classiﬁer inferred from incomplete cognitive data. [sent-1198, score-0.308]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cm', 0.814), ('ame', 0.222), ('credal', 0.165), ('aml', 0.146), ('ignorance', 0.129), ('st', 0.122), ('nbc', 0.115), ('missing', 0.114), ('mp', 0.111), ('imprecise', 0.111), ('zaffalon', 0.107), ('mar', 0.106), ('naive', 0.1), ('al', 0.081), ('ncc', 0.078), ('affalon', 0.066), ('orani', 0.066), ('redal', 0.066), ('latent', 0.066), ('cir', 0.058), ('nl', 0.058), ('lassifier', 0.058), ('determinate', 0.054), ('xm', 0.051), ('om', 0.049), ('walley', 0.049), ('manifest', 0.044), ('ae', 0.044), ('incomplete', 0.043), ('ci', 0.043), ('prior', 0.043), ('units', 0.041), ('dominance', 0.041), ('envelope', 0.041), ('classi', 0.037), ('missingness', 0.037), ('conservative', 0.037), ('priors', 0.035), ('instances', 0.034), ('partition', 0.034), ('indeterminate', 0.033), ('ail', 0.033), ('densities', 0.032), ('cations', 0.032), ('dirichlet', 0.031), ('posterior', 0.03), ('fe', 0.03), ('unreliable', 0.03), ('credible', 0.029), ('prescribes', 0.029), ('unit', 0.029), ('inf', 0.029), ('classify', 0.029), ('precise', 0.028), ('er', 0.027), ('probabilities', 0.023), ('bayes', 0.023), ('posteriors', 0.022), ('idsia', 0.022), ('giorgio', 0.022), ('drop', 0.022), ('attribute', 0.02), ('air', 0.02), ('amk', 0.019), ('amr', 0.019), ('anr', 0.019), ('ignorability', 0.019), ('isipta', 0.019), ('mps', 0.019), ('ncci', 0.019), ('nccmari', 0.019), ('ox', 0.019), ('peculiar', 0.019), ('pendigits', 0.019), ('prescribing', 0.019), ('ramoni', 0.019), ('rior', 0.019), ('seidenfeld', 0.019), ('specializes', 0.019), ('beliefs', 0.019), ('ai', 0.019), ('affected', 0.018), ('evaluations', 0.018), ('regarded', 0.018), ('matched', 0.018), ('likelihoods', 0.017), ('manage', 0.017), ('ecoli', 0.016), ('haberman', 0.016), ('nursery', 0.016), ('spambase', 0.016), ('ignorant', 0.016), ('ank', 0.016), ('corani', 0.016), ('deems', 0.016), ('spect', 0.016), ('accuracy', 0.016), ('arising', 0.016), ('declaring', 0.015), ('manno', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="50-tfidf-1" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: In this paper, the naive credal classiﬁer, which is a set-valued counterpart of naive Bayes, is extended to a general and ﬂexible treatment of incomplete data, yielding a new classiﬁer called naive credal classiﬁer 2 (NCC2). The new classiﬁer delivers classiﬁcations that are reliable even in the presence of small sample sizes and missing values. Extensive empirical evaluations show that, by issuing set-valued classiﬁcations, NCC2 is able to isolate and properly deal with instances that are hard to classify (on which naive Bayes accuracy drops considerably), and to perform as well as naive Bayes on the other instances. The experiments point to a general problem: they show that with missing values, empirical evaluations may not reliably estimate the accuracy of a traditional classiﬁer, such as naive Bayes. This phenomenon adds even more value to the robust approach to classiﬁcation implemented by NCC2. Keywords: naive Bayes, naive credal classiﬁer, imprecise probabilities, missing values, conservative inference rule, missing at random</p><p>2 0.28927562 <a title="50-tfidf-2" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: JNCC2 implements the naive credal classiﬁer 2 (NCC2). This is an extension of naive Bayes to imprecise probabilities that aims at delivering robust classiﬁcations also when dealing with small or incomplete data sets. Robustness is achieved by delivering set-valued classiﬁcations (that is, returning multiple classes) on the instances for which (i) the learning set is not informative enough to smooth the effect of choice of the prior density or (ii) the uncertainty arising from missing data prevents the reliable indication of a single class. JNCC2 is released under the GNU GPL license. Keywords: imprecise probabilities, missing data, naive Bayes, naive credal classiﬁer 2, Java</p><p>3 0.076987863 <a title="50-tfidf-3" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>Author: Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel, Daphne Koller</p><p>Abstract: We consider the problem of learning classiﬁers in structured domains, where some objects have a subset of features that are inherently absent due to complex relationships between the features. Unlike the case where a feature exists but its value is not observed, here we focus on the case where a feature may not even exist (structurally absent) for some of the samples. The common approach for handling missing features in discriminative models is to ﬁrst complete their unknown values, and then use a standard classiﬁcation procedure over the completed data. This paper focuses on features that are known to be non-existing, rather than have an unknown value. We show how incomplete data can be classiﬁed directly without any completion of the missing features using a max-margin learning framework. We formulate an objective function, based on the geometric interpretation of the margin, that aims to maximize the margin of each sample in its own relevant subspace. In this formulation, the linearly separable case can be transformed into a binary search over a series of second order cone programs (SOCP), a convex problem that can be solved efﬁciently. We also describe two approaches for optimizing the general case: an approximation that can be solved as a standard quadratic program (QP) and an iterative approach for solving the exact problem. By avoiding the pre-processing phase in which the data is completed, both of these approaches could offer considerable computational savings. More importantly, we show that the elegant handling of missing values by our approach allows it to both outperform other methods when the missing values have non-trivial structure, and be competitive with other methods when the values are missing at random. We demonstrate our results on several standard benchmarks and two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images. Keywords: max margin, missing features, network reconstruction, metabolic pa</p><p>4 0.051028091 <a title="50-tfidf-4" href="./jmlr-2008-Comments_on_the_Complete_Characterization_of_a_Family_of_Solutions_to_a_GeneralizedFisherCriterion.html">23 jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</a></p>
<p>Author: Jieping Ye</p><p>Abstract: Loog (2007) provided a complete characterization of the family of solutions to a generalized Fisher criterion. We show that this characterization is essentially equivalent to the original characterization proposed in Ye (2005). The computational advantage of the original characterization over the new one is discussed, which justiﬁes its practical use. Keywords: linear discriminant analysis, dimension reduction, linear transformation 1. Generalized Fisher Criterion For a given data set consisting of n data points {ai }n in IRd , a linear transformation G ∈ IRd× i=1 ( < d) maps each ai for 1 ≤ i ≤ n in the d-dimensional space to a vector ai in the -dimensional ˜ space as follows: G : ai ∈ IRd → ai = GT ai ∈ IR . ˜ Assume that there are k classes in the data set. The within-class scatter matrix S w , the betweenclass scatter matrix Sb , and the total scatter matrix St involved in linear discriminant analysis are deﬁned as follows (Fukunaga, 1990): k Sw = ∑ (Ai − ci eT )(Ai − ci eT )T , i=1 k Sb = ∑ ni (ci − c)(ci − c)T , i=1 k St = ∑ (Ai − ceT )(Ai − ceT )T , i=1 where Ai denotes the data matrix of the i-th class, ci = Ai e/ni is the centroid of the i-th class, ni is the sample size of the i-th class, c = Ae/n is the global centroid, and e is the vector of all ones with an appropriate length. It is easy to verify that St = Sb + Sw . In Ye (2005), the optimal transformation G is computed by maximizing a generalized Fisher criterion as follows: + G = arg max trace GT St G GT Sb G , (1) m× G∈IR c 2008 Jieping Ye. YE where M + denotes the pseudo-inverse (Golub and Van Loan, 1996) of M and it is introduced to overcome the singularity problem when dealing with high-dimensional low-sample-size data. 1.1 Equivalent Transformation Two linear transformations G1 and G2 can be considered equivalent if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. Indeed, in this case, the difference between the projections 1 2 by G1 and G2 is a mere shift. Deﬁnition 1.1 For a given data set {a1 , · · · , an }, two transformations G1 and G2 are equivalent, if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. 1 2 2. Characterization of Solutions to the Generalized Fisher Criterion Let St = UΣU T be the orthogonal eigendecomposition of St (note that St is symmetric and positive semi-deﬁnite), where U ∈ IRd×d is orthogonal and Σ ∈ IRd×d is diagonal with nonnegative diagonal entries sorted in nonincreasing order. Denote Σr as the r-th principal submatrix of Σ, where r = rank(St ). Partition U into two components as U = [U1 ,U2 ], where U1 ∈ IRd×r and U2 ∈ IRd×(d−r) . Note that r ≤ n, and for high-dimensional low-sample-size data, U1 is much smaller than U2 . In Loog (2007), a complete family of solutions S to the maximization problem in Eq. (1) is given as (We correct the error in Loog (2007) by using U instead of U T .) S= U ΛZ Y ∈ IRd× Z ∈ IR × is nonsingular , Y ∈ IR(n−r)× , where Λ ∈ IRr× maximizes the following objective function: F0 (X) = trace −1 X T Σr X T X T (U1 SbU1 )X . ˜ In Ye (2005), a family of solutions S is given as ˜ S= U ΛZ 0 ∈ IRd× Z ∈ IR × is nonsingular . The only difference between these two characterizations of solutions is the matrix Y in S , which is ˜ replaced by the zero matrix in S . We show in the next section the equivalence relationship between these two characterizations. 3. Equivalent Solution Characterizations ˜ Consider the following two transformations G1 and G2 from S and S respectively: G1 = U ΛZ Y ∈ S, G2 = U 518 ΛZ 0 ˜ ∈ S. O N THE C OMPLETE C HARACTERIZATION OF S OLUTIONS TO A G ENERALIZED F ISHER C RITERION Recall that U = [U1 ,U2 ], where the columns of U2 span the null space of St . Hence, n T T T 0 = U2 St U2 = ∑ U2 (ai − c) · (U2 (ai − c))T , i=1 T and U2 (ai − c) = 0, for i = 1, · · · , n, where c is the global centroid. It follows that T T T GT (ai − c) = Z T ΛT U1 (ai − c) +Y T U2 (ai − c) = Z T ΛT U1 (ai − c) = GT (ai − c), 1 2 for i = 1, · · · , n. That is, G1 and G2 are equivalent transformations. Hence, the two solution charac˜ terizations S and S are essentially equivalent. Remark 3.1 The analysis above shows that the additional information contained in S is the null ˜ space, U2 , of St , which leads to an equivalent transformation. In S , the null space U2 is removed, which can be further justiﬁed as follows. Since St = Sb + Sw , we have T T T 0 = U2 St U2 = U2 SbU2 +U2 SwU2 . T It follows that U2 SbU2 = 0, as both Sb and Sw are positive semi-deﬁnite. Thus, the null space U2 does not contain any discriminant information. This explains why the null space of St is removed in most discriminant analysis based algorithms proposed in the past. 4. Efﬁciency Comparison In S , the full matrix U is involved, whose computation may be expensive, especially for high˜ dimensional data. In contrast, only the ﬁrst component U1 ∈ IRd×r of U is involved in S , which can be computed efﬁciently for high-dimensional low-sample-size problem by directly working on the Gram matrix instead of the covariance matrix. ˜ In summary, we show that S and S are equivalent characterizations of the solutions to the generalized Fisher criterion in Eq. (1). However, the latter one is preferred in practice due to its relative efﬁciency for high-dimensional low-sample-size data. References K. Fukunaga. Introduction to Statistical Pattern Classiﬁcation. Academic Press, San Diego, California, USA, 1990. G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, MD, USA, third edition, 1996. M. Loog. A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion. Journal of Machine Learning Research, 8:2121–2123, 2007. J. Ye. Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems. Journal of Machine Learning Research, 6:483–502, 2005. 519</p><p>5 0.041661803 <a title="50-tfidf-5" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>Author: Gal Elidan, Stephen Gould</p><p>Abstract: With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufﬁciently expressive for generalization while at the same time allow for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overﬁtting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modiﬁcations and that is polynomial both in the size of the graph and the treewidth bound. At the heart of our method is a dynamic triangulation that we update in a way that facilitates the addition of chain structures that increase the bound on the model’s treewidth by at most one. We demonstrate the effectiveness of our “treewidth-friendly” method on several real-life data sets and show that it is superior to the greedy approach as soon as the bound on the treewidth is nontrivial. Importantly, we also show that by making use of global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth. Keywords: Bayesian networks, structure learning, model selection, bounded treewidth</p><p>6 0.041554898 <a title="50-tfidf-6" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>7 0.02968063 <a title="50-tfidf-7" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>8 0.029420931 <a title="50-tfidf-8" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>9 0.028630059 <a title="50-tfidf-9" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>10 0.024889197 <a title="50-tfidf-10" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>11 0.024671508 <a title="50-tfidf-11" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>12 0.02378661 <a title="50-tfidf-12" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>13 0.023776276 <a title="50-tfidf-13" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>14 0.020833891 <a title="50-tfidf-14" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>15 0.019457467 <a title="50-tfidf-15" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>16 0.018762285 <a title="50-tfidf-16" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>17 0.018158585 <a title="50-tfidf-17" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>18 0.017884221 <a title="50-tfidf-18" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>19 0.016370779 <a title="50-tfidf-19" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>20 0.016056363 <a title="50-tfidf-20" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.113), (1, -0.012), (2, 0.036), (3, 0.032), (4, -0.262), (5, 0.215), (6, 0.256), (7, 0.041), (8, -0.148), (9, 0.214), (10, 0.297), (11, -0.299), (12, 0.182), (13, 0.028), (14, 0.127), (15, 0.087), (16, 0.145), (17, -0.132), (18, -0.007), (19, -0.127), (20, -0.131), (21, -0.055), (22, 0.014), (23, -0.023), (24, -0.053), (25, 0.129), (26, 0.018), (27, 0.022), (28, -0.059), (29, -0.049), (30, -0.008), (31, 0.03), (32, -0.014), (33, -0.052), (34, 0.024), (35, 0.027), (36, 0.028), (37, 0.036), (38, 0.061), (39, -0.018), (40, -0.032), (41, -0.016), (42, -0.004), (43, -0.024), (44, 0.08), (45, 0.03), (46, 0.009), (47, 0.03), (48, -0.06), (49, -0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97360098 <a title="50-lsi-1" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: In this paper, the naive credal classiﬁer, which is a set-valued counterpart of naive Bayes, is extended to a general and ﬂexible treatment of incomplete data, yielding a new classiﬁer called naive credal classiﬁer 2 (NCC2). The new classiﬁer delivers classiﬁcations that are reliable even in the presence of small sample sizes and missing values. Extensive empirical evaluations show that, by issuing set-valued classiﬁcations, NCC2 is able to isolate and properly deal with instances that are hard to classify (on which naive Bayes accuracy drops considerably), and to perform as well as naive Bayes on the other instances. The experiments point to a general problem: they show that with missing values, empirical evaluations may not reliably estimate the accuracy of a traditional classiﬁer, such as naive Bayes. This phenomenon adds even more value to the robust approach to classiﬁcation implemented by NCC2. Keywords: naive Bayes, naive credal classiﬁer, imprecise probabilities, missing values, conservative inference rule, missing at random</p><p>2 0.87562484 <a title="50-lsi-2" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: JNCC2 implements the naive credal classiﬁer 2 (NCC2). This is an extension of naive Bayes to imprecise probabilities that aims at delivering robust classiﬁcations also when dealing with small or incomplete data sets. Robustness is achieved by delivering set-valued classiﬁcations (that is, returning multiple classes) on the instances for which (i) the learning set is not informative enough to smooth the effect of choice of the prior density or (ii) the uncertainty arising from missing data prevents the reliable indication of a single class. JNCC2 is released under the GNU GPL license. Keywords: imprecise probabilities, missing data, naive Bayes, naive credal classiﬁer 2, Java</p><p>3 0.26643199 <a title="50-lsi-3" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>Author: Gal Chechik, Geremy Heitz, Gal Elidan, Pieter Abbeel, Daphne Koller</p><p>Abstract: We consider the problem of learning classiﬁers in structured domains, where some objects have a subset of features that are inherently absent due to complex relationships between the features. Unlike the case where a feature exists but its value is not observed, here we focus on the case where a feature may not even exist (structurally absent) for some of the samples. The common approach for handling missing features in discriminative models is to ﬁrst complete their unknown values, and then use a standard classiﬁcation procedure over the completed data. This paper focuses on features that are known to be non-existing, rather than have an unknown value. We show how incomplete data can be classiﬁed directly without any completion of the missing features using a max-margin learning framework. We formulate an objective function, based on the geometric interpretation of the margin, that aims to maximize the margin of each sample in its own relevant subspace. In this formulation, the linearly separable case can be transformed into a binary search over a series of second order cone programs (SOCP), a convex problem that can be solved efﬁciently. We also describe two approaches for optimizing the general case: an approximation that can be solved as a standard quadratic program (QP) and an iterative approach for solving the exact problem. By avoiding the pre-processing phase in which the data is completed, both of these approaches could offer considerable computational savings. More importantly, we show that the elegant handling of missing values by our approach allows it to both outperform other methods when the missing values have non-trivial structure, and be competitive with other methods when the values are missing at random. We demonstrate our results on several standard benchmarks and two real-world problems: edge prediction in metabolic pathways, and automobile detection in natural images. Keywords: max margin, missing features, network reconstruction, metabolic pa</p><p>4 0.15736772 <a title="50-lsi-4" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>Author: Gal Elidan, Stephen Gould</p><p>Abstract: With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufﬁciently expressive for generalization while at the same time allow for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overﬁtting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modiﬁcations and that is polynomial both in the size of the graph and the treewidth bound. At the heart of our method is a dynamic triangulation that we update in a way that facilitates the addition of chain structures that increase the bound on the model’s treewidth by at most one. We demonstrate the effectiveness of our “treewidth-friendly” method on several real-life data sets and show that it is superior to the greedy approach as soon as the bound on the treewidth is nontrivial. Importantly, we also show that by making use of global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth. Keywords: Bayesian networks, structure learning, model selection, bounded treewidth</p><p>5 0.15508084 <a title="50-lsi-5" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>6 0.1280136 <a title="50-lsi-6" href="./jmlr-2008-Comments_on_the_Complete_Characterization_of_a_Family_of_Solutions_to_a_GeneralizedFisherCriterion.html">23 jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</a></p>
<p>7 0.10652091 <a title="50-lsi-7" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>8 0.10331842 <a title="50-lsi-8" href="./jmlr-2008-Consistency_of_Random_Forests_and_Other_Averaging_Classifiers.html">25 jmlr-2008-Consistency of Random Forests and Other Averaging Classifiers</a></p>
<p>9 0.099805675 <a title="50-lsi-9" href="./jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>10 0.098889284 <a title="50-lsi-10" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>11 0.098400064 <a title="50-lsi-11" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>12 0.097367652 <a title="50-lsi-12" href="./jmlr-2008-HPB%3A_A_Model_for_Handling_BN_Nodes_with_High_Cardinality_Parents.html">42 jmlr-2008-HPB: A Model for Handling BN Nodes with High Cardinality Parents</a></p>
<p>13 0.090773366 <a title="50-lsi-13" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>14 0.085827067 <a title="50-lsi-14" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>15 0.08472582 <a title="50-lsi-15" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>16 0.081941701 <a title="50-lsi-16" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>17 0.081705831 <a title="50-lsi-17" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>18 0.081176423 <a title="50-lsi-18" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>19 0.080657966 <a title="50-lsi-19" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>20 0.078560054 <a title="50-lsi-20" href="./jmlr-2008-Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">33 jmlr-2008-Evidence Contrary to the Statistical View of Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.024), (5, 0.013), (16, 0.181), (31, 0.013), (40, 0.019), (54, 0.035), (58, 0.029), (66, 0.048), (76, 0.025), (88, 0.056), (91, 0.33), (92, 0.044), (94, 0.043), (99, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74726593 <a title="50-lda-1" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: In this paper, the naive credal classiﬁer, which is a set-valued counterpart of naive Bayes, is extended to a general and ﬂexible treatment of incomplete data, yielding a new classiﬁer called naive credal classiﬁer 2 (NCC2). The new classiﬁer delivers classiﬁcations that are reliable even in the presence of small sample sizes and missing values. Extensive empirical evaluations show that, by issuing set-valued classiﬁcations, NCC2 is able to isolate and properly deal with instances that are hard to classify (on which naive Bayes accuracy drops considerably), and to perform as well as naive Bayes on the other instances. The experiments point to a general problem: they show that with missing values, empirical evaluations may not reliably estimate the accuracy of a traditional classiﬁer, such as naive Bayes. This phenomenon adds even more value to the robust approach to classiﬁcation implemented by NCC2. Keywords: naive Bayes, naive credal classiﬁer, imprecise probabilities, missing values, conservative inference rule, missing at random</p><p>2 0.61391157 <a title="50-lda-2" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>Author: Elisa Ricci, Tijl De Bie, Nello Cristianini</p><p>Abstract: Most approaches to structured output prediction rely on a hypothesis space of prediction functions that compute their output by maximizing a linear scoring function. In this paper we present two novel learning algorithms for this hypothesis class, and a statistical analysis of their performance. The methods rely on efﬁciently computing the ﬁrst two moments of the scoring function over the output space, and using them to create convex objective functions for training. We report extensive experimental results for sequence alignment, named entity recognition, and RNA secondary structure prediction. Keywords: structured output prediction, discriminative learning, Z-score, discriminant analysis, PAC bound</p><p>3 0.53806919 <a title="50-lda-3" href="./jmlr-2008-JNCC2%3A_The_Java_Implementation_Of_Naive_Credal_Classifier_2%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">45 jmlr-2008-JNCC2: The Java Implementation Of Naive Credal Classifier 2    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Giorgio Corani, Marco Zaffalon</p><p>Abstract: JNCC2 implements the naive credal classiﬁer 2 (NCC2). This is an extension of naive Bayes to imprecise probabilities that aims at delivering robust classiﬁcations also when dealing with small or incomplete data sets. Robustness is achieved by delivering set-valued classiﬁcations (that is, returning multiple classes) on the instances for which (i) the learning set is not informative enough to smooth the effect of choice of the prior density or (ii) the uncertainty arising from missing data prevents the reliable indication of a single class. JNCC2 is released under the GNU GPL license. Keywords: imprecise probabilities, missing data, naive Bayes, naive credal classiﬁer 2, Java</p><p>4 0.28875488 <a title="50-lda-4" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>Author: Thomas G. Dietterich, Guohua Hao, Adam Ashenfelter</p><p>Abstract: Conditional random ﬁelds (CRFs) provide a ﬂexible and powerful model for sequence labeling problems. However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features and feature combinations. This paper describes a new algorithm for training CRFs via gradient tree boosting. In tree boosting, the CRF potential functions are represented as weighted sums of regression trees, which provide compact representations of feature interactions. So the algorithm does not explicitly consider the potentially large parameter space. As a result, gradient tree boosting scales linearly in the order of the Markov model and in the order of the feature interactions, rather than exponentially as in previous algorithms based on iterative scaling and gradient descent. Gradient tree boosting also makes it possible to use instance weighting (as in C4.5) and surrogate splitting (as in CART) to handle missing values. Experimental studies of the effectiveness of these two methods (as well as standard imputation and indicator feature methods) show that instance weighting is the best method in most cases when feature values are missing at random. Keywords: sequential supervised learning, conditional random ﬁelds, functional gradient, gradient tree boosting, missing values</p><p>5 0.27720037 <a title="50-lda-5" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>6 0.27020636 <a title="50-lda-6" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>7 0.26828891 <a title="50-lda-7" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>8 0.26305455 <a title="50-lda-8" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>9 0.26295495 <a title="50-lda-9" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>10 0.26283866 <a title="50-lda-10" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>11 0.26241782 <a title="50-lda-11" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>12 0.26215786 <a title="50-lda-12" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>13 0.25949049 <a title="50-lda-13" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>14 0.258751 <a title="50-lda-14" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>15 0.258663 <a title="50-lda-15" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>16 0.25759053 <a title="50-lda-16" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>17 0.25641823 <a title="50-lda-17" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>18 0.25638938 <a title="50-lda-18" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>19 0.25630444 <a title="50-lda-19" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>20 0.25608054 <a title="50-lda-20" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
