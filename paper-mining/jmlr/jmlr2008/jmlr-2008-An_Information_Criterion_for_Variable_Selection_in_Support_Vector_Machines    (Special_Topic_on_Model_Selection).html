<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-15" href="#">jmlr2008-15</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2008-15-pdf" href="http://jmlr.org/papers/volume9/claeskens08a/claeskens08a.pdf">pdf</a></p><p>Author: Gerda Claeskens, Christophe Croux, Johan Van Kerckhoven</p><p>Abstract: Support vector machines for classiﬁcation have the advantage that the curse of dimensionality is circumvented. It has been shown that a reduction of the dimension of the input space leads to even better results. For this purpose, we propose two information criteria which can be computed directly from the deﬁnition of the support vector machine. We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. We arrive at the same ﬁndings for comparison on some real-world benchmark data sets. Keywords: information criterion, supervised classiﬁcation, support vector machine, variable selection</p><p>Reference: <a title="jmlr-2008-15-reference" href="../jmlr2008_reference/jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 BE  ORSTAT and Leuven Statistics Research Center Katholieke Universiteit Leuven B-3000 Leuven, Belgium  Editors: Isabelle Guyon and Amir Saffari  Abstract Support vector machines for classiﬁcation have the advantage that the curse of dimensionality is circumvented. [sent-10, score-0.234]
</p><p>2 For this purpose, we propose two information criteria which can be computed directly from the deﬁnition of the support vector machine. [sent-12, score-0.451]
</p><p>3 We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. [sent-13, score-0.737]
</p><p>4 The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. [sent-14, score-0.468]
</p><p>5 We arrive at the same ﬁndings for comparison on some real-world benchmark data sets. [sent-15, score-0.088]
</p><p>6 Keywords: information criterion, supervised classiﬁcation, support vector machine, variable selection  1. [sent-16, score-0.35]
</p><p>7 Introduction We study classiﬁcation using the support vector machine (SVM). [sent-17, score-0.135]
</p><p>8 We start from a training set {(xi , yi )} containing n observations. [sent-18, score-0.153]
</p><p>9 , xip ) has a class label yi assigned to it, which is either +1 or −1. [sent-22, score-0.257]
</p><p>10 We wish to ﬁnd a function f (·) such that for an observation x the predicted class y = +1 if f (x) is positive, and y = −1 if f (x) is negative. [sent-23, score-0.032]
</p><p>11 We want ˆ ˆ this function to assign the correct class labels to the training observations (low training error rate) and to accurately classify new observations (low generalization error rate). [sent-24, score-0.159]
</p><p>12 , xip reduces variability of the class-label estimator and might lead to better out-of-sample predictions. [sent-28, score-0.171]
</p><p>13 It is only true to some extent that variable selection would not be necessary in the support vector machine setting since it manages to circumvent the so-called “curse of dimensionality” (see, for ¨ example, Cristianini and Shawe-Taylor, 2000; Hastie et al. [sent-29, score-0.467]
</p><p>14 While the SVM approach avoids ﬁtting a number of parameters equal to the dimension of the input space, there remains the high probability of a perfect separation in high-dimensional problems. [sent-31, score-0.104]
</p><p>15 For example, if p is larger than the number of observations, it is always possible to perfectly separate the two classes of training data by a hyperplane. [sent-32, score-0.031]
</p><p>16 In general, the risk of overﬁtting will increases with the dimension for most data conﬁgurations. [sent-33, score-0.046]
</p><p>17 Hence, the risk of obtaining a decision rule with poor c 2008 Gerda Claeskens, Christophe Croux and Johan Van Kerckhoven. [sent-34, score-0.046]
</p><p>18 C LAESKENS , C ROUX AND VAN K ERCKHOVEN  generalization properties (high generalization error rate) cannot be avoided. [sent-35, score-0.098]
</p><p>19 (2002) illustrate this and show that variable selection can further improve the SVM’s performance. [sent-37, score-0.215]
</p><p>20 Variable selection techniques can be divided into three categories. [sent-38, score-0.156]
</p><p>21 Filters select subsets of variables as a pre-processing step, independently of the prediction method. [sent-39, score-0.04]
</p><p>22 Finally, embedded methods include variable selection into the construction of the classiﬁer. [sent-41, score-0.273]
</p><p>23 In this paper we propose new information criteria for SVMs, yielding a wrapper method where we consider the SVM merely as a black box. [sent-42, score-0.399]
</p><p>24 We refer to Guyon and Elisseeff (2003) for an introduction to variable and feature selection in Machine Learning. [sent-43, score-0.215]
</p><p>25 Information criteria are a standard tool for model selection in traditional statistics. [sent-44, score-0.426]
</p><p>26 Information criteria for variable selection assign a numerical value to each subset of the variables under consideration. [sent-45, score-0.579]
</p><p>27 The subset with the lowest value of the information criterion is then selected. [sent-46, score-0.082]
</p><p>28 Examples are the Akaike information criterion (AIC, Akaike, 1973) and the Bayesian information criterion (BIC, Schwarz, 1978). [sent-47, score-0.164]
</p><p>29 Claeskens and Hjort (2008) survey and explain the use of common information criteria for statistical variable selection in likelihood-based models, we refer to there for more references. [sent-48, score-0.566]
</p><p>30 For support vector machines only very few information criteria have been developed. [sent-49, score-0.514]
</p><p>31 The kernel regularisation information criterion (KRIC) of Kobayashi and Komaki (2006) was originally proposed for parameter tuning of the SVM. [sent-50, score-0.213]
</p><p>32 In this paper two new information criteria are proposed, one shares properties with AIC, the other with BIC. [sent-53, score-0.357]
</p><p>33 We want the new criteria to select a preferably compact subset of variables with good predictive properties. [sent-54, score-0.472]
</p><p>34 We will show that submodels selected by the new criteria are as performing as the ones chosen by the KRIC, while they incur substantially less computational overhead. [sent-55, score-0.445]
</p><p>35 We also make a comparison with using cross-validated error rate based criteria, as in Kearns et al. [sent-56, score-0.044]
</p><p>36 An important contribution of this paper is that our numerical comparisons show that the popular, but time consuming, cross-validation criteria are outperformed in generalization error by the new information criteria, where the latter are coming at almost no additional computational cost. [sent-58, score-0.451]
</p><p>37 Alternative approaches perform variable selection in feature space instead of in input space (Shih and Cheng, 2005), or select a set of “maximally separating directions” in the input space (Fortuna and Capson, 2004). [sent-59, score-0.305]
</p><p>38 These methods, however, do not select a set of original input variables. [sent-60, score-0.04]
</p><p>39 Various other authors have suggested different formulations for the SVM such that variable selection is performed automatically. [sent-61, score-0.249]
</p><p>40 Examples of such embedded methods can be found in Bi et al. [sent-62, score-0.058]
</p><p>41 In Section 2 we deﬁne the support vector machine setting, we review existing information criteria and we describe ranking techniques to speed up the variable selection process. [sent-68, score-0.813]
</p><p>42 In Section 3, we deﬁne the new information criteria and highlight their advantages. [sent-69, score-0.357]
</p><p>43 Section 4 contains the results of a simulation study and in Section 5 we compare the different techniques on a few real-world benchmark data sets. [sent-70, score-0.155]
</p><p>44 Section 6 concludes and gives some directions for further research. [sent-71, score-0.039]
</p><p>45 Problem Setting In this Section we ﬁrst review the deﬁnition of a support vector machine. [sent-73, score-0.135]
</p><p>46 Finally we present the ranking techniques that will be used in this paper. [sent-75, score-0.101]
</p><p>47 1 The Support Vector Machine We denote the training sample (xi , yi ), 1 ≤ i ≤ n, with xi a p-dimensional vector of explicative variables, and yi ∈ {−1, +1} the class label. [sent-77, score-0.532]
</p><p>48 The goal is to estimate a target function f (x) in the space of explicative variables such that f (xi ) > 0 for yi = +1, and f (xi ) < 0 for yi = −1. [sent-78, score-0.403]
</p><p>49 We start with linear support vector machines, where f (x) is of the form f (x) = w x + b. [sent-79, score-0.135]
</p><p>50 For binary classiﬁcation this function is obtained by solving the minimisation problem min w,b,ξi  1 w 2  2  n  +C ∑ ξi  subject to  i=1  yi (w xi + b) ≥ 1 − ξi , ξi ≥ 0, i = 1, . [sent-80, score-0.247]
</p><p>51 (1)  The ξi are slack margin variables, indicating how close a point x i lies to the separating boundary (if ξi < 1), or how badly it is misclassiﬁed (if ξi > 1). [sent-84, score-0.169]
</p><p>52 The tuning parameter C controls how much weight is put on trying to achieve perfect separation. [sent-85, score-0.198]
</p><p>53 i=1  n 1 min{ α Qα − ∑ αi } subject to α 2 i=1  (2)  Here αi is the weight given to the observation (xi , yi ), and Q is a positive semi-deﬁnite matrix with entries Qi, j = yi y j xi x j . [sent-90, score-0.367]
</p><p>54 The negative intercept b is i=1 found by computing b = 0. [sent-92, score-0.048]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('claeskens', 0.319), ('criteria', 0.316), ('christophe', 0.239), ('croux', 0.239), ('econ', 0.239), ('gerda', 0.239), ('kric', 0.239), ('kuleuven', 0.203), ('leuven', 0.203), ('johan', 0.182), ('explicative', 0.159), ('xip', 0.135), ('yi', 0.122), ('akaike', 0.121), ('aic', 0.111), ('selection', 0.11), ('variable', 0.105), ('svm', 0.103), ('guyon', 0.097), ('support', 0.094), ('curse', 0.083), ('criterion', 0.082), ('van', 0.077), ('tuning', 0.07), ('kobayashi', 0.068), ('minimisation', 0.068), ('neumann', 0.068), ('schwarz', 0.068), ('shih', 0.068), ('submodels', 0.068), ('machines', 0.063), ('perfect', 0.063), ('nformation', 0.061), ('consuming', 0.061), ('belgium', 0.061), ('incur', 0.061), ('manages', 0.061), ('preferably', 0.061), ('regularisation', 0.061), ('riteria', 0.061), ('saffari', 0.061), ('universiteit', 0.061), ('simulation', 0.059), ('embedded', 0.058), ('xi', 0.057), ('circumvent', 0.056), ('cheng', 0.056), ('afterwards', 0.056), ('hjort', 0.056), ('ranking', 0.055), ('predictive', 0.055), ('wrappers', 0.052), ('filters', 0.052), ('wrapper', 0.052), ('separating', 0.05), ('benchmark', 0.05), ('generalization', 0.049), ('intercept', 0.048), ('assign', 0.048), ('badly', 0.048), ('dimensionality', 0.047), ('existing', 0.046), ('techniques', 0.046), ('elisseeff', 0.046), ('amir', 0.046), ('bic', 0.046), ('risk', 0.046), ('rate', 0.044), ('coming', 0.043), ('outperformed', 0.043), ('isabelle', 0.043), ('zhang', 0.042), ('vector', 0.041), ('avoids', 0.041), ('highlight', 0.041), ('election', 0.041), ('shares', 0.041), ('select', 0.04), ('slack', 0.04), ('maximally', 0.04), ('kearns', 0.04), ('ndings', 0.04), ('directions', 0.039), ('arrive', 0.038), ('variability', 0.036), ('olkopf', 0.036), ('qi', 0.036), ('survey', 0.035), ('tting', 0.034), ('weight', 0.034), ('formulations', 0.034), ('wang', 0.034), ('zhu', 0.034), ('observation', 0.032), ('training', 0.031), ('cristianini', 0.031), ('lee', 0.031), ('indicating', 0.031), ('merely', 0.031), ('trying', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="15-tfidf-1" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>Author: Gerda Claeskens, Christophe Croux, Johan Van Kerckhoven</p><p>Abstract: Support vector machines for classiﬁcation have the advantage that the curse of dimensionality is circumvented. It has been shown that a reduction of the dimension of the input space leads to even better results. For this purpose, we propose two information criteria which can be computed directly from the deﬁnition of the support vector machine. We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. We arrive at the same ﬁndings for comparison on some real-world benchmark data sets. Keywords: information criterion, supervised classiﬁcation, support vector machine, variable selection</p><p>2 0.068267435 <a title="15-tfidf-2" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>Author: Imhoi Koo, Rhee Man Kil</p><p>Abstract: This paper presents a new method of model selection for regression problems using the modulus of continuity. For this purpose, we suggest the prediction risk bounds of regression models using the modulus of continuity which can be interpreted as the complexity of functions. We also present the model selection criterion referred to as the modulus of continuity information criterion (MCIC) which is derived from the suggested prediction risk bounds. The suggested MCIC provides a risk estimate using the modulus of continuity for a trained regression model (or an estimation function) while other model selection criteria such as the AIC and BIC use structural information such as the number of training parameters. As a result, the suggested MCIC is able to discriminate the performances of trained regression models, even with the same structure of training models. To show the effectiveness of the proposed method, the simulation for function approximation using the multilayer perceptrons (MLPs) was conducted. Through the simulation for function approximation, it was demonstrated that the suggested MCIC provides a good selection tool for nonlinear regression models, even with the limited size of data. Keywords: regression models, multilayer perceptrons, model selection, information criteria, modulus of continuity</p><p>3 0.065920487 <a title="15-tfidf-3" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>Author: Bo Jiang, Xuegong Zhang, Tianxi Cai</p><p>Abstract: Support vector machine (SVM) is one of the most popular and promising classiﬁcation algorithms. After a classiﬁcation rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples. Keywords: k-fold cross-validation, model evaluation, perturbation-resampling, prediction errors, support vector machine</p><p>4 0.051105909 <a title="15-tfidf-4" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>Author: Ja-Yong Koo, Yoonkyung Lee, Yuwon Kim, Changyi Park</p><p>Abstract: The support vector machine has been successful in a variety of applications. Also on the theoretical front, statistical properties of the support vector machine have been studied quite extensively with a particular attention to its Bayes risk consistency under some conditions. In this paper, we study somewhat basic statistical properties of the support vector machine yet to be investigated, namely the asymptotic behavior of the coefﬁcients of the linear support vector machine. A Bahadur type representation of the coefﬁcients is established under appropriate conditions, and their asymptotic normality and statistical variability are derived on the basis of the representation. These asymptotic results do not only help further our understanding of the support vector machine, but also they can be useful for related statistical inferences. Keywords: asymptotic normality, Bahadur representation, classiﬁcation, convexity lemma, Radon transform</p><p>5 0.046599016 <a title="15-tfidf-5" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>Author: Eyal Krupka, Amir Navot, Naftali Tishby</p><p>Abstract: Feature selection is the task of choosing a small subset of features that is sufﬁcient to predict the target labels well. Here, instead of trying to directly determine which features are better, we attempt to learn the properties of good features. For this purpose we assume that each feature is represented by a set of properties, referred to as meta-features. This approach enables prediction of the quality of features without measuring their value on the training instances. We use this ability to devise new selection algorithms that can efﬁciently search for new good features in the presence of a huge number of features, and to dramatically reduce the number of feature measurements needed. We demonstrate our algorithms on a handwritten digit recognition problem and a visual object category recognition problem. In addition, we show how this novel viewpoint enables derivation of better generalization bounds for the joint learning problem of selection and classiﬁcation, and how it contributes to a better understanding of the problem. Speciﬁcally, in the context of object recognition, previous works showed that it is possible to ﬁnd one set of features which ﬁts most object categories (aka a universal dictionary). Here we use our framework to analyze one such universal dictionary and ﬁnd that the quality of features in this dictionary can be predicted accurately by its meta-features. Keywords: feature selection, unobserved features, meta-features</p><p>6 0.044759341 <a title="15-tfidf-6" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>7 0.044630393 <a title="15-tfidf-7" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>8 0.043433819 <a title="15-tfidf-8" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>9 0.042219844 <a title="15-tfidf-9" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>10 0.037893601 <a title="15-tfidf-10" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>11 0.037289038 <a title="15-tfidf-11" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>12 0.03701032 <a title="15-tfidf-12" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>13 0.036856253 <a title="15-tfidf-13" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>14 0.036586843 <a title="15-tfidf-14" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>15 0.035221726 <a title="15-tfidf-15" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>16 0.035106689 <a title="15-tfidf-16" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>17 0.034237053 <a title="15-tfidf-17" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>18 0.033389382 <a title="15-tfidf-18" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>19 0.032451086 <a title="15-tfidf-19" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>20 0.031841192 <a title="15-tfidf-20" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.161), (1, -0.062), (2, 0.036), (3, -0.009), (4, 0.053), (5, 0.021), (6, 0.043), (7, -0.006), (8, 0.082), (9, -0.016), (10, 0.024), (11, -0.006), (12, -0.021), (13, -0.032), (14, -0.02), (15, -0.036), (16, 0.017), (17, 0.012), (18, 0.071), (19, 0.263), (20, -0.088), (21, 0.144), (22, -0.163), (23, 0.324), (24, -0.153), (25, 0.142), (26, 0.074), (27, 0.039), (28, 0.148), (29, 0.12), (30, 0.103), (31, 0.04), (32, 0.057), (33, 0.133), (34, 0.147), (35, 0.155), (36, 0.091), (37, 0.197), (38, -0.03), (39, 0.065), (40, 0.089), (41, 0.049), (42, 0.126), (43, -0.168), (44, -0.024), (45, -0.049), (46, 0.033), (47, 0.047), (48, -0.037), (49, 0.128)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95889145 <a title="15-lsi-1" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>Author: Gerda Claeskens, Christophe Croux, Johan Van Kerckhoven</p><p>Abstract: Support vector machines for classiﬁcation have the advantage that the curse of dimensionality is circumvented. It has been shown that a reduction of the dimension of the input space leads to even better results. For this purpose, we propose two information criteria which can be computed directly from the deﬁnition of the support vector machine. We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. We arrive at the same ﬁndings for comparison on some real-world benchmark data sets. Keywords: information criterion, supervised classiﬁcation, support vector machine, variable selection</p><p>2 0.62005818 <a title="15-lsi-2" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>Author: Imhoi Koo, Rhee Man Kil</p><p>Abstract: This paper presents a new method of model selection for regression problems using the modulus of continuity. For this purpose, we suggest the prediction risk bounds of regression models using the modulus of continuity which can be interpreted as the complexity of functions. We also present the model selection criterion referred to as the modulus of continuity information criterion (MCIC) which is derived from the suggested prediction risk bounds. The suggested MCIC provides a risk estimate using the modulus of continuity for a trained regression model (or an estimation function) while other model selection criteria such as the AIC and BIC use structural information such as the number of training parameters. As a result, the suggested MCIC is able to discriminate the performances of trained regression models, even with the same structure of training models. To show the effectiveness of the proposed method, the simulation for function approximation using the multilayer perceptrons (MLPs) was conducted. Through the simulation for function approximation, it was demonstrated that the suggested MCIC provides a good selection tool for nonlinear regression models, even with the limited size of data. Keywords: regression models, multilayer perceptrons, model selection, information criteria, modulus of continuity</p><p>3 0.44983634 <a title="15-lsi-3" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>Author: Bo Jiang, Xuegong Zhang, Tianxi Cai</p><p>Abstract: Support vector machine (SVM) is one of the most popular and promising classiﬁcation algorithms. After a classiﬁcation rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples. Keywords: k-fold cross-validation, model evaluation, perturbation-resampling, prediction errors, support vector machine</p><p>4 0.32341838 <a title="15-lsi-4" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>Author: Ja-Yong Koo, Yoonkyung Lee, Yuwon Kim, Changyi Park</p><p>Abstract: The support vector machine has been successful in a variety of applications. Also on the theoretical front, statistical properties of the support vector machine have been studied quite extensively with a particular attention to its Bayes risk consistency under some conditions. In this paper, we study somewhat basic statistical properties of the support vector machine yet to be investigated, namely the asymptotic behavior of the coefﬁcients of the linear support vector machine. A Bahadur type representation of the coefﬁcients is established under appropriate conditions, and their asymptotic normality and statistical variability are derived on the basis of the representation. These asymptotic results do not only help further our understanding of the support vector machine, but also they can be useful for related statistical inferences. Keywords: asymptotic normality, Bahadur representation, classiﬁcation, convexity lemma, Radon transform</p><p>5 0.28529173 <a title="15-lsi-5" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>Author: Sivan Sabato, Shai Shalev-Shwartz</p><p>Abstract: Feature ranking is a fundamental machine learning task with various applications, including feature selection and decision tree learning. We describe and analyze a new feature ranking method that supports categorical features with a large number of possible values. We show that existing ranking criteria rank a feature according to the training error of a predictor based on the feature. This approach can fail when ranking categorical features with many values. We propose the Ginger ranking criterion, that estimates the generalization error of the predictor associated with the Gini index. We show that for almost all training sets, the Ginger criterion produces an accurate estimation of the true generalization error, regardless of the number of values in a categorical feature. We also address the question of ﬁnding the optimal predictor that is based on a single categorical feature. It is shown that the predictor associated with the misclassiﬁcation error criterion has the minimal expected generalization error. We bound the bias of this predictor with respect to the generalization error of the Bayes optimal predictor, and analyze its concentration properties. We demonstrate the efﬁciency of our approach for feature selection and for learning decision trees in a series of experiments with synthetic and natural data sets. Keywords: feature ranking, categorical features, generalization bounds, Gini index, decision trees</p><p>6 0.28278351 <a title="15-lsi-6" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>7 0.26505342 <a title="15-lsi-7" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>8 0.25974992 <a title="15-lsi-8" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>9 0.25149086 <a title="15-lsi-9" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>10 0.24711248 <a title="15-lsi-10" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>11 0.24702254 <a title="15-lsi-11" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>12 0.22860202 <a title="15-lsi-12" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>13 0.22856028 <a title="15-lsi-13" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>14 0.20865208 <a title="15-lsi-14" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>15 0.20206283 <a title="15-lsi-15" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>16 0.20153917 <a title="15-lsi-16" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>17 0.18875079 <a title="15-lsi-17" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>18 0.18577057 <a title="15-lsi-18" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>19 0.17834345 <a title="15-lsi-19" href="./jmlr-2008-Shark%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">85 jmlr-2008-Shark    (Machine Learning Open Source Software Paper)</a></p>
<p>20 0.17832252 <a title="15-lsi-20" href="./jmlr-2008-Near-Optimal_Sensor_Placements_in_Gaussian_Processes%3A_Theory%2C_Efficient_Algorithms_and_Empirical_Studies.html">67 jmlr-2008-Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.5), (12, 0.015), (28, 0.065), (31, 0.012), (40, 0.062), (54, 0.031), (58, 0.043), (66, 0.034), (76, 0.01), (88, 0.064), (92, 0.027), (94, 0.042), (99, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9277513 <a title="15-lda-1" href="./jmlr-2008-Graphical_Methods_for_Efficient_Likelihood_Inference_in_Gaussian_Covariance_Models.html">40 jmlr-2008-Graphical Methods for Efficient Likelihood Inference in Gaussian Covariance Models</a></p>
<p>Author: Mathias Drton, Thomas S. Richardson</p><p>Abstract: In graphical modelling, a bi-directed graph encodes marginal independences among random variables that are identiﬁed with the vertices of the graph. We show how to transform a bi-directed graph into a maximal ancestral graph that (i) represents the same independence structure as the original bi-directed graph, and (ii) minimizes the number of arrowheads among all ancestral graphs satisfying (i). Here the number of arrowheads of an ancestral graph is the number of directed edges plus twice the number of bi-directed edges. In Gaussian models, this construction can be used for more efﬁcient iterative maximization of the likelihood function and to determine when maximum likelihood estimates are equal to empirical counterparts. Keywords: ancestral graph, covariance graph, graphical model, marginal independence, maximum likelihood estimation, multivariate normal distribution</p><p>2 0.91365486 <a title="15-lda-2" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>Author: Sébastien Loustau</p><p>Abstract: This paper investigates statistical performances of Support Vector Machines (SVM) and considers the problem of adaptation to the margin parameter and to complexity. In particular we provide a classiﬁer with no tuning parameter. It is a combination of SVM classiﬁers. Our contribution is two-fold: (1) we propose learning rates for SVM using Sobolev spaces and build a numerically realizable aggregate that converges with same rate; (2) we present practical experiments of this method of aggregation for SVM using both Sobolev spaces and Gaussian kernels. Keywords: classiﬁcation, support vector machines, learning rates, approximation, aggregation of classiﬁers</p><p>same-paper 3 0.89900237 <a title="15-lda-3" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>Author: Gerda Claeskens, Christophe Croux, Johan Van Kerckhoven</p><p>Abstract: Support vector machines for classiﬁcation have the advantage that the curse of dimensionality is circumvented. It has been shown that a reduction of the dimension of the input space leads to even better results. For this purpose, we propose two information criteria which can be computed directly from the deﬁnition of the support vector machine. We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. We arrive at the same ﬁndings for comparison on some real-world benchmark data sets. Keywords: information criterion, supervised classiﬁcation, support vector machine, variable selection</p><p>4 0.50887537 <a title="15-lda-4" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>Author: Peter L. Bartlett, Marten H. Wegkamp</p><p>Abstract: We consider the problem of binary classiﬁcation where the classiﬁer can, for a particular cost, choose not to classify an observation. Just as in the conventional classiﬁcation problem, minimization of the sample average of the cost is a difﬁcult optimization problem. As an alternative, we propose the optimization of a certain convex loss function φ, analogous to the hinge loss used in support vector machines (SVMs). Its convexity ensures that the sample average of this surrogate loss can be efﬁciently minimized. We study its statistical properties. We show that minimizing the expected surrogate loss—the φ-risk—also minimizes the risk. We also study the rate at which the φ-risk approaches its minimum value. We show that fast rates are possible when the conditional probability P(Y = 1|X) is unlikely to be close to certain critical values. Keywords: Bayes classiﬁers, classiﬁcation, convex surrogate loss, empirical risk minimization, hinge loss, large margin classiﬁers, margin condition, reject option, support vector machines</p><p>5 0.48911175 <a title="15-lda-5" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>Author: Zongming Ma, Xianchao Xie, Zhi Geng</p><p>Abstract: Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse. Keywords: chain graph, conditional independence, decomposition, graphical model, structural learning</p><p>6 0.46790043 <a title="15-lda-6" href="./jmlr-2008-Causal_Reasoning_with_Ancestral_Graphs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">20 jmlr-2008-Causal Reasoning with Ancestral Graphs    (Special Topic on Causality)</a></p>
<p>7 0.45892131 <a title="15-lda-7" href="./jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>8 0.42515838 <a title="15-lda-8" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>9 0.40779093 <a title="15-lda-9" href="./jmlr-2008-Active_Learning_of_Causal_Networks_with_Intervention_Experiments_and_Optimal_Designs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">10 jmlr-2008-Active Learning of Causal Networks with Intervention Experiments and Optimal Designs    (Special Topic on Causality)</a></p>
<p>10 0.40230569 <a title="15-lda-10" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>11 0.40153036 <a title="15-lda-11" href="./jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">80 jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>12 0.39887878 <a title="15-lda-12" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>13 0.37708437 <a title="15-lda-13" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>14 0.37621728 <a title="15-lda-14" href="./jmlr-2008-Regularization_on_Graphs_with_Function-adapted_Diffusion_Processes.html">81 jmlr-2008-Regularization on Graphs with Function-adapted Diffusion Processes</a></p>
<p>15 0.37399304 <a title="15-lda-15" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>16 0.37396982 <a title="15-lda-16" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>17 0.36696476 <a title="15-lda-17" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>18 0.35964826 <a title="15-lda-18" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>19 0.35834605 <a title="15-lda-19" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>20 0.35630584 <a title="15-lda-20" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
