<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-19" href="#">jmlr2008-19</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</h1>
<br/><p>Source: <a title="jmlr-2008-19-pdf" href="http://jmlr.org/papers/volume9/christmann08a/christmann08a.pdf">pdf</a></p><p>Author: Andreas Christmann, Arnout Van Messem</p><p>Abstract: We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand inﬂuence function (BIF) a modiﬁcation of F.R. Hampel’s inﬂuence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel’s inﬂuence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. Keywords: Bouligand derivatives, empirical risk minimization, inﬂuence function, robustness, support vector machines</p><p>Reference: <a title="jmlr-2008-19-reference" href="../jmlr2008_reference/jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 BE  Department of Mathematics Vrije Universiteit Brussel B-1050 Brussels, BELGIUM  Editor: Peter Bartlett  Abstract We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. [sent-5, score-0.115]
</p><p>2 These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. [sent-6, score-0.068]
</p><p>3 Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. [sent-7, score-0.247]
</p><p>4 Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. [sent-12, score-0.26]
</p><p>5 To formalize this aim one uses a continuous loss function L : Y × → [0, ∞) that assesses the quality of a prediction f (x) for an observed output y by L(y, f (x)). [sent-26, score-0.084]
</p><p>6 One tries to ﬁnd a predictor whose risk is close to the minimal ∗ risk, that is to the Bayes risk RL,P := inf{R L,P ( f ) ; f : d → measurable}. [sent-29, score-0.055]
</p><p>7 The Gaussian radial basis function kernel deﬁned by k RBF (x, x ) = exp(− x − x 2 /γ2 ), γ > 0, is bounded and universal on every compact subset of d (Steinwart, 2001) which partially explains its popularity. [sent-37, score-0.069]
</p><p>8 reg Of course, RL,P,λ ( f ) is not computable, because P is unknown. [sent-39, score-0.055]
</p><p>9 If we replace P by D in (1), we obtain the regularized empirical risk reg RL,D,λ ( f ) := ED L Y, f (X) + λ f 2 . [sent-42, score-0.076]
</p><p>10 Traditionally, research in nonparametric regression is often based on the least squares loss LLS (y,t) := (y − t)2 . [sent-44, score-0.073]
</p><p>11 The least squares loss function is convex in t, is useful to estimate the conditional mean function, and is advantageous from a numerical point of view, but L LS is not Lipschitz continuous. [sent-45, score-0.069]
</p><p>12 From a practical point of view there are situations in which a different loss function is more appropriate. [sent-46, score-0.047]
</p><p>13 (i) In some situations one is actually not interested in modeling the conditional mean, but in ﬁtting a conditional quantile function instead. [sent-47, score-0.058]
</p><p>14 The deeper reason to consider Lipschitz continuous loss functions is the following. [sent-53, score-0.099]
</p><p>15 ” reg reg Let us consider T (P) := RL,P,λ ( f ), with P a probability measure, as a mapping T : P → RL,P,λ ( f ). [sent-63, score-0.122]
</p><p>16 In robust statistics we are interested in smooth and bounded functions T , because this will give stable regularized risks within small neighborhoods of P. [sent-64, score-0.11]
</p><p>17 For every pair of normed real vector spaces (X,Y ) let a subset S (X,Y ) of the functions from X to Y be given. [sent-68, score-0.057]
</p><p>18 The continuous linear mapping ∇ S T (x) = A is called S -derivative of T at x. [sent-71, score-0.049]
</p><p>19 S -differentiations may be constructed in a special way by means of coverings C , whose elements are naturally assumed to be bounded sets C (so that th → 0 uniformly for h ∈ C as t → 0). [sent-76, score-0.044]
</p><p>20 For every normed real vector space X let a covering CX of X be given which consists of bounded subsets of X. [sent-77, score-0.086]
</p><p>21 If Y is another normed real vector space, deﬁne SC (X,Y ) = {ρ : X → Y | limt→0 suph∈C ρ(th) = t ρ(0) = 0 ∀C ∈ CX }. [sent-78, score-0.042]
</p><p>22 With X ranging through all normed real vector spaces, we can then deﬁne the following concepts of differentiation by varying the covering CX . [sent-80, score-0.057]
</p><p>23 One general approach to robustness (Hampel, 1968, 1974) is the one based on inﬂuence functions which are related to Gˆ teaux-derivatives. [sent-86, score-0.07]
</p><p>24 Let M1 be the set of probability distributions on a some measurable space (Z, B (Z)) and let H be a reproducing kernel Hilbert space. [sent-87, score-0.065]
</p><p>25 Within this approach robust estimators are those which have a bounded inﬂuence function. [sent-89, score-0.082]
</p><p>26 If the inﬂuence functions exists for all points z ∈ Z and if it is continuous and linear, then the IF is a special Gˆ teauxa derivative. [sent-91, score-0.088]
</p><p>27 Christmann and Steinwart (2004, 2007) and Steinwart and Christmann (2008b) showed that SVMs have a bounded inﬂuence function in binary classiﬁcation and in regression problems provided that the kernel is bounded and continuous, L is twice Fr´ chet-differentiable w. [sent-92, score-0.126]
</p><p>28 Hence Lipschitz continuous loss functions are of special interest from a robustness point of view. [sent-96, score-0.154]
</p><p>29 An example of a loss function with these properties is the logistic loss given by Llog (y,t) := − log 4Λ(y − t)(1 − Λ(y − t)) , y,t ∈ , where Λ(y − t) = 1/ 1 + e−(y−t) . [sent-97, score-0.094]
</p><p>30 However the important special cases Lε , Lτ−pin , and Lc−Huber are excluded in these results, because these loss functions are not everywhere Fr´ chet-differentiable. [sent-98, score-0.062]
</p><p>31 The second goal of this paper is to use a this new notion of robustness to show that SVMs for regression are robust in this sense even if the loss function has no Fr´ chet-derivative. [sent-101, score-0.14]
</p><p>32 For the rest of the introduction let X, Y , W , and Z be normed linear spaces, and we consider neighborhoods N (x0 ) of x0 in X, N (y0 ) of y0 in Y , and N (w0 ) of w0 in W . [sent-103, score-0.068]
</p><p>33 A function h1 strongly approximates h2 at w0 , written as h1 ≈ h2 at w0 , if for each ε > 0 there exists a neighborhood N (w0 ) of w0 such that whenever w and w belong to N (w0 ), h1 (w) − h2 (w) − h1 (w ) − h2 (w ) ≤ ε w − w . [sent-107, score-0.045]
</p><p>34 A function f strongly approximates F in x at (x0 , y0 ), written as f ≈x F at (x0 , y0 ), if for each ε > 0 there exist neighborhoods N (x0 ) of x0 and N (y0 ) of y0 such that whenever x and x belong to N (x0 ) and y belongs to N (y0 ) we have F(x, y) − f (x) − F(x , y) − f (x ) ≤ ε x − x . [sent-108, score-0.04]
</p><p>35 Given a function f from an open subset X of a normed linear space X into another normed linear space Z, we say that 1. [sent-115, score-0.084]
</p><p>36 918  B OULIGAND D ERIVATIVES AND ROBUSTNESS OF SVM S FOR R EGRESSION  f is Bouligand-differentiable at a point x0 ∈ X , if there exists a positive homogeneous function ∇B f (x0 ) : X → Z such that f (x0 + h) = f (x0 ) + ∇B f (x0 )(h) + o(h). [sent-117, score-0.037]
</p><p>37 4) because the B-derivative is not necessarily a continuous linear function. [sent-126, score-0.037]
</p><p>38 a In this paper, we will prove that many SVMs based on Lipschitz continuous loss functions have a bounded Bouligand inﬂuence function. [sent-130, score-0.143]
</p><p>39 These directional derivatives were to our best knowledge not used in robust statistics so far, but are successfully applied in approximation theory for non-smooth functions. [sent-132, score-0.039]
</p><p>40 Section 2 covers our deﬁnition of the Bouligand inﬂuence function (BIF) and contains the main result which gives the BIF for support vector machines based on a bounded kernel and a B-differentiable Lipschitz continuous convex loss function. [sent-133, score-0.202]
</p><p>41 In Section 3 it is shown that this result covers the loss functions Lε , Lτ−pin , Lc−Huber , and Llog as special cases. [sent-134, score-0.076]
</p><p>42 It is thus desirable that the function T has a bounded BIF. [sent-144, score-0.044]
</p><p>43 Hence the inﬂuence function is the special Gˆ teauxa derivative with Q = δz and h = δz − P, if the IF is continuous and linear. [sent-153, score-0.061]
</p><p>44 In general ˜ ˜ ˜ we have for B-derivatives with h = εh, where ε ∈ (0, ∞) and h ∈ X with 0 < h ≤ 2, f (x0 + h) − f (x0 ) − ∇B f (x0 )(h) h→0 h ˜ ˜ f (x0 + εh) − f (x0 ) − ε∇B f (x0 )(h) = lim ˜ ε↓0 ε h ˜ f (x0 + εh) − f (x0 ) ˜ = lim − ∇B f (x0 )(h) . [sent-157, score-0.058]
</p><p>45 We restrict attention to Lipschitz continuous loss functions, because the growth behavior of L plays an important role to obtain consistency and robustness results as was shown by Christmann and Steinwart (2007). [sent-161, score-0.139]
</p><p>46 Theorem 2 Let X ⊂ d and Y ⊂ be closed sets, H be a RKHS with a bounded, continuous kernel k, fP,λ ∈ H , and L : Y × → [0, ∞) be a convex loss function which is Lipschitz continuous w. [sent-164, score-0.168]
</p><p>47 Further, assume that L has measurable partial B-derivatives w. [sent-168, score-0.05]
</p><p>48 For some χ and each f ∈ Nδ1 ( fP,λ ), G(· , f ) is Lipschitz continuous on (−δ2 , δ2 ) with Lipschitz constant χ. [sent-176, score-0.037]
</p><p>49 2) f ∗ is Lipschitz continuous on (−δ4 , δ4 ) with Lipschitz constant | f ∗ |1 = ξ. [sent-187, score-0.037]
</p><p>50 The ﬁrst factor depends on the partial B-derivative of the loss function, and is hence bounded due to (7). [sent-202, score-0.13]
</p><p>51 For many loss functions this factor depends only on the residual term y − fP,λ (x). [sent-203, score-0.062]
</p><p>52 The following result treats SVMs based on the ε-insensitive loss function or Huber’s loss function for regression, and SVMs based on the pinball loss function for nonparametric quantile regression. [sent-208, score-0.269]
</p><p>53 These loss functions have uniformly bounded ﬁrst and second partial B-derivatives w. [sent-209, score-0.132]
</p><p>54 For the somewhat smoother Huber loss function we only need to exclude by (12) that the conditional probabilities of Y given X with respect to P and Q have no point probabilities at the two points fP,λ (x) − c and f P,λ (x) + c. [sent-219, score-0.047]
</p><p>55 Therefore, for this loss function Q can be a Dirac distribution and in this case we have BIF = IF. [sent-220, score-0.047]
</p><p>56 For the pinball loss function some calculations give BIF(Q; T, P) =  1 2λ  Z  −  X  1 2λ  P Y ≤ fP,λ (x) x − τ Φ(x) dPX (x) Z  X  Q Y ≤ fP,λ (x) x − τ Φ(x) dQX (x), 922  B OULIGAND D ERIVATIVES AND ROBUSTNESS OF SVM S FOR R EGRESSION  if the BIF exists. [sent-221, score-0.136]
</p><p>57 As will become clear from the proof, (11) and (12) guarantee that the regular conditional probabilities P(· |x) and Q(· |x) do not have large point masses at those points where the Lipschitz continuous loss function L is not F-differentiable or in small neighborhoods around these points. [sent-223, score-0.11]
</p><p>58 Even for the case of parametric quantile regression, that is for L = Lτ−pin , λ = 0 and the unbounded linear kernel k(x, x ) := x, x , some assumptions on the distribution P seem to be necessary for the existence of the IF, see Koenker (2005, p. [sent-224, score-0.097]
</p><p>59 This is—at least with the techniques we used—not possible for non-smooth loss functions as the following counterexample shows. [sent-228, score-0.062]
</p><p>60 Let us consider kernel based quantile regression based on the Gaussian RBF kernel, that is L = L τ−pin , k = kRBF , and λ > 0. [sent-229, score-0.096]
</p><p>61 Now we shall show for Llog that the assumptions (11) or (12) are not needed to obtain a bounded BIF. [sent-238, score-0.067]
</p><p>62 Obviously, these partial derivatives are bounded for all y,t ∈ . [sent-243, score-0.084]
</p><p>63 We like to mention that Corollary 6 shows that this inﬂuence function is even a Bouligand-derivative, hence positive homogeneous in h = ε(Q − P). [sent-249, score-0.038]
</p><p>64 To our best knowledge however, these concepts were not used so far to investigate robustness properties of statistical operators. [sent-254, score-0.055]
</p><p>65 The result covers e the important special cases of SVMs based on the ε-insensitive, Huber or logistic loss function for regression, and kernel based quantile regression based on the pinball loss function. [sent-262, score-0.261]
</p><p>66 The IF of SVMs based on the logistic loss was recently derived by Christmann and Steinwart (2007) and Steinwart and Christmann (2008b). [sent-263, score-0.047]
</p><p>67 Many robust estimators proposed in the literature are implicitly deﬁned as solutions of minimization problems where the objective function or loss function is continuous or Lipschitz continuous, but not necessarily twice Fr´ chet-differentiable. [sent-265, score-0.122]
</p><p>68 Bouligand-differentiation nicely ﬁlls the gap between Fr´ chet-differentiation, which is too strong for many robust estimators, and Gˆ teaux-differentiation e a which is the basis for the robustness approach based on inﬂuence functions. [sent-267, score-0.098]
</p><p>69 Bouligand-derivatives fulﬁll a chain rule and a theorem of implicit functions which is in general not true for Gˆ teauxa derivatives. [sent-268, score-0.074]
</p><p>70 1 Proofs for the Results in Section 2 For the proof of Theorem 2 we shall use the following implicit function theorem for B-derivatives, see Robinson (1991, Cor. [sent-276, score-0.042]
</p><p>71 Theorem 7 Let Y be a Banach space and X and Z be normed linear spaces. [sent-280, score-0.042]
</p><p>72 Let x 0 and y0 be points of X and Y , respectively, and let N (x0 ) be a neighborhood of x0 and N (y0 ) be a neighborhood of y0 . [sent-281, score-0.038]
</p><p>73 In particular, for some φ and each y ∈ N (y0 ), G(·, y) is assumed to be Lipschitz continuous on N (x0 ) with modulus φ. [sent-283, score-0.049]
</p><p>74 (b) f ∗ is Lipschitz continuous on N (x0 ) with modulus ξ. [sent-288, score-0.049]
</p><p>75 Then the inverse A−1 : Y → X is a bounded linear function. [sent-294, score-0.044]
</p><p>76 For ε ∈ [0, 1] we thus obtain    G(ε, f ) =  reg ∂RL,(1−ε)P+εQ,λ  ∂H  reg ( f ) = ∇B RL,(1−ε)P+εQ,λ ( f ) . [sent-302, score-0.11]
</p><p>77 2  (14)  reg Since f → RL,(1−ε)P+εQ,λ ( f ) is convex and continuous for all ε ∈ [0, 1] equation (14) shows that we have G(ε, f ) = 0 if and only if f = f (1−ε)P+εQ,λ for such ε. [sent-303, score-0.114]
</p><p>78 This expectation exists, as the term ∇B L Y, ( fP,λ (X) + h(X)) − ∇B L Y, fP,λ (X) is bounded due to 2 2 (2), (7), and k ∞ < ∞. [sent-319, score-0.044]
</p><p>79 Hence S−1 is also bounded and linear by Theorem 8. [sent-339, score-0.044]
</p><p>80 This gives the existence of a bounded BIF speciﬁed by (9) and (10). [sent-340, score-0.058]
</p><p>81 2 Calculations for the Results in Section 3 For the proof of Corollary 5 we need the partial B-derivatives for the three loss functions and also have to check that ∇B G(0, fP,λ ) is strong. [sent-342, score-0.088]
</p><p>82 We shall compute the partial B-derivatives for these loss 2 functions in advance. [sent-343, score-0.111]
</p><p>83 1 ε-I NSENSITIVE L OSS We shall show for the ε-insensitive loss L = Lε that   −h if {t < y − ε} or {y − t = ε, h < 0}   0 if {y − ε < t < y + ε} or {y − t = ε, h ≥ 0} B ∇2 L(y,t)(h) = or {y − t = −ε, h < 0}    h if {t > y + ε} or {y − t = −ε, h ≥ 0}  and ∇B L(y,t)(h) = 0. [sent-346, score-0.07]
</p><p>84 2 This gives the assertion for the ﬁrst partial B-derivative. [sent-366, score-0.052]
</p><p>85 2 P INBALL -L OSS It will be shown that for the pinball loss L = Lτ−pin we get ∇B L(y,t)(h) = 2  (1 − τ)h if {y − t < 0} or {y − t = 0, h ≥ 0} −τh if {y − t > 0} or {y − t = 0, h < 0}  and ∇B L(y,t)(h) = 0. [sent-370, score-0.104]
</p><p>86 3 H UBER L OSS It will be shown that for the Huber loss L = Lc−Huber we have ∇B L(y,t)(h) = 2  −c sign(y − t)h if |y − t| > c −(y − t)h if |y − t| ≤ c  and   h if {y − t = c, h ≥ 0} or {y − t = −c, h < 0} or {|y − t| < c} ∇B L(y,t)(h) = 2,2  0 if else . [sent-385, score-0.047]
</p><p>87 2,2 930  B OULIGAND D ERIVATIVES AND ROBUSTNESS OF SVM S FOR R EGRESSION  This gives the assertion for Huber’s loss function. [sent-416, score-0.073]
</p><p>88 Now that we have shown that these loss functions have bounded ﬁrst and second partial B-derivatives, we are ready to check if ∇ B G(0, fP,λ ) is strong in these cases. [sent-418, score-0.15]
</p><p>89 · Φ(X)  H  H H  (20)  We shall show that (20) is bounded from above by ε∗ f1 − f2 H . [sent-424, score-0.067]
</p><p>90 When we look at the ﬁrst partial B-derivatives of our loss functions, we see that we can separate them in 2 cases: for L ε and Lτ−pin there are one or more discontinuities in ∇B L, whereas ∇B L is continuous for Lc−Huber . [sent-425, score-0.11]
</p><p>91 Re2 2 call that the set D of points where Lipschitz continuous functions are not Fr´ chet-differentiable, e has Lebesgue measure zero by Rademacher’s theorem (Rademacher, 1919). [sent-426, score-0.071]
</p><p>92 4 P INBALL L OSS Using the ﬁrst part of this proof we see that for the pinball loss L = Lτ−pin we obtain |h(y, f1 (x), f2 (x))| ≤ c1 , with c1 = 1, D = {0}, m = 2, and ∇B L(y,t) = 0, for all t ∈ . [sent-435, score-0.104]
</p><p>93 5 ε-I NSENSITIVE L OSS The proof for the ε-insensitive loss L = Lε is analogous to the proof for Lτ−pin , but with c1 = 2, D = {−ε, +ε}, m = 4 and thus we must consider 4 cases instead of 2 where h(y, f 1 (x), f2 (x)) = 0. [sent-444, score-0.047]
</p><p>94 6 H UBER L OSS For Huber’s loss function L = Lc−Huber we have |∇B L(y,t)| ≤ 1 := c2 and h(y, f1 (x), f2 (x)) is 2,2 bounded by c1 = 2c. [sent-447, score-0.091]
</p><p>95 Due to symmetry of the Huber loss function, the calculations are quite similar, therefore we only consider here some cases. [sent-451, score-0.079]
</p><p>96 We only have to show that ∇B G(0, fP,λ ) is strong for L = Llog , that is that the term in (19) is bounded by ε∗ f1 − f2 H for 2 arbitrary chosen ε∗ > 0. [sent-478, score-0.062]
</p><p>97 934  B OULIGAND D ERIVATIVES AND ROBUSTNESS OF SVM S FOR R EGRESSION  Using this expansion and (2), (21), and (22) it follows that the term in (24) is bounded by 2 ∞ EP  2 k ≤  k  4 ∞  f1 − f2  ∞(  δ1 /2 + 2c4 δ2 k 1  f1 − fP,λ ∞ + 4c3 δ1  ∞  /4 + c4 δ2 k 1  2 ∞ ) + c3  f1 − f2  2 ∞  f1 − f2 H . [sent-482, score-0.044]
</p><p>98 (26)  Combining (25) and (26) shows that the term in (19) is bounded by ε ∗ f1 − f2 H with the positive constant ε∗ = k 3 δ1 k ∞ /2 + 2c4 δ2 k 2 + 4c3 δ1 k ∞ + |ε| , where δ1 > 0 and ε > 0 can be ∞ ∞ 1 chosen as small as necessary. [sent-484, score-0.044]
</p><p>99 On robust properties of convex risk minimization methods for pattern recognition. [sent-502, score-0.068]
</p><p>100 Consistency and robustness of kernel based regression in convex minimization. [sent-507, score-0.115]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fp', 0.84), ('bif', 0.308), ('ep', 0.173), ('huber', 0.11), ('pin', 0.105), ('hristmann', 0.097), ('llog', 0.097), ('christmann', 0.096), ('bouligand', 0.089), ('essem', 0.089), ('ouligand', 0.081), ('lipschitz', 0.079), ('erivatives', 0.069), ('eq', 0.068), ('steinwart', 0.059), ('quantile', 0.058), ('pinball', 0.057), ('uence', 0.056), ('reg', 0.055), ('hampel', 0.055), ('robustness', 0.055), ('egression', 0.052), ('loss', 0.047), ('cp', 0.045), ('bounded', 0.044), ('fr', 0.042), ('robinson', 0.042), ('normed', 0.042), ('lc', 0.041), ('oss', 0.041), ('svms', 0.037), ('continuous', 0.037), ('dpx', 0.034), ('cq', 0.034), ('van', 0.034), ('calculations', 0.032), ('lim', 0.029), ('koenker', 0.027), ('neighborhoods', 0.026), ('assertion', 0.026), ('lls', 0.026), ('partial', 0.026), ('robust', 0.025), ('homogeneous', 0.025), ('kernel', 0.025), ('remark', 0.025), ('averbukh', 0.024), ('bayreuth', 0.024), ('idh', 0.024), ('supy', 0.024), ('teauxa', 0.024), ('measurable', 0.024), ('shall', 0.023), ('convex', 0.022), ('svm', 0.021), ('risk', 0.021), ('uber', 0.021), ('corollary', 0.02), ('theorem', 0.019), ('neighborhood', 0.019), ('cx', 0.018), ('strong', 0.018), ('iii', 0.017), ('sup', 0.017), ('reproducing', 0.016), ('arnout', 0.016), ('dtrain', 0.016), ('inball', 0.016), ('maronna', 0.016), ('nsensitive', 0.016), ('rieder', 0.016), ('takeuchi', 0.016), ('ful', 0.016), ('chain', 0.016), ('functions', 0.015), ('rkhs', 0.015), ('differentiation', 0.015), ('derivatives', 0.014), ('iv', 0.014), ('covers', 0.014), ('existence', 0.014), ('dirac', 0.014), ('lipschitzian', 0.014), ('russian', 0.014), ('approximates', 0.014), ('sons', 0.013), ('hence', 0.013), ('hilbert', 0.013), ('regression', 0.013), ('machines', 0.013), ('estimators', 0.013), ('nonparametric', 0.013), ('predictor', 0.013), ('rl', 0.013), ('valid', 0.012), ('bijective', 0.012), ('modulus', 0.012), ('nonsmooth', 0.012), ('exists', 0.012), ('mapping', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="19-tfidf-1" href="./jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>Author: Andreas Christmann, Arnout Van Messem</p><p>Abstract: We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand inﬂuence function (BIF) a modiﬁcation of F.R. Hampel’s inﬂuence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel’s inﬂuence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. Keywords: Bouligand derivatives, empirical risk minimization, inﬂuence function, robustness, support vector machines</p><p>2 0.24257721 <a title="19-tfidf-2" href="./jmlr-2008-Closed_Sets_for_Labeled_Data.html">22 jmlr-2008-Closed Sets for Labeled Data</a></p>
<p>Author: Gemma C. Garriga, Petra Kralj, Nada Lavrač</p><p>Abstract: Closed sets have been proven successful in the context of compacted data representation for association rule learning. However, their use is mainly descriptive, dealing only with unlabeled data. This paper shows that when considering labeled data, closed sets can be adapted for classiﬁcation and discrimination purposes by conveniently contrasting covering properties on positive and negative examples. We formally prove that these sets characterize the space of relevant combinations of features for discriminating the target class. In practice, identifying relevant/irrelevant combinations of features through closed sets is useful in many applications: to compact emerging patterns of typical descriptive mining applications, to reduce the number of essential rules in classiﬁcation, and to efﬁciently learn subgroup descriptions, as demonstrated in real-life subgroup discovery experiments on a high dimensional microarray data set. Keywords: rule relevancy, closed sets, ROC space, emerging patterns, essential rules, subgroup discovery</p><p>3 0.12529896 <a title="19-tfidf-3" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>4 0.095588945 <a title="19-tfidf-4" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>Author: Michiel Debruyne, Mia Hubert, Johan A.K. Suykens</p><p>Abstract: Recent results about the robustness of kernel methods involve the analysis of inﬂuence functions. By deﬁnition the inﬂuence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the inﬂuence function is used in a similar way to analyze the statistical efﬁciency of a method. Links between both worlds are explored. The inﬂuence function is related to the ﬁrst term of a Taylor expansion. Higher order inﬂuence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate inﬂuence functions at a speciﬁc sample distribution to obtain an approximation of the leave-one-out error. A speciﬁc implementation is proposed using a L1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data. Keywords: kernel based regression, robustness, stability, inﬂuence function, model selection</p><p>5 0.058554024 <a title="19-tfidf-5" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>Author: Eric Perrier, Seiya Imoto, Satoru Miyano</p><p>Abstract: Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. However, a recent empirical study has shown that a hybrid algorithm improves sensitively accuracy and speed: it learns a skeleton with an independency test (IT) approach and constrains on the directed acyclic graphs (DAG) considered during the search-and-score phase. Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(γm n ), where γm < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree m and sparse structures ˜ allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even ﬁnds more accurate results when given a sound super-structure. Practically, S can be approximated by IT approaches; signiﬁcance level of the tests controls its sparseness, enabling to control the trade-off between speed and accuracy. For incomplete super-structures, a greedily post-processed version (COS+) still enables to signiﬁcantly outperform other heuristic searches. Keywords: subset Bayesian networks, structure learning, optimal search, super-structure, connected</p><p>6 0.047103163 <a title="19-tfidf-6" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>7 0.040101476 <a title="19-tfidf-7" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>8 0.036902945 <a title="19-tfidf-8" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>9 0.032075722 <a title="19-tfidf-9" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>10 0.030215669 <a title="19-tfidf-10" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>11 0.029354725 <a title="19-tfidf-11" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>12 0.028924929 <a title="19-tfidf-12" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>13 0.023408974 <a title="19-tfidf-13" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>14 0.022777911 <a title="19-tfidf-14" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>15 0.022552382 <a title="19-tfidf-15" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>16 0.021693679 <a title="19-tfidf-16" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>17 0.021305308 <a title="19-tfidf-17" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>18 0.020592794 <a title="19-tfidf-18" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>19 0.019975243 <a title="19-tfidf-19" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>20 0.017889686 <a title="19-tfidf-20" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.116), (1, -0.03), (2, -0.07), (3, -0.047), (4, 0.103), (5, 0.023), (6, 0.031), (7, -0.151), (8, 0.158), (9, 0.339), (10, -0.32), (11, -0.334), (12, -0.334), (13, -0.157), (14, -0.061), (15, 0.189), (16, 0.033), (17, 0.007), (18, -0.118), (19, -0.027), (20, 0.039), (21, -0.003), (22, 0.067), (23, -0.002), (24, 0.064), (25, 0.088), (26, -0.012), (27, 0.073), (28, 0.06), (29, -0.01), (30, 0.013), (31, 0.05), (32, -0.032), (33, 0.001), (34, -0.039), (35, -0.027), (36, 0.0), (37, -0.047), (38, -0.001), (39, -0.0), (40, -0.024), (41, -0.032), (42, 0.015), (43, -0.003), (44, -0.009), (45, 0.007), (46, 0.002), (47, -0.027), (48, -0.029), (49, -0.002)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97125602 <a title="19-lsi-1" href="./jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>Author: Andreas Christmann, Arnout Van Messem</p><p>Abstract: We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand inﬂuence function (BIF) a modiﬁcation of F.R. Hampel’s inﬂuence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel’s inﬂuence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. Keywords: Bouligand derivatives, empirical risk minimization, inﬂuence function, robustness, support vector machines</p><p>2 0.79040271 <a title="19-lsi-2" href="./jmlr-2008-Closed_Sets_for_Labeled_Data.html">22 jmlr-2008-Closed Sets for Labeled Data</a></p>
<p>Author: Gemma C. Garriga, Petra Kralj, Nada Lavrač</p><p>Abstract: Closed sets have been proven successful in the context of compacted data representation for association rule learning. However, their use is mainly descriptive, dealing only with unlabeled data. This paper shows that when considering labeled data, closed sets can be adapted for classiﬁcation and discrimination purposes by conveniently contrasting covering properties on positive and negative examples. We formally prove that these sets characterize the space of relevant combinations of features for discriminating the target class. In practice, identifying relevant/irrelevant combinations of features through closed sets is useful in many applications: to compact emerging patterns of typical descriptive mining applications, to reduce the number of essential rules in classiﬁcation, and to efﬁciently learn subgroup descriptions, as demonstrated in real-life subgroup discovery experiments on a high dimensional microarray data set. Keywords: rule relevancy, closed sets, ROC space, emerging patterns, essential rules, subgroup discovery</p><p>3 0.31598651 <a title="19-lsi-3" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>4 0.30943799 <a title="19-lsi-4" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>Author: Michiel Debruyne, Mia Hubert, Johan A.K. Suykens</p><p>Abstract: Recent results about the robustness of kernel methods involve the analysis of inﬂuence functions. By deﬁnition the inﬂuence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the inﬂuence function is used in a similar way to analyze the statistical efﬁciency of a method. Links between both worlds are explored. The inﬂuence function is related to the ﬁrst term of a Taylor expansion. Higher order inﬂuence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate inﬂuence functions at a speciﬁc sample distribution to obtain an approximation of the leave-one-out error. A speciﬁc implementation is proposed using a L1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data. Keywords: kernel based regression, robustness, stability, inﬂuence function, model selection</p><p>5 0.14768228 <a title="19-lsi-5" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>Author: Eric Perrier, Seiya Imoto, Satoru Miyano</p><p>Abstract: Classical approaches used to learn Bayesian network structure from data have disadvantages in terms of complexity and lower accuracy of their results. However, a recent empirical study has shown that a hybrid algorithm improves sensitively accuracy and speed: it learns a skeleton with an independency test (IT) approach and constrains on the directed acyclic graphs (DAG) considered during the search-and-score phase. Subsequently, we theorize the structural constraint by introducing the concept of super-structure S, which is an undirected graph that restricts the search to networks whose skeleton is a subgraph of S. We develop a super-structure constrained optimal search (COS): its time complexity is upper bounded by O(γm n ), where γm < 2 depends on the maximal degree m of S. Empirically, complexity depends on the average degree m and sparse structures ˜ allow larger graphs to be calculated. Our algorithm is faster than an optimal search by several orders and even ﬁnds more accurate results when given a sound super-structure. Practically, S can be approximated by IT approaches; signiﬁcance level of the tests controls its sparseness, enabling to control the trade-off between speed and accuracy. For incomplete super-structures, a greedily post-processed version (COS+) still enables to signiﬁcantly outperform other heuristic searches. Keywords: subset Bayesian networks, structure learning, optimal search, super-structure, connected</p><p>6 0.13186787 <a title="19-lsi-6" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>7 0.11956536 <a title="19-lsi-7" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>8 0.10553683 <a title="19-lsi-8" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>9 0.104426 <a title="19-lsi-9" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>10 0.10331179 <a title="19-lsi-10" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>11 0.093633488 <a title="19-lsi-11" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>12 0.090794772 <a title="19-lsi-12" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>13 0.090743482 <a title="19-lsi-13" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>14 0.089505687 <a title="19-lsi-14" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>15 0.088828631 <a title="19-lsi-15" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>16 0.084921107 <a title="19-lsi-16" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>17 0.082463942 <a title="19-lsi-17" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>18 0.077003919 <a title="19-lsi-18" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>19 0.075304516 <a title="19-lsi-19" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>20 0.073910668 <a title="19-lsi-20" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.06), (31, 0.011), (40, 0.019), (54, 0.039), (58, 0.026), (60, 0.042), (66, 0.044), (76, 0.019), (78, 0.027), (88, 0.073), (89, 0.414), (92, 0.04), (94, 0.042), (99, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71825159 <a title="19-lda-1" href="./jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>Author: Andreas Christmann, Arnout Van Messem</p><p>Abstract: We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand inﬂuence function (BIF) a modiﬁcation of F.R. Hampel’s inﬂuence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel’s inﬂuence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. Keywords: Bouligand derivatives, empirical risk minimization, inﬂuence function, robustness, support vector machines</p><p>2 0.65591681 <a title="19-lda-2" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>Author: Falk-Florian Henrich, Klaus Obermayer</p><p>Abstract: We introduce a computationally feasible, “constructive” active learning method for binary classiﬁcation. The learning algorithm is initially formulated for separable classiﬁcation problems, for a hyperspherical data space with constant data density, and for great spheres as classiﬁers. In order to reduce computational complexity the version space is restricted to spherical simplices and learning procedes by subdividing the edges of maximal length. We show that this procedure optimally reduces a tight upper bound on the generalization error. The method is then extended to other separable classiﬁcation problems using products of spheres as data spaces and isometries induced by charts of the sphere. An upper bound is provided for the probability of disagreement between classiﬁers (hence the generalization error) for non-constant data densities on the sphere. The emphasis of this work lies on providing mathematically exact performance estimates for active learning strategies. Keywords: active learning, spherical subdivision, error bounds, simplex halving</p><p>3 0.27375725 <a title="19-lda-3" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>Author: Michiel Debruyne, Mia Hubert, Johan A.K. Suykens</p><p>Abstract: Recent results about the robustness of kernel methods involve the analysis of inﬂuence functions. By deﬁnition the inﬂuence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the inﬂuence function is used in a similar way to analyze the statistical efﬁciency of a method. Links between both worlds are explored. The inﬂuence function is related to the ﬁrst term of a Taylor expansion. Higher order inﬂuence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate inﬂuence functions at a speciﬁc sample distribution to obtain an approximation of the leave-one-out error. A speciﬁc implementation is proposed using a L1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data. Keywords: kernel based regression, robustness, stability, inﬂuence function, model selection</p><p>4 0.27060485 <a title="19-lda-4" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>Author: Peter L. Bartlett, Marten H. Wegkamp</p><p>Abstract: We consider the problem of binary classiﬁcation where the classiﬁer can, for a particular cost, choose not to classify an observation. Just as in the conventional classiﬁcation problem, minimization of the sample average of the cost is a difﬁcult optimization problem. As an alternative, we propose the optimization of a certain convex loss function φ, analogous to the hinge loss used in support vector machines (SVMs). Its convexity ensures that the sample average of this surrogate loss can be efﬁciently minimized. We study its statistical properties. We show that minimizing the expected surrogate loss—the φ-risk—also minimizes the risk. We also study the rate at which the φ-risk approaches its minimum value. We show that fast rates are possible when the conditional probability P(Y = 1|X) is unlikely to be close to certain critical values. Keywords: Bayes classiﬁers, classiﬁcation, convex surrogate loss, empirical risk minimization, hinge loss, large margin classiﬁers, margin condition, reject option, support vector machines</p><p>5 0.26898876 <a title="19-lda-5" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>Author: Rémi Munos, Csaba Szepesvári</p><p>Abstract: In this paper we develop a theoretical analysis of the performance of sampling-based ﬁtted value iteration (FVI) to solve inﬁnite state-space, discounted-reward Markovian decision processes (MDPs) under the assumption that a generative model of the environment is available. Our main results come in the form of ﬁnite-time bounds on the performance of two versions of sampling-based FVI. The convergence rate results obtained allow us to show that both versions of FVI are well behaving in the sense that by using a sufﬁciently large number of samples for a large class of MDPs, arbitrary good performance can be achieved with high probability. An important feature of our proof technique is that it permits the study of weighted L p -norm performance bounds. As a result, our technique applies to a large class of function-approximation methods (e.g., neural networks, adaptive regression trees, kernel machines, locally weighted learning), and our bounds scale well with the effective horizon of the MDP. The bounds show a dependence on the stochastic stability properties of the MDP: they scale with the discounted-average concentrability of the future-state distributions. They also depend on a new measure of the approximation power of the function space, the inherent Bellman residual, which reﬂects how well the function space is “aligned” with the dynamics and rewards of the MDP. The conditions of the main result, as well as the concepts introduced in the analysis, are extensively discussed and compared to previous theoretical results. Numerical experiments are used to substantiate the theoretical ﬁndings. Keywords: ﬁtted value iteration, discounted Markovian decision processes, generative model, reinforcement learning, supervised learning, regression, Pollard’s inequality, statistical learning theory, optimal control</p><p>6 0.26552364 <a title="19-lda-6" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>7 0.26396099 <a title="19-lda-7" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>8 0.26264021 <a title="19-lda-8" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>9 0.26242459 <a title="19-lda-9" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>10 0.25998423 <a title="19-lda-10" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>11 0.25753 <a title="19-lda-11" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>12 0.255867 <a title="19-lda-12" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>13 0.25533459 <a title="19-lda-13" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>14 0.25520042 <a title="19-lda-14" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>15 0.25329563 <a title="19-lda-15" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>16 0.25276875 <a title="19-lda-16" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>17 0.2523627 <a title="19-lda-17" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>18 0.25220907 <a title="19-lda-18" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>19 0.25192803 <a title="19-lda-19" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>20 0.25113934 <a title="19-lda-20" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
