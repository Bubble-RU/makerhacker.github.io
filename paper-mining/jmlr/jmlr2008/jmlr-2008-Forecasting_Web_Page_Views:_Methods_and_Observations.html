<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-37" href="#">jmlr2008-37</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</h1>
<br/><p>Source: <a title="jmlr-2008-37-pdf" href="http://jmlr.org/papers/volume9/li08a/li08a.pdf">pdf</a></p><p>Author: Jia Li, Andrew W. Moore</p><p>Abstract: Web sites must forecast Web page views in order to plan computer resource allocation and estimate upcoming revenue and advertising growth. In this paper, we focus on extracting trends and seasonal patterns from page view series, two dominant factors in the variation of such series. We investigate the Holt-Winters procedure and a state space model for making relatively short-term prediction. It is found that Web page views exhibit strong impulsive changes occasionally. The impulses cause large prediction errors long after their occurrences. A method is developed to identify impulses and to alleviate their damage on prediction. We also develop a long-range trend and season extraction method, namely the Elastic Smooth Season Fitting (ESSF) algorithm, to compute scalable and smooth yearly seasons. ESSF derives the yearly season by minimizing the residual sum of squares under smoothness regularization, a quadratic optimization problem. It is shown that for longterm prediction, ESSF improves accuracy signiﬁcantly over other methods that ignore the yearly seasonality. Keywords: web page views, forecast, Holt-Winters, Kalman ﬁltering, elastic smooth season ﬁtting</p><p>Reference: <a title="jmlr-2008-37-reference" href="../jmlr2008_reference/jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We also develop a long-range trend and season extraction method, namely the Elastic Smooth Season Fitting (ESSF) algorithm, to compute scalable and smooth yearly seasons. [sent-12, score-0.951]
</p><p>2 ESSF derives the yearly season by minimizing the residual sum of squares under smoothness regularization, a quadratic optimization problem. [sent-13, score-0.866]
</p><p>3 Keywords: web page views, forecast, Holt-Winters, Kalman ﬁltering, elastic smooth season ﬁtting  1. [sent-15, score-0.704]
</p><p>4 There are multiple approaches to trend and season removal (Brockwell and Davis, 2002). [sent-31, score-0.611]
</p><p>5 In some cases, however, trend and season may be the dominant factors in prediction and require methods devoted to their extraction. [sent-40, score-0.658]
</p><p>6 For daily page view series, there is usually a weekly season and sometimes a long-range yearly season. [sent-53, score-1.042]
</p><p>7 Both HW and SSM can effectively extract the weekly season, but not the yearly season for several reasons elaborated in Section 4. [sent-54, score-0.921]
</p><p>8 It is observed that instead of being a periodic sequence, the yearly seasonality often emerges as a yearly pattern that may scale differently across the years. [sent-56, score-0.698]
</p><p>9 Experiments show that the prediction accuracy can be improved remarkably based on the yearly season computed by ESSF, especially for forecasting distant future. [sent-58, score-0.972]
</p><p>10 For long-term prediction several months ahead, we develop the ESSF algorithm to extract global trends and scalable yearly seasonal effects after separating the weekly season using HW. [sent-81, score-1.041]
</p><p>11 3 Application Scope The prediction methods in this paper focus on extracting trend and season at several scales, and are not suitable for modeling stationary stochastic processes. [sent-83, score-0.658]
</p><p>12 The yearly season extraction by ESSF is found to improve long-term prediction. [sent-92, score-0.839]
</p><p>13 The basic assumption of ESSF is that the time series exhibits a yearly pattern, possibly scaled differently across the years. [sent-93, score-0.417]
</p><p>14 Therefore, the series are insufﬁcient for exploiting 2219  L I AND M OORE  yearly seasonality in prediction. [sent-100, score-0.469]
</p><p>15 The Holt-Winters (HW) procedure (Chatﬁeld, 2004) decomposes the series into level Lt , season It , and noise. [sent-120, score-0.628]
</p><p>16 To better see this, let us assume the season and linear growth terms are absent. [sent-125, score-0.578]
</p><p>17 When the season is added, xt subtracted by the estimated season at t becomes the part of Lt indicated by current information. [sent-134, score-1.18]
</p><p>18 At this point of recursion, the most up-to-date estimation for the season at t is It−d under period d. [sent-135, score-0.552]
</p><p>19 Similar rationale applies to the update 2220  F ORECASTING W EB PAGE V IEWS  Original series Predicted series Error  1  Original series Predicted series  1  1  0. [sent-139, score-0.412]
</p><p>20 The linear function of h, Lt + hTt , with slope given by the most updated linear growth Tt , can be regarded as an estimation for Lt+h ; while It−d+h mod d , the most updated season at the same cyclic position as t + h, which is already available at t, is the estimation for It+h . [sent-186, score-0.578]
</p><p>21 At time t, let the prediction for xt based on the past series up to t − 1 be xt , and the prediction ˆ error be et = xt − xt . [sent-220, score-0.752]
</p><p>22 2222  F ORECASTING W EB PAGE V IEWS  Apply HW to obtain level L τ and weekly season I τ at τ =1,2,. [sent-240, score-0.607]
</p><p>23 , 2D Detect impulse and adjust the series up to t, using the HW procedure  Update L t , T t , I t  Does impulse exist? [sent-243, score-0.425]
</p><p>24 Apply ESSF to L τ in the immediate past two years to obtain yearly season  No  Estimate parameters H and Q based on series up to time t, using Kalman filtering and the EM algorithm  Initialize Kalman filter at τ=1  Yes Adjust I t , I t−1, . [sent-244, score-1.019]
</p><p>25 Forecast future at t  2D+1−−> t  Apply HW recursion to obtain L and I t  t  Compute global linear trend using level subtracted by yearly season  Forecast at time t  Yes  t+1−−> t  Forecast future at t based on Kalman filtering result  Does t enter a new year? [sent-248, score-0.993]
</p><p>26 This is equivalent to discarding the season computed during the impulse segment and using the most recent season right before the impulse. [sent-255, score-1.211]
</p><p>27 Let Lt = 1 Lt−s1 + 1 (xt − It ), where Lt−s1 is the level before the impulse and It is the already 2 2 revised season at t. [sent-257, score-0.686]
</p><p>28 1 The Level with Season Model Denote the level at t by µt and the season with period d by it . [sent-293, score-0.552]
</p><p>29 In its simplest form, with both the linear growth and season term removed, HW reduces to exponential smoothing with recursion Lt = ζxt + (1 − ζ)Lt−1 . [sent-300, score-0.661]
</p><p>30 When season is added, there is no complete match between the recursion of HW and that of the Kalman ﬁlter. [sent-306, score-0.575]
</p><p>31 In the LS model, it is assumed that ∑d it+τ = 0 τ=1 up to white noise, but HW does not enforce the zero sum of one period of the season terms. [sent-307, score-0.552]
</p><p>32 The decomposition of xt into level µt and season it by LS is however similar to that assumed by HW. [sent-308, score-0.655]
</p><p>33 Each time series demonstrates apparently a global trend and yearly seasonality. [sent-408, score-0.503]
</p><p>34 Assume the period of the long-range season is a year. [sent-412, score-0.552]
</p><p>35 Because the Internet is highly dynamic, it is necessary to derive the yearly season using past data over recent periods and usually only a few (e. [sent-413, score-0.874]
</p><p>36 A mechanism to control the smoothness of the long-range season is needed. [sent-417, score-0.552]
</p><p>37 By enforcing smoothness, the extracted season tends to be more robust, a valuable feature especially when given limited past data. [sent-418, score-0.56]
</p><p>38 The magnitude of the yearly season may vary across the years. [sent-420, score-0.839]
</p><p>39 The yearly season thus should be allowed to scale. [sent-422, score-0.839]
</p><p>40 Furthermore, HW requires a relatively large number of periods to settle to the intended season; and importantly, HW assumes a ﬁxed season over the years. [sent-427, score-0.525]
</p><p>41 Although HW is capable of adjusting with a slowly changing season when given enough periods of data, it does not directly treat the scaling of the season, and hence is vulnerable to the scaling phenomenon. [sent-428, score-0.525]
</p><p>42 We inject elasticity into the yearly season and allow it to scale from a certain yearly pattern. [sent-430, score-1.153]
</p><p>43 1 Elastic Smooth Season Fitting Before extracting long-range trend and season, we apply HW with impulse detection to obtain the weekly season and the smoothed level series Lt , t = 1, . [sent-434, score-0.975]
</p><p>44 We want to exploit the global trend and yearly season existing in the level series to better predict Lt+h based on {L1 , L2 , . [sent-439, score-1.028]
</p><p>45 , n, into a yearly season, yt , a global linear trend ut , and a volatility part nt : Lt = ut + yt + nt ,  t = 1, 2, . [sent-446, score-0.556]
</p><p>46 Thus the original series xt is decomposed into: xt = ut + yt + nt + It + Nt ,  (9)  where It and Nt are the season and noise terms from HW. [sent-450, score-0.986]
</p><p>47 Here zt is the yearly season combined with noise, taking out the current estimate of the global trend. [sent-485, score-0.886]
</p><p>48 We now present the ESSF algorithm for computing the yearly season based on the trend removed z. [sent-497, score-0.925]
</p><p>49 Denote the residue zt = Lt − ut by zk, j , the yearly season yt by yk, j (we abuse the notation here and assume the meaning is clear from the context), and the noise term n t by nk, j . [sent-499, score-0.928]
</p><p>50 We call the yearly season pattern y = {y1 , y2 , . [sent-502, score-0.839]
</p><p>51 Since we allow the yearly season yk, j to scale over time, it relates to the season template by yk, j = αk, j y j ,  k = 1, 2, . [sent-506, score-1.364]
</p><p>52 We optimize over both the season template y j , j = 1, . [sent-531, score-0.525]
</p><p>53 Since y is one period of the yearly season, when j is out of the range [1, D], y j is understood as y j mod D . [sent-549, score-0.341]
</p><p>54 Experiments show that allowing scalable yearly season improves prediction accuracy, so does the smoothness regularization of the yearly season. [sent-556, score-1.253]
</p><p>55 A more ad-hoc approach to enforce smoothness is to apply moving average to the yearly season extracted without smoothness regularization. [sent-559, score-0.911]
</p><p>56 To predict xt , the weekly season extracted by HW should be added to the level Lt . [sent-575, score-0.737]
</p><p>57 We assume that prediction starts on the 3rd year since the ﬁrst two years have to serve as past data for computing the yearly season. [sent-577, score-0.466]
</p><p>58 Apply HW to obtain the weekly season It , and the level Lt , t = 1, 2, . [sent-579, score-0.607]
</p><p>59 , take the series of Lt ’s in the past two years (year k − 2 and k − 1) and apply ESSF to this series to solve the yearly season template y and the scaling factors, c1 and c2 for year k − 2 and k − 1 respectively. [sent-587, score-1.15]
</p><p>60 Predict the yearly season for future years k ≥ k by c2 y. [sent-588, score-0.863]
</p><p>61 Denote the predicted yearly season at time t in any year k ≥ k by Yt,k , where the second subscript clariﬁes that only the series before year k is used by ESSF. [sent-589, score-1.071]
</p><p>62 Let the yearly season removed level be Lt = Lt − ˜ t−2D+1 , . [sent-592, score-0.839]
</p><p>63 5  Weekly season Yearly season Long−range growth  2. [sent-602, score-1.103]
</p><p>64 (14) and (9), we see that Lt + hTt is essentially the prediction for the global linear trend term ut+h , Yt+h,ν(t) the prediction for the yearly season yt+h , and It+h−r(t+h)·d the prediction for the weekly season It+h . [sent-617, score-1.673]
</p><p>65 If day t +h is in the same year as t, Yt+h,ν(t) = Yt+h,ν(t+h) is the freshest possible prediction for the yearly season at t + h. [sent-619, score-0.992]
</p><p>66 If instead ν(t) < ν(t + h), the yearly season at t + h is predicted based on data more than one year ago. [sent-620, score-0.922]
</p><p>67 One might have noticed that we use only two years of data to extract the yearly season regardless of the available amount of past data. [sent-621, score-0.898]
</p><p>68 Figure 3(a) shows that during these two months, the predicted yearly season is much more prominent than the weekly season and the slight linear growth. [sent-626, score-1.483]
</p><p>69 The series predicted by HW is weekly periodic with a ﬂat level, while that by ESSF incorporates the yearly seasonal variation and is much closer to the original series, as one might have expected. [sent-628, score-0.603]
</p><p>70 t −1  We can consider xt , the mean up to time t −1, as the simplest prediction of xt using past data. [sent-674, score-0.342]
</p><p>71 ˜  t=t0 +h  t=t0 +h  For the series Auton, Wang, and citeseer, t0 = 4d, where d is the season period. [sent-721, score-0.628]
</p><p>72 Because the series are not long enough for extracting long-range trend and season by the ESSF algorithm, we only test the HW procedure with or without impulse detection and the GLS approach. [sent-731, score-0.893]
</p><p>73 The smoothing parameters for the level and the season terms in Eq. [sent-742, score-0.558]
</p><p>74 To assess the importance of weekly (or daily) seasonality for forecasting, we compare HW and and its reduced form without the season term. [sent-747, score-0.659]
</p><p>75 Similarly as the linear growth term, the season term can be deleted by initializing it to zero and setting its corresponding smoothing parameter δ in Eq. [sent-748, score-0.611]
</p><p>76 The reduced HW procedure without the local linear growth and season terms is essentially Exponential Smoothing (ES) (Chatﬁeld, 2004). [sent-750, score-0.578]
</p><p>77 5  1 Wang  1 Citeseer  (b) ad j  Figure 4: Compare the prediction performance in terms of R e and Re on the three series Auton, Wang, and citeseer using different methods: HW with or without impulse detection, ES without season, MA, and MP. [sent-772, score-0.406]
</p><p>78 The predicted series by HW with impulse detection returns close to the original series shortly after the impulse, while that without ripples with large errors over several periods afterward. [sent-785, score-0.422]
</p><p>79 The ﬂuctuation of the predicted series obtained 2234  F ORECASTING W EB PAGE V IEWS  4  8  x 10  Original series HW HW impulse detection  Number of daily page views  7 6 5 4 3 2 1 0  400  410  420  430  440 Day  450  460  470  (a) 5  4  x 10  Number of weekly page views  3. [sent-798, score-0.841]
</p><p>80 5 0  50  100  150  200 250 Starting day  300  350  400  (b) Figure 5: Compare predicted series for Auton: (a) Results obtained by HW with and without impulse detection. [sent-802, score-0.361]
</p><p>81 Figure 7(c) and (d) show the yearly season templates extracted by ESSF from year 2004 and 2005 with smoothing parameter λ = 0, 1000 respectively. [sent-945, score-0.918]
</p><p>82 As expected, at λ = 1000, the yearly seasons are much smoother than those obtained at λ = 0, especially for the series Renoir and greenhouse effect. [sent-946, score-0.516]
</p><p>83 6  0 2004 Jan  2005 Jan  2006 Jan  2007 Jan  2  Renoir  1 0 2004 Jan  2005 Jan  2006 Jan  2007 Jan greenhouse effect  2  Scaling factor for yearly season  amazon 1. [sent-957, score-0.993]
</p><p>84 (d) Error rates obtained for the three series by ESSF, with λ = 1000, assuming a scalable yearly season versus ﬁxed season. [sent-986, score-0.994]
</p><p>85 This demonstrates the advantage of imposing smoothness on the yearly season. [sent-993, score-0.341]
</p><p>86 First, recall that the ﬁtting of the yearly season and the long-range trend is repeated multiple times, as described in Section 4. [sent-996, score-0.925]
</p><p>87 To study the effect of the number of iterations, we plot in Figure 9(a) the ratio of change in the yearly season after each iteration, as given by Eq. [sent-998, score-0.839]
</p><p>88 In ESSF, the yearly season is not assumed simply as a periodic series. [sent-1010, score-0.857]
</p><p>89 Instead, it can scale differently over the years based on the season template. [sent-1011, score-0.549]
</p><p>90 To evaluate the gain, we compare ESSF with scalable yearly seasons versus ﬁxed seasons. [sent-1012, score-0.362]
</p><p>91 Here, the ﬁxed season can be thought of as a special case of the scalable season with all the scaling parameters set to 1, or equivalently, the yearly season is the plain repeat of the season template. [sent-1013, score-2.44]
</p><p>92 Better performance is achieved by allowing scalable yearly seasons for all the three series. [sent-1024, score-0.362]
</p><p>93 In addition to the variation rate, the yearly correlation of the time series also indicates the potential for accurate prediction. [sent-1073, score-0.44]
</p><p>94 A series with high yearly correlation tends to beneﬁt more in prediction from the yearly season extraction of ESSF. [sent-1078, score-1.303]
</p><p>95 In contrast, for NBA (ID 16) and democracy (ID 9), which have high yearly correlation, ESSF achieves substantially better prediction accuracy than HW and MA at all the values of h. [sent-1081, score-0.361]
</p><p>96 To compare the computational load of the prediction algorithms, we acquire the average user running time over the twenty g-trends series for one day ahead (h = 1) prediction at all the days in the last two years, 2006 and 2007. [sent-1082, score-0.382]
</p><p>97 Discussion and Conclusions We have so far focused on extracting the trend and season parts of a time series using either HW or GLS, and have not considered predicting the noise part, as given in Eq. [sent-1091, score-0.734]
</p><p>98 Speciﬁcally, we compute the level Lt and the season It by HW and let the noise Nt = xt − Lt − It . [sent-1095, score-0.675]
</p><p>99 We developed the ESSF algorithm to extract global trend and scalable long-range season with smoothness regularization. [sent-1129, score-0.664]
</p><p>100 Let the one-step forecast error of xt given Xt−1 be vt and the variance of vt be Ft : vt  = xt − E(xt | Xt−1 ) = xt − Zt at ,  Ft  = Var(vt ) = Zt Pt Zt + Ht . [sent-1198, score-0.52]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('season', 0.525), ('hw', 0.394), ('essf', 0.362), ('yearly', 0.314), ('lt', 0.222), ('impulse', 0.161), ('gls', 0.146), ('xt', 0.13), ('auton', 0.112), ('ssm', 0.108), ('series', 0.103), ('page', 0.096), ('trend', 0.086), ('weekly', 0.082), ('amazon', 0.077), ('greenhouse', 0.077), ('citeseer', 0.077), ('oore', 0.073), ('renoir', 0.073), ('tt', 0.072), ('iews', 0.069), ('orecasting', 0.069), ('jan', 0.069), ('traf', 0.066), ('kalman', 0.065), ('web', 0.065), ('impulses', 0.065), ('forecasting', 0.062), ('views', 0.06), ('day', 0.06), ('ahead', 0.059), ('nt', 0.056), ('growth', 0.053), ('seasonality', 0.052), ('recursion', 0.05), ('prediction', 0.047), ('zt', 0.047), ('year', 0.046), ('eb', 0.045), ('forecast', 0.043), ('pt', 0.04), ('wang', 0.038), ('days', 0.038), ('predicted', 0.037), ('qt', 0.036), ('past', 0.035), ('xn', 0.034), ('smoothing', 0.033), ('id', 0.033), ('ht', 0.033), ('internet', 0.033), ('var', 0.032), ('zk', 0.031), ('window', 0.031), ('ft', 0.03), ('dec', 0.029), ('nov', 0.029), ('rss', 0.029), ('vt', 0.029), ('twenty', 0.028), ('smoothness', 0.027), ('kt', 0.027), ('period', 0.027), ('units', 0.027), ('rates', 0.026), ('brockwell', 0.026), ('impulsive', 0.026), ('seasonal', 0.026), ('sss', 0.026), ('scalable', 0.026), ('daily', 0.025), ('distant', 0.024), ('ay', 0.024), ('years', 0.024), ('ck', 0.024), ('variation', 0.023), ('apr', 0.022), ('jun', 0.022), ('aug', 0.022), ('feb', 0.022), ('ite', 0.022), ('jul', 0.022), ('oct', 0.022), ('yt', 0.022), ('chat', 0.022), ('seasons', 0.022), ('trends', 0.021), ('lter', 0.021), ('noise', 0.02), ('ls', 0.02), ('re', 0.019), ('ma', 0.019), ('ad', 0.018), ('sites', 0.018), ('elastic', 0.018), ('periodic', 0.018), ('detection', 0.018), ('filtering', 0.018), ('moving', 0.018), ('mp', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="37-tfidf-1" href="./jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations.html">37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</a></p>
<p>Author: Jia Li, Andrew W. Moore</p><p>Abstract: Web sites must forecast Web page views in order to plan computer resource allocation and estimate upcoming revenue and advertising growth. In this paper, we focus on extracting trends and seasonal patterns from page view series, two dominant factors in the variation of such series. We investigate the Holt-Winters procedure and a state space model for making relatively short-term prediction. It is found that Web page views exhibit strong impulsive changes occasionally. The impulses cause large prediction errors long after their occurrences. A method is developed to identify impulses and to alleviate their damage on prediction. We also develop a long-range trend and season extraction method, namely the Elastic Smooth Season Fitting (ESSF) algorithm, to compute scalable and smooth yearly seasons. ESSF derives the yearly season by minimizing the residual sum of squares under smoothness regularization, a quadratic optimization problem. It is shown that for longterm prediction, ESSF improves accuracy signiﬁcantly over other methods that ignore the yearly seasonality. Keywords: web page views, forecast, Holt-Winters, Kalman ﬁltering, elastic smooth season ﬁtting</p><p>2 0.063749455 <a title="37-tfidf-2" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>Author: Tianjiao Chu, Clark Glymour</p><p>Abstract: Pointwise consistent, feasible procedures for estimating contemporaneous linear causal structure from time series data have been developed using multiple conditional independence tests, but no such procedures are available for non-linear systems. We describe a feasible procedure for learning a class of non-linear time series structures, which we call additive non-linear time series. We show that for data generated from stationary models of this type, two classes of conditional independence relations among time series variables and their lags can be tested efﬁciently and consistently using tests based on additive model regression. Combining results of statistical tests for these two classes of conditional independence relations and the temporal structure of time series data, a new consistent model speciﬁcation procedure is able to extract relatively detailed causal information. We investigate the ﬁnite sample behavior of the procedure through simulation, and illustrate the application of this method through analysis of the possible causal connections among four ocean indices. Several variants of the procedure are also discussed. Keywords: conditional independence test, contemporaneous causation, additive model regression, Granger causality, ocean indices</p><p>3 0.046913434 <a title="37-tfidf-3" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>Author: Balázs Csanád Csáji, László Monostori</p><p>Abstract: The paper investigates the possibility of applying value function based reinforcement learning (RL) methods in cases when the environment may change over time. First, theorems are presented which show that the optimal value function of a discounted Markov decision process (MDP) Lipschitz continuously depends on the immediate-cost function and the transition-probability function. Dependence on the discount factor is also analyzed and shown to be non-Lipschitz. Afterwards, the concept of (ε, δ)-MDPs is introduced, which is a generalization of MDPs and ε-MDPs. In this model the environment may change over time, more precisely, the transition function and the cost function may vary from time to time, but the changes must be bounded in the limit. Then, learning algorithms in changing environments are analyzed. A general relaxed convergence theorem for stochastic iterative algorithms is presented. We also demonstrate the results through three classical RL methods: asynchronous value iteration, Q-learning and temporal difference learning. Finally, some numerical experiments concerning changing environments are presented. Keywords: Markov decision processes, reinforcement learning, changing environments, (ε, δ)MDPs, value function bounds, stochastic iterative algorithms</p><p>4 0.039932631 <a title="37-tfidf-4" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>Author: Jun Zhu, Zaiqing Nie, Bo Zhang, Ji-Rong Wen</p><p>Abstract: Existing template-independent web data extraction approaches adopt highly ineffective decoupled strategies—attempting to do data record detection and attribute labeling in two separate phases. In this paper, we propose an integrated web data extraction paradigm with hierarchical models. The proposed model is called Dynamic Hierarchical Markov Random Fields (DHMRFs). DHMRFs take structural uncertainty into consideration and deﬁne a joint distribution of both model structure and class labels. The joint distribution is an exponential family distribution. As a conditional model, DHMRFs relax the independence assumption as made in directed models. Since exact inference is intractable, a variational method is developed to learn the model’s parameters and to ﬁnd the MAP model structure and label assignments. We apply DHMRFs to a real-world web data extraction task. Experimental results show that: (1) integrated web data extraction models can achieve signiﬁcant improvements on both record detection and attribute labeling compared to decoupled models; (2) in diverse web data extraction DHMRFs can potentially address the blocky artifact issue which is suffered by ﬁxed-structured hierarchical models. Keywords: conditional random ﬁelds, dynamic hierarchical Markov random ﬁelds, integrated web data extraction, statistical hierarchical modeling, blocky artifact issue</p><p>5 0.038608506 <a title="37-tfidf-5" href="./jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">30 jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>Author: Vojtěch Franc, Bogdan Savchynskyy</p><p>Abstract: The max-sum classiﬁer predicts n-tuple of labels from n-tuple of observable variables by maximizing a sum of quality functions deﬁned over neighbouring pairs of labels and observable variables. Predicting labels as MAP assignments of a Random Markov Field is a particular example of the max-sum classiﬁer. Learning parameters of the max-sum classiﬁer is a challenging problem because even computing the response of such classiﬁer is NP-complete in general. Estimating parameters using the Maximum Likelihood approach is feasible only for a subclass of max-sum classiﬁers with an acyclic structure of neighbouring pairs. Recently, the discriminative methods represented by the perceptron and the Support Vector Machines, originally designed for binary linear classiﬁers, have been extended for learning some subclasses of the max-sum classiﬁer. Besides the max-sum classiﬁers with the acyclic neighbouring structure, it has been shown that the discriminative learning is possible even with arbitrary neighbouring structure provided the quality functions fulﬁll some additional constraints. In this article, we extend the discriminative approach to other three classes of max-sum classiﬁers with an arbitrary neighbourhood structure. We derive learning algorithms for two subclasses of max-sum classiﬁers whose response can be computed in polynomial time: (i) the max-sum classiﬁers with supermodular quality functions and (ii) the max-sum classiﬁers whose response can be computed exactly by a linear programming relaxation. Moreover, we show that the learning problem can be approximately solved even for a general max-sum classiﬁer. Keywords: max-xum classiﬁer, hidden Markov networks, support vector machines</p><p>6 0.033500873 <a title="37-tfidf-6" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>7 0.031143 <a title="37-tfidf-7" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>8 0.030747173 <a title="37-tfidf-8" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>9 0.02938268 <a title="37-tfidf-9" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>10 0.024295978 <a title="37-tfidf-10" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>11 0.022442974 <a title="37-tfidf-11" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>12 0.020313954 <a title="37-tfidf-12" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>13 0.019331908 <a title="37-tfidf-13" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>14 0.018034508 <a title="37-tfidf-14" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>15 0.018009448 <a title="37-tfidf-15" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>16 0.017466363 <a title="37-tfidf-16" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>17 0.016962551 <a title="37-tfidf-17" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>18 0.016770128 <a title="37-tfidf-18" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>19 0.015960954 <a title="37-tfidf-19" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>20 0.015687544 <a title="37-tfidf-20" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.092), (1, -0.012), (2, -0.009), (3, -0.001), (4, -0.077), (5, -0.038), (6, -0.014), (7, 0.011), (8, -0.101), (9, -0.103), (10, -0.073), (11, -0.03), (12, -0.038), (13, -0.151), (14, -0.015), (15, -0.023), (16, -0.061), (17, -0.026), (18, 0.07), (19, -0.027), (20, -0.065), (21, -0.041), (22, 0.013), (23, -0.033), (24, 0.11), (25, 0.126), (26, 0.18), (27, -0.073), (28, -0.199), (29, 0.144), (30, 0.075), (31, 0.127), (32, 0.224), (33, 0.025), (34, -0.228), (35, 0.215), (36, -0.043), (37, 0.288), (38, 0.304), (39, 0.216), (40, -0.392), (41, -0.218), (42, -0.057), (43, 0.053), (44, 0.068), (45, -0.005), (46, 0.028), (47, -0.052), (48, 0.189), (49, -0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97737575 <a title="37-lsi-1" href="./jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations.html">37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</a></p>
<p>Author: Jia Li, Andrew W. Moore</p><p>Abstract: Web sites must forecast Web page views in order to plan computer resource allocation and estimate upcoming revenue and advertising growth. In this paper, we focus on extracting trends and seasonal patterns from page view series, two dominant factors in the variation of such series. We investigate the Holt-Winters procedure and a state space model for making relatively short-term prediction. It is found that Web page views exhibit strong impulsive changes occasionally. The impulses cause large prediction errors long after their occurrences. A method is developed to identify impulses and to alleviate their damage on prediction. We also develop a long-range trend and season extraction method, namely the Elastic Smooth Season Fitting (ESSF) algorithm, to compute scalable and smooth yearly seasons. ESSF derives the yearly season by minimizing the residual sum of squares under smoothness regularization, a quadratic optimization problem. It is shown that for longterm prediction, ESSF improves accuracy signiﬁcantly over other methods that ignore the yearly seasonality. Keywords: web page views, forecast, Holt-Winters, Kalman ﬁltering, elastic smooth season ﬁtting</p><p>2 0.33785769 <a title="37-lsi-2" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>Author: Tianjiao Chu, Clark Glymour</p><p>Abstract: Pointwise consistent, feasible procedures for estimating contemporaneous linear causal structure from time series data have been developed using multiple conditional independence tests, but no such procedures are available for non-linear systems. We describe a feasible procedure for learning a class of non-linear time series structures, which we call additive non-linear time series. We show that for data generated from stationary models of this type, two classes of conditional independence relations among time series variables and their lags can be tested efﬁciently and consistently using tests based on additive model regression. Combining results of statistical tests for these two classes of conditional independence relations and the temporal structure of time series data, a new consistent model speciﬁcation procedure is able to extract relatively detailed causal information. We investigate the ﬁnite sample behavior of the procedure through simulation, and illustrate the application of this method through analysis of the possible causal connections among four ocean indices. Several variants of the procedure are also discussed. Keywords: conditional independence test, contemporaneous causation, additive model regression, Granger causality, ocean indices</p><p>3 0.21748385 <a title="37-lsi-3" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>Author: Jun Zhu, Zaiqing Nie, Bo Zhang, Ji-Rong Wen</p><p>Abstract: Existing template-independent web data extraction approaches adopt highly ineffective decoupled strategies—attempting to do data record detection and attribute labeling in two separate phases. In this paper, we propose an integrated web data extraction paradigm with hierarchical models. The proposed model is called Dynamic Hierarchical Markov Random Fields (DHMRFs). DHMRFs take structural uncertainty into consideration and deﬁne a joint distribution of both model structure and class labels. The joint distribution is an exponential family distribution. As a conditional model, DHMRFs relax the independence assumption as made in directed models. Since exact inference is intractable, a variational method is developed to learn the model’s parameters and to ﬁnd the MAP model structure and label assignments. We apply DHMRFs to a real-world web data extraction task. Experimental results show that: (1) integrated web data extraction models can achieve signiﬁcant improvements on both record detection and attribute labeling compared to decoupled models; (2) in diverse web data extraction DHMRFs can potentially address the blocky artifact issue which is suffered by ﬁxed-structured hierarchical models. Keywords: conditional random ﬁelds, dynamic hierarchical Markov random ﬁelds, integrated web data extraction, statistical hierarchical modeling, blocky artifact issue</p><p>4 0.14765705 <a title="37-lsi-4" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>Author: Balázs Csanád Csáji, László Monostori</p><p>Abstract: The paper investigates the possibility of applying value function based reinforcement learning (RL) methods in cases when the environment may change over time. First, theorems are presented which show that the optimal value function of a discounted Markov decision process (MDP) Lipschitz continuously depends on the immediate-cost function and the transition-probability function. Dependence on the discount factor is also analyzed and shown to be non-Lipschitz. Afterwards, the concept of (ε, δ)-MDPs is introduced, which is a generalization of MDPs and ε-MDPs. In this model the environment may change over time, more precisely, the transition function and the cost function may vary from time to time, but the changes must be bounded in the limit. Then, learning algorithms in changing environments are analyzed. A general relaxed convergence theorem for stochastic iterative algorithms is presented. We also demonstrate the results through three classical RL methods: asynchronous value iteration, Q-learning and temporal difference learning. Finally, some numerical experiments concerning changing environments are presented. Keywords: Markov decision processes, reinforcement learning, changing environments, (ε, δ)MDPs, value function bounds, stochastic iterative algorithms</p><p>5 0.11918345 <a title="37-lsi-5" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>Author: Eric Bax, Augusto Callejas</p><p>Abstract: This paper introduces a new PAC transductive error bound for classiﬁcation. The method uses information from the training examples and inputs of working examples to develop a set of likely assignments to outputs of the working examples. A likely assignment with maximum error determines the bound. The method is very effective for small data sets. Keywords: error bound, transduction, nearest neighbor, dynamic programming</p><p>6 0.11494651 <a title="37-lsi-6" href="./jmlr-2008-Regularization_on_Graphs_with_Function-adapted_Diffusion_Processes.html">81 jmlr-2008-Regularization on Graphs with Function-adapted Diffusion Processes</a></p>
<p>7 0.11401736 <a title="37-lsi-7" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>8 0.11024244 <a title="37-lsi-8" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>9 0.10912891 <a title="37-lsi-9" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>10 0.10615689 <a title="37-lsi-10" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>11 0.099292941 <a title="37-lsi-11" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>12 0.090484165 <a title="37-lsi-12" href="./jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">30 jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>13 0.088084288 <a title="37-lsi-13" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>14 0.070900925 <a title="37-lsi-14" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>15 0.067047782 <a title="37-lsi-15" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>16 0.065500759 <a title="37-lsi-16" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>17 0.063393563 <a title="37-lsi-17" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>18 0.062122341 <a title="37-lsi-18" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>19 0.061454605 <a title="37-lsi-19" href="./jmlr-2008-Learning_Balls_of_Strings_from_Edit_Corrections.html">47 jmlr-2008-Learning Balls of Strings from Edit Corrections</a></p>
<p>20 0.057610877 <a title="37-lsi-20" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.022), (5, 0.019), (9, 0.523), (31, 0.023), (40, 0.033), (54, 0.045), (58, 0.037), (66, 0.033), (76, 0.014), (88, 0.036), (92, 0.033), (94, 0.042), (99, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89196348 <a title="37-lda-1" href="./jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations.html">37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</a></p>
<p>Author: Jia Li, Andrew W. Moore</p><p>Abstract: Web sites must forecast Web page views in order to plan computer resource allocation and estimate upcoming revenue and advertising growth. In this paper, we focus on extracting trends and seasonal patterns from page view series, two dominant factors in the variation of such series. We investigate the Holt-Winters procedure and a state space model for making relatively short-term prediction. It is found that Web page views exhibit strong impulsive changes occasionally. The impulses cause large prediction errors long after their occurrences. A method is developed to identify impulses and to alleviate their damage on prediction. We also develop a long-range trend and season extraction method, namely the Elastic Smooth Season Fitting (ESSF) algorithm, to compute scalable and smooth yearly seasons. ESSF derives the yearly season by minimizing the residual sum of squares under smoothness regularization, a quadratic optimization problem. It is shown that for longterm prediction, ESSF improves accuracy signiﬁcantly over other methods that ignore the yearly seasonality. Keywords: web page views, forecast, Holt-Winters, Kalman ﬁltering, elastic smooth season ﬁtting</p><p>2 0.73002261 <a title="37-lda-2" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><p>3 0.23160392 <a title="37-lda-3" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>Author: Tianjiao Chu, Clark Glymour</p><p>Abstract: Pointwise consistent, feasible procedures for estimating contemporaneous linear causal structure from time series data have been developed using multiple conditional independence tests, but no such procedures are available for non-linear systems. We describe a feasible procedure for learning a class of non-linear time series structures, which we call additive non-linear time series. We show that for data generated from stationary models of this type, two classes of conditional independence relations among time series variables and their lags can be tested efﬁciently and consistently using tests based on additive model regression. Combining results of statistical tests for these two classes of conditional independence relations and the temporal structure of time series data, a new consistent model speciﬁcation procedure is able to extract relatively detailed causal information. We investigate the ﬁnite sample behavior of the procedure through simulation, and illustrate the application of this method through analysis of the possible causal connections among four ocean indices. Several variants of the procedure are also discussed. Keywords: conditional independence test, contemporaneous causation, additive model regression, Granger causality, ocean indices</p><p>4 0.22417101 <a title="37-lda-4" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>5 0.20101099 <a title="37-lda-5" href="./jmlr-2008-Incremental_Identification_of_Qualitative_Models_of_Biological_Systems_using_Inductive_Logic_Programming.html">44 jmlr-2008-Incremental Identification of Qualitative Models of Biological Systems using Inductive Logic Programming</a></p>
<p>Author: Ashwin Srinivasan, Ross D. King</p><p>Abstract: The use of computational models is increasingly expected to play an important role in predicting the behaviour of biological systems. Models are being sought at different scales of biological organisation namely: sub-cellular, cellular, tissue, organ, organism and ecosystem; with a view of identifying how different components are connected together, how they are controlled and how they behave when functioning as a system. Except for very simple biological processes, system identiﬁcation from ﬁrst principles can be extremely difﬁcult. This has brought into focus automated techniques for constructing models using data of system behaviour. Such techniques face three principal issues: (1) The model representation language must be rich enough to capture system behaviour; (2) The system identiﬁcation technique must be powerful enough to identify substantially complex models; and (3) There may not be sufﬁcient data to obtain both the model’s structure and precise estimates of all of its parameters. In this paper, we address these issues in the following ways: (1) Models are represented in an expressive subset of ﬁrst-order logic. Speciﬁcally, they are expressed as logic programs; (2) System identiﬁcation is done using techniques developed in Inductive Logic Programming (ILP). This allows the identiﬁcation of ﬁrst-order logic models from data. Speciﬁcally, we employ an incremental approach in which increasingly complex models are constructed from simpler ones using snapshots of system behaviour; and (3) We restrict ourselves to “qualitative” models. These are non-parametric: thus, usually less data are required than for identifying parametric quantitative models. A further advantage is that the data need not be precise numerical observations (instead, they are abstractions like positive, negative, zero, increasing, decreasing and so on). We describe incremental construction of qualitative models using a simple physical system and demonstrate its application to identiﬁcatio</p><p>6 0.19503745 <a title="37-lda-6" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>7 0.1936301 <a title="37-lda-7" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>8 0.18615422 <a title="37-lda-8" href="./jmlr-2008-Minimal_Nonlinear_Distortion_Principle_for_Nonlinear_Independent_Component_Analysis.html">60 jmlr-2008-Minimal Nonlinear Distortion Principle for Nonlinear Independent Component Analysis</a></p>
<p>9 0.1852476 <a title="37-lda-9" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>10 0.18408287 <a title="37-lda-10" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>11 0.18183646 <a title="37-lda-11" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>12 0.18031143 <a title="37-lda-12" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>13 0.17934482 <a title="37-lda-13" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>14 0.17899604 <a title="37-lda-14" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>15 0.17725968 <a title="37-lda-15" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>16 0.17546763 <a title="37-lda-16" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>17 0.17512277 <a title="37-lda-17" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>18 0.17469731 <a title="37-lda-18" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>19 0.17448875 <a title="37-lda-19" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>20 0.17227301 <a title="37-lda-20" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
