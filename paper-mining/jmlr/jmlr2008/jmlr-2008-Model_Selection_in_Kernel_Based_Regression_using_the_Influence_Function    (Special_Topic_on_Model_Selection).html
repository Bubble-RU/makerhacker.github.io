<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-64" href="#">jmlr2008-64</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2008-64-pdf" href="http://jmlr.org/papers/volume9/debruyne08a/debruyne08a.pdf">pdf</a></p><p>Author: Michiel Debruyne, Mia Hubert, Johan A.K. Suykens</p><p>Abstract: Recent results about the robustness of kernel methods involve the analysis of inﬂuence functions. By deﬁnition the inﬂuence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the inﬂuence function is used in a similar way to analyze the statistical efﬁciency of a method. Links between both worlds are explored. The inﬂuence function is related to the ﬁrst term of a Taylor expansion. Higher order inﬂuence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate inﬂuence functions at a speciﬁc sample distribution to obtain an approximation of the leave-one-out error. A speciﬁc implementation is proposed using a L1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data. Keywords: kernel based regression, robustness, stability, inﬂuence function, model selection</p><p>Reference: <a title="jmlr-2008-64-reference" href="../jmlr2008_reference/jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Leuven Kasteelpark Arenberg 10, B-3001 Leuven, Belgium  Editor: Isabelle Guyon  Abstract Recent results about the robustness of kernel methods involve the analysis of inﬂuence functions. [sent-16, score-0.108]
</p><p>2 A speciﬁc implementation is proposed using a L1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. [sent-25, score-0.214]
</p><p>3 The parameter in the Huber loss controlling the degree of robustness is optimized as well. [sent-26, score-0.156]
</p><p>4 In the ﬁeld of robust statistics the inﬂuence function was introduced in order to analyze the effects of outliers on an estimator. [sent-34, score-0.107]
</p><p>5 A speciﬁc implementation is proposed to obtain robustness with a Huber loss function in the estimation step and a L 1 loss in the model selection step. [sent-46, score-0.248]
</p><p>6 The degree of robustness is controlled by a parameter that can be chosen in a data driven way as well. [sent-47, score-0.091]
</p><p>7 ε The inﬂuence function measures the effect on the estimator T when adding an inﬁnitesimally small amount of contamination at the point z. [sent-65, score-0.087]
</p><p>8 Then small amounts of contamination cannot completely change the estimate and a certain degree of robustness is indeed present. [sent-70, score-0.1]
</p><p>9 If the median of the underlying distribution is uniquely deﬁned, that is if f (0) > 0, then the inﬂuence function of the median equals sign(z)/(2 f (0)) which is bounded. [sent-73, score-0.184]
</p><p>10 Since the asymptotic efﬁciency of an estimator is proportional to the reciprocal of the asymptotic variance, the integrated squared inﬂuence function should be as small as possible to achieve high efﬁciency. [sent-104, score-0.097]
</p><p>11 At R a standard normal distribution the asymptotic variance of the mean equals z2 dP(z) = 1, and that R of the median equals (sign(z)/(2 f (0)))2 dP(z) = 1. [sent-106, score-0.189]
</p><p>12 However, at a Cauchy distribution for instance, this is completely different: the ASV of the median equals 2. [sent-109, score-0.124]
</p><p>13 The leave-one-out error often plays a vital role, for example in hypothesis stability (Bousquet and Elisseeff, 2001), partial stability (Kutin 2379  D EBRUYNE , H UBERT AND S UYKENS  and Niyogi, 2002) and CVloo -stability (Poggio et al. [sent-114, score-0.142]
</p><p>14 Let Pn denote the empirical distribution of a sample without the ith observation zi = (xi , yi ) ∈ X × Y . [sent-118, score-0.129]
</p><p>15 (2004) call the map T CVloo -stable for a loss function L : Y → R+ if lim  −i sup |L(yi − T (Pn )(xi )) − L(yi − T (Pn )(xi ))| → 0  n→∞ i∈{1,. [sent-120, score-0.092]
</p><p>16 Let L be the absolute value loss and consider once again the simple case of estimating the location of a univariate distribution. [sent-127, score-0.141]
</p><p>17 2 2 2  If the median of the underlying distribution P is unique, then both y ( n+1 ) and y( n+3 ) converge to this 2 2 number and CVloo stability is obtained. [sent-139, score-0.153]
</p><p>18 Then one has CV loo stability and a ﬁnite asymptotic variance. [sent-156, score-0.178]
</p><p>19 Using the sample mean is ok for a normal distribution, but not for a Cauchy distribution (no CVloo stability and an inﬁnite asymptotic variance). [sent-157, score-0.138]
</p><p>20 2381  D EBRUYNE , H UBERT AND S UYKENS  Frequently used kernels include the linear kernel K(xi , x j ) = xti x j , polynomial kernel of degree p for which K(xi , x j ) = (τ + xti x j ) p with τ > 0 and RBF kernel K(xi , x j ) = exp(− xi − x j 2 /σ2 ) with 2 bandwidth σ > 0. [sent-187, score-0.203]
</p><p>21 Deﬁnition 4 Let K be a kernel function with corresponding feature space H and let L : R → R + be a twice differentiable convex loss function. [sent-189, score-0.136]
</p><p>22 ) primal-dual optimization methodology involving least squares kernel estimators were studied by Suykens et al. [sent-196, score-0.217]
</p><p>23 Possible loss functions include • the least squares loss: L(r) = r 2 . [sent-198, score-0.265]
</p><p>24 • Vapnik’s ε-insensitive loss: L(r) = max{|r| − ε, 0}, with special case the L 1 loss if ε = 0. [sent-199, score-0.092]
</p><p>25 Note that this is not the same loss function as used in logistic regression. [sent-201, score-0.092]
</p><p>26 • Huber loss with parameter b > 0: L(r) = r 2 if |r| ≤ b and L(r) = 2b|r| − b2 if |r| > b. [sent-202, score-0.092]
</p><p>27 Note that the least squares loss corresponds to the limit case b → ∞. [sent-203, score-0.265]
</p><p>28 Then the inﬂuence function of f λ,K exists for all z := (zx , zy ) ∈ X × Y and we have IF(z; fλ,K , P) = −S−1 2λ fλ,K,P + L (zy − fλ,K,P (zx ))S−1 Φ(zx ) where S : H → H is deﬁned by S( f ) = 2λ f + EP L (Y − fλ,K,P (X)) Φ(X), f Φ(X) . [sent-208, score-0.234]
</p><p>29 Thus if the kernel is bounded and the ﬁrst derivative of the loss function is bounded, then the inﬂuence function is bounded as well. [sent-209, score-0.159]
</p><p>30 Thus L1 type loss functions for instance lead to robust estimators. [sent-210, score-0.121]
</p><p>31 The logistic loss as well since the derivative of this loss function equals L (r) = 2 − 2382  M ODEL S ELECTION IN K ERNEL BASED R EGRESSIONUSING THE I NFLUENCE F UNCTION  1/(1 + e−r ) which is bounded by 2. [sent-211, score-0.249]
</p><p>32 For a least squares loss function on the other hand, the inﬂuence function is unbounded (L (r) = 2r): the effect of the smallest amount of contamination can be arbitrary large. [sent-215, score-0.323]
</p><p>33 Therefore it is said that the least squares estimator is not robust. [sent-216, score-0.224]
</p><p>34 Let L be a convex loss function that is three times differentiable. [sent-220, score-0.092]
</p><p>35 When the loss function is inﬁnitely differentiable, all higher order terms can in theory be calculated, but the number of terms grows rapidly since all derivatives of L come into play. [sent-222, score-0.092]
</p><p>36 Let L be a convex loss function such that the third derivative is 0. [sent-225, score-0.115]
</p><p>37 Finite Sample Expressions Since the Taylor expansion in (1) is now fully characterized for any distribution P and any z, we can use this to assess the inﬂuence of individual points in a sample with sample distribution Pn . [sent-228, score-0.088]
</p><p>38 1 Least Squares Loss First consider taking the least squares loss in (4). [sent-233, score-0.265]
</p><p>39 fλ,K,Pn (xn ) K(xi , xn ) IF(zi ; fλ,K , Pn )(xn )   (8)  In order to evaluate the inﬂuence function at sample point z i at a sample distribution Pn , we only −1 need the full sample ﬁt f λ,K,Pn and the matrix Sn , which is already obtained when computing f λ,K,Pn (cf. [sent-280, score-0.138]
</p><p>40 2 Huber Loss For the Huber loss function with parameter b > 0 we have that r2 2b|r| − b2  L(r) =  if |r| < b. [sent-310, score-0.092]
</p><p>41 For a general loss function L one has to solve Equation (13) to ﬁnd f λ,K,Pn . [sent-335, score-0.092]
</p><p>42 A fast way to do so is to use reweighted KBR with a least squares loss. [sent-336, score-0.22]
</p><p>43 yn  (16)  and thus fλ,K,Pn can be written as a reweighted least squares estimator with additional weights w i compared to Equations (6) and (7). [sent-348, score-0.271]
</p><p>44 (2006) the robustness of such stepwise reweighting algorithm is analyzed by calculating stepwise inﬂuence functions. [sent-362, score-0.154]
</p><p>45 For the Huber loss with parameter b Equation (15) means that the corresponding weight function equals W (r) = 1 if |r| ≤ b and W (r) = b/|r| if |r| > b. [sent-364, score-0.134]
</p><p>46 This gives a clear interpretation of this loss function: all observations with error smaller than b remain unchanged, but the ones with error larger than b are downweighted compared to the least squares loss. [sent-365, score-0.298]
</p><p>47 In this paper however we use the reweighting only to compute the full sample estimator fλ,K,Pn and we assume that it is fully converged to the solution of (13). [sent-370, score-0.117]
</p><p>48 1 Deﬁnition The traditional leave-one-out criterion is given by LOO(λ, K) =  1 n ∑ V (yi − fλ,K,Pn−i (xi )) n i=1  (17)  with V an appropriate loss function. [sent-376, score-0.118]
</p><p>49 The idea we investigate is to replace the explicit leave-one-out by the approximation in (12) for least squares and (14) for the Huber loss. [sent-378, score-0.173]
</p><p>50 Deﬁnition 8 The k-th order inﬂuence function criterion at a regularization parameter λ > 0 and kernel K for Huber loss KBR with parameter b is deﬁned as k CIF (λ, K, b) =  1 n ∑V n i=1  k−1  [IFMk ]i,i 1 1 [IFM j ]i,i − j j! [sent-379, score-0.162]
</p><p>51 For KBR with a least squares loss we write k CIF (λ, K, ∞) =  1 n ∑V n i=1  k−1  [IFMk ]i,i 1 1 [IFM j ]i,i − j j! [sent-382, score-0.265]
</p><p>52 since a least squares loss is a limit case of the Huber loss as b → ∞. [sent-385, score-0.357]
</p><p>53 For V one typically chooses the squared loss or the absolute value corresponding to the mean squared error and the mean absolute error. [sent-389, score-0.148]
</p><p>54 Note that V does not need to be the same as the loss function used to compute f λ,K,Pn (the latter is always denoted by L). [sent-390, score-0.092]
</p><p>55 Recall that a loss function L with bounded ﬁrst derivative L is needed to perform robust ﬁtting. [sent-391, score-0.144]
</p><p>56 However, if these parameters are selected in a data driven way, outliers in the data might have a large effect on the selection of the parameters. [sent-393, score-0.105]
</p><p>57 It is thus important to use a robust loss function V as well. [sent-395, score-0.121]
</p><p>58 Therefore we set V equal to the absolute value loss function unless we explicitly state differently. [sent-396, score-0.12]
</p><p>59 1 an illustration is given on what can go wrong if a least squares loss is chosen for V instead of the absolute value. [sent-398, score-0.293]
</p><p>60 2 Optimizing b 5 With k and V now speciﬁed, the criterion CIF can be used to select optimal hyperparameters for a KBR estimator with L the Huber loss with parameter b. [sent-400, score-0.199]
</p><p>61 3 it was argued that b controls the robustness of the estimator since all observations with error smaller than b are downweighted compared to the least squares estimator. [sent-403, score-0.321]
</p><p>62 Thus we want to choose b small enough such that outlying observations receive sufﬁciently small weight, but also large enough such that the good non outlying observations are not downweighted too much. [sent-404, score-0.099]
</p><p>63 Compute the residuals ri with respect to the least squares ﬁt with these optimal λ and K. [sent-412, score-0.204]
</p><p>64 We also want to compare to the least squares ﬁt and thus set ˆ ˆ ˆ B = {σerr , 2σerr , 3σerr , ∞}. [sent-425, score-0.173]
</p><p>65 5  σ=1  0  2  3  4  5  6  7  8  9  10  −2  11  2  3  4  5  6  X  (a)  7  8  9  x  (b)  Figure 1: (a) Data and least squares ﬁt. [sent-450, score-0.173]
</p><p>66 For KBR with the least squares loss condition (22) is indeed satisﬁed (cf. [sent-453, score-0.265]
</p><p>67 For KBR with a general loss function one does not have a linear equation of the form of (22), and thus it is more difﬁcult to apply this approximation. [sent-462, score-0.092]
</p><p>68 1 Toy Example As a toy example 50 data points were generated with xi uniformly distributed on the interval [2, 11] and yi = sin(xi ) + ei with ei Gaussian distributed noise with standard deviation 0. [sent-467, score-0.109]
</p><p>69 We start with kernel based regression with a least squares loss and a Gaussian kernel. [sent-469, score-0.309]
</p><p>70 This reﬂects the non-robustness of the least squares estimator: if we would continue raising the point z, then IF(z; f λ,K ) would become larger and larger, since it is an unbounded function of z. [sent-538, score-0.195]
</p><p>71 This reﬂects the loss in stability when small bandwidths are chosen: then the ﬁt is more sensitive to small changes in the data and thus less stable. [sent-541, score-0.163]
</p><p>72 The least squares loss has an unbounded ﬁrst derivative and thus the inﬂuence of outliers can be arbitrary large. [sent-585, score-0.388]
</p><p>73 The Huber loss has a bounded ﬁrst derivative and thus the inﬂuence of outliers is bounded as 2392  M ODEL S ELECTION IN K ERNEL BASED R EGRESSIONUSING THE I NFLUENCE F UNCTION  V Least Squares 1. [sent-586, score-0.193]
</p><p>74 Upper: using least squares loss V in the model selection. [sent-607, score-0.265]
</p><p>75 For the estimation the loss function L is always the Huber loss with b = 0. [sent-609, score-0.184]
</p><p>76 In the upper panel the least squares loss is used for V in the model selection criteria. [sent-622, score-0.265]
</p><p>77 In the lower panel of Figure 4 the L1 loss is used for V in the model selection 5 criteria. [sent-626, score-0.092]
</p><p>78 We clearly see that, although in both cases a robust estimation procedure is used (Huber loss for L), the outlier can still be quite inﬂuential through the model selection. [sent-630, score-0.185]
</p><p>79 Finally let us investigate the role of the parameter b used in the Huber loss function. [sent-632, score-0.092]
</p><p>80 This is quite expected: since there are no outliers, there is no reason why least squares (b = ∞) would not perform well. [sent-635, score-0.173]
</p><p>81 The 5 values of CIF are much higher for least squares than for the Huber loss with smaller b. [sent-640, score-0.265]
</p><p>82 Thus it is automatically detected that a least squares loss is not the appropriate choice, which is a correct assessment since the outlier will have a large effect (cf. [sent-641, score-0.329]
</p><p>83 First we consider the least squares loss for L with 5 the criterion CIF (λ, σ, ∞) (Deﬁnition 8), with exact leave-one-out (17) and with CV (22). [sent-709, score-0.291]
</p><p>84 Secondly, we considered each time the residuals of the least squares ﬁt with optimal λ and 5 ˆ σ according to CIF (λ, K, ∞). [sent-713, score-0.204]
</p><p>85 Then we applied KBR with a Huber loss and parameter b = 3 σerr . [sent-715, score-0.092]
</p><p>86 5 ˆ The resulting MSE with this loss and λ and σ minimizing CIF (λ, σ, 3σerr ) is given in column 4 in ˆ ˆ Table 1. [sent-716, score-0.092]
</p><p>87 For the data sets without contamination we see that using a Huber loss instead of least squares gives similar results except for the Boston housing data, Friedman 1 and especially Friedman 2. [sent-718, score-0.345]
</p><p>88 This might be explained by the relationship between the loss function and the error distribution. [sent-720, score-0.092]
</p><p>89 For a Gaussian error distribution least squares is often an optimal choice (cf. [sent-721, score-0.195]
</p><p>90 Since the errors in the Friedman data are explicitly generated as Gaussian, this might explain why least squares outperforms the Huber loss. [sent-723, score-0.173]
</p><p>91 For real data sets, the errors might not be exactly Gaussian, and thus other loss function can perform at least equally well as least squares. [sent-724, score-0.136]
</p><p>92 Now least squares is not a good option because of its lack of robustness. [sent-726, score-0.173]
</p><p>93 This is not the case when the Huber ˆ loss function is chosen. [sent-728, score-0.092]
</p><p>94 For the Friedman 1 and Friedman 2 data sets for instance this procedure indeed detects that least squares is an appropriate loss function and automatically avoids choosing b too small. [sent-736, score-0.265]
</p><p>95 For the contaminated data sets the procedure detects that least squares is not appropriate and that changing to a Huber loss with a small b is beneﬁcial, which is indeed a correct choice yielding smaller MSE’s. [sent-737, score-0.307]
</p><p>96 Friedman 1 (F1), Friedman 2 (F2), Friedman 3 (F3), Boston Housing (B), Ozone (O), Servo (S), Boston Housing with outliers (B+o), Ozone with outliers (O+o) and Servo with outliers (S+o). [sent-872, score-0.234]
</p><p>97 A speciﬁc procedure is suggested using an L1 loss in the model selection criterion and a Huber loss in the estimation. [sent-880, score-0.21]
</p><p>98 Due to an iterative reweighting algorithm to compute such a Huber loss estimator and due to the fast approximation of the leave-one-out error, everything can be computed fast starting from the least squares framework. [sent-881, score-0.36]
</p><p>99 With an a priori choice of the parameter b in the Huber loss this leads to better robustness if b is chosen small enough. [sent-882, score-0.156]
</p><p>100 Weighted least squares support vector machines : Robustness and sparse approximation. [sent-1010, score-0.173]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zx', 0.449), ('cif', 0.43), ('huber', 0.281), ('pn', 0.263), ('zy', 0.234), ('uence', 0.159), ('squares', 0.151), ('ifmk', 0.143), ('ep', 0.136), ('ebruyne', 0.132), ('ifk', 0.132), ('kbr', 0.132), ('ubert', 0.132), ('uykens', 0.132), ('err', 0.131), ('egressionusing', 0.121), ('nfluence', 0.121), ('loss', 0.092), ('unction', 0.084), ('loo', 0.084), ('ernel', 0.079), ('outliers', 0.078), ('cv', 0.077), ('stability', 0.071), ('ifm', 0.066), ('suykens', 0.066), ('christmann', 0.065), ('cvloo', 0.065), ('outlier', 0.064), ('robustness', 0.064), ('election', 0.063), ('median', 0.06), ('odel', 0.06), ('debruyne', 0.055), ('friedman', 0.055), ('xi', 0.051), ('estimator', 0.051), ('xn', 0.05), ('zi', 0.047), ('reweighted', 0.047), ('fwo', 0.044), ('housing', 0.044), ('ozone', 0.044), ('reweighting', 0.044), ('servo', 0.044), ('kernel', 0.044), ('equals', 0.042), ('contaminated', 0.042), ('yi', 0.038), ('leuven', 0.037), ('hubert', 0.037), ('contamination', 0.036), ('cauchy', 0.034), ('downweighted', 0.033), ('mad', 0.033), ('mia', 0.033), ('michiel', 0.033), ('outlying', 0.033), ('spn', 0.033), ('sn', 0.033), ('residuals', 0.031), ('hyperparameters', 0.03), ('boston', 0.03), ('robust', 0.029), ('kuleuven', 0.028), ('absolute', 0.028), ('driven', 0.027), ('criterion', 0.026), ('remainder', 0.026), ('poggio', 0.025), ('mse', 0.025), ('johan', 0.025), ('sb', 0.025), ('tikhonov', 0.025), ('dashed', 0.025), ('xk', 0.023), ('asymptotic', 0.023), ('derivative', 0.023), ('stepwise', 0.023), ('steinwart', 0.023), ('wahba', 0.023), ('taylor', 0.022), ('boos', 0.022), ('brabanter', 0.022), ('depository', 0.022), ('devito', 0.022), ('entrywise', 0.022), ('italic', 0.022), ('kep', 0.022), ('med', 0.022), ('nitesimally', 0.022), ('least', 0.022), ('unbounded', 0.022), ('distribution', 0.022), ('sample', 0.022), ('solid', 0.021), ('proposition', 0.021), ('univariate', 0.021), ('toy', 0.02), ('bandwidth', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="64-tfidf-1" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>Author: Michiel Debruyne, Mia Hubert, Johan A.K. Suykens</p><p>Abstract: Recent results about the robustness of kernel methods involve the analysis of inﬂuence functions. By deﬁnition the inﬂuence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the inﬂuence function is used in a similar way to analyze the statistical efﬁciency of a method. Links between both worlds are explored. The inﬂuence function is related to the ﬁrst term of a Taylor expansion. Higher order inﬂuence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate inﬂuence functions at a speciﬁc sample distribution to obtain an approximation of the leave-one-out error. A speciﬁc implementation is proposed using a L1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data. Keywords: kernel based regression, robustness, stability, inﬂuence function, model selection</p><p>2 0.112396 <a title="64-tfidf-2" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>3 0.095588945 <a title="64-tfidf-3" href="./jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>Author: Andreas Christmann, Arnout Van Messem</p><p>Abstract: We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand inﬂuence function (BIF) a modiﬁcation of F.R. Hampel’s inﬂuence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel’s inﬂuence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. Keywords: Bouligand derivatives, empirical risk minimization, inﬂuence function, robustness, support vector machines</p><p>4 0.061934453 <a title="64-tfidf-4" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>Author: Mikio L. Braun, Joachim M. Buhmann, Klaus-Robert Müller</p><p>Abstract: We show that the relevant information of a supervised learning problem is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem in the sense that it can asymptotically represent the function to be learned and is sufﬁciently smooth. Thus, kernels do not only transform data sets such that good generalization can be achieved using only linear discriminant functions, but this transformation is also performed in a manner which makes economical use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data for supervised learning problems. Practically, we propose an algorithm which enables us to recover the number of leading kernel PCA components relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to aid in model selection, and (3) to denoise in feature space in order to yield better classiﬁcation results. Keywords: kernel methods, feature space, dimension reduction, effective dimensionality</p><p>5 0.052936401 <a title="64-tfidf-5" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>6 0.046862934 <a title="64-tfidf-6" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>7 0.044630393 <a title="64-tfidf-7" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>8 0.040602885 <a title="64-tfidf-8" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>9 0.04044129 <a title="64-tfidf-9" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>10 0.03957675 <a title="64-tfidf-10" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>11 0.039181329 <a title="64-tfidf-11" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>12 0.037806313 <a title="64-tfidf-12" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>13 0.037583463 <a title="64-tfidf-13" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>14 0.03535096 <a title="64-tfidf-14" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>15 0.034331772 <a title="64-tfidf-15" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>16 0.032222353 <a title="64-tfidf-16" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>17 0.030864887 <a title="64-tfidf-17" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>18 0.029416762 <a title="64-tfidf-18" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>19 0.028807295 <a title="64-tfidf-19" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>20 0.028159549 <a title="64-tfidf-20" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.16), (1, -0.073), (2, -0.059), (3, -0.012), (4, 0.112), (5, 0.028), (6, -0.021), (7, -0.093), (8, 0.042), (9, 0.158), (10, -0.115), (11, -0.093), (12, -0.11), (13, -0.102), (14, 0.043), (15, -0.015), (16, 0.059), (17, -0.037), (18, 0.05), (19, 0.07), (20, -0.177), (21, -0.054), (22, -0.037), (23, 0.208), (24, -0.002), (25, 0.051), (26, 0.09), (27, -0.172), (28, -0.025), (29, 0.045), (30, -0.148), (31, 0.007), (32, -0.278), (33, 0.125), (34, -0.018), (35, -0.073), (36, -0.279), (37, 0.212), (38, -0.033), (39, -0.113), (40, -0.051), (41, 0.04), (42, -0.108), (43, 0.083), (44, -0.323), (45, -0.076), (46, -0.282), (47, 0.111), (48, -0.003), (49, -0.08)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94730115 <a title="64-lsi-1" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>Author: Michiel Debruyne, Mia Hubert, Johan A.K. Suykens</p><p>Abstract: Recent results about the robustness of kernel methods involve the analysis of inﬂuence functions. By deﬁnition the inﬂuence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the inﬂuence function is used in a similar way to analyze the statistical efﬁciency of a method. Links between both worlds are explored. The inﬂuence function is related to the ﬁrst term of a Taylor expansion. Higher order inﬂuence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate inﬂuence functions at a speciﬁc sample distribution to obtain an approximation of the leave-one-out error. A speciﬁc implementation is proposed using a L1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data. Keywords: kernel based regression, robustness, stability, inﬂuence function, model selection</p><p>2 0.40681261 <a title="64-lsi-2" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>3 0.37374818 <a title="64-lsi-3" href="./jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>Author: Andreas Christmann, Arnout Van Messem</p><p>Abstract: We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand inﬂuence function (BIF) a modiﬁcation of F.R. Hampel’s inﬂuence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel’s inﬂuence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. Keywords: Bouligand derivatives, empirical risk minimization, inﬂuence function, robustness, support vector machines</p><p>4 0.236063 <a title="64-lsi-4" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>5 0.19643772 <a title="64-lsi-5" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>Author: Sivan Sabato, Shai Shalev-Shwartz</p><p>Abstract: Feature ranking is a fundamental machine learning task with various applications, including feature selection and decision tree learning. We describe and analyze a new feature ranking method that supports categorical features with a large number of possible values. We show that existing ranking criteria rank a feature according to the training error of a predictor based on the feature. This approach can fail when ranking categorical features with many values. We propose the Ginger ranking criterion, that estimates the generalization error of the predictor associated with the Gini index. We show that for almost all training sets, the Ginger criterion produces an accurate estimation of the true generalization error, regardless of the number of values in a categorical feature. We also address the question of ﬁnding the optimal predictor that is based on a single categorical feature. It is shown that the predictor associated with the misclassiﬁcation error criterion has the minimal expected generalization error. We bound the bias of this predictor with respect to the generalization error of the Bayes optimal predictor, and analyze its concentration properties. We demonstrate the efﬁciency of our approach for feature selection and for learning decision trees in a series of experiments with synthetic and natural data sets. Keywords: feature ranking, categorical features, generalization bounds, Gini index, decision trees</p><p>6 0.18383063 <a title="64-lsi-6" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>7 0.17884049 <a title="64-lsi-7" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>8 0.17825964 <a title="64-lsi-8" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>9 0.17783351 <a title="64-lsi-9" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>10 0.16928893 <a title="64-lsi-10" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>11 0.15773298 <a title="64-lsi-11" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>12 0.15362082 <a title="64-lsi-12" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>13 0.15001355 <a title="64-lsi-13" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>14 0.14735627 <a title="64-lsi-14" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>15 0.1391701 <a title="64-lsi-15" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>16 0.13742393 <a title="64-lsi-16" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>17 0.13593426 <a title="64-lsi-17" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>18 0.13547374 <a title="64-lsi-18" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>19 0.12098315 <a title="64-lsi-19" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>20 0.11907987 <a title="64-lsi-20" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.032), (5, 0.011), (40, 0.044), (54, 0.027), (58, 0.048), (60, 0.489), (66, 0.075), (76, 0.02), (78, 0.014), (88, 0.04), (92, 0.031), (94, 0.045), (99, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.79068398 <a title="64-lda-1" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>Author: Manfred K. Warmuth, Dima Kuzmin</p><p>Abstract: We design an online algorithm for Principal Component Analysis. In each trial the current instance is centered and projected into a probabilistically chosen low dimensional subspace. The regret of our online algorithm, that is, the total expected quadratic compression loss of the online algorithm minus the total quadratic compression loss of the batch algorithm, is bounded by a term whose dependence on the dimension of the instances is only logarithmic. We ﬁrst develop our methodology in the expert setting of online learning by giving an algorithm for learning as well as the best subset of experts of a certain size. This algorithm is then lifted to the matrix setting where the subsets of experts correspond to subspaces. The algorithm represents the uncertainty over the best subspace as a density matrix whose eigenvalues are bounded. The running time is O(n2 ) per trial, where n is the dimension of the instances. Keywords: principal component analysis, online learning, density matrix, expert setting, quantum Bayes rule</p><p>same-paper 2 0.77951723 <a title="64-lda-2" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>Author: Michiel Debruyne, Mia Hubert, Johan A.K. Suykens</p><p>Abstract: Recent results about the robustness of kernel methods involve the analysis of inﬂuence functions. By deﬁnition the inﬂuence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the inﬂuence function is used in a similar way to analyze the statistical efﬁciency of a method. Links between both worlds are explored. The inﬂuence function is related to the ﬁrst term of a Taylor expansion. Higher order inﬂuence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate inﬂuence functions at a speciﬁc sample distribution to obtain an approximation of the leave-one-out error. A speciﬁc implementation is proposed using a L1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data. Keywords: kernel based regression, robustness, stability, inﬂuence function, model selection</p><p>3 0.29311579 <a title="64-lda-3" href="./jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>Author: Andreas Christmann, Arnout Van Messem</p><p>Abstract: We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand inﬂuence function (BIF) a modiﬁcation of F.R. Hampel’s inﬂuence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel’s inﬂuence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. Keywords: Bouligand derivatives, empirical risk minimization, inﬂuence function, robustness, support vector machines</p><p>4 0.28398672 <a title="64-lda-4" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>Author: Yonatan Amit, Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We describe and analyze an algorithmic framework for online classiﬁcation where each online trial consists of multiple prediction tasks that are tied together. We tackle the problem of updating the online predictor by deﬁning a projection problem in which each prediction task corresponds to a single linear constraint. These constraints are tied together through a single slack parameter. We then introduce a general method for approximately solving the problem by projecting simultaneously and independently on each constraint which corresponds to a prediction sub-problem, and then averaging the individual solutions. We show that this approach constitutes a feasible, albeit not necessarily optimal, solution of the original projection problem. We derive concrete simultaneous projection schemes and analyze them in the mistake bound model. We demonstrate the power of the proposed algorithm in experiments with synthetic data and with multiclass text categorization tasks. Keywords: online learning, parallel computation, mistake bounds, structured prediction</p><p>5 0.2557219 <a title="64-lda-5" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>Author: Elena Marchiori</p><p>Abstract: In supervised learning, a training set consisting of labeled instances is used by a learning algorithm for generating a model (classiﬁer) that is subsequently employed for deciding the class label of new instances (for generalization). Characteristics of the training set, such as presence of noisy instances and size, inﬂuence the learning algorithm and affect generalization performance. This paper introduces a new network-based representation of a training set, called hit miss network (HMN), which provides a compact description of the nearest neighbor relation over pairs of instances from each pair of classes. We show that structural properties of HMN’s correspond to properties of training points related to the one nearest neighbor (1-NN) decision rule, such as being border or central point. This motivates us to use HMN’s for improving the performance of a 1-NN classiﬁer by removing instances from the training set (instance selection). We introduce three new HMN-based algorithms for instance selection. HMN-C, which removes instances without affecting accuracy of 1-NN on the original training set, HMN-E, based on a more aggressive storage reduction, and HMN-EI, which applies iteratively HMN-E. Their performance is assessed on 22 data sets with different characteristics, such as input dimension, cardinality, class balance, number of classes, noise content, and presence of redundant variables. Results of experiments on these data sets show that accuracy of 1-NN classiﬁer increases signiﬁcantly when HMN-EI is applied. Comparison with state-of-the-art editing algorithms for instance selection on these data sets indicates best generalization performance of HMN-EI and no signiﬁcant difference in storage requirements. In general, these results indicate that HMN’s provide a powerful graph-based representation of a training set, which can be successfully applied for performing noise and redundance reduction in instance-based learning. Keywords: graph-based training set repre</p><p>6 0.25208268 <a title="64-lda-6" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>7 0.25117916 <a title="64-lda-7" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>8 0.23890877 <a title="64-lda-8" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>9 0.2358851 <a title="64-lda-9" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>10 0.23323089 <a title="64-lda-10" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>11 0.23030497 <a title="64-lda-11" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>12 0.22976051 <a title="64-lda-12" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>13 0.22768506 <a title="64-lda-13" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>14 0.22694474 <a title="64-lda-14" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>15 0.22573288 <a title="64-lda-15" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>16 0.2255929 <a title="64-lda-16" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>17 0.22503376 <a title="64-lda-17" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>18 0.22345608 <a title="64-lda-18" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>19 0.22262064 <a title="64-lda-19" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>20 0.22154069 <a title="64-lda-20" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
