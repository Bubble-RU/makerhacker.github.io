<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 jmlr-2008-Learning from Multiple Sources</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-52" href="#">jmlr2008-52</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>52 jmlr-2008-Learning from Multiple Sources</h1>
<br/><p>Source: <a title="jmlr-2008-52-pdf" href="http://jmlr.org/papers/volume9/crammer08a/crammer08a.pdf">pdf</a></p><p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields general results for classiﬁcation and regression. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. We discuss the related problem of learning parameters of a distribution from multiple data sources. Finally, we illustrate our theory through a series of synthetic simulations. Keywords: error bounds, multi-task learning</p><p>Reference: <a title="jmlr-2008-52-reference" href="../jmlr2008_reference/jmlr-2008-Learning_from_Multiple_Sources_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Department of Computer and Information Science University of Pennsylvania Philadelphia, PA 19104, USA  Editor: Peter Bartlett  Abstract We consider the problem of learning accurate models from multiple sources of “nearby” data. [sent-7, score-0.456]
</p><p>2 Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. [sent-8, score-0.588]
</p><p>3 Introduction We introduce and analyze a theoretical model for the problem of learning from multiple sources of “nearby” data. [sent-14, score-0.426]
</p><p>4 In our model there are K unknown data sources, with source i generating a distinct sample Si of ni observations. [sent-20, score-0.427]
</p><p>5 We assume we are given only the samples S i , and a disparity1 matrix D whose entry D(i, j) bounds the difference between source i and source j. [sent-21, score-0.414]
</p><p>6 Our framework includes settings in which the sources produce data for classiﬁcation, regression, and density estimation (and more generally any additive-loss learning problem obeying certain conditions). [sent-29, score-0.386]
</p><p>7 Our main result is a general theorem establishing a bound on the expected loss incurred by using all data sources within a given disparity of the target source. [sent-30, score-0.974]
</p><p>8 Our bound clearly expresses a trade-off between three quantities: the sample size used (which increases as we include data from more distant models), a weighted average of the disparities of the sources whose data is used, and a model complexity term. [sent-32, score-0.678]
</p><p>9 It can be applied to any learning setting in which the underlying loss function obeys an approximate triangle inequality, and in which the class of hypothesis models under consideration obeys uniform convergence of empirical estimates of loss to expectations. [sent-33, score-0.564]
</p><p>10 Uniform convergence bounds for the settings we consider may be obtained via standard data-independent model complexity measures such as VC dimension and pseudo-dimension, or via more recent measures such as Rademacher complexity. [sent-36, score-0.215]
</p><p>11 (2006) examines the considerably more limited problem of learning a model when all data sources are corrupted versions of a single, ﬁxed source, for instance when each data source provides noisy samples of a ﬁxed binary function, but with varying levels of noise. [sent-38, score-0.603]
</p><p>12 In the current work, the labels on each source may be entirely unrelated to those on other source except as constrained by the bounds on disparities, requiring us to develop new techniques. [sent-39, score-0.366]
</p><p>13 (2007) study the related problem of training classiﬁers using multiple sources of data drawn from different underlying domains but labeled using identical or similar labeling functions. [sent-41, score-0.483]
</p><p>14 In Section 3 we provide our main result, which is a general bound on the expected loss incurred by using all data within a given disparity of a target source. [sent-45, score-0.522]
</p><p>15 In each case, we are interested in the expected loss of a model g2 on target g1 , e(g1 , g2 ) = Ez∼Pg1 [L (g2 , z)]. [sent-62, score-0.22]
</p><p>16 In our multiple source model, we are presented with K distinct mutually independent samples or sources of data S1 , . [sent-64, score-0.603]
</p><p>17 Each source Si contains ni observations that are generated from a ﬁxed and unknown model f i , and D satisﬁes max(e( f i , f j ), e( f j , fi )) ≤ D(i, j). [sent-68, score-0.649]
</p><p>18 Our goal is to decide which sources S j to use in order to learn the best approximation (in terms of expected loss) to each f i . [sent-70, score-0.468]
</p><p>19 Thus without loss of generality let us suppose that we are given sources S1 , . [sent-72, score-0.535]
</p><p>20 In all cases ˆ we will analyze, for any k ≤ K, the hypothesis hk minimizing the empirical loss ek (h) on the ﬁrst k ˆ sources S1 , . [sent-88, score-0.779]
</p><p>21 , Sk , that is 1 ˆ hk = argmin ek (h) = argmin ˆ n1:k h∈H h∈H  k  nj  ∑ ∑ L (h, zij ) ,  j=1 i=1  where n1:k = n1 + · · · + nk . [sent-91, score-0.424]
</p><p>22 We also denote the expected error of function h with respect to the ﬁrst k sources of data as k ni e( fi , h). [sent-92, score-0.883]
</p><p>23 General Theory for the Multiple Source Problem In this section we provide the ﬁrst of our main results: a general bound on the expected loss of the model minimizing the empirical loss on the nearest k sources. [sent-94, score-0.378]
</p><p>24 Optimization of this bound leads to a recommended set of sources to incorporate when learning f = f 1 . [sent-95, score-0.529]
</p><p>25 The key ingredients needed to apply this bound are an approximate triangle inequality and a uniform convergence bound, which we deﬁne below. [sent-96, score-0.432]
</p><p>26 Deﬁnition 1 For α ≥ 1, we say that the α-triangle inequality holds for a class of models F and expected loss function e if for all g1 , g2 , g3 ∈ F we have e(g1 , g2 ) ≤ α(e(g1 , g3 ) + e(g3 , g2 )). [sent-98, score-0.264]
</p><p>27 Our results will require only that the unknown source models f 1 , . [sent-102, score-0.188]
</p><p>28 Deﬁnition 2 A uniform convergence bound for a hypothesis space H and loss function L is a bound that states that for any 0 < δ < 1, with probability at least 1 − δ for any h ∈ H |e(h) − e(h)| ≤ β(n, δ) , ˆ 1 where e(h) = n ∑n L (h, zi ) for n observations z1 , . [sent-107, score-0.547]
</p><p>29 This deﬁnition simply asserts that for every model in H , its empirical loss on a sample of size n and the expectation of this loss will be “close” when β(n, δ) is small. [sent-118, score-0.265]
</p><p>30 Our bounds will be derived from the rich literature on uniform convergence. [sent-120, score-0.185]
</p><p>31 Theorem 3 Let e be the expected loss function for loss L , and let F be a class of models for which the α-triangle inequality holds with respect to e. [sent-124, score-0.413]
</p><p>32 Let H ⊆ F be a class of hypothesis models for which there is a uniform convergence bound β for L . [sent-125, score-0.316]
</p><p>33 , K} k  ˆ e( f , hk ) ≤ α2 min {e( f , h)} + (α + α2 ) ∑ h∈H  i=1  ni n1:k  εi + 2αβ(n1:k , δ/2K) . [sent-133, score-0.398]
</p><p>34 The ﬁrst term in the bound is simply the approximation error, the residual loss that we incur by limiting our hypothesis model to fall in the restricted class H . [sent-135, score-0.258]
</p><p>35 We expect this term to decrease with added sources due to the increased sample size. [sent-139, score-0.386]
</p><p>36 Theorem 3 and this optimization make the implicit assumption that the best subset of sources to use will be a preﬁx of the sources—that is, that we should not “skip” a nearby source in favor of more distant ones. [sent-142, score-0.645]
</p><p>37 This assumption will be true for typical data-independent uniform convergence such as VC dimension bounds, and will be true on average for data-dependent bounds, where we expect uniform convergence bounds to improve with increased sample size. [sent-143, score-0.386]
</p><p>38 , k}, ni n1:k  ni n1:k  e( f , h) ≤  (αe( f , fi ) + αe( fi , h)) . [sent-151, score-0.902]
</p><p>39 , k}, we ﬁnd k  e( f , h) ≤  ni n1:k  ∑  i=1 k  = α∑  i=1  ni n1:k  (αe( f , fi ) + αe( fi , h)) k  ni n1:k  e( f , fi ) + α ∑  i=1  k  e( fi , h) ≤ α ∑  i=1  ni n1:k  εi + αek (h) . [sent-155, score-1.804]
</p><p>40 In the second line, we have broken up the summation using the fact that e( f , fi ) ≤ εi and the deﬁnition of ek (h). [sent-157, score-0.334]
</p><p>41 Notice that the ﬁrst summation is a weighted average of the expected loss of each f i , while the second summation is the expected loss of h on the data. [sent-158, score-0.386]
</p><p>42 Using the uniform convergence bound, we may assert that with high probability e k (h) ≤ ek (h)+β(n1:k , δ/2K), ˆ and with high probability ek (hk ) = min{ek (h)} ≤ min ˆ ˆ ˆ h∈H  h∈H  k  ∑  i=1  ni n1:k  e( fi , h) + β(n1:k , δ/2K)  . [sent-159, score-0.822]
</p><p>43 The loss function L (h, x, y ) is deﬁned as 0 if y = h(x) and 1 otherwise, and the corresponding expected loss is e(g1 , g2 ) = E x,y ∼Pg1 [L (g2 , x, y )] = Prx∼P [g1 (x) = g2 (x)]. [sent-167, score-0.268]
</p><p>44 For 0/1 loss it is well-known and easy to see that the (standard) 1-triangle inequality holds. [sent-168, score-0.188]
</p><p>45 The following function β is a uniform convergence bound for H and L when n ≥ d/2: 8(d ln(2en/d) + ln(4/δ)) β(n, δ) = . [sent-171, score-0.249]
</p><p>46 n  The proof is analogous to the standard proof of uniform convergence using the VC Dimension (see, for example, Chapters 2–4 of Anthony and Bartlett (1999)), requiring only minor modiﬁcations to the symmetrization argument to handle the fact that the samples need not be uniformly distributed. [sent-172, score-0.245]
</p><p>47 Then Pr  1 m ∑ fi (xi ) − m i=1  1 m ∑ Ex∼Di [ fi (x)] m i=1  ≥ε  ≤ 2 exp  −2ε2 m2 ∑m (bi − ai )2 i=1  ,  where the probability is over the sequence of values xi distributed according to Di for all i = 1, · · · , m. [sent-175, score-0.431]
</p><p>48 , K} k  ˆ e( f , hk ) ≤ min {e( f , h)} + 2 ∑ h∈H  i=1  ni n1:k  εi +  1762  32 (d ln (2en1:k /d) + ln (8K/δ)) . [sent-186, score-0.792]
</p><p>49 75) (marked “MAX DATA” in the left panel); the resulting source sizes for each model are shown in the bar plot in the right panel, where the origin (0, 0) is in the near corner, (1, 1) in the far corner, and the source sizes clearly peak near (0. [sent-218, score-0.258]
</p><p>50 For every function fi we used Theorem 6 to ﬁnd the best sources j to be used to estimate its parameters. [sent-221, score-0.568]
</p><p>51 We start by deriving bounds for settings in which generic Lipschitz loss functions are used, and then discuss speciﬁc applications to classiﬁcation and to regression with squared loss. [sent-235, score-0.322]
</p><p>52 , xn is deﬁned as 2 n ˆ Rn (H ) = E sup ∑ σi h(xi ) h∈H n i=1  ,  where the expectation is taken with respect to independent uniform {±1}-valued random variables σ1 , . [sent-240, score-0.248]
</p><p>53 Lemma 8 below gives a uniform convergence bound for any loss function with a corresponding Lipschitz cost function. [sent-262, score-0.36]
</p><p>54 Let H : X → Y be a class of functions and let { xi , yi }n be sampled independently according to some i=1 probability distributed P. [sent-270, score-0.21]
</p><p>55 2 Application to Classiﬁcation Using Rademacher Complexity Theorem 9 below follows from the application of Theorem 3 using the 1-triangle inequality and an application of Lemma 8 with  1 if ya ≤ 0,  φ(y, a) = 1 − ya if 0 < ya ≤ 1,   0 if ya > 1. [sent-273, score-0.365]
</p><p>56 Theorem 9 Let F be a set of functions from an input set X into {-1,1} and let R n1:k (H ) be the Rademacher complexity of H ⊆ F on the ﬁrst k sources of data. [sent-275, score-0.505]
</p><p>57 , K} k  ˆ e( f , hk ) ≤ min {e( f , h)} + 2 ∑ h∈H  i=1  ni n1:k  εi + 2  2 ln(4K/δ) + 4Rn1:k (H ) . [sent-284, score-0.398]
</p><p>58 Similarly to the VC-based bound given in Theorem 6, as k increases and more sources of data are combined, the second term will grow while the third will shrink. [sent-286, score-0.496]
</p><p>59 The behavior of the ﬁnal term R n1:k (H ), however, is less predictable and may grow or shrink as more sources of data are combined. [sent-287, score-0.386]
</p><p>60 ) Our loss function is L (h, x, y ) = (y − h(x)) 2 , and the expected loss is thus e(g1 , g2 ) = E x,y ∼Pg1 [L (g2 , x, y )] = Ex∼P (g1 (x) − g2 (x))2 . [sent-295, score-0.268]
</p><p>61 Lemma 10 Given any three functions g1 , g2 , g3 : X → R, a ﬁxed and unknown distribution P on the inputs X , and the expected loss e(g1 , g2 ) = Ex∼P (g1 (x) − g2 (x))2 , e(g1 , g2 ) ≤ 2 (e(g1 , g3 ) + e(g3 , g1 )) . [sent-298, score-0.222]
</p><p>62 We can derive a uniform convergence bound for squared loss using Rademacher complexity as long as the region Y is bounded. [sent-300, score-0.444]
</p><p>63 The following function β is a uniform convergence bound for H and L : β(n, δ) = 8BRn (H ) + 4B2  2 ln(2/δ) . [sent-302, score-0.249]
</p><p>64 4B2 2B B  Applying Lemma 8 gives a uniform convergence bound of (2/B)R n (H ) + Scaling by 4B2 yields the bound for L . [sent-308, score-0.359]
</p><p>65 , K} k  ˆ e( f , hk ) ≤ 4 min {e( f , h)} + 6 ∑ h∈H  i=1  ni εi + 32BRn1:k (H ) + 16B2 n1:k  2 ln(4K/δ) . [sent-320, score-0.398]
</p><p>66 n 1766  L EARNING FROM M ULTIPLE S OURCES  ˆ This lemma immediately allows us to replace Rn (H ) with that data-dependent quantity Rn in any of the bounds above for only a small penalty. [sent-325, score-0.202]
</p><p>67 While the use of data-dependent complexity measures can be expected to yield more accurate bounds and thus better decisions about the number k ∗ of sources to use, it is not without its costs in comparison to the more standard data-independent approaches. [sent-326, score-0.585]
</p><p>68 By Hoeffding’s inequality, with probability 1−δ , ln(2/δ ) |e( fi , f j ) − e( fi , f j )| ≤ ˆ . [sent-336, score-0.364]
</p><p>69 ˆ Using the lemma we set the upper bound on the mutual error e( f i , f j ) between the pair of function fi and f j to be Di, j = e( fi , f j ) + ε. [sent-346, score-0.568]
</p><p>70 Thus many fewer labeled examples are required to estimate the disparity matrix than to actually learn the best function in the class. [sent-349, score-0.258]
</p><p>71 These commonly rated movies can be used to determine how similar each pair of users are, while ratings of additional movies can be reserved to learn the prediction functions. [sent-353, score-0.253]
</p><p>72 Estimating the Parameters of a Distribution We now proceed with the study of the related problem of estimating the unknown parameters of a distribution from multiple sources of data. [sent-355, score-0.455]
</p><p>73 As in the previous sections, we provide a bound on the diversity of an estimator based on the ﬁrst k sources from the target. [sent-356, score-0.553]
</p><p>74 Up until this point, we have measured the diversity between two functions by using the expected value of a loss function. [sent-357, score-0.25]
</p><p>75 Example 1 We wish to estimate the bias θ of a coin given K sources of training observations N1 , . [sent-362, score-0.514]
</p><p>76 Each source Nk contains nk outcomes of ﬂips of a coin with bias θk . [sent-366, score-0.343]
</p><p>77 Example 2 We wish to estimate the probability Θ(p) of a die to fall on its pth side (out of D possible outcomes) given K sources of training observations N1 , . [sent-369, score-0.491]
</p><p>78 Each source Nk contains nk outcomes (p) using a die with parameters Θk . [sent-373, score-0.32]
</p><p>79 n i=1 In our setting, we wish to estimate the parameters Θ of a parametric distribution Pr [X|Θ] given K sources of training observations N1 , . [sent-388, score-0.46]
</p><p>80 Each source Nk contains nk outcomes from a distribution with parameters Θk , that is, Pr [X|Θk ]. [sent-392, score-0.289]
</p><p>81 Pr  E Θ(p) − Θ(p) ≥ 2n  We can use the union bound to bound on this difference for all D parameters at once and get   D B2 ln( 2D ) δ δ  ˆ ˆ Pr ∃p : E Θ(p) − Θ(p) ≥ ≤ ∑ =δ. [sent-405, score-0.22]
</p><p>82 We deﬁne the estimator using the ﬁrst k sources to be, 1 k ˆ Θk = ∑ ∑ Ψ(X) , n1:k i=1 X∈Ni where as before n1:k = ∑k ni . [sent-425, score-0.655]
</p><p>83 We denote the expectation of this estimate by i=1 1 k ¯ ˆ Θk = E Θk = ∑ ni Θi . [sent-426, score-0.312]
</p><p>84 n1:k i=1 ˆ We now bound the deviation of the estimate Θk from the true set of parameters Θ using the expec¯ tation Θk , ˆ Θ − Θk  ∞  = ≤  ¯ ¯ ˆ Θ − Θk + Θk − Θk ∞ ¯ ¯ ˆ Θ − Θk ∞ + Θk − Θk  ≤  ni Θ − Θ i n1:k i=1  ≤  ∑ n1:k εk +  k  ∑ k  ni  i=1  1769  ∞  ∞  ¯ ˆ + Θk − Θk  ¯ ˆ Θk − Θk  ∞  . [sent-427, score-0.648]
</p><p>85 Then for any δ > 0, with probability ≥ 1 − δ we have ˆ Θ − Θk  k  ∞  ni εi + i=1 n1:k  ≤∑  4B2 ln( 2DK ) δ 2n1:k  simultaneously for all k = 1, . [sent-466, score-0.269]
</p><p>86 Given the K sources of data we simply compute the bounds provided by these theorems for each preﬁx of the sources of length k and select the subset of sources that yields the smallest bound. [sent-471, score-1.266]
</p><p>87 That bound has the same form as the bound given here in Theorem 16 but with better constants. [sent-474, score-0.22]
</p><p>88 Synthetic Simulations In this section, we illustrate the bounds of our main theorem through a simple synthetic simulation. [sent-476, score-0.212]
</p><p>89 To predict the optimal set of training data sources to use for each model, we calculated an approximation of the multiple-source VC bound for classiﬁcation. [sent-487, score-0.496]
</p><p>90 It is well known that the constants in the VC-based uniform convergence bounds are not tight. [sent-488, score-0.247]
</p><p>91 Thus for the purpose of illustrating how these bounds might be used in practice, we have chosen to show approximations of our bounds with a variety of constants. [sent-489, score-0.216]
</p><p>92 In particular, we have chosen to approximate the bound with k  2∑  i=1  nk n1:K  εk +C  (d ln (2en1:K /d) + ln (8K/δ)) n1:K  with δ = 0. [sent-490, score-0.619]
</p><p>93 On the x axis is the number of data sources used in training. [sent-496, score-0.386]
</p><p>94 Dashed red curves show our multiple source error bound with C √ set to 1/4 in the lowest curve, 1/2 in the middle curve, and 1/ 2 in the highest curve. [sent-499, score-0.313]
</p><p>95 When too few sources are used, there is not enough data available to learn a 15-dimensional function. [sent-502, score-0.422]
</p><p>96 When too many sources are used, the labels on the training data often will not correspond to the labels that would have been assigned by the target function. [sent-503, score-0.449]
</p><p>97 Although the VC bounds remain loose even after constants have been dropped, the bounds tend to maintain the appropriate shape and thus predict the optimal set of sources quite well. [sent-505, score-0.665]
</p><p>98 In both cases, although the bounds are loose, they can still prove useful in determining the optimal set of sources to consider. [sent-508, score-0.494]
</p><p>99 , n}, let x i , yi be the ith training instance, distributed according to Pi , and let xi , yi be independent random variables drawn according to Pi . [sent-540, score-0.281]
</p><p>100 When only one instance xi , yi changes, the sup term can change by at most 2/n. [sent-542, score-0.21]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sources', 0.386), ('ni', 0.269), ('rademacher', 0.263), ('earns', 0.213), ('ortman', 0.213), ('rammer', 0.213), ('ln', 0.197), ('ources', 0.189), ('fi', 0.182), ('disparity', 0.165), ('hk', 0.129), ('source', 0.129), ('vc', 0.118), ('crammer', 0.117), ('ek', 0.116), ('nk', 0.115), ('ultiple', 0.115), ('loss', 0.111), ('lipschitz', 0.11), ('bound', 0.11), ('bounds', 0.108), ('lemma', 0.094), ('fk', 0.082), ('pr', 0.081), ('ex', 0.077), ('inequality', 0.077), ('uniform', 0.077), ('sup', 0.074), ('hoeffding', 0.072), ('ya', 0.072), ('disparities', 0.071), ('triangle', 0.07), ('yi', 0.069), ('xi', 0.067), ('distant', 0.066), ('mcdiarmid', 0.066), ('theorem', 0.066), ('nearby', 0.064), ('target', 0.063), ('earning', 0.063), ('bartlett', 0.063), ('convergence', 0.062), ('upenn', 0.06), ('users', 0.06), ('diversity', 0.057), ('labeled', 0.057), ('mendelson', 0.054), ('pre', 0.054), ('coin', 0.054), ('koltchinskii', 0.054), ('xn', 0.054), ('rn', 0.048), ('samples', 0.048), ('jennifer', 0.047), ('kearns', 0.047), ('expected', 0.046), ('movies', 0.046), ('complexity', 0.045), ('outcomes', 0.045), ('panel', 0.045), ('classi', 0.044), ('ers', 0.044), ('expectation', 0.043), ('observations', 0.04), ('koby', 0.04), ('cis', 0.04), ('blitzer', 0.04), ('corrupted', 0.04), ('multiple', 0.04), ('squared', 0.039), ('synthetic', 0.038), ('let', 0.038), ('hypothesis', 0.037), ('functions', 0.036), ('learn', 0.036), ('ratings', 0.036), ('ingredients', 0.036), ('summation', 0.036), ('zn', 0.035), ('curves', 0.034), ('wish', 0.034), ('recommended', 0.033), ('obeys', 0.033), ('anthony', 0.033), ('loose', 0.032), ('argmin', 0.032), ('shape', 0.031), ('thirty', 0.031), ('die', 0.031), ('models', 0.03), ('user', 0.03), ('proof', 0.029), ('commonly', 0.029), ('unknown', 0.029), ('sk', 0.029), ('disagreement', 0.029), ('aggregate', 0.029), ('regression', 0.028), ('incurred', 0.027), ('alternate', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="52-tfidf-1" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields general results for classiﬁcation and regression. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. We discuss the related problem of learning parameters of a distribution from multiple data sources. Finally, we illustrate our theory through a series of synthetic simulations. Keywords: error bounds, multi-task learning</p><p>2 0.17014936 <a title="52-tfidf-2" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>3 0.14393555 <a title="52-tfidf-3" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>Author: Andreas Maurer</p><p>Abstract: A method is introduced to learn and represent similarity with linear operators in kernel induced Hilbert spaces. Transferring error bounds for vector valued large-margin classiﬁers to the setting of Hilbert-Schmidt operators leads to dimension free bounds on a risk functional for linear representations and motivates a regularized objective functional. Minimization of this objective is effected by a simple technique of stochastic gradient descent. The resulting representations are tested on transfer problems in image processing, involving plane and spatial geometric invariants, handwritten characters and face recognition. Keywords: learning similarity, similarity, transfer learning</p><p>4 0.11148667 <a title="52-tfidf-4" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><p>5 0.10889544 <a title="52-tfidf-5" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>Author: Bernadetta Tarigan, Sara A. van de Geer</p><p>Abstract: The success of support vector machines in binary classiﬁcation relies on the fact that hinge loss employed in the risk minimization targets the Bayes rule. Recent research explores some extensions of this large margin based method to the multicategory case. We show a moment bound for the socalled multi-hinge loss minimizers based on two kinds of complexity constraints: entropy with bracketing and empirical entropy. Obtaining such a result based on the latter is harder than ﬁnding one based on the former. We obtain fast rates of convergence that adapt to the unknown margin. Keywords: multi-hinge classiﬁcation, all-at-once, moment bound, fast rate, entropy</p><p>6 0.093511559 <a title="52-tfidf-6" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>7 0.092316508 <a title="52-tfidf-7" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>8 0.07847622 <a title="52-tfidf-8" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>9 0.076154806 <a title="52-tfidf-9" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>10 0.071850114 <a title="52-tfidf-10" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>11 0.07022161 <a title="52-tfidf-11" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>12 0.068699017 <a title="52-tfidf-12" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>13 0.065229058 <a title="52-tfidf-13" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>14 0.064669549 <a title="52-tfidf-14" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>15 0.062650807 <a title="52-tfidf-15" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>16 0.059857339 <a title="52-tfidf-16" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>17 0.057208572 <a title="52-tfidf-17" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>18 0.055234812 <a title="52-tfidf-18" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>19 0.05366857 <a title="52-tfidf-19" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>20 0.052166678 <a title="52-tfidf-20" href="./jmlr-2008-Minimal_Nonlinear_Distortion_Principle_for_Nonlinear_Independent_Component_Analysis.html">60 jmlr-2008-Minimal Nonlinear Distortion Principle for Nonlinear Independent Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.3), (1, -0.116), (2, -0.146), (3, -0.108), (4, 0.057), (5, 0.067), (6, 0.055), (7, -0.298), (8, 0.094), (9, -0.105), (10, 0.153), (11, 0.062), (12, 0.073), (13, -0.007), (14, -0.053), (15, -0.034), (16, -0.142), (17, 0.079), (18, -0.034), (19, -0.022), (20, 0.043), (21, -0.047), (22, -0.028), (23, -0.084), (24, 0.016), (25, 0.148), (26, -0.134), (27, 0.163), (28, 0.017), (29, -0.011), (30, -0.055), (31, 0.037), (32, -0.006), (33, -0.094), (34, -0.042), (35, 0.01), (36, 0.02), (37, -0.111), (38, 0.11), (39, -0.111), (40, -0.064), (41, 0.008), (42, -0.109), (43, 0.029), (44, -0.054), (45, 0.105), (46, -0.15), (47, -0.001), (48, 0.025), (49, 0.07)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94138557 <a title="52-lsi-1" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields general results for classiﬁcation and regression. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. We discuss the related problem of learning parameters of a distribution from multiple data sources. Finally, we illustrate our theory through a series of synthetic simulations. Keywords: error bounds, multi-task learning</p><p>2 0.5711112 <a title="52-lsi-2" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>3 0.56912589 <a title="52-lsi-3" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>Author: Andreas Maurer</p><p>Abstract: A method is introduced to learn and represent similarity with linear operators in kernel induced Hilbert spaces. Transferring error bounds for vector valued large-margin classiﬁers to the setting of Hilbert-Schmidt operators leads to dimension free bounds on a risk functional for linear representations and motivates a regularized objective functional. Minimization of this objective is effected by a simple technique of stochastic gradient descent. The resulting representations are tested on transfer problems in image processing, involving plane and spatial geometric invariants, handwritten characters and face recognition. Keywords: learning similarity, similarity, transfer learning</p><p>4 0.4970322 <a title="52-lsi-4" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>Author: Elisa Ricci, Tijl De Bie, Nello Cristianini</p><p>Abstract: Most approaches to structured output prediction rely on a hypothesis space of prediction functions that compute their output by maximizing a linear scoring function. In this paper we present two novel learning algorithms for this hypothesis class, and a statistical analysis of their performance. The methods rely on efﬁciently computing the ﬁrst two moments of the scoring function over the output space, and using them to create convex objective functions for training. We report extensive experimental results for sequence alignment, named entity recognition, and RNA secondary structure prediction. Keywords: structured output prediction, discriminative learning, Z-score, discriminant analysis, PAC bound</p><p>5 0.46991685 <a title="52-lsi-5" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>Author: Bernadetta Tarigan, Sara A. van de Geer</p><p>Abstract: The success of support vector machines in binary classiﬁcation relies on the fact that hinge loss employed in the risk minimization targets the Bayes rule. Recent research explores some extensions of this large margin based method to the multicategory case. We show a moment bound for the socalled multi-hinge loss minimizers based on two kinds of complexity constraints: entropy with bracketing and empirical entropy. Obtaining such a result based on the latter is harder than ﬁnding one based on the former. We obtain fast rates of convergence that adapt to the unknown margin. Keywords: multi-hinge classiﬁcation, all-at-once, moment bound, fast rate, entropy</p><p>6 0.44404572 <a title="52-lsi-6" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>7 0.42168257 <a title="52-lsi-7" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>8 0.36846304 <a title="52-lsi-8" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>9 0.36315057 <a title="52-lsi-9" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>10 0.35572454 <a title="52-lsi-10" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>11 0.35379124 <a title="52-lsi-11" href="./jmlr-2008-Minimal_Nonlinear_Distortion_Principle_for_Nonlinear_Independent_Component_Analysis.html">60 jmlr-2008-Minimal Nonlinear Distortion Principle for Nonlinear Independent Component Analysis</a></p>
<p>12 0.33955294 <a title="52-lsi-12" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>13 0.31239662 <a title="52-lsi-13" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>14 0.29057652 <a title="52-lsi-14" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>15 0.2822113 <a title="52-lsi-15" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>16 0.28045949 <a title="52-lsi-16" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>17 0.27745548 <a title="52-lsi-17" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>18 0.2767629 <a title="52-lsi-18" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>19 0.27519855 <a title="52-lsi-19" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>20 0.26909769 <a title="52-lsi-20" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.024), (40, 0.03), (54, 0.033), (58, 0.032), (66, 0.056), (76, 0.034), (88, 0.067), (92, 0.579), (94, 0.039), (99, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96723878 <a title="52-lda-1" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields general results for classiﬁcation and regression. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. We discuss the related problem of learning parameters of a distribution from multiple data sources. Finally, we illustrate our theory through a series of synthetic simulations. Keywords: error bounds, multi-task learning</p><p>2 0.95261872 <a title="52-lda-2" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>Author: Amit Dhurandhar, Alin Dobra</p><p>Abstract: In this paper we use the methodology introduced by Dhurandhar and Dobra (2009) for analyzing the error of classiﬁers and the model selection measures, to analyze decision tree algorithms. The methodology consists of obtaining parametric expressions for the moments of the generalization error (GE) for the classiﬁcation model of interest, followed by plotting these expressions for interpretability. The major challenge in applying the methodology to decision trees, the main theme of this work, is customizing the generic expressions for the moments of GE to this particular classiﬁcation algorithm. The speciﬁc contributions we make in this paper are: (a) we primarily characterize a subclass of decision trees namely, Random decision trees, (b) we discuss how the analysis extends to other decision tree algorithms and (c) in order to extend the analysis to certain model selection measures, we generalize the relationships between the moments of GE and moments of the model selection measures given in (Dhurandhar and Dobra, 2009) to randomized classiﬁcation algorithms. An empirical comparison of the proposed method with Monte Carlo and distribution free bounds obtained using Breiman’s formula, depicts the advantages of the method in terms of running time and accuracy. It thus showcases the use of the deployed methodology as an exploratory tool to study learning algorithms. Keywords: moments, generalization error, decision trees</p><p>3 0.67324996 <a title="52-lda-3" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>Author: Sivan Sabato, Shai Shalev-Shwartz</p><p>Abstract: Feature ranking is a fundamental machine learning task with various applications, including feature selection and decision tree learning. We describe and analyze a new feature ranking method that supports categorical features with a large number of possible values. We show that existing ranking criteria rank a feature according to the training error of a predictor based on the feature. This approach can fail when ranking categorical features with many values. We propose the Ginger ranking criterion, that estimates the generalization error of the predictor associated with the Gini index. We show that for almost all training sets, the Ginger criterion produces an accurate estimation of the true generalization error, regardless of the number of values in a categorical feature. We also address the question of ﬁnding the optimal predictor that is based on a single categorical feature. It is shown that the predictor associated with the misclassiﬁcation error criterion has the minimal expected generalization error. We bound the bias of this predictor with respect to the generalization error of the Bayes optimal predictor, and analyze its concentration properties. We demonstrate the efﬁciency of our approach for feature selection and for learning decision trees in a series of experiments with synthetic and natural data sets. Keywords: feature ranking, categorical features, generalization bounds, Gini index, decision trees</p><p>4 0.62014586 <a title="52-lda-4" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>Author: Eric Bax</p><p>Abstract: This paper develops bounds on out-of-sample error rates for support vector machines (SVMs). The bounds are based on the numbers of support vectors in the SVMs rather than on VC dimension. The bounds developed here improve on support vector counting bounds derived using Littlestone and Warmuth’s compression-based bounding technique. Keywords: compression, error bound, support vector machine, nearly uniform</p><p>5 0.59816873 <a title="52-lda-5" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>Author: Bernadetta Tarigan, Sara A. van de Geer</p><p>Abstract: The success of support vector machines in binary classiﬁcation relies on the fact that hinge loss employed in the risk minimization targets the Bayes rule. Recent research explores some extensions of this large margin based method to the multicategory case. We show a moment bound for the socalled multi-hinge loss minimizers based on two kinds of complexity constraints: entropy with bracketing and empirical entropy. Obtaining such a result based on the latter is harder than ﬁnding one based on the former. We obtain fast rates of convergence that adapt to the unknown margin. Keywords: multi-hinge classiﬁcation, all-at-once, moment bound, fast rate, entropy</p><p>6 0.58725518 <a title="52-lda-6" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>7 0.56499994 <a title="52-lda-7" href="./jmlr-2008-Causal_Reasoning_with_Ancestral_Graphs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">20 jmlr-2008-Causal Reasoning with Ancestral Graphs    (Special Topic on Causality)</a></p>
<p>8 0.54700959 <a title="52-lda-8" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>9 0.48964784 <a title="52-lda-9" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>10 0.47903806 <a title="52-lda-10" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>11 0.4702796 <a title="52-lda-11" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>12 0.45515698 <a title="52-lda-12" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>13 0.44942918 <a title="52-lda-13" href="./jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>14 0.44702157 <a title="52-lda-14" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>15 0.44564253 <a title="52-lda-15" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>16 0.44447467 <a title="52-lda-16" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>17 0.44394073 <a title="52-lda-17" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>18 0.44106662 <a title="52-lda-18" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>19 0.44101158 <a title="52-lda-19" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>20 0.43925166 <a title="52-lda-20" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
