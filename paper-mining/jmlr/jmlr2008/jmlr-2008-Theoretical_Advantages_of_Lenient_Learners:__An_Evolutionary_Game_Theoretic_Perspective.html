<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-90" href="#">jmlr2008-90</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</h1>
<br/><p>Source: <a title="jmlr-2008-90-pdf" href="http://jmlr.org/papers/volume9/panait08a/panait08a.pdf">pdf</a></p><p>Author: Liviu Panait, Karl Tuyls, Sean Luke</p><p>Abstract: This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the beneﬁts of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction</p><p>Reference: <a title="jmlr-2008-90-reference" href="../jmlr2008_reference/jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Department of Computer Science George Mason University Fairfax, VA 22030, USA  Editor: Leslie Pack Kaelbling  Abstract This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. [sent-8, score-0.426]
</p><p>2 We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. [sent-9, score-1.391]
</p><p>3 In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. [sent-12, score-0.705]
</p><p>4 Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. [sent-13, score-0.464]
</p><p>5 Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction  1. [sent-14, score-0.897]
</p><p>6 Different reasons account for this: other agents are learning as well; the reinforcement an agent receives depends on the actions taken by the other agents; and not all information is observable. [sent-30, score-0.715]
</p><p>7 This paper presents a theoretical foundation for multiagent learning in coordination games with no internal state; we only consider symmetric games where all agents receive equal reward. [sent-32, score-0.703]
</p><p>8 This foundation may be directly applied to two popular algorithms: cooperative coevolutionary algorithms (CCEAs) and multiagent Q-learning. [sent-33, score-0.569]
</p><p>9 These two techniques are representative of the two families (evolutionary algorithms and multiagent reinforcement learning) which form the bulk of the cooperative multiagent learning ﬁeld (Panait and Luke, 2005a). [sent-34, score-0.789]
</p><p>10 In both of these algorithms it is common for an agent to base its assessment of a particular action on the expected reward received for that action. [sent-35, score-0.625]
</p><p>11 For example, a reinforcement learning agent usually updates its estimate for an action’s utility every time1 that action is performed in the environment, despite the dependence of the reward on the actions performed by the other agents. [sent-36, score-0.901]
</p><p>12 The formal analysis in this paper demonstrates that the reinforcement provided by the expected reward might be the primary cause for many observed problems with multiagent learning. [sent-37, score-0.543]
</p><p>13 We call this approach lenience: each agent shows lenience to its teammates by ignoring low rewards due to actions chosen by teammates that are poor matches to the agent’s current action. [sent-39, score-0.817]
</p><p>14 Lenience is particularly important early in the game, when the agents have not yet identiﬁed high-performing actions and so the average reward of a given action can be misleadingly low due to partnering with poor-performing joint actions. [sent-51, score-0.829]
</p><p>15 Here, we will establish a more formal justiﬁcation for the effectiveness of lenience in multiagent learning, accompanied by an intuitive visualization of the impact that lenience has onto the tendency of these algorithms to converge to suboptimal solutions. [sent-54, score-0.749]
</p><p>16 Background This paper presents a formal analysis of two multiagent learning algorithms: cooperative coevolutionary algorithms and multiagent Q-learning. [sent-61, score-0.91]
</p><p>17 2 present two evolutionary game theory models that capture the dynamics of the two learning algorithms, that is, CCEA and multiagent Q-learning. [sent-76, score-0.503]
</p><p>18 The populations evolve concurrently (this is closer to the asynchronous nature of multiagent systems). [sent-102, score-0.482]
</p><p>19 The algorithm associates a utility (or Quality) Q with each (s, a) pair, where s is a state of the environment, and a is an action that the agent can perform when in state s. [sent-107, score-0.477]
</p><p>20 Given this assumption that each agent has its own population, we sometimes use the term population instead of agent when the context deems it more appropriate. [sent-112, score-0.571]
</p><p>21 All agents choose their actions independently and concurrently, perform them in parallel, and observe the same reward associated with the joint action. [sent-119, score-0.621]
</p><p>22 In contrast to single-agent Q-learning, the information an agent observes depends on the actions chosen by other agents. [sent-120, score-0.448]
</p><p>23 EGT also commonly presumes one or more matrixes that represent the performance of actions in the context of actions of the other agents. [sent-128, score-0.444]
</p><p>24 Each timestep, the proportions of actions for each agent are changed based on the rewards in the matrix as well as on the current probabilities of other agents’ choosing their actions. [sent-129, score-0.645]
</p><p>25 In essence the replicator equations describe how a population of different actions evolves through time. [sent-133, score-0.484]
</p><p>26 Let A = (ai j )n j=1 be the reward matrix (ai j ≥ 0 is the reward for the joint action (i, j), i, 427  PANAIT, T UYLS AND L UKE  and n is the total number of possible actions). [sent-143, score-0.542]
</p><p>27 Let us assume that the individuals in the population (considered inﬁnite) represent different actions that the agent can perform. [sent-144, score-0.632]
</p><p>28 , xn (t)), where xi (t) denotes the proportion of individual i in the population (it is also directly related to the probability that the agent will select action i). [sent-148, score-0.656]
</p><p>29 More precisely, the expected number of offsprings for a single individual representing action i equals the ratio between the expected payoff for such an individual, ∑n ai j x j (t), j=1 and the average payoff ∑n xk (t) ∑n ak j x j (t) for all individuals in the population. [sent-150, score-0.854]
</p><p>30 Therefore, j=1 k=1 the ratio xi (t + 1) of individuals representing action i at the next time step equals: xi (t + 1) = xi (t)  ∑n ai j x j (t) j=1 . [sent-151, score-0.687]
</p><p>31 = n xi (t) xk (t) ∑n ak j x j (t) ∑k=1 j=1 A minor manipulation of the fractions leads to the general discrete time replicator dynamics: n Δxi (t) ∑ j=1 ai j xi (t) − x(t)Ax(t) = . [sent-153, score-0.518]
</p><p>32 xi (t) x(t)Ax(t)  This system of equations expresses Darwinian selection as follows: the rate of change Δxi of a particular action i is the difference between its average payoff, ∑n ai j xi (t), and the average payoffs j=1 for all actions, x(t)Ax(t). [sent-154, score-0.584]
</p><p>33 xi (t) xi (t) x(t)δAx(t) + (1 − δ)  Now, at the limit when δ approaches 0, the continuous replicator equations are: dxi = dt  n  ∑ ai j xi − x · Ax  xi  j=1  which can be rewritten as: dxi = [(Ax)i − x · Ax] xi . [sent-163, score-0.888]
</p><p>34 Hence (Ax)i is the payoff that replicator i receives in a population dxi  dt with state x and x · Ax describes the average payoff in the population. [sent-169, score-0.658]
</p><p>35 The growth rate xi of the proportion of action i in the population equals the difference between the action’s current payoff and the average payoff in the population. [sent-170, score-0.75]
</p><p>36 For simplicity, the discussion focuses on only two such learning agents in a symmetric game (both agents receive the same payoff). [sent-174, score-0.567]
</p><p>37 This setup corresponds to an RD for asymmetric games, where the available actions or strategies of the agents to belong to two different population. [sent-176, score-0.454]
</p><p>38 For example, consider the case of extending the continuous RD model to a multiagent learning scenario involving a two-player symmetric game with two agents, and consider that AT is the payoff matrix that describes the reinforcements received by the second agent. [sent-177, score-0.569]
</p><p>39 The model applies to stateless domains involving only two agents, and where each agent has a ﬁnite set of actions to choose from. [sent-195, score-0.494]
</p><p>40 The two populations (one per agent) contain individuals, each of them representing an action that the agent might perform. [sent-196, score-0.607]
</p><p>41 The model further assumes that the two populations are inﬁnite, and it computes the proportions of individuals in the populations at each generation. [sent-197, score-0.445]
</p><p>42 If the ﬁrst agent has a ﬁnite number n of distinct actions to choose from, its population at each generation is an element of the set Δn = {x ∈ [0, 1]n | ∑n xi = 1}. [sent-198, score-0.643]
</p><p>43 A higher i=1 proportion xi indicates that the agent is more likely to choose action i. [sent-199, score-0.537]
</p><p>44 Given that the second agent might have a different set (of size m) of actions to choose from, its population is an element of the 429  PANAIT, T UYLS AND L UKE  set Δm = y ∈ [0, 1]m | ∑m y j = 1 . [sent-200, score-0.567]
</p><p>45 The ﬁtness computation in the EGT model involves the game j=1 payoff matrix A, where the ai j element represents the reward for the joint action (i, j). [sent-201, score-0.797]
</p><p>46 The ﬁtness of action i is estimated as the mean payoff over pairwise collaborations with every action in the other population. [sent-208, score-0.576]
</p><p>47 For simplicity, we only consider games between two agents, and assume that the game is stateless: the reward that the agents receive depends solely on the actions they have currently performed in the environment. [sent-217, score-0.761]
</p><p>48 Each agent has a probability vector over its action set, more precisely x1 , . [sent-220, score-0.434]
</p><p>49 j  j  , it follows that α ∑ x j ln j  xj α = ∑ x j (Qa j − Qai ) xi τ j  which gives us dxi dt  α τ  =  xi  r ai − ∑ x j r a j j  xj xi  + α ∑ x j ln j  . [sent-259, score-0.528]
</p><p>50 For clarity, we use ui (instead of rai ) to indicate the expected reward that the ﬁrst agent expects for performing action i, and w j to indicate the expected reward that the second agent expects for performing action j. [sent-261, score-1.413]
</p><p>51 Remember that the payoff matrix for the second agent is simply the transposed payoff matrix used by the ﬁrst agent. [sent-262, score-0.546]
</p><p>52 Thus ui = ∑k aik yk is the expected reward that would be observed by the ﬁrst agent, given the other agent’s current probabilities of selecting among its actions (and similarly w j = ∑k ak j xk ). [sent-263, score-0.677]
</p><p>53 The RD model above therefore has the simpler form, ui =  ∑ aik yk ,  (14)  ∑ ak j xk ,  (15)  k  wj =  k  dxi dt  xi dy j dt  yj  = =  α τ  ui − ∑ xk uk + α ∑ xk ln  α τ  w j − ∑ yk wk + α ∑ yk ln  k  k  k  k  xk , xi yk . [sent-264, score-1.281]
</p><p>54 433  PANAIT, T UYLS AND L UKE  reward(i,j)  B C  A  j  i  Figure 1: The relative generalization pathology in multiagent cooperative learning. [sent-270, score-0.469]
</p><p>55 The axes i and j are the various actions afforded to two different agents, and the axis reward(i, j) is the reward received from a given joint action (higher is better). [sent-271, score-0.621]
</p><p>56 Both agents receive the same reward based on the joint action selected, and the objective of the two agents is to arrive on the joint action that optimizes their joint reward. [sent-278, score-1.074]
</p><p>57 Line A represents the possible rewards for the ﬁrst population following action iA when paired with various actions j from the other population. [sent-283, score-0.712]
</p><p>58 Some of these rewards are near the optimum; but most of them are below the suboptimal rewards that action iB (line B) receives. [sent-284, score-0.588]
</p><p>59 First, the rewards ob434  T HEORETICAL A DVANTAGES OF L ENIENT L EARNERS : A N EGT P ERSPECTIVE  served by the agent upon performing action i over a period of time are higher than the ones observed for action j during the same period of time. [sent-291, score-0.832]
</p><p>60 This results in an expected higher utility for performing action i, as opposed to performing action j. [sent-292, score-0.513]
</p><p>61 Second, the reward obtained for j when the teammate selects a speciﬁc action k is higher than the reward obtain for i with any action selected by the teammate. [sent-293, score-0.823]
</p><p>62 The circles represent how good actions iA and iB could be (when matched by speciﬁc actions of the teammate), while the triangle illustrates the low rewards due to action jC that contribute to action shadowing (action shadowing is the result of many such actions). [sent-296, score-1.071]
</p><p>63 Note informally that both relative overgeneralization and action shadowing might be solved by using the same approach: basing an action’s utility on the maximum reward over some large number N of interactions with actions chosen by the other player. [sent-297, score-0.691]
</p><p>64 As N grows, the utility of an action approaches the joint reward it would produce were it paired with its optimum collaborating action; and thus each population could select from its actions knowing the true optimum potential of each. [sent-298, score-0.841]
</p><p>65 4 by allowing agents to ignore the lower rewards received for a given action, notionally due to poorly-matched actions from the teammate, and instead concentrate on the higher rewards seen so far for that action. [sent-303, score-0.804]
</p><p>66 Consider the Climb and Penalty domains introduced in Claus and Boutilier (1998) and used in previous investigations of multiagent Q-learning (Kapetanakis and Kudenko, 2002; Lauer and Riedmiller, 2000) and cooperative coevolution (Panait et al. [sent-304, score-0.546]
</p><p>67 This means that (2, 2) risk dominates (1, 1) and (3, 3), because playing action 2 will provide a higher expected payoff when an agent is uncertain about the other agent’s action,. [sent-311, score-0.594]
</p><p>68 Given that the agents have just started to learn and have no knowledge about the structure of the payoff matrix, we may assume that the agents choose their actions randomly. [sent-313, score-0.846]
</p><p>69 The expected reward for each action an agent might perform is: 435  PANAIT, T UYLS AND L UKE  ExpectedReward(a1 ) =  11 ∗ 1 − 30 ∗ 1 + 0 ∗ 1 3 3 3  ExpectedReward(a2 ) = −30 ∗ + 7 ∗ + 6 ∗ 1 3  ExpectedReward(a3 ) =  1 3  0∗ +0∗ +5∗ 1 3  1 3  = −6. [sent-314, score-0.601]
</p><p>70 The agent might thus become less likely to choose action a1 due to the lower rewards it receives at early stages of learning. [sent-318, score-0.597]
</p><p>71 67  1 3 1 3  =0  One solution to remedy this problem is to allow agents to show lenience towards their teammates: the agents can ignore lower rewards observed upon performing their actions, and only update the utilities of actions based on the higher rewards. [sent-321, score-1.014]
</p><p>72 The paper introduced a time-dependent degree of lenience: the agents start by ignoring most of the low rewards that they observe, but as learning progresses, agents tend to explore certain “better” actions and also ignore fewer low rewards. [sent-327, score-0.849]
</p><p>73 The paper also presented empirical evidence that several multiagent learning paradigms can signiﬁcantly beneﬁt from agents being lenient to one another. [sent-328, score-0.654]
</p><p>74 Similarly, cooperative coevolutionary algorithms often use agents that ignore lower rewards (for example, in Wiegand et al. [sent-329, score-0.655]
</p><p>75 This paper focuses on the mathematical foundations of a simpler approach to lenient learners: each agent collects the rewards it receives for performing actions. [sent-332, score-0.529]
</p><p>76 The more low rewards the agents ignore (the higher N is), the more lenient the agents can be said to be. [sent-336, score-0.74]
</p><p>77 First, it provides a valuable addition to our set of tools to study such multiagent learning algorithms, and also strengthens evolutionary game theory as a primary framework for such analysis. [sent-338, score-0.464]
</p><p>78 Theorem 1 Let (ai j )n j=1 be the payoff matrix (ai j is the payoff received by the agents when one i, performs action i and the other performs action j), and let (p j ) j∈1. [sent-343, score-0.992]
</p><p>79 Lemma 1 basically proved that the probability that an agent starts with an extremely low or an extremely high probability of selecting an action is very low, given a random initialization of the multiagent learning process. [sent-391, score-0.743]
</p><p>80 Following, Lemma 2 derives a complementary result, namely that each agent should have a fair likelihood of selecting each of its actions when the learning process starts. [sent-392, score-0.448]
</p><p>81 Theorem 2 Given a joint reward matrix A with a unique global optimum ai j , for any ε > 0 there exists Nε ≥ 1 such that the theoretical CCEA model in Equations 18 – 21 converges to the global optimum with probability greater than (1 − ε) for any number of collaborators N such that N ≥ Nε . [sent-439, score-0.561]
</p><p>82 It follows from Equation 26 that ⎛ (0)  ui  ≥ ai  j  − (1 − ηε )N ⎝ai  However, 442  ⎞ j  −  ∑  j= j ∧ai j <0  ai j ⎠ . [sent-449, score-0.464]
</p><p>83 xi  yj  (t)  ∑  ai j ⎠ ,  j= j ∧ai j <0  j  −  ∑  (33)  ⎞  ai j ⎠ ,  i=i ∧ai j <0  (34)  (t)  (35) 443  PANAIT, T UYLS AND L UKE  At the ﬁrst generation (t = 0), the ﬁrst two inequalities hold (Equations 27 and 32). [sent-456, score-0.581]
</p><p>84 As a consequence, Equation 20 implies that  (1)  xi  (0)  = > =  ui  (0)  xi  (0) (0) ∑n xk uk k=1 (0) ui (0) (0) ∑n xk ui k=1 (0) xi . [sent-458, score-0.59]
</p><p>85 (0)  xi  and Equation 21 similarly implies that  (0)  (1) yj  = > =  wj  (0)  yj  (0) (0) ∑m yk wk k=1 (0) wj (0) (0) ∑m yk w j k=1 (0) yj . [sent-459, score-0.819]
</p><p>86 j= j ∧ai j <0  (t+1)  Given the deﬁnitions of N and α, this also implies that ui consequence, 444  ai j ⎠  j= j ∧ai j <0  ⎛ ≥  ai j ⎠  ∑  −  (t+1)  > α > ui  for all i = i . [sent-461, score-0.556]
</p><p>87 As a  T HEORETICAL A DVANTAGES OF L ENIENT L EARNERS : A N EGT P ERSPECTIVE  (t+1)  xi  (t)  = > =  ui  (t)  xi  (t) (t) ∑n xk uk k=1 (t) ui (t) (t) ∑n xk ui k=1 (t) xi . [sent-462, score-0.59]
</p><p>88 (t)  xi  Similarly, Equation 31 and the inductive hypothesis imply that ⎛ (t+1)  wj  ≥  ai  j  (t+1) N ⎝  − 1 − xi  ai  ⎞ i=i ∧ai j <0  ⎛ ≥  ai  j  −  (t) N ⎝ 1 − xi ai j  ··· ai  j  i=i ∧ai j <0  ⎞  − (1 − ηε )N ⎝ai  j  i=i ∧ai j <0 (t+1)  (t+1)  (t+1)  > α > wj  (t)  = > =  (t)  ai j ⎠ . [sent-463, score-1.536]
</p><p>89 ∑  −  Again, given the deﬁnition of N and α, this implies w j consequence,  yj  ⎞  ai j ⎠  ∑  −  ⎛  ≥  ai j ⎠  ∑  −  j  wj  for all j = j . [sent-464, score-0.601]
</p><p>90 As a  (t)  yj  (t) (t) ∑m yk wk k=1 (t) wj (t) (t) ∑m yk w j k=1 (t) yj  (t)  yj  (t)  Having shown that both xi and y j are monotonically increasing (Equations 34 – 35), t t and given that they are bounded between 0 and 1, it follows that they converge to some value. [sent-465, score-0.748]
</p><p>91 We also have that  (t+1)  xi  (t)  xi  (t)  =  ui  (t) (t)  ∑n uk xk k=1  (t)  ≥  ui (t) (t)  (t)  ui xi + α 1 − xi (t)  = 1+  ui − α (t) (t)  (t)  1 − xi  (t)  ui xi + α 1 − xi 445  . [sent-469, score-1.035]
</p><p>92 PANAIT, T UYLS AND L UKE  (t+1)  xi  But limt→∞  (t)  xi  = 1, and therefore (t)  lim  ui − α  (t)  1 − xi  t→∞ (t) (t) ui xi + α  (t)  1 − xi  =0  1 − xi  (t)  which implies lim ui − α  t→∞  (t)  (t)  = 0. [sent-470, score-0.8]
</p><p>93 The proof that  follows that  (t) xi  and  (t) yj  (t) yj  ai  j  2  −  α 2  > 0. [sent-472, score-0.528]
</p><p>94 Next, we use a visualization technique to study the impact of lenience onto cooperative coevolutionary algorithms as applied to domains with one optimum, as well as to domains with multiple global optima. [sent-482, score-0.543]
</p><p>95 As shown by Wiegand (2004), the populations are expected to converge to Nash equilibria in the payoff matrix. [sent-486, score-0.433]
</p><p>96 The sampling also does not cover initial populations on the edges or vertexes of the simplex, but the probability that an evolutionary algorithm starts from those initial populations is negligibly small. [sent-515, score-0.425]
</p><p>97 The visualization of the basins of attraction to Nash equilibria provided an intuitive approach to grasping the impact that lenience has onto cooperative coevolutionary algorithms. [sent-538, score-0.756]
</p><p>98 Next, we apply this visualization technique to show that lenience is not speciﬁc to only cooperative coevolution, but it can be used to improve the performance of other multiagent learning algorithms as well. [sent-540, score-0.61]
</p><p>99 Evolutionary Game Theory Models for Lenient Multiagent Q-Learning The RD model in Equations 14–17 assumes that the agent will update the utility of an action based on the average reward it expects to receive for that action. [sent-542, score-0.671]
</p><p>100 As argued in Section 3 and demonstrated in Section 4 (for cooperative coevolutionary algorithms only), using the average reward usually results in poor estimates for the quality of actions, causing potential attraction to towards suboptimal solutions. [sent-544, score-0.598]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('multiagent', 0.309), ('egt', 0.259), ('agents', 0.232), ('agent', 0.226), ('actions', 0.222), ('action', 0.208), ('qai', 0.194), ('ai', 0.186), ('populations', 0.173), ('reward', 0.167), ('rewards', 0.163), ('payoff', 0.16), ('panait', 0.158), ('lenience', 0.138), ('cooperative', 0.136), ('yj', 0.133), ('qa', 0.129), ('collaborators', 0.126), ('coevolutionary', 0.124), ('population', 0.119), ('attraction', 0.117), ('basins', 0.113), ('lenient', 0.113), ('replicator', 0.105), ('dvantages', 0.097), ('earners', 0.097), ('enient', 0.097), ('heoretical', 0.097), ('uke', 0.097), ('uyls', 0.097), ('wiegand', 0.096), ('wj', 0.096), ('ui', 0.092), ('tness', 0.085), ('erspective', 0.082), ('evolutionary', 0.079), ('nash', 0.079), ('game', 0.076), ('yk', 0.076), ('xi', 0.076), ('equilibria', 0.075), ('teammate', 0.073), ('individuals', 0.065), ('cceas', 0.065), ('climb', 0.065), ('dxi', 0.065), ('rai', 0.065), ('coordination', 0.061), ('coevolution', 0.055), ('suboptimal', 0.054), ('dt', 0.049), ('collaborator', 0.049), ('expectedreward', 0.049), ('tuyls', 0.049), ('domains', 0.046), ('aik', 0.045), ('ax', 0.044), ('xk', 0.043), ('potter', 0.043), ('utility', 0.043), ('luke', 0.041), ('optimum', 0.041), ('dynamics', 0.039), ('equations', 0.038), ('games', 0.037), ('reinforcement', 0.035), ('lim', 0.034), ('proportions', 0.034), ('jong', 0.034), ('teammates', 0.034), ('rd', 0.033), ('ccea', 0.032), ('replicators', 0.032), ('tumer', 0.032), ('ak', 0.032), ('formal', 0.032), ('ia', 0.031), ('ib', 0.031), ('learners', 0.031), ('genotypes', 0.027), ('overgeneralization', 0.027), ('claus', 0.027), ('proportion', 0.027), ('visualization', 0.027), ('receive', 0.027), ('performing', 0.027), ('impact', 0.026), ('st', 0.025), ('converge', 0.025), ('boundaries', 0.025), ('agogino', 0.024), ('dqa', 0.024), ('dqai', 0.024), ('kapetanakis', 0.024), ('kudenko', 0.024), ('liviu', 0.024), ('pathology', 0.024), ('sean', 0.024), ('shadowing', 0.024), ('received', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999893 <a title="90-tfidf-1" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>Author: Liviu Panait, Karl Tuyls, Sean Luke</p><p>Abstract: This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the beneﬁts of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction</p><p>2 0.18187638 <a title="90-tfidf-2" href="./jmlr-2008-Multi-Agent_Reinforcement_Learning_in_Common_Interest_and_Fixed_Sum_Stochastic_Games%3A_An_Experimental_Study.html">65 jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</a></p>
<p>Author: Avraham Bab, Ronen I. Brafman</p><p>Abstract: Multi Agent Reinforcement Learning (MARL) has received continually growing attention in the past decade. Many algorithms that vary in their approaches to the different subtasks of MARL have been developed. However, the theoretical convergence results for these algorithms do not give a clue as to their practical performance nor supply insights to the dynamics of the learning process itself. This work is a comprehensive empirical study conducted on MGS, a simulation system developed for this purpose. It surveys the important algorithms in the ﬁeld, demonstrates the strengths and weaknesses of the different approaches to MARL through application of FriendQ, OAL, WoLF, FoeQ, Rmax, and other algorithms to a variety of fully cooperative and fully competitive domains in self and heterogeneous play, and supplies an informal analysis of the resulting learning processes. The results can aid in the design of new learning algorithms, in matching existing algorithms to speciﬁc tasks, and may guide further research and formal analysis of the learning processes. Keywords: reinforcement learning, multi-agent reinforcement learning, stochastic games</p><p>3 0.13983364 <a title="90-tfidf-3" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>Author: Faustino Gomez, Jürgen Schmidhuber, Risto Miikkulainen</p><p>Abstract: Many complex control problems require sophisticated solutions that are not amenable to traditional controller design. Not only is it difﬁcult to model real world systems, but often it is unclear what kind of behavior is required to solve the task. Reinforcement learning (RL) approaches have made progress by using direct interaction with the task environment, but have so far not scaled well to large state spaces and environments that are not fully observable. In recent years, neuroevolution, the artiﬁcial evolution of neural networks, has had remarkable success in tasks that exhibit these two properties. In this paper, we compare a neuroevolution method called Cooperative Synapse Neuroevolution (CoSyNE), that uses cooperative coevolution at the level of individual synaptic weights, to a broad range of reinforcement learning algorithms on very difﬁcult versions of the pole balancing problem that involve large (continuous) state spaces and hidden state. CoSyNE is shown to be signiﬁcantly more efﬁcient and powerful than the other methods on these tasks. Keywords: coevolution, recurrent neural networks, non-linear control, genetic algorithms, experimental comparison</p><p>4 0.083526887 <a title="90-tfidf-4" href="./jmlr-2008-Learning_Control_Knowledge_for_Forward_Search_Planning.html">49 jmlr-2008-Learning Control Knowledge for Forward Search Planning</a></p>
<p>Author: Sungwook Yoon, Alan Fern, Robert Givan</p><p>Abstract: A number of today’s state-of-the-art planners are based on forward state-space search. The impressive performance can be attributed to progress in computing domain independent heuristics that perform well across many domains. However, it is easy to ﬁnd domains where such heuristics provide poor guidance, leading to planning failure. Motivated by such failures, the focus of this paper is to investigate mechanisms for learning domain-speciﬁc knowledge to better control forward search in a given domain. While there has been a large body of work on inductive learning of control knowledge for AI planning, there is a void of work aimed at forward-state-space search. One reason for this may be that it is challenging to specify a knowledge representation for compactly representing important concepts across a wide range of domains. One of the main contributions of this work is to introduce a novel feature space for representing such control knowledge. The key idea is to deﬁne features in terms of information computed via relaxed plan extraction, which has been a major source of success for non-learning planners. This gives a new way of leveraging relaxed planning techniques in the context of learning. Using this feature space, we describe three forms of control knowledge—reactive policies (decision list rules and measures of progress) and linear heuristics—and show how to learn them and incorporate them into forward state-space search. Our empirical results show that our approaches are able to surpass state-of-the-art nonlearning planners across a wide range of planning competition domains. Keywords: planning, machine learning, knowledge representation, search</p><p>5 0.065989934 <a title="90-tfidf-5" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>Author: Balázs Csanád Csáji, László Monostori</p><p>Abstract: The paper investigates the possibility of applying value function based reinforcement learning (RL) methods in cases when the environment may change over time. First, theorems are presented which show that the optimal value function of a discounted Markov decision process (MDP) Lipschitz continuously depends on the immediate-cost function and the transition-probability function. Dependence on the discount factor is also analyzed and shown to be non-Lipschitz. Afterwards, the concept of (ε, δ)-MDPs is introduced, which is a generalization of MDPs and ε-MDPs. In this model the environment may change over time, more precisely, the transition function and the cost function may vary from time to time, but the changes must be bounded in the limit. Then, learning algorithms in changing environments are analyzed. A general relaxed convergence theorem for stochastic iterative algorithms is presented. We also demonstrate the results through three classical RL methods: asynchronous value iteration, Q-learning and temporal difference learning. Finally, some numerical experiments concerning changing environments are presented. Keywords: Markov decision processes, reinforcement learning, changing environments, (ε, δ)MDPs, value function bounds, stochastic iterative algorithms</p><p>6 0.062678784 <a title="90-tfidf-6" href="./jmlr-2008-Comments_on_the_Complete_Characterization_of_a_Family_of_Solutions_to_a_GeneralizedFisherCriterion.html">23 jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</a></p>
<p>7 0.060655516 <a title="90-tfidf-7" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>8 0.054268073 <a title="90-tfidf-8" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>9 0.049487699 <a title="90-tfidf-9" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>10 0.039666887 <a title="90-tfidf-10" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>11 0.034324002 <a title="90-tfidf-11" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>12 0.030618008 <a title="90-tfidf-12" href="./jmlr-2008-Shark%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">85 jmlr-2008-Shark    (Machine Learning Open Source Software Paper)</a></p>
<p>13 0.029094953 <a title="90-tfidf-13" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>14 0.028623579 <a title="90-tfidf-14" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>15 0.028245263 <a title="90-tfidf-15" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>16 0.026604343 <a title="90-tfidf-16" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>17 0.025868544 <a title="90-tfidf-17" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>18 0.025865078 <a title="90-tfidf-18" href="./jmlr-2008-Non-Parametric_Modeling_of_Partially_Ranked_Data.html">69 jmlr-2008-Non-Parametric Modeling of Partially Ranked Data</a></p>
<p>19 0.024957027 <a title="90-tfidf-19" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>20 0.024093797 <a title="90-tfidf-20" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.141), (1, -0.055), (2, -0.127), (3, -0.061), (4, -0.271), (5, -0.269), (6, -0.183), (7, 0.001), (8, 0.003), (9, 0.164), (10, 0.064), (11, 0.051), (12, 0.016), (13, 0.048), (14, 0.185), (15, -0.032), (16, 0.241), (17, 0.212), (18, 0.009), (19, 0.126), (20, 0.01), (21, 0.102), (22, 0.031), (23, 0.084), (24, 0.158), (25, -0.141), (26, -0.032), (27, 0.074), (28, 0.134), (29, -0.06), (30, -0.128), (31, -0.069), (32, 0.019), (33, -0.069), (34, -0.12), (35, 0.028), (36, 0.141), (37, 0.038), (38, 0.033), (39, 0.032), (40, -0.079), (41, -0.041), (42, -0.012), (43, 0.036), (44, 0.016), (45, 0.02), (46, -0.091), (47, 0.108), (48, -0.073), (49, -0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96519637 <a title="90-lsi-1" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>Author: Liviu Panait, Karl Tuyls, Sean Luke</p><p>Abstract: This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the beneﬁts of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction</p><p>2 0.80023831 <a title="90-lsi-2" href="./jmlr-2008-Multi-Agent_Reinforcement_Learning_in_Common_Interest_and_Fixed_Sum_Stochastic_Games%3A_An_Experimental_Study.html">65 jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</a></p>
<p>Author: Avraham Bab, Ronen I. Brafman</p><p>Abstract: Multi Agent Reinforcement Learning (MARL) has received continually growing attention in the past decade. Many algorithms that vary in their approaches to the different subtasks of MARL have been developed. However, the theoretical convergence results for these algorithms do not give a clue as to their practical performance nor supply insights to the dynamics of the learning process itself. This work is a comprehensive empirical study conducted on MGS, a simulation system developed for this purpose. It surveys the important algorithms in the ﬁeld, demonstrates the strengths and weaknesses of the different approaches to MARL through application of FriendQ, OAL, WoLF, FoeQ, Rmax, and other algorithms to a variety of fully cooperative and fully competitive domains in self and heterogeneous play, and supplies an informal analysis of the resulting learning processes. The results can aid in the design of new learning algorithms, in matching existing algorithms to speciﬁc tasks, and may guide further research and formal analysis of the learning processes. Keywords: reinforcement learning, multi-agent reinforcement learning, stochastic games</p><p>3 0.55886388 <a title="90-lsi-3" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>Author: Faustino Gomez, Jürgen Schmidhuber, Risto Miikkulainen</p><p>Abstract: Many complex control problems require sophisticated solutions that are not amenable to traditional controller design. Not only is it difﬁcult to model real world systems, but often it is unclear what kind of behavior is required to solve the task. Reinforcement learning (RL) approaches have made progress by using direct interaction with the task environment, but have so far not scaled well to large state spaces and environments that are not fully observable. In recent years, neuroevolution, the artiﬁcial evolution of neural networks, has had remarkable success in tasks that exhibit these two properties. In this paper, we compare a neuroevolution method called Cooperative Synapse Neuroevolution (CoSyNE), that uses cooperative coevolution at the level of individual synaptic weights, to a broad range of reinforcement learning algorithms on very difﬁcult versions of the pole balancing problem that involve large (continuous) state spaces and hidden state. CoSyNE is shown to be signiﬁcantly more efﬁcient and powerful than the other methods on these tasks. Keywords: coevolution, recurrent neural networks, non-linear control, genetic algorithms, experimental comparison</p><p>4 0.29796574 <a title="90-lsi-4" href="./jmlr-2008-Learning_Control_Knowledge_for_Forward_Search_Planning.html">49 jmlr-2008-Learning Control Knowledge for Forward Search Planning</a></p>
<p>Author: Sungwook Yoon, Alan Fern, Robert Givan</p><p>Abstract: A number of today’s state-of-the-art planners are based on forward state-space search. The impressive performance can be attributed to progress in computing domain independent heuristics that perform well across many domains. However, it is easy to ﬁnd domains where such heuristics provide poor guidance, leading to planning failure. Motivated by such failures, the focus of this paper is to investigate mechanisms for learning domain-speciﬁc knowledge to better control forward search in a given domain. While there has been a large body of work on inductive learning of control knowledge for AI planning, there is a void of work aimed at forward-state-space search. One reason for this may be that it is challenging to specify a knowledge representation for compactly representing important concepts across a wide range of domains. One of the main contributions of this work is to introduce a novel feature space for representing such control knowledge. The key idea is to deﬁne features in terms of information computed via relaxed plan extraction, which has been a major source of success for non-learning planners. This gives a new way of leveraging relaxed planning techniques in the context of learning. Using this feature space, we describe three forms of control knowledge—reactive policies (decision list rules and measures of progress) and linear heuristics—and show how to learn them and incorporate them into forward state-space search. Our empirical results show that our approaches are able to surpass state-of-the-art nonlearning planners across a wide range of planning competition domains. Keywords: planning, machine learning, knowledge representation, search</p><p>5 0.24568911 <a title="90-lsi-5" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: We consider the least-square regression problem with regularization by a block 1 -norm, that is, a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1 -norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic group selection consistency of the group Lasso. We derive necessary and sufﬁcient conditions for the consistency of group Lasso under practical assumptions, such as model misspeciﬁcation. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this inﬁnite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisﬁed. Keywords: sparsity, regularization, consistency, convex optimization, covariance operators</p><p>6 0.22647092 <a title="90-lsi-6" href="./jmlr-2008-Comments_on_the_Complete_Characterization_of_a_Family_of_Solutions_to_a_GeneralizedFisherCriterion.html">23 jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</a></p>
<p>7 0.17821464 <a title="90-lsi-7" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>8 0.14106101 <a title="90-lsi-8" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>9 0.12988245 <a title="90-lsi-9" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>10 0.12797584 <a title="90-lsi-10" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>11 0.11861094 <a title="90-lsi-11" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>12 0.10856151 <a title="90-lsi-12" href="./jmlr-2008-Shark%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">85 jmlr-2008-Shark    (Machine Learning Open Source Software Paper)</a></p>
<p>13 0.10606307 <a title="90-lsi-13" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>14 0.10192873 <a title="90-lsi-14" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>15 0.094817966 <a title="90-lsi-15" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>16 0.093945652 <a title="90-lsi-16" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>17 0.093582436 <a title="90-lsi-17" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>18 0.092961133 <a title="90-lsi-18" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>19 0.091864854 <a title="90-lsi-19" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>20 0.088986233 <a title="90-lsi-20" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.014), (5, 0.01), (9, 0.011), (24, 0.498), (31, 0.014), (40, 0.029), (51, 0.015), (54, 0.05), (58, 0.038), (66, 0.037), (76, 0.017), (78, 0.015), (88, 0.083), (92, 0.025), (94, 0.036), (99, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81395918 <a title="90-lda-1" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>Author: Liviu Panait, Karl Tuyls, Sean Luke</p><p>Abstract: This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the beneﬁts of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction</p><p>2 0.60377806 <a title="90-lda-2" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>Author: Thomas G. Dietterich, Guohua Hao, Adam Ashenfelter</p><p>Abstract: Conditional random ﬁelds (CRFs) provide a ﬂexible and powerful model for sequence labeling problems. However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features and feature combinations. This paper describes a new algorithm for training CRFs via gradient tree boosting. In tree boosting, the CRF potential functions are represented as weighted sums of regression trees, which provide compact representations of feature interactions. So the algorithm does not explicitly consider the potentially large parameter space. As a result, gradient tree boosting scales linearly in the order of the Markov model and in the order of the feature interactions, rather than exponentially as in previous algorithms based on iterative scaling and gradient descent. Gradient tree boosting also makes it possible to use instance weighting (as in C4.5) and surrogate splitting (as in CART) to handle missing values. Experimental studies of the effectiveness of these two methods (as well as standard imputation and indicator feature methods) show that instance weighting is the best method in most cases when feature values are missing at random. Keywords: sequential supervised learning, conditional random ﬁelds, functional gradient, gradient tree boosting, missing values</p><p>3 0.27438346 <a title="90-lda-3" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>Author: Faustino Gomez, Jürgen Schmidhuber, Risto Miikkulainen</p><p>Abstract: Many complex control problems require sophisticated solutions that are not amenable to traditional controller design. Not only is it difﬁcult to model real world systems, but often it is unclear what kind of behavior is required to solve the task. Reinforcement learning (RL) approaches have made progress by using direct interaction with the task environment, but have so far not scaled well to large state spaces and environments that are not fully observable. In recent years, neuroevolution, the artiﬁcial evolution of neural networks, has had remarkable success in tasks that exhibit these two properties. In this paper, we compare a neuroevolution method called Cooperative Synapse Neuroevolution (CoSyNE), that uses cooperative coevolution at the level of individual synaptic weights, to a broad range of reinforcement learning algorithms on very difﬁcult versions of the pole balancing problem that involve large (continuous) state spaces and hidden state. CoSyNE is shown to be signiﬁcantly more efﬁcient and powerful than the other methods on these tasks. Keywords: coevolution, recurrent neural networks, non-linear control, genetic algorithms, experimental comparison</p><p>4 0.23490615 <a title="90-lda-4" href="./jmlr-2008-Multi-Agent_Reinforcement_Learning_in_Common_Interest_and_Fixed_Sum_Stochastic_Games%3A_An_Experimental_Study.html">65 jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</a></p>
<p>Author: Avraham Bab, Ronen I. Brafman</p><p>Abstract: Multi Agent Reinforcement Learning (MARL) has received continually growing attention in the past decade. Many algorithms that vary in their approaches to the different subtasks of MARL have been developed. However, the theoretical convergence results for these algorithms do not give a clue as to their practical performance nor supply insights to the dynamics of the learning process itself. This work is a comprehensive empirical study conducted on MGS, a simulation system developed for this purpose. It surveys the important algorithms in the ﬁeld, demonstrates the strengths and weaknesses of the different approaches to MARL through application of FriendQ, OAL, WoLF, FoeQ, Rmax, and other algorithms to a variety of fully cooperative and fully competitive domains in self and heterogeneous play, and supplies an informal analysis of the resulting learning processes. The results can aid in the design of new learning algorithms, in matching existing algorithms to speciﬁc tasks, and may guide further research and formal analysis of the learning processes. Keywords: reinforcement learning, multi-agent reinforcement learning, stochastic games</p><p>5 0.22372958 <a title="90-lda-5" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>Author: Hsuan-Tien Lin, Ling Li</p><p>Abstract: Ensemble learning algorithms such as boosting can achieve better performance by averaging over the predictions of some base hypotheses. Nevertheless, most existing algorithms are limited to combining only a ﬁnite number of hypotheses, and the generated ensemble is usually sparse. Thus, it is not clear whether we should construct an ensemble classiﬁer with a larger or even an inﬁnite number of hypotheses. In addition, constructing an inﬁnite ensemble itself is a challenging task. In this paper, we formulate an inﬁnite ensemble learning framework based on the support vector machine (SVM). The framework can output an inﬁnite and nonsparse ensemble through embedding inﬁnitely many hypotheses into an SVM kernel. We use the framework to derive two novel kernels, the stump kernel and the perceptron kernel. The stump kernel embodies inﬁnitely many decision stumps, and the perceptron kernel embodies inﬁnitely many perceptrons. We also show that the Laplacian radial basis function kernel embodies inﬁnitely many decision trees, and can thus be explained through inﬁnite ensemble learning. Experimental results show that SVM with these kernels is superior to boosting with the same base hypothesis set. In addition, SVM with the stump kernel or the perceptron kernel performs similarly to SVM with the Gaussian radial basis function kernel, but enjoys the beneﬁt of faster parameter selection. These properties make the novel kernels favorable choices in practice. Keywords: ensemble learning, boosting, support vector machine, kernel</p><p>6 0.22331645 <a title="90-lda-6" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>7 0.22213189 <a title="90-lda-7" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>8 0.21849594 <a title="90-lda-8" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>9 0.21759708 <a title="90-lda-9" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>10 0.21645421 <a title="90-lda-10" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>11 0.21585537 <a title="90-lda-11" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>12 0.21547656 <a title="90-lda-12" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>13 0.21523808 <a title="90-lda-13" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>14 0.21522769 <a title="90-lda-14" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>15 0.21390975 <a title="90-lda-15" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>16 0.21311161 <a title="90-lda-16" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>17 0.21261972 <a title="90-lda-17" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>18 0.21201593 <a title="90-lda-18" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>19 0.21071009 <a title="90-lda-19" href="./jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">53 jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>20 0.21024106 <a title="90-lda-20" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
