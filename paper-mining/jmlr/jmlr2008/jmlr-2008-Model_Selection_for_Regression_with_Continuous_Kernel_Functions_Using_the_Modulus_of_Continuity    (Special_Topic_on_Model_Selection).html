<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-63" href="#">jmlr2008-63</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2008-63-pdf" href="http://jmlr.org/papers/volume9/koo08b/koo08b.pdf">pdf</a></p><p>Author: Imhoi Koo, Rhee Man Kil</p><p>Abstract: This paper presents a new method of model selection for regression problems using the modulus of continuity. For this purpose, we suggest the prediction risk bounds of regression models using the modulus of continuity which can be interpreted as the complexity of functions. We also present the model selection criterion referred to as the modulus of continuity information criterion (MCIC) which is derived from the suggested prediction risk bounds. The suggested MCIC provides a risk estimate using the modulus of continuity for a trained regression model (or an estimation function) while other model selection criteria such as the AIC and BIC use structural information such as the number of training parameters. As a result, the suggested MCIC is able to discriminate the performances of trained regression models, even with the same structure of training models. To show the effectiveness of the proposed method, the simulation for function approximation using the multilayer perceptrons (MLPs) was conducted. Through the simulation for function approximation, it was demonstrated that the suggested MCIC provides a good selection tool for nonlinear regression models, even with the limited size of data. Keywords: regression models, multilayer perceptrons, model selection, information criteria, modulus of continuity</p><p>Reference: <a title="jmlr-2008-63-reference" href="../jmlr2008_reference/jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 KR  Department of Mathematical Sciences Korea Advanced Institute of Science and Technology Daejeon 305-701, Korea  Editors: Isabelle Guyon and Amir Saffari  Abstract This paper presents a new method of model selection for regression problems using the modulus of continuity. [sent-6, score-0.541]
</p><p>2 For this purpose, we suggest the prediction risk bounds of regression models using the modulus of continuity which can be interpreted as the complexity of functions. [sent-7, score-0.936]
</p><p>3 We also present the model selection criterion referred to as the modulus of continuity information criterion (MCIC) which is derived from the suggested prediction risk bounds. [sent-8, score-0.988]
</p><p>4 The suggested MCIC provides a risk estimate using the modulus of continuity for a trained regression model (or an estimation function) while other model selection criteria such as the AIC and BIC use structural information such as the number of training parameters. [sent-9, score-1.118]
</p><p>5 Keywords: regression models, multilayer perceptrons, model selection, information criteria, modulus of continuity  1. [sent-13, score-0.783]
</p><p>6 However, the proper network size (or number of parameters) of a regression model is difﬁcult to choose, as it is possible to obtain the empirical risk only in the case of the limited size of data while the expected risk of a regression model should be measured for the entire data distribution. [sent-18, score-0.475]
</p><p>7 In this context, we consider the bounds on the expected risks using the modulus of continuity representing a measure of the continuity for the given function. [sent-39, score-1.062]
</p><p>8 Lorentz (1986) applied the modulus of continuity to function approximation theories. [sent-40, score-0.681]
</p><p>9 Section 3 describes the suggested model selection method referred to as the MCIC method starting from the deﬁnition of the modulus of continuity for continuous functions. [sent-47, score-0.817]
</p><p>10 We also describe how we can estimate the modulus of continuity for the regression models with different type of kernel functions. [sent-48, score-0.765]
</p><p>11 In general, an estimation function fn can be constructed as a linear combination of kernel functions; that is, n  fn (x) =  ∑ wk φk (x),  (4)  k=1  where wk and φk represent the kth weight value and kernel function, respectively. [sent-62, score-1.308]
</p><p>12 Rather, we usually ﬁnd f n by minimizing the empirical risk Remp ( fn ) evaluated by the mean of loss function values for the given samples; that is, Remp ( fn ) =  1 N ∑ L(yi , fn (xi )). [sent-64, score-1.969]
</p><p>13 In statistical regression models, a popular criterion is the Akaike information criterion (AIC), in which an estimate of the expected risk is given by AIC( fn ) = Remp ( fn ) + 2 · 2609  n 2 σ N ε  (6)  KOO AND K IL  ˆε under the assumption that the noise term ε has a normal distribution. [sent-68, score-1.491]
</p><p>14 Here, the noise term σ2 can be estimated by RSS ˆε σ2 = , (7) N − DoF( fn )  where RSS represents the sum of the square error over the training samples; that is, RSS = NR emp ( fn ), and DoF( f n ) represents the degree of freedom of an estimation function f n . [sent-69, score-1.325]
</p><p>15 As an alternative to this criterion, the Bayesian approach to model selection referred to as the Bayesian information criterion (BIC) can be considered: BIC( fn ) = Remp ( fn ) + log N ·  n 2 σ . [sent-71, score-1.285]
</p><p>16 In this method, for the regression model f n and the data D, the description length l( f n , D) is described by l( fn , D) = l(D| fn ) + l( fn ), where l( fn ) represents the length of the regression model and l(D| f n ) represents the length of the data given the regression model. [sent-76, score-2.738]
</p><p>17 According to Shannon’s information theory, the description length in number of bits is then described by l( fn , D) = − log2 p(D| fn ) − log2 p( fn ), where p( f n |D) represents the probability of the output data given the regression model and p( f n ) represents a priori model probability. [sent-77, score-1.976]
</p><p>18 with a normal distribution, the description length of the regression model (Cohen and Intrator, 2004) can be described by MDL( fn ) = log Remp ( fn ) +  n N  log(2π) + log(  1 n 2 ∑ w )+1 . [sent-82, score-1.315]
</p><p>19 n k=1 k  (9)  This formula for the MDL shows that the description length of the regression model is composed of the empirical risk and the complexity term, which is mainly dependent upon the ratio of the number of parameters to the number of samples and the mean square of weight values. [sent-83, score-0.409]
</p><p>20 The structural risk minimization (SRM) principle considers both the empirical risk and the complexity of the regression model to decide the optimal structure of the regression model. [sent-91, score-0.451]
</p><p>21 1: R( fn )  Remp ( fn )TSEB (n, N),  where TSEB (n, l) = K=  1−  (11)  1 + n/(NK) and 1 − (n/N)  n(1 + ln(2N/n)) + 4 N  . [sent-96, score-1.196]
</p><p>22 In this context, we consider to use the modulus of continuity representing a measure of continuity for the given function. [sent-104, score-0.95]
</p><p>23 From this result, a model selection criterion referred to as the modulus of continuity information criterion (MCIC) is suggested and it is applied to the selection of nonlinear regression models. [sent-106, score-0.99]
</p><p>24 Model Selection Criteria Based on the Modulus of Continuity For the description of the bounds on expected risks, the modulus of continuity deﬁned for continuous functions is used. [sent-109, score-0.774]
</p><p>25 In this section, starting from the deﬁnition of the modulus of continuity, the bounds on expected risks are described and the model selection criterion referred to as the MCIC using the described bounds is suggested. [sent-110, score-0.634]
</p><p>26 1 The Modulus of Continuity for Continuous Functions The modulus of continuity is a measure of continuity for continuous functions. [sent-112, score-0.966]
</p><p>27 This modulus of continuity of f has the following properties: • ω( f , h) is continuous at h for each f , • ω( f , h) is positive and increases as h increases, and • ω( f , h) is sub-additive; that is, ω( f , h1 + h2 ) and h2 . [sent-116, score-0.697]
</p><p>28 ω( f , h1 ) + ω( f , h2 ) for positive constants h1  As a function of f , the modulus of continuity has the following properties of a semi-norm: ω(a f , h) ω( f1 + f2 , h)  |a|ω( f , h) for a constant a and  ω( f1 , h) + ω( f2 , h) for f1 and f2 ∈ C(X). [sent-117, score-0.681]
</p><p>29 One famous example of the modulus of continuity of a function f is that f is deﬁned on A = [a, b] (b > a) and satisﬁes a Lipschitz condition with the constant M 0 and the exponent α (0 < α 1), denoted by LipM α; that is, | f (a1 ) − f (a2 )|  M|a1 − a2 |α , a1 , a2 ∈ A. [sent-118, score-0.681]
</p><p>30 In this case, the modulus of continuity is given by ω( f , h)  Mhα . [sent-119, score-0.681]
</p><p>31 In the multi-dimensional input spaces; that is, X ⊂ Rm (m > 1), there are different deﬁnitions of the modulus of continuity for a continuous function f . [sent-120, score-0.697]
</p><p>32 The following two deﬁnitions of the modulus of continuity are considered (Lorentz, 1986; Anastassiou and Gal, 2000): Deﬁnition 1 Let m = 2 and X ⊂ Rm . [sent-121, score-0.681]
</p><p>33 • Then, the modulus of continuity for f ∈ C(X) is deﬁned by ωA ( f , h) = sup {| f (x1 , y1 ) − f (x2 , y2 )|} subject to  (x1 , y1 ), (x2 , y2 ) ∈ X and (x1 , y1 ) − (x2 , y2 ) 2 h, for h > 0. [sent-122, score-0.681]
</p><p>34 2612  M ODEL S ELECTION FOR R EGRESSION  • Another deﬁnition of the modulus of continuity is ωB ( f , α, β) = sup  | f (x1 , y) − f (x2 , y)|, | f (x, y1 ) − f (x, y2 )|  (13)  (x1 , y), (x2 , y), (x, y1 ), (x, y2 ) ∈ X and |x1 − x2 | α, |y1 − y2 | β, for α, β > 0. [sent-123, score-0.681]
</p><p>35 subject to  For f ∈ C(X) on a compact subset X ⊂ Rm , where m > 2, it is possible to deﬁne the modulus of continuity by induction. [sent-124, score-0.694]
</p><p>36 The main difference in these two deﬁnitions of the modulus of continuity is the direction. [sent-125, score-0.681]
</p><p>37 Furthermore, each deﬁnition of the modulus of continuity has the following upper bound: Lemma 2 Let f ∈ C1 (X), the class of continuous functions having continuous 1st derivative on X, a compact subset of Rm , m > 1. [sent-130, score-0.742]
</p><p>38 From this lemma, the second deﬁnition of the modulus of continuity wB ( f , h, h) was chosen because it has a smaller upper bound compared to the ﬁrst modulus of continuity. [sent-134, score-1.093]
</p><p>39 The computation of the modulus of continuity requires the value of h. [sent-136, score-0.681]
</p><p>40 Let us also consider a point x0 ∈ X such that x0 = arg max | f (x) − fn (x)|. [sent-138, score-0.617]
</p><p>41 This range of h is considered to describe the modulus of continuity for the target and estimation functions. [sent-140, score-0.787]
</p><p>42 2 Risk Bounds Based on the Modulus of Continuity In this subsection, the modulus of continuity for the target and estimation functions are investigated, and the manner in which they are related to the expected risks is considered. [sent-142, score-0.894]
</p><p>43 First, let us consider the loss function for the observed model y and the estimation function f n (x) with n parameters as L(y, fn ) = |y − fn (x)|. [sent-143, score-1.263]
</p><p>44 Then, the expected and the empirical risks are deﬁned by the following L 1 measure: Z R( fn )L1 = |y − fn (x)|dP(x, y) and X×R  Remp ( fn )L1 =  1 N ∑ |yi − fn (xi )|. [sent-144, score-2.498]
</p><p>45 Then, the following inequality holds with a probability of at least 1 − 2δ: R( fn )L1  3Remp ( fn )L1 +  2 max{λi } + (ω( fn , h0 ) +C) N  2 1 ln , 2N δ  (16)  where λi represents the ith eigenvalue of the matrix Hy . [sent-154, score-1.907]
</p><p>46 Rather, the coefﬁcient ratio between the empirical risk and modulus of continuity terms plays an important role for model selection problems because only these two terms are mainly dependent upon the estimation function f n . [sent-160, score-0.972]
</p><p>47 From this point of view, the following model selection criterion referred to as the modulus of continuity information criterion (MCIC) is suggested: MCIC( fn ) = Remp ( fn )L1 +  ω( fn , h0 ) 3  1 2 ln . [sent-161, score-2.634]
</p><p>48 Then, as the number of parameters n increases, the empirical risk Remp ( fn ) decreases while the modulus of continuity ω( f n , h0 ) increases, as the estimation function f n becomes a more complex function. [sent-163, score-1.475]
</p><p>49 The following theorem of the prediction risk bounds for multivariate continuous functions is suggested using the deﬁnition of the modulus of continuity (13): Theorem 2 Let f ∈ C1 (X) with X, a compact subset of Rm (m > 1), and h0 be a constant satisfying (14). [sent-168, score-0.938]
</p><p>50 In this theorem, w( f − f n , h0 ) can be replaced with w( f , h0 ) + w( fn , h0 ); that is, the sum of the modulus of continuity for the target and estimation functions because the following inequalities always hold: ω( fn , h0 ) − ω( f , h0 )  ω( f − fn , h0 )  ω( fn , h0 ) + ω( f , h0 ). [sent-172, score-3.195]
</p><p>51 The suggested theorem states that the expected risk is mainly bounded by the empirical risk R emp ( fn ), the modulus of continuity for the target function w( f , h0 ), and also the modulus of continuity for the estimation function w( f n , h0 ). [sent-173, score-2.445]
</p><p>52 As the number of parameters n varies, the empirical risk, the modulus of continuity for the estimation function, and the term | f (x i0 ) − fn (xi0 )| are changed while other terms remain constant. [sent-174, score-1.342]
</p><p>53 In this case, the effect of the term | f (xi0 ) − fn (xi0 )| is small compared with the other two terms, as the regression model becomes well ﬁtted to the samples as the number of parameters n increases. [sent-176, score-0.749]
</p><p>54 Summarizing the properties of the suggested MCIC, the distinct characteristics are described as follows: • The suggested MCIC is dependent upon the modulus of continuity for the trained estimation function. [sent-179, score-0.9]
</p><p>55 For the computation of the suggested MCIC, the modulus of continuity of the trained estimation function should be evaluated, as explained in the next subsection. [sent-182, score-0.809]
</p><p>56 3 The Modulus of Continuity for Estimation Functions The modulus of continuity for the estimation function w( f n , h) is dependent upon the basis function φk in (4). [sent-184, score-0.762]
</p><p>57 2616  M ODEL S ELECTION FOR R EGRESSION  Therefore, the modulus of continuity for f n has the following upper bound: n  ∑ kh|wk | · max  ω( fn , h)  |a|k−1 , |b|k−1 . [sent-187, score-1.298]
</p><p>58 2  Therefore, the modulus of continuity for f n has the following upper bound: ω( fn , h)  n  ∑ h|wk | ·  k=0  k . [sent-190, score-1.279]
</p><p>59 2  • A case of the estimation function f n with sigmoid function φk (x) = φk (x1 , · · · , xm ) on X ⊂ Rm : n  fn (x) =  ∑ wk φk (x1 , · · · , xm ) + w0 ,  k=1  where m  ∑ vk j x j + vk0  φk (x1 , · · · , xm ) = tanh  . [sent-191, score-0.754]
</p><p>60 j=1  Applying the mean value theorem to φk with respect to each coordinate x1 , · · · , xm , we get | fn (· · · , x j , · · · ) − fn (· · · , x j − h, · · · )|  h·  ∂ fn ∂x j  ∞  for j = 1, · · · , m. [sent-192, score-1.835]
</p><p>61 Therefore, the modulus of continuity for f n has the following upper bound: ω( fn , h)  h · max  1 j m  ∂ fn ∂x j  ∞  n  h · max  1 j m  h · max  1 j m  ∑ wk v k j ·  k=1 n  ∑  wk v k j . [sent-193, score-1.998]
</p><p>62 k=1  2617  1 − tanh2  m  ∑ vki xi + vk0  i=1  ∞  (18)  KOO AND K IL  As shown in these examples, the modulus of continuity for the estimation function f n is dependent upon the trained parameter values associated with f n and h0 whose range is given by (14). [sent-194, score-0.833]
</p><p>63 After the value of h0 is determined, the computation of the modulus of continuity requires access to all the parameter values of f n which are obtained after the learning of training samples. [sent-199, score-0.681]
</p><p>64 In this context, the computational complexity of the modulus of continuity is proportional to the number of parameters n in the estimation function, that is, in big-O notation, O(n). [sent-200, score-0.746]
</p><p>65 Once the modulus of continuity is determined, the MCIC can be determined by (17). [sent-203, score-0.681]
</p><p>66 Here, n is selected such that n = arg min MCIC( fn ). [sent-205, score-0.598]
</p><p>67 Once the MLP was trained, the empirical risk R emp ( fn ) evaluated by the training samples was obtained, and the estimated risk R( fn ) value could then be determined by the AIC, BIC, MDL, and MCIC-based methods. [sent-237, score-1.56]
</p><p>68 of MCIC for MLPs using the modulus of continuity described as (18): MCIC( fn ) = Remp ( fn )L1 +  n h0 max ∑ wk vk j 3 1 j m k=1  1 2 ln , 2N δ  (20)  where h0 was set to the half of the average distance between two adjacent samples using (19) and δ was set to 0. [sent-254, score-2.05]
</p><p>69 To compare the performance of the model selection methods, the risks for the selected f n were evaluated by the test samples and the results were compared with the minimum risk among all risks for fn , n = 1, · · · , 50. [sent-257, score-0.996]
</p><p>70 Quantitatively, the log ratio rR of two risks Rtest ( fn ) and minn R( fn ) were computed: Rtest ( fn ) rR = log , (21) minn Rtest ( fn ) where Rtest represents the empirical risk for the squared error loss function evaluated by the test samples. [sent-258, score-2.631]
</p><p>71 This is mainly due to the fact that in the MCIC method, the modulus of continuity, which can be interpreted as the complexity of the estimation function was computed for each trained estimation function directly. [sent-359, score-0.547]
</p><p>72 This is mainly due to the fact that the complexity term as a form of the modulus of continuity of the trained regression model provides high inﬂuence on selecting the regression model especially for smaller number of samples. [sent-495, score-0.892]
</p><p>73 In this estimation function, the variation of the predicted values for the unobserved data with respect to the function values for the known data (or training samples) can be described by the modulus of continuity, as presented in the deﬁnition of the modulus of continuity. [sent-497, score-0.887]
</p><p>74 Then, in the suggested MCIC, this makes high inﬂuence of the modulus of continuity compared to the empirical risk which usually has a small value. [sent-499, score-0.887]
</p><p>75 Conclusion We have suggested a new method of model selection in regression problems based on the modulus of continuity. [sent-504, score-0.599]
</p><p>76 The prediction risk bounds are investigated from a view point of the modulus of continuity for the target and estimation functions. [sent-505, score-0.941]
</p><p>77 Through the simulation for function approximation using the MLPs, it was shown that the model selection method using the suggested MCIC has the advantages of risk ratio performances over other model selection methods such as the AIC, BIC, and MDL methods in various situations of benchmark data. [sent-592, score-0.423]
</p><p>78 For regression models with other types of estimation functions that have some smoothness constraints, the suggested MCIC method can be easily extended to the given regression models by evaluating the modulus of continuity for the corresponding estimation functions. [sent-595, score-1.019]
</p><p>79 Then, |x1 − x2 | of the modulus of continuity, we obtain  h and |y1 − y2 |  ωA ( f , h)  h. [sent-607, score-0.412]
</p><p>80 Then, from the Hoeffding inequality, the probability of an event A is bounded by Pr{A}  exp maxx∈X  1 N  −2ε2 N  ∑N | fn (x) − fn (xi )| i=1  2  . [sent-632, score-1.21]
</p><p>81 For the denominator in the argument of the exponent, we can consider the following inequality: max x∈X  1 N ∑ | fn (x) − fn (xi )| N i=1  1 N ∑ max | fn (x) − fn (xi )| N i=1 x∈X max max | fn (x) − fn (xi )|. [sent-633, score-3.664]
</p><p>82 i  x∈X  Let xi = arg maxx∈X | fn (x) − fn (xi )|, xi = arg min j d(x j , xi ), and hi = d(xi , xi ) where d(x, y) represents the distance measure deﬁned by d(x, y) = |x − y|. [sent-634, score-1.439]
</p><p>83 Then, max x∈X  1 N ∑ | fn (x) − fn (xi )| N i=1  max | fn (xi ) − fn (xi )| + | fn (xi ) − fn (xi )| i  max (ω( fn , hi ) + | fn (xi ) − fn (xi )|) i  ω( fn , h0 ) + | fn (x0 ) − fn (x0 )|,  where h0 ∈ {h1 , . [sent-635, score-7.258]
</p><p>84 , xN } satisfy ω( fn , hi ) + | fn (xi ) − fn (x j )|  ω( fn , h0 ) + | fn (x0 ) − fn (x0 )| for i, j = 1, · · · , N. [sent-641, score-3.613]
</p><p>85 Thus, the probability of an event A is bounded by  Pr{A}  exp  −2ε2 N (ω( fn , h0 ) + | fn (x0 ) − fn (x0 )|)2  . [sent-645, score-1.808]
</p><p>86 Here, let us set δ1 −2ε2 N = exp 2 (ω( fn , h0 ) + | fn (x0 ) − fn (x0 )|)2  . [sent-646, score-1.794]
</p><p>87 Then, with a probability of at least 1 − δ1 /2, we have 1 N ∑ N i=1  Z  X  | fn (x) − fn (xi )|dP(x)  1 N | fn (xi ) − fn (x j )| N 2 i,∑ j=1 +  2 1 ω( fn , h0 ) + | fn (x0 ) − fn (x0 )| . [sent-647, score-4.186]
</p><p>88 2 Similar to (24), the difference between the expected and empirical risks of |y − f n (x)| is bounded by > 1−  Z  |y − fn (x)|dP(x, y) −  X×R  1 N ∑ |yi − fn (xi )| N i=1 1 N ∑ N i=1  Z  X×R  |y − yi | + | fn (x) − fn (xi )|dP(x, y). [sent-669, score-2.522]
</p><p>89 Then, ﬁnally, from (22) and (26), with a probability of at least 1 − 2δ Z  X×R  |y − fn (x)|dP(x, y) −  1 N ∑ |yi − fn (xi )| N i=1  1 N (|y j − yi | + | fn (x j ) − fn (xi )|) N 2 i,∑ j=1 + (ω( fn , h0 ) +C) where C = | f n (x0 ) − fn (y0 )| + 2 f  ∞ + 2σε  1/δ. [sent-672, score-3.612]
</p><p>90 i N i,∑ i=1 j=1 Now, let us consider the following inequality: 1 N | fn (xi ) − fn (x j )| N 2 i,∑ j=1  1 N | fn (xi ) − yi | N 2 i,∑ j=1 +  1 N 1 N |yi − y j | + 2 ∑ |y j − fn (x j )| N 2 i,∑ N i, j=1 j=1  = 2Remp ( fn )L1 + 2Remp ( fn )L1 +  1 N |yi − y j | N 2 i,∑ j=1 1 max{λi }. [sent-678, score-3.612]
</p><p>91 N i  Therefore, from the above inequality and (27), the following inequality holds with a probability of at least 1 − 2δ: R( fn )  3Remp ( fn ) +  2 max{λi } + (ω( fn , h0 ) +C) N i  1 2 ln . [sent-679, score-1.897]
</p><p>92 The probability that the difference between the expected and empirical risks is larger than a positive constant ε can be described by Pr {R( fn )L1 − Remp ( fn )L1 > ε}  −2ε2 N (maxx∈X |y − fn (x)|)2  exp  (30)  from the Hoeffding inequality (Hoeffding, 1963). [sent-685, score-1.93]
</p><p>93 Here, there exist x 0 ∈ X and xi0 ∈ {x0 , · · · , xN } such that x0 = arg max | f (x) − fn (x)| and d(xi0 , x0 ) h0 x∈X  because f − f n ∈ C(X) and X is a compact subset of Rm . [sent-686, score-0.63]
</p><p>94 Thus, from the dominator term of the righthand side of (30), we have max | f (x) − fn (x)| x∈X  | f (x0 ) − fn (x0 ) − f (xi0 ) + fn (xi0 )| + | f (xi0 ) − fn (xi0 )| ω( f − fn , h0 ) + | f (xi0 ) − fn (xi0 )|. [sent-687, score-3.607]
</p><p>95 (31)  Here, we set the bound on the probability of (30) as exp  −2ε2 N (maxx∈X | f (x) − fn (x)|)2  exp δ . [sent-688, score-0.598]
</p><p>96 2  −2ε2 N (ω( f − fn , h0 ) + | f (xi0 ) − fn (xi0 )|)2 (32)  Therefore, from (30), (31), and (32), the following inequality holds with a probability of at least 1 − δ/2: 1 2 R( fn )L1 Remp ( fn )L1 + {ω( f − fn , h0 ) + | f (xi0 ) − fn (xi0 )|} ln . [sent-689, score-3.661]
</p><p>97 In this case, from the triangular inequality, |y − f n (x)| has the following upper bound: max |y − fn (x)| x∈X  max | f (x) − fn (x)| + |ε|. [sent-692, score-1.234]
</p><p>98 a2 2 2631  (36)  KOO AND K IL  Then, from (31), (35), and (36), the following inequality holds with a probability of at least 1 − δ/2: max |y − fn (x)| x∈X  max | f (x) − fn (x)| + σε x∈X  2 δ 2 . [sent-694, score-1.264]
</p><p>99 δ  ω( f − fn , h0 ) + | f (xi0 ) − fn (xi0 )| + σε  (37)  Therefore, from (33) and (37), the following inequality holds with a probability of at least 1 − δ: R( fn )L1  Remp ( fn )L1 + ω( f − fn , h0 ) + | f (xi0 ) − fn (xi0 )| + σε  2 δ  1 2 ln . [sent-695, score-3.661]
</p><p>100 Nonlinear model selection based on the modulus of continuity. [sent-778, score-0.474]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fn', 0.598), ('mcic', 0.451), ('modulus', 0.412), ('continuity', 0.269), ('aic', 0.167), ('risk', 0.133), ('bic', 0.129), ('mdl', 0.123), ('remp', 0.108), ('koo', 0.096), ('risks', 0.069), ('regression', 0.067), ('samples', 0.065), ('egression', 0.059), ('target', 0.058), ('suggested', 0.058), ('dp', 0.052), ('election', 0.051), ('xi', 0.049), ('doppler', 0.049), ('mlps', 0.049), ('estimation', 0.048), ('simulation', 0.047), ('odel', 0.045), ('selection', 0.043), ('ln', 0.043), ('bumps', 0.042), ('cherkassky', 0.042), ('vc', 0.042), ('il', 0.04), ('performances', 0.037), ('hy', 0.036), ('wk', 0.032), ('pr', 0.032), ('ratios', 0.031), ('inequality', 0.03), ('korea', 0.03), ('abalone', 0.03), ('heavysine', 0.028), ('rtest', 0.028), ('wb', 0.028), ('criteria', 0.028), ('mean', 0.027), ('criterion', 0.027), ('hoeffding', 0.026), ('nonlinear', 0.025), ('hi', 0.025), ('yi', 0.024), ('benchmark', 0.024), ('rm', 0.024), ('expected', 0.022), ('trained', 0.022), ('represents', 0.022), ('bounds', 0.021), ('imhoi', 0.021), ('lorentz', 0.021), ('sigmoid', 0.02), ('model', 0.019), ('noise', 0.019), ('max', 0.019), ('mg', 0.018), ('maxx', 0.018), ('ith', 0.018), ('emp', 0.018), ('rss', 0.018), ('description', 0.018), ('upon', 0.018), ('complexity', 0.017), ('perceptrons', 0.017), ('donoho', 0.017), ('decreased', 0.017), ('models', 0.017), ('univariate', 0.017), ('showed', 0.016), ('bc', 0.016), ('multilayer', 0.016), ('akaike', 0.016), ('polynomials', 0.016), ('ei', 0.016), ('functions', 0.016), ('continuous', 0.016), ('blocks', 0.015), ('ga', 0.015), ('length', 0.015), ('empirical', 0.015), ('variation', 0.015), ('dependent', 0.015), ('xm', 0.014), ('anastassiou', 0.014), ('barron', 0.014), ('daejeon', 0.014), ('dof', 0.014), ('karpinski', 0.014), ('kil', 0.014), ('shattered', 0.014), ('tseb', 0.014), ('xg', 0.014), ('vk', 0.014), ('event', 0.014), ('compact', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="63-tfidf-1" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>Author: Imhoi Koo, Rhee Man Kil</p><p>Abstract: This paper presents a new method of model selection for regression problems using the modulus of continuity. For this purpose, we suggest the prediction risk bounds of regression models using the modulus of continuity which can be interpreted as the complexity of functions. We also present the model selection criterion referred to as the modulus of continuity information criterion (MCIC) which is derived from the suggested prediction risk bounds. The suggested MCIC provides a risk estimate using the modulus of continuity for a trained regression model (or an estimation function) while other model selection criteria such as the AIC and BIC use structural information such as the number of training parameters. As a result, the suggested MCIC is able to discriminate the performances of trained regression models, even with the same structure of training models. To show the effectiveness of the proposed method, the simulation for function approximation using the multilayer perceptrons (MLPs) was conducted. Through the simulation for function approximation, it was demonstrated that the suggested MCIC provides a good selection tool for nonlinear regression models, even with the limited size of data. Keywords: regression models, multilayer perceptrons, model selection, information criteria, modulus of continuity</p><p>2 0.07769306 <a title="63-tfidf-2" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>Author: Rémi Munos, Csaba Szepesvári</p><p>Abstract: In this paper we develop a theoretical analysis of the performance of sampling-based ﬁtted value iteration (FVI) to solve inﬁnite state-space, discounted-reward Markovian decision processes (MDPs) under the assumption that a generative model of the environment is available. Our main results come in the form of ﬁnite-time bounds on the performance of two versions of sampling-based FVI. The convergence rate results obtained allow us to show that both versions of FVI are well behaving in the sense that by using a sufﬁciently large number of samples for a large class of MDPs, arbitrary good performance can be achieved with high probability. An important feature of our proof technique is that it permits the study of weighted L p -norm performance bounds. As a result, our technique applies to a large class of function-approximation methods (e.g., neural networks, adaptive regression trees, kernel machines, locally weighted learning), and our bounds scale well with the effective horizon of the MDP. The bounds show a dependence on the stochastic stability properties of the MDP: they scale with the discounted-average concentrability of the future-state distributions. They also depend on a new measure of the approximation power of the function space, the inherent Bellman residual, which reﬂects how well the function space is “aligned” with the dynamics and rewards of the MDP. The conditions of the main result, as well as the concepts introduced in the analysis, are extensively discussed and compared to previous theoretical results. Numerical experiments are used to substantiate the theoretical ﬁndings. Keywords: ﬁtted value iteration, discounted Markovian decision processes, generative model, reinforcement learning, supervised learning, regression, Pollard’s inequality, statistical learning theory, optimal control</p><p>3 0.068609208 <a title="63-tfidf-3" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>Author: Peter L. Bartlett, Marten H. Wegkamp</p><p>Abstract: We consider the problem of binary classiﬁcation where the classiﬁer can, for a particular cost, choose not to classify an observation. Just as in the conventional classiﬁcation problem, minimization of the sample average of the cost is a difﬁcult optimization problem. As an alternative, we propose the optimization of a certain convex loss function φ, analogous to the hinge loss used in support vector machines (SVMs). Its convexity ensures that the sample average of this surrogate loss can be efﬁciently minimized. We study its statistical properties. We show that minimizing the expected surrogate loss—the φ-risk—also minimizes the risk. We also study the rate at which the φ-risk approaches its minimum value. We show that fast rates are possible when the conditional probability P(Y = 1|X) is unlikely to be close to certain critical values. Keywords: Bayes classiﬁers, classiﬁcation, convex surrogate loss, empirical risk minimization, hinge loss, large margin classiﬁers, margin condition, reject option, support vector machines</p><p>4 0.068267435 <a title="63-tfidf-4" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>Author: Gerda Claeskens, Christophe Croux, Johan Van Kerckhoven</p><p>Abstract: Support vector machines for classiﬁcation have the advantage that the curse of dimensionality is circumvented. It has been shown that a reduction of the dimension of the input space leads to even better results. For this purpose, we propose two information criteria which can be computed directly from the deﬁnition of the support vector machine. We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. We arrive at the same ﬁndings for comparison on some real-world benchmark data sets. Keywords: information criterion, supervised classiﬁcation, support vector machine, variable selection</p><p>5 0.059523698 <a title="63-tfidf-5" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>Author: Sébastien Loustau</p><p>Abstract: This paper investigates statistical performances of Support Vector Machines (SVM) and considers the problem of adaptation to the margin parameter and to complexity. In particular we provide a classiﬁer with no tuning parameter. It is a combination of SVM classiﬁers. Our contribution is two-fold: (1) we propose learning rates for SVM using Sobolev spaces and build a numerically realizable aggregate that converges with same rate; (2) we present practical experiments of this method of aggregation for SVM using both Sobolev spaces and Gaussian kernels. Keywords: classiﬁcation, support vector machines, learning rates, approximation, aggregation of classiﬁers</p><p>6 0.057208572 <a title="63-tfidf-6" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>7 0.053559378 <a title="63-tfidf-7" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>8 0.045295712 <a title="63-tfidf-8" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>9 0.042712409 <a title="63-tfidf-9" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>10 0.042158041 <a title="63-tfidf-10" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>11 0.033583686 <a title="63-tfidf-11" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>12 0.032787636 <a title="63-tfidf-12" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>13 0.028866639 <a title="63-tfidf-13" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>14 0.027128793 <a title="63-tfidf-14" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>15 0.027007913 <a title="63-tfidf-15" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>16 0.026690239 <a title="63-tfidf-16" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>17 0.026030965 <a title="63-tfidf-17" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>18 0.024340987 <a title="63-tfidf-18" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>19 0.022641305 <a title="63-tfidf-19" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>20 0.021540273 <a title="63-tfidf-20" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.136), (1, -0.052), (2, -0.041), (3, -0.025), (4, 0.056), (5, -0.069), (6, 0.018), (7, -0.134), (8, 0.108), (9, -0.002), (10, 0.064), (11, -0.079), (12, 0.036), (13, -0.001), (14, -0.039), (15, -0.057), (16, -0.165), (17, -0.123), (18, -0.047), (19, 0.151), (20, -0.084), (21, 0.283), (22, -0.072), (23, 0.141), (24, -0.178), (25, 0.152), (26, 0.006), (27, -0.05), (28, 0.123), (29, 0.05), (30, 0.18), (31, -0.163), (32, 0.122), (33, -0.044), (34, 0.06), (35, 0.19), (36, 0.201), (37, 0.09), (38, 0.092), (39, 0.109), (40, 0.066), (41, -0.004), (42, -0.095), (43, -0.058), (44, -0.021), (45, -0.08), (46, -0.112), (47, 0.098), (48, -0.214), (49, -0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96919101 <a title="63-lsi-1" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>Author: Imhoi Koo, Rhee Man Kil</p><p>Abstract: This paper presents a new method of model selection for regression problems using the modulus of continuity. For this purpose, we suggest the prediction risk bounds of regression models using the modulus of continuity which can be interpreted as the complexity of functions. We also present the model selection criterion referred to as the modulus of continuity information criterion (MCIC) which is derived from the suggested prediction risk bounds. The suggested MCIC provides a risk estimate using the modulus of continuity for a trained regression model (or an estimation function) while other model selection criteria such as the AIC and BIC use structural information such as the number of training parameters. As a result, the suggested MCIC is able to discriminate the performances of trained regression models, even with the same structure of training models. To show the effectiveness of the proposed method, the simulation for function approximation using the multilayer perceptrons (MLPs) was conducted. Through the simulation for function approximation, it was demonstrated that the suggested MCIC provides a good selection tool for nonlinear regression models, even with the limited size of data. Keywords: regression models, multilayer perceptrons, model selection, information criteria, modulus of continuity</p><p>2 0.56198317 <a title="63-lsi-2" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>Author: Gerda Claeskens, Christophe Croux, Johan Van Kerckhoven</p><p>Abstract: Support vector machines for classiﬁcation have the advantage that the curse of dimensionality is circumvented. It has been shown that a reduction of the dimension of the input space leads to even better results. For this purpose, we propose two information criteria which can be computed directly from the deﬁnition of the support vector machine. We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. We arrive at the same ﬁndings for comparison on some real-world benchmark data sets. Keywords: information criterion, supervised classiﬁcation, support vector machine, variable selection</p><p>3 0.35695711 <a title="63-lsi-3" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>Author: Peter L. Bartlett, Marten H. Wegkamp</p><p>Abstract: We consider the problem of binary classiﬁcation where the classiﬁer can, for a particular cost, choose not to classify an observation. Just as in the conventional classiﬁcation problem, minimization of the sample average of the cost is a difﬁcult optimization problem. As an alternative, we propose the optimization of a certain convex loss function φ, analogous to the hinge loss used in support vector machines (SVMs). Its convexity ensures that the sample average of this surrogate loss can be efﬁciently minimized. We study its statistical properties. We show that minimizing the expected surrogate loss—the φ-risk—also minimizes the risk. We also study the rate at which the φ-risk approaches its minimum value. We show that fast rates are possible when the conditional probability P(Y = 1|X) is unlikely to be close to certain critical values. Keywords: Bayes classiﬁers, classiﬁcation, convex surrogate loss, empirical risk minimization, hinge loss, large margin classiﬁers, margin condition, reject option, support vector machines</p><p>4 0.29988462 <a title="63-lsi-4" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>Author: Rémi Munos, Csaba Szepesvári</p><p>Abstract: In this paper we develop a theoretical analysis of the performance of sampling-based ﬁtted value iteration (FVI) to solve inﬁnite state-space, discounted-reward Markovian decision processes (MDPs) under the assumption that a generative model of the environment is available. Our main results come in the form of ﬁnite-time bounds on the performance of two versions of sampling-based FVI. The convergence rate results obtained allow us to show that both versions of FVI are well behaving in the sense that by using a sufﬁciently large number of samples for a large class of MDPs, arbitrary good performance can be achieved with high probability. An important feature of our proof technique is that it permits the study of weighted L p -norm performance bounds. As a result, our technique applies to a large class of function-approximation methods (e.g., neural networks, adaptive regression trees, kernel machines, locally weighted learning), and our bounds scale well with the effective horizon of the MDP. The bounds show a dependence on the stochastic stability properties of the MDP: they scale with the discounted-average concentrability of the future-state distributions. They also depend on a new measure of the approximation power of the function space, the inherent Bellman residual, which reﬂects how well the function space is “aligned” with the dynamics and rewards of the MDP. The conditions of the main result, as well as the concepts introduced in the analysis, are extensively discussed and compared to previous theoretical results. Numerical experiments are used to substantiate the theoretical ﬁndings. Keywords: ﬁtted value iteration, discounted Markovian decision processes, generative model, reinforcement learning, supervised learning, regression, Pollard’s inequality, statistical learning theory, optimal control</p><p>5 0.27420658 <a title="63-lsi-5" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>Author: Onureena Banerjee, Laurent El Ghaoui, Alexandre d'Aspremont</p><p>Abstract: We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1 -norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our ﬁrst algorithm uses block coordinate descent, and can be interpreted as recursive 1 -norm penalized regression. Our second algorithm, based on Nesterov’s ﬁrst order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data. Keywords: model selection, maximum likelihood estimation, convex optimization, Gaussian graphical model, binary data</p><p>6 0.24880034 <a title="63-lsi-6" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>7 0.22890972 <a title="63-lsi-7" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>8 0.21763688 <a title="63-lsi-8" href="./jmlr-2008-Minimal_Nonlinear_Distortion_Principle_for_Nonlinear_Independent_Component_Analysis.html">60 jmlr-2008-Minimal Nonlinear Distortion Principle for Nonlinear Independent Component Analysis</a></p>
<p>9 0.20881335 <a title="63-lsi-9" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>10 0.18928021 <a title="63-lsi-10" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>11 0.18095522 <a title="63-lsi-11" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>12 0.15845349 <a title="63-lsi-12" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>13 0.14598857 <a title="63-lsi-13" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>14 0.13877535 <a title="63-lsi-14" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>15 0.13787763 <a title="63-lsi-15" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>16 0.13464203 <a title="63-lsi-16" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>17 0.12872566 <a title="63-lsi-17" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<p>18 0.12395026 <a title="63-lsi-18" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>19 0.11708647 <a title="63-lsi-19" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>20 0.11689033 <a title="63-lsi-20" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.033), (9, 0.013), (12, 0.383), (40, 0.036), (54, 0.033), (58, 0.039), (66, 0.065), (76, 0.025), (78, 0.014), (88, 0.085), (92, 0.066), (94, 0.051), (99, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73882043 <a title="63-lda-1" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>Author: Imhoi Koo, Rhee Man Kil</p><p>Abstract: This paper presents a new method of model selection for regression problems using the modulus of continuity. For this purpose, we suggest the prediction risk bounds of regression models using the modulus of continuity which can be interpreted as the complexity of functions. We also present the model selection criterion referred to as the modulus of continuity information criterion (MCIC) which is derived from the suggested prediction risk bounds. The suggested MCIC provides a risk estimate using the modulus of continuity for a trained regression model (or an estimation function) while other model selection criteria such as the AIC and BIC use structural information such as the number of training parameters. As a result, the suggested MCIC is able to discriminate the performances of trained regression models, even with the same structure of training models. To show the effectiveness of the proposed method, the simulation for function approximation using the multilayer perceptrons (MLPs) was conducted. Through the simulation for function approximation, it was demonstrated that the suggested MCIC provides a good selection tool for nonlinear regression models, even with the limited size of data. Keywords: regression models, multilayer perceptrons, model selection, information criteria, modulus of continuity</p><p>2 0.6979391 <a title="63-lda-2" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: We consider the least-square regression problem with regularization by a block 1 -norm, that is, a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1 -norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic group selection consistency of the group Lasso. We derive necessary and sufﬁcient conditions for the consistency of group Lasso under practical assumptions, such as model misspeciﬁcation. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this inﬁnite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisﬁed. Keywords: sparsity, regularization, consistency, convex optimization, covariance operators</p><p>3 0.34205022 <a title="63-lda-3" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>Author: Rémi Munos, Csaba Szepesvári</p><p>Abstract: In this paper we develop a theoretical analysis of the performance of sampling-based ﬁtted value iteration (FVI) to solve inﬁnite state-space, discounted-reward Markovian decision processes (MDPs) under the assumption that a generative model of the environment is available. Our main results come in the form of ﬁnite-time bounds on the performance of two versions of sampling-based FVI. The convergence rate results obtained allow us to show that both versions of FVI are well behaving in the sense that by using a sufﬁciently large number of samples for a large class of MDPs, arbitrary good performance can be achieved with high probability. An important feature of our proof technique is that it permits the study of weighted L p -norm performance bounds. As a result, our technique applies to a large class of function-approximation methods (e.g., neural networks, adaptive regression trees, kernel machines, locally weighted learning), and our bounds scale well with the effective horizon of the MDP. The bounds show a dependence on the stochastic stability properties of the MDP: they scale with the discounted-average concentrability of the future-state distributions. They also depend on a new measure of the approximation power of the function space, the inherent Bellman residual, which reﬂects how well the function space is “aligned” with the dynamics and rewards of the MDP. The conditions of the main result, as well as the concepts introduced in the analysis, are extensively discussed and compared to previous theoretical results. Numerical experiments are used to substantiate the theoretical ﬁndings. Keywords: ﬁtted value iteration, discounted Markovian decision processes, generative model, reinforcement learning, supervised learning, regression, Pollard’s inequality, statistical learning theory, optimal control</p><p>4 0.34116507 <a title="63-lda-4" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>Author: Yair Goldberg, Alon Zakai, Dan Kushnir, Ya'acov Ritov</p><p>Abstract: We analyze the performance of a class of manifold-learning algorithms that ﬁnd their output by minimizing a quadratic form under some normalization constraints. This class consists of Locally Linear Embedding (LLE), Laplacian Eigenmap, Local Tangent Space Alignment (LTSA), Hessian Eigenmaps (HLLE), and Diffusion maps. We present and prove conditions on the manifold that are necessary for the success of the algorithms. Both the ﬁnite sample case and the limit case are analyzed. We show that there are simple manifolds in which the necessary conditions are violated, and hence the algorithms cannot recover the underlying manifolds. Finally, we present numerical results that demonstrate our claims. Keywords: dimensionality reduction, manifold learning, Laplacian eigenmap, diffusion maps, locally linear embedding, local tangent space alignment, Hessian eigenmap</p><p>5 0.34026155 <a title="63-lda-5" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>6 0.33837214 <a title="63-lda-6" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>7 0.33676022 <a title="63-lda-7" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>8 0.33611909 <a title="63-lda-8" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>9 0.33381039 <a title="63-lda-9" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>10 0.33155695 <a title="63-lda-10" href="./jmlr-2008-Classification_with_a_Reject_Option_using_a_Hinge_Loss.html">21 jmlr-2008-Classification with a Reject Option using a Hinge Loss</a></p>
<p>11 0.33134764 <a title="63-lda-11" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>12 0.33114943 <a title="63-lda-12" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>13 0.33051744 <a title="63-lda-13" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>14 0.32918024 <a title="63-lda-14" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>15 0.32898384 <a title="63-lda-15" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>16 0.32798451 <a title="63-lda-16" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>17 0.32702598 <a title="63-lda-17" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>18 0.32695833 <a title="63-lda-18" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>19 0.32329783 <a title="63-lda-19" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>20 0.32085571 <a title="63-lda-20" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
