<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-32" href="#">jmlr2008-32</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</h1>
<br/><p>Source: <a title="jmlr-2008-32-pdf" href="http://jmlr.org/papers/volume9/jiang08a/jiang08a.pdf">pdf</a></p><p>Author: Bo Jiang, Xuegong Zhang, Tianxi Cai</p><p>Abstract: Support vector machine (SVM) is one of the most popular and promising classiﬁcation algorithms. After a classiﬁcation rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples. Keywords: k-fold cross-validation, model evaluation, perturbation-resampling, prediction errors, support vector machine</p><p>Reference: <a title="jmlr-2008-32-reference" href="../jmlr2008_reference/jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. [sent-14, score-0.433]
</p><p>2 Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. [sent-15, score-0.333]
</p><p>3 A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. [sent-16, score-0.459]
</p><p>4 With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. [sent-17, score-0.6]
</p><p>5 After a prediction rule is constructed, the common practice is to provide a point estimate of the corresponding accuracy without accounting for the sampling variability in the estimated accuracy of the prediction rule. [sent-22, score-0.494]
</p><p>6 However, to ensure the reproducibility of the reported results, it is crucial to account for such sampling variability and provide interval estimates for the accuracy measures, especially when the sample size is not large relative to the number of unknown model parameters. [sent-23, score-0.373]
</p><p>7 Various methods have been available to estimate the prediction error of classiﬁers based on the cross-validation and bootstrap methods (Efron, 1986; Efron and Tibshirani, 1997; Fu et al. [sent-24, score-0.387]
</p><p>8 It appears that the polynomial kernel outperforms the linear kernel for both data sets with higher accuracy. [sent-31, score-0.235]
</p><p>9 As such, the point estimates of the accuracy measures may not provide sufﬁcient evidence for determining which type of kernel should be used. [sent-34, score-0.229]
</p><p>10 To adequately assess the accuracy and draw valid conclusions, it is important to account for the sampling variability in the estimated prediction error. [sent-35, score-0.315]
</p><p>11 Some studies have suggested performing hypothesis testing by considering the variability in the cross-validated estimator (Dietterich, 1998). [sent-36, score-0.247]
</p><p>12 Bengio and Grandvalet (2004) and Nadeau and Bengio (2003) pointed out that there exists no universally unbiased estimator of the variance of K-fold cross-validated estimator that is based only on the results of the cross-validation experiments. [sent-37, score-0.336]
</p><p>13 Therefore, the estimation of uncertainty around the prediction error estimators remains a theoretical, as well as practical problem. [sent-38, score-0.269]
</p><p>14 Data Type 1 2  Sample Size 100 100  Linear Kernel Accuracy 94% 92%  Polynomial Kernel Accuracy 95% 96%  Table 1: Kernel selection in SVM classiﬁers based on the cross-validation point estimates for the prediction error. [sent-39, score-0.202]
</p><p>15 In this article, we develop procedures to approximate the distribution of the estimated accuracy measures for SVM classiﬁers and construct conﬁdence intervals for the accuracy measures. [sent-44, score-0.42]
</p><p>16 The proposed method, which may be linked to the weighted bootstrap resampling (Hall and Mammen, 1994; Hall and Maesono, 2000) and the Bayesian bootstrap method (Rubin, 1981), directly builds on the perturbation-resampling procedure considered in Park and Wei (2003) and Cai et al. [sent-45, score-0.746]
</p><p>17 For SVMs with ﬁnite-dimensional kernels, we show that the accuracy measure can be consistently estimated via cross-validation procedures, and the resulting estimators are asymptotically normal. [sent-48, score-0.28]
</p><p>18 A practical perturbation-resampling procedure is proposed to approximate the sampling distribution of the prediction error. [sent-49, score-0.177]
</p><p>19 Numerical studies based on simulated data and a benchmark repository suggest that both the variance estimator and the interval estimator centered at the cross-validated point estimator perform well. [sent-52, score-0.76]
</p><p>20 The proposed procedure is further illustrated with applications in kernel selection and in the genotypic testing for drug resistance. [sent-53, score-0.308]
</p><p>21 To construct an optimal prediction rule, one may consider the prediction function f (X; θ) that minimizes the SVM risk function Q(θ) = E{[1 −Y f (X; θ)]+ } . [sent-60, score-0.246]
</p><p>22 Hence, if there is a kernel function K(·, ·) such that K(Xi , X j ) = Φ(Xi ) Φ(X j ), one may carry out the minimization based on kernel function K(·, ·) only. [sent-73, score-0.188]
</p><p>23 Other examples include the polynomial kernel K(X i , X j ) = (γXi X j + b)d , and the RBF kernel K(Xi , X j ) = exp{− Xi − X j 2 /2σ2 }, with speciﬁed hyper-parameters γ, b, d and σ. [sent-75, score-0.235]
</p><p>24 For each k, we use all observations which are not in Ik to obtain an estimate θ(−k) ˆ for θ via (1), and then compute the prediction error estimate D(k) (θ) via (4) based on observations in Ik . [sent-95, score-0.172]
</p><p>25 Then, the cross-validated prediction error estimator for D 0 is K  ˆ ˆ ˆ D = K −1 ∑ D(k) (θ(−k) ) . [sent-96, score-0.284]
</p><p>26 (5)  k=1  ˆ We show in the next section that the cross-validation estimator D is consistent for estimating the prediction error of SVM classiﬁers under certain conditions. [sent-97, score-0.284]
</p><p>27 However, as we have mentioned above, point estimates are not adequate in drawing valid conclusions, and we need to further study the distributional properties of the estimated prediction error. [sent-98, score-0.254]
</p><p>28 Interval Estimators for the Prediction Error In this section, we provide large sample properties of the estimated prediction error. [sent-100, score-0.175]
</p><p>29 Based on these theoretical results, we present a simple perturbation-resampling procedure to obtain interval estimates for the prediction error. [sent-102, score-0.41]
</p><p>30 (6)  ˆ ˆ Note that although D(θ) is not differentiable with respect to θ, E[D(θ)] is continuously differentiable at θ0 . [sent-110, score-0.189]
</p><p>31 In Theorem 2 of Appendix B, we show that W is asymptotically equivalent to n −1/2 ∑n ηi , i=1 and converges in distribution to a zero mean normal with variance E(η 2 ), where ηi is deﬁned in i (14) of Appendix B. [sent-111, score-0.257]
</p><p>32 Furthermore, we show in Theorem 3 of Appendix C that ˆ W = n1/2 {D − D0 }  (8)  is asymptotically equivalent to W in (6) and thus W also converges in distribution to a zero mean ˆ normal with variance E(η2 ). [sent-116, score-0.257]
</p><p>33 This implies that the cross-validated estimator D , while potentially i ˆ is expected to have the same magnitude of variability has less bias compared to the training error D, ˆ ˆ as that of D. [sent-117, score-0.245]
</p><p>34 Thus, we recommend to construct conﬁdence intervals for D 0 by centering at D with width determined by the variability in W . [sent-118, score-0.281]
</p><p>35 The random variables Gi used in (10) may be linked to the Bayesian bootstrap method (Rubin, 1981) with Gi /(n−1 ∑n Gi ) being the weights instead. [sent-141, score-0.215]
</p><p>36 , n}, we may obtain a large number of realizations of W ∗ which may be used to approximate the distribution of W and construct conﬁdence intervals for D0 . [sent-155, score-0.197]
</p><p>37 , n} from an exponential distribution with unit mean and unit variance 5: Solve the quadratic programming problem (11), and calculate Wr∗ by using (10) 6: end for 7: Estimate the resampling distribution of W ∗ based on {Wr∗ ; r = 1, . [sent-164, score-0.322]
</p><p>38 The theoretical and empirical prediction errors D 0 j and D j (θ j ) are deﬁned by (3) and (4) accordingly, j = 1, 2. [sent-173, score-0.176]
</p><p>39 ˆ Note that even if ∆ is a consistent estimator for the prediction gain ∆, it represents the ﬁtting gain of using Model 2 and may lead to a wrong comparison between models with a large probability. [sent-184, score-0.235]
</p><p>40 By applying the cross-validation procedure, the overﬁtted model is likely to have a larger prediction error and one would choose the more parsimonious model. [sent-185, score-0.172]
</p><p>41 Again, the resampling ∗ distribution of W2 − W1 can be asymptotically approximated by W∆ . [sent-188, score-0.337]
</p><p>42 Numerical Studies and Examples In this section, we examine the ﬁnite-sample performance of the proposed inference procedure via extensive numerical studies based on both simulated data and a benchmark repository. [sent-191, score-0.219]
</p><p>43 Furthermore, we illustrate the new procedure with examples in kernel and biomarker selections. [sent-192, score-0.189]
</p><p>44 The distribution of the empirical absolute prediction is obtained by using perturbation-resampling procedure with 1, 000 times of perturbations (N = 1, 000 in Algorithm 1). [sent-201, score-0.177]
</p><p>45 Conﬁdence interval with nominal level of 95% is then constructed based on empirical percentiles of the resampling distribution. [sent-202, score-0.473]
</p><p>46 To evaluate normal approximation and cross-validation procedures, we also construct conﬁdence intervals based on normal assumption with both the estimated variance and the true variance calculated from the simulation parameters of the samples. [sent-203, score-0.505]
</p><p>47 To evaluate these interval estimates, the true prediction errors of the trained SVM classiﬁers are calculated according to 10, 000 replications of simulated data sets for each setting. [sent-205, score-0.449]
</p><p>48 Conﬁdence intervals are compared with the true prediction error, and their coverage accuracies are obtained by averaging on 1, 000 data sets. [sent-206, score-0.45]
</p><p>49 Coverage accuracy is deﬁned as the frequency for true value to fall inside the estimated conﬁdence interval, which measures the accuracy of interval estimates. [sent-207, score-0.318]
</p><p>50 In the ideal case, the coverage accuracy of an estimated interval should be equal or close to its level of conﬁdence, and with its length as small as possible. [sent-208, score-0.325]
</p><p>51 In Table 2, we report the coverage accuracies and average lengths of 95% conﬁdence intervals centered at 5-fold cross-validation errors for different procedures. [sent-209, score-0.38]
</p><p>52 0  Table 2: Coverage accuracies (CA) and average lengths (AL) of 95% conﬁdence intervals obtained by using different procedures on simulated data. [sent-258, score-0.392]
</p><p>53 As shown in Table 2, at sample size of n = 100, the empirical coverage levels for the 95% conﬁdence intervals under normal approximation with the true variance range from 95. [sent-259, score-0.388]
</p><p>54 In practice, the true variance of the prediction error estimator is unknown and thus the perturbation-resampling procedure would be used to ascertain the variability of the estimator. [sent-262, score-0.482]
</p><p>55 Interval estimates are constructed by using empirical percentiles of the resampling distribution obtained by perturbation-resampling. [sent-264, score-0.398]
</p><p>56 2 Variance Estimation on Benchmark Repository We further validate the ability of the proposed procedure in estimating the variance of the crossvalidation estimator on the benchmark repository used in Mika et al. [sent-276, score-0.316]
</p><p>57 To evaluate the variance estimator obtained by the perturbation-resampling procedure, we estimate the standard deviation of 5-fold cross-validation error based only on the ﬁrst partition of each data set. [sent-281, score-0.297]
</p><p>58 The results in Table 3 suggest that the perturbation-resampling based estimate of the standard deviation using only the ﬁrst partition of each data set is rather close to the sample standard deviation estimated using the entire data set. [sent-288, score-0.204]
</p><p>59 To the contrary, the standard deviation estimated by splitting the data set tends to be biased upward, while the ρ-based method tends to underestimate the standard deviation of the cross-validation error. [sent-289, score-0.3]
</p><p>60 In the results shown above, 1, 000 times of randomly splitting or resampling are used in all the three methods, and as a result, the actual computational efﬁciencies of different methods are comparable. [sent-290, score-0.307]
</p><p>61 3 Example in Kernel Selection To illustrate the application of the proposed procedure in model comparison, we perform kernel selection for SVM classiﬁers on simulated data. [sent-293, score-0.217]
</p><p>62 Then we apply the SVM algorithm by using the linear kernel and the polynomial kernel with optimal hyper-parameter C chosen by a cross-validation procedure, respectively. [sent-366, score-0.235]
</p><p>63 3 to obtain 95% conﬁdence intervals for the difference in cross-validation errors when using different kernels. [sent-368, score-0.25]
</p><p>64 We also compute the true prediction errors, together with the exact conﬁdence intervals on the difference between their cross-validated estimates, based on the prediction results of 1, 000 replications of simulated data sets for each setting. [sent-369, score-0.562]
</p><p>65 In Table 4, we report the 10-fold cross-validation errors by using linear and polynomial kernels, the 95% conﬁdence intervals on the difference between errors, and their respective true values. [sent-370, score-0.297]
</p><p>66 For the ﬁrst type of data, although the polynomial kernel could potentially lead to slightly lower error rates compared to the linear kernel, 95% conﬁdence intervals for error difference are quite tight around zero. [sent-371, score-0.436]
</p><p>67 On the other hand, for the second type of data, 95% conﬁdence intervals for error differences tend to deviate downward from zero, which indicates that the polynomial kernel indeed performs better than the linear kernel. [sent-373, score-0.434]
</p><p>68 The true standard deviation (Sd) is calculated based on 5-fold cross-validation errors estimated on the rest 99 partitions of the data. [sent-377, score-0.181]
</p><p>69 005]  Table 4: Kernel selection based on the interval estimates of the difference in cross-validation errors. [sent-417, score-0.233]
</p><p>70 In particular, the predicted results are consistent with the true values of both point and interval estimates obtained by simulating a large number of data sets. [sent-419, score-0.233]
</p><p>71 4 Example in the Genotypic Testing for Drug Resistance In this section, we give an example to show how the proposed procedure can be used in selecting important markers in the genotypic testing for HIV protease inhibitor (PI) resistance on the HIV RT and Protease Sequence Database (Rhee et al. [sent-422, score-0.51]
</p><p>72 First, we divide the sample set into two classes by labeling each protease sequence sample with 99 amino acids as either ”resistant” or ”susceptible”, depending on whether the resistance factor of the sample exceeds a certain drugspeciﬁc cutoff value or not (Beerenwinkel et al. [sent-424, score-0.312]
</p><p>73 Then, we predict the resistance to seven FDA-approved PIs using 10 sites on the substrate binding cleft or its ﬂap that are reported to cause resistance by reducing the binding afﬁnity between the inhibitor and the mutant protease enzyme. [sent-426, score-0.521]
</p><p>74 To this end, we compare the prediction errors for the models with and without X(90) and evaluate the incremental value of X(90) based on the reduction in the prediction error, denoted by ∆X(90) . [sent-429, score-0.299]
</p><p>75 We obtain the point and interval estimates of ∆X(90) based on the model comparison method discussed in Section 3. [sent-430, score-0.233]
</p><p>76 The results in Table 5 show that the 95% conﬁdence intervals for ∆ X(90) are tight around zero for drugs APV, ATV, and LPV, which indicates that X(90) adds rather modest value, if any, on top of other variables, for predicting resistance to these drugs. [sent-433, score-0.396]
</p><p>77 On the other hand, by including information on X(90) , the prediction of drug resistance to IDV and RTV can be signiﬁcantly improved in a sense that 8. [sent-434, score-0.348]
</p><p>78 The true errors are estimated based on the prediction results of 1, 000 replications of simulated data sets for each setting. [sent-437, score-0.347]
</p><p>79 The exact conﬁdence intervals are estimated based on the prediction results of 1, 000 replications of simulated data sets for each setting. [sent-439, score-0.491]
</p><p>80 002]  Table 5: Interval estimates for the prediction errors and their difference in the genotypic testing for HIV drug resistance with or without mutation information at site 90 on the protease sequence. [sent-506, score-0.727]
</p><p>81 the 95% conﬁdence intervals for ∆X(90) tend to locate on the negative side of the zero point. [sent-507, score-0.197]
</p><p>82 Discussion In this paper, we propose procedures for making inference about the prediction error of SVM classiﬁers based on cross-validated point estimators and their corresponding interval estimators. [sent-515, score-0.482]
</p><p>83 We establish large sample theory for the cross-validated estimators, and present a perturbation-resampling procedure to construct the conﬁdence interval for prediction errors. [sent-516, score-0.331]
</p><p>84 The proposed interval estimates are obtained by approximating the spread of W with that of W . [sent-517, score-0.233]
</p><p>85 However, such a perturbation procedure may be computationally intensive since a K-fold cross-validation scheme has to be conducted for each realization of the resampling weights. [sent-519, score-0.316]
</p><p>86 Results from extensive simulation studies suggest that the proposed point and interval estimators perform well in ﬁnite samples. [sent-520, score-0.302]
</p><p>87 Furthermore, through numerical studies, we demonstrate that the interval estimates provide much more information about the true underlying prediction accuracy than the point estimates. [sent-521, score-0.412]
</p><p>88 The proposed procedures also allow us to tackle the issue of model evaluation and selection by taking the uncertainty of estimators for the prediction error into account. [sent-523, score-0.328]
</p><p>89 We give several examples to illustrate some direct applications of the method, such as to provide conﬁdence intervals around the estimated prediction error in kernel and biomarker selections. [sent-524, score-0.556]
</p><p>90 Consistency of θ and D ˆ ˆ ˆ In the following theorem, we will show that as n → ∞, θ → θ0 and the training error D(θ) will converge to the absolute prediction error D0 in probability. [sent-529, score-0.221]
</p><p>91 Since g0 (X) is continuously differentiable, It remains to show the consistency of D( ˆ ˆ E|Y0 − Y (X0 , θ)| is continuously differentiable in θ with bounded derivatives. [sent-543, score-0.174]
</p><p>92 We note that although n−1 ∑n η2 is a consistent estimator i=1 ˆ i 2 , we approximate σ2 based on the resampling method, not n −1 n η2 . [sent-577, score-0.374]
</p><p>93 of σ ∑i=1 ˆ i For a more general case, when the prediction function f (X) in (1) is not a linear function of the input vector X, one can rewrite the prediction function in the form of f (X) = w Φ(X) + b, where Φ is a mapping from the input vector space to a ”feature space” H . [sent-578, score-0.246]
</p><p>94 To justify the resampling method, we ﬁrst note that it follows from the arguments in Appendix B that n √ ˆ n(θ − θ0 ) = −n−1/2 H−1 ∑ Mi (θ0 ) + o p (1) , i=1  and that  n √ ∗ n(θ − θ0 ) = −n−1/2 H−1 ∑ Gi Mi (θ0 ) + o p (1) , i=1  where Mi (θ0 ) = −Yi I(Yi f (Xi ; θ0 ) < 1)(Xi , 1) + 2λn (w0 , 0) . [sent-607, score-0.262]
</p><p>95 9) that the conditional distribution of W ∗ converges to a normal with mean 0 and variance n−1 ∑n η2 , which are the same as the unconditional distribution of W (or i=1 i its cross-validation counterpart W ). [sent-615, score-0.182]
</p><p>96 Efﬁcacy of indinavir-ritonavir-based regimens in HIV-1-infected patients with prior protease inhibitor failures. [sent-673, score-0.227]
</p><p>97 How biased is the apparent error rate of a prediction rule. [sent-703, score-0.223]
</p><p>98 Estimating misclassiﬁcation error with small samples via bootstrap cross-validation. [sent-726, score-0.264]
</p><p>99 On general resampling algorithms and their performance in distribution estimation. [sent-736, score-0.262]
</p><p>100 Jackknife, bootstrap, and other resampling methods in regression analysis (with discussion). [sent-912, score-0.262]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('resampling', 0.262), ('bootstrap', 0.215), ('iang', 0.206), ('intervals', 0.197), ('gi', 0.195), ('onfidence', 0.185), ('protease', 0.165), ('svm', 0.159), ('nterval', 0.157), ('rrors', 0.157), ('stimating', 0.157), ('interval', 0.154), ('dence', 0.149), ('resistance', 0.147), ('rediction', 0.129), ('mi', 0.126), ('hang', 0.125), ('nadeau', 0.124), ('prediction', 0.123), ('sd', 0.115), ('estimator', 0.112), ('ws', 0.109), ('qn', 0.1), ('estimators', 0.097), ('kernel', 0.094), ('variability', 0.084), ('efron', 0.083), ('bengio', 0.083), ('genotypic', 0.082), ('hiv', 0.082), ('iyi', 0.082), ('estimates', 0.079), ('drug', 0.078), ('deviation', 0.076), ('asymptotically', 0.075), ('xi', 0.075), ('simulated', 0.069), ('yi', 0.068), ('normal', 0.068), ('differentiable', 0.068), ('accuracies', 0.067), ('minimizer', 0.066), ('regularity', 0.064), ('coverage', 0.063), ('donsker', 0.062), ('immunode', 0.062), ('inhibitor', 0.062), ('con', 0.061), ('variance', 0.06), ('procedures', 0.059), ('percentiles', 0.057), ('cai', 0.057), ('jiang', 0.057), ('ers', 0.056), ('accuracy', 0.056), ('converges', 0.054), ('procedure', 0.054), ('continuously', 0.053), ('errors', 0.053), ('universally', 0.052), ('varma', 0.052), ('tsinghua', 0.052), ('drugs', 0.052), ('estimated', 0.052), ('studies', 0.051), ('biased', 0.051), ('zi', 0.05), ('replications', 0.05), ('error', 0.049), ('polynomial', 0.047), ('vc', 0.047), ('downward', 0.047), ('benchmark', 0.045), ('splitting', 0.045), ('repository', 0.045), ('kernels', 0.044), ('bo', 0.043), ('bioinformatics', 0.042), ('atv', 0.041), ('beerenwinkel', 0.041), ('biomarker', 0.041), ('campo', 0.041), ('idv', 0.041), ('indinavir', 0.041), ('infectious', 0.041), ('lpv', 0.041), ('mcfadden', 0.041), ('molinaro', 0.041), ('mutations', 0.041), ('newey', 0.041), ('para', 0.041), ('pis', 0.041), ('rtv', 0.041), ('saah', 0.041), ('saquinavir', 0.041), ('shivaprakash', 0.041), ('shulman', 0.041), ('tianxi', 0.041), ('xuegong', 0.041), ('ki', 0.041)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="32-tfidf-1" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>Author: Bo Jiang, Xuegong Zhang, Tianxi Cai</p><p>Abstract: Support vector machine (SVM) is one of the most popular and promising classiﬁcation algorithms. After a classiﬁcation rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples. Keywords: k-fold cross-validation, model evaluation, perturbation-resampling, prediction errors, support vector machine</p><p>2 0.10459213 <a title="32-tfidf-2" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>Author: Eric Bax</p><p>Abstract: This paper develops bounds on out-of-sample error rates for support vector machines (SVMs). The bounds are based on the numbers of support vectors in the SVMs rather than on VC dimension. The bounds developed here improve on support vector counting bounds derived using Littlestone and Warmuth’s compression-based bounding technique. Keywords: compression, error bound, support vector machine, nearly uniform</p><p>3 0.096545555 <a title="32-tfidf-3" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>Author: Mikio L. Braun, Joachim M. Buhmann, Klaus-Robert Müller</p><p>Abstract: We show that the relevant information of a supervised learning problem is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem in the sense that it can asymptotically represent the function to be learned and is sufﬁciently smooth. Thus, kernels do not only transform data sets such that good generalization can be achieved using only linear discriminant functions, but this transformation is also performed in a manner which makes economical use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data for supervised learning problems. Practically, we propose an algorithm which enables us to recover the number of leading kernel PCA components relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to aid in model selection, and (3) to denoise in feature space in order to yield better classiﬁcation results. Keywords: kernel methods, feature space, dimension reduction, effective dimensionality</p><p>4 0.096154295 <a title="32-tfidf-4" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>Author: Sébastien Loustau</p><p>Abstract: This paper investigates statistical performances of Support Vector Machines (SVM) and considers the problem of adaptation to the margin parameter and to complexity. In particular we provide a classiﬁer with no tuning parameter. It is a combination of SVM classiﬁers. Our contribution is two-fold: (1) we propose learning rates for SVM using Sobolev spaces and build a numerically realizable aggregate that converges with same rate; (2) we present practical experiments of this method of aggregation for SVM using both Sobolev spaces and Gaussian kernels. Keywords: classiﬁcation, support vector machines, learning rates, approximation, aggregation of classiﬁers</p><p>5 0.094113864 <a title="32-tfidf-5" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>Author: Ja-Yong Koo, Yoonkyung Lee, Yuwon Kim, Changyi Park</p><p>Abstract: The support vector machine has been successful in a variety of applications. Also on the theoretical front, statistical properties of the support vector machine have been studied quite extensively with a particular attention to its Bayes risk consistency under some conditions. In this paper, we study somewhat basic statistical properties of the support vector machine yet to be investigated, namely the asymptotic behavior of the coefﬁcients of the linear support vector machine. A Bahadur type representation of the coefﬁcients is established under appropriate conditions, and their asymptotic normality and statistical variability are derived on the basis of the representation. These asymptotic results do not only help further our understanding of the support vector machine, but also they can be useful for related statistical inferences. Keywords: asymptotic normality, Bahadur representation, classiﬁcation, convexity lemma, Radon transform</p><p>6 0.080512822 <a title="32-tfidf-6" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>7 0.071407042 <a title="32-tfidf-7" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>8 0.071084417 <a title="32-tfidf-8" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>9 0.0685848 <a title="32-tfidf-9" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>10 0.065920487 <a title="32-tfidf-10" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>11 0.064669549 <a title="32-tfidf-11" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>12 0.063071348 <a title="32-tfidf-12" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>13 0.062787607 <a title="32-tfidf-13" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>14 0.061741397 <a title="32-tfidf-14" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>15 0.061689418 <a title="32-tfidf-15" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>16 0.055992696 <a title="32-tfidf-16" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>17 0.054870471 <a title="32-tfidf-17" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>18 0.05362748 <a title="32-tfidf-18" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>19 0.052129768 <a title="32-tfidf-19" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>20 0.050631918 <a title="32-tfidf-20" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.276), (1, -0.136), (2, 0.059), (3, -0.037), (4, 0.108), (5, -0.023), (6, 0.09), (7, 0.003), (8, 0.082), (9, -0.071), (10, 0.055), (11, -0.026), (12, -0.011), (13, -0.039), (14, -0.022), (15, -0.066), (16, 0.197), (17, 0.077), (18, 0.049), (19, 0.096), (20, -0.055), (21, 0.051), (22, -0.035), (23, 0.027), (24, -0.011), (25, 0.044), (26, 0.108), (27, -0.005), (28, -0.031), (29, 0.125), (30, -0.138), (31, 0.245), (32, 0.042), (33, 0.149), (34, 0.083), (35, 0.187), (36, -0.01), (37, -0.077), (38, -0.022), (39, -0.092), (40, 0.057), (41, -0.114), (42, 0.164), (43, 0.122), (44, 0.065), (45, -0.079), (46, 0.121), (47, 0.084), (48, -0.023), (49, -0.005)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94184804 <a title="32-lsi-1" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>Author: Bo Jiang, Xuegong Zhang, Tianxi Cai</p><p>Abstract: Support vector machine (SVM) is one of the most popular and promising classiﬁcation algorithms. After a classiﬁcation rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples. Keywords: k-fold cross-validation, model evaluation, perturbation-resampling, prediction errors, support vector machine</p><p>2 0.60327601 <a title="32-lsi-2" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>Author: Ja-Yong Koo, Yoonkyung Lee, Yuwon Kim, Changyi Park</p><p>Abstract: The support vector machine has been successful in a variety of applications. Also on the theoretical front, statistical properties of the support vector machine have been studied quite extensively with a particular attention to its Bayes risk consistency under some conditions. In this paper, we study somewhat basic statistical properties of the support vector machine yet to be investigated, namely the asymptotic behavior of the coefﬁcients of the linear support vector machine. A Bahadur type representation of the coefﬁcients is established under appropriate conditions, and their asymptotic normality and statistical variability are derived on the basis of the representation. These asymptotic results do not only help further our understanding of the support vector machine, but also they can be useful for related statistical inferences. Keywords: asymptotic normality, Bahadur representation, classiﬁcation, convexity lemma, Radon transform</p><p>3 0.50901043 <a title="32-lsi-3" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>Author: Gerda Claeskens, Christophe Croux, Johan Van Kerckhoven</p><p>Abstract: Support vector machines for classiﬁcation have the advantage that the curse of dimensionality is circumvented. It has been shown that a reduction of the dimension of the input space leads to even better results. For this purpose, we propose two information criteria which can be computed directly from the deﬁnition of the support vector machine. We assess the predictive performance of the models selected by our new criteria and compare them to existing variable selection techniques in a simulation study. The simulation results show that the new criteria are competitive in terms of generalization error rate while being much easier to compute. We arrive at the same ﬁndings for comparison on some real-world benchmark data sets. Keywords: information criterion, supervised classiﬁcation, support vector machine, variable selection</p><p>4 0.41272259 <a title="32-lsi-4" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>Author: Hsuan-Tien Lin, Ling Li</p><p>Abstract: Ensemble learning algorithms such as boosting can achieve better performance by averaging over the predictions of some base hypotheses. Nevertheless, most existing algorithms are limited to combining only a ﬁnite number of hypotheses, and the generated ensemble is usually sparse. Thus, it is not clear whether we should construct an ensemble classiﬁer with a larger or even an inﬁnite number of hypotheses. In addition, constructing an inﬁnite ensemble itself is a challenging task. In this paper, we formulate an inﬁnite ensemble learning framework based on the support vector machine (SVM). The framework can output an inﬁnite and nonsparse ensemble through embedding inﬁnitely many hypotheses into an SVM kernel. We use the framework to derive two novel kernels, the stump kernel and the perceptron kernel. The stump kernel embodies inﬁnitely many decision stumps, and the perceptron kernel embodies inﬁnitely many perceptrons. We also show that the Laplacian radial basis function kernel embodies inﬁnitely many decision trees, and can thus be explained through inﬁnite ensemble learning. Experimental results show that SVM with these kernels is superior to boosting with the same base hypothesis set. In addition, SVM with the stump kernel or the perceptron kernel performs similarly to SVM with the Gaussian radial basis function kernel, but enjoys the beneﬁt of faster parameter selection. These properties make the novel kernels favorable choices in practice. Keywords: ensemble learning, boosting, support vector machine, kernel</p><p>5 0.4090102 <a title="32-lsi-5" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>Author: Eric Bax</p><p>Abstract: This paper develops bounds on out-of-sample error rates for support vector machines (SVMs). The bounds are based on the numbers of support vectors in the SVMs rather than on VC dimension. The bounds developed here improve on support vector counting bounds derived using Littlestone and Warmuth’s compression-based bounding technique. Keywords: compression, error bound, support vector machine, nearly uniform</p><p>6 0.40016973 <a title="32-lsi-6" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>7 0.39818889 <a title="32-lsi-7" href="./jmlr-2008-Aggregation_of_SVM_Classifiers_Using_Sobolev_Spaces.html">11 jmlr-2008-Aggregation of SVM Classifiers Using Sobolev Spaces</a></p>
<p>8 0.39140555 <a title="32-lsi-8" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>9 0.37061104 <a title="32-lsi-9" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>10 0.36875495 <a title="32-lsi-10" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>11 0.36589795 <a title="32-lsi-11" href="./jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>12 0.35386419 <a title="32-lsi-12" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>13 0.34209633 <a title="32-lsi-13" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>14 0.3388322 <a title="32-lsi-14" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>15 0.33836588 <a title="32-lsi-15" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>16 0.33400121 <a title="32-lsi-16" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>17 0.2975637 <a title="32-lsi-17" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>18 0.27684754 <a title="32-lsi-18" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>19 0.27579784 <a title="32-lsi-19" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>20 0.27080104 <a title="32-lsi-20" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.029), (5, 0.011), (31, 0.014), (40, 0.03), (54, 0.022), (58, 0.607), (66, 0.055), (76, 0.01), (88, 0.068), (92, 0.035), (94, 0.034), (99, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9580071 <a title="32-lda-1" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>Author: Bo Jiang, Xuegong Zhang, Tianxi Cai</p><p>Abstract: Support vector machine (SVM) is one of the most popular and promising classiﬁcation algorithms. After a classiﬁcation rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples. Keywords: k-fold cross-validation, model evaluation, perturbation-resampling, prediction errors, support vector machine</p><p>2 0.93921781 <a title="32-lda-2" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>Author: Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui</p><p>Abstract: Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefﬁcients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semideﬁnite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefﬁcients, with total complexity O(n3 ), where n is the number of variables. We then use the same relaxation to derive sufﬁcient conditions for global optimality of a solution, which can be tested in O(n3 ) per pattern. We discuss applications in subset selection and sparse recovery and show on artiﬁcial examples and biological data that our algorithm does provide globally optimal solutions in many cases. Keywords: PCA, subset selection, sparse eigenvalues, sparse recovery, lasso</p><p>3 0.524167 <a title="32-lda-3" href="./jmlr-2008-Consistency_of_Trace_Norm_Minimization.html">26 jmlr-2008-Consistency of Trace Norm Minimization</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufﬁcient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulﬁlled. Keywords: convex optimization, singular value decomposition, trace norm, consistency</p><p>4 0.48776993 <a title="32-lda-4" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><p>5 0.48139715 <a title="32-lda-5" href="./jmlr-2008-Comments_on_the_Complete_Characterization_of_a_Family_of_Solutions_to_a_GeneralizedFisherCriterion.html">23 jmlr-2008-Comments on the Complete Characterization of a Family of Solutions to a GeneralizedFisherCriterion</a></p>
<p>Author: Jieping Ye</p><p>Abstract: Loog (2007) provided a complete characterization of the family of solutions to a generalized Fisher criterion. We show that this characterization is essentially equivalent to the original characterization proposed in Ye (2005). The computational advantage of the original characterization over the new one is discussed, which justiﬁes its practical use. Keywords: linear discriminant analysis, dimension reduction, linear transformation 1. Generalized Fisher Criterion For a given data set consisting of n data points {ai }n in IRd , a linear transformation G ∈ IRd× i=1 ( < d) maps each ai for 1 ≤ i ≤ n in the d-dimensional space to a vector ai in the -dimensional ˜ space as follows: G : ai ∈ IRd → ai = GT ai ∈ IR . ˜ Assume that there are k classes in the data set. The within-class scatter matrix S w , the betweenclass scatter matrix Sb , and the total scatter matrix St involved in linear discriminant analysis are deﬁned as follows (Fukunaga, 1990): k Sw = ∑ (Ai − ci eT )(Ai − ci eT )T , i=1 k Sb = ∑ ni (ci − c)(ci − c)T , i=1 k St = ∑ (Ai − ceT )(Ai − ceT )T , i=1 where Ai denotes the data matrix of the i-th class, ci = Ai e/ni is the centroid of the i-th class, ni is the sample size of the i-th class, c = Ae/n is the global centroid, and e is the vector of all ones with an appropriate length. It is easy to verify that St = Sb + Sw . In Ye (2005), the optimal transformation G is computed by maximizing a generalized Fisher criterion as follows: + G = arg max trace GT St G GT Sb G , (1) m× G∈IR c 2008 Jieping Ye. YE where M + denotes the pseudo-inverse (Golub and Van Loan, 1996) of M and it is introduced to overcome the singularity problem when dealing with high-dimensional low-sample-size data. 1.1 Equivalent Transformation Two linear transformations G1 and G2 can be considered equivalent if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. Indeed, in this case, the difference between the projections 1 2 by G1 and G2 is a mere shift. Deﬁnition 1.1 For a given data set {a1 , · · · , an }, two transformations G1 and G2 are equivalent, if there is a vector v such that GT (ai − v) = GT (ai − v), for i = 1, · · · , n. 1 2 2. Characterization of Solutions to the Generalized Fisher Criterion Let St = UΣU T be the orthogonal eigendecomposition of St (note that St is symmetric and positive semi-deﬁnite), where U ∈ IRd×d is orthogonal and Σ ∈ IRd×d is diagonal with nonnegative diagonal entries sorted in nonincreasing order. Denote Σr as the r-th principal submatrix of Σ, where r = rank(St ). Partition U into two components as U = [U1 ,U2 ], where U1 ∈ IRd×r and U2 ∈ IRd×(d−r) . Note that r ≤ n, and for high-dimensional low-sample-size data, U1 is much smaller than U2 . In Loog (2007), a complete family of solutions S to the maximization problem in Eq. (1) is given as (We correct the error in Loog (2007) by using U instead of U T .) S= U ΛZ Y ∈ IRd× Z ∈ IR × is nonsingular , Y ∈ IR(n−r)× , where Λ ∈ IRr× maximizes the following objective function: F0 (X) = trace −1 X T Σr X T X T (U1 SbU1 )X . ˜ In Ye (2005), a family of solutions S is given as ˜ S= U ΛZ 0 ∈ IRd× Z ∈ IR × is nonsingular . The only difference between these two characterizations of solutions is the matrix Y in S , which is ˜ replaced by the zero matrix in S . We show in the next section the equivalence relationship between these two characterizations. 3. Equivalent Solution Characterizations ˜ Consider the following two transformations G1 and G2 from S and S respectively: G1 = U ΛZ Y ∈ S, G2 = U 518 ΛZ 0 ˜ ∈ S. O N THE C OMPLETE C HARACTERIZATION OF S OLUTIONS TO A G ENERALIZED F ISHER C RITERION Recall that U = [U1 ,U2 ], where the columns of U2 span the null space of St . Hence, n T T T 0 = U2 St U2 = ∑ U2 (ai − c) · (U2 (ai − c))T , i=1 T and U2 (ai − c) = 0, for i = 1, · · · , n, where c is the global centroid. It follows that T T T GT (ai − c) = Z T ΛT U1 (ai − c) +Y T U2 (ai − c) = Z T ΛT U1 (ai − c) = GT (ai − c), 1 2 for i = 1, · · · , n. That is, G1 and G2 are equivalent transformations. Hence, the two solution charac˜ terizations S and S are essentially equivalent. Remark 3.1 The analysis above shows that the additional information contained in S is the null ˜ space, U2 , of St , which leads to an equivalent transformation. In S , the null space U2 is removed, which can be further justiﬁed as follows. Since St = Sb + Sw , we have T T T 0 = U2 St U2 = U2 SbU2 +U2 SwU2 . T It follows that U2 SbU2 = 0, as both Sb and Sw are positive semi-deﬁnite. Thus, the null space U2 does not contain any discriminant information. This explains why the null space of St is removed in most discriminant analysis based algorithms proposed in the past. 4. Efﬁciency Comparison In S , the full matrix U is involved, whose computation may be expensive, especially for high˜ dimensional data. In contrast, only the ﬁrst component U1 ∈ IRd×r of U is involved in S , which can be computed efﬁciently for high-dimensional low-sample-size problem by directly working on the Gram matrix instead of the covariance matrix. ˜ In summary, we show that S and S are equivalent characterizations of the solutions to the generalized Fisher criterion in Eq. (1). However, the latter one is preferred in practice due to its relative efﬁciency for high-dimensional low-sample-size data. References K. Fukunaga. Introduction to Statistical Pattern Classiﬁcation. Academic Press, San Diego, California, USA, 1990. G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, MD, USA, third edition, 1996. M. Loog. A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion. Journal of Machine Learning Research, 8:2121–2123, 2007. J. Ye. Characterization of a Family of Algorithms for Generalized Discriminant Analysis on Undersampled Problems. Journal of Machine Learning Research, 6:483–502, 2005. 519</p><p>6 0.46576476 <a title="32-lda-6" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>7 0.45345676 <a title="32-lda-7" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>8 0.45219815 <a title="32-lda-8" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>9 0.44589365 <a title="32-lda-9" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>10 0.42922971 <a title="32-lda-10" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>11 0.42092645 <a title="32-lda-11" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>12 0.41894576 <a title="32-lda-12" href="./jmlr-2008-Near-Optimal_Sensor_Placements_in_Gaussian_Processes%3A_Theory%2C_Efficient_Algorithms_and_Empirical_Studies.html">67 jmlr-2008-Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies</a></p>
<p>13 0.4160257 <a title="32-lda-13" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>14 0.41561735 <a title="32-lda-14" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>15 0.41344628 <a title="32-lda-15" href="./jmlr-2008-An_Extension_on_%22Statistical_Comparisons_of_Classifiers_over_Multiple_Data_Sets%22_for_all_Pairwise_Comparisons.html">14 jmlr-2008-An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons</a></p>
<p>16 0.41276827 <a title="32-lda-16" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>17 0.4104813 <a title="32-lda-17" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>18 0.40830395 <a title="32-lda-18" href="./jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">80 jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>19 0.4046613 <a title="32-lda-19" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>20 0.39865971 <a title="32-lda-20" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
