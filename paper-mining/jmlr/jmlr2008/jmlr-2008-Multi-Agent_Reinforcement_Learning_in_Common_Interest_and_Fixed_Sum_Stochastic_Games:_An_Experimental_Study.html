<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-65" href="#">jmlr2008-65</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>65 jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</h1>
<br/><p>Source: <a title="jmlr-2008-65-pdf" href="http://jmlr.org/papers/volume9/bab08a/bab08a.pdf">pdf</a></p><p>Author: Avraham Bab, Ronen I. Brafman</p><p>Abstract: Multi Agent Reinforcement Learning (MARL) has received continually growing attention in the past decade. Many algorithms that vary in their approaches to the different subtasks of MARL have been developed. However, the theoretical convergence results for these algorithms do not give a clue as to their practical performance nor supply insights to the dynamics of the learning process itself. This work is a comprehensive empirical study conducted on MGS, a simulation system developed for this purpose. It surveys the important algorithms in the ﬁeld, demonstrates the strengths and weaknesses of the different approaches to MARL through application of FriendQ, OAL, WoLF, FoeQ, Rmax, and other algorithms to a variety of fully cooperative and fully competitive domains in self and heterogeneous play, and supplies an informal analysis of the resulting learning processes. The results can aid in the design of new learning algorithms, in matching existing algorithms to speciﬁc tasks, and may guide further research and formal analysis of the learning processes. Keywords: reinforcement learning, multi-agent reinforcement learning, stochastic games</p><p>Reference: <a title="jmlr-2008-65-reference" href="../jmlr2008_reference/jmlr-2008-Multi-Agent_Reinforcement_Learning_in_Common_Interest_and_Fixed_Sum_Stochastic_Games%3A_An_Experimental_Study_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 [R(s, a, s )]i is i’s reward upon transition from state s to state s under joint action a. [sent-103, score-0.436]
</p><p>2 , πn ) for n players of a SG is called a policy proﬁle. [sent-114, score-0.318]
</p><p>3 While we use formulations that aim to maximize IHDR, the games we experiment on are such that any good policy will reach an absorbing state (following which the agents are placed in their initial states) quickly. [sent-121, score-0.43]
</p><p>4 Consequently, we will sometimes ﬁnd it more natural to report performance measures such as average reward per step. [sent-123, score-0.288]
</p><p>5 Therefore, all agents have identical interests and we may speak of optimal joint policies, namely, policy proﬁles that maximize the common IHDR for the team of agents. [sent-163, score-0.34]
</p><p>6 In addition, they challenge the agents to coordinate behavior since to obtain maximum value may require that agents 2638  M ULTI -AGENT RL IN S TOCHASTIC G AMES  select a particular joint action. [sent-166, score-0.382]
</p><p>7 For efﬁcient learning in CISGs, agents are required to coordinate on two levels: (i) select whether to explore or exploit in unison; and (ii) coordinate the exploration and exploitation moves. [sent-168, score-0.345]
</p><p>8 Furthermore, even when the model is known, multiple NEs yielding maximal payoffs to the agents are likely to exist, and the agents still face the task of reaching consensus on which speciﬁc NE to play. [sent-172, score-0.391]
</p><p>9 , an ) in state s at time t and reaching state s with reward rcur , each agent updates its Q-value estimates for s, a as follows: Qt (s, a) ← (1 − αt )Qt−1 (s, a) + αt rcur + γ max Q(s , a ) . [sent-182, score-0.38]
</p><p>10 5 Two popular undirected exploration methods are ε-greedy action selection and Boltzman distributed action selection. [sent-189, score-0.321]
</p><p>11 There is no established technique for applying Boltzman exploration to FriendQ, so in our experiments it is executed with ε-greedy exploration only. [sent-190, score-0.29]
</p><p>12 ε-greedy exploration is applied to SGs in the following way: each agent randomly picks an exploratory private action with probability ε, and with probability 1 − ε takes its part of an optimal (greedy) joint action with  4. [sent-191, score-0.458]
</p><p>13 Since full state observability, perfect monitoring, and identical initial Q-values to all agents are assumed, all agents maintain identical Q-values throughout the process, and consequently the same classiﬁcation of greedy actions. [sent-200, score-0.401]
</p><p>14 But, two problems arise: (i) Because randomization is used to select exploration or exploitation, the agents cannot coordinate their choice of when and what to explore. [sent-201, score-0.315]
</p><p>15 At each point in time, all agents have an identical model of the environment and know what joint action needs to be executed next (when a number of actions are optimal with respect to the current state, the agents use the shared order over joint actions to select among these actions). [sent-251, score-0.616]
</p><p>16 Rmax computes an optimal policy with respect to M and follows this policy until some entry becomes known. [sent-269, score-0.302]
</p><p>17 Hence, some popular techniques for decreasing exploration in the single agent case lead to ﬁnite exploration in the multi agent case. [sent-302, score-0.44]
</p><p>18 Finally, to test how well each algorithm scales up with the number of players, we introduced a fourth game in which the state and action spaces do not grow too fast with the number of players. [sent-338, score-0.312]
</p><p>19 OAL with ε-greedy exploration is referred to as ε-OAL, and OAL with Boltzman exploration is referred to as B-OAL. [sent-362, score-0.29]
</p><p>20 In the goal state G, both agents are in their goal positions and their reward is 48. [sent-377, score-0.435]
</p><p>21 The optimal behavior in deterministic mode reaches G in four steps and yields an average reward per step (a. [sent-380, score-0.403]
</p><p>22 Thus, we chose to report the performance of the learned policies using more intuitive measures such as average reward per step and average steps to reach the goal. [sent-412, score-0.403]
</p><p>23 2645  BAB AND B RAFMAN  10 average reward per step. [sent-413, score-0.288]
</p><p>24 Averaged over 100 trials 0  200000  400000 600000 round number  800000  1e+06  (d) Rmax; OAL; FriendQ  Figure 2: Game 1 – average reward per step under deterministic mode. [sent-427, score-0.434]
</p><p>25 This behavior results from a sudden switch of the FriendQ agents from sub-optimal to optimal behavior once the relative order of the Q-values of different agents changes. [sent-433, score-0.386]
</p><p>26 polynomial decay of ε-greedy exploration probability OAL agents converge relatively quickly to optimal or second-best behavior, and from that time onwards stick to their behavior (Fig. [sent-469, score-0.389]
</p><p>27 Later on, ε-greedy maintains a low exploration probability that decays very slowly while Boltzman exploration drops faster to zero. [sent-476, score-0.29]
</p><p>28 When the agents make many stochastic action choices in 2647  BAB AND B RAFMAN  early stages of learning, ﬁctitious play ampliﬁes the random behavior. [sent-482, score-0.366]
</p><p>29 Another interesting difference from the deterministic setting is that initially ε-OAL gains lower return than B-OAL but while B-OAL keeps attaining the same average reward, ε-OAL improves slowly over time and eventually gains a higher average reward than B-OAL. [sent-500, score-0.346]
</p><p>30 In this case, the slower convergence of the exploration probability to zero enables ε-OAL to “overcome” randomly “bad” exploration in initial learning phases. [sent-501, score-0.29]
</p><p>31 2648  M ULTI -AGENT RL IN S TOCHASTIC G AMES  average reward per step in last 10,000 rounds  5. [sent-525, score-0.347]
</p><p>32 Average reward per step of learned policy against # of trials: x = # of trials in which avg reward of learned policy exceeded y  4 3. [sent-544, score-0.824]
</p><p>33 8  0  20  40 60 number of experiments  80  100  (d) Average reward of learned policies per number of trials  Figure 4: Game 1 – Average reward per step and learned policies per number of trials in stochastic mode. [sent-546, score-0.877]
</p><p>34 2 G AME 2 This game was designed to minimize the effects of equilibrium selection, to show how GLIELPs may keep agents exploiting suboptimal possibilities, and to emphasize the importance of coordinated exploration. [sent-550, score-0.399]
</p><p>35 The average reward per step of an optimal strategy under deterministic mode is 10, and the discounted return is ∼ 465. [sent-568, score-0.407]
</p><p>36 The main reasons for the sub-optimal performance of OAL and CFQ in this game are: (i) Random exploration has a greater chance of reaching G 2 than G1 . [sent-589, score-0.329]
</p><p>37 4  average reward per step  average reward per step  9  8. [sent-601, score-0.576]
</p><p>38 Since both agents react to each other’s past plays using BAP, it may take long to converge to a new NE when the optimal joint actions are changed. [sent-629, score-0.309]
</p><p>39 4  average reward per step  average reward per step  3. [sent-639, score-0.576]
</p><p>40 However, in stochastic mode the GLIELPs no longer have this advantage since stochastic transitions do not enable the agents to concentrate on exploiting a local set of states (Fig. [sent-686, score-0.316]
</p><p>41 10  8 7 UFQ CFQ MQ e-OAL B-OAL Rmax, K1=200 Rmax K1=100 Rmax K1=50  8 7  average reward per step  average reward per step  9  6 5  6 5  3  4  All algorithms on deterministic Game-3. [sent-688, score-0.628]
</p><p>42 0  1e+05 2e+05 3e+05 4e+05 5e+05 6e+05 7e+05 8e+05 9e+05 1e+06 round number  (b) stochastic mode  Figure 8: Game 3 – average reward per step under deterministic and stochastic modes. [sent-694, score-0.55]
</p><p>43 5  average reward per step  average reward per step  4. [sent-723, score-0.576]
</p><p>44 When two players attempt to move to the same position, the result is with probability 1/3 none move, and with probability 1/3 each one of the players makes the move and the other stays in place. [sent-749, score-0.334]
</p><p>45 Generally, the reward at each state is the number of players located at their goal positions. [sent-752, score-0.432]
</p><p>46 However, when all players are in their goal position, they receive a reward of 3 times the number of players, at which point all players transition automatically to the initial position. [sent-753, score-0.583]
</p><p>47 5  3  3  average reward per step  average reward per step  3. [sent-755, score-0.576]
</p><p>48 5  0  0  50000 100000 150000 200000 250000 300000 350000 400000 450000 round number  0  0  500000  (a) Game 4 R-max  1000000 round number  1500000  2000000  (b) Game 4 OAL Game 4 Friend-Q Stochastic Setup  average reward per step  3. [sent-762, score-0.416]
</p><p>49 5 0  0  500000  1000000 round number  1500000  2000000  (c) Game 4 Friend-Q  Figure 10: Results for 2-5 players on game 4. [sent-766, score-0.415]
</p><p>50 2655  BAB AND B RAFMAN  For all algorithms, the value is greater as the number of players increases due to the game’s reward deﬁnition, which is sensitive to the number of players. [sent-774, score-0.392]
</p><p>51 However, the convergence dynamics of Rmax does not suit tasks in which the agents must attain some value during the learning period, because during its exploration phase, Rmax is indifferent to rewards lower than R max . [sent-791, score-0.347]
</p><p>52 π∈PD(A) b∈B a∈A  If SG G is obtained from ZSSG G by adding a constant c to all payoffs of both players, then = ViG (π1 , π2 ) + c/(1 − γ) for any policy proﬁle (π1 , π2 ) and the strategic properties of the game are unchanged. [sent-821, score-0.407]
</p><p>53 The adversarial nature of FSSGs calls for agents that perform well not only in self play but also in heterogeneous play, namely when engaged by agents that employ different learning algorithms. [sent-823, score-0.493]
</p><p>54 After taking a joint action (a, b) in state s at time t and reaching state s with reward rcur , the agent updates the Q-value of s, (a, b) by Qt (s, a, b) ← (1 − αt )Qt−1 (s, a, b) + αt rcur + γ max min  ∑ π(a )Q(s , a , b )  π∈PD(A) b ∈B a ∈A  . [sent-832, score-0.487]
</p><p>55 That is, the policy is improved by increasing the probability of selecting a greedy action according to a policy learning rate δ (which is distinct from the Q-value learning rate α), enabling mixed policies. [sent-841, score-0.411]
</p><p>56 The uniqueness of WoLF is in using a variable policy learning rate according to the “Win or Learn Fast” (hence WoLF) principle: if the expected return of the current policy given the current Q-values is below (above) a certain threshold then a high, δl (low, δw ), learning rate is set. [sent-842, score-0.329]
</p><p>57 A good threshold would be the NE value of the game because if the player is receiving less than its value, its likely playing a sub-optimal strategy, whereas if it receives more than the NE value, the other players must be playing sub-optimally. [sent-843, score-0.454]
</p><p>58 Seeing that the optimal policy maximizes return against the worst opponent, if the opponent prevents Rmax from visiting unknown entries then Rmax attains near-optimal return because the known entries are accurately modeled. [sent-856, score-0.293]
</p><p>59 Thus, attempted exploration may result, depending on the opponent’s action choice, in joint actions that are of low informative and materialistic value. [sent-865, score-0.327]
</p><p>60 Speciﬁcally, this exploration is regulated by the variable policy learning rate to explore more while “winning”. [sent-868, score-0.296]
</p><p>61 When A reaches the rightmost column of the grid, it receives a reward of 40, D receives a reward of 0 and the players are reset in their initial positions. [sent-933, score-0.617]
</p><p>62 5e+06 round number  26  24  24 FoeQ attacks RmaxK50 FoeQ attacks RmaxK100 FoeQ attacks RmaxK200  22  average reward per step  18  3. [sent-954, score-0.673]
</p><p>63 20  3e+06  (b) FoeQ and WoLF in self play  26  average reward per step  1e+06  16  20  18  16 Deterministic Wall game on 3x3 grid. [sent-958, score-0.598]
</p><p>64 5e+06 round number  (d) Rmax vs WoLF  Figure 12: 3x3 wall game – average reward per step 2661  3e+06  3. [sent-963, score-0.666]
</p><p>65 obtained by the different agents playing the Attacker’s role in self and heterogeneous play. [sent-1156, score-0.3]
</p><p>66 The FoeQ Defender learns correct Q-values and an optimal policy while the Attacker learns a rough estimation of the Q-values and a suboptimal policy (Table 4). [sent-1162, score-0.302]
</p><p>67 Second, its learning policy is a best response policy rather than an equilibrium policy. [sent-1178, score-0.346]
</p><p>68 And third, the hill climbing policy updates combined with the variable learning rate serve as an additional exploration mechanism. [sent-1180, score-0.296]
</p><p>69 As long as the WoLF players have not converged to equilibrium, an increased policy learning rate will always be used by one of the players. [sent-1181, score-0.318]
</p><p>70 In self play, the identical exploration and exploitation techniques of both players gives rise to efﬁcient joint exploration and hence to fast convergence to policies that are close or equal to the minimax policies. [sent-1186, score-0.764]
</p><p>71 its inaccurate estimation of Q-values and strategic structure and since unknown entries are estimated by FoeQ as having low Q-values, the joint policy does not frequently reach unknown entries but rather yields high rewards for Rmax. [sent-1213, score-0.289]
</p><p>72 Before new entries become known to Rmax, WoLF learns a deterministic response policy with a higher return to WoLF than the minimax value. [sent-1229, score-0.376]
</p><p>73 Each joint policy that results from Rmax’s policy updates has one of the following properties: (i) The new joint policy seldom visits unknown entries. [sent-1231, score-0.491]
</p><p>74 In cases (i) and (ii), WoLF will switch to “learn fast” mode and, unless Rmax is already playing the minimax policy, will manage to learn a new response policy with return higher than the minimax before the next stage. [sent-1235, score-0.456]
</p><p>75 This joint policy is guaranteed to visit unknown entries but directs exploration to entries more proﬁtable to WoLF. [sent-1236, score-0.381]
</p><p>76 WoLF’s hill climbing method, variable learning rate and small action space enable it to adjust fast to Rmax’s new policies and maintain an average return higher than the minimax until Rmax converges to the NE policy. [sent-1237, score-0.323]
</p><p>77 Figure 13 depicts the initial position of the game and A’s reward structure. [sent-1249, score-0.409]
</p><p>78 The game is designed to fool GLIELPs with random exploration played by 2664  M ULTI -AGENT RL IN S TOCHASTIC G AMES  A. [sent-1259, score-0.329]
</p><p>79 5e+06  4e+06  115  FoeQ attacks RmaxK50 FoeQ attacks RmaxK100 FoeQ attacks RmaxK200 RmaxK50 attacks FoeQ RmaxK100 attacks FoeQ RmaxK200 attacks FoeQ minimax solution  110  110  105 average reward per step  105 average reward per step  1. [sent-1272, score-1.311]
</p><p>80 5e+06 round number  (b) FoeQ and WoLF in self play  115  100 Deterministic Wall game on 5x2 grid. [sent-1274, score-0.374]
</p><p>81 5e+06  4e+06  (c) Rmax vs FoeQ  100 WoLF attacks RmaxK50 WoLF attacks RmaxK100 WoLF attacks RmaxK200 RmaxK50 attacks WoLF RmaxK100 attacks WoLF RmaxK200 attacks WoLF minimax solution Deterministic Wall game on 5x2 grid. [sent-1280, score-0.961]
</p><p>82 5e+06 round number  (d) Rmax vs WoLF  Figure 14: 5 × 2 wall game – average reward per step  2665  3e+06  3. [sent-1285, score-0.666]
</p><p>83 With values of 100 and 200 of K1, Rmax still has a small exploration bias after 4 × 10 6 rounds (Table 5) and attains almost the minimax value (Fig. [sent-1478, score-0.297]
</p><p>84 Since the entries associated with states of both players being in the bottom rows are unknown to the Rmax Defender, they are modeled as unrewarding by FoeQ, and hence, its policy defends only in the top rows. [sent-1499, score-0.351]
</p><p>85 In plays of Rmax against WoLF, the players converge to minimax in the examined time interval only for Rmax with K1 = 50 and WoLF gains a. [sent-1501, score-0.305]
</p><p>86 The minimax policies and values are also unchanged since the attacker’s optimal policy in both settings attacks only in the two bottom rows. [sent-1517, score-0.445]
</p><p>87 115  115 WoLF attacks RmaxK50 WoLF attacks RmaxK100 WoLF attacks RmaxK200 minimax WoLF in selfplay  110  105  num of discovered entries  num of discovered entries  110  100  95 Deterministic Wall game on 5x2 grid, second version. [sent-1518, score-0.664]
</p><p>88 5e+06  RmaxK50 attacks WoLF RmaxK100 attacks WoLF RmaxK200 attacks WoLF minimax  4e+06  (a) WoLF attacks Rmax and WoLF in self-play  105  100  95 Deterministic Wall game on 5x2 grid, second version. [sent-1527, score-0.705]
</p><p>89 5e+06  4e+06  (b) Rmax attacks WoLF  Figure 15: Modiﬁed 5 × 2 wall game – a. [sent-1536, score-0.379]
</p><p>90 15a) is slower than its convergence on the ﬁrst reward structure, yet still faster than the other algorithms in self play. [sent-1541, score-0.296]
</p><p>91 In the case of a tag, C receives a reward of 40, E receives a reward of 0 and the players’ positions are unchanged. [sent-1555, score-0.45]
</p><p>92 From state (0, 2), (0, 3) the players can transit to state (0, 3), (0, 2) by the joint action right,left without the occurrence of a tag. [sent-1582, score-0.354]
</p><p>93 5 19 average reward per step  average reward per step  19 18. [sent-1590, score-0.576]
</p><p>94 5  FoeQ WoLF minimax solution  Stochastic Catch game on 2x4 grid with blocked corner. [sent-1595, score-0.313]
</p><p>95 5  1e+06  (a) Rmax in self play (ﬁrst 106 rounds)  Stochastic Catch game on 2x4 grid with blocked corner. [sent-1599, score-0.346]
</p><p>96 17 16 15  WoLF catches RmaxK50 WoLF catches RmaxK100 WoLF catches RmaxK200 RmaxK50 catches WoLF RmaxK100 catches WoLF RmaxK200 catches WoLF minimax solution  0  500000  1e+06  1. [sent-1605, score-0.309]
</p><p>97 5e+06  4e+06  (d) Rmax vs WoLF  Figure 17: 2 × 4 tag game – average reward per step  stochastic environmental dynamic. [sent-1614, score-0.586]
</p><p>98 When FoeQ plays Escaper against Rmax, FoeQ learns better policies when Rmax uses larger values of K1 (Table 6) and receives a greater average reward (Fig. [sent-1615, score-0.363]
</p><p>99 Optimizing behavior during learning introduces a tradeoff between exercising opponents’ exploration in order to gain higher return (may-be at the expense of fast convergence to some ﬁxed learning target), to exercising opponents’ exploration for joint exploration. [sent-1810, score-0.359]
</p><p>100 • giveActions(int[] actions) - receives the action choices of the players and updates the state variables to characterize the new state of the environment. [sent-1839, score-0.335]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('rmax', 0.536), ('foeq', 0.318), ('wolf', 0.267), ('oal', 0.24), ('reward', 0.225), ('game', 0.184), ('friendq', 0.176), ('agents', 0.17), ('players', 0.167), ('policy', 0.151), ('exploration', 0.145), ('rx', 0.126), ('attacker', 0.109), ('attacks', 0.107), ('policies', 0.094), ('minimax', 0.093), ('fq', 0.092), ('wall', 0.088), ('action', 0.088), ('cfq', 0.085), ('actions', 0.075), ('agent', 0.075), ('wf', 0.075), ('bab', 0.075), ('self', 0.071), ('ames', 0.071), ('marl', 0.071), ('rafman', 0.071), ('games', 0.069), ('round', 0.064), ('brafman', 0.064), ('dfq', 0.06), ('fssgs', 0.06), ('mgs', 0.06), ('ulti', 0.06), ('rounds', 0.059), ('stand', 0.057), ('cisgs', 0.056), ('play', 0.055), ('tochastic', 0.054), ('stochastic', 0.053), ('deterministic', 0.052), ('payoffs', 0.051), ('reinforcement', 0.046), ('littman', 0.045), ('tennenholtz', 0.042), ('vs', 0.042), ('rl', 0.042), ('per', 0.042), ('mode', 0.04), ('state', 0.04), ('player', 0.039), ('boltzman', 0.039), ('defender', 0.039), ('dfqeed', 0.039), ('glielps', 0.039), ('ihdr', 0.039), ('mq', 0.039), ('ufq', 0.039), ('catches', 0.036), ('grid', 0.036), ('entries', 0.033), ('sgs', 0.033), ('playing', 0.032), ('traces', 0.032), ('coordination', 0.032), ('rewards', 0.032), ('bap', 0.032), ('trials', 0.03), ('eligibility', 0.03), ('ctitious', 0.03), ('exploitation', 0.03), ('payoff', 0.029), ('decay', 0.029), ('cisg', 0.028), ('sg', 0.028), ('spi', 0.028), ('heterogeneous', 0.027), ('return', 0.027), ('ke', 0.025), ('modelq', 0.025), ('transition', 0.024), ('equilibrium', 0.024), ('behavior', 0.023), ('plays', 0.023), ('converge', 0.022), ('exploratory', 0.022), ('opponent', 0.022), ('referee', 0.022), ('pro', 0.022), ('private', 0.021), ('greedy', 0.021), ('average', 0.021), ('strategic', 0.021), ('environments', 0.021), ('coordinated', 0.021), ('response', 0.02), ('joint', 0.019), ('tag', 0.019), ('veloso', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="65-tfidf-1" href="./jmlr-2008-Multi-Agent_Reinforcement_Learning_in_Common_Interest_and_Fixed_Sum_Stochastic_Games%3A_An_Experimental_Study.html">65 jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</a></p>
<p>Author: Avraham Bab, Ronen I. Brafman</p><p>Abstract: Multi Agent Reinforcement Learning (MARL) has received continually growing attention in the past decade. Many algorithms that vary in their approaches to the different subtasks of MARL have been developed. However, the theoretical convergence results for these algorithms do not give a clue as to their practical performance nor supply insights to the dynamics of the learning process itself. This work is a comprehensive empirical study conducted on MGS, a simulation system developed for this purpose. It surveys the important algorithms in the ﬁeld, demonstrates the strengths and weaknesses of the different approaches to MARL through application of FriendQ, OAL, WoLF, FoeQ, Rmax, and other algorithms to a variety of fully cooperative and fully competitive domains in self and heterogeneous play, and supplies an informal analysis of the resulting learning processes. The results can aid in the design of new learning algorithms, in matching existing algorithms to speciﬁc tasks, and may guide further research and formal analysis of the learning processes. Keywords: reinforcement learning, multi-agent reinforcement learning, stochastic games</p><p>2 0.18187638 <a title="65-tfidf-2" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>Author: Liviu Panait, Karl Tuyls, Sean Luke</p><p>Abstract: This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the beneﬁts of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction</p><p>3 0.11657625 <a title="65-tfidf-3" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>Author: Rémi Munos, Csaba Szepesvári</p><p>Abstract: In this paper we develop a theoretical analysis of the performance of sampling-based ﬁtted value iteration (FVI) to solve inﬁnite state-space, discounted-reward Markovian decision processes (MDPs) under the assumption that a generative model of the environment is available. Our main results come in the form of ﬁnite-time bounds on the performance of two versions of sampling-based FVI. The convergence rate results obtained allow us to show that both versions of FVI are well behaving in the sense that by using a sufﬁciently large number of samples for a large class of MDPs, arbitrary good performance can be achieved with high probability. An important feature of our proof technique is that it permits the study of weighted L p -norm performance bounds. As a result, our technique applies to a large class of function-approximation methods (e.g., neural networks, adaptive regression trees, kernel machines, locally weighted learning), and our bounds scale well with the effective horizon of the MDP. The bounds show a dependence on the stochastic stability properties of the MDP: they scale with the discounted-average concentrability of the future-state distributions. They also depend on a new measure of the approximation power of the function space, the inherent Bellman residual, which reﬂects how well the function space is “aligned” with the dynamics and rewards of the MDP. The conditions of the main result, as well as the concepts introduced in the analysis, are extensively discussed and compared to previous theoretical results. Numerical experiments are used to substantiate the theoretical ﬁndings. Keywords: ﬁtted value iteration, discounted Markovian decision processes, generative model, reinforcement learning, supervised learning, regression, Pollard’s inequality, statistical learning theory, optimal control</p><p>4 0.09314359 <a title="65-tfidf-4" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>Author: Balázs Csanád Csáji, László Monostori</p><p>Abstract: The paper investigates the possibility of applying value function based reinforcement learning (RL) methods in cases when the environment may change over time. First, theorems are presented which show that the optimal value function of a discounted Markov decision process (MDP) Lipschitz continuously depends on the immediate-cost function and the transition-probability function. Dependence on the discount factor is also analyzed and shown to be non-Lipschitz. Afterwards, the concept of (ε, δ)-MDPs is introduced, which is a generalization of MDPs and ε-MDPs. In this model the environment may change over time, more precisely, the transition function and the cost function may vary from time to time, but the changes must be bounded in the limit. Then, learning algorithms in changing environments are analyzed. A general relaxed convergence theorem for stochastic iterative algorithms is presented. We also demonstrate the results through three classical RL methods: asynchronous value iteration, Q-learning and temporal difference learning. Finally, some numerical experiments concerning changing environments are presented. Keywords: Markov decision processes, reinforcement learning, changing environments, (ε, δ)MDPs, value function bounds, stochastic iterative algorithms</p><p>5 0.079214245 <a title="65-tfidf-5" href="./jmlr-2008-Learning_Control_Knowledge_for_Forward_Search_Planning.html">49 jmlr-2008-Learning Control Knowledge for Forward Search Planning</a></p>
<p>Author: Sungwook Yoon, Alan Fern, Robert Givan</p><p>Abstract: A number of today’s state-of-the-art planners are based on forward state-space search. The impressive performance can be attributed to progress in computing domain independent heuristics that perform well across many domains. However, it is easy to ﬁnd domains where such heuristics provide poor guidance, leading to planning failure. Motivated by such failures, the focus of this paper is to investigate mechanisms for learning domain-speciﬁc knowledge to better control forward search in a given domain. While there has been a large body of work on inductive learning of control knowledge for AI planning, there is a void of work aimed at forward-state-space search. One reason for this may be that it is challenging to specify a knowledge representation for compactly representing important concepts across a wide range of domains. One of the main contributions of this work is to introduce a novel feature space for representing such control knowledge. The key idea is to deﬁne features in terms of information computed via relaxed plan extraction, which has been a major source of success for non-learning planners. This gives a new way of leveraging relaxed planning techniques in the context of learning. Using this feature space, we describe three forms of control knowledge—reactive policies (decision list rules and measures of progress) and linear heuristics—and show how to learn them and incorporate them into forward state-space search. Our empirical results show that our approaches are able to surpass state-of-the-art nonlearning planners across a wide range of planning competition domains. Keywords: planning, machine learning, knowledge representation, search</p><p>6 0.052355096 <a title="65-tfidf-6" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>7 0.029820126 <a title="65-tfidf-7" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>8 0.029140729 <a title="65-tfidf-8" href="./jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">80 jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>9 0.025230242 <a title="65-tfidf-9" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>10 0.020991592 <a title="65-tfidf-10" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>11 0.020321971 <a title="65-tfidf-11" href="./jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">53 jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>12 0.017814329 <a title="65-tfidf-12" href="./jmlr-2008-Non-Parametric_Modeling_of_Partially_Ranked_Data.html">69 jmlr-2008-Non-Parametric Modeling of Partially Ranked Data</a></p>
<p>13 0.01710826 <a title="65-tfidf-13" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>14 0.016937269 <a title="65-tfidf-14" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>15 0.015879387 <a title="65-tfidf-15" href="./jmlr-2008-Stationary_Features_and_Cat_Detection.html">87 jmlr-2008-Stationary Features and Cat Detection</a></p>
<p>16 0.015573189 <a title="65-tfidf-16" href="./jmlr-2008-Learning_to_Select_Features_using_their_Properties.html">54 jmlr-2008-Learning to Select Features using their Properties</a></p>
<p>17 0.015021327 <a title="65-tfidf-17" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>18 0.014505876 <a title="65-tfidf-18" href="./jmlr-2008-Linear-Time_Computation_of_Similarity_Measures_for_Sequential_Data.html">55 jmlr-2008-Linear-Time Computation of Similarity Measures for Sequential Data</a></p>
<p>19 0.014225311 <a title="65-tfidf-19" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>20 0.01421977 <a title="65-tfidf-20" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.101), (1, -0.027), (2, -0.092), (3, -0.075), (4, -0.314), (5, -0.305), (6, -0.185), (7, -0.029), (8, 0.068), (9, 0.109), (10, 0.019), (11, -0.01), (12, 0.073), (13, 0.042), (14, 0.106), (15, 0.006), (16, 0.137), (17, 0.129), (18, 0.031), (19, 0.032), (20, 0.019), (21, -0.017), (22, 0.081), (23, 0.079), (24, -0.022), (25, -0.094), (26, 0.082), (27, -0.035), (28, 0.14), (29, 0.094), (30, -0.146), (31, 0.021), (32, 0.006), (33, -0.042), (34, -0.078), (35, -0.053), (36, 0.179), (37, -0.019), (38, -0.095), (39, 0.037), (40, -0.053), (41, -0.119), (42, -0.017), (43, -0.053), (44, 0.126), (45, 0.094), (46, -0.163), (47, 0.131), (48, -0.048), (49, -0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97911924 <a title="65-lsi-1" href="./jmlr-2008-Multi-Agent_Reinforcement_Learning_in_Common_Interest_and_Fixed_Sum_Stochastic_Games%3A_An_Experimental_Study.html">65 jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</a></p>
<p>Author: Avraham Bab, Ronen I. Brafman</p><p>Abstract: Multi Agent Reinforcement Learning (MARL) has received continually growing attention in the past decade. Many algorithms that vary in their approaches to the different subtasks of MARL have been developed. However, the theoretical convergence results for these algorithms do not give a clue as to their practical performance nor supply insights to the dynamics of the learning process itself. This work is a comprehensive empirical study conducted on MGS, a simulation system developed for this purpose. It surveys the important algorithms in the ﬁeld, demonstrates the strengths and weaknesses of the different approaches to MARL through application of FriendQ, OAL, WoLF, FoeQ, Rmax, and other algorithms to a variety of fully cooperative and fully competitive domains in self and heterogeneous play, and supplies an informal analysis of the resulting learning processes. The results can aid in the design of new learning algorithms, in matching existing algorithms to speciﬁc tasks, and may guide further research and formal analysis of the learning processes. Keywords: reinforcement learning, multi-agent reinforcement learning, stochastic games</p><p>2 0.76262534 <a title="65-lsi-2" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>Author: Liviu Panait, Karl Tuyls, Sean Luke</p><p>Abstract: This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the beneﬁts of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction</p><p>3 0.35580057 <a title="65-lsi-3" href="./jmlr-2008-Learning_Control_Knowledge_for_Forward_Search_Planning.html">49 jmlr-2008-Learning Control Knowledge for Forward Search Planning</a></p>
<p>Author: Sungwook Yoon, Alan Fern, Robert Givan</p><p>Abstract: A number of today’s state-of-the-art planners are based on forward state-space search. The impressive performance can be attributed to progress in computing domain independent heuristics that perform well across many domains. However, it is easy to ﬁnd domains where such heuristics provide poor guidance, leading to planning failure. Motivated by such failures, the focus of this paper is to investigate mechanisms for learning domain-speciﬁc knowledge to better control forward search in a given domain. While there has been a large body of work on inductive learning of control knowledge for AI planning, there is a void of work aimed at forward-state-space search. One reason for this may be that it is challenging to specify a knowledge representation for compactly representing important concepts across a wide range of domains. One of the main contributions of this work is to introduce a novel feature space for representing such control knowledge. The key idea is to deﬁne features in terms of information computed via relaxed plan extraction, which has been a major source of success for non-learning planners. This gives a new way of leveraging relaxed planning techniques in the context of learning. Using this feature space, we describe three forms of control knowledge—reactive policies (decision list rules and measures of progress) and linear heuristics—and show how to learn them and incorporate them into forward state-space search. Our empirical results show that our approaches are able to surpass state-of-the-art nonlearning planners across a wide range of planning competition domains. Keywords: planning, machine learning, knowledge representation, search</p><p>4 0.29671717 <a title="65-lsi-4" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>Author: Rémi Munos, Csaba Szepesvári</p><p>Abstract: In this paper we develop a theoretical analysis of the performance of sampling-based ﬁtted value iteration (FVI) to solve inﬁnite state-space, discounted-reward Markovian decision processes (MDPs) under the assumption that a generative model of the environment is available. Our main results come in the form of ﬁnite-time bounds on the performance of two versions of sampling-based FVI. The convergence rate results obtained allow us to show that both versions of FVI are well behaving in the sense that by using a sufﬁciently large number of samples for a large class of MDPs, arbitrary good performance can be achieved with high probability. An important feature of our proof technique is that it permits the study of weighted L p -norm performance bounds. As a result, our technique applies to a large class of function-approximation methods (e.g., neural networks, adaptive regression trees, kernel machines, locally weighted learning), and our bounds scale well with the effective horizon of the MDP. The bounds show a dependence on the stochastic stability properties of the MDP: they scale with the discounted-average concentrability of the future-state distributions. They also depend on a new measure of the approximation power of the function space, the inherent Bellman residual, which reﬂects how well the function space is “aligned” with the dynamics and rewards of the MDP. The conditions of the main result, as well as the concepts introduced in the analysis, are extensively discussed and compared to previous theoretical results. Numerical experiments are used to substantiate the theoretical ﬁndings. Keywords: ﬁtted value iteration, discounted Markovian decision processes, generative model, reinforcement learning, supervised learning, regression, Pollard’s inequality, statistical learning theory, optimal control</p><p>5 0.2671012 <a title="65-lsi-5" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>Author: Balázs Csanád Csáji, László Monostori</p><p>Abstract: The paper investigates the possibility of applying value function based reinforcement learning (RL) methods in cases when the environment may change over time. First, theorems are presented which show that the optimal value function of a discounted Markov decision process (MDP) Lipschitz continuously depends on the immediate-cost function and the transition-probability function. Dependence on the discount factor is also analyzed and shown to be non-Lipschitz. Afterwards, the concept of (ε, δ)-MDPs is introduced, which is a generalization of MDPs and ε-MDPs. In this model the environment may change over time, more precisely, the transition function and the cost function may vary from time to time, but the changes must be bounded in the limit. Then, learning algorithms in changing environments are analyzed. A general relaxed convergence theorem for stochastic iterative algorithms is presented. We also demonstrate the results through three classical RL methods: asynchronous value iteration, Q-learning and temporal difference learning. Finally, some numerical experiments concerning changing environments are presented. Keywords: Markov decision processes, reinforcement learning, changing environments, (ε, δ)MDPs, value function bounds, stochastic iterative algorithms</p><p>6 0.2168415 <a title="65-lsi-6" href="./jmlr-2008-Accelerated_Neural_Evolution_through_Cooperatively_Coevolved_Synapses.html">8 jmlr-2008-Accelerated Neural Evolution through Cooperatively Coevolved Synapses</a></p>
<p>7 0.15851252 <a title="65-lsi-7" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>8 0.14634871 <a title="65-lsi-8" href="./jmlr-2008-Ranking_Individuals_by_Group_Comparisons.html">80 jmlr-2008-Ranking Individuals by Group Comparisons</a></p>
<p>9 0.14632268 <a title="65-lsi-9" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>10 0.11786191 <a title="65-lsi-10" href="./jmlr-2008-Stationary_Features_and_Cat_Detection.html">87 jmlr-2008-Stationary Features and Cat Detection</a></p>
<p>11 0.10917259 <a title="65-lsi-11" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>12 0.090355009 <a title="65-lsi-12" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>13 0.088014886 <a title="65-lsi-13" href="./jmlr-2008-Generalization_from_Observed_to_Unobserved_Features_by_Clustering.html">38 jmlr-2008-Generalization from Observed to Unobserved Features by Clustering</a></p>
<p>14 0.084553279 <a title="65-lsi-14" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>15 0.082962446 <a title="65-lsi-15" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>16 0.080807164 <a title="65-lsi-16" href="./jmlr-2008-Active_Learning_of_Causal_Networks_with_Intervention_Experiments_and_Optimal_Designs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">10 jmlr-2008-Active Learning of Causal Networks with Intervention Experiments and Optimal Designs    (Special Topic on Causality)</a></p>
<p>17 0.080346532 <a title="65-lsi-17" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>18 0.080339558 <a title="65-lsi-18" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>19 0.077299647 <a title="65-lsi-19" href="./jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">53 jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>20 0.075226799 <a title="65-lsi-20" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.011), (5, 0.016), (24, 0.023), (40, 0.023), (51, 0.51), (54, 0.077), (58, 0.017), (66, 0.032), (76, 0.021), (88, 0.068), (92, 0.024), (94, 0.036), (99, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.85275906 <a title="65-lda-1" href="./jmlr-2008-Multi-Agent_Reinforcement_Learning_in_Common_Interest_and_Fixed_Sum_Stochastic_Games%3A_An_Experimental_Study.html">65 jmlr-2008-Multi-Agent Reinforcement Learning in Common Interest and Fixed Sum Stochastic Games: An Experimental Study</a></p>
<p>Author: Avraham Bab, Ronen I. Brafman</p><p>Abstract: Multi Agent Reinforcement Learning (MARL) has received continually growing attention in the past decade. Many algorithms that vary in their approaches to the different subtasks of MARL have been developed. However, the theoretical convergence results for these algorithms do not give a clue as to their practical performance nor supply insights to the dynamics of the learning process itself. This work is a comprehensive empirical study conducted on MGS, a simulation system developed for this purpose. It surveys the important algorithms in the ﬁeld, demonstrates the strengths and weaknesses of the different approaches to MARL through application of FriendQ, OAL, WoLF, FoeQ, Rmax, and other algorithms to a variety of fully cooperative and fully competitive domains in self and heterogeneous play, and supplies an informal analysis of the resulting learning processes. The results can aid in the design of new learning algorithms, in matching existing algorithms to speciﬁc tasks, and may guide further research and formal analysis of the learning processes. Keywords: reinforcement learning, multi-agent reinforcement learning, stochastic games</p><p>2 0.81690919 <a title="65-lda-2" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>Author: Zach Jorgensen, Yan Zhou, Meador Inge</p><p>Abstract: Statistical spam ﬁlters are known to be vulnerable to adversarial attacks. One of the more common adversarial attacks, known as the good word attack, thwarts spam ﬁlters by appending to spam messages sets of “good” words, which are words that are common in legitimate email but rare in spam. We present a counterattack strategy that attempts to differentiate spam from legitimate email in the input space by transforming each email into a bag of multiple segments, and subsequently applying multiple instance logistic regression on the bags. We treat each segment in the bag as an instance. An email is classiﬁed as spam if at least one instance in the corresponding bag is spam, and as legitimate if all the instances in it are legitimate. We show that a classiﬁer using our multiple instance counterattack strategy is more robust to good word attacks than its single instance counterpart and other single instance learners commonly used in the spam ﬁltering domain. Keywords: spam ﬁltering, multiple instance learning, good word attack, adversarial learning</p><p>3 0.2249752 <a title="65-lda-3" href="./jmlr-2008-Discriminative_Learning_of_Max-Sum_Classifiers.html">30 jmlr-2008-Discriminative Learning of Max-Sum Classifiers</a></p>
<p>Author: Vojtěch Franc, Bogdan Savchynskyy</p><p>Abstract: The max-sum classiﬁer predicts n-tuple of labels from n-tuple of observable variables by maximizing a sum of quality functions deﬁned over neighbouring pairs of labels and observable variables. Predicting labels as MAP assignments of a Random Markov Field is a particular example of the max-sum classiﬁer. Learning parameters of the max-sum classiﬁer is a challenging problem because even computing the response of such classiﬁer is NP-complete in general. Estimating parameters using the Maximum Likelihood approach is feasible only for a subclass of max-sum classiﬁers with an acyclic structure of neighbouring pairs. Recently, the discriminative methods represented by the perceptron and the Support Vector Machines, originally designed for binary linear classiﬁers, have been extended for learning some subclasses of the max-sum classiﬁer. Besides the max-sum classiﬁers with the acyclic neighbouring structure, it has been shown that the discriminative learning is possible even with arbitrary neighbouring structure provided the quality functions fulﬁll some additional constraints. In this article, we extend the discriminative approach to other three classes of max-sum classiﬁers with an arbitrary neighbourhood structure. We derive learning algorithms for two subclasses of max-sum classiﬁers whose response can be computed in polynomial time: (i) the max-sum classiﬁers with supermodular quality functions and (ii) the max-sum classiﬁers whose response can be computed exactly by a linear programming relaxation. Moreover, we show that the learning problem can be approximately solved even for a general max-sum classiﬁer. Keywords: max-xum classiﬁer, hidden Markov networks, support vector machines</p><p>4 0.22463402 <a title="65-lda-4" href="./jmlr-2008-Theoretical_Advantages_of_Lenient_Learners%3A__An_Evolutionary_Game_Theoretic_Perspective.html">90 jmlr-2008-Theoretical Advantages of Lenient Learners:  An Evolutionary Game Theoretic Perspective</a></p>
<p>Author: Liviu Panait, Karl Tuyls, Sean Luke</p><p>Abstract: This paper presents the dynamics of multiple learning agents from an evolutionary game theoretic perspective. We provide replicator dynamics models for cooperative coevolutionary algorithms and for traditional multiagent Q-learning, and we extend these differential equations to account for lenient learners: agents that forgive possible mismatched teammate actions that resulted in low rewards. We use these extended formal models to study the convergence guarantees for these algorithms, and also to visualize the basins of attraction to optimal and suboptimal solutions in two benchmark coordination problems. The paper demonstrates that lenience provides learners with more accurate information about the beneﬁts of performing their actions, resulting in higher likelihood of convergence to the globally optimal solution. In addition, the analysis indicates that the choice of learning algorithm has an insigniﬁcant impact on the overall performance of multiagent learning algorithms; rather, the performance of these algorithms depends primarily on the level of lenience that the agents exhibit to one another. Finally, the research herein supports the strength and generality of evolutionary game theory as a backbone for multiagent learning. Keywords: multiagent learning, reinforcement learning, cooperative coevolution, evolutionary game theory, formal models, visualization, basins of attraction</p><p>5 0.21665432 <a title="65-lda-5" href="./jmlr-2008-Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">33 jmlr-2008-Evidence Contrary to the Statistical View of Boosting</a></p>
<p>Author: David Mease, Abraham Wyner</p><p>Abstract: The statistical perspective on boosting algorithms focuses on optimization, drawing parallels with maximum likelihood estimation for logistic regression. In this paper we present empirical evidence that raises questions about this view. Although the statistical perspective provides a theoretical framework within which it is possible to derive theorems and create new algorithms in general contexts, we show that there remain many unanswered important questions. Furthermore, we provide examples that reveal crucial ﬂaws in the many practical suggestions and new methods that are derived from the statistical view. We perform carefully designed experiments using simple simulation models to illustrate some of these ﬂaws and their practical consequences. Keywords: boosting algorithms, LogitBoost, AdaBoost</p><p>6 0.21331181 <a title="65-lda-6" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>7 0.20964779 <a title="65-lda-7" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>8 0.2082817 <a title="65-lda-8" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>9 0.2018584 <a title="65-lda-9" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>10 0.20159633 <a title="65-lda-10" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>11 0.19712736 <a title="65-lda-11" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>12 0.19308576 <a title="65-lda-12" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>13 0.19287679 <a title="65-lda-13" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>14 0.19265121 <a title="65-lda-14" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>15 0.19147821 <a title="65-lda-15" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>16 0.19054061 <a title="65-lda-16" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>17 0.19013514 <a title="65-lda-17" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>18 0.18929666 <a title="65-lda-18" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>19 0.18904084 <a title="65-lda-19" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>20 0.18853626 <a title="65-lda-20" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
