<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>7 jmlr-2008-A Tutorial on Conformal Prediction</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-7" href="#">jmlr2008-7</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>7 jmlr-2008-A Tutorial on Conformal Prediction</h1>
<br/><p>Source: <a title="jmlr-2008-7-pdf" href="http://jmlr.org/papers/volume9/shafer08a/shafer08a.pdf">pdf</a></p><p>Author: Glenn Shafer, Vladimir Vovk</p><p>Abstract: Conformal prediction uses past experience to determine precise levels of conÄ?Ĺš dence in new predictions. Given an error probability ĂŽÄž, together with a method that makes a prediction y of a label Ă&lsaquo;&dagger; y, it produces a set of labels, typically containing y, that also contains y with probability 1 Ă˘&circ;&rsquo; ĂŽÄž. Ă&lsaquo;&dagger; Conformal prediction can be applied to any method for producing y: a nearest-neighbor method, a Ă&lsaquo;&dagger; support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 Ă˘&circ;&rsquo; ĂŽÄž of the time, even though they are based on an accumulating data set rather than on independent data sets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005). Keywords: conÄ?Ĺš dence, on-line compression modeling, on-line learning, prediction regions</p><p>Reference: <a title="jmlr-2008-7-reference" href="../jmlr2008_reference/jmlr-2008-A_Tutorial_on_Conformal_Prediction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Starting from the method for point prediction, we construct a nonconformity measure, which measures how unusual an example looks relative to previous examples, and the conformal algorithm turns this nonconformity measure into prediction regions. [sent-41, score-1.15]
</p><p>2 Given a nonconformity measure, the conformal algorithm produces a prediction region Ă&#x17D;&ldquo; Ă&#x17D;Äž for every probability of error Ă&#x17D;Äž. [sent-42, score-0.814]
</p><p>3 Ĺš dence for a 95% conformal prediction region is valid under exchangeability, no matter what the probability distribution the examples follow and no matter what nonconformity measure is used to construct the conformal prediction region. [sent-87, score-1.37]
</p><p>4 Ĺš ciency of conformal prediction will depend on the probability distribution and the nonconformity measure. [sent-89, score-0.773]
</p><p>5 Fisher, who explained how to give a 95% prediction interval for zn based on z1 , . [sent-136, score-0.753]
</p><p>6 But the simplicity of the set-up where we predict zn from z1 , . [sent-147, score-0.664]
</p><p>7 The natural point predictor for zn is the average so far: 1 nĂ˘&circ;&rsquo;1 znĂ˘&circ;&rsquo;1 := Ă˘&circ;&lsquo; zi , n Ă˘&circ;&rsquo; 1 i=1 but we want to give an interval that will contain zn 95% of the time. [sent-161, score-1.438]
</p><p>8 nĂ˘&circ;&rsquo;1  (1)  Fisher based this procedure on the fact that zn Ă˘&circ;&rsquo; znĂ˘&circ;&rsquo;1 snĂ˘&circ;&rsquo;1  nĂ˘&circ;&rsquo;1 n  has the t-distribution with n Ă˘&circ;&rsquo; 2 degrees of freedom, which is symmetric about 0. [sent-174, score-0.66]
</p><p>9 This implies that (1) will contain zn with probability 95% regardless of the values of the mean and variance. [sent-175, score-0.642]
</p><p>10 025 Ă˘&permil;Â¤ zn Ă˘&permil;Â¤ znĂ˘&circ;&rsquo;1 + tnĂ˘&circ;&rsquo;2 snĂ˘&circ;&rsquo;1 nĂ˘&circ;&rsquo;1  n nĂ˘&circ;&rsquo;1  (2)  for successive n are probabilistically independent in spite of the overlap. [sent-217, score-0.671]
</p><p>11 Before any of the zi are observed, the event (2) involves multiple uncertainties: z nĂ˘&circ;&rsquo;1 , snĂ˘&circ;&rsquo;1 , and zn are all uncertain. [sent-242, score-0.746]
</p><p>12 Ĺš rst n Ă˘&circ;&rsquo; 1 examples that we calculate znĂ˘&circ;&rsquo;1 and snĂ˘&circ;&rsquo;1 and then calculate the interval (1), and we would like to be able to say at this point that there is still a 95% probability that zn will be in (1). [sent-246, score-0.762]
</p><p>13 For any n, 1 Ă˘&permil;Â¤ n Ă˘&permil;Â¤ N, zn is independent of zn+1 , . [sent-526, score-0.642]
</p><p>14 , zn and for any bag B of size n, k Pr (zn = a | z1 , . [sent-532, score-0.833]
</p><p>15 , zn = B) = , (4) n where k is the number of times a occurs in B. [sent-535, score-0.642]
</p><p>16 , zn symbolize drawing an example zn out at random, leaving the smaller bag z1 , . [sent-589, score-1.475]
</p><p>17 , zn , and for simplicity, we set the initial capital KN equal to $1. [sent-605, score-0.695]
</p><p>18 Ă˘&euro;Ë&tilde; An event E is an n-event, where 1 Ă˘&permil;Â¤ n Ă˘&permil;Â¤ N, if its happening or failing is determined by the value of zn and the value of the bag z1 , . [sent-637, score-0.862]
</p><p>19 Conformal Prediction under Exchangeability We are now in a position to state the conformal algorithm under exchangeability and explain why it produces valid nested prediction regions. [sent-683, score-0.657]
</p><p>20 For some readers, the simplicity of the conformal algorithm may be obscured by its generality and the scope of our preliminary discussion of nonconformity measures. [sent-723, score-0.699]
</p><p>21 1 Nonconformity Measures The starting point for conformal prediction is what we call a nonconformity measure, a real-valued function A(B, z) that measures how different an example z is from the examples in a bag B. [sent-733, score-0.995]
</p><p>22 The conformal algorithm assumes that a nonconformity measure has been chosen. [sent-734, score-0.719]
</p><p>23 A method z(B) for obtaining a point prediction z for a new example from a bag B of old examples Ă&lsaquo;&dagger; Ă&lsaquo;&dagger; usually leads naturally to a nonconformity measure A. [sent-738, score-0.717]
</p><p>24 z  (7)  The prediction regions produced by the conformal algorithm do not change when the nonconformity measure A is transformed monotonically. [sent-742, score-0.84]
</p><p>25 For simplicity in stating this algorithm, we provisionally use the symbol zn for z, as if we were assuming that zn is in fact equal to z. [sent-799, score-1.313]
</p><p>26 The event that our nth prediction is an error, zn Ă˘&circ;&circ; Ă&#x17D;Ĺ&sbquo;Ă&#x17D;Äž (z1 , . [sent-872, score-0.745]
</p><p>27 This is an n-event, because the / value of pzn is determined by zn and the bag z1 , . [sent-876, score-0.833]
</p><p>28 Ă˘&circ;&rsquo; zi = 20 20  (14)  Under the hypothesis that z is the actual value of zn , these 20 numbers are exchangeable. [sent-908, score-0.74]
</p><p>29 , zn \ zi , zi ), (15) apparently signaling that we do not want to include zi in the bag to which it is compared. [sent-925, score-1.058]
</p><p>30 , zn , zi )  (16)  in the conformal algorithm and using A(B, z) := |zB Ă˘&circ;&rsquo; z| . [sent-930, score-1.081]
</p><p>31 Ĺš ning nonconformity scores, (15) and (16), are equivalent, inasmuch as whatever we can get with one of them we can get from the other by changing the nonconformity measure. [sent-932, score-0.695]
</p><p>32 For simplicity in stating this algorithm, we provisionally use the symbol zn for (xn , y), as if we were assuming that yn is in fact equal to y. [sent-957, score-0.758]
</p><p>33 , znĂ˘&circ;&rsquo;1 ) Ă˘&Scaron;&dagger; Z,  (17)  contains the new example zn = (xn , yn ) at least 95% of the time. [sent-1000, score-0.729]
</p><p>34 We calculate conformal prediction regions using three different nonconformity measures: one based on distance to the nearest neighbors, one based on distance to the species average, and one based on a support-vector machine. [sent-1029, score-0.909]
</p><p>35 08 1  Table 2: Conformal prediction of iris species from sepal length, using three different nonconformity measures. [sent-1160, score-0.646]
</p><p>36 For each nonconformity measure, we calculate nonconformity scores under each hypothesis, y25 = s and y25 = v. [sent-1166, score-0.721]
</p><p>37 A close look at the nonconformity scores reveals that it is being perceived as unusual simply because 2/3 of the plants have other plants in the sample with exactly the same sepal length, whereas there is no other plant with the sepal length 6. [sent-1192, score-0.827]
</p><p>38 Ĺš ned by A(B, (x, y)) = |xBĂ˘&circ;Ĺ&#x17E;  (x,y) ,y Ă˘&circ;&rsquo; x|,  (20)  where xB,y denotes the average sepal length of all plants of species y in the bag B, and B Ă˘&circ;Ĺ&#x17E; z denotes the bag obtained by adding z to B. [sent-1232, score-0.665]
</p><p>39 Ă&lsaquo;&dagger; Ă&lsaquo;&dagger; The fourth column of Table 4 gives the nonconformity scores for our sample using this nonconformity measure. [sent-1563, score-0.695]
</p><p>40 Ĺš nding conformal prediction intervals using a least-squares or ridge-regression nonconformity measure with an object space of any Ä? [sent-1631, score-0.817]
</p><p>41 , zn are exchangeable, Pr{zn Ă˘&circ;&circ; Ă&#x17D;Ĺ&sbquo;Ă&#x17D;Äž ( z1 , . [sent-1649, score-0.642]
</p><p>42 But as we now demonstrate, any Ă&#x17D;Ĺ&sbquo; satisfying them can be improved on by a conformal predictor: there always exists a nonconformity measure A such that the predictor Ă&#x17D;Ĺ&sbquo; A constructed from A by the conformal algorithm satisÄ? [sent-1658, score-1.125]
</p><p>43 , an has an equal probability of being zn , and in this case, (22) is a mistake. [sent-1681, score-0.642]
</p><p>44 Given the region predictor Ă&#x17D;Ĺ&sbquo;, what nonconformity measure will give us a conformal predictor that improves on it? [sent-1838, score-0.844]
</p><p>45 The conformal prediction intervals using the least-squares nonconformity measure are quite close to the standard intervals based on least-squares with normal errors. [sent-1855, score-0.841]
</p><p>46 The conformal predictor Ă&#x17D;Ĺ&sbquo;A obtained from this nonconformity measure, though it agrees with Ă&#x17D;Ĺ&sbquo; on how to rank different z with respect to their nonconformity with B, may produce tighter prediction regions if Ă&#x17D;Ĺ&sbquo; is too conservative in the levels of conÄ? [sent-1858, score-1.197]
</p><p>47 According to the conformal algorithm, (24) means that when we provisionally set zn equal to z and calculate the nonconformity scores Ă&#x17D;Ä&hellip;i = sup{1 Ă˘&circ;&rsquo; Ă&#x17D;Â´ | zi Ă˘&circ;&circ; Ă&#x17D;Ĺ&sbquo;Ă&#x17D;Â´ ( z1 , . [sent-1867, score-1.496]
</p><p>48 , 2005), it is used to illustrate conformal prediction with a number of different nonconformity measures. [sent-1905, score-0.773]
</p><p>49 Ĺš gure records the performance of the 95% conformal predictor using the nearest-neighbor nonconformity measure (10), applied to the USPS data in two ways. [sent-1916, score-0.761]
</p><p>50 On-Line Compression Models In this section, we generalize conformal prediction from the exchangeability model to a whole class of models, which we call on-line compression models. [sent-1936, score-0.696]
</p><p>51 , zn | Ä&#x17D;&fnof;n ), where Ä&#x17D;&fnof;n summarizes the examples z1 , . [sent-1954, score-0.673]
</p><p>52 , zn by z=  1 n Ă˘&circ;&lsquo; zi n i=1  n  and  r2 = Ă˘&circ;&lsquo; (zi Ă˘&circ;&rsquo; z)2 i=1  and gives these summaries to Bill, who does not know z1 , . [sent-1967, score-0.717]
</p><p>53 , zn that is uniform over the possibilities, which form the (n Ă˘&circ;&rsquo; 1)-dimensional sphere of radius r centered around (z, . [sent-1974, score-0.692]
</p><p>54 , zn are exchangeable for all n, then the distribution of z1 , z2 , . [sent-1994, score-0.711]
</p><p>55 For the exchangeability model, conformal prediction is optimal for obtaining prediction regions (Ă&sbquo;Â§4. [sent-2008, score-0.75]
</p><p>56 , zn ) to the bag containing these examples: Ă&#x17D;Ĺ n (z1 , . [sent-2040, score-0.833]
</p><p>57 Given the bag Ä&#x17D;&fnof; n , what are the probabilities for zn and Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;1 ? [sent-2105, score-0.858]
</p><p>58 They are the same as if we drew zn out of Ä&#x17D;&fnof;n at random. [sent-2106, score-0.642]
</p><p>59 In other words, for each z that appears in Ä&#x17D;&fnof;n , there is a probability k/n, where k is the number of times z appears in Ä&#x17D;&fnof;n , that (1) zn = z and (2) Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;1 is the bag obtained by removing one instance of z from Ä&#x17D;&fnof;n . [sent-2107, score-0.833]
</p><p>60 , zn , which has the distribution Pn (Ă&sbquo;Ë&Dagger; | Ä&#x17D;&fnof;n ). [sent-2122, score-0.642]
</p><p>61 In the case of the exchangeability model, we obtain the bag Ä&#x17D;&fnof; i by adding the new example zi to the old bag Ä&#x17D;&fnof;iĂ˘&circ;&rsquo;1 . [sent-2134, score-0.714]
</p><p>62 z1 6  2   znĂ˘&circ;&rsquo;1  z2 6  Ä&#x17D;&fnof;1   6  Ä&#x17D;&fnof;2   Ă&sbquo;Ë&Dagger;Ă&sbquo;Ë&Dagger;Ă&sbquo;Ë&Dagger;   Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;1   zn 6  Ä&#x17D;&fnof;n  Backward probabilities. [sent-2135, score-0.642]
</p><p>63 First we draw zn and Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;1 from Rn (Ă&sbquo;Ë&Dagger; | Ä&#x17D;&fnof;n ), then we draw znĂ˘&circ;&rsquo;1 and Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;2 from RnĂ˘&circ;&rsquo;1 (Ă&sbquo;Ë&Dagger; | Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;1 ), and so on. [sent-2180, score-0.642]
</p><p>64 , zn obtained in this way has the distribution Pn (Ă&sbquo;Ë&Dagger; | Ä&#x17D;&fnof;n ). [sent-2184, score-0.642]
</p><p>65 , zn with the distribution Pn (Ă&sbquo;Ë&Dagger; | Ä&#x17D;&fnof;n ). [sent-2213, score-0.642]
</p><p>66 Another way to verify it, without necessarily exhibiting the one-step kernels, is to verify the conditional independence relations represented by Figure 8: zn (and hence also Ä&#x17D;&fnof;n ) is probabilistically independent of z1 , . [sent-2214, score-0.642]
</p><p>67 Ă&lsaquo;&oelig; In order to state the conformal algorithm, we write Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;1 and zn for random variables with a joint Ă&lsaquo;&oelig; probability distribution given by the one-step kernel R(Ă&sbquo;Ë&Dagger; | Ä&#x17D;&fnof;n ). [sent-2221, score-1.006]
</p><p>68 Set pz := Rn (A(Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;1 , zn ) Ă˘&permil;Ä˝ A(Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;1 , zn ) | Ä&#x17D;&fnof;n ). [sent-2233, score-1.311]
</p><p>69 , zn \ zn in that model, so that Ă&lsaquo;&oelig; Ă&lsaquo;&oelig; A(Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;1 , zn ) = A( z1 , . [sent-2246, score-1.926]
</p><p>70 , zn \ zn , zn ) Ă&lsaquo;&oelig; Ă&lsaquo;&oelig; Ă&lsaquo;&oelig;  (25)  A(Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;1 , zn ) = A( z1 , . [sent-2249, score-2.568]
</p><p>71 , zn ), the random variable zn has equal chances of being any of the zi , so that the Ă&lsaquo;&oelig; probability of (25) being greater than or equal to (26) is simply the fraction of the z i for which A( z1 , . [sent-2256, score-1.359]
</p><p>72 , znĂ˘&circ;&rsquo;1 , zn ), and this is how pz is deÄ? [sent-2262, score-0.669]
</p><p>73 Set py := Rn (A(Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;1 , zn ) Ă˘&permil;Ä˝ A(Ä&#x17D;&fnof;nĂ˘&circ;&rsquo;1 , zn ) | Ä&#x17D;&fnof;n ). [sent-2300, score-1.284]
</p><p>74 The graph in the bottom panel shows the results of the exchangeabilitywithin-label conformal predictor using the same nearest-neighbor nonconformity measure; here the error rate stays close to 5%. [sent-2374, score-0.741]
</p><p>75 , zN , of the form zn = (xn , yn ), where yn is a number and xn is a row vector consisting of p numbers. [sent-2390, score-0.902]
</p><p>76 , zn | Ä&#x17D;&fnof;n ) distributes its probability uniformly over the set of vectors (y1 , . [sent-2422, score-0.642]
</p><p>77 , zn gives Pi (Ă&sbquo;Ë&Dagger; | Ä&#x17D;&fnof;i ), we note that conditioning the uniform distribution on a sphere on values yi+1 = ai+1 , . [sent-2465, score-0.692]
</p><p>78 Ă&lsaquo;&dagger; Proposition 2 When (30) is used as the nonconformity measure, the 1 Ă˘&circ;&rsquo; Ă&#x17D;Äž conformal prediction region for yn is (29), the interval given by the t-distribution in the classical theory. [sent-2528, score-0.961]
</p><p>79 Proof When (30) is used as the nonconformity measure, the test statistic A(Ä&#x17D;&fnof; nĂ˘&circ;&rsquo;1 , zn ) used in the conformal algorithm becomes |yn Ă˘&circ;&rsquo; yn |. [sent-2529, score-1.428]
</p><p>80 4 to establish the validity of conformal prediction in the exchangeability model. [sent-2565, score-0.663]
</p><p>81 , zn , but the equation says that it is not really random, for it is always equal to Ă&#x17D;Äž. [sent-2601, score-0.659]
</p><p>82 Just before he observes zn , he knows the bag z1 , . [sent-2665, score-0.833]
</p><p>83 , zn and can bet on the value of zn at odds corresponding to the probabilities the bag determines: k Pr (zn = a | z1 , . [sent-2668, score-1.552]
</p><p>84 , zn = B) = , n  (33)  where k is the number of times a occurs in B. [sent-2671, score-0.642]
</p><p>85 , zn , zn+1 are independent normal random variables with mean 0 and standard deviation 1, the distribution of the ratio zn+1  (41)  Ă˘&circ;&lsquo;n z2 /n i=1 i  is called the t-distribution with n degrees of freedom. [sent-2738, score-0.66]
</p><p>86 Proof It is straightforward to verify that zn Ă˘&circ;&rsquo; znĂ˘&circ;&rsquo;1 = and s2 = nĂ˘&circ;&rsquo;1  n (zn Ă˘&circ;&rsquo; zn ) nĂ˘&circ;&rsquo;1  (44)  (n Ă˘&circ;&rsquo; 1)s2 n(zn Ă˘&circ;&rsquo; zn )2 n Ă˘&circ;&rsquo; . [sent-2767, score-1.926]
</p><p>87 (47)  The value of (47) is unaffected if all the zi Ă˘&circ;&rsquo; zn are multiplied by a nonzero constant. [sent-2769, score-0.717]
</p><p>88 tn Proof Given zn , we can calculate znĂ˘&circ;&rsquo;1 and snĂ˘&circ;&rsquo;1 from (44) and (45) and then calculate tn from (42). [sent-2774, score-0.846]
</p><p>89 Given znĂ˘&circ;&rsquo;1 and snĂ˘&circ;&rsquo;1 , we can calculate zn from (44) or (45) and then tn from (42). [sent-2775, score-0.744]
</p><p>90 Ĺš xed, this equation expresses tn as a monotonically increasing function of zn ) and then calculate znĂ˘&circ;&rsquo;1 and snĂ˘&circ;&rsquo;1 from (44) and (45). [sent-2778, score-0.744]
</p><p>91 , zn are independent and normal with a common mean and standard deviation, then conditional on zn = w and Ă˘&circ;&lsquo;n (zi Ă˘&circ;&rsquo; zn )2 = r2 , the vector (z1 , . [sent-2786, score-1.926]
</p><p>92 , zn ) is distributed uniformly i=1 over the (n Ă˘&circ;&rsquo; 2)-dimensional sphere of vectors satisfying these conditions (the intersection of the hyperplane zn = w with the (n Ă˘&circ;&rsquo; 1)-dimensional sphere of radius r centered on the point (w, . [sent-2789, score-1.401]
</p><p>93 , zn ) only through zn and Ă˘&circ;&lsquo;n (zi Ă˘&circ;&rsquo;zn )2 , the distribution of (z1 , . [sent-2800, score-1.284]
</p><p>94 , zn ) conditional on zn = w and Ă˘&circ;&lsquo;n (zi Ă˘&circ;&rsquo; i=1 i=1 zn )2 = r2 is uniform over the set of vectors satisfying these conditions. [sent-2803, score-1.926]
</p><p>95 , zn ) is distributed uniformly over the (n Ă˘&circ;&rsquo; 2)-dimensional sphere deÄ? [sent-2807, score-0.692]
</p><p>96 Ĺš ned by the conditions zn = w and Ă˘&circ;&lsquo;n (zi Ă˘&circ;&rsquo; zn )2 = r2 , then tn has the t-distribution with n Ă˘&circ;&rsquo; 2 i=1 degrees of freedom. [sent-2808, score-1.378]
</p><p>97 Lemma 8 says that conditional on zn = w and (nĂ˘&circ;&rsquo;1)s2 = r2 , the vector (z1 , . [sent-2815, score-0.659]
</p><p>98 , zn ) is distributed n uniformly over the sphere of radius r centered on w, . [sent-2818, score-0.692]
</p><p>99 Ĺš ned by the conditions zn = w and Ă˘&circ;&lsquo;n (zi Ă˘&circ;&rsquo; zn )2 = r2 . [sent-2827, score-1.284]
</p><p>100 , zn ) is distributed uniformly over an (n Ă˘&circ;&rsquo; 1)-dimensional sphere in Rn , and so tn has the t-distribution with n Ă˘&circ;&rsquo; 2 degrees of freedom by Lemma 9. [sent-2857, score-0.803]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zn', 0.642), ('conformal', 0.364), ('nonconformity', 0.335), ('vovk', 0.2), ('exchangeability', 0.191), ('bag', 0.191), ('sepal', 0.155), ('bill', 0.118), ('hafer', 0.102), ('onformal', 0.102), ('utorial', 0.102), ('yn', 0.087), ('xn', 0.086), ('tn', 0.076), ('zi', 0.075), ('prediction', 0.074), ('rediction', 0.074), ('exchangeable', 0.069), ('compression', 0.067), ('old', 0.066), ('species', 0.063), ('en', 0.061), ('capital', 0.053), ('joe', 0.053), ('petal', 0.053), ('law', 0.052), ('sphere', 0.05), ('shafer', 0.049), ('hits', 0.048), ('regions', 0.047), ('bn', 0.047), ('plants', 0.045), ('fisher', 0.042), ('predictor', 0.042), ('versicolor', 0.042), ('region', 0.041), ('dence', 0.039), ('kn', 0.039), ('setosa', 0.038), ('interval', 0.037), ('betting', 0.037), ('validity', 0.034), ('sn', 0.033), ('bet', 0.033), ('glenn', 0.033), ('pr', 0.033), ('examples', 0.031), ('successive', 0.029), ('label', 0.029), ('summary', 0.029), ('event', 0.029), ('freqn', 0.029), ('neyman', 0.029), ('provisionally', 0.029), ('valid', 0.028), ('pn', 0.028), ('pz', 0.027), ('calculate', 0.026), ('scores', 0.025), ('probabilities', 0.025), ('rn', 0.025), ('book', 0.025), ('informal', 0.025), ('plant', 0.025), ('inasmuch', 0.025), ('intervals', 0.024), ('numbers', 0.023), ('uncertain', 0.023), ('classical', 0.023), ('events', 0.022), ('predict', 0.022), ('yi', 0.022), ('unusual', 0.022), ('backwards', 0.021), ('kernels', 0.021), ('mutually', 0.021), ('bettor', 0.02), ('credibility', 0.02), ('gammerman', 0.02), ('length', 0.02), ('measure', 0.02), ('picture', 0.019), ('summarizing', 0.019), ('predictions', 0.019), ('width', 0.019), ('proposition', 0.019), ('odds', 0.019), ('iris', 0.019), ('errors', 0.018), ('degrees', 0.018), ('vladimir', 0.018), ('normally', 0.018), ('hyperplane', 0.017), ('bl', 0.017), ('con', 0.017), ('says', 0.017), ('freedom', 0.017), ('announces', 0.016), ('bets', 0.016), ('cournot', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="7-tfidf-1" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>Author: Glenn Shafer, Vladimir Vovk</p><p>Abstract: Conformal prediction uses past experience to determine precise levels of conÄ?Ĺš dence in new predictions. Given an error probability ĂŽÄž, together with a method that makes a prediction y of a label Ă&lsaquo;&dagger; y, it produces a set of labels, typically containing y, that also contains y with probability 1 Ă˘&circ;&rsquo; ĂŽÄž. Ă&lsaquo;&dagger; Conformal prediction can be applied to any method for producing y: a nearest-neighbor method, a Ă&lsaquo;&dagger; support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 Ă˘&circ;&rsquo; ĂŽÄž of the time, even though they are based on an accumulating data set rather than on independent data sets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005). Keywords: conÄ?Ĺš dence, on-line compression modeling, on-line learning, prediction regions</p><p>2 0.15329947 <a title="7-tfidf-2" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>Author: Bernadetta Tarigan, Sara A. van de Geer</p><p>Abstract: The success of support vector machines in binary classiﬁcation relies on the fact that hinge loss employed in the risk minimization targets the Bayes rule. Recent research explores some extensions of this large margin based method to the multicategory case. We show a moment bound for the socalled multi-hinge loss minimizers based on two kinds of complexity constraints: entropy with bracketing and empirical entropy. Obtaining such a result based on the latter is harder than ﬁnding one based on the former. We obtain fast rates of convergence that adapt to the unknown margin. Keywords: multi-hinge classiﬁcation, all-at-once, moment bound, fast rate, entropy</p><p>3 0.12302469 <a title="7-tfidf-3" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>Author: Xing Sun, Andrew B. Nobel</p><p>Abstract: Binary matrices, and their associated submatrices of 1s, play a central role in the study of random bipartite graphs and in core data mining problems such as frequent itemset mining (FIM). Motivated by these connections, this paper addresses several statistical questions regarding submatrices of 1s in a random binary matrix with independent Bernoulli entries. We establish a three-point concentration result, and a related probability bound, for the size of the largest square submatrix of 1s in a square Bernoulli matrix, and extend these results to non-square matrices and submatrices with ﬁxed aspect ratios. We then consider the noise sensitivity of frequent itemset mining under a simple binary additive noise model, and show that, even at small noise levels, large blocks of 1s leave behind fragments of only logarithmic size. As a result, standard FIM algorithms, which search only for submatrices of 1s, cannot directly recover such blocks when noise is present. On the positive side, we show that an error-tolerant frequent itemset criterion can recover a submatrix of 1s against a background of 0s plus noise, even when the size of the submatrix of 1s is very small. 1 Keywords: frequent itemset mining, bipartite graph, biclique, submatrix of 1s, statistical signiﬁcance</p><p>4 0.062787607 <a title="7-tfidf-4" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>Author: Bo Jiang, Xuegong Zhang, Tianxi Cai</p><p>Abstract: Support vector machine (SVM) is one of the most popular and promising classiﬁcation algorithms. After a classiﬁcation rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples. Keywords: k-fold cross-validation, model evaluation, perturbation-resampling, prediction errors, support vector machine</p><p>5 0.051215932 <a title="7-tfidf-5" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields general results for classiﬁcation and regression. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. We discuss the related problem of learning parameters of a distribution from multiple data sources. Finally, we illustrate our theory through a series of synthetic simulations. Keywords: error bounds, multi-task learning</p><p>6 0.037978396 <a title="7-tfidf-6" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>7 0.032847118 <a title="7-tfidf-7" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>8 0.032082174 <a title="7-tfidf-8" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>9 0.03176643 <a title="7-tfidf-9" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>10 0.030173432 <a title="7-tfidf-10" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>11 0.028591441 <a title="7-tfidf-11" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>12 0.028210035 <a title="7-tfidf-12" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>13 0.027991045 <a title="7-tfidf-13" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>14 0.027659804 <a title="7-tfidf-14" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>15 0.027446643 <a title="7-tfidf-15" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>16 0.026499411 <a title="7-tfidf-16" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>17 0.024597196 <a title="7-tfidf-17" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>18 0.022333341 <a title="7-tfidf-18" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>19 0.021164952 <a title="7-tfidf-19" href="./jmlr-2008-Non-Parametric_Modeling_of_Partially_Ranked_Data.html">69 jmlr-2008-Non-Parametric Modeling of Partially Ranked Data</a></p>
<p>20 0.019645872 <a title="7-tfidf-20" href="./jmlr-2008-Ranking_Categorical_Features_Using_Generalization_Properties.html">79 jmlr-2008-Ranking Categorical Features Using Generalization Properties</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.131), (1, -0.051), (2, -0.025), (3, -0.071), (4, 0.047), (5, -0.008), (6, 0.112), (7, -0.177), (8, 0.05), (9, -0.173), (10, 0.12), (11, -0.005), (12, -0.175), (13, -0.012), (14, 0.272), (15, 0.009), (16, 0.006), (17, 0.202), (18, 0.349), (19, -0.269), (20, 0.06), (21, -0.015), (22, 0.183), (23, 0.088), (24, -0.0), (25, 0.012), (26, 0.035), (27, -0.04), (28, -0.089), (29, 0.088), (30, 0.008), (31, 0.04), (32, 0.002), (33, 0.074), (34, 0.099), (35, -0.022), (36, -0.039), (37, 0.111), (38, -0.091), (39, 0.075), (40, 0.03), (41, 0.071), (42, 0.002), (43, 0.036), (44, -0.051), (45, -0.028), (46, 0.061), (47, -0.019), (48, -0.004), (49, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97206438 <a title="7-lsi-1" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>Author: Glenn Shafer, Vladimir Vovk</p><p>Abstract: Conformal prediction uses past experience to determine precise levels of conÄ?Ĺš dence in new predictions. Given an error probability ĂŽÄž, together with a method that makes a prediction y of a label Ă&lsaquo;&dagger; y, it produces a set of labels, typically containing y, that also contains y with probability 1 Ă˘&circ;&rsquo; ĂŽÄž. Ă&lsaquo;&dagger; Conformal prediction can be applied to any method for producing y: a nearest-neighbor method, a Ă&lsaquo;&dagger; support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 Ă˘&circ;&rsquo; ĂŽÄž of the time, even though they are based on an accumulating data set rather than on independent data sets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005). Keywords: conÄ?Ĺš dence, on-line compression modeling, on-line learning, prediction regions</p><p>2 0.70599455 <a title="7-lsi-2" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>Author: Xing Sun, Andrew B. Nobel</p><p>Abstract: Binary matrices, and their associated submatrices of 1s, play a central role in the study of random bipartite graphs and in core data mining problems such as frequent itemset mining (FIM). Motivated by these connections, this paper addresses several statistical questions regarding submatrices of 1s in a random binary matrix with independent Bernoulli entries. We establish a three-point concentration result, and a related probability bound, for the size of the largest square submatrix of 1s in a square Bernoulli matrix, and extend these results to non-square matrices and submatrices with ﬁxed aspect ratios. We then consider the noise sensitivity of frequent itemset mining under a simple binary additive noise model, and show that, even at small noise levels, large blocks of 1s leave behind fragments of only logarithmic size. As a result, standard FIM algorithms, which search only for submatrices of 1s, cannot directly recover such blocks when noise is present. On the positive side, we show that an error-tolerant frequent itemset criterion can recover a submatrix of 1s against a background of 0s plus noise, even when the size of the submatrix of 1s is very small. 1 Keywords: frequent itemset mining, bipartite graph, biclique, submatrix of 1s, statistical signiﬁcance</p><p>3 0.55109555 <a title="7-lsi-3" href="./jmlr-2008-A_Moment_Bound_for_Multi-hinge_Classifiers.html">3 jmlr-2008-A Moment Bound for Multi-hinge Classifiers</a></p>
<p>Author: Bernadetta Tarigan, Sara A. van de Geer</p><p>Abstract: The success of support vector machines in binary classiﬁcation relies on the fact that hinge loss employed in the risk minimization targets the Bayes rule. Recent research explores some extensions of this large margin based method to the multicategory case. We show a moment bound for the socalled multi-hinge loss minimizers based on two kinds of complexity constraints: entropy with bracketing and empirical entropy. Obtaining such a result based on the latter is harder than ﬁnding one based on the former. We obtain fast rates of convergence that adapt to the unknown margin. Keywords: multi-hinge classiﬁcation, all-at-once, moment bound, fast rate, entropy</p><p>4 0.23453447 <a title="7-lsi-4" href="./jmlr-2008-Estimating_the_Confidence_Interval_for_Prediction_Errors_of_Support_Vector_Machine_Classifiers.html">32 jmlr-2008-Estimating the Confidence Interval for Prediction Errors of Support Vector Machine Classifiers</a></p>
<p>Author: Bo Jiang, Xuegong Zhang, Tianxi Cai</p><p>Abstract: Support vector machine (SVM) is one of the most popular and promising classiﬁcation algorithms. After a classiﬁcation rule is constructed via the SVM, it is essential to evaluate its prediction accuracy. In this paper, we develop procedures for obtaining both point and interval estimators for the prediction error. Under mild regularity conditions, we derive the consistency and asymptotic normality of the prediction error estimators for SVM with ﬁnite-dimensional kernels. A perturbationresampling procedure is proposed to obtain interval estimates for the prediction error in practice. With numerical studies on simulated data and a benchmark repository, we recommend the use of interval estimates centered at the cross-validated point estimates for the prediction error. Further applications of the proposed procedure in model evaluation and feature selection are illustrated with two examples. Keywords: k-fold cross-validation, model evaluation, perturbation-resampling, prediction errors, support vector machine</p><p>5 0.18204342 <a title="7-lsi-5" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>Author: Koby Crammer, Michael Kearns, Jennifer Wortman</p><p>Abstract: We consider the problem of learning accurate models from multiple sources of “nearby” data. Given distinct samples from multiple data sources and estimates of the dissimilarities between these sources, we provide a general theory of which samples should be used to learn models for each source. This theory is applicable in a broad decision-theoretic learning framework, and yields general results for classiﬁcation and regression. A key component of our approach is the development of approximate triangle inequalities for expected loss, which may be of independent interest. We discuss the related problem of learning parameters of a distribution from multiple data sources. Finally, we illustrate our theory through a series of synthetic simulations. Keywords: error bounds, multi-task learning</p><p>6 0.1757056 <a title="7-lsi-6" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>7 0.16838801 <a title="7-lsi-7" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>8 0.15402521 <a title="7-lsi-8" href="./jmlr-2008-A_Multiple_Instance_Learning_Strategy_for_Combating_Good_Word_Attacks_on_Spam_Filters.html">4 jmlr-2008-A Multiple Instance Learning Strategy for Combating Good Word Attacks on Spam Filters</a></p>
<p>9 0.12549466 <a title="7-lsi-9" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>10 0.12425321 <a title="7-lsi-10" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>11 0.12381221 <a title="7-lsi-11" href="./jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations.html">37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</a></p>
<p>12 0.11851662 <a title="7-lsi-12" href="./jmlr-2008-An_Information_Criterion_for_Variable_Selection_in_Support_Vector_Machines%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">15 jmlr-2008-An Information Criterion for Variable Selection in Support Vector Machines    (Special Topic on Model Selection)</a></p>
<p>13 0.11475721 <a title="7-lsi-13" href="./jmlr-2008-On_the_Suitable_Domain_for_SVM_Training_in_Image_Coding.html">73 jmlr-2008-On the Suitable Domain for SVM Training in Image Coding</a></p>
<p>14 0.11284033 <a title="7-lsi-14" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>15 0.11227731 <a title="7-lsi-15" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>16 0.11189722 <a title="7-lsi-16" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>17 0.11158295 <a title="7-lsi-17" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>18 0.1113678 <a title="7-lsi-18" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>19 0.11106788 <a title="7-lsi-19" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>20 0.10977075 <a title="7-lsi-20" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.016), (31, 0.017), (40, 0.034), (54, 0.03), (58, 0.058), (60, 0.017), (66, 0.055), (76, 0.028), (88, 0.05), (92, 0.034), (94, 0.028), (99, 0.504)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89175463 <a title="7-lda-1" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>Author: Glenn Shafer, Vladimir Vovk</p><p>Abstract: Conformal prediction uses past experience to determine precise levels of conÄ?Ĺš dence in new predictions. Given an error probability ĂŽÄž, together with a method that makes a prediction y of a label Ă&lsaquo;&dagger; y, it produces a set of labels, typically containing y, that also contains y with probability 1 Ă˘&circ;&rsquo; ĂŽÄž. Ă&lsaquo;&dagger; Conformal prediction can be applied to any method for producing y: a nearest-neighbor method, a Ă&lsaquo;&dagger; support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 Ă˘&circ;&rsquo; ĂŽÄž of the time, even though they are based on an accumulating data set rather than on independent data sets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005). Keywords: conÄ?Ĺš dence, on-line compression modeling, on-line learning, prediction regions</p><p>2 0.86795008 <a title="7-lda-2" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>3 0.49145404 <a title="7-lda-3" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>4 0.37564236 <a title="7-lda-4" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>Author: Hannes Nickisch, Carl Edward Rasmussen</p><p>Abstract: We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classiﬁcation. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches. Keywords: Gaussian process priors, probabilistic classiﬁcation, Laplaces’s approximation, expectation propagation, variational bounding, mean ﬁeld methods, marginal likelihood evidence, MCMC</p><p>5 0.34408849 <a title="7-lda-5" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>Author: Andreas Krause, H. Brendan McMahan, Carlos Guestrin, Anupam Gupta</p><p>Abstract: In many applications, one has to actively select among a set of expensive observations before making an informed decision. For example, in environmental monitoring, we want to select locations to measure in order to most effectively predict spatial phenomena. Often, we want to select observations which are robust against a number of possible objective functions. Examples include minimizing the maximum posterior variance in Gaussian Process regression, robust experimental design, and sensor placement for outbreak detection. In this paper, we present the Submodular Saturation algorithm, a simple and efﬁcient algorithm with strong theoretical approximation guarantees for cases where the possible objective functions exhibit submodularity, an intuitive diminishing returns property. Moreover, we prove that better approximation algorithms do not exist unless NP-complete problems admit efﬁcient algorithms. We show how our algorithm can be extended to handle complex cost functions (incorporating non-unit observation cost or communication and path costs). We also show how the algorithm can be used to near-optimally trade off expected-case (e.g., the Mean Square Prediction Error in Gaussian Process regression) and worst-case (e.g., maximum predictive variance) performance. We show that many important machine learning problems ﬁt our robust submodular observation selection formalism, and provide extensive empirical evaluation on several real-world problems. For Gaussian Process regression, our algorithm compares favorably with state-of-the-art heuristics described in the geostatistics literature, while being simpler, faster and providing theoretical guarantees. For robust experimental design, our algorithm performs favorably compared to SDP-based algorithms. c 2008 Andreas Krause, H. Brendan McMahan, Carlos Guestrin and Anupam Gupta. K RAUSE , M C M AHAN , G UESTRIN AND G UPTA Keywords: observation selection, experimental design, active learning, submodular functions, Gaussi</p><p>6 0.34056026 <a title="7-lda-6" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>7 0.34019691 <a title="7-lda-7" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>8 0.33726868 <a title="7-lda-8" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>9 0.3324303 <a title="7-lda-9" href="./jmlr-2008-Learning_Reliable_Classifiers_From_Small_or_Incomplete_Data_Sets%3A_The_Naive_Credal_Classifier_2.html">50 jmlr-2008-Learning Reliable Classifiers From Small or Incomplete Data Sets: The Naive Credal Classifier 2</a></p>
<p>10 0.33112776 <a title="7-lda-10" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>11 0.33079189 <a title="7-lda-11" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>12 0.32824263 <a title="7-lda-12" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>13 0.32764217 <a title="7-lda-13" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>14 0.32721254 <a title="7-lda-14" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>15 0.32441977 <a title="7-lda-15" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>16 0.32348388 <a title="7-lda-16" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>17 0.32253236 <a title="7-lda-17" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>18 0.32187778 <a title="7-lda-18" href="./jmlr-2008-Value_Function_Approximation_using_Multiple_Aggregation_for_Multiattribute_Resource_Management.html">94 jmlr-2008-Value Function Approximation using Multiple Aggregation for Multiattribute Resource Management</a></p>
<p>19 0.32140401 <a title="7-lda-19" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>20 0.31647351 <a title="7-lda-20" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
