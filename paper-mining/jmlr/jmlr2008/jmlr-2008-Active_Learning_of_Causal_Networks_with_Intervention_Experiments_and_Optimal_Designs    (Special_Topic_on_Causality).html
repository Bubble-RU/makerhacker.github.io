<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 jmlr-2008-Active Learning of Causal Networks with Intervention Experiments and Optimal Designs    (Special Topic on Causality)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-10" href="#">jmlr2008-10</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>10 jmlr-2008-Active Learning of Causal Networks with Intervention Experiments and Optimal Designs    (Special Topic on Causality)</h1>
<br/><p>Source: <a title="jmlr-2008-10-pdf" href="http://jmlr.org/papers/volume9/he08a/he08a.pdf">pdf</a></p><p>Author: Yang-Bo He, Zhi Geng</p><p>Abstract: The causal discovery from data is important for various scientiﬁc investigations. Because we cannot distinguish the different directed acyclic graphs (DAGs) in a Markov equivalence class learned from observational data, we have to collect further information on causal structures from experiments with external interventions. In this paper, we propose an active learning approach for discovering causal structures in which we ﬁrst ﬁnd a Markov equivalence class from observational data, and then we orient undirected edges in every chain component via intervention experiments separately. In the experiments, some variables are manipulated through external interventions. We discuss two kinds of intervention experiments, randomized experiment and quasi-experiment. Furthermore, we give two optimal designs of experiments, a batch-intervention design and a sequential-intervention design, to minimize the number of manipulated variables and the set of candidate structures based on the minimax and the maximum entropy criteria. We show theoretically that structural learning can be done locally in subgraphs of chain components without need of checking illegal v-structures and cycles in the whole network and that a Markov equivalence subclass obtained after each intervention can still be depicted as a chain graph. Keywords: active learning, causal networks, directed acyclic graphs, intervention, Markov equivalence class, optimal design, structural learning</p><p>Reference: <a title="jmlr-2008-10-reference" href="../jmlr2008_reference/jmlr-2008-Active_Learning_of_Causal_Networks_with_Intervention_Experiments_and_Optimal_Designs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Because we cannot distinguish the different directed acyclic graphs (DAGs) in a Markov equivalence class learned from observational data, we have to collect further information on causal structures from experiments with external interventions. [sent-8, score-0.578]
</p><p>2 In this paper, we propose an active learning approach for discovering causal structures in which we ﬁrst ﬁnd a Markov equivalence class from observational data, and then we orient undirected edges in every chain component via intervention experiments separately. [sent-9, score-1.426]
</p><p>3 Furthermore, we give two optimal designs of experiments, a batch-intervention design and a sequential-intervention design, to minimize the number of manipulated variables and the set of candidate structures based on the minimax and the maximum entropy criteria. [sent-12, score-0.702]
</p><p>4 We show theoretically that structural learning can be done locally in subgraphs of chain components without need of checking illegal v-structures and cycles in the whole network and that a Markov equivalence subclass obtained after each intervention can still be depicted as a chain graph. [sent-13, score-0.908]
</p><p>5 A chain graph contains both directed and undirected edges. [sent-32, score-0.663]
</p><p>6 A chain component of a chain graph is a connected undirected graph obtained by removing all directed edges from the chain graph. [sent-33, score-1.442]
</p><p>7 We propose a method of local orientations in every chain component, and we show theoretically that the method of local orientations does not create any new v-structure or cycle in the whole DAG provided that neither v-structure nor cycle is created in any chain component. [sent-39, score-0.844]
</p><p>8 In active learning, we ﬁrst ﬁnd a Markov equivalence class from observational data, which can be represented by a chain graph, and then we orient undirected edges via intervention experiments. [sent-44, score-1.179]
</p><p>9 Furthermore, we discuss the optimal designs by which the number of manipulated variables is minimized or the uncertainty of candidate structures is minimized at each experiment step based on the minimax and the maximum entropy criteria. [sent-51, score-0.688]
</p><p>10 For the former, we try to ﬁnd the minimum set of variables to be manipulated in a batch such that undirected edges are all oriented after the interventions. [sent-53, score-0.921]
</p><p>11 In Section 4, we propose two optimal designs of intervention experiments, a batch-intervention design and a sequen2524  ACTIVE L EARNING OF C AUSAL N ETWORKS  tial intervention design. [sent-57, score-0.692]
</p><p>12 A graph is undirected if all edges of the graph are undirected. [sent-67, score-0.57]
</p><p>13 A directed cycle is a directed path from a node to itself, and a partially directed cycle is a partially directed path from a node to itself. [sent-74, score-0.73]
</p><p>14 A graph with both directed and undirected edges is a chain graph if there is not any partially directed cycle. [sent-75, score-1.054]
</p><p>15 A chain component is a node set whose nodes are connected in an undirected graph obtained by removing all directed edges from the chain graph. [sent-77, score-1.179]
</p><p>16    V2 r  rV1           r    V4 r  V3   r V5 s c  Figure 1: A chain graph G∗ depicts the essential graph of G, G1 , G2 and G3 . [sent-79, score-0.601]
</p><p>17 A directed acyclic graph (DAG) is a directed graph which does not contain any directed cycle. [sent-80, score-0.626]
</p><p>18 Deﬁnition 2 The essential graph G∗ = (V, E∗ ) of G has the same node set and the same skeleton as G, whose one edge is directed if and only if it has the same orientation in every DAG in [G] and whose other edges are undirected. [sent-106, score-0.798]
</p><p>19 The edges V2 → V5 and V3 → V5 in G∗ are directed since they have the same orientation for all DAGs of [G] in Figure 2, and other edges are undirected. [sent-108, score-0.55]
</p><p>20 Then G∗ has the following properties: 2526  ACTIVE L EARNING OF C AUSAL N ETWORKS  (i) G∗ is a chain graph, (ii) G∗ is chordal for every chain component τ, and τ (iii) Vi → V j −Vk does not occur as an induced subgraph of G∗ . [sent-111, score-0.615]
</p><p>21 Suppose that G is an unknown underlying causal graph and that its essential graph G ∗ = (V, E) has been obtained from observational data, and has k chain components {τ 1 , · · · , τk }. [sent-112, score-0.801]
</p><p>22 Its edge set E can be partitioned into the set E1 of directed edges and the set E2 of undirected edges. [sent-113, score-0.554]
</p><p>23 Any subgraph of the essential graph induced by a chain component is undirected. [sent-115, score-0.603]
</p><p>24 Given an essential graph G∗ , we need to orient all undirected edges in each chain component to discover the whole causal graph G. [sent-119, score-1.285]
</p><p>25 If orientation of undirected edges in the subgraph G∗ does not create any directed cycle in the subgraph, then the orientation does not create any τ directed cycle in the whole DAG. [sent-127, score-1.046]
</p><p>26 According to Theorems 4 and 5, we ﬁnd that the undirected edges can be oriented separately in each chain component regardless of directed and undirected edges in other part of the essential graph as long as neither cycles nor v-structures are constructed in any chain component. [sent-128, score-1.754]
</p><p>27 Active Learning of Causal Structures via External Interventions To discover causal structures further from a Markov equivalence class obtained from observational data, we have to perform external interventions on some variables. [sent-132, score-0.568]
</p><p>28 The other is the quasi-experiment, in which the distribution of the manipulated variable Vi conditional on its parents pa(Vi ) is changed by manipulating Vi . [sent-135, score-0.601]
</p><p>29 1 Interventions by Randomized Experiments In this subsection, we conduct interventions as randomized experiments, in which some variables are manipulated from external interventions by assigning individuals to some levels of these variables in a probabilistic way. [sent-138, score-0.804]
</p><p>30 Under the faithfulness assumption, it is obvious that an undirected edge between Vi and its neighbor V j can be oriented as Vi ← V j if the post-intervention distribution has Vi V j , otherwise it is oriented as Vi → V j , where Vi V j denotes that Vi is independent of V j . [sent-144, score-0.565]
</p><p>31 If Vi belongs to a chain component τ (that is, it connects at least one undirected edge), then the Markov equivalence class [G] can be reduced by manipulating Vi to the post-intervention Markov equivalence class [G]e(Vi ) [G]e(Vi ) = {G ∈ [G]|G has the same orientation as e(Vi )}. [sent-148, score-1.027]
</p><p>32 Theorem 6 Let τ be a chain component of the pre-intervention essential graph G ∗ and Vi be a node in the component τ. [sent-152, score-0.632]
</p><p>33 The post-intervention graph G∗ i ) is also a chain graph, that is, G∗ i ) has the e(V e(V following properties: (i) G∗ i ) is a chain graph, e(V (ii) G∗ i ) is chordal, and e(V (iii) V j → Vk −Vl does not occur as an induced subgraph of G∗ i ) . [sent-153, score-0.644]
</p><p>34 e(V By Theorem 6, the pre-intervention chain graph is changed by manipulating a variable to another chain graph which has less undirected edges. [sent-154, score-1.095]
</p><p>35 Thus variables in chain components can be manipulated repeatedly until the Markov equivalence subclass is reduced to a subclass with a single DAG, and properties of chain graphs are not lost in this intervention process. [sent-155, score-1.343]
</p><p>36 According to the above results, we ﬁrst learn an essential graph from observational data, which is a chain graph (Andersson et al. [sent-156, score-0.632]
</p><p>37 Next we choose a variable Vi to be manipulated from a chain component, and we can orient the undirected edges connecting Vi and some other undirected edges whose reverse orientations create v-structures or cycles. [sent-159, score-1.58]
</p><p>38 Repeating this process, we choose a next variable to be manipulated until all undirected edges are oriented. [sent-160, score-0.724]
</p><p>39 After obtaining the essential graph from observational data, we manipulate some variables in randomized experiments to identify a causal structure in the 12 DAGs. [sent-164, score-0.579]
</p><p>40 Notice that undirected edges not connecting V1 can also be oriented by manipulating V1 . [sent-168, score-0.709]
</p><p>41 Let Vi − Vk be an undirected edge in a chain component τ, and we want to orient the undirected edge by manipulating Vi . [sent-188, score-1.163]
</p><p>42 4 (3) (6) (1) (2) (7) (4) (5) (8) (9) (11) (10) (12)  Table 2: The intervention process to identify a causal structure from the essential graph in Figure 3, where ∗ means that the intervention is unnecessary. [sent-190, score-0.909]
</p><p>43 Below we show a result which can be used to identify the direction of the undirected edge Vi −Vk via a quasi-experiment of intervention on Vi . [sent-192, score-0.534]
</p><p>44 We use a quasiexperiment of manipulating V1 in order to orient the undirected edges connecting V1 (V3 −V1 −V2 ). [sent-202, score-0.702]
</p><p>45 Optimal Designs of Intervention Experiments In this section, we discuss the optimal designs of intervention experiments which are used to minimize the number of manipulated variables or to minimize the uncertainty of candidate structures after an intervention experiment based on some criteria. [sent-213, score-1.091]
</p><p>46 Since the orientation for one chain component is unrelated to the orientations for other components, we can design an intervention experiment for each chain component separately. [sent-214, score-1.141]
</p><p>47 As shown in Section 2, given a chain component τ, we orient the subgraph over τ into a DAG Gτ without any v-structure or cycle via experiments of interventions in variables in τ. [sent-215, score-0.729]
</p><p>48 In the following subsections, we discuss intervention designs for only one chain component. [sent-217, score-0.622]
</p><p>49 Under this assumption, all undirected edges connecting a node Vi can be oriented via a quasi-experiment of intervention on variable Vi . [sent-220, score-0.841]
</p><p>50 Without the assumption, there may be some undirected edge which cannot be oriented even if we perform interventions in both of its two nodes. [sent-221, score-0.559]
</p><p>51 That is, we can orient all undirected edges of the essential graph G ∗ no matter which G in [G] is the true DAG. [sent-227, score-0.714]
</p><p>52 Let g denote the number of nodes in the chain component, and h the number of undirected edges within the component. [sent-229, score-0.606]
</p><p>53 , e(Vk )} is a legal combination of orientations if there is not any v-structure or cycle formed and there is not any undirected edge oriented in two different directions by these orientations. [sent-244, score-0.618]
</p><p>54 Especially, any undirected edge can be oriented by manipulating either of its two nodes. [sent-273, score-0.607]
</p><p>55 2 Optimization for Batch Interventions We say that an intervention experiment is a batch-intervention experiment if all variables in a sufﬁcient set S are manipulated in a batch to orient all undirected edges of an essential graph. [sent-275, score-1.35]
</p><p>56 We say that a batch intervention design is optimal if its sufﬁcient set So has the smallest number of manipulated variables, that is, |S o | = min{|S | : S ∈ S}. [sent-277, score-0.713]
</p><p>57 Below we give an algorithm to ﬁnd the optimal design for batch interventions, in which we ﬁrst try all sets with a single manipulated variable, then try all sets with two variables, and so on, until each post-intervention Markov equivalence class has a single DAG. [sent-279, score-0.568]
</p><p>58 2533  H E AND G ENG  Algorithm 1 Algorithm for ﬁnding the optimal designs of batch interventions Input: A chain graph G induced by a chain component τ = {V1 , . [sent-289, score-0.972]
</p><p>59 A possible greedy approach is to select a node to be ﬁrst manipulated from the chain component which has the largest number of neighbors such that the largest number of undirected edges are oriented by manipulating it, and then delete these oriented edges. [sent-310, score-1.501]
</p><p>60 At step t of the sequential experiment, according to the current Markov equivalence class [G] e(S (t−1) ) obtained by manipulating the previous variables in S (t−1) , we choose a variable V to be manipulated 2534  ACTIVE L EARNING OF C AUSAL N ETWORKS  based on some criterion. [sent-321, score-0.751]
</p><p>61 Based on the maximum entropy criterion, the postintervention subclasses have sizes as small as possible and they have sizes as equal as possible, which means uncertainty for identifying a causal DAG from the Markov equivalence class is minimized by manipulating V . [sent-328, score-0.584]
</p><p>62 Below we give two examples to illustrate how to choose variables to be manipulated in the optimal design of sequential interventions based on the two criteria. [sent-329, score-0.604]
</p><p>63 Generally the size g of a chain component is much less than the number n of the full variable set and the number h of undirected edges in a chain component is not very large. [sent-362, score-0.946]
</p><p>64 Since there are no v-structures in any chain component, all undirected edges in a subtree can be oriented as long as we ﬁnd its root. [sent-367, score-0.73]
</p><p>65 Another way to draw a DAG is that we randomly orient each undirected edge of the essential graph, but we need to check whether there is any cycle besides v-structure. [sent-398, score-0.595]
</p><p>66 In the ﬁrst experiment, we evaluate a whole process of structural learning and orientation in which we ﬁrst ﬁnd an essential graph using the PC algorithm and then orient the undirected edges using the approaches proposed in this paper. [sent-401, score-0.834]
</p><p>67 This essential graph can also be seen as a chain component of a large essential graph. [sent-405, score-0.654]
</p><p>68 Then we use the intervention approach proposed in Section 3 to orient undirected edges of the essential graph. [sent-410, score-0.861]
</p><p>69 To compare the performance of the experiment designs, we further give the numbers of manipulated variables that are necessary to orient all undirected edges of the same essential graphs in various intervention designs. [sent-418, score-1.295]
</p><p>70 To orient an undirected edge Vi −V j , we implemented both the independence test of the manipulated Vi and its each neighbor variable V j for randomized experiments and the equivalence test of pre- and post-intervention distributions (i. [sent-427, score-0.955]
</p><p>71 To evaluate the performance of orientation, we deﬁne the percentage of correct orientations as the ratio of the number of correctly oriented edges to the number of edges that are obtained from the PC algorithm and belong to the DAG (1) in Figure 4. [sent-432, score-0.587]
</p><p>72 To separate the false orientations due to the PC algorithm from those due to intervention experiments, we further check the cases that the essential graph in Figure 3 is correctly obtained from the PC algorithm. [sent-434, score-0.647]
</p><p>73 Comparing λ and λ , it can be seen that there are more edges oriented correctly when the essential graph is correctly obtained from the PC algorithm. [sent-438, score-0.58]
</p><p>74 From the sixth to eleven columns, we give the cumulative distributions of the number of edges oriented correctly when the essential graph is correctly obtained. [sent-439, score-0.58]
</p><p>75 The column labeled ‘≥ i’ means that we correctly oriented more than or equal to i of 6 edges of the essential graph in Figure 3, and the values in this column denote the percents of DAGs with more than or equal to i edges correctly oriented in those simulations. [sent-440, score-0.903]
</p><p>76 000  Table 8: The simulation results  In the second experiment, we compare the numbers of manipulated variables to orient the same underlying essential graph for different experimental designs. [sent-672, score-0.767]
</p><p>77 2, the optimal batch design and the design by the greedy method always need three variables to be manipulated for orientation of the essential graph. [sent-676, score-0.764]
</p><p>78 In the random design labeled ’Random’, we randomly select a variable to be manipulated at each sequential step, only one variable is manipulated for orientations in 268 of 1000 simulations, and four variables are manipulated in 55 of 1000 simulations. [sent-678, score-1.334]
</p><p>79 We show three approximate designs which draw h, h × 5 and h × 10 DAGs from a chain component with h undirected edges respectively. [sent-681, score-0.769]
</p><p>80 For 2539  H E AND G ENG  example, the sample sizes of DAGs from the initial essential graph [G] with h = 6 undirected edges are 6, 30 and 60, respectively. [sent-682, score-0.585]
</p><p>81 According to Table 9, all of the sequential intervention designs (Random, Minimax, Entropy and their approximations) are more efﬁcient than the batch design, and the optimal designs based on the minimax and the maximum entropy criteria are more efﬁcient than the random design. [sent-685, score-0.744]
</p><p>82 Conclusions In this paper, we proposed a framework for active learning of causal structures via intervention experiments, and further we proposed optimal designs of batch and sequential interventions based on the minimax and the maximum entropy criteria. [sent-693, score-0.994]
</p><p>83 A Markov equivalence class can be split into subclasses by manipulating a variable, and a causal structure can be identiﬁed by manipulating variables repeatedly. [sent-694, score-0.763]
</p><p>84 For a randomized experiment, the orientations of an undirected edge can be determined by testing the independence of the manipulated variable and its neighbor variable only with experimental data. [sent-697, score-0.876]
</p><p>85 For the optimal batch design, a smallest set of variables to be manipulated is found before interventions, which is sufﬁcient to orient all undirected edges of an essential graph. [sent-700, score-1.041]
</p><p>86 But the optimal batch design does not use orientation results obtained by manipulating the previous variables during the intervention process, and thus it may be less efﬁcient than the optimal sequential designs. [sent-701, score-0.738]
</p><p>87 For the optimal sequential design, we choose a variable to be manipulated sequentially such that the current Markov equivalence class can be reduced to a subclass with potential causal DAGs as little as possible. [sent-702, score-0.736]
</p><p>88 The scalability of the optimal designs proposed in this paper depends only on the sizes of chain components but does not depend on the size of a DAG since the optimal designs are performed separately within every chain component. [sent-705, score-0.743]
</p><p>89 When both the number h of undirected edges and the number g of nodes in a chain component are very large, instead of using the optimal designs, we may use the approximate designs via sampling DAGs. [sent-708, score-0.801]
</p><p>90 , 2000) for constructing a graph, in which some undirected edges of the initial essential graph are oriented with the information of e(V ). [sent-753, score-0.721]
</p><p>91 Let τ be a chain graph of G ∗ , V ∈ τ and e(V ) be an orientation of undirected edges connecting V . [sent-754, score-0.837]
</p><p>92 Algorithm 2 Find the post-intervention essential graph via orientation e(V ) Input: The essential graph G∗ and e(V ) Output: The graph H Orient the undirected edges connecting V in the essential graph G ∗ according to e(V ) and denote the graph as H. [sent-755, score-1.438]
</p><p>93 Repeat the following two rules to orient some other undirected edges until no rules can be applied: (i) if V1 → V2 − V3 ∈ H and V1 and V3 are not adjacent in H, then orient V2 − V3 as V2 → V3 and update H; (ii) if V1 → V2 → V3 ∈ H and V1 −V3 ∈ H, then orient V1 −V3 as V1 → V3 and update H. [sent-756, score-0.733]
</p><p>94 return the graph H It can be shown that H constructed by Algorithm 2 is a chain graph and H is equal to the post-intervention essential graph G∗ ) . [sent-757, score-0.691]
</p><p>95 Proof If H is not a chain graph, there must be a directed cycle in subgraph Hτ for some chain component of G∗ . [sent-769, score-0.784]
</p><p>96 Lemma 13 Let G∗ ) be the post intervention essential graph with the orientation e(V ) and H be e(V the graph constructed by Algorithm 2. [sent-800, score-0.73]
</p><p>97 By deﬁnition of G∗ ) , we have that G∗ ) has the same skeleton as the e(V e(V essential graph G∗ and contains all directed edges of G∗ . [sent-825, score-0.553]
</p><p>98 If Vk ∈ pa(Vi ), then we have PVi (an(vk ), vk ) = P(an(vk ), vk ) and thus PVi (Vk ) = P(Vk ). [sent-860, score-0.692]
</p><p>99 Manipulating a node Vi will orient all of undirected edges connecting Vi . [sent-862, score-0.551]
</p><p>100 This is feasible since any  undirected edge can be oriented by manipulating either of its two nodes. [sent-872, score-0.607]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('manipulated', 0.354), ('vk', 0.346), ('vi', 0.305), ('intervention', 0.259), ('dags', 0.248), ('chain', 0.228), ('manipulating', 0.196), ('undirected', 0.195), ('dag', 0.193), ('causal', 0.152), ('edges', 0.151), ('interventions', 0.148), ('oriented', 0.136), ('designs', 0.135), ('orient', 0.129), ('directed', 0.128), ('essential', 0.127), ('orientations', 0.122), ('pa', 0.12), ('orientation', 0.12), ('graph', 0.112), ('pvi', 0.109), ('equivalence', 0.096), ('subclasses', 0.081), ('edge', 0.08), ('subgraph', 0.076), ('minimax', 0.074), ('markov', 0.07), ('ch', 0.065), ('cycle', 0.064), ('parent', 0.062), ('batch', 0.061), ('component', 0.06), ('eng', 0.059), ('ausal', 0.059), ('randomized', 0.059), ('qe', 0.058), ('li', 0.056), ('etworks', 0.055), ('subclass', 0.053), ('observational', 0.053), ('rv', 0.052), ('manipulate', 0.052), ('active', 0.05), ('external', 0.047), ('node', 0.045), ('dr', 0.042), ('entropy', 0.041), ('sequential', 0.039), ('design', 0.039), ('structures', 0.035), ('skeleton', 0.035), ('nodes', 0.032), ('andersson', 0.032), ('graphs', 0.031), ('connecting', 0.031), ('earning', 0.03), ('ni', 0.028), ('pc', 0.028), ('pearl', 0.028), ('parents', 0.027), ('correctly', 0.027), ('cycles', 0.027), ('patients', 0.026), ('experiment', 0.025), ('variables', 0.024), ('variable', 0.024), ('dc', 0.024), ('chordal', 0.023), ('yoo', 0.023), ('depicts', 0.022), ('legal', 0.021), ('simulation', 0.021), ('spirtes', 0.02), ('subtree', 0.02), ('verma', 0.02), ('hv', 0.019), ('discover', 0.019), ('acyclic', 0.018), ('neighbor', 0.018), ('suf', 0.018), ('column', 0.018), ('class', 0.018), ('china', 0.017), ('cooper', 0.017), ('components', 0.017), ('contradicts', 0.017), ('tq', 0.017), ('permutation', 0.017), ('criterion', 0.017), ('lemma', 0.017), ('theorem', 0.017), ('th', 0.017), ('figure', 0.017), ('tian', 0.016), ('causality', 0.016), ('heckerman', 0.016), ('neither', 0.016), ('subsection', 0.016), ('bayesian', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="10-tfidf-1" href="./jmlr-2008-Active_Learning_of_Causal_Networks_with_Intervention_Experiments_and_Optimal_Designs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">10 jmlr-2008-Active Learning of Causal Networks with Intervention Experiments and Optimal Designs    (Special Topic on Causality)</a></p>
<p>Author: Yang-Bo He, Zhi Geng</p><p>Abstract: The causal discovery from data is important for various scientiﬁc investigations. Because we cannot distinguish the different directed acyclic graphs (DAGs) in a Markov equivalence class learned from observational data, we have to collect further information on causal structures from experiments with external interventions. In this paper, we propose an active learning approach for discovering causal structures in which we ﬁrst ﬁnd a Markov equivalence class from observational data, and then we orient undirected edges in every chain component via intervention experiments separately. In the experiments, some variables are manipulated through external interventions. We discuss two kinds of intervention experiments, randomized experiment and quasi-experiment. Furthermore, we give two optimal designs of experiments, a batch-intervention design and a sequential-intervention design, to minimize the number of manipulated variables and the set of candidate structures based on the minimax and the maximum entropy criteria. We show theoretically that structural learning can be done locally in subgraphs of chain components without need of checking illegal v-structures and cycles in the whole network and that a Markov equivalence subclass obtained after each intervention can still be depicted as a chain graph. Keywords: active learning, causal networks, directed acyclic graphs, intervention, Markov equivalence class, optimal design, structural learning</p><p>2 0.28620893 <a title="10-tfidf-2" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>Author: Zongming Ma, Xianchao Xie, Zhi Geng</p><p>Abstract: Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse. Keywords: chain graph, conditional independence, decomposition, graphical model, structural learning</p><p>3 0.25276697 <a title="10-tfidf-3" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>Author: Xianchao Xie, Zhi Geng</p><p>Abstract: In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is ﬁrst decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efﬁciency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method. Keywords: Bayesian network, conditional independence, decomposition, directed acyclic graph, structural learning</p><p>4 0.25160912 <a title="10-tfidf-4" href="./jmlr-2008-Causal_Reasoning_with_Ancestral_Graphs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">20 jmlr-2008-Causal Reasoning with Ancestral Graphs    (Special Topic on Causality)</a></p>
<p>Author: Jiji Zhang</p><p>Abstract: Causal reasoning is primarily concerned with what would happen to a system under external interventions. In particular, we are often interested in predicting the probability distribution of some random variables that would result if some other variables were forced to take certain values. One prominent approach to tackling this problem is based on causal Bayesian networks, using directed acyclic graphs as causal diagrams to relate post-intervention probabilities to pre-intervention probabilities that are estimable from observational data. However, such causal diagrams are seldom fully testable given observational data. In consequence, many causal discovery algorithms based on data-mining can only output an equivalence class of causal diagrams (rather than a single one). This paper is concerned with causal reasoning given an equivalence class of causal diagrams, represented by a (partial) ancestral graph. We present two main results. The ﬁrst result extends Pearl (1995)’s celebrated do-calculus to the context of ancestral graphs. In the second result, we focus on a key component of Pearl’s calculus—the property of invariance under interventions, and give stronger graphical conditions for this property than those implied by the ﬁrst result. The second result also improves the earlier, similar results due to Spirtes et al. (1993). Keywords: ancestral graphs, causal Bayesian network, do-calculus, intervention</p><p>5 0.18287985 <a title="10-tfidf-5" href="./jmlr-2008-Graphical_Methods_for_Efficient_Likelihood_Inference_in_Gaussian_Covariance_Models.html">40 jmlr-2008-Graphical Methods for Efficient Likelihood Inference in Gaussian Covariance Models</a></p>
<p>Author: Mathias Drton, Thomas S. Richardson</p><p>Abstract: In graphical modelling, a bi-directed graph encodes marginal independences among random variables that are identiﬁed with the vertices of the graph. We show how to transform a bi-directed graph into a maximal ancestral graph that (i) represents the same independence structure as the original bi-directed graph, and (ii) minimizes the number of arrowheads among all ancestral graphs satisfying (i). Here the number of arrowheads of an ancestral graph is the number of directed edges plus twice the number of bi-directed edges. In Gaussian models, this construction can be used for more efﬁcient iterative maximization of the likelihood function and to determine when maximum likelihood estimates are equal to empirical counterparts. Keywords: ancestral graph, covariance graph, graphical model, marginal independence, maximum likelihood estimation, multivariate normal distribution</p><p>6 0.14358401 <a title="10-tfidf-6" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>7 0.13106282 <a title="10-tfidf-7" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>8 0.10936549 <a title="10-tfidf-8" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>9 0.10715615 <a title="10-tfidf-9" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>10 0.097123563 <a title="10-tfidf-10" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>11 0.088144295 <a title="10-tfidf-11" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>12 0.058880899 <a title="10-tfidf-12" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>13 0.050806522 <a title="10-tfidf-13" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>14 0.043720085 <a title="10-tfidf-14" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>15 0.041749433 <a title="10-tfidf-15" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>16 0.041426934 <a title="10-tfidf-16" href="./jmlr-2008-An_Error_Bound_Based_on_a_Worst_Likely_Assignment.html">13 jmlr-2008-An Error Bound Based on a Worst Likely Assignment</a></p>
<p>17 0.040851988 <a title="10-tfidf-17" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>18 0.030583801 <a title="10-tfidf-18" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>19 0.028959597 <a title="10-tfidf-19" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>20 0.028493106 <a title="10-tfidf-20" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.262), (1, 0.539), (2, -0.025), (3, -0.007), (4, 0.093), (5, -0.078), (6, -0.047), (7, -0.023), (8, 0.023), (9, 0.028), (10, 0.098), (11, -0.019), (12, 0.083), (13, 0.04), (14, -0.072), (15, 0.058), (16, -0.049), (17, -0.012), (18, -0.008), (19, -0.02), (20, 0.02), (21, -0.017), (22, 0.017), (23, -0.027), (24, -0.001), (25, -0.026), (26, -0.015), (27, -0.047), (28, 0.021), (29, -0.018), (30, 0.01), (31, 0.109), (32, -0.084), (33, 0.008), (34, -0.06), (35, 0.001), (36, -0.006), (37, 0.015), (38, -0.032), (39, 0.05), (40, 0.041), (41, -0.03), (42, 0.06), (43, 0.05), (44, -0.032), (45, 0.06), (46, -0.044), (47, 0.037), (48, -0.042), (49, -0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98464346 <a title="10-lsi-1" href="./jmlr-2008-Active_Learning_of_Causal_Networks_with_Intervention_Experiments_and_Optimal_Designs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">10 jmlr-2008-Active Learning of Causal Networks with Intervention Experiments and Optimal Designs    (Special Topic on Causality)</a></p>
<p>Author: Yang-Bo He, Zhi Geng</p><p>Abstract: The causal discovery from data is important for various scientiﬁc investigations. Because we cannot distinguish the different directed acyclic graphs (DAGs) in a Markov equivalence class learned from observational data, we have to collect further information on causal structures from experiments with external interventions. In this paper, we propose an active learning approach for discovering causal structures in which we ﬁrst ﬁnd a Markov equivalence class from observational data, and then we orient undirected edges in every chain component via intervention experiments separately. In the experiments, some variables are manipulated through external interventions. We discuss two kinds of intervention experiments, randomized experiment and quasi-experiment. Furthermore, we give two optimal designs of experiments, a batch-intervention design and a sequential-intervention design, to minimize the number of manipulated variables and the set of candidate structures based on the minimax and the maximum entropy criteria. We show theoretically that structural learning can be done locally in subgraphs of chain components without need of checking illegal v-structures and cycles in the whole network and that a Markov equivalence subclass obtained after each intervention can still be depicted as a chain graph. Keywords: active learning, causal networks, directed acyclic graphs, intervention, Markov equivalence class, optimal design, structural learning</p><p>2 0.89200336 <a title="10-lsi-2" href="./jmlr-2008-Graphical_Methods_for_Efficient_Likelihood_Inference_in_Gaussian_Covariance_Models.html">40 jmlr-2008-Graphical Methods for Efficient Likelihood Inference in Gaussian Covariance Models</a></p>
<p>Author: Mathias Drton, Thomas S. Richardson</p><p>Abstract: In graphical modelling, a bi-directed graph encodes marginal independences among random variables that are identiﬁed with the vertices of the graph. We show how to transform a bi-directed graph into a maximal ancestral graph that (i) represents the same independence structure as the original bi-directed graph, and (ii) minimizes the number of arrowheads among all ancestral graphs satisfying (i). Here the number of arrowheads of an ancestral graph is the number of directed edges plus twice the number of bi-directed edges. In Gaussian models, this construction can be used for more efﬁcient iterative maximization of the likelihood function and to determine when maximum likelihood estimates are equal to empirical counterparts. Keywords: ancestral graph, covariance graph, graphical model, marginal independence, maximum likelihood estimation, multivariate normal distribution</p><p>3 0.81287533 <a title="10-lsi-3" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>Author: Xianchao Xie, Zhi Geng</p><p>Abstract: In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is ﬁrst decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efﬁciency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method. Keywords: Bayesian network, conditional independence, decomposition, directed acyclic graph, structural learning</p><p>4 0.7867189 <a title="10-lsi-4" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>Author: Zongming Ma, Xianchao Xie, Zhi Geng</p><p>Abstract: Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse. Keywords: chain graph, conditional independence, decomposition, graphical model, structural learning</p><p>5 0.77968192 <a title="10-lsi-5" href="./jmlr-2008-Causal_Reasoning_with_Ancestral_Graphs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">20 jmlr-2008-Causal Reasoning with Ancestral Graphs    (Special Topic on Causality)</a></p>
<p>Author: Jiji Zhang</p><p>Abstract: Causal reasoning is primarily concerned with what would happen to a system under external interventions. In particular, we are often interested in predicting the probability distribution of some random variables that would result if some other variables were forced to take certain values. One prominent approach to tackling this problem is based on causal Bayesian networks, using directed acyclic graphs as causal diagrams to relate post-intervention probabilities to pre-intervention probabilities that are estimable from observational data. However, such causal diagrams are seldom fully testable given observational data. In consequence, many causal discovery algorithms based on data-mining can only output an equivalence class of causal diagrams (rather than a single one). This paper is concerned with causal reasoning given an equivalence class of causal diagrams, represented by a (partial) ancestral graph. We present two main results. The ﬁrst result extends Pearl (1995)’s celebrated do-calculus to the context of ancestral graphs. In the second result, we focus on a key component of Pearl’s calculus—the property of invariance under interventions, and give stronger graphical conditions for this property than those implied by the ﬁrst result. The second result also improves the earlier, similar results due to Spirtes et al. (1993). Keywords: ancestral graphs, causal Bayesian network, do-calculus, intervention</p><p>6 0.52421987 <a title="10-lsi-6" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>7 0.49649483 <a title="10-lsi-7" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>8 0.41942179 <a title="10-lsi-8" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>9 0.37274033 <a title="10-lsi-9" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>10 0.31598511 <a title="10-lsi-10" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>11 0.31120479 <a title="10-lsi-11" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>12 0.21644804 <a title="10-lsi-12" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>13 0.19915077 <a title="10-lsi-13" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>14 0.19856839 <a title="10-lsi-14" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>15 0.17096119 <a title="10-lsi-15" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>16 0.1629065 <a title="10-lsi-16" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<p>17 0.16131592 <a title="10-lsi-17" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>18 0.15748706 <a title="10-lsi-18" href="./jmlr-2008-Value_Function_Based_Reinforcement_Learning_in_Changing_Markovian_Environments.html">95 jmlr-2008-Value Function Based Reinforcement Learning in Changing Markovian Environments</a></p>
<p>19 0.15697385 <a title="10-lsi-19" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>20 0.15219076 <a title="10-lsi-20" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.043), (31, 0.012), (40, 0.027), (54, 0.029), (58, 0.027), (61, 0.092), (66, 0.042), (76, 0.04), (87, 0.459), (88, 0.042), (92, 0.03), (94, 0.041), (99, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86668557 <a title="10-lda-1" href="./jmlr-2008-Active_Learning_of_Causal_Networks_with_Intervention_Experiments_and_Optimal_Designs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">10 jmlr-2008-Active Learning of Causal Networks with Intervention Experiments and Optimal Designs    (Special Topic on Causality)</a></p>
<p>Author: Yang-Bo He, Zhi Geng</p><p>Abstract: The causal discovery from data is important for various scientiﬁc investigations. Because we cannot distinguish the different directed acyclic graphs (DAGs) in a Markov equivalence class learned from observational data, we have to collect further information on causal structures from experiments with external interventions. In this paper, we propose an active learning approach for discovering causal structures in which we ﬁrst ﬁnd a Markov equivalence class from observational data, and then we orient undirected edges in every chain component via intervention experiments separately. In the experiments, some variables are manipulated through external interventions. We discuss two kinds of intervention experiments, randomized experiment and quasi-experiment. Furthermore, we give two optimal designs of experiments, a batch-intervention design and a sequential-intervention design, to minimize the number of manipulated variables and the set of candidate structures based on the minimax and the maximum entropy criteria. We show theoretically that structural learning can be done locally in subgraphs of chain components without need of checking illegal v-structures and cycles in the whole network and that a Markov equivalence subclass obtained after each intervention can still be depicted as a chain graph. Keywords: active learning, causal networks, directed acyclic graphs, intervention, Markov equivalence class, optimal design, structural learning</p><p>2 0.54961079 <a title="10-lda-2" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>Author: Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, Peter L. Bartlett</p><p>Abstract: Log-linear and maximum-margin models are two commonly-used methods in supervised machine learning, and are frequently used in structured prediction problems. Efﬁcient learning of parameters in these models is therefore an important problem, and becomes a key factor when learning from very large data sets. This paper describes exponentiated gradient (EG) algorithms for training such models, where EG updates are applied to the convex dual of either the log-linear or maxmargin objective function; the dual in both the log-linear and max-margin cases corresponds to minimizing a convex function with simplex constraints. We study both batch and online variants of the algorithm, and provide rates of convergence for both cases. In the max-margin case, O( 1 ) EG ε updates are required to reach a given accuracy ε in the dual; in contrast, for log-linear models only O(log( 1 )) updates are required. For both the max-margin and log-linear cases, our bounds suggest ε that the online EG algorithm requires a factor of n less computation to reach a desired accuracy than the batch EG algorithm, where n is the number of training examples. Our experiments conﬁrm that the online algorithms are much faster than the batch algorithms in practice. We describe how the EG updates factor in a convenient way for structured prediction problems, allowing the algorithms to be efﬁciently applied to problems such as sequence learning or natural language parsing. We perform extensive evaluation of the algorithms, comparing them to L-BFGS and stochastic gradient descent for log-linear models, and to SVM-Struct for max-margin models. The algorithms are applied to a multi-class problem as well as to a more complex large-scale parsing task. In all these settings, the EG algorithms presented here outperform the other methods. Keywords: exponentiated gradient, log-linear models, maximum-margin models, structured prediction, conditional random ﬁelds ∗. These authors contributed equally. c 2008 Michael Col</p><p>3 0.35053578 <a title="10-lda-3" href="./jmlr-2008-Causal_Reasoning_with_Ancestral_Graphs%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">20 jmlr-2008-Causal Reasoning with Ancestral Graphs    (Special Topic on Causality)</a></p>
<p>Author: Jiji Zhang</p><p>Abstract: Causal reasoning is primarily concerned with what would happen to a system under external interventions. In particular, we are often interested in predicting the probability distribution of some random variables that would result if some other variables were forced to take certain values. One prominent approach to tackling this problem is based on causal Bayesian networks, using directed acyclic graphs as causal diagrams to relate post-intervention probabilities to pre-intervention probabilities that are estimable from observational data. However, such causal diagrams are seldom fully testable given observational data. In consequence, many causal discovery algorithms based on data-mining can only output an equivalence class of causal diagrams (rather than a single one). This paper is concerned with causal reasoning given an equivalence class of causal diagrams, represented by a (partial) ancestral graph. We present two main results. The ﬁrst result extends Pearl (1995)’s celebrated do-calculus to the context of ancestral graphs. In the second result, we focus on a key component of Pearl’s calculus—the property of invariance under interventions, and give stronger graphical conditions for this property than those implied by the ﬁrst result. The second result also improves the earlier, similar results due to Spirtes et al. (1993). Keywords: ancestral graphs, causal Bayesian network, do-calculus, intervention</p><p>4 0.34021047 <a title="10-lda-4" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>Author: Xianchao Xie, Zhi Geng</p><p>Abstract: In this paper, we propose a recursive method for structural learning of directed acyclic graphs (DAGs), in which a problem of structural learning for a large DAG is ﬁrst decomposed into two problems of structural learning for two small vertex subsets, each of which is then decomposed recursively into two problems of smaller subsets until none subset can be decomposed further. In our approach, search for separators of a pair of variables in a large DAG is localized to small subsets, and thus the approach can improve the efﬁciency of searches and the power of statistical tests for structural learning. We show how the recent advances in the learning of undirected graphical models can be employed to facilitate the decomposition. Simulations are given to demonstrate the performance of the proposed method. Keywords: Bayesian network, conditional independence, decomposition, directed acyclic graph, structural learning</p><p>5 0.301303 <a title="10-lda-5" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>Author: Zongming Ma, Xianchao Xie, Zhi Geng</p><p>Abstract: Chain graphs present a broad class of graphical models for description of conditional independence structures, including both Markov networks and Bayesian networks as special cases. In this paper, we propose a computationally feasible method for the structural learning of chain graphs based on the idea of decomposing the learning problem into a set of smaller scale problems on its decomposed subgraphs. The decomposition requires conditional independencies but does not require the separators to be complete subgraphs. Algorithms for both skeleton recovery and complex arrow orientation are presented. Simulations under a variety of settings demonstrate the competitive performance of our method, especially when the underlying graph is sparse. Keywords: chain graph, conditional independence, decomposition, graphical model, structural learning</p><p>6 0.28668943 <a title="10-lda-6" href="./jmlr-2008-Graphical_Methods_for_Efficient_Likelihood_Inference_in_Gaussian_Covariance_Models.html">40 jmlr-2008-Graphical Methods for Efficient Likelihood Inference in Gaussian Covariance Models</a></p>
<p>7 0.26950604 <a title="10-lda-7" href="./jmlr-2008-Consistency_of_Trace_Norm_Minimization.html">26 jmlr-2008-Consistency of Trace Norm Minimization</a></p>
<p>8 0.26938903 <a title="10-lda-8" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>9 0.22958376 <a title="10-lda-9" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>10 0.22895931 <a title="10-lda-10" href="./jmlr-2008-Complete_Identification_Methods_for_the_Causal_Hierarchy%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">24 jmlr-2008-Complete Identification Methods for the Causal Hierarchy    (Special Topic on Causality)</a></p>
<p>11 0.20720269 <a title="10-lda-11" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>12 0.19738007 <a title="10-lda-12" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>13 0.19619323 <a title="10-lda-13" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>14 0.19500312 <a title="10-lda-14" href="./jmlr-2008-Using_Markov_Blankets_for_Causal_Structure_Learning%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Causality%29.html">93 jmlr-2008-Using Markov Blankets for Causal Structure Learning    (Special Topic on Causality)</a></p>
<p>15 0.19383407 <a title="10-lda-15" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>16 0.19255769 <a title="10-lda-16" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>17 0.19241586 <a title="10-lda-17" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>18 0.19172589 <a title="10-lda-18" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>19 0.19160537 <a title="10-lda-19" href="./jmlr-2008-Support_Vector_Machinery_for_Infinite_Ensemble_Learning.html">89 jmlr-2008-Support Vector Machinery for Infinite Ensemble Learning</a></p>
<p>20 0.19140348 <a title="10-lda-20" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
