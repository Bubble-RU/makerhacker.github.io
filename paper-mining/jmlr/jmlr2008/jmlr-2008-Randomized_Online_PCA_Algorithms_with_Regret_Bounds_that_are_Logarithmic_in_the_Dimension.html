<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-78" href="#">jmlr2008-78</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</h1>
<br/><p>Source: <a title="jmlr-2008-78-pdf" href="http://jmlr.org/papers/volume9/warmuth08a/warmuth08a.pdf">pdf</a></p><p>Author: Manfred K. Warmuth, Dima Kuzmin</p><p>Abstract: We design an online algorithm for Principal Component Analysis. In each trial the current instance is centered and projected into a probabilistically chosen low dimensional subspace. The regret of our online algorithm, that is, the total expected quadratic compression loss of the online algorithm minus the total quadratic compression loss of the batch algorithm, is bounded by a term whose dependence on the dimension of the instances is only logarithmic. We ﬁrst develop our methodology in the expert setting of online learning by giving an algorithm for learning as well as the best subset of experts of a certain size. This algorithm is then lifted to the matrix setting where the subsets of experts correspond to subspaces. The algorithm represents the uncertainty over the best subspace as a density matrix whose eigenvalues are bounded. The running time is O(n2 ) per trial, where n is the dimension of the instances. Keywords: principal component analysis, online learning, density matrix, expert setting, quantum Bayes rule</p><p>Reference: <a title="jmlr-2008-78-reference" href="../jmlr2008_reference/jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Computer Science Department University of California - Santa Cruz Santa Cruz, CA, 95064  Editor: John Shawe-Taylor  Abstract We design an online algorithm for Principal Component Analysis. [sent-6, score-0.358]
</p><p>2 In each trial the current instance is centered and projected into a probabilistically chosen low dimensional subspace. [sent-7, score-0.298]
</p><p>3 The regret of our online algorithm, that is, the total expected quadratic compression loss of the online algorithm minus the total quadratic compression loss of the batch algorithm, is bounded by a term whose dependence on the dimension of the instances is only logarithmic. [sent-8, score-2.446]
</p><p>4 We ﬁrst develop our methodology in the expert setting of online learning by giving an algorithm for learning as well as the best subset of experts of a certain size. [sent-9, score-0.616]
</p><p>5 This algorithm is then lifted to the matrix setting where the subsets of experts correspond to subspaces. [sent-10, score-0.32]
</p><p>6 The algorithm represents the uncertainty over the best subspace as a density matrix whose eigenvalues are bounded. [sent-11, score-0.421]
</p><p>7 The running time is O(n2 ) per trial, where n is the dimension of the instances. [sent-12, score-0.024]
</p><p>8 Keywords: principal component analysis, online learning, density matrix, expert setting, quantum Bayes rule  1. [sent-13, score-0.539]
</p><p>9 Introduction In Principal Component Analysis (PCA) the n-dimensional data instances are projected into a kdimensional subspace (k < n) so that the total quadratic compression loss is minimized. [sent-14, score-0.822]
</p><p>10 After centering the data, the problem is equivalent to ﬁnding the eigenvectors of the k largest eigenvalues of the data covariance matrix. [sent-15, score-0.247]
</p><p>11 The goal is to obtain online algorithms whose total compression loss in all trials is T  close to the total compression loss min ∑ (xt − m) − P(xt − m) m, P  2 2  of the batch algorithm which can  t=1  choose its center and k-dimensional subspace in hindsight based on all T instances. [sent-18, score-1.906]
</p><p>12 WARMUTH AND K UZMIN  in this paper we obtain randomized online algorithms with bounded regret. [sent-24, score-0.383]
</p><p>13 Here we deﬁne regret as the difference between the total expected compression loss of the randomized online algorithm and the compression loss of the best mean and subspace of rank k chosen ofﬂine. [sent-25, score-1.995]
</p><p>14 In other words the regret is essentially the expected additional compression loss incurred by the online algorithm compared to normal batch PCA. [sent-26, score-1.408]
</p><p>15 The expectation is over the internal randomization of the algorithm. [sent-27, score-0.079]
</p><p>16 We begin by developing our online PCA algorithm for the uncentered case, that is, all mt = 0 and T  the compression loss of the ofﬂine comparator is simpliﬁed to min ∑ xt − Pxt P  2 2. [sent-28, score-1.218]
</p><p>17 In this simpler  t=1  case our algorithm is motivated by a related problem in the expert setting of online learning, where our goal is to perform as well as the best size k subset of experts. [sent-29, score-0.418]
</p><p>18 The algorithm maintains a mixture vector over the n experts. [sent-30, score-0.18]
</p><p>19 At the beginning of trial t the algorithm chooses a subset Pt−1 of k experts based on the current mixture vector wt−1 that summarizes the previous t − 1 trials. [sent-31, score-0.519]
</p><p>20 Now the subset Pt−1 corresponds to the subspace onto which we “compress” or “project” the data. [sent-33, score-0.163]
</p><p>21 The algorithm incurs no loss on the k components of Pt−1 and its compression loss equals the sum of the remaining n − k components of the loss vector, that is, ∑i∈{1,. [sent-34, score-0.871]
</p><p>22 The key insight is to maintain a mixture vector wt−1 as a parameter with the additional constraint 1 that wt−1 ≤ n−k . [sent-39, score-0.141]
</p><p>23 We will show that this “capped” mixture vector represents an implicit mixture i over all subsets of experts of size n − k, and given wt−1 we can efﬁciently sample a subset of size n − k from the implicit mixture and choose Pt−1 as the complementary subset of size k. [sent-40, score-0.687]
</p><p>24 This gives an online algorithm whose total loss over all trials is close to the smallest n − k components T of the total loss vector ∑t=1 t . [sent-41, score-0.756]
</p><p>25 We will show how this algorithm generalizes to an online PCA algorithm when the mixture vector wt−1 is replaced by a density matrix W t−1 whose eigenvalues are 1 capped by n−k . [sent-42, score-0.836]
</p><p>26 Now the constrained density matrix W t−1 represents an implicit mixture of (n − k)dimensional subspaces. [sent-43, score-0.29]
</p><p>27 Again, we can efﬁciently sample from this mixture, and the complementary k-dimensional subspace Pt−1 is used for projecting the current instance xt at trial t. [sent-44, score-0.523]
</p><p>28 A simple way to construct an online algorithm is to run the ofﬂine or batch algorithm on all data received so far and use the resulting hypothesis on the next data instance. [sent-45, score-0.656]
</p><p>29 When the ofﬂine algorithm just minimizes the loss on the past instances, then this algorithm is also called the “Follow the Leader (FL) Algorithm” (Kalai and Vempala, 2005). [sent-47, score-0.205]
</p><p>30 For uncentered PCA we can easily construct a sen quence of instances for which the total online compression loss of FL is n−k times larger than the total compression loss of batch PCA. [sent-48, score-1.839]
</p><p>31 However, in this paper we have a more stringent goal. [sent-49, score-0.03]
</p><p>32 We design randomized online algorithms whose total expected compression loss is at most one times the compression loss of batch PCA plus an additional lower order term which we optimize. [sent-50, score-1.633]
</p><p>33 In other words, we are seeking online algorithms with bounded regret. [sent-51, score-0.319]
</p><p>34 Our regret bounds are worst-case in that they hold for arbitrary sequences of instances. [sent-52, score-0.378]
</p><p>35 Simple online algorithms such as the Generalized Hebbian Algorithm (Sanger, 1989) have been investigated previously that provably converge to the best ofﬂine solution. [sent-53, score-0.319]
</p><p>36 No worst-case regret bounds have been proven for these algorithms. [sent-54, score-0.411]
</p><p>37 More recently, the online PCA problem was also addressed in Crammer (2006). [sent-55, score-0.319]
</p><p>38 However, that paper does not fully capture the PCA problem because the presented algorithm uses a full-rank matrix as its hypothesis in each trial, whereas we use a probabilistically chosen projection matrix of the desired rank k. [sent-56, score-0.322]
</p><p>39 Furthermore, that paper proves bounds on the ﬁltering loss, which are typically easier to obtain, and it is not clear how the ﬁltering loss relates to the more standard regret bounds for the compression loss proven in this paper. [sent-57, score-1.068]
</p><p>40 2288  O NLINE PCA  Our algorithm is unique in that we can prove a regret bound for it that is linear in the target dimension k of the subspace but logarithmic in the dimension of the instance space. [sent-58, score-0.646]
</p><p>41 The key methodology is to use a density matrix as the parameter and employ the quantum relative entropy as a regularizer and measure of progress. [sent-59, score-0.223]
</p><p>42 (2005) for a generalization of linear regression to the case when the parameter matrix is a density matrix. [sent-61, score-0.111]
</p><p>43 Our update of the density matrix can be seen as a “soft” version of computing the top k eigenvectors and eigenvalues of the covariance matrix. [sent-62, score-0.329]
</p><p>44 It involves matrix logarithms and exponentials which are seemingly more complicated than the FL Algorithm which simply picks the top k directions. [sent-63, score-0.118]
</p><p>45 Actually, the most expensive step in both algorithms is to update the eigendecomposition of the covariance matrix after each new instance, and this costs O(n2 ) time (see, e. [sent-64, score-0.154]
</p><p>46 We begin by introducing some basics about batch and online PCA (Section 2) as well as the Hedge Algorithm from the expert setting of online learning (Section 3). [sent-68, score-0.948]
</p><p>47 We then develop a version of this algorithm that learns as well as the best subset of experts of ﬁxed size (Section 4). [sent-69, score-0.204]
</p><p>48 When lifted to the matrix setting, this algorithm does uncentered PCA online (Section 5). [sent-70, score-0.62]
</p><p>49 Surprisingly, the regret bound for the matrix setting stays the same and this is an example of a phenomenon that has been dubbed the “free matrix lunch” (Warmuth, 2007b). [sent-71, score-0.464]
</p><p>50 We brieﬂy discuss the merits of various alternate algorithms in sections 4. [sent-72, score-0.024]
</p><p>51 Our online algorithm for centered online PCA is more involved since it has to learn the center as well (Section 6). [sent-75, score-0.756]
</p><p>52 After motivating the updates to the parameters (Section 6. [sent-76, score-0.023]
</p><p>53 1) we generalize our regret bound to the centered case (Section 6. [sent-77, score-0.376]
</p><p>54 We then brieﬂy describe how to construct batch PCA algorithms from our online algorithms via standard conversion techniques (Section 6. [sent-79, score-0.575]
</p><p>55 Surprisingly, the bounds obtained this way are competitive with the best known batch PCA bounds. [sent-81, score-0.268]
</p><p>56 A brief experimental evaluation is given in Section 8 and we conclude with an overview of online algorithms for matrix parameters and discuss a number of open problems (Section 9). [sent-83, score-0.383]
</p><p>57 Setup of Batch PCA and Online PCA Given a set (or batch) of instance vectors {x1 , . [sent-85, score-0.024]
</p><p>58 , xT }, the goal of batch PCA is to ﬁnd a lowdimensional approximation of this data that minimizes the quadratic compression loss. [sent-88, score-0.653]
</p><p>59 Speciﬁcally, we want to ﬁnd a center vector m ∈ Rn and a rank k projection matrix1 P such that the following loss function is minimized: T  comp(P, m) = ∑ (xt − m) − P(xt − m) 2 . [sent-89, score-0.273]
</p><p>60 Substituting this optimal x, x ∗ into loss (1) we obtain center m T  comp(P) =  ∑  t=1  T  (I − P)(xt − ¯ 2 = ∑ (xt − ¯ (I − P)2 (xt − ¯ x) 2 x) x)  = tr (I − P)2  t=1  T  x)(x x) ∑ (xt − ¯ t − ¯  . [sent-91, score-0.384]
</p><p>61 Projection matrices are symmetric matrices P with eigenvalues in {0, 1}. [sent-93, score-0.108]
</p><p>62 2289  WARMUTH AND K UZMIN  The sum of outer products in the above trace is called the data covariance matrix C. [sent-95, score-0.099]
</p><p>63 Since I − P is a projection matrix, (I − P)2 = I − P, and comp(P) = tr(( I − P )C) = tr(C) − tr( P C). [sent-96, score-0.057]
</p><p>64 rank k  rank n−k  We call the above loss the compression loss of P or the loss of subspace I − P. [sent-97, score-1.005]
</p><p>65 Observe that tr(C) equals tr(CP) + tr(C(I − P)), the sum of the losses of the complementary subspaces. [sent-99, score-0.073]
</p><p>66 However, we project the data into subspace P and the projected parts of the data are perfectly reconstructed. [sent-100, score-0.213]
</p><p>67 We charge the subspace P with the parts that are missed, that is, tr((I − P)C), and therefore call this the compression loss of P. [sent-101, score-0.651]
</p><p>68 We now show that tr(PC) is maximized (or tr((I − P)C) minimized) if P consists of the k eigendirections of C with the largest eigenvalues. [sent-102, score-0.068]
</p><p>69 By rewriting C in terms of its eigendecomposition, that is, C = ∑n γi ci ci , we can upper bound tr(PC) as follows: i=1 n  n  tr(PC) = ∑ γi tr(P ci ci ) = ∑ γi ci Pci ≤ i=1  i=1  n  ∑ γ i δi . [sent-104, score-0.57]
</p><p>70 A linear function is maximized at one of the vertices of its polytope of feasible solutions. [sent-106, score-0.166]
</p><p>71 The vertices of this polytope deﬁned by the constraints 0 ≤ δi ≤ 1 and ∑i δi = k are those δ vectors with exactly k ones and n − k zeros. [sent-107, score-0.127]
</p><p>72 Thus the vertices of the polytope correspond to sets of size k and k  tr(PC) ≤  max  ∑ γi . [sent-108, score-0.127]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('compression', 0.361), ('regret', 0.336), ('online', 0.319), ('pca', 0.245), ('batch', 0.226), ('tr', 0.218), ('warmuth', 0.168), ('pt', 0.164), ('subspace', 0.163), ('xt', 0.15), ('mixture', 0.141), ('experts', 0.138), ('trial', 0.136), ('pci', 0.134), ('loss', 0.127), ('comp', 0.119), ('dima', 0.119), ('uncentered', 0.119), ('ci', 0.114), ('ine', 0.114), ('eigenvalues', 0.108), ('mt', 0.103), ('wt', 0.097), ('polytope', 0.083), ('capped', 0.079), ('cruz', 0.079), ('kuzmin', 0.079), ('lifted', 0.079), ('quantum', 0.079), ('ucsc', 0.079), ('uzmin', 0.079), ('fl', 0.077), ('eigenvectors', 0.075), ('manfred', 0.072), ('incurs', 0.067), ('pc', 0.064), ('randomized', 0.064), ('matrix', 0.064), ('cse', 0.06), ('expert', 0.06), ('projection', 0.057), ('internal', 0.056), ('eigendecomposition', 0.055), ('santa', 0.051), ('rank', 0.05), ('projected', 0.05), ('complementary', 0.05), ('trials', 0.048), ('probabilistically', 0.048), ('total', 0.048), ('density', 0.047), ('vertices', 0.044), ('bounds', 0.042), ('centered', 0.04), ('algorithm', 0.039), ('maximized', 0.039), ('center', 0.039), ('implicit', 0.038), ('quadratic', 0.038), ('chooses', 0.038), ('logarithmic', 0.036), ('covariance', 0.035), ('instances', 0.035), ('principal', 0.034), ('quence', 0.034), ('ending', 0.034), ('sen', 0.034), ('hebbian', 0.034), ('nline', 0.034), ('methodology', 0.033), ('minimized', 0.033), ('received', 0.033), ('proven', 0.033), ('kalai', 0.03), ('stringent', 0.03), ('vempala', 0.03), ('hedge', 0.03), ('sanger', 0.03), ('tsuda', 0.03), ('conversion', 0.03), ('ltering', 0.029), ('largest', 0.029), ('gu', 0.028), ('compress', 0.028), ('lowdimensional', 0.028), ('cp', 0.028), ('logarithms', 0.028), ('summarizes', 0.027), ('develop', 0.027), ('surprisingly', 0.026), ('picks', 0.026), ('differentiating', 0.026), ('scalars', 0.026), ('instance', 0.024), ('dimension', 0.024), ('basics', 0.024), ('merits', 0.024), ('updates', 0.023), ('equals', 0.023), ('randomization', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="78-tfidf-1" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>Author: Manfred K. Warmuth, Dima Kuzmin</p><p>Abstract: We design an online algorithm for Principal Component Analysis. In each trial the current instance is centered and projected into a probabilistically chosen low dimensional subspace. The regret of our online algorithm, that is, the total expected quadratic compression loss of the online algorithm minus the total quadratic compression loss of the batch algorithm, is bounded by a term whose dependence on the dimension of the instances is only logarithmic. We ﬁrst develop our methodology in the expert setting of online learning by giving an algorithm for learning as well as the best subset of experts of a certain size. This algorithm is then lifted to the matrix setting where the subsets of experts correspond to subspaces. The algorithm represents the uncertainty over the best subspace as a density matrix whose eigenvalues are bounded. The running time is O(n2 ) per trial, where n is the dimension of the instances. Keywords: principal component analysis, online learning, density matrix, expert setting, quantum Bayes rule</p><p>2 0.14780867 <a title="78-tfidf-2" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>3 0.13842349 <a title="78-tfidf-3" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>Author: Mikio L. Braun, Joachim M. Buhmann, Klaus-Robert Müller</p><p>Abstract: We show that the relevant information of a supervised learning problem is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem in the sense that it can asymptotically represent the function to be learned and is sufﬁciently smooth. Thus, kernels do not only transform data sets such that good generalization can be achieved using only linear discriminant functions, but this transformation is also performed in a manner which makes economical use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data for supervised learning problems. Practically, we propose an algorithm which enables us to recover the number of leading kernel PCA components relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to aid in model selection, and (3) to denoise in feature space in order to yield better classiﬁcation results. Keywords: kernel methods, feature space, dimension reduction, effective dimensionality</p><p>4 0.13337222 <a title="78-tfidf-4" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>Author: Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui</p><p>Abstract: Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefﬁcients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semideﬁnite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefﬁcients, with total complexity O(n3 ), where n is the number of variables. We then use the same relaxation to derive sufﬁcient conditions for global optimality of a solution, which can be tested in O(n3 ) per pattern. We discuss applications in subset selection and sparse recovery and show on artiﬁcial examples and biological data that our algorithm does provide globally optimal solutions in many cases. Keywords: PCA, subset selection, sparse eigenvalues, sparse recovery, lasso</p><p>5 0.11658286 <a title="78-tfidf-5" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>Author: Eric Bax</p><p>Abstract: This paper develops bounds on out-of-sample error rates for support vector machines (SVMs). The bounds are based on the numbers of support vectors in the SVMs rather than on VC dimension. The bounds developed here improve on support vector counting bounds derived using Littlestone and Warmuth’s compression-based bounding technique. Keywords: compression, error bound, support vector machine, nearly uniform</p><p>6 0.099375024 <a title="78-tfidf-6" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>7 0.090412237 <a title="78-tfidf-7" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>8 0.078880154 <a title="78-tfidf-8" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>9 0.072571129 <a title="78-tfidf-9" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>10 0.053105917 <a title="78-tfidf-10" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>11 0.049456842 <a title="78-tfidf-11" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>12 0.041692041 <a title="78-tfidf-12" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>13 0.04025906 <a title="78-tfidf-13" href="./jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">53 jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>14 0.039504331 <a title="78-tfidf-14" href="./jmlr-2008-On_the_Suitable_Domain_for_SVM_Training_in_Image_Coding.html">73 jmlr-2008-On the Suitable Domain for SVM Training in Image Coding</a></p>
<p>15 0.037427053 <a title="78-tfidf-15" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>16 0.035962421 <a title="78-tfidf-16" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>17 0.032082174 <a title="78-tfidf-17" href="./jmlr-2008-A_Tutorial_on_Conformal_Prediction.html">7 jmlr-2008-A Tutorial on Conformal Prediction</a></p>
<p>18 0.031619228 <a title="78-tfidf-18" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>19 0.031143 <a title="78-tfidf-19" href="./jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations.html">37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</a></p>
<p>20 0.030969987 <a title="78-tfidf-20" href="./jmlr-2008-Consistency_of_Trace_Norm_Minimization.html">26 jmlr-2008-Consistency of Trace Norm Minimization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.203), (1, -0.074), (2, -0.136), (3, 0.021), (4, 0.07), (5, -0.014), (6, -0.063), (7, 0.023), (8, -0.39), (9, -0.168), (10, -0.046), (11, -0.041), (12, 0.007), (13, 0.002), (14, -0.167), (15, 0.327), (16, 0.124), (17, -0.089), (18, 0.163), (19, -0.113), (20, 0.218), (21, 0.11), (22, -0.055), (23, 0.033), (24, -0.026), (25, -0.098), (26, 0.001), (27, -0.056), (28, 0.113), (29, 0.024), (30, -0.078), (31, 0.056), (32, -0.044), (33, 0.003), (34, -0.021), (35, -0.055), (36, 0.032), (37, -0.012), (38, 0.126), (39, 0.075), (40, -0.007), (41, -0.046), (42, 0.027), (43, -0.122), (44, -0.123), (45, -0.003), (46, -0.098), (47, -0.083), (48, -0.007), (49, 0.103)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98572809 <a title="78-lsi-1" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>Author: Manfred K. Warmuth, Dima Kuzmin</p><p>Abstract: We design an online algorithm for Principal Component Analysis. In each trial the current instance is centered and projected into a probabilistically chosen low dimensional subspace. The regret of our online algorithm, that is, the total expected quadratic compression loss of the online algorithm minus the total quadratic compression loss of the batch algorithm, is bounded by a term whose dependence on the dimension of the instances is only logarithmic. We ﬁrst develop our methodology in the expert setting of online learning by giving an algorithm for learning as well as the best subset of experts of a certain size. This algorithm is then lifted to the matrix setting where the subsets of experts correspond to subspaces. The algorithm represents the uncertainty over the best subspace as a density matrix whose eigenvalues are bounded. The running time is O(n2 ) per trial, where n is the dimension of the instances. Keywords: principal component analysis, online learning, density matrix, expert setting, quantum Bayes rule</p><p>2 0.57816839 <a title="78-lsi-2" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>3 0.40078247 <a title="78-lsi-3" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>Author: Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui</p><p>Abstract: Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefﬁcients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semideﬁnite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefﬁcients, with total complexity O(n3 ), where n is the number of variables. We then use the same relaxation to derive sufﬁcient conditions for global optimality of a solution, which can be tested in O(n3 ) per pattern. We discuss applications in subset selection and sparse recovery and show on artiﬁcial examples and biological data that our algorithm does provide globally optimal solutions in many cases. Keywords: PCA, subset selection, sparse eigenvalues, sparse recovery, lasso</p><p>4 0.38005924 <a title="78-lsi-4" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>Author: Mikio L. Braun, Joachim M. Buhmann, Klaus-Robert Müller</p><p>Abstract: We show that the relevant information of a supervised learning problem is contained up to negligible error in a ﬁnite number of leading kernel PCA components if the kernel matches the underlying learning problem in the sense that it can asymptotically represent the function to be learned and is sufﬁciently smooth. Thus, kernels do not only transform data sets such that good generalization can be achieved using only linear discriminant functions, but this transformation is also performed in a manner which makes economical use of feature space dimensions. In the best case, kernels provide efﬁcient implicit representations of the data for supervised learning problems. Practically, we propose an algorithm which enables us to recover the number of leading kernel PCA components relevant for good classiﬁcation. Our algorithm can therefore be applied (1) to analyze the interplay of data set and kernel in a geometric fashion, (2) to aid in model selection, and (3) to denoise in feature space in order to yield better classiﬁcation results. Keywords: kernel methods, feature space, dimension reduction, effective dimensionality</p><p>5 0.35225084 <a title="78-lsi-5" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>Author: Arnak S. Dalalyan, Anatoly Juditsky, Vladimir Spokoiny</p><p>Abstract: The statistical problem of estimating the effective dimension-reduction (EDR) subspace in the multi-index regression model with deterministic design and additive noise is considered. A new procedure for recovering the directions of the EDR subspace is proposed. Many methods for estimating the EDR subspace perform principal component analysis on a family of vectors, say ˆ ˆ β1 , . . . , βL , nearly lying in the EDR subspace. This is in particular the case for the structure-adaptive approach proposed by Hristache et al. (2001a). In the present work, we propose to estimate the projector onto the EDR subspace by the solution to the optimization problem minimize ˆ ˆ max β (I − A)β =1,...,L subject to A ∈ Am∗ , where Am∗ is the set of all symmetric matrices with eigenvalues in [0, 1] and trace less than or equal √ to m∗ , with m∗ being the true structural dimension. Under mild assumptions, n-consistency of the proposed procedure is proved (up to a logarithmic factor) in the case when the structural dimension is not larger than 4. Moreover, the stochastic error of the estimator of the projector onto the EDR subspace is shown to depend on L logarithmically. This enables us to use a large number of vectors ˆ β for estimating the EDR subspace. The empirical behavior of the algorithm is studied through numerical simulations. Keywords: dimension-reduction, multi-index regression model, structure-adaptive approach, central subspace</p><p>6 0.30921164 <a title="78-lsi-6" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>7 0.29195401 <a title="78-lsi-7" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>8 0.27006799 <a title="78-lsi-8" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>9 0.25774178 <a title="78-lsi-9" href="./jmlr-2008-Nearly_Uniform_Validation_Improves_Compression-Based_Error_Bounds.html">68 jmlr-2008-Nearly Uniform Validation Improves Compression-Based Error Bounds</a></p>
<p>10 0.22846569 <a title="78-lsi-10" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>11 0.19371711 <a title="78-lsi-11" href="./jmlr-2008-Search_for_Additive_Nonlinear_Time_Series_Causal_Models.html">84 jmlr-2008-Search for Additive Nonlinear Time Series Causal Models</a></p>
<p>12 0.18606417 <a title="78-lsi-12" href="./jmlr-2008-Probabilistic_Characterization_of_Random_Decision_Trees.html">77 jmlr-2008-Probabilistic Characterization of Random Decision Trees</a></p>
<p>13 0.1757855 <a title="78-lsi-13" href="./jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">53 jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>14 0.16683996 <a title="78-lsi-14" href="./jmlr-2008-On_the_Suitable_Domain_for_SVM_Training_in_Image_Coding.html">73 jmlr-2008-On the Suitable Domain for SVM Training in Image Coding</a></p>
<p>15 0.16011809 <a title="78-lsi-15" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>16 0.16003673 <a title="78-lsi-16" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>17 0.1424222 <a title="78-lsi-17" href="./jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations.html">37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</a></p>
<p>18 0.14098024 <a title="78-lsi-18" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>19 0.13525124 <a title="78-lsi-19" href="./jmlr-2008-Near-Optimal_Sensor_Placements_in_Gaussian_Processes%3A_Theory%2C_Efficient_Algorithms_and_Empirical_Studies.html">67 jmlr-2008-Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies</a></p>
<p>20 0.13276801 <a title="78-lsi-20" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.01), (40, 0.052), (54, 0.015), (58, 0.057), (60, 0.446), (66, 0.074), (76, 0.019), (88, 0.036), (92, 0.069), (94, 0.113), (99, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.86721438 <a title="78-lda-1" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>Author: Manfred K. Warmuth, Dima Kuzmin</p><p>Abstract: We design an online algorithm for Principal Component Analysis. In each trial the current instance is centered and projected into a probabilistically chosen low dimensional subspace. The regret of our online algorithm, that is, the total expected quadratic compression loss of the online algorithm minus the total quadratic compression loss of the batch algorithm, is bounded by a term whose dependence on the dimension of the instances is only logarithmic. We ﬁrst develop our methodology in the expert setting of online learning by giving an algorithm for learning as well as the best subset of experts of a certain size. This algorithm is then lifted to the matrix setting where the subsets of experts correspond to subspaces. The algorithm represents the uncertainty over the best subspace as a density matrix whose eigenvalues are bounded. The running time is O(n2 ) per trial, where n is the dimension of the instances. Keywords: principal component analysis, online learning, density matrix, expert setting, quantum Bayes rule</p><p>2 0.8190828 <a title="78-lda-2" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>Author: Michiel Debruyne, Mia Hubert, Johan A.K. Suykens</p><p>Abstract: Recent results about the robustness of kernel methods involve the analysis of inﬂuence functions. By deﬁnition the inﬂuence function is closely related to leave-one-out criteria. In statistical learning, the latter is often used to assess the generalization of a method. In statistics, the inﬂuence function is used in a similar way to analyze the statistical efﬁciency of a method. Links between both worlds are explored. The inﬂuence function is related to the ﬁrst term of a Taylor expansion. Higher order inﬂuence functions are calculated. A recursive relation between these terms is found characterizing the full Taylor expansion. It is shown how to evaluate inﬂuence functions at a speciﬁc sample distribution to obtain an approximation of the leave-one-out error. A speciﬁc implementation is proposed using a L1 loss in the selection of the hyperparameters and a Huber loss in the estimation procedure. The parameter in the Huber loss controlling the degree of robustness is optimized as well. The resulting procedure gives good results, even when outliers are present in the data. Keywords: kernel based regression, robustness, stability, inﬂuence function, model selection</p><p>3 0.35099763 <a title="78-lda-3" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>Author: Yonatan Amit, Shai Shalev-Shwartz, Yoram Singer</p><p>Abstract: We describe and analyze an algorithmic framework for online classiﬁcation where each online trial consists of multiple prediction tasks that are tied together. We tackle the problem of updating the online predictor by deﬁning a projection problem in which each prediction task corresponds to a single linear constraint. These constraints are tied together through a single slack parameter. We then introduce a general method for approximately solving the problem by projecting simultaneously and independently on each constraint which corresponds to a prediction sub-problem, and then averaging the individual solutions. We show that this approach constitutes a feasible, albeit not necessarily optimal, solution of the original projection problem. We derive concrete simultaneous projection schemes and analyze them in the mistake bound model. We demonstrate the power of the proposed algorithm in experiments with synthetic data and with multiclass text categorization tasks. Keywords: online learning, parallel computation, mistake bounds, structured prediction</p><p>4 0.33151466 <a title="78-lda-4" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>Author: Thomas G. Dietterich, Guohua Hao, Adam Ashenfelter</p><p>Abstract: Conditional random ﬁelds (CRFs) provide a ﬂexible and powerful model for sequence labeling problems. However, existing learning algorithms are slow, particularly in problems with large numbers of potential input features and feature combinations. This paper describes a new algorithm for training CRFs via gradient tree boosting. In tree boosting, the CRF potential functions are represented as weighted sums of regression trees, which provide compact representations of feature interactions. So the algorithm does not explicitly consider the potentially large parameter space. As a result, gradient tree boosting scales linearly in the order of the Markov model and in the order of the feature interactions, rather than exponentially as in previous algorithms based on iterative scaling and gradient descent. Gradient tree boosting also makes it possible to use instance weighting (as in C4.5) and surrogate splitting (as in CART) to handle missing values. Experimental studies of the effectiveness of these two methods (as well as standard imputation and indicator feature methods) show that instance weighting is the best method in most cases when feature values are missing at random. Keywords: sequential supervised learning, conditional random ﬁelds, functional gradient, gradient tree boosting, missing values</p><p>5 0.33013293 <a title="78-lda-5" href="./jmlr-2008-Bouligand_Derivatives_and_Robustness_of_Support_Vector_Machines_for_Regression.html">19 jmlr-2008-Bouligand Derivatives and Robustness of Support Vector Machines for Regression</a></p>
<p>Author: Andreas Christmann, Arnout Van Messem</p><p>Abstract: We investigate robustness properties for a broad class of support vector machines with non-smooth loss functions. These kernel methods are inspired by convex risk minimization in inﬁnite dimensional Hilbert spaces. Leading examples are the support vector machine based on the ε-insensitive loss function, and kernel based quantile regression based on the pinball loss function. Firstly, we propose with the Bouligand inﬂuence function (BIF) a modiﬁcation of F.R. Hampel’s inﬂuence function. The BIF has the advantage of being positive homogeneous which is in general not true for Hampel’s inﬂuence function. Secondly, we show that many support vector machines based on a Lipschitz continuous loss function and a bounded kernel have a bounded BIF and are thus robust in the sense of robust statistics based on inﬂuence functions. Keywords: Bouligand derivatives, empirical risk minimization, inﬂuence function, robustness, support vector machines</p><p>6 0.32608423 <a title="78-lda-6" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>7 0.3235389 <a title="78-lda-7" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>8 0.32236645 <a title="78-lda-8" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>9 0.31928527 <a title="78-lda-9" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>10 0.31427655 <a title="78-lda-10" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>11 0.30929521 <a title="78-lda-11" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>12 0.30291322 <a title="78-lda-12" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>13 0.30273059 <a title="78-lda-13" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>14 0.30252472 <a title="78-lda-14" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>15 0.30138364 <a title="78-lda-15" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>16 0.29761338 <a title="78-lda-16" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>17 0.29629508 <a title="78-lda-17" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>18 0.28960407 <a title="78-lda-18" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>19 0.28845003 <a title="78-lda-19" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>20 0.28741148 <a title="78-lda-20" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
