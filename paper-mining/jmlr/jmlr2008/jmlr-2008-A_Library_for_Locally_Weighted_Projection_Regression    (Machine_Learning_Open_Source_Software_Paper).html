<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-2" href="#">jmlr2008-2</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</h1>
<br/><p>Source: <a title="jmlr-2008-2-pdf" href="http://jmlr.org/papers/volume9/klanke08a/klanke08a.pdf">pdf</a></p><p>Author: Stefan Klanke, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages. Keywords: regression, local learning, online learning, C, C++, Matlab, Octave, Python</p><p>Reference: <a title="jmlr-2008-2-reference" href="../jmlr2008_reference/jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages. [sent-10, score-0.102]
</p><p>2 Keywords: regression, local learning, online learning, C, C++, Matlab, Octave, Python  1. [sent-11, score-0.104]
</p><p>3 Introduction Locally weighted projection regression (LWPR) is an algorithm that achieves nonlinear function approximation in high dimensional spaces even in the presence of redundant and irrelevant input dimensions (Vijayakumar et al. [sent-12, score-0.151]
</p><p>4 At its core, it uses locally linear models, spanned by a small number of univariate regressions in selected directions in input space. [sent-14, score-0.126]
</p><p>5 This nonparametric local learning system learns rapidly with second order learning methods based on incremental training, using statistically sound stochastic cross validation. [sent-15, score-0.1]
</p><p>6 The implementation of LWPR we present in this work is written in low-level C, requires no additional libraries, and comes with convenient wrappers for C ++, Matlab and Python. [sent-16, score-0.131]
</p><p>7 Together with documentation, tutorials and additional supporting materials, it is freely available for download from http://www. [sent-17, score-0.06]
</p><p>8 The LWPR Algorithm The goal of LWPR is to learn a regression function from training data that incrementally arrive as input-output tuples (xi , yi ), where we assume univariate output data for now. [sent-24, score-0.164]
</p><p>9 The LWPR regression function is constructed by blending local linear models ψ k (x) in the form  f (x) =  1 K ∑ wk (x)ψk (x), W (x) k=1  c 2008 Stefan Klanke, Sethu Vijayakumar and Stefan Schaal. [sent-25, score-0.26]
</p><p>10 During training, all updates to the local models are weighted by their activation w k (x), facilitating fully localised and inpedendent learning. [sent-28, score-0.327]
</p><p>11 If no existing local model yields an activation above a certain threshold (e. [sent-29, score-0.116]
</p><p>12 1), a new local model is created with its center ck set to the input datum. [sent-32, score-0.158]
</p><p>13 Thus, the number K of local models is adapted automatically. [sent-33, score-0.146]
</p><p>14 For learning the linear models ψk (x) themselves, LWPR employs an online formulation of weighted partial least squares (PLS) regression. [sent-34, score-0.107]
</p><p>15 The output of the local model is then formed by a linear combination of the latent variables (also called PLS factor loadings) R  ψk (x) = β0 + ∑ βi si . [sent-36, score-0.198]
</p><p>16 (3)  i=1  The number R of regression directions is automatically adapted to the local dimensionality of the training data, and the parameters ui , pi , and βi can be robustly estimated from accumulating certain statistics of the training set (for details please see Vijayakumar et al. [sent-37, score-0.294]
</p><p>17 In a similar fashion, the distance metrics D can be adapted using stochastic cross-validation, such that the input space is covered by wide receptive ﬁelds in regions of low curvature, and narrow receptive ﬁelds where the curvature is high. [sent-39, score-0.497]
</p><p>18 The initial distance metric assigned to a newly created receptive ﬁeld is a rather critical open parameter. [sent-40, score-0.204]
</p><p>19 As a valuable feature of the algorithm, LWPR can optionally yield conﬁdence bounds for its predictions (Vijayakumar et al. [sent-42, score-0.078]
</p><p>20 There are two possible choices with regard to handling multivariate output data: First, the local models itself could be made multivariate, in which case only one layer of receptive ﬁelds is needed. [sent-44, score-0.388]
</p><p>21 Alternately, one can learn all output dimensions independently, effectively using univariate PLS regression (see, e. [sent-45, score-0.175]
</p><p>22 In the present implementation we use the latter approach, which is computationally more costly for many output dimensions, but usually exhibits superior prediction performance. [sent-48, score-0.156]
</p><p>23 LWPR is an algorithm that is particularly suited (and recommended) for regression problems with a sufﬁciently large number of training examples. [sent-49, score-0.036]
</p><p>24 Details of the Implementation This section describes several important aspects of our implementation, all of which are related to execution speed. [sent-52, score-0.064]
</p><p>25 For notational convenience we drop the index k of the local model. [sent-54, score-0.073]
</p><p>26 1 Data Structures and Memory Allocation On modern computers, the speed at which many algorithms run does not only depend on the processor, but also critically on the speed of memory access. [sent-56, score-0.066]
</p><p>27 We designed our library so that memory access is as continuous as possible, thus minimising the chance of cache misses. [sent-57, score-0.197]
</p><p>28 In our library, all variables2 of each local model are allocated together in a contiguous piece of memory. [sent-58, score-0.127]
</p><p>29 Moreover, we allocate “workspaces” as part of the LWPR model for storing intermediate results, such that no further allocations are needed during the computations. [sent-59, score-0.063]
</p><p>30 The library currently supports POSIX threads on Linux/Unix machines and native threads on the Windows platform. [sent-62, score-0.508]
</p><p>31 The desired number of threads has to be deﬁned at compile time, and threading can be deactivated altogether. [sent-63, score-0.297]
</p><p>32 In order to balance the overhead of creating threads with the expected reduction in computation time, we chose to implement two complimentary strategies for distributing the work. [sent-64, score-0.218]
</p><p>33 Predictions of the LWPR model are split up on a per-output-dimension level, which implies that LWPR models for one-dimensional output data will not be accelerated by using multiple threads. [sent-65, score-0.079]
</p><p>34 The more costly update operations, however, are distributed across multiple threads on the receptive ﬁeld level, so even single-output models may beneﬁt (see Fig. [sent-66, score-0.477]
</p><p>35 1/5  1/2 1/3 1/4  1/6 1/7 1/8  1/9 1/10  2/1 2/2  2/5 2/6  2/3 2/4  2/7 2/8  3/1 3/2 3/3 3/4  3/5  3/9  3/6 3/7 3/8  Figure 1: Illustration of our threading implementation for the case of (up to) 4 threads. [sent-68, score-0.133]
</p><p>36 The example LWPR model has 3 output dimensions with 10, 8, and 9 receptive ﬁelds, respectively. [sent-69, score-0.29]
</p><p>37 A label M/N denotes the N-th receptive ﬁeld in the M-th output sub-model. [sent-70, score-0.251]
</p><p>38 For predictions, each thread handles a different output dimension. [sent-71, score-0.047]
</p><p>39 For updates, the workload of each output dimension is split up among threads, and outputs are handled one after another. [sent-72, score-0.047]
</p><p>40 3 Fast Computation of Predictions and Their Gradients For some applications of LWPR, it may be useful to compute analytic derivatives of the model, for example, to retrieve the Jacobian from a learned forward kinematics relation. [sent-74, score-0.032]
</p><p>41 The gradient of a single predicted output (1) is given by 1 ∂ f (x) = ∂x W  ∑ k  ∂wk ∂ψk ψk + w k ∂x ∂x  −  ∂wl 1 wψ , 2 ∑ k k ∑ ∂x W k l  2. [sent-75, score-0.047]
</p><p>42 Local models require no less than 27 variables, most of them vectors or matrices, for storing all the memory terms and sufﬁcient statistics, etc. [sent-76, score-0.107]
</p><p>43 625  K LANKE , V IJAYAKUMAR AND S CHAAL  where ∂wk = −wk Dk (x − ck ) for the Gaussian kernel (2). [sent-77, score-0.085]
</p><p>44 The local models ψk (x) are computed by ∂x PLS recursions, and therefore also the gradients of (3) have to be calculated in this rather costly way: ∂ψ ∂x  R  =  i=1  =  ∂si  R  ∑ βi ∂x0 = ∑ βi i=1  uT i  ∂xi−1 ∂x0  T  R  = ∑ βi  ∂x1 ∂xi−1 . [sent-78, score-0.191]
</p><p>45 However, between updates (for example, during prediction-only cycles) or after training has ﬁnished, the slopes ∂ψ of the local models do not change. [sent-85, score-0.277]
</p><p>46 Our implementation exploits this by memo∂x rising the slopes once a gradient is calculated, and directly using these slopes for predictions without running through the PLS recursions. [sent-86, score-0.358]
</p><p>47 4 Matlab Interface Our library started its life as a Matlab-only implementation, and therefore the Matlab struct describing an LWPR model is practically identical to the data structure used within the C library. [sent-88, score-0.099]
</p><p>48 When calling the C functions from Matlab via MEX-wrappers, however, these data structures normally have to be converted back and forth, which is time-consuming. [sent-89, score-0.039]
</p><p>49 Therefore, we added a special storage mechanism to our MEX-wrappers, which allows us to transfer Matlab data to and from Cmanaged memory. [sent-90, score-0.058]
</p><p>50 Then, updates and predictions of an LWPR model can be computed by calling the “normal” MEX-functions, but passing a certain reference identiﬁer instead of the complete Matlab struct. [sent-91, score-0.194]
</p><p>51 For accomplishing 10,000 updates, the Matlab-only implementation took roughly 100 seconds, using the MEX wrappers alone took 11. [sent-93, score-0.219]
</p><p>52 3s, but with our storage mechanism the task is ﬁnished after only 0. [sent-94, score-0.058]
</p><p>53 The Matlab implementation—including the MEX-wrappers and the storage scheme—is also compatible with recent versions of Octave. [sent-96, score-0.058]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lwpr', 0.71), ('vijayakumar', 0.299), ('receptive', 0.204), ('threads', 0.19), ('pls', 0.187), ('wk', 0.119), ('klanke', 0.112), ('sethu', 0.112), ('matlab', 0.105), ('library', 0.099), ('stefan', 0.097), ('slopes', 0.095), ('ck', 0.085), ('predictions', 0.078), ('updates', 0.077), ('chaal', 0.075), ('ijayakumar', 0.075), ('lanke', 0.075), ('octave', 0.075), ('souza', 0.075), ('threading', 0.075), ('local', 0.073), ('wrappers', 0.073), ('execution', 0.064), ('dk', 0.061), ('implementation', 0.058), ('pt', 0.058), ('storage', 0.058), ('nished', 0.057), ('univariate', 0.053), ('ut', 0.053), ('costly', 0.051), ('eld', 0.049), ('curvature', 0.048), ('output', 0.047), ('si', 0.046), ('locally', 0.045), ('edinburgh', 0.045), ('elds', 0.045), ('pi', 0.045), ('weighted', 0.044), ('activation', 0.043), ('adapted', 0.041), ('memory', 0.04), ('dimensions', 0.039), ('calling', 0.039), ('computers', 0.039), ('ui', 0.039), ('uk', 0.036), ('regression', 0.036), ('gradients', 0.035), ('storing', 0.035), ('models', 0.032), ('projection', 0.032), ('accomplishing', 0.032), ('accumulating', 0.032), ('alternately', 0.032), ('compile', 0.032), ('download', 0.032), ('eighted', 0.032), ('ibrary', 0.032), ('kinematics', 0.032), ('localised', 0.032), ('mainstream', 0.032), ('minimising', 0.032), ('orthogonality', 0.032), ('processors', 0.032), ('recursions', 0.032), ('rising', 0.032), ('schaal', 0.032), ('shibata', 0.032), ('capable', 0.032), ('handling', 0.032), ('latent', 0.032), ('online', 0.031), ('supports', 0.029), ('allocate', 0.028), ('angeles', 0.028), ('distributing', 0.028), ('freely', 0.028), ('incrementally', 0.028), ('libraries', 0.028), ('materials', 0.028), ('memo', 0.028), ('piece', 0.028), ('robots', 0.028), ('soeren', 0.028), ('southern', 0.028), ('wl', 0.028), ('took', 0.028), ('directions', 0.028), ('incremental', 0.027), ('xi', 0.026), ('cache', 0.026), ('contiguous', 0.026), ('critically', 0.026), ('facilitating', 0.026), ('forth', 0.026), ('jacobian', 0.026), ('loadings', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="2-tfidf-1" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Stefan Klanke, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages. Keywords: regression, local learning, online learning, C, C++, Matlab, Octave, Python</p><p>2 0.076479286 <a title="2-tfidf-2" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>3 0.059189789 <a title="2-tfidf-3" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>4 0.038904931 <a title="2-tfidf-4" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>Author: Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, Peter L. Bartlett</p><p>Abstract: Log-linear and maximum-margin models are two commonly-used methods in supervised machine learning, and are frequently used in structured prediction problems. Efﬁcient learning of parameters in these models is therefore an important problem, and becomes a key factor when learning from very large data sets. This paper describes exponentiated gradient (EG) algorithms for training such models, where EG updates are applied to the convex dual of either the log-linear or maxmargin objective function; the dual in both the log-linear and max-margin cases corresponds to minimizing a convex function with simplex constraints. We study both batch and online variants of the algorithm, and provide rates of convergence for both cases. In the max-margin case, O( 1 ) EG ε updates are required to reach a given accuracy ε in the dual; in contrast, for log-linear models only O(log( 1 )) updates are required. For both the max-margin and log-linear cases, our bounds suggest ε that the online EG algorithm requires a factor of n less computation to reach a desired accuracy than the batch EG algorithm, where n is the number of training examples. Our experiments conﬁrm that the online algorithms are much faster than the batch algorithms in practice. We describe how the EG updates factor in a convenient way for structured prediction problems, allowing the algorithms to be efﬁciently applied to problems such as sequence learning or natural language parsing. We perform extensive evaluation of the algorithms, comparing them to L-BFGS and stochastic gradient descent for log-linear models, and to SVM-Struct for max-margin models. The algorithms are applied to a multi-class problem as well as to a more complex large-scale parsing task. In all these settings, the EG algorithms presented here outperform the other methods. Keywords: exponentiated gradient, log-linear models, maximum-margin models, structured prediction, conditional random ﬁelds ∗. These authors contributed equally. c 2008 Michael Col</p><p>5 0.038219526 <a title="2-tfidf-5" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: We propose a highly efﬁcient framework for penalized likelihood kernel methods applied to multiclass models with a large, structured set of classes. As opposed to many previous approaches which try to decompose the ﬁtting problem into many smaller ones, we focus on a Newton optimization of the complete model, making use of model structure and linear conjugate gradients in order to approximate Newton search directions. Crucially, our learning method is based entirely on matrix-vector multiplication primitives with the kernel matrices and their derivatives, allowing straightforward specialization to new kernels, and focusing code optimization efforts to these primitives only. Kernel parameters are learned automatically, by maximizing the cross-validation log likelihood in a gradient-based way, and predictive probabilities are estimated. We demonstrate our approach on large scale text classiﬁcation tasks with hierarchical structure on thousands of classes, achieving state-of-the-art results in an order of magnitude less time than previous work. Parts of this work appeared in the conference paper Seeger (2007). Keywords: multi-way classiﬁcation, kernel logistic regression, hierarchical classiﬁcation, cross validation optimization, Newton-Raphson optimization</p><p>6 0.03234525 <a title="2-tfidf-6" href="./jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">53 jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>7 0.027774911 <a title="2-tfidf-7" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>8 0.026007978 <a title="2-tfidf-8" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>9 0.025237132 <a title="2-tfidf-9" href="./jmlr-2008-On_Relevant_Dimensions_in_Kernel_Feature_Spaces.html">70 jmlr-2008-On Relevant Dimensions in Kernel Feature Spaces</a></p>
<p>10 0.024771262 <a title="2-tfidf-10" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>11 0.023469675 <a title="2-tfidf-11" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>12 0.023024226 <a title="2-tfidf-12" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>13 0.022438947 <a title="2-tfidf-13" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>14 0.022349646 <a title="2-tfidf-14" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>15 0.021749644 <a title="2-tfidf-15" href="./jmlr-2008-Shark%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">85 jmlr-2008-Shark    (Machine Learning Open Source Software Paper)</a></p>
<p>16 0.020829679 <a title="2-tfidf-16" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>17 0.019457541 <a title="2-tfidf-17" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>18 0.018914098 <a title="2-tfidf-18" href="./jmlr-2008-Finding_Optimal_Bayesian_Network_Given_a_Super-Structure.html">35 jmlr-2008-Finding Optimal Bayesian Network Given a Super-Structure</a></p>
<p>19 0.017216561 <a title="2-tfidf-19" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>20 0.01718892 <a title="2-tfidf-20" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.102), (1, -0.04), (2, 0.007), (3, 0.196), (4, -0.027), (5, -0.019), (6, -0.022), (7, -0.014), (8, -0.009), (9, -0.007), (10, -0.05), (11, 0.036), (12, 0.002), (13, -0.003), (14, 0.018), (15, 0.083), (16, 0.005), (17, -0.014), (18, 0.067), (19, -0.047), (20, 0.029), (21, 0.126), (22, -0.058), (23, 0.009), (24, 0.003), (25, 0.039), (26, -0.177), (27, -0.019), (28, 0.001), (29, -0.047), (30, 0.076), (31, -0.158), (32, 0.021), (33, -0.102), (34, 0.002), (35, 0.328), (36, -0.171), (37, 0.084), (38, -0.297), (39, -0.084), (40, 0.098), (41, -0.499), (42, -0.285), (43, 0.049), (44, -0.039), (45, 0.159), (46, -0.084), (47, -0.197), (48, 0.148), (49, -0.058)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97103351 <a title="2-lsi-1" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Stefan Klanke, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages. Keywords: regression, local learning, online learning, C, C++, Matlab, Octave, Python</p><p>2 0.24045251 <a title="2-lsi-2" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>Author: Michael Collins, Amir Globerson, Terry Koo, Xavier Carreras, Peter L. Bartlett</p><p>Abstract: Log-linear and maximum-margin models are two commonly-used methods in supervised machine learning, and are frequently used in structured prediction problems. Efﬁcient learning of parameters in these models is therefore an important problem, and becomes a key factor when learning from very large data sets. This paper describes exponentiated gradient (EG) algorithms for training such models, where EG updates are applied to the convex dual of either the log-linear or maxmargin objective function; the dual in both the log-linear and max-margin cases corresponds to minimizing a convex function with simplex constraints. We study both batch and online variants of the algorithm, and provide rates of convergence for both cases. In the max-margin case, O( 1 ) EG ε updates are required to reach a given accuracy ε in the dual; in contrast, for log-linear models only O(log( 1 )) updates are required. For both the max-margin and log-linear cases, our bounds suggest ε that the online EG algorithm requires a factor of n less computation to reach a desired accuracy than the batch EG algorithm, where n is the number of training examples. Our experiments conﬁrm that the online algorithms are much faster than the batch algorithms in practice. We describe how the EG updates factor in a convenient way for structured prediction problems, allowing the algorithms to be efﬁciently applied to problems such as sequence learning or natural language parsing. We perform extensive evaluation of the algorithms, comparing them to L-BFGS and stochastic gradient descent for log-linear models, and to SVM-Struct for max-margin models. The algorithms are applied to a multi-class problem as well as to a more complex large-scale parsing task. In all these settings, the EG algorithms presented here outperform the other methods. Keywords: exponentiated gradient, log-linear models, maximum-margin models, structured prediction, conditional random ﬁelds ∗. These authors contributed equally. c 2008 Michael Col</p><p>3 0.20211931 <a title="2-lsi-3" href="./jmlr-2008-Learning_to_Combine_Motor_Primitives_Via_Greedy_Additive_Regression.html">53 jmlr-2008-Learning to Combine Motor Primitives Via Greedy Additive Regression</a></p>
<p>Author: Manu Chhabra, Robert A. Jacobs</p><p>Abstract: The computational complexities arising in motor control can be ameliorated through the use of a library of motor synergies. We present a new model, referred to as the Greedy Additive Regression (GAR) model, for learning a library of torque sequences, and for learning the coefﬁcients of a linear combination of sequences minimizing a cost function. From the perspective of numerical optimization, the GAR model is interesting because it creates a library of “local features”—each sequence in the library is a solution to a single training task—and learns to combine these sequences using a local optimization procedure, namely, additive regression. We speculate that learners with local representational primitives and local optimization procedures will show good performance on nonlinear tasks. The GAR model is also interesting from the perspective of motor control because it outperforms several competing models. Results using a simulated two-joint arm suggest that the GAR model consistently shows excellent performance in the sense that it rapidly learns to perform novel, complex motor tasks. Moreover, its library is overcomplete and sparse, meaning that only a small fraction of the stored torque sequences are used when learning a new movement. The library is also robust in the sense that, after an initial training period, nearly all novel movements can be learned as additive combinations of sequences in the library, and in the sense that it shows good generalization when an arm’s dynamics are altered between training and test conditions, such as when a payload is added to the arm. Lastly, the GAR model works well regardless of whether motor tasks are speciﬁed in joint space or Cartesian space. We conclude that learning techniques using local primitives and optimization procedures are viable and potentially important methods for motor control and possibly other domains, and that these techniques deserve further examination by the artiﬁcial intelligence and cognitive science</p><p>4 0.17555583 <a title="2-lsi-4" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>5 0.15070958 <a title="2-lsi-5" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>Author: Arnak S. Dalalyan, Anatoly Juditsky, Vladimir Spokoiny</p><p>Abstract: The statistical problem of estimating the effective dimension-reduction (EDR) subspace in the multi-index regression model with deterministic design and additive noise is considered. A new procedure for recovering the directions of the EDR subspace is proposed. Many methods for estimating the EDR subspace perform principal component analysis on a family of vectors, say ˆ ˆ β1 , . . . , βL , nearly lying in the EDR subspace. This is in particular the case for the structure-adaptive approach proposed by Hristache et al. (2001a). In the present work, we propose to estimate the projector onto the EDR subspace by the solution to the optimization problem minimize ˆ ˆ max β (I − A)β =1,...,L subject to A ∈ Am∗ , where Am∗ is the set of all symmetric matrices with eigenvalues in [0, 1] and trace less than or equal √ to m∗ , with m∗ being the true structural dimension. Under mild assumptions, n-consistency of the proposed procedure is proved (up to a logarithmic factor) in the case when the structural dimension is not larger than 4. Moreover, the stochastic error of the estimator of the projector onto the EDR subspace is shown to depend on L logarithmically. This enables us to use a large number of vectors ˆ β for estimating the EDR subspace. The empirical behavior of the algorithm is studied through numerical simulations. Keywords: dimension-reduction, multi-index regression model, structure-adaptive approach, central subspace</p><p>6 0.13835502 <a title="2-lsi-6" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>7 0.12958193 <a title="2-lsi-7" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>8 0.1224561 <a title="2-lsi-8" href="./jmlr-2008-Shark%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">85 jmlr-2008-Shark    (Machine Learning Open Source Software Paper)</a></p>
<p>9 0.12031855 <a title="2-lsi-9" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>10 0.11769669 <a title="2-lsi-10" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>11 0.11306977 <a title="2-lsi-11" href="./jmlr-2008-Forecasting_Web_Page_Views%3A_Methods_and_Observations.html">37 jmlr-2008-Forecasting Web Page Views: Methods and Observations</a></p>
<p>12 0.10926824 <a title="2-lsi-12" href="./jmlr-2008-Visualizing_Data_using_t-SNE.html">96 jmlr-2008-Visualizing Data using t-SNE</a></p>
<p>13 0.10665867 <a title="2-lsi-13" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>14 0.10155875 <a title="2-lsi-14" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>15 0.101288 <a title="2-lsi-15" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>16 0.093654409 <a title="2-lsi-16" href="./jmlr-2008-A_Bahadur_Representation_of_the_Linear_Support_Vector_Machine.html">1 jmlr-2008-A Bahadur Representation of the Linear Support Vector Machine</a></p>
<p>17 0.093129635 <a title="2-lsi-17" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>18 0.091161117 <a title="2-lsi-18" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<p>19 0.088216357 <a title="2-lsi-19" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>20 0.087934524 <a title="2-lsi-20" href="./jmlr-2008-Dynamic_Hierarchical_Markov_Random_Fields_for_Integrated_Web_Data_Extraction.html">31 jmlr-2008-Dynamic Hierarchical Markov Random Fields for Integrated Web Data Extraction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.013), (31, 0.011), (40, 0.022), (58, 0.017), (66, 0.031), (76, 0.01), (88, 0.026), (92, 0.016), (94, 0.715), (99, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98545939 <a title="2-lda-1" href="./jmlr-2008-A_Library_for_Locally_Weighted_Projection_Regression%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">2 jmlr-2008-A Library for Locally Weighted Projection Regression    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Stefan Klanke, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In this paper we introduce an improved implementation of locally weighted projection regression (LWPR), a supervised learning algorithm that is capable of handling high-dimensional input data. As the key features, our code supports multi-threading, is available for multiple platforms, and provides wrappers for several programming languages. Keywords: regression, local learning, online learning, C, C++, Matlab, Octave, Python</p><p>2 0.95453614 <a title="2-lda-2" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>Author: Chih-Jen Lin, Ruby C. Weng, S. Sathiya Keerthi</p><p>Abstract: Large-scale logistic regression arises in many applications such as document classiﬁcation and natural language processing. In this paper, we apply a trust region Newton method to maximize the log-likelihood of the logistic regression model. The proposed method uses only approximate Newton steps in the beginning, but achieves fast convergence in the end. Experiments show that it is faster than the commonly used quasi Newton approach for logistic regression. We also extend the proposed method to large-scale L2-loss linear support vector machines (SVM). Keywords: logistic regression, newton method, trust region, conjugate gradient, support vector machines</p><p>3 0.93158156 <a title="2-lda-3" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>Author: Suhrid Balakrishnan, David Madigan</p><p>Abstract: Classiﬁers favoring sparse solutions, such as support vector machines, relevance vector machines, LASSO-regression based classiﬁers, etc., provide competitive methods for classiﬁcation problems in high dimensions. However, current algorithms for training sparse classiﬁers typically scale quite unfavorably with respect to the number of training examples. This paper proposes online and multipass algorithms for training sparse linear classiﬁers for high dimensional data. These algorithms have computational complexity and memory requirements that make learning on massive data sets feasible. The central idea that makes this possible is a straightforward quadratic approximation to the likelihood function. Keywords: Laplace approximation, expectation propagation, LASSO</p><p>4 0.79926628 <a title="2-lda-4" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>Author: Kai-Wei Chang, Cho-Jui Hsieh, Chih-Jen Lin</p><p>Abstract: Linear support vector machines (SVM) are useful for classifying large-scale sparse data. Problems with sparse features are common in applications such as document classiﬁcation and natural language processing. In this paper, we propose a novel coordinate descent algorithm for training linear SVM with the L2-loss function. At each step, the proposed method minimizes a one-variable sub-problem while ﬁxing other variables. The sub-problem is solved by Newton steps with the line search technique. The procedure globally converges at the linear rate. As each sub-problem involves only values of a corresponding feature, the proposed approach is suitable when accessing a feature is more convenient than accessing an instance. Experiments show that our method is more efﬁcient and stable than state of the art methods such as Pegasos and TRON. Keywords: linear support vector machines, document classiﬁcation, coordinate descent</p><p>5 0.53099322 <a title="2-lda-5" href="./jmlr-2008-LIBLINEAR%3A_A_Library_for_Large_Linear_Classification%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">46 jmlr-2008-LIBLINEAR: A Library for Large Linear Classification    (Machine Learning Open Source Software Paper)</a></p>
<p>Author: Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, Chih-Jen Lin</p><p>Abstract: LIBLINEAR is an open source library for large-scale linear classiﬁcation. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efﬁcient on large sparse data sets. Keywords: large-scale linear classiﬁcation, logistic regression, support vector machines, open source, machine learning</p><p>6 0.51910281 <a title="2-lda-6" href="./jmlr-2008-Randomized_Online_PCA_Algorithms_with_Regret_Bounds_that_are_Logarithmic_in_the_Dimension.html">78 jmlr-2008-Randomized Online PCA Algorithms with Regret Bounds that are Logarithmic in the Dimension</a></p>
<p>7 0.51154059 <a title="2-lda-7" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>8 0.51004612 <a title="2-lda-8" href="./jmlr-2008-Gradient_Tree_Boosting_for_Training_Conditional_Random_Fields.html">39 jmlr-2008-Gradient Tree Boosting for Training Conditional Random Fields</a></p>
<p>9 0.50580412 <a title="2-lda-9" href="./jmlr-2008-Exponentiated_Gradient_Algorithms_for_Conditional_Random_Fields_and_Max-Margin_Markov_Networks.html">34 jmlr-2008-Exponentiated Gradient Algorithms for Conditional Random Fields and Max-Margin Markov Networks</a></p>
<p>10 0.48852119 <a title="2-lda-10" href="./jmlr-2008-SimpleMKL.html">86 jmlr-2008-SimpleMKL</a></p>
<p>11 0.45107099 <a title="2-lda-11" href="./jmlr-2008-Maximal_Causes_for_Non-linear_Component_Extraction.html">59 jmlr-2008-Maximal Causes for Non-linear Component Extraction</a></p>
<p>12 0.45091423 <a title="2-lda-12" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>13 0.44989407 <a title="2-lda-13" href="./jmlr-2008-Optimization_Techniques_for_Semi-Supervised_Support_Vector_Machines.html">76 jmlr-2008-Optimization Techniques for Semi-Supervised Support Vector Machines</a></p>
<p>14 0.43722019 <a title="2-lda-14" href="./jmlr-2008-Hit_Miss_Networks_with_Applications_to_Instance_Selection.html">43 jmlr-2008-Hit Miss Networks with Applications to Instance Selection</a></p>
<p>15 0.42572159 <a title="2-lda-15" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<p>16 0.42014974 <a title="2-lda-16" href="./jmlr-2008-Shark%C2%A0%C2%A0%C2%A0%C2%A0%28Machine_Learning_Open_Source_Software_Paper%29.html">85 jmlr-2008-Shark    (Machine Learning Open Source Software Paper)</a></p>
<p>17 0.41103619 <a title="2-lda-17" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>18 0.40533173 <a title="2-lda-18" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>19 0.39870438 <a title="2-lda-19" href="./jmlr-2008-Active_Learning_by_Spherical_Subdivision.html">9 jmlr-2008-Active Learning by Spherical Subdivision</a></p>
<p>20 0.38730311 <a title="2-lda-20" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
