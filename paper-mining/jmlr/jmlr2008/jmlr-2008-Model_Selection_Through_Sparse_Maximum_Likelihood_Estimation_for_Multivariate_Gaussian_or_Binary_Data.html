<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2008" href="../home/jmlr2008_home.html">jmlr2008</a> <a title="jmlr-2008-62" href="#">jmlr2008-62</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</h1>
<br/><p>Source: <a title="jmlr-2008-62-pdf" href="http://jmlr.org/papers/volume9/banerjee08a/banerjee08a.pdf">pdf</a></p><p>Author: Onureena Banerjee, Laurent El Ghaoui, Alexandre d'Aspremont</p><p>Abstract: We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1 -norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our ﬁrst algorithm uses block coordinate descent, and can be interpreted as recursive 1 -norm penalized regression. Our second algorithm, based on Nesterov’s ﬁrst order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data. Keywords: model selection, maximum likelihood estimation, convex optimization, Gaussian graphical model, binary data</p><p>Reference: <a title="jmlr-2008-62-reference" href="../jmlr2008_reference/jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our approach is to solve a maximum likelihood problem with an added 1 -norm penalty term. [sent-7, score-0.211]
</p><p>2 Our ﬁrst algorithm uses block coordinate descent, and can be interpreted as recursive 1 -norm penalized regression. [sent-10, score-0.219]
</p><p>3 Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. [sent-12, score-0.661]
</p><p>4 We test our algorithms on synthetic data, as well as on gene expression and senate voting records data. [sent-13, score-0.322]
</p><p>5 Keywords: model selection, maximum likelihood estimation, convex optimization, Gaussian graphical model, binary data  1. [sent-14, score-0.237]
</p><p>6 In this paper we consider practical ways of implementing the following approach to ﬁnding such a model: given a set of data, we solve a maximum likelihood problem with an added 1 -norm penalty to make the resulting graph as sparse as possible. [sent-17, score-0.329]
</p><p>7 In the Gaussian case, model selection involves ﬁnding the pattern of zeros in the inverse covariance matrix, since these zeros correspond to conditional independencies among the variables. [sent-19, score-0.369]
</p><p>8 BANERJEE , E L G HAOUI , AND D ’A SPREMONT  a gradient descent algorithm in which they account for the sparsity of the inverse covariance matrix by deﬁning a loss function that is the negative of the log likelihood function. [sent-25, score-0.7]
</p><p>9 (Revised 2007) proposed a set of large scale methods for problems where a sparsity pattern for the inverse covariance is given and one must estimate the nonzero elements of the matrix. [sent-27, score-0.45]
</p><p>10 Another way to estimate the graphical model is to ﬁnd the set of neighbors of each node in the graph by regressing that variable against the remaining variables. [sent-28, score-0.195]
</p><p>11 (2004) apply an 1 norm penalty to sparse principle component analysis. [sent-32, score-0.207]
</p><p>12 Our objective here is to both estimate the sparsity pattern of the underlying graph and to obtain a regularized estimate of the covariance matrix. [sent-37, score-0.393]
</p><p>13 In Section 3 we present a provably convergent block coordinate descent algorithm that can be interpreted a sequence of iterative 1 -norm penalized regressions. [sent-47, score-0.31]
</p><p>14 In Section 5 we show that the algorithms we developed for the Gaussian case can also be used to solve an approximate sparse maximum likelihood problem for multivariate binary data, using a log determinant relaxation for the log partition function given by Wainwright and Jordan (2006). [sent-49, score-0.661]
</p><p>15 In Section 6, we test our methods on synthetic as well as gene expression and senate voting records data. [sent-50, score-0.322]
</p><p>16 , y(n) ∼ N (µ, Σ p ), where the covariance matrix Σ is to be estimated. [sent-57, score-0.194]
</p><p>17 n k=1  S :=  ˆ Let Σ−1 denote our estimate of the inverse covariance matrix. [sent-59, score-0.268]
</p><p>18 Our estimator takes the form: ˆ Σ−1 = arg max log det X − trace(SX) − λ X X 0  1. [sent-60, score-0.391]
</p><p>19 By trading off maximality of the log likelihood for sparsity, we hope to ﬁnd a very sparse solution that still adequately explains the data. [sent-68, score-0.364]
</p><p>20 2 The Dual Problem and Bounds on the Solution By expressing the  1  norm in (1) as X  where U (1) as  ∞  1  = max trace(XU), U  ∞ ≤1  denotes the maximum absolute value element of the symmetric matrix U, we can write max min log det X − trace(X, S +U). [sent-74, score-0.447]
</p><p>21 X 0 U  ∞ ≤λ  This corresponds to seeking an estimate with the maximum worst-case log likelihood, over all additive perturbations of the second moment matrix S. [sent-75, score-0.268]
</p><p>22 Note that the log determinant function acts a log barrier, creating an implicit constraint that S +U 0. [sent-80, score-0.322]
</p><p>23 Then the dual of our sparse maximum likelihood problem is ˆ Σ := max{log detW : W − S ∞ ≤ λ}. [sent-82, score-0.232]
</p><p>24 (2) 487  BANERJEE , E L G HAOUI , AND D ’A SPREMONT  Observe that the dual problem (2) estimates the covariance matrix while the primal problem estimates its inverse. [sent-83, score-0.244]
</p><p>25 (Revised 2007), is to compute a maximum likelihood estimate of the covariance matrix when the sparsity structure of the inverse is known in advance. [sent-96, score-0.536]
</p><p>26 Our constraint set is unbounded as we hope to uncover the sparsity structure automatically, starting with a dense second moment matrix S. [sent-98, score-0.208]
</p><p>27 Meinshausen and B¨ hlmann (2006) show that the Lasso, when used to estimate the set of neighu bors for each node, yields an estimate of the sparsity pattern of the graph in a way that is asymptotically consistent. [sent-104, score-0.319]
</p><p>28 Thus, asymptotically we recover the classical maximum likelihood estimate, S, which in turn converges in probability to the true covariance Σ. [sent-117, score-0.221]
</p><p>29 Meinshausen and B¨ hlmann (2006) prove that Lasso recovers the underlying sparsity pattern u consistently, in a scheme where the number of variables is allowed to grow with the number of samples: p ∈ O (nγ ). [sent-118, score-0.194]
</p><p>30 2 Convergence and Property of Solution Using Schur complements, we can prove convergence: Theorem 3 The block coordinate descent algorithm described above converges, achieving an εsuboptimal solution to (2). [sent-141, score-0.313]
</p><p>31 This means that, for a given second moment matrix S, if λ is chosen such that the condition in Theorem 4 is met for some column k, then the sparse maximum likelihood method estimates variable k to be independent of all other variables in the distribution. [sent-149, score-0.281]
</p><p>32 Our purpose is not to obtain another algorithm, as we have found that the block coordinate descent is fairly efﬁcient; rather, we seek to use Nesterov’s formalism to derive a rigorous complexity estimate for the problem, improved over that offered by interior-point methods. [sent-172, score-0.3]
</p><p>33 In particular, although we have found in practice that a small, ﬁxed number K of sweeps through all columns is sufﬁcient to achieve convergence using the block coordinate descent method, we have not been able to compute a bound on K. [sent-173, score-0.319]
</p><p>34 We also deﬁne fˆ(x) := − log det x + S, x , and A := I. [sent-186, score-0.303]
</p><p>35 For Q1 we choose the Frobenius norm, and a prox-function d 1 (x) = − log det x + p log b. [sent-188, score-0.424]
</p><p>36 (2006) applied an 1 -norm penalty to the logistic regression problem to obtain a binary version of the high-dimensional consistency results of Meinshausen and B¨ hlmann (2006). [sent-226, score-0.209]
</p><p>37 We apply the log determinant relaxation of Wainwright and Jordan u (2006) to formulate an approximate sparse maximum likelihood (ASML) problem for estimating the parameters in a multivariate binary distribution. [sent-227, score-0.495]
</p><p>38 We show that the resulting problem is the same as the Gaussian sparse maximum likelihood (SML) problem, and that we can therefore apply our previously-developed algorithms to sparse model selection in a binary setting. [sent-228, score-0.308]
</p><p>39 An Ising model for this distribution is p  p(x; θ) = exp{ ∑ θi xi +  p−1  i=1  p  ∑ ∑  i=1 j=i+1  θi j xi x j − A(θ)}  (10)  where A(θ) = log  ∑  x∈X p  p  exp{ ∑ θi xi + i=1  p−1  p  ∑ ∑  i=1 j=i+1  θi j x i x j }  (11)  is the log partition function. [sent-231, score-0.287]
</p><p>40 The sparse maximum likelihood problem in this case is to maximize (10) with an added 1 -norm penalty on terms θk j . [sent-232, score-0.3]
</p><p>41 However, if we use the log determinant relaxation for the log partition function developed by Wainwright and Jordan (2006), we can obtain an approximate sparse maximum likelihood (ASML) estimate. [sent-235, score-0.624]
</p><p>42 Wainwright and Jordan (2006) propose a relaxation by using a Gaussian bound on the log partition function (11) and a semideﬁnite outer bound on the polytope of marginal probabilities. [sent-238, score-0.313]
</p><p>43 In particular, this means that we can reuse the block coordinate descent or Nesterov algorithms to approximately estimate graphical models in the case of binary data. [sent-240, score-0.443]
</p><p>44 The sparse maximum likelihood problem is 1 ˆ θexact := arg max R(θ), R(¯) − A(θ) − λ θ 1 . [sent-260, score-0.27]
</p><p>45 Wainwright and Jordan (2006) give 3 3 an upper bound on the log partition function as the solution to the following variational problem: 1 A(θ) ≤ maxµ 2 log det(R(µ) + diag(m)) + θ, µ = 1 · maxµ log det(R(µ) + diag(m)) + R(θ), R(µ) . [sent-265, score-0.505]
</p><p>46 2  (13)  If we use the bound (13) in our sparse maximum likelihood problem (12), we won’t be able to ˆ extract an optimizing argument θ. [sent-266, score-0.218]
</p><p>47 ¯i  Using (17), we have the following binary version of Theorem 2: Theorem 7 With (17) chosen as the penalty parameter in the approximate sparse maximum likelihood problem, for a ﬁxed level α, ˆλ P(∃k ∈ {1, . [sent-279, score-0.337]
</p><p>48 1 Synthetic Experiments Synthetic experiments require that we generate underlying sparse inverse covariance matrices. [sent-288, score-0.309]
</p><p>49 2 Sparsity and Thresholding A very simple approach to obtaining a sparse estimate of the inverse covariance matrix would be to apply a threshold to the inverse empirical covariance matrix, S −1 . [sent-294, score-0.643]
</p><p>50 ˆ Applying a threshold to either S−1 or Σ−1 would decrease the log likelihood of the estimate by an unknown amount. [sent-300, score-0.262]
</p><p>51 3 Recovering Structure We begin with a small experiment to test the ability of the method to recover the sparse structure of an underlying covariance matrix. [sent-307, score-0.217]
</p><p>52 Figure 2 (A) shows a sparse inverse covariance matrix of size p = 30. [sent-308, score-0.375]
</p><p>53 Nevertheless, we can still pick out features that were present in the true underlying inverse covariance matrix. [sent-313, score-0.22]
</p><p>54 We plot (A) the original inverse covariance matrix Σ −1 , (B) the noisy sample inverse S−1 , and (C) the solution to problem (1) for λ = 0. [sent-315, score-0.48]
</p><p>55 Using the same underlying inverse covariance matrix, we repeat the experiment using smaller sample sizes. [sent-317, score-0.22]
</p><p>56 As expected, our ability to pick out features of the true inverse covariance matrix diminishes with the number of samples. [sent-320, score-0.286]
</p><p>57 We plot (A) the original inverse covariance matrix Σ−1 , (B) the solution to problem (1) for n = 30 and (C) the solution for n = 20. [sent-323, score-0.449]
</p><p>58 4 CPU Times Versus Problem Size For a sense of the practical performance of the block coordinate descent method, we implemented it in Matlab, using Mosek to solve the quadratic program at each step. [sent-327, score-0.252]
</p><p>59 Using this very simple implementation, the block coordinate descent method solves a problem of size p = 1000 in about an hour and a half. [sent-336, score-0.252]
</p><p>60 The red lines correspond to elements of the solution that are zero in the true underlying inverse covariance matrix. [sent-343, score-0.281]
</p><p>61 On a related note, we observe that (1) also works well in recovering the sparsity pattern of a matrix masked by noise. [sent-347, score-0.249]
</p><p>62 We generate a sparse inverse covariance matrix of size p = 50 as described above. [sent-349, score-0.375]
</p><p>63 In Figure 6, for each value of λ shown, we randomly selected 10 sample covariance matrices S of size p = 50 and computed the number of misclassiﬁed zeros and nonzero elements in the solution 497  BANERJEE , E L G HAOUI , AND D ’A SPREMONT  4  Average CPU time (seconds)  10  3  10  2  10  Blk. [sent-353, score-0.29]
</p><p>64 The standard deviations for the block coordinate descent method are: 0. [sent-358, score-0.252]
</p><p>65 We plot the average percentage of errors (number of misclassiﬁed zeros plus misclassiﬁed nonzeros divided by p2 ), as well as error bars corresponding to one standard deviation. [sent-375, score-0.205]
</p><p>66 Dashed lines correspond to elements that are zero in the true inverse covariance matrix; solid lines correspond to true nonzeros. [sent-401, score-0.22]
</p><p>67 5  log(λ/σ)  1  Figure 6: Recovering sparsity pattern in a matrix with added uniform noise of size σ = 0. [sent-405, score-0.206]
</p><p>68 Then, from the corresponding covariance matrix Σ, we generated three data sets of sizes n = 30, 60, and 100. [sent-423, score-0.194]
</p><p>69 In Figure 7 we plot estimated instability versus a range of values for the penalty parameter λ, using 10-fold cross validation. [sent-424, score-0.299]
</p><p>70 7 Performance as a Binary Classiﬁer In this section we numerically examine the ability of the sparse maximum likelihood (SML) method to correctly classify elements of the inverse covariance matrix as zero or nonzero. [sent-432, score-0.468]
</p><p>71 Upon obtaining a solution θ(k) for each variable k, one can estimate sparsity in one of two ways: either by 500  M ODEL S ELECTION T HROUGH S PARSE M AXIMUM L IKELIHOOD E STIMATION  0. [sent-435, score-0.218]
</p><p>72 01  PSfrag replacements 0  2  4  6  8  10  K  12  14  16  18  20  Figure 8: Estimating instability of the solution using cross validation. [sent-441, score-0.277]
</p><p>73 We plot the estimate of the instability obtained using K samples, for various values of K. [sent-442, score-0.196]
</p><p>74 In the following experiments, we ﬁxed the problem size p at 30 and generated sparse underlying inverse covariance matrices as described above. [sent-449, score-0.309]
</p><p>75 For each value of n shown, we ran 30 trials in which we estimated the sparsity pattern of the inverse covariance matrix using the SML, Lasso-OR, and Lasso-AND methods. [sent-451, score-0.459]
</p><p>76 We plot the power (proportion of correctly identiﬁed nonzeros), positive predictive value (proportion of estimated nonzeros that are correct), and the density estimated by each method. [sent-457, score-0.316]
</p><p>77 Meinshausen and B¨ hlmann (2006) report that, asymptotically, Lasso-AND and Lasso-OR yield u the same estimate of the sparsity pattern of the inverse covariance matrix. [sent-461, score-0.462]
</p><p>78 005 0  1  2  3  4  n/p  5  6  7  8  9  10  11  Figure 9: Classifying zeros and nonzeros for a true density of δ = 0. [sent-488, score-0.199]
</p><p>79 In exchange, SML can offer a more accurate estimate of the sparsity pattern, as well as a well-conditioned estimate of the covariance matrix itself. [sent-499, score-0.399]
</p><p>80 04  PSfrag replacements  PSfrag replacements Positive predictive value  0. [sent-515, score-0.256]
</p><p>81 05  0  1  2  3  4  n/p  5  6  7  8  9  10  11  Figure 10: Classifying zeros and nonzeros for a true density of δ = 0. [sent-529, score-0.199]
</p><p>82 samples and then attempted to recover the underlying sparsity pattern using both the approximate sparse maximum likelihood method and 1 -norm penalized logistic regression as described by Wainwright et al. [sent-536, score-0.428]
</p><p>83 In Figure 12 we plot the average sensitivity (the fraction of actual nonzeros that are correctly identiﬁed as nonzeros) as well as the average speciﬁcity (the fraction of actual zeros that are correctly identiﬁed as zeros) for a range of sample sizes. [sent-538, score-0.205]
</p><p>84 96  2000  0  500  n  1000  1500  2000  Figure 12: Comparing sparsity pattern recovery to the 1 -norm penalized logistic regression (PLR). [sent-567, score-0.198]
</p><p>85 In the estimated obtained by solving (1), we found that LDL receptor had one of the largest number of ﬁrst-order neighbors in the Gaussian graphical model. [sent-604, score-0.22]
</p><p>86 3 Senate Voting Records Data We conclude our numerical experiments by testing our approximate sparse maximum likelihood estimation method on binary data. [sent-616, score-0.219]
</p><p>87 The data set consists of US senate voting records data from the 109th congress (2004 - 2006). [sent-617, score-0.237]
</p><p>88 The graph displays the solution to (12) obtained using the log determinant relaxation to the log partition function of Wainwright and Jordan (2006). [sent-620, score-0.532]
</p><p>89 Thus, although we obtained this graphical model via a relaxation of the log partition function, the resulting picture is supported by conventional wisdom. [sent-635, score-0.318]
</p><p>90 Acknowledgments We are indebted to Georges Natsoulis for his interpretation of the Iconix data set analysis and for gathering the senate voting records. [sent-643, score-0.196]
</p><p>91 Proof of Solution Properties and Block Coordinate Descent Convergence In this section, we give short proofs of the two theorems on properties of the solution to (1), as well as the convergence of the block coordinate descent method. [sent-646, score-0.313]
</p><p>92 Likewise, we can show that Σ−1 the optimum, the primal dual gap is zero: ˆ ˆ ˆ − log det Σ−1 + trace(SΣ−1 ) + λ Σ−1 −1 ) + λ Σ−1 − p = 0 ˆ ˆ = trace(SΣ 1 We therefore have  1 − log det Σ − p  ˆ ˆ ˆ Σ−1 2 ≤ Σ−1 F ≤ Σ−1 1 ˆ −1 )/λ ≤ p/λ = p/λ − trace(SΣ 509  2  ˆ  is bounded above. [sent-648, score-0.656]
</p><p>93 By general results on block coordinate descent algorithms (e. [sent-653, score-0.252]
</p><p>94 By Theorem 3, the block coˆ ordinate descent algorithm converges to a solution, and so therefore the solution must have Σ j = 0. [sent-665, score-0.246]
</p><p>95 1 Preliminaries Before we begin, consider problem (1), for a matrix S of any size: ˆ X = arg min − log det X + trace(SX) + λ X  1  where we have dropped the constraint X 0 since it is implicit, due to the log determinant function. [sent-672, score-0.619]
</p><p>96 The correct zero pattern for the covariance matrix is then block diagonal. [sent-678, score-0.319]
</p><p>97 ,C )  (19)  The inverse (Σcorrect )−1 must also be block diagonal, with possible additional zeros inside the blocks. [sent-682, score-0.245]
</p><p>98 Analogous to (22), we have: P(|Sk j | ≥ λ|Skk = skk , S j j = s j j ) = 2P(nR2 j ≥ nλ2 skk s j j |Skk = skk , S j j = s j j ) k ˜ ˆk ˆ j ≤ 2G(nλ2 σ2 σ2 ) ˜ ˆk where σ2 is the sample variance of variable k, and G = 1 − G is the CDF of the chi-squared distribution with one degree of freedom. [sent-702, score-0.579]
</p><p>99 Proof of Connection Between Gaussian SML and Binary ASML We end with a proof of Theorem 6, which connects the exact Gaussian sparse maximum likelihood problem with the approximate sparse maximum likelihood problem obtained by using the log determinant relaxation of Wainwright and Jordan (2006). [sent-706, score-0.64]
</p><p>100 Having expressed the upper bound on the log partition function as a constant minus a maximization problem will help when we formulate the sparse approximate maximum likelihood problem. [sent-718, score-0.384]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sml', 0.276), ('lasso', 0.212), ('banerjee', 0.211), ('hrough', 0.207), ('skk', 0.193), ('haoui', 0.187), ('spremont', 0.187), ('det', 0.182), ('stimation', 0.176), ('ikelihood', 0.176), ('aximum', 0.176), ('diag', 0.157), ('nesterov', 0.152), ('psfrag', 0.136), ('ck', 0.132), ('covariance', 0.128), ('senate', 0.124), ('wainwright', 0.122), ('log', 0.121), ('parse', 0.119), ('penalty', 0.118), ('sparsity', 0.109), ('replacements', 0.109), ('election', 0.107), ('instability', 0.107), ('nonzeros', 0.105), ('odel', 0.103), ('meinshausen', 0.095), ('block', 0.094), ('likelihood', 0.093), ('inverse', 0.092), ('descent', 0.091), ('sparse', 0.089), ('ldl', 0.083), ('determinant', 0.08), ('graphical', 0.077), ('relaxation', 0.075), ('voting', 0.072), ('receptor', 0.069), ('coordinate', 0.067), ('matrix', 0.066), ('hughes', 0.063), ('sweep', 0.063), ('sk', 0.062), ('solution', 0.061), ('zeros', 0.059), ('penalized', 0.058), ('genes', 0.058), ('interior', 0.057), ('democrats', 0.055), ('detw', 0.055), ('senator', 0.055), ('hlmann', 0.054), ('trace', 0.051), ('dual', 0.05), ('arg', 0.049), ('jordan', 0.048), ('schur', 0.048), ('estimate', 0.048), ('samples', 0.048), ('ij', 0.046), ('synthetic', 0.045), ('partition', 0.045), ('recovering', 0.043), ('theorem', 0.042), ('buhlmann', 0.042), ('compendium', 0.042), ('nonzero', 0.042), ('asml', 0.041), ('dahl', 0.041), ('natsoulis', 0.041), ('onureena', 0.041), ('plr', 0.041), ('senators', 0.041), ('neighbors', 0.041), ('records', 0.041), ('plot', 0.041), ('gene', 0.04), ('cpu', 0.039), ('max', 0.039), ('predictive', 0.038), ('concentration', 0.038), ('binary', 0.037), ('ghaoui', 0.036), ('maxx', 0.036), ('bound', 0.036), ('density', 0.035), ('moment', 0.033), ('estimated', 0.033), ('gaussian', 0.033), ('np', 0.031), ('sweeps', 0.031), ('jt', 0.031), ('pattern', 0.031), ('power', 0.031), ('tn', 0.03), ('convex', 0.03), ('xk', 0.029), ('graph', 0.029), ('reuse', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="62-tfidf-1" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>Author: Onureena Banerjee, Laurent El Ghaoui, Alexandre d'Aspremont</p><p>Abstract: We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1 -norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our ﬁrst algorithm uses block coordinate descent, and can be interpreted as recursive 1 -norm penalized regression. Our second algorithm, based on Nesterov’s ﬁrst order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data. Keywords: model selection, maximum likelihood estimation, convex optimization, Gaussian graphical model, binary data</p><p>2 0.20539866 <a title="62-tfidf-2" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>Author: Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui</p><p>Abstract: Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefﬁcients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semideﬁnite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefﬁcients, with total complexity O(n3 ), where n is the number of variables. We then use the same relaxation to derive sufﬁcient conditions for global optimality of a solution, which can be tested in O(n3 ) per pattern. We discuss applications in subset selection and sparse recovery and show on artiﬁcial examples and biological data that our algorithm does provide globally optimal solutions in many cases. Keywords: PCA, subset selection, sparse eigenvalues, sparse recovery, lasso</p><p>3 0.092963107 <a title="62-tfidf-3" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>4 0.085994981 <a title="62-tfidf-4" href="./jmlr-2008-Consistency_of_Trace_Norm_Minimization.html">26 jmlr-2008-Consistency of Trace Norm Minimization</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufﬁcient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulﬁlled. Keywords: convex optimization, singular value decomposition, trace norm, consistency</p><p>5 0.078589246 <a title="62-tfidf-5" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>Author: David C. Hoyle</p><p>Abstract: Bayesian inference from high-dimensional data involves the integration over a large number of model parameters. Accurate evaluation of such high-dimensional integrals raises a unique set of issues. These issues are illustrated using the exemplar of model selection for principal component analysis (PCA). A Bayesian model selection criterion, based on a Laplace approximation to the model evidence for determining the number of signal principal components present in a data set, has previously been show to perform well on various test data sets. Using simulated data we show that for d-dimensional data and small sample sizes, N, the accuracy of this model selection method is strongly affected by increasing values of d. By taking proper account of the contribution to the evidence from the large number of model parameters we show that model selection accuracy is substantially improved. The accuracy of the improved model evidence is studied in the asymptotic limit d → ∞ at ﬁxed ratio α = N/d, with α < 1. In this limit, model selection based upon the improved model evidence agrees with a frequentist hypothesis testing approach. Keywords: PCA, Bayesian model selection, random matrix theory, high dimensional inference</p><p>6 0.078096196 <a title="62-tfidf-6" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>7 0.073614746 <a title="62-tfidf-7" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>8 0.068077974 <a title="62-tfidf-8" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>9 0.065882824 <a title="62-tfidf-9" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>10 0.061446615 <a title="62-tfidf-10" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>11 0.06087568 <a title="62-tfidf-11" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>12 0.060643386 <a title="62-tfidf-12" href="./jmlr-2008-Learning_Similarity_with_Operator-valued_Large-margin_Classifiers.html">51 jmlr-2008-Learning Similarity with Operator-valued Large-margin Classifiers</a></p>
<p>13 0.054481097 <a title="62-tfidf-13" href="./jmlr-2008-Graphical_Models_for_Structured_Classification%2C_with_an_Application_to_Interpreting_Images_of_Protein_Subcellular_Location_Patterns.html">41 jmlr-2008-Graphical Models for Structured Classification, with an Application to Interpreting Images of Protein Subcellular Location Patterns</a></p>
<p>14 0.053408086 <a title="62-tfidf-14" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>15 0.050639253 <a title="62-tfidf-15" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>16 0.049620785 <a title="62-tfidf-16" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>17 0.047995653 <a title="62-tfidf-17" href="./jmlr-2008-A_Recursive_Method_for_Structural_Learning_of_Directed_Acyclic_Graphs.html">6 jmlr-2008-A Recursive Method for Structural Learning of Directed Acyclic Graphs</a></p>
<p>18 0.047072563 <a title="62-tfidf-18" href="./jmlr-2008-Mixed_Membership_Stochastic_Blockmodels.html">61 jmlr-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>19 0.046630394 <a title="62-tfidf-19" href="./jmlr-2008-Universal_Multi-Task_Kernels.html">92 jmlr-2008-Universal Multi-Task Kernels</a></p>
<p>20 0.044581775 <a title="62-tfidf-20" href="./jmlr-2008-Learning_from_Multiple_Sources.html">52 jmlr-2008-Learning from Multiple Sources</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.265), (1, -0.043), (2, -0.13), (3, 0.091), (4, 0.074), (5, 0.062), (6, -0.132), (7, 0.064), (8, -0.203), (9, -0.055), (10, -0.035), (11, -0.12), (12, -0.096), (13, 0.205), (14, 0.121), (15, -0.193), (16, -0.022), (17, -0.045), (18, -0.181), (19, 0.065), (20, -0.011), (21, 0.039), (22, 0.14), (23, -0.006), (24, -0.099), (25, 0.014), (26, 0.014), (27, -0.1), (28, -0.029), (29, 0.085), (30, 0.167), (31, -0.049), (32, 0.055), (33, 0.075), (34, -0.018), (35, 0.027), (36, 0.005), (37, -0.086), (38, -0.013), (39, 0.021), (40, 0.149), (41, 0.106), (42, -0.143), (43, 0.032), (44, 0.044), (45, 0.006), (46, 0.091), (47, 0.017), (48, -0.04), (49, -0.275)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94090873 <a title="62-lsi-1" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>Author: Onureena Banerjee, Laurent El Ghaoui, Alexandre d'Aspremont</p><p>Abstract: We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1 -norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our ﬁrst algorithm uses block coordinate descent, and can be interpreted as recursive 1 -norm penalized regression. Our second algorithm, based on Nesterov’s ﬁrst order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data. Keywords: model selection, maximum likelihood estimation, convex optimization, Gaussian graphical model, binary data</p><p>2 0.77945626 <a title="62-lsi-2" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>Author: Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui</p><p>Abstract: Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a linear combination of the input variables while constraining the number of nonzero coefﬁcients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semideﬁnite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all target numbers of non zero coefﬁcients, with total complexity O(n3 ), where n is the number of variables. We then use the same relaxation to derive sufﬁcient conditions for global optimality of a solution, which can be tested in O(n3 ) per pattern. We discuss applications in subset selection and sparse recovery and show on artiﬁcial examples and biological data that our algorithm does provide globally optimal solutions in many cases. Keywords: PCA, subset selection, sparse eigenvalues, sparse recovery, lasso</p><p>3 0.52026099 <a title="62-lsi-3" href="./jmlr-2008-Consistency_of_Trace_Norm_Minimization.html">26 jmlr-2008-Consistency of Trace Norm Minimization</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: Regularization by the sum of singular values, also referred to as the trace norm, is a popular technique for estimating low rank rectangular matrices. In this paper, we extend some of the consistency results of the Lasso to provide necessary and sufﬁcient conditions for rank consistency of trace norm minimization with the square loss. We also provide an adaptive version that is rank consistent even when the necessary condition for the non adaptive version is not fulﬁlled. Keywords: convex optimization, singular value decomposition, trace norm, consistency</p><p>4 0.41110882 <a title="62-lsi-4" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>Author: Matthias W. Seeger</p><p>Abstract: The linear model with sparsity-favouring prior on the coefﬁcients has important applications in many different domains. In machine learning, most methods to date search for maximum a posteriori sparse solutions and neglect to represent posterior uncertainties. In this paper, we address problems of Bayesian optimal design (or experiment planning), for which accurate estimates of uncertainty are essential. To this end, we employ expectation propagation approximate inference for the linear model with Laplace prior, giving new insight into numerical stability properties and proposing a robust algorithm. We also show how to estimate model hyperparameters by empirical Bayesian maximisation of the marginal likelihood, and propose ideas in order to scale up the method to very large underdetermined problems. We demonstrate the versatility of our framework on the application of gene regulatory network identiﬁcation from micro-array expression data, where both the Laplace prior and the active experimental design approach are shown to result in signiﬁcant improvements. We also address the problem of sparse coding of natural images, and show how our framework can be used for compressive sensing tasks. Part of this work appeared in Seeger et al. (2007b). The gene network identiﬁcation application appears in Steinke et al. (2007). Keywords: sparse linear model, Laplace prior, expectation propagation, approximate inference, optimal design, Bayesian statistics, gene network recovery, image coding, compressive sensing</p><p>5 0.29209438 <a title="62-lsi-5" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>Author: Imhoi Koo, Rhee Man Kil</p><p>Abstract: This paper presents a new method of model selection for regression problems using the modulus of continuity. For this purpose, we suggest the prediction risk bounds of regression models using the modulus of continuity which can be interpreted as the complexity of functions. We also present the model selection criterion referred to as the modulus of continuity information criterion (MCIC) which is derived from the suggested prediction risk bounds. The suggested MCIC provides a risk estimate using the modulus of continuity for a trained regression model (or an estimation function) while other model selection criteria such as the AIC and BIC use structural information such as the number of training parameters. As a result, the suggested MCIC is able to discriminate the performances of trained regression models, even with the same structure of training models. To show the effectiveness of the proposed method, the simulation for function approximation using the multilayer perceptrons (MLPs) was conducted. Through the simulation for function approximation, it was demonstrated that the suggested MCIC provides a good selection tool for nonlinear regression models, even with the limited size of data. Keywords: regression models, multilayer perceptrons, model selection, information criteria, modulus of continuity</p><p>6 0.28005525 <a title="62-lsi-6" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>7 0.27715763 <a title="62-lsi-7" href="./jmlr-2008-Cross-Validation_Optimization_for_Large_Scale_Structured_Classification_Kernel_Methods.html">29 jmlr-2008-Cross-Validation Optimization for Large Scale Structured Classification Kernel Methods</a></p>
<p>8 0.27159593 <a title="62-lsi-8" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>9 0.25749817 <a title="62-lsi-9" href="./jmlr-2008-Graphical_Methods_for_Efficient_Likelihood_Inference_in_Gaussian_Covariance_Models.html">40 jmlr-2008-Graphical Methods for Efficient Likelihood Inference in Gaussian Covariance Models</a></p>
<p>10 0.2419011 <a title="62-lsi-10" href="./jmlr-2008-Automatic_PCA_Dimension_Selection_for_High_Dimensional_Data_and_Small_Sample_Sizes.html">17 jmlr-2008-Automatic PCA Dimension Selection for High Dimensional Data and Small Sample Sizes</a></p>
<p>11 0.23691857 <a title="62-lsi-11" href="./jmlr-2008-Algorithms_for_Sparse_Linear_Classifiers_in_the_Massive_Data_Setting.html">12 jmlr-2008-Algorithms for Sparse Linear Classifiers in the Massive Data Setting</a></p>
<p>12 0.23592664 <a title="62-lsi-12" href="./jmlr-2008-On_the_Size_and_Recovery_of_Submatrices_of_Ones_in_a_Random_Binary_Matrix.html">72 jmlr-2008-On the Size and Recovery of Submatrices of Ones in a Random Binary Matrix</a></p>
<p>13 0.21835844 <a title="62-lsi-13" href="./jmlr-2008-Finite-Time_Bounds_for_Fitted_Value_Iteration.html">36 jmlr-2008-Finite-Time Bounds for Fitted Value Iteration</a></p>
<p>14 0.20753333 <a title="62-lsi-14" href="./jmlr-2008-Coordinate_Descent_Method_for_Large-scale_L2-loss_Linear_Support_Vector_Machines.html">28 jmlr-2008-Coordinate Descent Method for Large-scale L2-loss Linear Support Vector Machines</a></p>
<p>15 0.20045523 <a title="62-lsi-15" href="./jmlr-2008-Trust_Region_Newton_Method_for_Logistic_Regression.html">91 jmlr-2008-Trust Region Newton Method for Logistic Regression</a></p>
<p>16 0.19937059 <a title="62-lsi-16" href="./jmlr-2008-Near-Optimal_Sensor_Placements_in_Gaussian_Processes%3A_Theory%2C_Efficient_Algorithms_and_Empirical_Studies.html">67 jmlr-2008-Near-Optimal Sensor Placements in Gaussian Processes: Theory, Efficient Algorithms and Empirical Studies</a></p>
<p>17 0.19849679 <a title="62-lsi-17" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>18 0.19755033 <a title="62-lsi-18" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>19 0.19250146 <a title="62-lsi-19" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>20 0.19223434 <a title="62-lsi-20" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.021), (5, 0.011), (40, 0.018), (54, 0.032), (58, 0.047), (66, 0.621), (76, 0.019), (88, 0.038), (92, 0.038), (94, 0.052), (99, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99642313 <a title="62-lda-1" href="./jmlr-2008-Responses_to_Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">82 jmlr-2008-Responses to Evidence Contrary to the Statistical View of Boosting</a></p>
<p>Author: Kristin P. Bennett</p><p>Abstract: unkown-abstract</p><p>same-paper 2 0.97513086 <a title="62-lda-2" href="./jmlr-2008-Model_Selection_Through_Sparse_Maximum_Likelihood_Estimation_for_Multivariate_Gaussian_or_Binary_Data.html">62 jmlr-2008-Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</a></p>
<p>Author: Onureena Banerjee, Laurent El Ghaoui, Alexandre d'Aspremont</p><p>Abstract: We consider the problem of estimating the parameters of a Gaussian or binary distribution in such a way that the resulting undirected graphical model is sparse. Our approach is to solve a maximum likelihood problem with an added 1 -norm penalty term. The problem as formulated is convex but the memory requirements and complexity of existing interior point methods are prohibitive for problems with more than tens of nodes. We present two new algorithms for solving problems with at least a thousand nodes in the Gaussian case. Our ﬁrst algorithm uses block coordinate descent, and can be interpreted as recursive 1 -norm penalized regression. Our second algorithm, based on Nesterov’s ﬁrst order method, yields a complexity estimate with a better dependence on problem size than existing interior point methods. Using a log determinant relaxation of the log partition function (Wainwright and Jordan, 2006), we show that these same algorithms can be used to solve an approximate sparse maximum likelihood problem for the binary case. We test our algorithms on synthetic data, as well as on gene expression and senate voting records data. Keywords: model selection, maximum likelihood estimation, convex optimization, Gaussian graphical model, binary data</p><p>3 0.95861161 <a title="62-lda-3" href="./jmlr-2008-A_New_Algorithm_for_Estimating_the_Effective_Dimension-Reduction_Subspace.html">5 jmlr-2008-A New Algorithm for Estimating the Effective Dimension-Reduction Subspace</a></p>
<p>Author: Arnak S. Dalalyan, Anatoly Juditsky, Vladimir Spokoiny</p><p>Abstract: The statistical problem of estimating the effective dimension-reduction (EDR) subspace in the multi-index regression model with deterministic design and additive noise is considered. A new procedure for recovering the directions of the EDR subspace is proposed. Many methods for estimating the EDR subspace perform principal component analysis on a family of vectors, say ˆ ˆ β1 , . . . , βL , nearly lying in the EDR subspace. This is in particular the case for the structure-adaptive approach proposed by Hristache et al. (2001a). In the present work, we propose to estimate the projector onto the EDR subspace by the solution to the optimization problem minimize ˆ ˆ max β (I − A)β =1,...,L subject to A ∈ Am∗ , where Am∗ is the set of all symmetric matrices with eigenvalues in [0, 1] and trace less than or equal √ to m∗ , with m∗ being the true structural dimension. Under mild assumptions, n-consistency of the proposed procedure is proved (up to a logarithmic factor) in the case when the structural dimension is not larger than 4. Moreover, the stochastic error of the estimator of the projector onto the EDR subspace is shown to depend on L logarithmically. This enables us to use a large number of vectors ˆ β for estimating the EDR subspace. The empirical behavior of the algorithm is studied through numerical simulations. Keywords: dimension-reduction, multi-index regression model, structure-adaptive approach, central subspace</p><p>4 0.65636969 <a title="62-lda-4" href="./jmlr-2008-Consistency_of_the_Group_Lasso_and_Multiple_Kernel_Learning.html">27 jmlr-2008-Consistency of the Group Lasso and Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: We consider the least-square regression problem with regularization by a block 1 -norm, that is, a sum of Euclidean norms over spaces of dimensions larger than one. This problem, referred to as the group Lasso, extends the usual regularization by the 1 -norm where all spaces have dimension one, where it is commonly referred to as the Lasso. In this paper, we study the asymptotic group selection consistency of the group Lasso. We derive necessary and sufﬁcient conditions for the consistency of group Lasso under practical assumptions, such as model misspeciﬁcation. When the linear predictors and Euclidean norms are replaced by functions and reproducing kernel Hilbert norms, the problem is usually referred to as multiple kernel learning and is commonly used for learning from heterogeneous data sources and for non linear variable selection. Using tools from functional analysis, and in particular covariance operators, we extend the consistency results to this inﬁnite dimensional case and also propose an adaptive scheme to obtain a consistent model estimate, even when the necessary condition required for the non adaptive scheme is not satisﬁed. Keywords: sparsity, regularization, consistency, convex optimization, covariance operators</p><p>5 0.56543308 <a title="62-lda-5" href="./jmlr-2008-Evidence_Contrary_to_the_Statistical_View_of_Boosting.html">33 jmlr-2008-Evidence Contrary to the Statistical View of Boosting</a></p>
<p>Author: David Mease, Abraham Wyner</p><p>Abstract: The statistical perspective on boosting algorithms focuses on optimization, drawing parallels with maximum likelihood estimation for logistic regression. In this paper we present empirical evidence that raises questions about this view. Although the statistical perspective provides a theoretical framework within which it is possible to derive theorems and create new algorithms in general contexts, we show that there remain many unanswered important questions. Furthermore, we provide examples that reveal crucial ﬂaws in the many practical suggestions and new methods that are derived from the statistical view. We perform carefully designed experiments using simple simulation models to illustrate some of these ﬂaws and their practical consequences. Keywords: boosting algorithms, LogitBoost, AdaBoost</p><p>6 0.55334556 <a title="62-lda-6" href="./jmlr-2008-Consistency_of_Trace_Norm_Minimization.html">26 jmlr-2008-Consistency of Trace Norm Minimization</a></p>
<p>7 0.51412773 <a title="62-lda-7" href="./jmlr-2008-Non-Parametric_Modeling_of_Partially_Ranked_Data.html">69 jmlr-2008-Non-Parametric Modeling of Partially Ranked Data</a></p>
<p>8 0.50555068 <a title="62-lda-8" href="./jmlr-2008-Robust_Submodular_Observation_Selection.html">83 jmlr-2008-Robust Submodular Observation Selection</a></p>
<p>9 0.50368899 <a title="62-lda-9" href="./jmlr-2008-Manifold_Learning%3A_The_Price_of_Normalization.html">57 jmlr-2008-Manifold Learning: The Price of Normalization</a></p>
<p>10 0.49634555 <a title="62-lda-10" href="./jmlr-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">48 jmlr-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>11 0.49102226 <a title="62-lda-11" href="./jmlr-2008-Structural_Learning_of_Chain_Graphs_via_Decomposition.html">88 jmlr-2008-Structural Learning of Chain Graphs via Decomposition</a></p>
<p>12 0.4893817 <a title="62-lda-12" href="./jmlr-2008-Max-margin_Classification_of_Data_with_Absent_Features.html">58 jmlr-2008-Max-margin Classification of Data with Absent Features</a></p>
<p>13 0.48838624 <a title="62-lda-13" href="./jmlr-2008-Bayesian_Inference_and_Optimal_Design_for_the_Sparse_Linear_Model.html">18 jmlr-2008-Bayesian Inference and Optimal Design for the Sparse Linear Model</a></p>
<p>14 0.48671958 <a title="62-lda-14" href="./jmlr-2008-Magic_Moments_for_Structured_Output_Prediction.html">56 jmlr-2008-Magic Moments for Structured Output Prediction</a></p>
<p>15 0.47907475 <a title="62-lda-15" href="./jmlr-2008-Model_Selection_for_Regression_with_Continuous_Kernel_Functions_Using_the_Modulus_of_Continuity%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">63 jmlr-2008-Model Selection for Regression with Continuous Kernel Functions Using the Modulus of Continuity    (Special Topic on Model Selection)</a></p>
<p>16 0.47216702 <a title="62-lda-16" href="./jmlr-2008-Optimal_Solutions_for_Sparse_Principal_Component_Analysis.html">75 jmlr-2008-Optimal Solutions for Sparse Principal Component Analysis</a></p>
<p>17 0.46943724 <a title="62-lda-17" href="./jmlr-2008-Model_Selection_in_Kernel_Based_Regression_using_the_Influence_Function%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">64 jmlr-2008-Model Selection in Kernel Based Regression using the Influence Function    (Special Topic on Model Selection)</a></p>
<p>18 0.45854262 <a title="62-lda-18" href="./jmlr-2008-Multi-class_Discriminant_Kernel_Learning_via_Convex_Programming%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">66 jmlr-2008-Multi-class Discriminant Kernel Learning via Convex Programming    (Special Topic on Model Selection)</a></p>
<p>19 0.45794517 <a title="62-lda-19" href="./jmlr-2008-Approximations_for_Binary_Gaussian_Process_Classification.html">16 jmlr-2008-Approximations for Binary Gaussian Process Classification</a></p>
<p>20 0.45533052 <a title="62-lda-20" href="./jmlr-2008-Online_Learning_of_Complex_Prediction_Problems_Using_Simultaneous_Projections.html">74 jmlr-2008-Online Learning of Complex Prediction Problems Using Simultaneous Projections</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
