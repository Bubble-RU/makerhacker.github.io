<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-36" href="#">jmlr2013-36</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</h1>
<br/><p>Source: <a title="jmlr-2013-36-pdf" href="http://jmlr.org/papers/volume14/cai13a/cai13a.pdf">pdf</a></p><p>Author: Tony Cai, Jianqing Fan, Tiefeng Jiang</p><p>Abstract: This paper studies the asymptotic behaviors of the pairwise angles among n randomly and uniformly distributed unit vectors in R p as the number of points n → ∞, while the dimension p is either ﬁxed or growing with n. For both settings, we derive the limiting empirical distribution of the random angles and the limiting distributions of the extreme angles. The results reveal interesting differences in the two settings and provide a precise characterization of the folklore that “all high-dimensional random vectors are almost always nearly orthogonal to each other”. Applications to statistics and machine learning and connections with some open problems in physics and mathematics are also discussed. Keywords: random angle, uniform distribution on sphere, empirical law, maximum of random variables, minimum of random variables, extreme-value distribution, packing on sphere</p><p>Reference: <a title="jmlr-2013-36-reference" href="../jmlr2013_reference/jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 For both settings, we derive the limiting empirical distribution of the random angles and the limiting distributions of the extreme angles. [sent-7, score-0.994]
</p><p>2 Keywords: random angle, uniform distribution on sphere, empirical law, maximum of random variables, minimum of random variables, extreme-value distribution, packing on sphere  1. [sent-10, score-0.566]
</p><p>3 For example, Hammersley (1950), Lord (1954), Alagar (1976) and Garc´a-Pelayo (2005) studied the distribution of the Euclidean distance ı between two random points on the unit sphere S p−1 . [sent-13, score-0.292]
</p><p>4 Williams (2001) showed that, when the underlying geometric object is a sphere or an ellipsoid, the distribution has a strong connection to the neutron transport theory. [sent-14, score-0.299]
</p><p>5 In this paper we consider the empirical law and extreme laws of the pairwise angles among a large number of random unit vectors. [sent-19, score-0.902]
</p><p>6 More speciﬁcally, let X1 , · · · , Xn be random points independently chosen with the uniform distribution on S p−1 , the unit sphere in R p . [sent-20, score-0.292]
</p><p>7 In the case of a ﬁxed dimension, the global behavior of the angles Θi j is captured by its empirical distribution µn =  1  ∑  δΘi j , n 2 1≤i< j≤n  n ≥ 2. [sent-23, score-0.537]
</p><p>8 (2)  In many applications it is of signiﬁcant interest to consider the extreme angles Θmin and Θmax deﬁned by Θmin = min{Θi j ; 1 ≤ i < j ≤ n};  Θmax = max{Θi j ; 1 ≤ i < j ≤ n}. [sent-25, score-0.55]
</p><p>9 (3) (4)  We will study both the empirical distribution of the angles Θi j , 1 ≤ i < j ≤ n, and the distributions of the extreme angles Θmin and Θmax as the number of points n → ∞, while the dimension p is either ﬁxed or growing with n. [sent-26, score-1.21]
</p><p>10 The distribution of minimum angle of n points randomly distributed on the p-dimensional unit sphere has important implications in statistics and machine learning. [sent-27, score-0.374]
</p><p>11 The present paper systematically investigates the asymptotic behaviors of the random angles {Θi j ; 1 ≤ i < j ≤ n}. [sent-35, score-0.525]
</p><p>12 It is shown that, when the dimension p is ﬁxed, as n → ∞, the empirical distribution µn converges to a distribution with the density function given by p 1 Γ( 2 ) √ h(θ) = · (sin θ) p−2 , θ ∈ [0, π]. [sent-36, score-0.369]
</p><p>13 p−1 π Γ( 2 )  On the other hand, when the dimension p grows with n, it is shown that the limiting normalized empirical distribution µn,p of the random angles Θi j , 1 ≤ i < j ≤ n is Gaussian. [sent-37, score-0.746]
</p><p>14 When the dimension is high, most of the angles are concentrated around π/2. [sent-38, score-0.5]
</p><p>15 In addition to the empirical law of the angles Θi j , we also consider the extreme laws of the random angles in both the ﬁxed and growing dimension settings. [sent-42, score-1.333]
</p><p>16 Furthermore, the limiting distribution of the sum of the two extreme angles Θmin + Θmax is also established. [sent-44, score-0.754]
</p><p>17 The distributions of the minimum and maximum angles as well as the empirical distributions of all pairwise angles have important applications in statistics. [sent-46, score-0.996]
</p><p>18 The extreme laws of the random angles considered in this paper is also related to the study of the coherence of a random matrix, which is deﬁned to be the largest magnitude of the Pearson correlation coefﬁcients between the columns of the random matrix. [sent-56, score-0.901]
</p><p>19 Section 2 studies the limiting empirical and extreme laws of the angles Θi j in the setting of the ﬁxed dimension p as the number of points n going to ∞. [sent-60, score-0.869]
</p><p>20 When The Dimension p Is Fixed In this section we consider the limiting empirical distribution of the angles Θi j , 1 ≤ i < j ≤ n when the number of random points n → ∞ while the dimension p is ﬁxed. [sent-66, score-0.746]
</p><p>21 Throughout the paper, we let X1 , X2 , · · · , Xn be independent random points with the uniform distribution on the unit sphere S p−1 for some ﬁxed p ≥ 2. [sent-68, score-0.292]
</p><p>22 We begin with the limiting empirical distribution of the random angles. [sent-69, score-0.279]
</p><p>23 Theorem 1 (Empirical Law for Fixed p) Let the empirical distribution µn of the angles Θi j , 1 ≤ i < j ≤ n, be deﬁned as in (1). [sent-70, score-0.537]
</p><p>24 Then, as n → ∞, with probability one, µn converges weakly to the 1839  C AI , FAN AND J IANG  distribution with density p 1 Γ( 2 ) h(θ) = √ · (sin θ) p−2 , θ ∈ [0, π]. [sent-71, score-0.437]
</p><p>25 Theorem 1 says that the average of these angles asymptotically has the same density as that of Θ12 . [sent-74, score-0.466]
</p><p>26 Theorem 1 implies that most of the angles in the total of n angles are 2 concentrated around π/2. [sent-76, score-0.875]
</p><p>27 In fact, in the extreme case when p → ∞, √ almost all of n angles go to π/2 at the rate p. [sent-78, score-0.55]
</p><p>28 Figure 1 plots the function h p (θ) = =  1 π θ h −√ p−2 2 p−2 p Γ( 2 ) θ 1 √ · cos √ √ p−1 p−2 π Γ( 2 ) p − 2 √  p−2  ,  θ ∈ [0, π]  (6)  which is the asymptotic density of the normalized empirical distribution µn,p deﬁned in (2) when the √ √ dimension p is ﬁxed. [sent-81, score-0.503]
</p><p>29 This can also be seen from the asymptotic approximation h p (θ) ∝ exp (p − 2) log cos √  θ p−2  2 /2  ≈ e−θ  . [sent-85, score-0.39]
</p><p>30 (7)  We now consider the limiting distribution of the extreme angles Θmin and Θmax . [sent-86, score-0.754]
</p><p>31 Then, both n2/(p−1) Θmin and n2/(p−1) (π − Θmax ) converge weakly to a distribution given by F(x) = as n → ∞, where  1 − e−Kx 0,  p−1  ,  if x ≥ 0; if x < 0,  p 1 Γ( 2 ) . [sent-88, score-0.269]
</p><p>32 K= √ 4 π Γ( p+1 ) 2  (8)  (9)  The above theorem says that the smallest angle Θmin is close to zero, and the largest angle Θmax is close to π as n grows. [sent-89, score-0.283]
</p><p>33 1 from Resnick (2007)), it is easy to check that n2Wn converges weakly to Exp(1) with the probability density function e−x I(x ≥ 0). [sent-110, score-0.361]
</p><p>34 5  (b)  Sum of min and max angles  0  0  50  100  200  100  300  150  400  (a)  Dist of min angles  3. [sent-155, score-1.169]
</p><p>35 (a) A realization of the empirical distribution µn ; (b) The average distribution of 200 realizations of µn ; (c) the distribution of Θmin and its asymptotic distribution exp(−x/(2π))/(2π); (d) the distribution of Θmin + Θmax ; the vertical line indicating the location π. [sent-168, score-0.569]
</p><p>36 Then, n2/(p−1) Θmax + Θmin − π converges weakly to the distribution of X − Y , where X and Y are i. [sent-181, score-0.355]
</p><p>37 6  An empirical dist  −4  −2  0  2  4  −4  −2  0  2  4  (b)  Sum of min and max angles  0  0  1  2  2  4  3  6  4  8  5  10  6  (a)  Dists of min and max angles  0. [sent-199, score-1.383]
</p><p>38 (a) A realization of the normalized empirical distribution µn,p given by (2); (b) The average distribution of 200 realizations of µn,p ; (c) the distribution of Θmin and its asymptotic distribution; (d) the distribution of Θmin + Θmax ; the vertical line indicating the location π. [sent-215, score-0.493]
</p><p>39 This is clearly different from the limiting distribution given in Theorem 1 when the dimension p is ﬁxed. [sent-227, score-0.25]
</p><p>40 6  An empirical dist  −4  −2  0  2  4  −4  −2  0  2  4  (b)  Sum of min and max angles  0  0  1  1  2  2  3  4  3  5  4  6  (a)  Dists of min and max angles  0. [sent-242, score-1.383]
</p><p>41 (a) A realization of the normalized empirical distribution µn,p given by (2); (b) The average distribution of 200 realizations of µn,p ; (c) the distribution of Θmin and its asymptotic distribution; (d) the distribution of Θmin + Θmax ; the vertical line indicating the location π. [sent-259, score-0.493]
</p><p>42 Then, with probability one, µn,p converges weakly to N(0, 1) as n → ∞. [sent-262, score-0.316]
</p><p>43 The theorem implies √ that most of the n random angles go to π/2 very quickly. [sent-265, score-0.505]
</p><p>44 Take any γ p → 0 such that pγ p → ∞ 2 and denote by Nn,p the number of the angles Θi j that are within γ p of π/2, that is, | π − Θi j | ≤ γ p . [sent-266, score-0.421]
</p><p>45 Note that cos ε ≤ 1 − ε2 /2 + ε4 /24, so π P |Θ − | ≥ 2  c log p p  c log p c2 log2 p √ ≤ K p 1− + 2p 24p2  p−2 1  ≤ K ′ p− 2 (c−1)  for all sufﬁciently large p, where K ′ is a constant depending only on c. [sent-279, score-0.415]
</p><p>46 We now turn to the limiting extreme laws of the angles when both n and p → ∞. [sent-282, score-0.783]
</p><p>47 For the extreme laws, it is necessary to divide into three asymptotic regimes: sub-exponential case 1 log n → 0, p exponential case 1 log n → β ∈ (0, ∞), and super-exponential case 1 log n → ∞. [sent-283, score-0.48]
</p><p>48 The limiting extreme p p laws are different in these three regimes. [sent-284, score-0.362]
</p><p>49 As n → ∞, 2p log sin Θmin + 4 log n − log log n converges weakly to the extreme value distri√ y/2 bution with the distribution function F(y) = 1 − e−Ke , y ∈ R and K = 1/(4 2π ). [sent-288, score-1.003]
</p><p>50 The above extreme value distribution differs from that in (8) where the dimension p is ﬁxed. [sent-291, score-0.251]
</p><p>51 Then p cos2 Θmin − 4 log n + log log n con1 verges weakly to a distribution with the cumulative distribution function exp{− 4√2π e−(y+8α y ∈ R. [sent-294, score-0.627]
</p><p>52 As n → ∞, 2p log sin Θmin + 4 log n − log log n converges weakly to a distribution with the distribution function F(y) = 1 − exp −K(β)e(y+8β)/2 , y ∈ R, where K(β) =  β 8π(1 − e−4β )  1/2  ,  and the conclusion still holds if Θmin is replaced by Θmax . [sent-298, score-0.95]
</p><p>53 As n → ∞, 2p log sin Θmin +  4p p−1  log n − log p converges weakly to the extreme value distri√ y/2 bution with the distribution function F(y) = 1 − e−Ke , y ∈ R with K = 1/(2 2π). [sent-304, score-0.909]
</p><p>54 Theorem 3 provides the limiting distribution of Θmax + Θmin − π when the dimension p is ﬁxed. [sent-311, score-0.25]
</p><p>55 Remark 10 As mentioned in the introduction, Cai and Jiang (2011, 2012) considered the limiting distribution of the coherence of a random matrix and the coherence is closely related to the minimum angle Θmin . [sent-314, score-0.518]
</p><p>56 This maximum is analyzed through modifying the proofs of the results for the limiting distribution of the coherence Ln,p in Cai and Jiang (2012). [sent-318, score-0.285]
</p><p>57 This provides the minimum angle test for sphericity or the packing test on sphericity. [sent-337, score-0.317]
</p><p>58 65  Table 1: The power (percent of rejections) of the packing test based on 2000 simulations  The packing test does not examine whether there is a gap in the data on the sphere. [sent-368, score-0.4]
</p><p>59 Discussions We have established the limiting empirical and extreme laws of the angles between random unit vectors, both for the ﬁxed dimension and growing dimension cases. [sent-387, score-1.026]
</p><p>60 For ﬁxed p, we study the empirical law of angles, the extreme law of angles and the law of the sum of the largest and smallest angles in Theorems 1, 2 and 3. [sent-388, score-1.299]
</p><p>61 Assuming p is large, we establish the empirical law of random angles in Theorem 4. [sent-389, score-0.592]
</p><p>62 The study of the random angles Θi j ’s, Θmin and Θmax is also related to several problems in machine learning as well as some deterministic open problems in physics and mathematics. [sent-396, score-0.512]
</p><p>63 1 Connections to Machine Learning Our studies shed lights on random geometric graphs, which are formed by n random points on the p-dimensional unit sphere as vertices with edge connecting between points Xi and X j if Θi j > δ for certain δ (Penrose, 2003; Devroye et al. [sent-399, score-0.282]
</p><p>64 (2013) showed an interesting asymptotic conical structure in the critical sample eigenvectors under a spike covariance models when the ratio between the dimension and the product of the sample size with the spike size converges to a nonzero constant. [sent-410, score-0.266]
</p><p>65 The behavior of the randomness of the eigenvectors within the cones is related to the behavior of the random angles studied in the present paper. [sent-413, score-0.489]
</p><p>66 2 Connections to Some Open Problems in Mathematics and Physics The results on random angles established in this paper can be potentially used to study a number of open deterministic problems in mathematics and physics. [sent-418, score-0.456]
</p><p>67 Note that xi − x j 2 = 2(1 − cos θi j ), where θi j is the angle −→  −→  between vectors Oxi and Ox j . [sent-436, score-0.344]
</p><p>68 Then  1 ˜ = max (1 − cos θi j ) = 1 − cos Θmax , 2E(R, −∞)2 x1 ,··· ,xn ∈S p−1 ˜ where Θmax = max{θi j ; 1 ≤ i < j ≤ n}. [sent-437, score-0.559]
</p><p>69 Then, it is not difﬁcult to see 1 1 ˜ = sup = sup(1 − cos Θmax ) = 1 − cos ∆ 2 2 2ε(R, −∞) R 2E(R, −∞) R where ∆ := ess · sup(Θmax ) is the essential upper bound of the random variable Θmax as deﬁned in (4). [sent-442, score-0.55]
</p><p>70 2(1 − cos ∆)  (10)  The essential upper bound ∆ of the random variable Θmax can be approximated by random sampling of Θmax . [sent-444, score-0.297]
</p><p>71 So the approach outlined above provides a direct way for using a stochastic method to study these deterministic problems and establishes connections between the random angles and open problems mentioned above. [sent-445, score-0.505]
</p><p>72 1 Technical Results Recall that X1 , X2 , · · · are random points independently chosen with the uniform distribution on −→  −→  S p−1 , the unit sphere in R p , and Θi j is the angle between OXi and OX j and ρi j = cos Θi j for any i = j. [sent-453, score-0.636]
</p><p>73 204 from Ahlfors (1979)): √ 1 1 log Γ(z) = z log z − z − log z + log 2π + O 2 x as x = Re (z) → ∞, it is easy to verify that p Γ( 2 )  Γ( p−1 ) 2  ∼ 1852  p 2  (17)  D ISTRIBUTIONS OF A NGLES IN R ANDOM PACKING ON S PHERES  as p → ∞. [sent-499, score-0.376]
</p><p>74 2 Proofs of Main Results in Section 2 Lemma 16 Let X1 , X2 , · · · be independent random points with the uniform distribution on the unit sphere in R p . [sent-504, score-0.292]
</p><p>75 Then, with probability one, µn in (1) converges weakly to µ as n → ∞. [sent-506, score-0.316]
</p><p>76 If ϕn (Θ12 ) converges weakly to a probability measure ν as n → ∞, then, with probability one, νn :=  1  ∑  δϕn (Θi j ) n 2 1≤i< j≤n  (18)  converges weakly to ν as n → ∞. [sent-508, score-0.632]
</p><p>77 This leads to that, with probability one, µn in (1) converges weakly to µ as n → ∞. [sent-523, score-0.316]
</p><p>78 (ii) Since ϕn (Θ12 ) converges weakly to ν as n → ∞, we know that, for any bounded continu∞ ous function u(x) deﬁned on R, Eu(ϕn (Θ12 )) → −∞ u(x) dν(x) as n → ∞. [sent-524, score-0.279]
</p><p>79 Reviewing the deﬁnition of νn in (18), the above asserts that, with probability one, νn converges weakly to ν as n → ∞. [sent-529, score-0.316]
</p><p>80 Recall X1 , · · · , Xn are random points independently chosen with the uniform distribution on −→  S p−1 ,  −→  the unit sphere in R p , and Θi j is the angle between OXi and OX j and ρi j = cos Θi j for all 1 ≤ i, j ≤ n. [sent-532, score-0.636]
</p><p>81 First, since Mn = cos Θmin by (3), then use the identity 1 − cos h = 2 sin2 h for 2 all h ∈ R to have n4/(p−1) (1 − Mn ) = 2n4/(p−1) sin2  Θmin . [sent-565, score-0.454]
</p><p>82 2  (27)  min By Proposition 17 and the Slusky lemma, sin Θ2 → 0 in probability as n → ∞. [sent-566, score-0.291]
</p><p>83 By Proposition 17 and the Slusky lemma again, 1 n4/(p−1) Θ2 converges min 2 in distribution to F1 (x) as in Proposition 17. [sent-569, score-0.327]
</p><p>84 K = 2(1−p)/2 K1 = √ 4 π Γ( p+1 ) 2  (29)  n2/(p−1) (π − Θmax ) converges weakly to F(x) as n → ∞. [sent-571, score-0.279]
</p><p>85 By the standard subsequence argument, we obtain that Qn converges weakly to the distribution of (X,Y ) as n → ∞. [sent-585, score-0.355]
</p><p>86 We claim that 2 Yn converges weakly to N(0, 1)  (40)  √ as n → ∞. [sent-615, score-0.279]
</p><p>87 Assuming this is true, taking ϕn (θ) = p( π − θ) for θ ∈ [0, π] and ν = N(0, 1) in (ii) of 2 Lemma 16, then, with probability one, µn,p converges weakly to N(0, 1) as n → ∞. [sent-616, score-0.316]
</p><p>88 In fact, noticing Θ12 has density h(θ) in (13), it is easy to see that Yn has density function hn (y) : = =  p 1 Γ( 2 ) π y √ −√ · sin p−1 2 p π Γ( 2 ) p y 1 Γ( 2 ) · cos √ √ pπ Γ( p−1 ) p 2  p−2  1 · −√ p  p−2  (41)  for any y ∈ R as n is sufﬁciently large since limn→∞ pn = ∞. [sent-618, score-0.589]
</p><p>89 Then, as n → ∞, pTn + 4 log n − log log n y/2  converges weakly to an extreme value distribution with the distribution function F(y) = 1−e−Ke , y ∈ √ √ R and K = 1/(2 8π) = 1/(4 2π). [sent-638, score-0.842]
</p><p>90 From (11) we know Mn = max ρi j = cos Θmin and Θmin ∈ [0, π];  (44)  2 Tn = log(1 − Mn ) = 2 log sin Θmin . [sent-639, score-0.569]
</p><p>91 Now, observe that min {π − Θi j } = π − Θmax and sin(π − Θmax ) = sin Θmax . [sent-641, score-0.254]
</p><p>92 Finally, by the same argument between (30) and (31) again, and by (46) we obtain 2p log sin Θmax + 4 log n − log log n 1860  D ISTRIBUTIONS OF A NGLES IN R ANDOM PACKING ON S PHERES  √ y/2 converges weakly to F(y) = 1 − e−Ke , y ∈ R and K = 1/(4 2π ). [sent-645, score-0.798]
</p><p>93 Replacing Ln and Theorem 1 there by Mn and Theorem 6, we get that 2 pMn − 4 log n + log log n 1 converges weakly to the distribution function exp{− 4√2π e−(y+8α sion follows since Mn = cos Θmin . [sent-650, score-0.864]
</p><p>94 Then, as n → ∞, pTn + 4 log n − log log n converges weakly to the distribution function F(y) = 1 − exp −K(β)e(y+8β)/2 , y ∈ R, where K(β) =  β 1 2 2π(1 − e−4β )  1/2  =  β 8π(1 − e−4β )  1/2  . [sent-660, score-0.637]
</p><p>95 From (44) and (45) we obtain Θmin → cos−1  1 − e−4β in probability and  (47)  2p log sin Θmin + 4 log n − log log n  (48)  converges weakly to the distribution function F(y) = 1 − exp −K(β)e(y+8β)/2 , y ∈ R, where K(β) =  β 8π(1 − e−4β )  1/2  (49)  as n → ∞. [sent-661, score-0.911]
</p><p>96 Now, reviewing (46) and the argument between (30) and (31), by (47) and (48), we conclude that Θmax → π − cos−1 1 − e−4β in probability and 2p log sin Θmax + 4 log n − log log n converges weakly to the distribution function F(y) as in (49). [sent-662, score-0.911]
</p><p>97 1861  C AI , FAN AND J IANG  ii) As n → ∞, pMn +  4p log n − log p p−1  √ y/2 converges weakly to the distribution function F(y) = 1 − e−Ke , y ∈ R with K = 1/(2 2π). [sent-671, score-0.543]
</p><p>98 Combining i), ii), (44) and (45), we see that, as n → ∞, Θmin → 0 in probability; 4p 2p log sin Θmin + log n − log p converges weakly to p−1  √ y/2 F(y) = 1 − e−Ke , y ∈ R with K = 1/(2 2π). [sent-672, score-0.704]
</p><p>99 Finally, combining the above two convergence results with (46) and the argument between (30) and (31), we have Θmax → π in probability; 4p 2p log sin Θmax + log n − log p converges weakly to p−1  √ y/2 F(y) = 1 − e−Ke , y ∈ R with K = 1/(2 2π). [sent-673, score-0.704]
</p><p>100 Phase transition in limiting distributions of coherence of highdimensional random matrices. [sent-702, score-0.281]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('angles', 0.421), ('cos', 0.227), ('iang', 0.222), ('ngles', 0.206), ('pheres', 0.206), ('packing', 0.2), ('weakly', 0.193), ('istributions', 0.176), ('mn', 0.172), ('jiang', 0.163), ('andom', 0.146), ('sphere', 0.145), ('sin', 0.143), ('fan', 0.134), ('cai', 0.133), ('extreme', 0.129), ('limiting', 0.128), ('angle', 0.117), ('min', 0.111), ('tiefeng', 0.111), ('laws', 0.105), ('max', 0.105), ('law', 0.096), ('log', 0.094), ('spurious', 0.09), ('converges', 0.086), ('coherence', 0.081), ('oxi', 0.079), ('ke', 0.079), ('distribution', 0.076), ('pn', 0.071), ('ty', 0.069), ('dist', 0.069), ('tx', 0.069), ('asymptotic', 0.069), ('limn', 0.063), ('katanforoush', 0.063), ('ess', 0.061), ('correlation', 0.06), ('hn', 0.058), ('physics', 0.056), ('lemma', 0.054), ('tn', 0.053), ('ai', 0.053), ('tony', 0.053), ('eu', 0.05), ('connections', 0.049), ('theorem', 0.049), ('deli', 0.047), ('eun', 0.047), ('folklore', 0.047), ('jianqing', 0.047), ('kuijlaars', 0.047), ('neutron', 0.047), ('shahshahani', 0.047), ('dimension', 0.046), ('density', 0.045), ('realizations', 0.043), ('un', 0.043), ('ox', 0.042), ('pairwise', 0.04), ('growing', 0.04), ('empirical', 0.04), ('cp', 0.04), ('ii', 0.039), ('dx', 0.038), ('extremal', 0.038), ('probability', 0.037), ('distributions', 0.037), ('realization', 0.037), ('ave', 0.037), ('xn', 0.036), ('unit', 0.036), ('random', 0.035), ('kendall', 0.034), ('kx', 0.034), ('eigenvectors', 0.033), ('concentrated', 0.033), ('arxiv', 0.032), ('armentano', 0.032), ('arratia', 0.032), ('assertions', 0.032), ('castro', 0.032), ('conical', 0.032), ('coulomb', 0.032), ('diaconis', 0.032), ('dists', 0.032), ('gautier', 0.032), ('isotropicity', 0.032), ('nimum', 0.032), ('pmn', 0.032), ('saff', 0.032), ('slusky', 0.032), ('stoyan', 0.032), ('tammes', 0.032), ('weidong', 0.032), ('wilfrid', 0.032), ('vy', 0.032), ('proposition', 0.031), ('geometric', 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="36-tfidf-1" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>Author: Tony Cai, Jianqing Fan, Tiefeng Jiang</p><p>Abstract: This paper studies the asymptotic behaviors of the pairwise angles among n randomly and uniformly distributed unit vectors in R p as the number of points n → ∞, while the dimension p is either ﬁxed or growing with n. For both settings, we derive the limiting empirical distribution of the random angles and the limiting distributions of the extreme angles. The results reveal interesting differences in the two settings and provide a precise characterization of the folklore that “all high-dimensional random vectors are almost always nearly orthogonal to each other”. Applications to statistics and machine learning and connections with some open problems in physics and mathematics are also discussed. Keywords: random angle, uniform distribution on sphere, empirical law, maximum of random variables, minimum of random variables, extreme-value distribution, packing on sphere</p><p>2 0.11312951 <a title="36-tfidf-2" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<p>Author: Nakul Verma</p><p>Abstract: Low dimensional embeddings of manifold data have gained popularity in the last decade. However, a systematic ﬁnite sample analysis of manifold embedding algorithms largely eludes researchers. Here we present two algorithms that embed a general n-dimensional manifold into Rd (where d only depends on some key manifold properties such as its intrinsic dimension, volume and curvature) that guarantee to approximately preserve all interpoint geodesic distances. Keywords: manifold learning, isometric embeddings, non-linear dimensionality reduction, Nash’s embedding theorem</p><p>3 0.10331292 <a title="36-tfidf-3" href="./jmlr-2013-On_the_Mutual_Nearest_Neighbors_Estimate_in_Regression.html">79 jmlr-2013-On the Mutual Nearest Neighbors Estimate in Regression</a></p>
<p>Author: Arnaud Guyader, Nick Hengartner</p><p>Abstract: Motivated by promising experimental results, this paper investigates the theoretical properties of a recently proposed nonparametric estimator, called the Mutual Nearest Neighbors rule, which estimates the regression function m(x) = E[Y |X = x] as follows: ﬁrst identify the k nearest neighbors of x in the sample Dn , then keep only those for which x is itself one of the k nearest neighbors, and ﬁnally take the average over the corresponding response variables. We prove that this estimator is consistent and that its rate of convergence is optimal. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, we also present adaptation results by data-splitting. Keywords: nonparametric estimation, nearest neighbor methods, mathematical statistics</p><p>4 0.072219931 <a title="36-tfidf-4" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>Author: Tony Cai, Wen-Xin Zhou</p><p>Abstract: We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed. Keywords: 1-bit matrix completion, low-rank matrix, max-norm, trace-norm, constrained optimization, maximum likelihood estimate, optimal rate of convergence</p><p>5 0.071226202 <a title="36-tfidf-5" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>Author: Eva L. Dyer, Aswin C. Sankaranarayanan, Richard G. Baraniuk</p><p>Abstract: Unions of subspaces provide a powerful generalization of single subspace models for collections of high-dimensional data; however, learning multiple subspaces from data is challenging due to the fact that segmentation—the identiﬁcation of points that live in the same subspace—and subspace estimation must be performed simultaneously. Recently, sparse recovery methods were shown to provide a provable and robust strategy for exact feature selection (EFS)—recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with ℓ1 -minimization, in this paper, we develop sufﬁcient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide signiﬁcant advantages over NN methods and that the gap between the two approaches is particularly pronounced when the sampling of subspaces in the data set is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble. Keywords: subspace clustering, unions of subspaces, hybrid linear models, sparse approximation, structured sparsity, nearest neighbors, low-rank approximation</p><p>6 0.06882266 <a title="36-tfidf-6" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>7 0.063187093 <a title="36-tfidf-7" href="./jmlr-2013-CODA%3A_High_Dimensional_Copula_Discriminant_Analysis.html">20 jmlr-2013-CODA: High Dimensional Copula Discriminant Analysis</a></p>
<p>8 0.057235669 <a title="36-tfidf-8" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>9 0.054746836 <a title="36-tfidf-9" href="./jmlr-2013-On_the_Convergence_of_Maximum_Variance_Unfolding.html">77 jmlr-2013-On the Convergence of Maximum Variance Unfolding</a></p>
<p>10 0.054637399 <a title="36-tfidf-10" href="./jmlr-2013-Variable_Selection_in_High-Dimension_with_Random_Designs_and_Orthogonal_Matching_Pursuit.html">119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</a></p>
<p>11 0.054623879 <a title="36-tfidf-11" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>12 0.053801466 <a title="36-tfidf-12" href="./jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning.html">35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</a></p>
<p>13 0.052590765 <a title="36-tfidf-13" href="./jmlr-2013-Consistent_Selection_of_Tuning_Parameters_via_Variable_Selection_Stability.html">27 jmlr-2013-Consistent Selection of Tuning Parameters via Variable Selection Stability</a></p>
<p>14 0.050635051 <a title="36-tfidf-14" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>15 0.050165597 <a title="36-tfidf-15" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>16 0.049690884 <a title="36-tfidf-16" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>17 0.049570747 <a title="36-tfidf-17" href="./jmlr-2013-Risk_Bounds_of_Learning_Processes_for_L%C3%A9vy_Processes.html">97 jmlr-2013-Risk Bounds of Learning Processes for Lévy Processes</a></p>
<p>18 0.048088886 <a title="36-tfidf-18" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>19 0.044969562 <a title="36-tfidf-19" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>20 0.044462051 <a title="36-tfidf-20" href="./jmlr-2013-Derivative_Estimation_with_Local_Polynomial_Fitting.html">31 jmlr-2013-Derivative Estimation with Local Polynomial Fitting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.225), (1, 0.076), (2, 0.094), (3, -0.011), (4, 0.028), (5, -0.078), (6, 0.02), (7, 0.051), (8, 0.122), (9, 0.009), (10, -0.17), (11, 0.007), (12, 0.069), (13, 0.021), (14, -0.125), (15, 0.118), (16, -0.008), (17, -0.109), (18, -0.075), (19, -0.147), (20, 0.198), (21, -0.031), (22, -0.083), (23, -0.079), (24, -0.047), (25, 0.157), (26, -0.004), (27, -0.094), (28, -0.035), (29, 0.057), (30, 0.061), (31, 0.102), (32, -0.039), (33, 0.192), (34, 0.07), (35, -0.033), (36, -0.043), (37, 0.022), (38, 0.127), (39, 0.05), (40, -0.135), (41, 0.104), (42, 0.02), (43, 0.036), (44, -0.021), (45, -0.204), (46, 0.09), (47, 0.082), (48, -0.063), (49, -0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93859524 <a title="36-lsi-1" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>Author: Tony Cai, Jianqing Fan, Tiefeng Jiang</p><p>Abstract: This paper studies the asymptotic behaviors of the pairwise angles among n randomly and uniformly distributed unit vectors in R p as the number of points n → ∞, while the dimension p is either ﬁxed or growing with n. For both settings, we derive the limiting empirical distribution of the random angles and the limiting distributions of the extreme angles. The results reveal interesting differences in the two settings and provide a precise characterization of the folklore that “all high-dimensional random vectors are almost always nearly orthogonal to each other”. Applications to statistics and machine learning and connections with some open problems in physics and mathematics are also discussed. Keywords: random angle, uniform distribution on sphere, empirical law, maximum of random variables, minimum of random variables, extreme-value distribution, packing on sphere</p><p>2 0.5359835 <a title="36-lsi-2" href="./jmlr-2013-On_the_Mutual_Nearest_Neighbors_Estimate_in_Regression.html">79 jmlr-2013-On the Mutual Nearest Neighbors Estimate in Regression</a></p>
<p>Author: Arnaud Guyader, Nick Hengartner</p><p>Abstract: Motivated by promising experimental results, this paper investigates the theoretical properties of a recently proposed nonparametric estimator, called the Mutual Nearest Neighbors rule, which estimates the regression function m(x) = E[Y |X = x] as follows: ﬁrst identify the k nearest neighbors of x in the sample Dn , then keep only those for which x is itself one of the k nearest neighbors, and ﬁnally take the average over the corresponding response variables. We prove that this estimator is consistent and that its rate of convergence is optimal. Since the estimate with the optimal rate of convergence depends on the unknown distribution of the observations, we also present adaptation results by data-splitting. Keywords: nonparametric estimation, nearest neighbor methods, mathematical statistics</p><p>3 0.45807573 <a title="36-lsi-3" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>Author: Tony Cai, Wen-Xin Zhou</p><p>Abstract: We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed. Keywords: 1-bit matrix completion, low-rank matrix, max-norm, trace-norm, constrained optimization, maximum likelihood estimate, optimal rate of convergence</p><p>4 0.45594126 <a title="36-lsi-4" href="./jmlr-2013-Variable_Selection_in_High-Dimension_with_Random_Designs_and_Orthogonal_Matching_Pursuit.html">119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</a></p>
<p>Author: Antony Joseph</p><p>Abstract: The performance of orthogonal matching pursuit (OMP) for variable selection is analyzed for random designs. When contrasted with the deterministic case, since the performance is here measured after averaging over the distribution of the design matrix, one can have far less stringent sparsity constraints on the coefﬁcient vector. We demonstrate that for exact sparse vectors, the performance of the OMP is similar to known results on the Lasso algorithm (Wainwright, 2009). Moreover, variable selection under a more relaxed sparsity assumption on the coefﬁcient vector, whereby one has only control on the ℓ1 norm of the smaller coefﬁcients, is also analyzed. As consequence of these results, we also show that the coefﬁcient estimate satisﬁes strong oracle type inequalities. Keywords: high dimensional regression, greedy algorithms, Lasso, compressed sensing</p><p>5 0.45315102 <a title="36-lsi-5" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>Author: Eva L. Dyer, Aswin C. Sankaranarayanan, Richard G. Baraniuk</p><p>Abstract: Unions of subspaces provide a powerful generalization of single subspace models for collections of high-dimensional data; however, learning multiple subspaces from data is challenging due to the fact that segmentation—the identiﬁcation of points that live in the same subspace—and subspace estimation must be performed simultaneously. Recently, sparse recovery methods were shown to provide a provable and robust strategy for exact feature selection (EFS)—recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with ℓ1 -minimization, in this paper, we develop sufﬁcient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide signiﬁcant advantages over NN methods and that the gap between the two approaches is particularly pronounced when the sampling of subspaces in the data set is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble. Keywords: subspace clustering, unions of subspaces, hybrid linear models, sparse approximation, structured sparsity, nearest neighbors, low-rank approximation</p><p>6 0.43142182 <a title="36-lsi-6" href="./jmlr-2013-CODA%3A_High_Dimensional_Copula_Discriminant_Analysis.html">20 jmlr-2013-CODA: High Dimensional Copula Discriminant Analysis</a></p>
<p>7 0.43079513 <a title="36-lsi-7" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<p>8 0.38624704 <a title="36-lsi-8" href="./jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning.html">35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</a></p>
<p>9 0.35360935 <a title="36-lsi-9" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>10 0.31664506 <a title="36-lsi-10" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>11 0.30724019 <a title="36-lsi-11" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>12 0.30213737 <a title="36-lsi-12" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>13 0.29901645 <a title="36-lsi-13" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>14 0.2817359 <a title="36-lsi-14" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>15 0.28063437 <a title="36-lsi-15" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>16 0.27457854 <a title="36-lsi-16" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>17 0.27412054 <a title="36-lsi-17" href="./jmlr-2013-On_the_Convergence_of_Maximum_Variance_Unfolding.html">77 jmlr-2013-On the Convergence of Maximum Variance Unfolding</a></p>
<p>18 0.26672325 <a title="36-lsi-18" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>19 0.26238406 <a title="36-lsi-19" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>20 0.25734478 <a title="36-lsi-20" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.018), (5, 0.152), (6, 0.025), (10, 0.077), (20, 0.013), (23, 0.032), (44, 0.011), (68, 0.02), (70, 0.037), (75, 0.035), (85, 0.036), (87, 0.024), (94, 0.425)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70567298 <a title="36-lda-1" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>Author: Tony Cai, Jianqing Fan, Tiefeng Jiang</p><p>Abstract: This paper studies the asymptotic behaviors of the pairwise angles among n randomly and uniformly distributed unit vectors in R p as the number of points n → ∞, while the dimension p is either ﬁxed or growing with n. For both settings, we derive the limiting empirical distribution of the random angles and the limiting distributions of the extreme angles. The results reveal interesting differences in the two settings and provide a precise characterization of the folklore that “all high-dimensional random vectors are almost always nearly orthogonal to each other”. Applications to statistics and machine learning and connections with some open problems in physics and mathematics are also discussed. Keywords: random angle, uniform distribution on sphere, empirical law, maximum of random variables, minimum of random variables, extreme-value distribution, packing on sphere</p><p>2 0.40501443 <a title="36-lda-2" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>Author: Eva L. Dyer, Aswin C. Sankaranarayanan, Richard G. Baraniuk</p><p>Abstract: Unions of subspaces provide a powerful generalization of single subspace models for collections of high-dimensional data; however, learning multiple subspaces from data is challenging due to the fact that segmentation—the identiﬁcation of points that live in the same subspace—and subspace estimation must be performed simultaneously. Recently, sparse recovery methods were shown to provide a provable and robust strategy for exact feature selection (EFS)—recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with ℓ1 -minimization, in this paper, we develop sufﬁcient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide signiﬁcant advantages over NN methods and that the gap between the two approaches is particularly pronounced when the sampling of subspaces in the data set is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble. Keywords: subspace clustering, unions of subspaces, hybrid linear models, sparse approximation, structured sparsity, nearest neighbors, low-rank approximation</p><p>3 0.39815998 <a title="36-lda-3" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>Author: Yuchen Zhang, John C. Duchi, Martin J. Wainwright</p><p>Abstract: We analyze two communication-efﬁcient algorithms for distributed optimization in statistical settings involving large-scale data sets. The ﬁrst algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves √ mean-squared error (MSE) that decays as O (N −1 + (N/m)−2 ). Whenever m ≤ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O (N −1 + (N/m)−3 ), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O (N −1 + (N/m)−3/2 ), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efﬁciently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 108 samples and d ≈ 740,000 covariates. Keywords: distributed learning, stochastic optimization, averaging, subsampling</p><p>4 0.39364123 <a title="36-lda-4" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>Author: Tony Cai, Wen-Xin Zhou</p><p>Abstract: We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed. Keywords: 1-bit matrix completion, low-rank matrix, max-norm, trace-norm, constrained optimization, maximum likelihood estimate, optimal rate of convergence</p><p>5 0.3892687 <a title="36-lda-5" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>Author: Sébastien Gerchinovitz</p><p>Abstract: We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T . We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same ﬂavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with ﬁxed design. Keywords: sparsity, online linear regression, individual sequences, adaptive regret bounds</p><p>6 0.38911483 <a title="36-lda-6" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>7 0.38829115 <a title="36-lda-7" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>8 0.38783297 <a title="36-lda-8" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>9 0.38561872 <a title="36-lda-9" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>10 0.38526002 <a title="36-lda-10" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>11 0.38470727 <a title="36-lda-11" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>12 0.38424316 <a title="36-lda-12" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>13 0.38365945 <a title="36-lda-13" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>14 0.38354763 <a title="36-lda-14" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>15 0.38349608 <a title="36-lda-15" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>16 0.38280049 <a title="36-lda-16" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>17 0.38273084 <a title="36-lda-17" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>18 0.38200405 <a title="36-lda-18" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>19 0.38162139 <a title="36-lda-19" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>20 0.38073817 <a title="36-lda-20" href="./jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications.html">18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
