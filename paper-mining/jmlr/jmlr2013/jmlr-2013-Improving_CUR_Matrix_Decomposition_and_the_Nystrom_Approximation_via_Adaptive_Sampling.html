<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 jmlr-2013-Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-53" href="#">jmlr2013-53</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 jmlr-2013-Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling</h1>
<br/><p>Source: <a title="jmlr-2013-53-pdf" href="http://jmlr.org/papers/volume14/wang13c/wang13c.pdf">pdf</a></p><p>Author: Shusen Wang, Zhihua Zhang</p><p>Abstract: The CUR matrix decomposition and the Nystr¨ m approximation are two important low-rank matrix o approximation techniques. The Nystr¨ m method approximates a symmetric positive semideﬁnite o matrix in terms of a small number of its columns, while CUR approximates an arbitrary data matrix by a small number of its columns and rows. Thus, CUR decomposition can be regarded as an extension of the Nystr¨ m approximation. o In this paper we establish a more general error bound for the adaptive column/row sampling algorithm, based on which we propose more accurate CUR and Nystr¨ m algorithms with expected o relative-error bounds. The proposed CUR and Nystr¨ m algorithms also have low time complexity o and can avoid maintaining the whole data matrix in RAM. In addition, we give theoretical analysis for the lower error bounds of the standard Nystr¨ m method and the ensemble Nystr¨ m method. o o The main theoretical results established in this paper are novel, and our analysis makes no special assumption on the data matrices. Keywords: large-scale matrix computation, CUR matrix decomposition, the Nystr¨ m method, o randomized algorithms, adaptive sampling</p><p>Reference: <a title="jmlr-2013-53-reference" href="../jmlr2013_reference/jmlr-2013-Improving_CUR_Matrix_Decomposition_and_the_Nystrom_Approximation_via_Adaptive_Sampling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 CN  Department of Computer Science and Engineering Shanghai Jiao Tong University 800 Dong Chuan Road, Shanghai, China 200240  Editor: Mehryar Mohri  Abstract The CUR matrix decomposition and the Nystr¨ m approximation are two important low-rank matrix o approximation techniques. [sent-5, score-0.184]
</p><p>2 The Nystr¨ m method approximates a symmetric positive semideﬁnite o matrix in terms of a small number of its columns, while CUR approximates an arbitrary data matrix by a small number of its columns and rows. [sent-6, score-0.179]
</p><p>3 o In this paper we establish a more general error bound for the adaptive column/row sampling algorithm, based on which we propose more accurate CUR and Nystr¨ m algorithms with expected o relative-error bounds. [sent-8, score-0.379]
</p><p>4 Keywords: large-scale matrix computation, CUR matrix decomposition, the Nystr¨ m method, o randomized algorithms, adaptive sampling  1. [sent-12, score-0.433]
</p><p>5 Given a matrix A ∈ Rm×n , column selection algorithms aim to choose c columns of A to construct a matrix C ∈ Rm×c such that A − CC† A ξ achieves the minimum. [sent-44, score-0.252]
</p><p>6 Here “ξ = 2,” “ξ = F,” and “ξ = ∗” respectively represent the matrix spectral norm, the matrix Frobenius norm, and the matrix nuclear norm, and C† denotes the Moore-Penrose inverse of C. [sent-45, score-0.184]
</p><p>7 Particularly, a CUR decomposition algorithm seeks to ﬁnd a subset of c columns of A to form a matrix C ∈ Rm×c , a subset of r rows to form a matrix R ∈ Rr×n , and an intersection ˜ matrix U ∈ Rc×r such that A − CUR ξ is small. [sent-70, score-0.315]
</p><p>8 For example, for an m×n matrix A and a target rank k ≪ min{m, n}, the subspace sampling algorithm (Drineas et al. [sent-83, score-0.425]
</p><p>9 The subspace sampling algorithm selects columns/rows according to the statistical leverage scores, so the computational cost of this algorithm is at least equal to the cost of the truncated SVD of A, that is, O (mnk) in general. [sent-87, score-0.392]
</p><p>10 (2012) devised fast approximation to statistical leverage scores which can be used to speedup the subspace sampling algorithm heuristically—yet no theoretical results have been reported that the leverage scores approximation can give provably efﬁcient subspace sampling algorithm. [sent-90, score-0.84]
</p><p>11 Despite their strong resemblance, CUR is a harder problem than column selection because “one can get good columns or rows separately” does not mean that one can get good columns and rows together. [sent-94, score-0.291]
</p><p>12 Specifo ically, given an m×m SPSD matrix A, they require sampling c (< m) columns of A to construct an m × c matrix C. [sent-110, score-0.372]
</p><p>13 Since there exists an m×m permutation matrix Π such that ΠC consists of the ﬁrst c columns of ΠAΠT , we always assume that C consists of the ﬁrst c columns of A without loss of generality. [sent-111, score-0.211]
</p><p>14 Then the ensemble method combines the samples to construct an approximation in the form of ˜ ens At,c =  t  ∑ µ(i) C(i) W(i)  †  T  C(i) ,  (2)  i=1  where µ(i) are the weights of the samples. [sent-122, score-0.208]
</p><p>15 The matrix multiplications can be executed very efﬁciently in multi-processor environment, so ideally computing the intersection matrix costs time only linear in m. [sent-129, score-0.179]
</p><p>16 o o To generate effective approximations, much work has been built on the upper error bounds of the sampling techniques for the Nystr¨ m method. [sent-134, score-0.231]
</p><p>17 4 Contributions and Outline The main technical contribution of this work is the adaptive sampling bound in Theorem 5, which is an extension of Theorem 2. [sent-149, score-0.352]
</p><p>18 (2006) bounds the error incurred by projection onto column or row space, while our Theorem 5 bounds the error incurred by the projection simultaneously onto column space and row space. [sent-154, score-0.288]
</p><p>19 More importantly, our adaptive sampling bound provides an approach for improving CUR and the Nystr¨ m approximation: no matter which relative-error column selection algorithm is employed, o Theorem 5 ensures relative-error bounds for CUR and the Nystr¨ m approximation. [sent-158, score-0.436]
</p><p>20 Based on the adaptive sampling bound in Theorem 5 and its corollary 7, we provide a concrete CUR algorithm which beats the best existing algorithm—the subspace sampling algorithm—both theoretically and empirically. [sent-160, score-0.681]
</p><p>21 In Table 1 we present a comparison between our proposed CUR algorithm and the subspace sampling algorithm. [sent-162, score-0.329]
</p><p>22 Our method is more scalable for it works on only a few columns or rows of the data matrix in question; in contrast, the subspace sampling algorithm maintains the whole data matrix in RAM to implement SVD. [sent-164, score-0.545]
</p><p>23 Another important application of the adaptive sampling bound is to yield an algorithm for the modiﬁed Nystr¨ m method. [sent-165, score-0.352]
</p><p>24 The algorithm has a strong relative-error upper bound: for a target rank o k, by sampling 2k 1 + o(1) columns it achieves relative-error bound in expectation. [sent-166, score-0.32]
</p><p>25 From the table we can see that the upper error bound of our adaptive sampling algorithm for the modiﬁed Nystr¨ m method is o even better than the lower bounds of the conventional Nystr¨ m methods. [sent-175, score-0.433]
</p><p>26 Informally speaking, the columns (or rows) with high leverage scores have greater inﬂuence in rank-k approximation than those with low leverage scores. [sent-201, score-0.209]
</p><p>27 1 we present an adaptive sampling algorithm and its relative-error bound established by Deshpande et al. [sent-214, score-0.352]
</p><p>28 1 The Adaptive Sampling Algorithm Adaptive sampling is an effective and efﬁcient column sampling algorithm for reducing the error incurred by the ﬁrst round of sampling. [sent-224, score-0.465]
</p><p>29 After one has selected a small subset of columns (denoted C1 ), an adaptive sampling method is used to further select a proportion of columns according to the residual of the ﬁrst round, that is, A − C1 C† A. [sent-225, score-0.513]
</p><p>30 The approximation error is guaranteed to be 1 decreasing by a factor after the adaptive sampling (Deshpande et al. [sent-226, score-0.392]
</p><p>31 We will establish in Theorem 5 a more general and more useful error bound for this adaptive sampling algorithm. [sent-242, score-0.379]
</p><p>32 , 2011), and the adaptive sampling algorithm (Deshpande et al. [sent-255, score-0.335]
</p><p>33 Given a matrix A ∈ Rm×n , the truncated pivoted QR decomposition procedure deterministically ﬁnds a set of columns C ∈ Rm×c by column pivoting, whose span approximates the column space of A, and computes an upper triangular matrix TC ∈ Rc×c that orthogonalizes those columns. [sent-277, score-0.383]
</p><p>34 The sampling probabilities in the two stages are proportional to the leverage scores of A and C, respectively. [sent-291, score-0.236]
</p><p>35 That is, in the ﬁrst stage the sampling probabilities are proportional to the squared ℓ2 -norm of the rows of VA,k ; in the second stage the sampling probabilities are proportional to the squared ℓ2 -norm of the rows of UC . [sent-292, score-0.424]
</p><p>36 That is why it is called the subspace sampling algorithm. [sent-293, score-0.329]
</p><p>37 Here we show the main results of the subspace sampling algorithm in the following lemma. [sent-294, score-0.329]
</p><p>38 Lemma 3 (Subspace Sampling for CUR ) Given an m × n matrix A and a target rank k ≪ min{m, n}, the subspace sampling algorithm selects c = O (kε−2 log k log(1/δ)) columns and r = O cε−2 log c log(1/δ) rows without replacement. [sent-295, score-0.543]
</p><p>39 2, the Nystr¨ m algorithm also samples columns by the o [k] subspace sampling of Drineas et al. [sent-304, score-0.41]
</p><p>40 Here S ∈ Rm×c is a column selection matrix that si j = 1 if the i-th column of A is the j-th column selected, and D ∈ Rc×c is a diagonal scaling matrix satisfying d j j = √1 i if si j = 1. [sent-308, score-0.263]
</p><p>41 cp Lemma 4 (Subspace Sampling for the Nystr¨ m Approximation) Given an m × m SPSD matrix o A and a target rank k ≪ m, the subspace sampling algorithm selects c = 3200ε−1 k log(16k/δ) columns without replacement and constructs C and W by scaling the selected columns. [sent-309, score-0.506]
</p><p>42 We establish a new error bound for the adaptive sampling algorithm in Section 4. [sent-314, score-0.379]
</p><p>43 We apply adaptive sampling to the CUR and modiﬁed Nystr¨ m problems, o obtaining effective and efﬁcient CUR and Nystr¨ m algorithms in Section 4. [sent-316, score-0.335]
</p><p>44 1 Adaptive Sampling The relative-error adaptive sampling algorithm is originally established in Theorem 2. [sent-327, score-0.335]
</p><p>45 Here we prove a 1 new and more general error bound for the same adaptive sampling algorithm. [sent-332, score-0.379]
</p><p>46 Remark 6 This theorem shows a more general bound for adaptive sampling than the original one in Theorem 2. [sent-345, score-0.371]
</p><p>47 The original one bounds the error incurred by projection onto the column space of C, while Theorem 5 bounds the error incurred by projection onto the column space of C and row space of R simultaneously—such situation rises in problems such as CUR and the Nystr¨ m approximation. [sent-348, score-0.288]
</p><p>48 2, selecting good columns or rows separately does not ensure good columns and rows together for CUR and the Nystr¨ m approximation. [sent-355, score-0.236]
</p><p>49 Based on Corollary 7, we attempt to solve CUR and the Nystr¨ m by adaptive sampling algoo rithms. [sent-364, score-0.335]
</p><p>50 2 Adaptive Sampling for CUR Matrix Decomposition Guaranteed by the novel adaptive sampling bound in Theorem 5, we combine the near-optimal column selection algorithm of Boutsidis et al. [sent-371, score-0.407]
</p><p>51 (2011) and the adaptive sampling algorithm for solving the CUR problem, giving rise to an algorithm with a much tighter theoretical bound than existing algorithms. [sent-372, score-0.352]
</p><p>52 Theorem 8 (Adaptive Sampling for CUR) Given a matrix A ∈ Rm×n and a positive integer k ≪ min{m, n}, the CUR algorithm described in Algorithm 2 randomly selects c = 2k (1+o(1)) columns ε of A to construct C ∈ Rm×c , and then selects r = c (1+ε) rows of A to construct R ∈ Rr×n . [sent-375, score-0.203]
</p><p>53 Neither the nearoptimal column selection algorithm nor the adaptive sampling algorithm requires loading the whole of A into RAM. [sent-380, score-0.39]
</p><p>54 o Remark 9 If we replace the near-optimal column selection algorithm in Theorem 8 by the optimal algorithm of Guruswami and Sinop (2012), it sufﬁces to select c = kε−1 (1 + o(1)) columns and r = cε−1 (1 + ε) rows totally. [sent-384, score-0.191]
</p><p>55 If one uses the optimal o column selection algorithm of Guruswami and Sinop (2012), which is less efﬁcient, the error bound is further improved: only c = εk2 (1 + o(1)) columns are required. [sent-401, score-0.198]
</p><p>56 Let A denote ˜ trix A ∈ R k either the rank-c approximation to A constructed by the standard Nystr¨ m method in (1), or the o approximation constructed by the ensemble Nystr¨ m method in (2) with t non-overlapping samples, o each of which contains c columns of A. [sent-416, score-0.219]
</p><p>57 Then there exists an SPSD matrix such that for any sampling ˜ strategy the approximation errors of the conventional Nystr¨ m methods, that is, A − A ξ , (ξ = 2, o F, or “∗”), are lower bounded by some factors which are shown in Table 3. [sent-417, score-0.279]
</p><p>58 Repeating the sampling procedure for t times and letting X(i) correspond to the error ratio of the i-th sample, we obtain an upper bound on the failure probability: P min{X(i) } > 1 + sε i  = P X(i) > 1 + sε ∀i = 1, · · · ,t  <  1+ε 1 + sε  t  δ,  (4)  which decays exponentially with t. [sent-430, score-0.249]
</p><p>59 2 we conduct empirical comparisons between the standard Nystr¨ m and our modiﬁed Nystr¨ m, and comparisons among three sampling algorithms. [sent-443, score-0.194]
</p><p>60 1 Comparison among the CUR Algorithms In this section we empirically compare our adaptive sampling based CUR algorithm (Algorithm 2) with the subspace sampling algorithm of Drineas et al. [sent-451, score-0.664]
</p><p>61 As for the subspace sampling algorithm, we compute the leverages scores exactly via the truncated SVD. [sent-454, score-0.379]
</p><p>62 , 2012) can signiﬁcantly speedup subspace sampling, we do not use it because the approximation has no theoretical guarantee when applied to subspace sampling. [sent-456, score-0.338]
</p><p>63 We repeat each of the two randomized algorithms 10 times, and report the minimum error ratio and the total elapsed time of the 10 rounds. [sent-538, score-0.192]
</p><p>64 We depict the error ratios and the elapsed time of the three CUR matrix decomposition algorithms in Figures 1, 2, 3, and 4. [sent-539, score-0.237]
</p><p>65 We can see from Figures 1, 2, 3, and 4 that our adaptive sampling based CUR algorithm has much lower approximation error than the subspace sampling algorithm in all cases. [sent-540, score-0.721]
</p><p>66 Our adaptive sampling based algorithm is better than the deterministic SCRA on the Farm Ads data set and the Gisette data set, worse than SCRA on the Enron data set, and comparable to SCRA on the Dexter data set. [sent-541, score-0.358]
</p><p>67 ≤ 1+ A − Ak F c a As for the running time, the subspace sampling algorithm and our adaptive sampling based algorithm are much more efﬁcient than SCRA, especially when c and r are large. [sent-544, score-0.664]
</p><p>68 Our adaptive sampling based algorithm is comparable to the subspace sampling algorithm when c and r are small; however, our algorithm becomes less efﬁcient when c and r are large. [sent-545, score-0.664]
</p><p>69 First, the computational cost of the subspace sampling algorithm is dominated by the truncated SVD of A, which is determined by the target rank k and the size and sparsity of the data matrix. [sent-547, score-0.402]
</p><p>70 The four data sets are all very sparse, so the subspace sampling algorithm has advantages. [sent-551, score-0.329]
</p><p>71 2 Comparison among the Nystr¨ m Algorithms o In this section we empirically compare our adaptive sampling algorithm (in Theorem 10) with some other sampling algorithms including the subspace sampling of Drineas et al. [sent-555, score-0.839]
</p><p>72 We also conduct comparison between the standard Nystr¨ m o and our modiﬁed Nystr¨ m, both use the three sampling algorithms to select columns. [sent-557, score-0.194]
</p><p>73 We run each algorithm for 10 times, and report the the minimum error ratio as well as the total elapsed time of the 10 repeats. [sent-613, score-0.192]
</p><p>74 The standard deviation of the leverage scores reﬂects whether the advanced importance sampling techniques such as the subspace sampling and adaptive sampling are useful. [sent-623, score-0.919]
</p><p>75 Figures 5, 6, and 7 show that the advantage of the subspace sampling and adaptive sampling over the uniform sampling is signiﬁcant whenever the standard deviation of the leverage scores is large (see Table 5), and vise versa. [sent-624, score-0.96]
</p><p>76 The experimental results also show that the subspace sampling and adaptive sampling algorithms signiﬁcantly outperform the uniform sampling when c is reasonably small, say c < 10k. [sent-627, score-0.88]
</p><p>77 This indicates that the subspace sampling and adaptive sampling algorithms are good at choosing “good” columns as basis vectors. [sent-628, score-0.745]
</p><p>78 In most cases our adaptive sampling algorithm achieves the lowest approximation error among the three algorithms. [sent-631, score-0.392]
</p><p>79 a  As for the running time, our adaptive sampling algorithm is more efﬁcient than the subspace sampling algorithm. [sent-782, score-0.664]
</p><p>80 This is partly because the RBF kernel matrix is dense, and hence the subspace sampling algorithm costs O (m2 k) time to compute the truncated SVD. [sent-783, score-0.461]
</p><p>81 Conclusion In this paper we have built a novel and more general relative-error bound for the adaptive sampling algorithm. [sent-789, score-0.352]
</p><p>82 To achieve relative-error bound, the best previous algorithm— the subspace sampling algorithm—requires c = O (kε−2 log k) columns and r = O (cε−2 log c) rows. [sent-792, score-0.41]
</p><p>83 We have shown that our adaptive sampling algorithm for the modiﬁed Nystr¨ m achieves relative-error upper bound by sampling only c = 2kε−2 (1+o(1)) columns, which o even beats the lower error bounds of the standard Nystr¨ m and the ensemble Nystr¨ m. [sent-794, score-0.661]
</p><p>84 Our proposed o o CUR and Nystr¨ m algorithms are scalable because they need only to maintain a small fraction of o columns or rows in RAM, and their time complexities are low provided that matrix multiplication can be highly efﬁciently executed. [sent-795, score-0.19]
</p><p>85 Theorem 15 (The Adaptive Sampling Algorithm) Given a matrix A ∈ Rm×n and a matrix R ∈ Rr×n such that rank(R) = rank(AR† R) = ρ (ρ ≤ r ≤ m), let C1 ∈ Rm×c1 consist of c1 columns of A, and deﬁne the residual B = A − C1 C† A. [sent-831, score-0.195]
</p><p>86 Let C2 ∈ Rm×c2 contain the c2 sampled columns and C = [C1 , C2 ] ∈ Rm×(c1 +c2 ) contain the columns of both C1 and C2 , all of which are columns of A. [sent-837, score-0.243]
</p><p>87 2 The Proof of Corollary 7 Since C is constructed by columns of A and the column space of C is contained in the column space of A, we have rank(CC† A) = rank(C) = ρ ≤ c. [sent-872, score-0.191]
</p><p>88 Then the adaptive sampling algorithm costs O nk2 ε−2 + TMultiply mnkε−1 time to construct R2 . [sent-894, score-0.41]
</p><p>89 The near-optimal column selection algorithm costs O mk2 ε−4/3 + mk3 ε−2/3 + TMultiply m2 kε−2/3 time to select c1 = O (kε−1 ) columns of A construct C1 . [sent-900, score-0.211]
</p><p>90 Then the adaptive sampling algorithm costs O mk2 ε−2 + TMultiply m2 kε−1 time to select c2 = O (kε−2 ) columns construct C2 . [sent-901, score-0.491]
</p><p>91 ˆ Proof Let C consist of c column sampled from A and Ci consist of ci columns sampled from the i-th ˆ block diagonal matrix in A. [sent-983, score-0.219]
</p><p>92 Without loss of generality, we assume Ci consists of the ﬁrst ci columns ˆ ˆ ˆ of B, and accordingly Wi consists of the top left ci × ci block of B. [sent-984, score-0.183]
</p><p>93 T † ˜ ens ˜ ens where Bt,c = 1 ∑ti=1 C(i) W(i) C(i) . [sent-1027, score-0.202]
</p><p>94 2763  WANG AND Z HANG  ens o Figure 8: An illustration of the matrix B − Bt,c for the ensemble Nystr¨ m method where B is deﬁned in (9). [sent-1032, score-0.209]
</p><p>95 For the ensemble Nystr¨ m o ens method without overlapping, the matrix B − Bt,c can always be partitioned into four regions as annotated. [sent-1035, score-0.209]
</p><p>96 We can express it as ˜ ens B − Bt,c = B −  T 1 t (i) (i) † (i) T 1 t ∑ C W C = t ∑ B − C(i) W† C(i) , t i=1 i=1  (18)  ˜ ens and then a discreet examination reveals that B − Bt,c can be partitioned into four kinds of regions as illustrated in Figure 8. [sent-1039, score-0.202]
</p><p>97 Therefore, the nuclear norm ens ) equals to the matrix trace, that is, ˜ of (B − Bt,c ˜ ens B − Bt,c  ∗  ˜ ens = tr B − Bt,c t −1 (1 − η) + (m − tc) · (1 − η) = tc · t 1 c+ α = (1 − α)(m − c) , c + 1−α α  which proves the nuclear norm bound in the lemma. [sent-1045, score-0.539]
</p><p>98 ˆ (i) ˆ ˆ (i) Furthermore, since the matrix B − 1 ∑ti=1 C j W† C j j t ˜ ens diagonal matrix (A − At,c ) is also SPSD. [sent-1058, score-0.199]
</p><p>99 Thus we have ˜ ens A − At,c  ∗  = (1 − α) ∑ (p − ci ) i=1  1 ci + α  ci +  1−α α  T  2  ,  is SPSD by Lemma 23, so the block  ≥ (1 − α)(m − c) 1 +  k c + 1−α k α  ,  which proves the nuclear norm bound in the theorem. [sent-1059, score-0.281]
</p><p>100 Then there exists an m×m SPSD matrix A such that the relative-error ratio of the ensemble Nystr¨ m method is lower bounded o by ˜ ens A − At,c F A − Ak F ˜ ens A − At,c ∗ A − Ak ∗ †  ≥ ≥ T  m − 2c + c/t − k k(m − 2c + c/t) 1+ , m−k c2  k m−c 1+ , m−k c  ˜ ens where At,c = 1 ∑ti=1 C(i) W(i) C(i) . [sent-1061, score-0.441]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cur', 0.501), ('nystr', 0.487), ('modified', 0.417), ('sampling', 0.175), ('ar', 0.164), ('adaptive', 0.16), ('subspace', 0.154), ('cc', 0.121), ('tmultiply', 0.113), ('elapsed', 0.112), ('ens', 0.101), ('drineas', 0.093), ('spsd', 0.093), ('ak', 0.091), ('ecomposition', 0.09), ('om', 0.09), ('ystr', 0.09), ('near', 0.088), ('columns', 0.081), ('mproving', 0.077), ('ct', 0.068), ('bnys', 0.068), ('mnk', 0.068), ('pproximation', 0.067), ('atrix', 0.06), ('ensemble', 0.059), ('deshpande', 0.058), ('mahoney', 0.058), ('column', 0.055), ('svd', 0.054), ('cw', 0.052), ('boutsidis', 0.05), ('matrix', 0.049), ('tc', 0.048), ('scra', 0.045), ('pc', 0.044), ('hang', 0.044), ('uniform', 0.041), ('rows', 0.037), ('leverage', 0.037), ('nuclear', 0.037), ('rm', 0.035), ('qr', 0.035), ('nys', 0.035), ('costs', 0.034), ('ci', 0.034), ('incurred', 0.033), ('vt', 0.031), ('bv', 0.031), ('approximation', 0.03), ('ratio', 0.03), ('bounds', 0.029), ('rank', 0.029), ('sparsi', 0.028), ('singular', 0.028), ('frobenius', 0.028), ('cj', 0.027), ('guruswami', 0.027), ('error', 0.027), ('truncated', 0.026), ('decomposition', 0.026), ('conventional', 0.025), ('rbf', 0.024), ('intersection', 0.024), ('span', 0.024), ('scores', 0.024), ('norm', 0.024), ('time', 0.023), ('deterministic', 0.023), ('anys', 0.023), ('sinop', 0.023), ('wang', 0.022), ('av', 0.022), ('ck', 0.021), ('im', 0.02), ('talwalkar', 0.019), ('theorem', 0.019), ('standard', 0.019), ('kumar', 0.019), ('optimal', 0.018), ('construct', 0.018), ('cuct', 0.018), ('pivoted', 0.018), ('target', 0.018), ('abalone', 0.017), ('rv', 0.017), ('apc', 0.017), ('pi', 0.017), ('bc', 0.017), ('bound', 0.017), ('wk', 0.016), ('rr', 0.016), ('stewart', 0.016), ('residual', 0.016), ('ops', 0.015), ('gittens', 0.015), ('blkdiag', 0.015), ('avr', 0.015), ('lemma', 0.015), ('rt', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="53-tfidf-1" href="./jmlr-2013-Improving_CUR_Matrix_Decomposition_and_the_Nystrom_Approximation_via_Adaptive_Sampling.html">53 jmlr-2013-Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling</a></p>
<p>Author: Shusen Wang, Zhihua Zhang</p><p>Abstract: The CUR matrix decomposition and the Nystr¨ m approximation are two important low-rank matrix o approximation techniques. The Nystr¨ m method approximates a symmetric positive semideﬁnite o matrix in terms of a small number of its columns, while CUR approximates an arbitrary data matrix by a small number of its columns and rows. Thus, CUR decomposition can be regarded as an extension of the Nystr¨ m approximation. o In this paper we establish a more general error bound for the adaptive column/row sampling algorithm, based on which we propose more accurate CUR and Nystr¨ m algorithms with expected o relative-error bounds. The proposed CUR and Nystr¨ m algorithms also have low time complexity o and can avoid maintaining the whole data matrix in RAM. In addition, we give theoretical analysis for the lower error bounds of the standard Nystr¨ m method and the ensemble Nystr¨ m method. o o The main theoretical results established in this paper are novel, and our analysis makes no special assumption on the data matrices. Keywords: large-scale matrix computation, CUR matrix decomposition, the Nystr¨ m method, o randomized algorithms, adaptive sampling</p><p>2 0.40476725 <a title="53-tfidf-2" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>Author: Ameet Talwalkar, Sanjiv Kumar, Mehryar Mohri, Henry Rowley</p><p>Abstract: This paper examines the efﬁcacy of sampling-based low-rank approximation techniques when applied to large dense kernel matrices. We analyze two common approximate singular value decomposition techniques, namely the Nystr¨ m and Column sampling methods. We present a theoretical o comparison between these two methods, provide novel insights regarding their suitability for various tasks and present experimental results that support our theory. Our results illustrate the relative strengths of each method. We next examine the performance of these two techniques on the largescale task of extracting low-dimensional manifold structure given millions of high-dimensional face images. We address the computational challenges of non-linear dimensionality reduction via Isomap and Laplacian Eigenmaps, using a graph containing about 18 million nodes and 65 million edges. We present extensive experiments on learning low-dimensional embeddings for two large face data sets: CMU-PIE (35 thousand faces) and a web data set (18 million faces). Our comparisons show that the Nystr¨ m approximation is superior to the Column sampling method for this o task. Furthermore, approximate Isomap tends to perform better than Laplacian Eigenmaps on both clustering and classiﬁcation with the labeled CMU-PIE data set. Keywords: low-rank approximation, manifold learning, large-scale matrix factorization</p><p>3 0.061525665 <a title="53-tfidf-3" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>Author: Tony Cai, Wen-Xin Zhou</p><p>Abstract: We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed. Keywords: 1-bit matrix completion, low-rank matrix, max-norm, trace-norm, constrained optimization, maximum likelihood estimate, optimal rate of convergence</p><p>4 0.06004997 <a title="53-tfidf-4" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>Author: Eva L. Dyer, Aswin C. Sankaranarayanan, Richard G. Baraniuk</p><p>Abstract: Unions of subspaces provide a powerful generalization of single subspace models for collections of high-dimensional data; however, learning multiple subspaces from data is challenging due to the fact that segmentation—the identiﬁcation of points that live in the same subspace—and subspace estimation must be performed simultaneously. Recently, sparse recovery methods were shown to provide a provable and robust strategy for exact feature selection (EFS)—recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with ℓ1 -minimization, in this paper, we develop sufﬁcient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide signiﬁcant advantages over NN methods and that the gap between the two approaches is particularly pronounced when the sampling of subspaces in the data set is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble. Keywords: subspace clustering, unions of subspaces, hybrid linear models, sparse approximation, structured sparsity, nearest neighbors, low-rank approximation</p><p>5 0.03800888 <a title="53-tfidf-5" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>Author: Nima Noorshams, Martin J. Wainwright</p><p>Abstract: The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP ﬁxed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefﬁcients via Monte Carlo techniques and damped updates. We prove that the SOSMP iterates converge to a δ-neighborhood of the unique BP ﬁxed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefﬁcients as a function of the desired approximation accuracy δ and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical ﬂow estimation. Keywords: graphical models, sum-product for continuous state spaces, low-complexity belief propagation, stochastic approximation, Monte Carlo methods, orthogonal basis expansion</p><p>6 0.037900161 <a title="53-tfidf-6" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>7 0.031978197 <a title="53-tfidf-7" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>8 0.031297613 <a title="53-tfidf-8" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>9 0.030863581 <a title="53-tfidf-9" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>10 0.030712619 <a title="53-tfidf-10" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>11 0.030659508 <a title="53-tfidf-11" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>12 0.03020256 <a title="53-tfidf-12" href="./jmlr-2013-A_Theory_of_Multiclass_Boosting.html">8 jmlr-2013-A Theory of Multiclass Boosting</a></p>
<p>13 0.02800444 <a title="53-tfidf-13" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<p>14 0.026601091 <a title="53-tfidf-14" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>15 0.025999237 <a title="53-tfidf-15" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<p>16 0.025777778 <a title="53-tfidf-16" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>17 0.02254216 <a title="53-tfidf-17" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>18 0.021059969 <a title="53-tfidf-18" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>19 0.020991957 <a title="53-tfidf-19" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>20 0.020773007 <a title="53-tfidf-20" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.147), (1, 0.106), (2, 0.033), (3, -0.229), (4, -0.033), (5, 0.063), (6, -0.072), (7, -0.173), (8, 0.233), (9, 0.413), (10, 0.511), (11, 0.042), (12, 0.223), (13, -0.042), (14, -0.099), (15, 0.083), (16, -0.009), (17, 0.075), (18, 0.109), (19, -0.025), (20, -0.071), (21, -0.058), (22, -0.003), (23, 0.014), (24, 0.031), (25, 0.018), (26, -0.001), (27, -0.005), (28, -0.042), (29, -0.006), (30, -0.063), (31, -0.025), (32, 0.001), (33, 0.025), (34, -0.024), (35, 0.018), (36, -0.022), (37, -0.017), (38, 0.017), (39, 0.03), (40, -0.007), (41, -0.025), (42, 0.007), (43, -0.011), (44, -0.008), (45, -0.013), (46, 0.023), (47, -0.016), (48, 0.022), (49, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95983011 <a title="53-lsi-1" href="./jmlr-2013-Improving_CUR_Matrix_Decomposition_and_the_Nystrom_Approximation_via_Adaptive_Sampling.html">53 jmlr-2013-Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling</a></p>
<p>Author: Shusen Wang, Zhihua Zhang</p><p>Abstract: The CUR matrix decomposition and the Nystr¨ m approximation are two important low-rank matrix o approximation techniques. The Nystr¨ m method approximates a symmetric positive semideﬁnite o matrix in terms of a small number of its columns, while CUR approximates an arbitrary data matrix by a small number of its columns and rows. Thus, CUR decomposition can be regarded as an extension of the Nystr¨ m approximation. o In this paper we establish a more general error bound for the adaptive column/row sampling algorithm, based on which we propose more accurate CUR and Nystr¨ m algorithms with expected o relative-error bounds. The proposed CUR and Nystr¨ m algorithms also have low time complexity o and can avoid maintaining the whole data matrix in RAM. In addition, we give theoretical analysis for the lower error bounds of the standard Nystr¨ m method and the ensemble Nystr¨ m method. o o The main theoretical results established in this paper are novel, and our analysis makes no special assumption on the data matrices. Keywords: large-scale matrix computation, CUR matrix decomposition, the Nystr¨ m method, o randomized algorithms, adaptive sampling</p><p>2 0.86996657 <a title="53-lsi-2" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>Author: Ameet Talwalkar, Sanjiv Kumar, Mehryar Mohri, Henry Rowley</p><p>Abstract: This paper examines the efﬁcacy of sampling-based low-rank approximation techniques when applied to large dense kernel matrices. We analyze two common approximate singular value decomposition techniques, namely the Nystr¨ m and Column sampling methods. We present a theoretical o comparison between these two methods, provide novel insights regarding their suitability for various tasks and present experimental results that support our theory. Our results illustrate the relative strengths of each method. We next examine the performance of these two techniques on the largescale task of extracting low-dimensional manifold structure given millions of high-dimensional face images. We address the computational challenges of non-linear dimensionality reduction via Isomap and Laplacian Eigenmaps, using a graph containing about 18 million nodes and 65 million edges. We present extensive experiments on learning low-dimensional embeddings for two large face data sets: CMU-PIE (35 thousand faces) and a web data set (18 million faces). Our comparisons show that the Nystr¨ m approximation is superior to the Column sampling method for this o task. Furthermore, approximate Isomap tends to perform better than Laplacian Eigenmaps on both clustering and classiﬁcation with the labeled CMU-PIE data set. Keywords: low-rank approximation, manifold learning, large-scale matrix factorization</p><p>3 0.27908397 <a title="53-lsi-3" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>Author: Tony Cai, Wen-Xin Zhou</p><p>Abstract: We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed. Keywords: 1-bit matrix completion, low-rank matrix, max-norm, trace-norm, constrained optimization, maximum likelihood estimate, optimal rate of convergence</p><p>4 0.20827514 <a title="53-lsi-4" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>Author: Eva L. Dyer, Aswin C. Sankaranarayanan, Richard G. Baraniuk</p><p>Abstract: Unions of subspaces provide a powerful generalization of single subspace models for collections of high-dimensional data; however, learning multiple subspaces from data is challenging due to the fact that segmentation—the identiﬁcation of points that live in the same subspace—and subspace estimation must be performed simultaneously. Recently, sparse recovery methods were shown to provide a provable and robust strategy for exact feature selection (EFS)—recovering subsets of points from the ensemble that live in the same subspace. In parallel with recent studies of EFS with ℓ1 -minimization, in this paper, we develop sufﬁcient conditions for EFS with a greedy method for sparse signal recovery known as orthogonal matching pursuit (OMP). Following our analysis, we provide an empirical study of feature selection strategies for signals living on unions of subspaces and characterize the gap between sparse recovery methods and nearest neighbor (NN)-based approaches. In particular, we demonstrate that sparse recovery methods provide signiﬁcant advantages over NN methods and that the gap between the two approaches is particularly pronounced when the sampling of subspaces in the data set is sparse. Our results suggest that OMP may be employed to reliably recover exact feature sets in a number of regimes where NN approaches fail to reveal the subspace membership of points in the ensemble. Keywords: subspace clustering, unions of subspaces, hybrid linear models, sparse approximation, structured sparsity, nearest neighbors, low-rank approximation</p><p>5 0.1529886 <a title="53-lsi-5" href="./jmlr-2013-Similarity-based_Clustering_by_Left-Stochastic_Matrix_Factorization.html">100 jmlr-2013-Similarity-based Clustering by Left-Stochastic Matrix Factorization</a></p>
<p>Author: Raman Arora, Maya R. Gupta, Amol Kapila, Maryam Fazel</p><p>Abstract: For similarity-based clustering, we propose modeling the entries of a given similarity matrix as the inner products of the unknown cluster probabilities. To estimate the cluster probabilities from the given similarity matrix, we introduce a left-stochastic non-negative matrix factorization problem. A rotation-based algorithm is proposed for the matrix factorization. Conditions for unique matrix factorizations and clusterings are given, and an error bound is provided. The algorithm is particularly efﬁcient for the case of two clusters, which motivates a hierarchical variant for cases where the number of desired clusters is large. Experiments show that the proposed left-stochastic decomposition clustering model produces relatively high within-cluster similarity on most data sets and can match given class labels, and that the efﬁcient hierarchical variant performs surprisingly well. Keywords: clustering, non-negative matrix factorization, rotation, indeﬁnite kernel, similarity, completely positive</p><p>6 0.15140536 <a title="53-lsi-6" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>7 0.14633353 <a title="53-lsi-7" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>8 0.14565216 <a title="53-lsi-8" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>9 0.12832524 <a title="53-lsi-9" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>10 0.12516725 <a title="53-lsi-10" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>11 0.1235923 <a title="53-lsi-11" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>12 0.11873133 <a title="53-lsi-12" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<p>13 0.11482601 <a title="53-lsi-13" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>14 0.11299884 <a title="53-lsi-14" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>15 0.11056688 <a title="53-lsi-15" href="./jmlr-2013-Fast_MCMC_Sampling_for_Markov_Jump_Processes_and_Extensions.html">43 jmlr-2013-Fast MCMC Sampling for Markov Jump Processes and Extensions</a></p>
<p>16 0.10891166 <a title="53-lsi-16" href="./jmlr-2013-Approximating_the_Permanent_with_Fractional_Belief_Propagation.html">13 jmlr-2013-Approximating the Permanent with Fractional Belief Propagation</a></p>
<p>17 0.10580428 <a title="53-lsi-17" href="./jmlr-2013-Truncated_Power_Method_for_Sparse_Eigenvalue_Problems.html">116 jmlr-2013-Truncated Power Method for Sparse Eigenvalue Problems</a></p>
<p>18 0.098798677 <a title="53-lsi-18" href="./jmlr-2013-A_Theory_of_Multiclass_Boosting.html">8 jmlr-2013-A Theory of Multiclass Boosting</a></p>
<p>19 0.094300658 <a title="53-lsi-19" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>20 0.094219156 <a title="53-lsi-20" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.027), (5, 0.131), (6, 0.038), (10, 0.071), (23, 0.021), (34, 0.415), (67, 0.025), (68, 0.016), (70, 0.024), (75, 0.024), (85, 0.02), (87, 0.045), (89, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.65110123 <a title="53-lda-1" href="./jmlr-2013-Improving_CUR_Matrix_Decomposition_and_the_Nystrom_Approximation_via_Adaptive_Sampling.html">53 jmlr-2013-Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling</a></p>
<p>Author: Shusen Wang, Zhihua Zhang</p><p>Abstract: The CUR matrix decomposition and the Nystr¨ m approximation are two important low-rank matrix o approximation techniques. The Nystr¨ m method approximates a symmetric positive semideﬁnite o matrix in terms of a small number of its columns, while CUR approximates an arbitrary data matrix by a small number of its columns and rows. Thus, CUR decomposition can be regarded as an extension of the Nystr¨ m approximation. o In this paper we establish a more general error bound for the adaptive column/row sampling algorithm, based on which we propose more accurate CUR and Nystr¨ m algorithms with expected o relative-error bounds. The proposed CUR and Nystr¨ m algorithms also have low time complexity o and can avoid maintaining the whole data matrix in RAM. In addition, we give theoretical analysis for the lower error bounds of the standard Nystr¨ m method and the ensemble Nystr¨ m method. o o The main theoretical results established in this paper are novel, and our analysis makes no special assumption on the data matrices. Keywords: large-scale matrix computation, CUR matrix decomposition, the Nystr¨ m method, o randomized algorithms, adaptive sampling</p><p>2 0.5736903 <a title="53-lda-2" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>Author: Chong Zhang, Yufeng Liu</p><p>Abstract: Hard and soft classiﬁers are two important groups of techniques for classiﬁcation problems. Logistic regression and Support Vector Machines are typical examples of soft and hard classiﬁers respectively. The essential difference between these two groups is whether one needs to estimate the class conditional probability for the classiﬁcation task or not. In particular, soft classiﬁers predict the label based on the obtained class conditional probabilities, while hard classiﬁers bypass the estimation of probabilities and focus on the decision boundary. In practice, for the goal of accurate classiﬁcation, it is unclear which one to use in a given situation. To tackle this problem, the Largemargin Uniﬁed Machine (LUM) was recently proposed as a uniﬁed family to embrace both groups. The LUM family enables one to study the behavior change from soft to hard binary classiﬁers. For multicategory cases, however, the concept of soft and hard classiﬁcation becomes less clear. In that case, class probability estimation becomes more involved as it requires estimation of a probability vector. In this paper, we propose a new Multicategory LUM (MLUM) framework to investigate the behavior of soft versus hard classiﬁcation under multicategory settings. Our theoretical and numerical results help to shed some light on the nature of multicategory classiﬁcation and its transition behavior from soft to hard classiﬁers. The numerical results suggest that the proposed tuned MLUM yields very competitive performance. Keywords: hard classiﬁcation, large-margin, soft classiﬁcation, support vector machine</p><p>3 0.36769724 <a title="53-lda-3" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>Author: Ameet Talwalkar, Sanjiv Kumar, Mehryar Mohri, Henry Rowley</p><p>Abstract: This paper examines the efﬁcacy of sampling-based low-rank approximation techniques when applied to large dense kernel matrices. We analyze two common approximate singular value decomposition techniques, namely the Nystr¨ m and Column sampling methods. We present a theoretical o comparison between these two methods, provide novel insights regarding their suitability for various tasks and present experimental results that support our theory. Our results illustrate the relative strengths of each method. We next examine the performance of these two techniques on the largescale task of extracting low-dimensional manifold structure given millions of high-dimensional face images. We address the computational challenges of non-linear dimensionality reduction via Isomap and Laplacian Eigenmaps, using a graph containing about 18 million nodes and 65 million edges. We present extensive experiments on learning low-dimensional embeddings for two large face data sets: CMU-PIE (35 thousand faces) and a web data set (18 million faces). Our comparisons show that the Nystr¨ m approximation is superior to the Column sampling method for this o task. Furthermore, approximate Isomap tends to perform better than Laplacian Eigenmaps on both clustering and classiﬁcation with the labeled CMU-PIE data set. Keywords: low-rank approximation, manifold learning, large-scale matrix factorization</p><p>4 0.36059776 <a title="53-lda-4" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>Author: Yuchen Zhang, John C. Duchi, Martin J. Wainwright</p><p>Abstract: We analyze two communication-efﬁcient algorithms for distributed optimization in statistical settings involving large-scale data sets. The ﬁrst algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves √ mean-squared error (MSE) that decays as O (N −1 + (N/m)−2 ). Whenever m ≤ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O (N −1 + (N/m)−3 ), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O (N −1 + (N/m)−3/2 ), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efﬁciently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 108 samples and d ≈ 740,000 covariates. Keywords: distributed learning, stochastic optimization, averaging, subsampling</p><p>5 0.3553623 <a title="53-lda-5" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>Author: Tony Cai, Wen-Xin Zhou</p><p>Abstract: We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed. Keywords: 1-bit matrix completion, low-rank matrix, max-norm, trace-norm, constrained optimization, maximum likelihood estimate, optimal rate of convergence</p><p>6 0.35453182 <a title="53-lda-6" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>7 0.35373864 <a title="53-lda-7" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>8 0.35182342 <a title="53-lda-8" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>9 0.35134998 <a title="53-lda-9" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>10 0.35019368 <a title="53-lda-10" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>11 0.35010305 <a title="53-lda-11" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>12 0.34872395 <a title="53-lda-12" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>13 0.34861237 <a title="53-lda-13" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>14 0.34758419 <a title="53-lda-14" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>15 0.34695584 <a title="53-lda-15" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>16 0.34649178 <a title="53-lda-16" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>17 0.34575644 <a title="53-lda-17" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>18 0.3457011 <a title="53-lda-18" href="./jmlr-2013-GURLS%3A_A_Least_Squares_Library_for_Supervised_Learning.html">46 jmlr-2013-GURLS: A Least Squares Library for Supervised Learning</a></p>
<p>19 0.34458673 <a title="53-lda-19" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>20 0.3445583 <a title="53-lda-20" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
