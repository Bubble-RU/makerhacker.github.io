<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 jmlr-2013-Divvy: Fast and Intuitive Exploratory Data Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-37" href="#">jmlr2013-37</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 jmlr-2013-Divvy: Fast and Intuitive Exploratory Data Analysis</h1>
<br/><p>Source: <a title="jmlr-2013-37-pdf" href="http://jmlr.org/papers/volume14/lewis13a/lewis13a.pdf">pdf</a></p><p>Author: Joshua M. Lewis, Virginia R. de Sa, Laurens van der Maaten</p><p>Abstract: Divvy is an application for applying unsupervised machine learning techniques (clustering and dimensionality reduction) to the data analysis process. Divvy provides a novel UI that allows researchers to tighten the action-perception loop of changing algorithm parameters and seeing a visualization of the result. Machine learning researchers can use Divvy to publish easy to use reference implementations of their algorithms, which helps the machine learning ﬁeld have a greater impact on research practices elsewhere. Keywords: clustering, dimensionality reduction, open source software, human computer interaction, data visualization</p><p>Reference: <a title="jmlr-2013-37-reference" href="../jmlr2013_reference/jmlr-2013-Divvy%3A_Fast_and_Intuitive_Exploratory_Data_Analysis_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('divvy', 0.876), ('plugin', 0.304), ('basebal', 0.142), ('joshu', 0.073), ('ayasd', 0.071), ('isomap', 0.064), ('platform', 0.062), ('interfac', 0.061), ('ui', 0.059), ('vis', 0.056), ('rend', 0.055), ('clust', 0.055), ('maat', 0.055), ('threads', 0.055), ('laur', 0.051), ('cogsc', 0.047), ('csv', 0.047), ('delft', 0.047), ('ggob', 0.047), ('ivvy', 0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="37-tfidf-1" href="./jmlr-2013-Divvy%3A_Fast_and_Intuitive_Exploratory_Data_Analysis.html">37 jmlr-2013-Divvy: Fast and Intuitive Exploratory Data Analysis</a></p>
<p>Author: Joshua M. Lewis, Virginia R. de Sa, Laurens van der Maaten</p><p>Abstract: Divvy is an application for applying unsupervised machine learning techniques (clustering and dimensionality reduction) to the data analysis process. Divvy provides a novel UI that allows researchers to tighten the action-perception loop of changing algorithm parameters and seeing a visualization of the result. Machine learning researchers can use Divvy to publish easy to use reference implementations of their algorithms, which helps the machine learning ﬁeld have a greater impact on research practices elsewhere. Keywords: clustering, dimensionality reduction, open source software, human computer interaction, data visualization</p><p>2 0.036603007 <a title="37-tfidf-2" href="./jmlr-2013-Tapkee%3A_An_Efficient_Dimension_Reduction_Library.html">112 jmlr-2013-Tapkee: An Efficient Dimension Reduction Library</a></p>
<p>Author: Sergey Lisitsyn, Christian Widmer, Fernando J. Iglesias Garcia</p><p>Abstract: We present Tapkee, a C++ template library that provides efﬁcient implementations of more than 20 widely used dimensionality reduction techniques ranging from Locally Linear Embedding (Roweis and Saul, 2000) and Isomap (de Silva and Tenenbaum, 2002) to the recently introduced BarnesHut-SNE (van der Maaten, 2013). Our library was designed with a focus on performance and ﬂexibility. For performance, we combine efﬁcient multi-core algorithms, modern data structures and state-of-the-art low-level libraries. To achieve ﬂexibility, we designed a clean interface for applying methods to user data and provide a callback API that facilitates integration with the library. The library is freely available as open-source software and is distributed under the permissive BSD 3-clause license. We encourage the integration of Tapkee into other open-source toolboxes and libraries. For example, Tapkee has been integrated into the codebase of the Shogun toolbox (Sonnenburg et al., 2010), giving us access to a rich set of kernels, distance measures and bindings to common programming languages including Python, Octave, Matlab, R, Java, C#, Ruby, Perl and Lua. Source code, examples and documentation are available at http://tapkee.lisitsyn.me. Keywords: dimensionality reduction, machine learning, C++, open source software</p><p>3 0.029218905 <a title="37-tfidf-3" href="./jmlr-2013-Orange%3A_Data_Mining_Toolbox_in_Python.html">83 jmlr-2013-Orange: Data Mining Toolbox in Python</a></p>
<p>Author: Janez Demšar, Tomaž Curk, Aleš Erjavec, Črt Gorup, Tomaž Hočevar, Mitar Milutinovič, Martin Možina, Matija Polajnar, Marko Toplak, Anže Starič, Miha Štajdohar, Lan Umek, Lan Žagar, Jure Žbontar, Marinka Žitnik, Blaž Zupan</p><p>Abstract: Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the ﬂexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationallyintensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining. Keywords: Python, data mining, machine learning, toolbox, scripting</p><p>4 0.029105611 <a title="37-tfidf-4" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>Author: Ameet Talwalkar, Sanjiv Kumar, Mehryar Mohri, Henry Rowley</p><p>Abstract: This paper examines the efﬁcacy of sampling-based low-rank approximation techniques when applied to large dense kernel matrices. We analyze two common approximate singular value decomposition techniques, namely the Nystr¨ m and Column sampling methods. We present a theoretical o comparison between these two methods, provide novel insights regarding their suitability for various tasks and present experimental results that support our theory. Our results illustrate the relative strengths of each method. We next examine the performance of these two techniques on the largescale task of extracting low-dimensional manifold structure given millions of high-dimensional face images. We address the computational challenges of non-linear dimensionality reduction via Isomap and Laplacian Eigenmaps, using a graph containing about 18 million nodes and 65 million edges. We present extensive experiments on learning low-dimensional embeddings for two large face data sets: CMU-PIE (35 thousand faces) and a web data set (18 million faces). Our comparisons show that the Nystr¨ m approximation is superior to the Column sampling method for this o task. Furthermore, approximate Isomap tends to perform better than Laplacian Eigenmaps on both clustering and classiﬁcation with the labeled CMU-PIE data set. Keywords: low-rank approximation, manifold learning, large-scale matrix factorization</p><p>5 0.02900601 <a title="37-tfidf-5" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>Author: Binbin Lin, Xiaofei He, Chiyuan Zhang, Ming Ji</p><p>Abstract: We propose a novel local isometry based dimensionality reduction method from the perspective of vector ﬁelds, which is called parallel vector ﬁeld embedding (PFE). We ﬁrst give a discussion on local isometry and global isometry to show the intrinsic connection between parallel vector ﬁelds and isometry. The problem of ﬁnding an isometry turns out to be equivalent to ﬁnding orthonormal parallel vector ﬁelds on the data manifold. Therefore, we ﬁrst ﬁnd orthonormal parallel vector ﬁelds by solving a variational problem on the manifold. Then each embedding function can be obtained by requiring its gradient ﬁeld to be as close to the corresponding parallel vector ﬁeld as possible. Theoretical results show that our method can precisely recover the manifold if it is isometric to a connected open subset of Euclidean space. Both synthetic and real data examples demonstrate the effectiveness of our method even if there is heavy noise and high curvature. Keywords: manifold learning, isometry, vector ﬁeld, covariant derivative, out-of-sample extension</p><p>6 0.023143988 <a title="37-tfidf-6" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>7 0.018974546 <a title="37-tfidf-7" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>8 0.018339183 <a title="37-tfidf-8" href="./jmlr-2013-The_CAM_Software_for_Nonnegative_Blind_Source_Separation_in_R-Java.html">113 jmlr-2013-The CAM Software for Nonnegative Blind Source Separation in R-Java</a></p>
<p>9 0.017584015 <a title="37-tfidf-9" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>10 0.015932057 <a title="37-tfidf-10" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>11 0.014232219 <a title="37-tfidf-11" href="./jmlr-2013-On_the_Learnability_of_Shuffle_Ideals.html">78 jmlr-2013-On the Learnability of Shuffle Ideals</a></p>
<p>12 0.014124991 <a title="37-tfidf-12" href="./jmlr-2013-Language-Motivated_Approaches_to_Action_Recognition.html">58 jmlr-2013-Language-Motivated Approaches to Action Recognition</a></p>
<p>13 0.01393615 <a title="37-tfidf-13" href="./jmlr-2013-Keep_It_Simple_And_Sparse%3A_Real-Time_Action_Recognition.html">56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</a></p>
<p>14 0.013382339 <a title="37-tfidf-14" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>15 0.01311139 <a title="37-tfidf-15" href="./jmlr-2013-Similarity-based_Clustering_by_Left-Stochastic_Matrix_Factorization.html">100 jmlr-2013-Similarity-based Clustering by Left-Stochastic Matrix Factorization</a></p>
<p>16 0.012084444 <a title="37-tfidf-16" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>17 0.011151665 <a title="37-tfidf-17" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<p>18 0.010710294 <a title="37-tfidf-18" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>19 0.010572936 <a title="37-tfidf-19" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>20 0.010365671 <a title="37-tfidf-20" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.052), (1, -0.017), (2, -0.046), (3, -0.02), (4, 0.009), (5, -0.021), (6, 0.028), (7, -0.008), (8, 0.024), (9, 0.056), (10, -0.048), (11, 0.031), (12, 0.018), (13, 0.032), (14, -0.042), (15, 0.026), (16, -0.004), (17, -0.082), (18, -0.098), (19, -0.06), (20, -0.113), (21, 0.024), (22, -0.033), (23, -0.023), (24, -0.012), (25, 0.003), (26, 0.008), (27, 0.023), (28, 0.022), (29, 0.043), (30, -0.019), (31, -0.027), (32, 0.058), (33, 0.017), (34, -0.021), (35, -0.051), (36, -0.139), (37, 0.023), (38, -0.0), (39, 0.145), (40, 0.266), (41, 0.216), (42, 0.1), (43, -0.338), (44, -0.122), (45, -0.328), (46, 0.097), (47, -0.134), (48, 0.096), (49, 0.123)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95539123 <a title="37-lsi-1" href="./jmlr-2013-Divvy%3A_Fast_and_Intuitive_Exploratory_Data_Analysis.html">37 jmlr-2013-Divvy: Fast and Intuitive Exploratory Data Analysis</a></p>
<p>Author: Joshua M. Lewis, Virginia R. de Sa, Laurens van der Maaten</p><p>Abstract: Divvy is an application for applying unsupervised machine learning techniques (clustering and dimensionality reduction) to the data analysis process. Divvy provides a novel UI that allows researchers to tighten the action-perception loop of changing algorithm parameters and seeing a visualization of the result. Machine learning researchers can use Divvy to publish easy to use reference implementations of their algorithms, which helps the machine learning ﬁeld have a greater impact on research practices elsewhere. Keywords: clustering, dimensionality reduction, open source software, human computer interaction, data visualization</p><p>2 0.56228787 <a title="37-lsi-2" href="./jmlr-2013-Orange%3A_Data_Mining_Toolbox_in_Python.html">83 jmlr-2013-Orange: Data Mining Toolbox in Python</a></p>
<p>Author: Janez Demšar, Tomaž Curk, Aleš Erjavec, Črt Gorup, Tomaž Hočevar, Mitar Milutinovič, Martin Možina, Matija Polajnar, Marko Toplak, Anže Starič, Miha Štajdohar, Lan Umek, Lan Žagar, Jure Žbontar, Marinka Žitnik, Blaž Zupan</p><p>Abstract: Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the ﬂexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationallyintensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining. Keywords: Python, data mining, machine learning, toolbox, scripting</p><p>3 0.26288885 <a title="37-lsi-3" href="./jmlr-2013-Tapkee%3A_An_Efficient_Dimension_Reduction_Library.html">112 jmlr-2013-Tapkee: An Efficient Dimension Reduction Library</a></p>
<p>Author: Sergey Lisitsyn, Christian Widmer, Fernando J. Iglesias Garcia</p><p>Abstract: We present Tapkee, a C++ template library that provides efﬁcient implementations of more than 20 widely used dimensionality reduction techniques ranging from Locally Linear Embedding (Roweis and Saul, 2000) and Isomap (de Silva and Tenenbaum, 2002) to the recently introduced BarnesHut-SNE (van der Maaten, 2013). Our library was designed with a focus on performance and ﬂexibility. For performance, we combine efﬁcient multi-core algorithms, modern data structures and state-of-the-art low-level libraries. To achieve ﬂexibility, we designed a clean interface for applying methods to user data and provide a callback API that facilitates integration with the library. The library is freely available as open-source software and is distributed under the permissive BSD 3-clause license. We encourage the integration of Tapkee into other open-source toolboxes and libraries. For example, Tapkee has been integrated into the codebase of the Shogun toolbox (Sonnenburg et al., 2010), giving us access to a rich set of kernels, distance measures and bindings to common programming languages including Python, Octave, Matlab, R, Java, C#, Ruby, Perl and Lua. Source code, examples and documentation are available at http://tapkee.lisitsyn.me. Keywords: dimensionality reduction, machine learning, C++, open source software</p><p>4 0.22261681 <a title="37-lsi-4" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>Author: Anastasios Roussos, Stavros Theodorakis, Vassilis Pitsikalis, Petros Maragos</p><p>Abstract: We propose the novel approach of dynamic afﬁne-invariant shape-appearance model (Aff-SAM) and employ it for handshape classiﬁcation and sign recognition in sign language (SL) videos. AffSAM offers a compact and descriptive representation of hand conﬁgurations as well as regularized model-ﬁtting, assisting hand tracking and extracting handshape features. We construct SA images representing the hand’s shape and appearance without landmark points. We model the variation of the images by linear combinations of eigenimages followed by afﬁne transformations, accounting for 3D hand pose changes and improving model’s compactness. We also incorporate static and dynamic handshape priors, offering robustness in occlusions, which occur often in signing. The approach includes an afﬁne signer adaptation component at the visual level, without requiring training from scratch a new singer-speciﬁc model. We rather employ a short development data set to adapt the models for a new signer. Experiments on the Boston-University-400 continuous SL corpus demonstrate improvements on handshape classiﬁcation when compared to other feature extraction approaches. Supplementary evaluations of sign recognition experiments, are conducted on a multi-signer, 100-sign data set, from the Greek sign language lemmas corpus. These explore the fusion with movement cues as well as signer adaptation of Aff-SAM to multiple signers providing promising results. Keywords: afﬁne-invariant shape-appearance model, landmarks-free shape representation, static and dynamic priors, feature extraction, handshape classiﬁcation</p><p>5 0.19935903 <a title="37-lsi-5" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>Author: Lisha Chen, Andreas Buja</p><p>Abstract: Multidimensional scaling (MDS) is the art of reconstructing pointsets (embeddings) from pairwise distance data, and as such it is at the basis of several approaches to nonlinear dimension reduction and manifold learning. At present, MDS lacks a unifying methodology as it consists of a discrete collection of proposals that differ in their optimization criteria, called “stress functions”. To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. This methodology yields the following beneﬁts and problem solutions: (a) It provides guidance in tailoring stress functions to a given data situation, responding to the fact that no single stress function dominates all others across all data situations; (b) the methodology enriches the supply of available stress functions; (c) it helps our understanding of stress functions by replacing the comparison of discrete proposals with a characterization of the effect of parameters on embeddings; (d) it builds a bridge to graph drawing, which is the related but not identical art of constructing embeddings from graphs. Keywords: multidimensional scaling, force-directed layout, cluster analysis, clustering strength, unsupervised learning, Box-Cox transformations</p><p>6 0.19098273 <a title="37-lsi-6" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>7 0.1599344 <a title="37-lsi-7" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>8 0.14854918 <a title="37-lsi-8" href="./jmlr-2013-The_CAM_Software_for_Nonnegative_Blind_Source_Separation_in_R-Java.html">113 jmlr-2013-The CAM Software for Nonnegative Blind Source Separation in R-Java</a></p>
<p>9 0.13916415 <a title="37-lsi-9" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>10 0.1184044 <a title="37-lsi-10" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>11 0.11500569 <a title="37-lsi-11" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>12 0.11248381 <a title="37-lsi-12" href="./jmlr-2013-Language-Motivated_Approaches_to_Action_Recognition.html">58 jmlr-2013-Language-Motivated Approaches to Action Recognition</a></p>
<p>13 0.10947016 <a title="37-lsi-13" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>14 0.10641476 <a title="37-lsi-14" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>15 0.1006581 <a title="37-lsi-15" href="./jmlr-2013-A_Risk_Comparison_of_Ordinary_Least_Squares_vs_Ridge_Regression.html">7 jmlr-2013-A Risk Comparison of Ordinary Least Squares vs Ridge Regression</a></p>
<p>16 0.096844085 <a title="37-lsi-16" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>17 0.096392922 <a title="37-lsi-17" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>18 0.093882903 <a title="37-lsi-18" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>19 0.088644952 <a title="37-lsi-19" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>20 0.087647915 <a title="37-lsi-20" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.02), (18, 0.024), (21, 0.036), (34, 0.029), (49, 0.01), (55, 0.037), (58, 0.077), (61, 0.089), (81, 0.028), (85, 0.423), (98, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.64922696 <a title="37-lda-1" href="./jmlr-2013-Divvy%3A_Fast_and_Intuitive_Exploratory_Data_Analysis.html">37 jmlr-2013-Divvy: Fast and Intuitive Exploratory Data Analysis</a></p>
<p>Author: Joshua M. Lewis, Virginia R. de Sa, Laurens van der Maaten</p><p>Abstract: Divvy is an application for applying unsupervised machine learning techniques (clustering and dimensionality reduction) to the data analysis process. Divvy provides a novel UI that allows researchers to tighten the action-perception loop of changing algorithm parameters and seeing a visualization of the result. Machine learning researchers can use Divvy to publish easy to use reference implementations of their algorithms, which helps the machine learning ﬁeld have a greater impact on research practices elsewhere. Keywords: clustering, dimensionality reduction, open source software, human computer interaction, data visualization</p><p>2 0.28052181 <a title="37-lda-2" href="./jmlr-2013-Tapkee%3A_An_Efficient_Dimension_Reduction_Library.html">112 jmlr-2013-Tapkee: An Efficient Dimension Reduction Library</a></p>
<p>Author: Sergey Lisitsyn, Christian Widmer, Fernando J. Iglesias Garcia</p><p>Abstract: We present Tapkee, a C++ template library that provides efﬁcient implementations of more than 20 widely used dimensionality reduction techniques ranging from Locally Linear Embedding (Roweis and Saul, 2000) and Isomap (de Silva and Tenenbaum, 2002) to the recently introduced BarnesHut-SNE (van der Maaten, 2013). Our library was designed with a focus on performance and ﬂexibility. For performance, we combine efﬁcient multi-core algorithms, modern data structures and state-of-the-art low-level libraries. To achieve ﬂexibility, we designed a clean interface for applying methods to user data and provide a callback API that facilitates integration with the library. The library is freely available as open-source software and is distributed under the permissive BSD 3-clause license. We encourage the integration of Tapkee into other open-source toolboxes and libraries. For example, Tapkee has been integrated into the codebase of the Shogun toolbox (Sonnenburg et al., 2010), giving us access to a rich set of kernels, distance measures and bindings to common programming languages including Python, Octave, Matlab, R, Java, C#, Ruby, Perl and Lua. Source code, examples and documentation are available at http://tapkee.lisitsyn.me. Keywords: dimensionality reduction, machine learning, C++, open source software</p><p>3 0.27601984 <a title="37-lda-3" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>Author: Chong Zhang, Yufeng Liu</p><p>Abstract: Hard and soft classiﬁers are two important groups of techniques for classiﬁcation problems. Logistic regression and Support Vector Machines are typical examples of soft and hard classiﬁers respectively. The essential difference between these two groups is whether one needs to estimate the class conditional probability for the classiﬁcation task or not. In particular, soft classiﬁers predict the label based on the obtained class conditional probabilities, while hard classiﬁers bypass the estimation of probabilities and focus on the decision boundary. In practice, for the goal of accurate classiﬁcation, it is unclear which one to use in a given situation. To tackle this problem, the Largemargin Uniﬁed Machine (LUM) was recently proposed as a uniﬁed family to embrace both groups. The LUM family enables one to study the behavior change from soft to hard binary classiﬁers. For multicategory cases, however, the concept of soft and hard classiﬁcation becomes less clear. In that case, class probability estimation becomes more involved as it requires estimation of a probability vector. In this paper, we propose a new Multicategory LUM (MLUM) framework to investigate the behavior of soft versus hard classiﬁcation under multicategory settings. Our theoretical and numerical results help to shed some light on the nature of multicategory classiﬁcation and its transition behavior from soft to hard classiﬁers. The numerical results suggest that the proposed tuned MLUM yields very competitive performance. Keywords: hard classiﬁcation, large-margin, soft classiﬁcation, support vector machine</p><p>4 0.26997733 <a title="37-lda-4" href="./jmlr-2013-Kernel_Bayes%27_Rule%3A_Bayesian_Inference_with_Positive_Definite_Kernels.html">57 jmlr-2013-Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels</a></p>
<p>Author: Kenji Fukumizu, Le Song, Arthur Gretton</p><p>Abstract: A kernel method for realizing Bayes’ rule is proposed, based on representations of probabilities in reproducing kernel Hilbert spaces. Probabilities are uniquely characterized by the mean of the canonical map to the RKHS. The prior and conditional probabilities are expressed in terms of RKHS functions of an empirical sample: no explicit parametric model is needed for these quantities. The posterior is likewise an RKHS mean of a weighted sample. The estimator for the expectation of a function of the posterior is derived, and rates of consistency are shown. Some representative applications of the kernel Bayes’ rule are presented, including Bayesian computation without likelihood and ﬁltering with a nonparametric state-space model. Keywords: kernel method, Bayes’ rule, reproducing kernel Hilbert space</p><p>5 0.2547161 <a title="37-lda-5" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>Author: Vinay Jethava, Anders Martinsson, Chiranjib Bhattacharyya, Devdatt Dubhashi</p><p>Abstract: In this paper we establish that the Lov´ sz ϑ function on a graph can be restated as a kernel learning a problem. We introduce the notion of SVM − ϑ graphs, on which Lov´ sz ϑ function can be apa proximated well by a Support vector machine (SVM). We show that Erd¨ s-R´ nyi random G(n, p) o e log4 n np graphs are SVM − ϑ graphs for n ≤ p < 1. Even if we embed a large clique of size Θ 1−p in a G(n, p) graph the resultant graph still remains a SVM − ϑ graph. This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. Associated with the ϑ function is the notion of orthogonal labellings. We introduce common orthogonal labellings which extends the idea of orthogonal labellings to multiple graphs. This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude. Keywords: orthogonal labellings of graphs, planted cliques, random graphs, common dense subgraph</p><p>6 0.24358052 <a title="37-lda-6" href="./jmlr-2013-GURLS%3A_A_Least_Squares_Library_for_Supervised_Learning.html">46 jmlr-2013-GURLS: A Least Squares Library for Supervised Learning</a></p>
<p>7 0.24344784 <a title="37-lda-7" href="./jmlr-2013-On_the_Learnability_of_Shuffle_Ideals.html">78 jmlr-2013-On the Learnability of Shuffle Ideals</a></p>
<p>8 0.24272534 <a title="37-lda-8" href="./jmlr-2013-BudgetedSVM%3A_A_Toolbox_for_Scalable_SVM_Approximations.html">19 jmlr-2013-BudgetedSVM: A Toolbox for Scalable SVM Approximations</a></p>
<p>9 0.24228469 <a title="37-lda-9" href="./jmlr-2013-MLPACK%3A_A_Scalable_C%2B%2B_Machine_Learning_Library.html">67 jmlr-2013-MLPACK: A Scalable C++ Machine Learning Library</a></p>
<p>10 0.2391174 <a title="37-lda-10" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>11 0.23748016 <a title="37-lda-11" href="./jmlr-2013-Keep_It_Simple_And_Sparse%3A_Real-Time_Action_Recognition.html">56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</a></p>
<p>12 0.22698697 <a title="37-lda-12" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>13 0.22526243 <a title="37-lda-13" href="./jmlr-2013-CODA%3A_High_Dimensional_Copula_Discriminant_Analysis.html">20 jmlr-2013-CODA: High Dimensional Copula Discriminant Analysis</a></p>
<p>14 0.22279438 <a title="37-lda-14" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>15 0.22154249 <a title="37-lda-15" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>16 0.22129317 <a title="37-lda-16" href="./jmlr-2013-The_CAM_Software_for_Nonnegative_Blind_Source_Separation_in_R-Java.html">113 jmlr-2013-The CAM Software for Nonnegative Blind Source Separation in R-Java</a></p>
<p>17 0.22037852 <a title="37-lda-17" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>18 0.21897127 <a title="37-lda-18" href="./jmlr-2013-Learning_Trees_from_Strings%3A_A_Strong_Learning_Algorithm_for_some_Context-Free_Grammars.html">63 jmlr-2013-Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars</a></p>
<p>19 0.2173304 <a title="37-lda-19" href="./jmlr-2013-Segregating_Event_Streams_and_Noise_with_a_Markov_Renewal_Process_Model.html">98 jmlr-2013-Segregating Event Streams and Noise with a Markov Renewal Process Model</a></p>
<p>20 0.21386327 <a title="37-lda-20" href="./jmlr-2013-One-shot_Learning_Gesture_Recognition_from_RGB-D_Data_Using_Bag_of_Features.html">80 jmlr-2013-One-shot Learning Gesture Recognition from RGB-D Data Using Bag of Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
