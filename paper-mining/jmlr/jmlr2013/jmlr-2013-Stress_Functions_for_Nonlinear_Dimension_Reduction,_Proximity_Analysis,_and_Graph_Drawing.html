<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-109" href="#">jmlr2013-109</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</h1>
<br/><p>Source: <a title="jmlr-2013-109-pdf" href="http://jmlr.org/papers/volume14/chen13a/chen13a.pdf">pdf</a></p><p>Author: Lisha Chen, Andreas Buja</p><p>Abstract: Multidimensional scaling (MDS) is the art of reconstructing pointsets (embeddings) from pairwise distance data, and as such it is at the basis of several approaches to nonlinear dimension reduction and manifold learning. At present, MDS lacks a unifying methodology as it consists of a discrete collection of proposals that differ in their optimization criteria, called “stress functions”. To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. This methodology yields the following beneﬁts and problem solutions: (a) It provides guidance in tailoring stress functions to a given data situation, responding to the fact that no single stress function dominates all others across all data situations; (b) the methodology enriches the supply of available stress functions; (c) it helps our understanding of stress functions by replacing the comparison of discrete proposals with a characterization of the effect of parameters on embeddings; (d) it builds a bridge to graph drawing, which is the related but not identical art of constructing embeddings from graphs. Keywords: multidimensional scaling, force-directed layout, cluster analysis, clustering strength, unsupervised learning, Box-Cox transformations</p><p>Reference: <a title="jmlr-2013-109-reference" href="../jmlr2013_reference/jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. [sent-5, score-1.315]
</p><p>2 Distance scaling and the ﬁrst stress function were ﬁrst introduced by Kruskal (1964a,b), followed with proposals by Sammon (1969), Takane, Young, and De Leeuw (ALSCAL, 1977), Kamada and Kawai (1989), among others. [sent-20, score-0.628]
</p><p>3 For example, embeddings from two stress functions on the same data may both be insightful in that one better reﬂects local structure, the other global structure. [sent-22, score-0.682]
</p><p>4 Needed is a methodology that organizes stress functions and provides guidance to their speciﬁc performance on any given data set. [sent-24, score-0.568]
</p><p>5 For part (1) of the program we took a page from graph drawing which had been in a situation similar to MDS: a collection of discrete proposals for so-called “energy functions”, the analogs of stress functions for graph data. [sent-27, score-0.857]
</p><p>6 Inspired by this work, the ﬁrst author (Chen, 2006) proposed in her thesis the four-parameter family of distancebased stress functions presented here for the ﬁrst time. [sent-29, score-0.622]
</p><p>7 This family provides an umbrella for several stress functions from the MDS literature as well as energy functions from the graph drawing literature. [sent-31, score-0.877]
</p><p>8 A related two-parameter family of energy functions for weighted graph data was proposed by Noack (2009), and we study its connection to stress functions for distance data in Section 2. [sent-32, score-0.893]
</p><p>9 1146  S TRESS F UNCTIONS  For part (2) of the program, the analysis and interpretation of the stress function parameters, we develop the nucleus of a theory that explains the effects of the some of the parameters on embeddings. [sent-34, score-0.546]
</p><p>10 For one thing, distance data, unlike weighted graph data, have a natural concept of “perfect embedding”, which is achieved when the target distance data are perfectly matched by the embedding distances. [sent-36, score-0.348]
</p><p>11 We show that all members in the B-C family of stress functions for complete distance data have the property that they are minimized by perfect embeddings if such exist (Section 2. [sent-37, score-0.833]
</p><p>12 In the general case, when there exists no perfect embedding, a natural question is how the minimization of stress functions creates compromises between conﬂicting distance information. [sent-39, score-0.673]
</p><p>13 To answer this question we introduce the notion of “scale sensitivity”, which is the degree to which the compromise is dominated by small or large distances through the interaction of two stress function parameters (Section 2. [sent-40, score-0.674]
</p><p>14 Before we outline step (3) of our program, we make a point that is of interest to machine learning: The B-C family of stress functions encompasses energy functions for graph drawing through an extension from complete to incomplete distance data. [sent-42, score-0.995]
</p><p>15 This limiting process offers up a parameter to control the relative strength of the pervasive repulsion vis-` -vis the partial stress for a the known distances, thereby acting as a regularization parameter that stabilizes embeddings by reducing variance at the cost of some bias. [sent-48, score-0.857]
</p><p>16 This device, ﬁrst applied by the authors (Chen and Buja, 2009) to Kruskal’s stress function, brings numerous energy functions for unweighted graphs under the umbrella of the B-C family of stress functions. [sent-49, score-1.347]
</p><p>17 Finally, in step (3) of our program, we turn to the problem of selecting “good embeddings” from the multitude that can be obtained from the B-C family of stress functions. [sent-50, score-0.6]
</p><p>18 Thus the parameters of the B-C family of stress functions can be chosen to optimize a meta-criterion. [sent-54, score-0.622]
</p><p>19 1147  C HEN AND B UJA  This article proceeds as follows: Section 2 introduces the B-C family of stress functions in steps. [sent-56, score-0.622]
</p><p>20 MDS Stress Functions Based on Power Laws This section ﬁrst interprets Kruskal’s stress (Kruskal, 1964a) in the framework of attracting and repulsing energies (Section 2. [sent-60, score-0.952]
</p><p>21 The section concludes with technical aspects concerning the irrelevance of the relative strengths of attracting and repulsing energies (Section 2. [sent-67, score-0.406]
</p><p>22 Kruskal’s original MDS proposal (Kruskal, 1964a) solves the problem by proposing a stress function that is essentially a residual sum of squares (RSS) between the target distances given as data and the distances in the embedding. [sent-77, score-0.843]
</p><p>23 The goal is to ﬁnd an embedding X whose distances di, j ﬁt the target distances Di, j as best as possible. [sent-82, score-0.379]
</p><p>24 Kruskal’s stress function is therefore S(d|D) = ∑(di, j − Di, j )2 , i, j  where we let d = (di, j )i, j = ( xi − x j )i, j and D = (Di, j )i, j . [sent-83, score-0.546]
</p><p>25 Taking a page from the graph drawing literature, we interpret Kruskal’s stress function as composed of an “attracting energy” and a “repulsing energy” as follows: S(d|D) =  ∑(di, j 2 − 2Di, j di, j ) + const. [sent-85, score-0.683]
</p><p>26 ) A stress term (di, j − Di, j )2 is therefore seen to be equivalent to a sum of an attracting and a repulsing energy term that balance each other in such a way that the minimum energy is achieved at di, j = Di, j . [sent-89, score-1.078]
</p><p>27 2 The B-C Family of Stress Functions We next introduce a family of stress functions whose attracting and repulsing energies follow power laws, in analogy to Noack’s generalized energy functions for graph drawing (Noack, 2003, 2007, 2009). [sent-91, score-1.33]
</p><p>28 We denote the attracting power by µ + λ and the repulsing power by µ with the understanding that λ > 0 and −∞ < µ < +∞. [sent-99, score-0.434]
</p><p>29 Deﬁnition 1 The B-C family of stress functions for complete distance data D = (Di j)i, j is given by S(d|D) = (1) ∑ Di, j ν BCµ+λ (di, j ) − Di, j λ BCµ (di, j ) . [sent-100, score-0.694]
</p><p>30 Thus Di, j ν upweights the summands for large Di, j when ν > 0 and downweights them when ν < 0; for ν = 0 the stress function is an unweighted sum. [sent-105, score-0.593]
</p><p>31 The parameter ν allows us to capture a couple of extant stress functions; see Table 1. [sent-106, score-0.572]
</p><p>32 Kruskal’s stress function does not require ν as it arises from µ = 1, λ = 1 and ν = 0. [sent-107, score-0.546]
</p><p>33 This property holds only for this particular choice of the power Dλ in the repulsing energy term. [sent-114, score-0.334]
</p><p>34 Edgewise unbiasedness is essential to grant the following exact reconstruction property: Proposition 2 If the target data Di, j form a set of Euclidean distances in the embedding dimension, Di, j = xi − x j (i, j = 1, . [sent-115, score-0.301]
</p><p>35 , N), then all B-C stress functions are minimized by the embeddings that reproduce the target distances exactly: di, j = Di, j . [sent-118, score-0.876]
</p><p>36 In practice, one often reduces multiple distances by averaging them, but a more principled approach is to form a stress function with multiple stress terms per object pair (i, j). [sent-127, score-1.22]
</p><p>37 In general, if target distances Di, j,k for the object pair (i, j) are observed Ki, j times, the B-C stress function will be  ∑  S =  ∑  i, j=1,. [sent-128, score-0.715]
</p><p>38 The stress function for the single embedding distance d is S = D1 ν BCµ+λ (d) − D1 λ BCµ (d)  + D2 ν BCµ+λ (d) − D2 λ BCµ (d) . [sent-136, score-0.7]
</p><p>39 However, α1 and α2 are also functions of (D1 , D2 ), hence the minimizing distance dmin = d(D1 , D2 ) is a function of the target distances in a complex way. [sent-139, score-0.338]
</p><p>40 ) While large distances win out in the limit for λ ↑ +∞, ﬁxed small distances > 0 will never win out entirely for λ ↓ 0, although for ever smaller λ the compromise will be shifted ever more toward the smaller distance. [sent-147, score-0.3]
</p><p>41 1151  C HEN AND B UJA  Conclusion: Embeddings that minimize B-C stress compromise ever more in favor of . [sent-148, score-0.568]
</p><p>42 We use the term “small scale sensitivity” for the behavior of stress functions as λ ↓ 0 and/or ν ↓ −∞. [sent-158, score-0.593]
</p><p>43 5 Distances versus Weights Noack (2009) presents a family of “energy functions” for weighted graphs/networks that should be discussed here because it might be thought to be identical to the B-C family of stress functions— which it is not, though there exists a connection. [sent-163, score-0.654]
</p><p>44 ) The family of energy functions he considers uses a general form of power laws for attracting and repulsing energies: di, j a+1 di, j r+1 U(d|W ) = (3) ∑ wi, j a + 1 − r + 1 , i, j=1,. [sent-171, score-0.631]
</p><p>45 Unweighted graphs are characterized by wi, j ∈ {0, 1}, in which case the total energy (3) amounts to (1) the sum of attracting energies limited to the edges in the graph, and (2) the sum of repulsing energies for all pairs of nodes. [sent-180, score-0.604]
</p><p>46 We now ask how the energy functions (3) and the B-C stress functions (1) relate to each other. [sent-182, score-0.686]
</p><p>47 (5)  A comparison with (1) shows that the 2-parameter family of energy functions (5) forms a subfamily of the 3-parameter family of distance-based B-C stress functions (1) as follows: ν = −(a − r), µ = r + 1, λ = a − r. [sent-191, score-0.794]
</p><p>48 4 this constraint implies a counterbalancing of distance sensitivities implied by these parameters: as λ ↑ ∞ large distance sensitivity increases, but simultaneously ν = −λ ↓ −∞ and hence small scale sensitivity increases as well. [sent-194, score-0.313]
</p><p>49 4 in the case ν = −λ: Given two target distances D1 and D2 for N = 2 objects, the minimizing distance is obtained by specializing (2) to ν = −λ: dmin =  ↓ √ min(D1 , D2 ) as λ ↑ ∞ , as λ ↓ 0 . [sent-196, score-0.316]
</p><p>50 6 B-C Stress Functions for Incomplete Distance Data or Distance Graphs In order to arrive at stress functions for non-full graphs, we extend a device we used previously to transform Kruskal-Shepard MDS into a localized or graph version called “local MDS” or “LMDS” (Chen and Buja, 2009). [sent-203, score-0.679]
</p><p>51 Starting with stress functions (1) for full graphs, we replace the dissimilarities Di, j for nonedges (i, j) ∈ E with a single large dissimilarity D∞ which we let go to inﬁnity. [sent-205, score-0.568]
</p><p>52 We call (6) the B-C family of stress functions for distance graphs. [sent-209, score-0.694]
</p><p>53 The parameter t balances the relative strength of the combined attraction and repulsion inside the graph with the repulsion outside the graph. [sent-210, score-0.423]
</p><p>54 The ﬁrst four entries refer to stress functions for complete distance data; the last ﬁve entries refer to energy functions for plain graphs (in which case Di, j = 1 for all edges and hence ν is vacuous). [sent-214, score-0.794]
</p><p>55 (Not included is the family of power laws for weighted graphs by Noack (2009) because they become stress functions for distance graphs only after a mapping of weights to distances. [sent-216, score-0.885]
</p><p>56 5) observed that for his LinLog energy function the relative weighting of the attracting energy relative to the repulsing energy is irrelevant in the sense that such weighting would only change the scale of the minimizing layout but not the shape. [sent-219, score-0.719]
</p><p>57 A similar statement can be made for all members of the B-C family of stress functions. [sent-220, score-0.6]
</p><p>58 The repulsion terms are still differentially weighted depending on whether (i, j) is an edge of the graph E or not, which is in contrast to most energy functions proposed in the graph layout literature where invariably t = 1. [sent-222, score-0.443]
</p><p>59 As a consequence, if d is a minimizing set of conﬁguration distances for Sc (·), then the distances c d of the scaled embedding c X minimize the original unweighted B-C stress function S1 (·). [sent-224, score-0.931]
</p><p>60 It is in this sense that Noack’s PolyLog family of stress functions can be considered as a special case of the B-C family: PolyLog energies agree with B-C stress functions for unweighted graphs (Di, j = 1) for µ = 0 and t = 1 up to a multiplicative factor in the attracting energy. [sent-225, score-1.488]
</p><p>61 8 Unit-Invariant Forms of the Repulsion Weight In the B-C family of stress functions (6), the relative strength of attracting and repulsing forces is balanced by the parameter t. [sent-227, score-0.986]
</p><p>62 Meta-Criteria for Parameter Selection Following Chen and Buja (2009) and Akkucuk and Carroll (2006), we describe “meta-criteria” to measure the quality of conﬁgurations independently of the primary stress functions. [sent-238, score-0.546]
</p><p>63 1 Simulated Data We introduced three parameters in the B-C stress functions for complete distance data, namely, λ, µ and ν, and a fourth parameter, τ, in the B-C stress functions for incomplete distance graph data. [sent-273, score-1.407]
</p><p>64 We will simplify the task and eliminate the parameter ν by setting it to zero, so that the weight Di, j ν = 1 disappears from the stress functions. [sent-275, score-0.546]
</p><p>65 To deﬁne an initial local graph, as input to the stress functions, we connected each point with its adjacent neighbors with distance 1 (Figure 2, bottom). [sent-282, score-0.618]
</p><p>66 We produced 2-D conﬁgurations using B-C stress functions, and to explore the effect of the parameters on the conﬁgurations, we varied each of the three parameters one at a time. [sent-286, score-0.546]
</p><p>67 Starting from the true input conﬁgurations is of course not actionable in practice, but it serves a purpose: it demonstrates the biases and distortions implied by minimization of the stress function under the best of circumstances. [sent-288, score-0.546]
</p><p>68 An overall observation is that the larger µ, the more spread out 1158  S TRESS F UNCTIONS  adj  adj  λ = 2, Mε=1 = 0. [sent-302, score-0.512]
</p><p>69 With smaller µ (such as µ = −1) the stress function ﬂattens out as the distance increases (Figure 1) and the points are subject to very weak repulsion. [sent-359, score-0.618]
</p><p>70 1159  C HEN AND B UJA  adj  adj  λ = 2, Mε=1 = 0. [sent-366, score-0.512]
</p><p>71 887  −60  −40  −20  0  20  40  60  −2000  −60  −40  −1000  −20  0  0  20  1000  40  60  λ = 1,  0 adj Mε=1  2000  −15  −2000  −1000  0  1000  2000  Figure 4: The conﬁgurations with varying λ with other parameters ﬁxed at µ = 0, τ = 1, starting from a random conﬁguration. [sent-371, score-0.294]
</p><p>72 This indicates that the B-C stress functions for small τ are quite successful in recreating local distances as they are supposed to. [sent-377, score-0.696]
</p><p>73 1160  S TRESS F UNCTIONS  adj  adj  µ = 0, Mε=1 = 0. [sent-380, score-0.512]
</p><p>74 We then deﬁned a local graph using 4-NN, that is, 1161  C HEN AND B UJA  adj  adj  µ = 0, Mε=1 = 0. [sent-442, score-0.593]
</p><p>75 Figure 9 shows 2D conﬁgurations generated by different stress functions (6) with different clustering powers, λ = 2, 1, 2/3, 1/2, while the other parameters are ﬁxed at µ = 0 and τ = 1. [sent-452, score-0.59]
</p><p>76 The conﬁgurations do not 1162  S TRESS F UNCTIONS  adj  adj  τ = 1, Mε=1 = 0. [sent-455, score-0.512]
</p><p>77 For example, we chose a different neighborhood size K = 8 for LLE and Isomap 1163  C HEN AND B UJA  adj  adj  τ = 1, Mε=1 = 0. [sent-511, score-0.54]
</p><p>78 In our experiments we use a subset of 500 images in order to save on computations and in order to obtain less cluttered low 1164  S TRESS F UNCTIONS  adj  adj  λ = 1, Mk=4 = 0. [sent-536, score-0.548]
</p><p>79 The local neighborhood 1165  C HEN AND B UJA  adj  adj  MDS, Mk=4 = 0. [sent-552, score-0.54]
</p><p>80 The systematization consists of devising a multi-parameter family of stress functions that comprises many published proposals from the literature on proximity analysis (MDS) and graph drawing. [sent-571, score-0.78]
</p><p>81 A beneﬁt is that the seemingly arbitrary selection of a loss function is turned into a parameter selection problem based on external “meta-criteria” that measure the quality of embeddings independently of the stress functions. [sent-572, score-0.66]
</p><p>82 1166  S TRESS F UNCTIONS  adj  adj  µ = 0, M6 = 0. [sent-573, score-0.512]
</p><p>83 The parameters of the proposed family have the following interpretations: • λ: This parameter determines the relative strengths of the attracting and repulsing forces to each other, while maintaining “edgewise unbiasedness” of the stress function. [sent-605, score-0.94]
</p><p>84 For example, as ν decreases below zero, stress terms for large distances Di, j will be progressively down-weighted. [sent-612, score-0.674]
</p><p>85 1167  C HEN AND B UJA  • τ: A regularization parameter that stabilizes conﬁgurations for incomplete distance data, that is, distance graphs, at the cost of some bias (stretching of conﬁgurations), achieved by imputing inﬁnite input distances with inﬁnitesimal repulsion. [sent-614, score-0.318]
</p><p>86 The problem of incomplete distance information is often solved by completing it with additive imputations provided by the shortest-path algorithm, so that MDS-style stress functions can be used—the route taken by Isomap (Tenenbaum et al. [sent-618, score-0.686]
</p><p>87 The argument against such completion is that stress functions tend to be driven by the largest distances, which are imputed and hence noisy. [sent-620, score-0.568]
</p><p>88 In both cases the family of loss functions proposed here offers control over the scale sensitivity parameter λ, the repulsion power µ, and the weighting power ν. [sent-623, score-0.43]
</p><p>89 Stress Minimization Minimizing stress functions can be a very high-dimensional optimization problem involving all coordinates of all points in an embedding, amounting to N p parameters. [sent-627, score-0.568]
</p><p>90 1 provides gradients for plain stress functions as presented in the body of this article; Section A. [sent-631, score-0.586]
</p><p>91 The B-C 1168  S TRESS F UNCTIONS  stress function for µ = 0 and µ + λ = 0 (but ν = 0) is S(x1 , · · · , xN ) =  ∑ (i, j)∈E  − tλ  di, j µ − 1 di, j µ+λ − 1 − Di, j λ (µ + λ) µ  di, j µ − 1 . [sent-638, score-0.546]
</p><p>92 ∑ µ (i, j)∈E C  Let ∇S = (∇1 , · · · , ∇N )T be the gradient of the stress function with respect to X: ∇i =  ∂S = ∂xi  ∑ j∈ND (i)  − tλ  di, j µ+λ−2 − Di, j λ di, j µ−2 (xi − x j )  ∑ c j∈ND (i)  di, j µ−2 (xi − x j ). [sent-639, score-0.546]
</p><p>93 2 Size-Invariant Forms of B-C Stress Functions As mentioned, it is a common experience that algorithms for minimizing stress functions spend much effort on getting the size of the embedding right. [sent-645, score-0.667]
</p><p>94 We have therefore a desire to re-express stress in a manner that is independent of size. [sent-647, score-0.546]
</p><p>95 Fortunately, there exists a general method that achieves this goal: For any conﬁguration, minimize stress with regard to size and replace the original stress with its size-minimized value. [sent-648, score-1.116]
</p><p>96 The result is a new form of stress that is minimized by the same shapes as the original stress, but it is independent of size and hence purely driven by shape. [sent-650, score-0.571]
</p><p>97 The computational advantage of size-invariant stress is that gradient-based optimization descends along directions that change shape, not size. [sent-651, score-0.546]
</p><p>98 To ﬁnd the stationary size factor s of the stress as a function of s, S = S(s), we observe that ∂ BCµ (s di, j ) = sµ−1 di, j µ (∀µ ∈ IR). [sent-662, score-0.546]
</p><p>99 Evaluating S(s∗ ) we arrive at a size-invariant yet shape-equivalent form of the stress function. [sent-668, score-0.546]
</p><p>100 3 Gradients for Size-Invariant Stress Functions ˜ To describe the gradient of the size-invariant stress function S (d). [sent-683, score-0.546]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('stress', 0.546), ('di', 0.322), ('adj', 0.256), ('tden', 0.208), ('tnum', 0.208), ('noack', 0.191), ('repulsing', 0.191), ('gurations', 0.17), ('attracting', 0.149), ('repulsion', 0.141), ('mds', 0.134), ('distances', 0.128), ('bc', 0.124), ('tress', 0.116), ('uja', 0.116), ('embeddings', 0.114), ('kruskal', 0.102), ('energy', 0.096), ('con', 0.083), ('unctions', 0.083), ('embedding', 0.082), ('guration', 0.081), ('graph', 0.081), ('buja', 0.078), ('hen', 0.077), ('dmin', 0.075), ('edgewise', 0.075), ('distance', 0.072), ('laws', 0.072), ('sensitivity', 0.072), ('md', 0.066), ('energies', 0.066), ('drawing', 0.056), ('family', 0.054), ('proposals', 0.051), ('ad', 0.05), ('lmds', 0.05), ('unbiasedness', 0.05), ('unweighted', 0.047), ('power', 0.047), ('incomplete', 0.046), ('isomap', 0.045), ('frey', 0.045), ('multidimensional', 0.043), ('transformations', 0.042), ('chen', 0.042), ('lisha', 0.042), ('olivetti', 0.042), ('venna', 0.042), ('target', 0.041), ('neighborhoods', 0.038), ('images', 0.036), ('graphs', 0.036), ('attraction', 0.036), ('powers', 0.035), ('objects', 0.035), ('clusters', 0.034), ('mk', 0.034), ('compromises', 0.033), ('pervasive', 0.032), ('scaling', 0.031), ('device', 0.03), ('icting', 0.03), ('neighborhood', 0.028), ('andreas', 0.028), ('lle', 0.028), ('proximity', 0.026), ('extant', 0.026), ('facial', 0.026), ('minimized', 0.025), ('akkucuk', 0.025), ('drawings', 0.025), ('kamada', 0.025), ('koren', 0.025), ('linlog', 0.025), ('takane', 0.025), ('trustworthiness', 0.025), ('scale', 0.025), ('nd', 0.024), ('regard', 0.024), ('strength', 0.024), ('kaski', 0.024), ('nonlinear', 0.023), ('ever', 0.022), ('layout', 0.022), ('clustering', 0.022), ('functions', 0.022), ('weighting', 0.022), ('monotone', 0.022), ('nonmetric', 0.021), ('polylog', 0.021), ('reduction', 0.021), ('ir', 0.021), ('varying', 0.02), ('situation', 0.02), ('gradients', 0.018), ('law', 0.018), ('starting', 0.018), ('sd', 0.017), ('experience', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999917 <a title="109-tfidf-1" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>Author: Lisha Chen, Andreas Buja</p><p>Abstract: Multidimensional scaling (MDS) is the art of reconstructing pointsets (embeddings) from pairwise distance data, and as such it is at the basis of several approaches to nonlinear dimension reduction and manifold learning. At present, MDS lacks a unifying methodology as it consists of a discrete collection of proposals that differ in their optimization criteria, called “stress functions”. To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. This methodology yields the following beneﬁts and problem solutions: (a) It provides guidance in tailoring stress functions to a given data situation, responding to the fact that no single stress function dominates all others across all data situations; (b) the methodology enriches the supply of available stress functions; (c) it helps our understanding of stress functions by replacing the comparison of discrete proposals with a characterization of the effect of parameters on embeddings; (d) it builds a bridge to graph drawing, which is the related but not identical art of constructing embeddings from graphs. Keywords: multidimensional scaling, force-directed layout, cluster analysis, clustering strength, unsupervised learning, Box-Cox transformations</p><p>2 0.060904138 <a title="109-tfidf-2" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>Author: Binbin Lin, Xiaofei He, Chiyuan Zhang, Ming Ji</p><p>Abstract: We propose a novel local isometry based dimensionality reduction method from the perspective of vector ﬁelds, which is called parallel vector ﬁeld embedding (PFE). We ﬁrst give a discussion on local isometry and global isometry to show the intrinsic connection between parallel vector ﬁelds and isometry. The problem of ﬁnding an isometry turns out to be equivalent to ﬁnding orthonormal parallel vector ﬁelds on the data manifold. Therefore, we ﬁrst ﬁnd orthonormal parallel vector ﬁelds by solving a variational problem on the manifold. Then each embedding function can be obtained by requiring its gradient ﬁeld to be as close to the corresponding parallel vector ﬁeld as possible. Theoretical results show that our method can precisely recover the manifold if it is isometric to a connected open subset of Euclidean space. Both synthetic and real data examples demonstrate the effectiveness of our method even if there is heavy noise and high curvature. Keywords: manifold learning, isometry, vector ﬁeld, covariant derivative, out-of-sample extension</p><p>3 0.058813237 <a title="109-tfidf-3" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>Author: Ameet Talwalkar, Sanjiv Kumar, Mehryar Mohri, Henry Rowley</p><p>Abstract: This paper examines the efﬁcacy of sampling-based low-rank approximation techniques when applied to large dense kernel matrices. We analyze two common approximate singular value decomposition techniques, namely the Nystr¨ m and Column sampling methods. We present a theoretical o comparison between these two methods, provide novel insights regarding their suitability for various tasks and present experimental results that support our theory. Our results illustrate the relative strengths of each method. We next examine the performance of these two techniques on the largescale task of extracting low-dimensional manifold structure given millions of high-dimensional face images. We address the computational challenges of non-linear dimensionality reduction via Isomap and Laplacian Eigenmaps, using a graph containing about 18 million nodes and 65 million edges. We present extensive experiments on learning low-dimensional embeddings for two large face data sets: CMU-PIE (35 thousand faces) and a web data set (18 million faces). Our comparisons show that the Nystr¨ m approximation is superior to the Column sampling method for this o task. Furthermore, approximate Isomap tends to perform better than Laplacian Eigenmaps on both clustering and classiﬁcation with the labeled CMU-PIE data set. Keywords: low-rank approximation, manifold learning, large-scale matrix factorization</p><p>4 0.051898088 <a title="109-tfidf-4" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>Author: Matthew J. Urry, Peter Sollich</p><p>Abstract: We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to signiﬁcant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is signiﬁcantly more accurate than previous approximations and should become exact in the limit of large random graphs. Keywords: Gaussian process, generalisation error, learning curve, cavity method, belief propagation, graph, random walk kernel</p><p>5 0.050478958 <a title="109-tfidf-5" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>Author: Nicolò Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella</p><p>Abstract: We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world data sets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice. Keywords: online learning, learning on graphs, graph prediction, random spanning trees</p><p>6 0.046117332 <a title="109-tfidf-6" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>7 0.045752682 <a title="109-tfidf-7" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<p>8 0.041999754 <a title="109-tfidf-8" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>9 0.037627924 <a title="109-tfidf-9" href="./jmlr-2013-Tapkee%3A_An_Efficient_Dimension_Reduction_Library.html">112 jmlr-2013-Tapkee: An Efficient Dimension Reduction Library</a></p>
<p>10 0.03279563 <a title="109-tfidf-10" href="./jmlr-2013-Counterfactual_Reasoning_and_Learning_Systems%3A_The_Example_of_Computational_Advertising.html">30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</a></p>
<p>11 0.032130398 <a title="109-tfidf-11" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>12 0.031786278 <a title="109-tfidf-12" href="./jmlr-2013-Manifold_Regularization_and_Semi-supervised_Learning%3A_Some_Theoretical_Analyses.html">69 jmlr-2013-Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses</a></p>
<p>13 0.031149233 <a title="109-tfidf-13" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>14 0.030934064 <a title="109-tfidf-14" href="./jmlr-2013-On_the_Mutual_Nearest_Neighbors_Estimate_in_Regression.html">79 jmlr-2013-On the Mutual Nearest Neighbors Estimate in Regression</a></p>
<p>15 0.029922364 <a title="109-tfidf-15" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>16 0.029571377 <a title="109-tfidf-16" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>17 0.029522512 <a title="109-tfidf-17" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>18 0.02936637 <a title="109-tfidf-18" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>19 0.029134242 <a title="109-tfidf-19" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>20 0.028212218 <a title="109-tfidf-20" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.141), (1, 0.046), (2, -0.016), (3, -0.114), (4, 0.009), (5, 0.108), (6, -0.021), (7, 0.027), (8, -0.01), (9, -0.049), (10, 0.017), (11, -0.021), (12, -0.022), (13, 0.034), (14, 0.052), (15, 0.103), (16, 0.015), (17, 0.065), (18, 0.0), (19, -0.006), (20, 0.11), (21, 0.05), (22, 0.029), (23, -0.171), (24, -0.089), (25, 0.08), (26, -0.026), (27, -0.051), (28, -0.13), (29, -0.065), (30, 0.027), (31, -0.119), (32, 0.014), (33, -0.011), (34, 0.226), (35, 0.026), (36, 0.173), (37, 0.087), (38, 0.199), (39, -0.14), (40, 0.184), (41, 0.144), (42, -0.174), (43, 0.064), (44, 0.097), (45, 0.012), (46, -0.07), (47, 0.047), (48, -0.041), (49, -0.166)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97182471 <a title="109-lsi-1" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>Author: Lisha Chen, Andreas Buja</p><p>Abstract: Multidimensional scaling (MDS) is the art of reconstructing pointsets (embeddings) from pairwise distance data, and as such it is at the basis of several approaches to nonlinear dimension reduction and manifold learning. At present, MDS lacks a unifying methodology as it consists of a discrete collection of proposals that differ in their optimization criteria, called “stress functions”. To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. This methodology yields the following beneﬁts and problem solutions: (a) It provides guidance in tailoring stress functions to a given data situation, responding to the fact that no single stress function dominates all others across all data situations; (b) the methodology enriches the supply of available stress functions; (c) it helps our understanding of stress functions by replacing the comparison of discrete proposals with a characterization of the effect of parameters on embeddings; (d) it builds a bridge to graph drawing, which is the related but not identical art of constructing embeddings from graphs. Keywords: multidimensional scaling, force-directed layout, cluster analysis, clustering strength, unsupervised learning, Box-Cox transformations</p><p>2 0.41870219 <a title="109-lsi-2" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>Author: Philémon Brakel, Dirk Stroobandt, Benjamin Schrauwen</p><p>Abstract: Imputing missing values in high dimensional time-series is a difﬁcult problem. This paper presents a strategy for training energy-based graphical models for imputation directly, bypassing difﬁculties probabilistic approaches would face. The training strategy is inspired by recent work on optimization-based learning (Domke, 2012) and allows complex neural models with convolutional and recurrent structures to be trained for imputation tasks. In this work, we use this training strategy to derive learning rules for three substantially different neural architectures. Inference in these models is done by either truncated gradient descent or variational mean-ﬁeld iterations. In our experiments, we found that the training methods outperform the Contrastive Divergence learning algorithm. Moreover, the training methods can easily handle missing values in the training data itself during learning. We demonstrate the performance of this learning scheme and the three models we introduce on one artiﬁcial and two real-world data sets. Keywords: neural networks, energy-based models, time-series, missing values, optimization</p><p>3 0.38486663 <a title="109-lsi-3" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>Author: Nicolò Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella</p><p>Abstract: We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world data sets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice. Keywords: online learning, learning on graphs, graph prediction, random spanning trees</p><p>4 0.3495791 <a title="109-lsi-4" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>Author: Vinay Jethava, Anders Martinsson, Chiranjib Bhattacharyya, Devdatt Dubhashi</p><p>Abstract: In this paper we establish that the Lov´ sz ϑ function on a graph can be restated as a kernel learning a problem. We introduce the notion of SVM − ϑ graphs, on which Lov´ sz ϑ function can be apa proximated well by a Support vector machine (SVM). We show that Erd¨ s-R´ nyi random G(n, p) o e log4 n np graphs are SVM − ϑ graphs for n ≤ p < 1. Even if we embed a large clique of size Θ 1−p in a G(n, p) graph the resultant graph still remains a SVM − ϑ graph. This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. Associated with the ϑ function is the notion of orthogonal labellings. We introduce common orthogonal labellings which extends the idea of orthogonal labellings to multiple graphs. This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude. Keywords: orthogonal labellings of graphs, planted cliques, random graphs, common dense subgraph</p><p>5 0.34649056 <a title="109-lsi-5" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>Author: Anastasios Roussos, Stavros Theodorakis, Vassilis Pitsikalis, Petros Maragos</p><p>Abstract: We propose the novel approach of dynamic afﬁne-invariant shape-appearance model (Aff-SAM) and employ it for handshape classiﬁcation and sign recognition in sign language (SL) videos. AffSAM offers a compact and descriptive representation of hand conﬁgurations as well as regularized model-ﬁtting, assisting hand tracking and extracting handshape features. We construct SA images representing the hand’s shape and appearance without landmark points. We model the variation of the images by linear combinations of eigenimages followed by afﬁne transformations, accounting for 3D hand pose changes and improving model’s compactness. We also incorporate static and dynamic handshape priors, offering robustness in occlusions, which occur often in signing. The approach includes an afﬁne signer adaptation component at the visual level, without requiring training from scratch a new singer-speciﬁc model. We rather employ a short development data set to adapt the models for a new signer. Experiments on the Boston-University-400 continuous SL corpus demonstrate improvements on handshape classiﬁcation when compared to other feature extraction approaches. Supplementary evaluations of sign recognition experiments, are conducted on a multi-signer, 100-sign data set, from the Greek sign language lemmas corpus. These explore the fusion with movement cues as well as signer adaptation of Aff-SAM to multiple signers providing promising results. Keywords: afﬁne-invariant shape-appearance model, landmarks-free shape representation, static and dynamic priors, feature extraction, handshape classiﬁcation</p><p>6 0.34260252 <a title="109-lsi-6" href="./jmlr-2013-Tapkee%3A_An_Efficient_Dimension_Reduction_Library.html">112 jmlr-2013-Tapkee: An Efficient Dimension Reduction Library</a></p>
<p>7 0.31347945 <a title="109-lsi-7" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>8 0.29416171 <a title="109-lsi-8" href="./jmlr-2013-GURLS%3A_A_Least_Squares_Library_for_Supervised_Learning.html">46 jmlr-2013-GURLS: A Least Squares Library for Supervised Learning</a></p>
<p>9 0.29198021 <a title="109-lsi-9" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>10 0.287579 <a title="109-lsi-10" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>11 0.25675225 <a title="109-lsi-11" href="./jmlr-2013-Counterfactual_Reasoning_and_Learning_Systems%3A_The_Example_of_Computational_Advertising.html">30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</a></p>
<p>12 0.23634429 <a title="109-lsi-12" href="./jmlr-2013-Divvy%3A_Fast_and_Intuitive_Exploratory_Data_Analysis.html">37 jmlr-2013-Divvy: Fast and Intuitive Exploratory Data Analysis</a></p>
<p>13 0.22683923 <a title="109-lsi-13" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>14 0.21886581 <a title="109-lsi-14" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>15 0.21690574 <a title="109-lsi-15" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>16 0.21487807 <a title="109-lsi-16" href="./jmlr-2013-On_the_Mutual_Nearest_Neighbors_Estimate_in_Regression.html">79 jmlr-2013-On the Mutual Nearest Neighbors Estimate in Regression</a></p>
<p>17 0.21055973 <a title="109-lsi-17" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>18 0.19631352 <a title="109-lsi-18" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<p>19 0.1917108 <a title="109-lsi-19" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>20 0.19056423 <a title="109-lsi-20" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.02), (5, 0.081), (6, 0.034), (10, 0.055), (23, 0.021), (68, 0.016), (70, 0.023), (75, 0.578), (85, 0.011), (87, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98323017 <a title="109-lda-1" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>same-paper 2 0.94667107 <a title="109-lda-2" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>Author: Lisha Chen, Andreas Buja</p><p>Abstract: Multidimensional scaling (MDS) is the art of reconstructing pointsets (embeddings) from pairwise distance data, and as such it is at the basis of several approaches to nonlinear dimension reduction and manifold learning. At present, MDS lacks a unifying methodology as it consists of a discrete collection of proposals that differ in their optimization criteria, called “stress functions”. To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. This methodology yields the following beneﬁts and problem solutions: (a) It provides guidance in tailoring stress functions to a given data situation, responding to the fact that no single stress function dominates all others across all data situations; (b) the methodology enriches the supply of available stress functions; (c) it helps our understanding of stress functions by replacing the comparison of discrete proposals with a characterization of the effect of parameters on embeddings; (d) it builds a bridge to graph drawing, which is the related but not identical art of constructing embeddings from graphs. Keywords: multidimensional scaling, force-directed layout, cluster analysis, clustering strength, unsupervised learning, Box-Cox transformations</p><p>3 0.8742559 <a title="109-lda-3" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>Author: Philémon Brakel, Dirk Stroobandt, Benjamin Schrauwen</p><p>Abstract: Imputing missing values in high dimensional time-series is a difﬁcult problem. This paper presents a strategy for training energy-based graphical models for imputation directly, bypassing difﬁculties probabilistic approaches would face. The training strategy is inspired by recent work on optimization-based learning (Domke, 2012) and allows complex neural models with convolutional and recurrent structures to be trained for imputation tasks. In this work, we use this training strategy to derive learning rules for three substantially different neural architectures. Inference in these models is done by either truncated gradient descent or variational mean-ﬁeld iterations. In our experiments, we found that the training methods outperform the Contrastive Divergence learning algorithm. Moreover, the training methods can easily handle missing values in the training data itself during learning. We demonstrate the performance of this learning scheme and the three models we introduce on one artiﬁcial and two real-world data sets. Keywords: neural networks, energy-based models, time-series, missing values, optimization</p><p>4 0.8471269 <a title="109-lda-4" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>Author: Wei Pan, Xiaotong Shen, Binghui Liu</p><p>Abstract: Clustering analysis is widely used in many ﬁelds. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classiﬁcation and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classiﬁcation and regression, such as model selection criteria to select the number of clusters, a difﬁcult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method’s promising performance. Keywords: generalized degrees of freedom, grouping, K-means clustering, Lasso, penalized regression, truncated Lasso penalty (TLP)</p><p>5 0.79176939 <a title="109-lda-5" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>Author: Ran Gilad-Bachrach, Christopher J.C. Burges</p><p>Abstract: Typically, one approaches a supervised machine learning problem by writing down an objective function and ﬁnding a hypothesis that minimizes it. This is equivalent to ﬁnding the Maximum A Posteriori (MAP) hypothesis for a Boltzmann distribution. However, MAP is not a robust statistic. We present an alternative approach by deﬁning a median of the distribution, which we show is both more robust, and has good generalization guarantees. We present algorithms to approximate this median. One contribution of this work is an efﬁcient method for approximating the Tukey median. The Tukey median, which is often used for data visualization and outlier detection, is a special case of the family of medians we deﬁne: however, computing it exactly is exponentially slow in the dimension. Our algorithm approximates such medians in polynomial time while making weaker assumptions than those required by previous work. Keywords: classiﬁcation, estimation, median, Tukey depth</p><p>6 0.620974 <a title="109-lda-6" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>7 0.55465925 <a title="109-lda-7" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>8 0.52094233 <a title="109-lda-8" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>9 0.49842608 <a title="109-lda-9" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>10 0.4706898 <a title="109-lda-10" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>11 0.45851636 <a title="109-lda-11" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>12 0.45826593 <a title="109-lda-12" href="./jmlr-2013-Using_Symmetry_and_Evolutionary_Search_to_Minimize_Sorting_Networks.html">118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</a></p>
<p>13 0.44636887 <a title="109-lda-13" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>14 0.43507186 <a title="109-lda-14" href="./jmlr-2013-Differential_Privacy_for_Functions_and_Functional_Data.html">32 jmlr-2013-Differential Privacy for Functions and Functional Data</a></p>
<p>15 0.4307698 <a title="109-lda-15" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>16 0.42362484 <a title="109-lda-16" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>17 0.42304474 <a title="109-lda-17" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>18 0.42250997 <a title="109-lda-18" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>19 0.41658944 <a title="109-lda-19" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>20 0.41443446 <a title="109-lda-20" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
