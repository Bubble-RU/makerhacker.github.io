<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-102" href="#">jmlr2013-102</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</h1>
<br/><p>Source: <a title="jmlr-2013-102-pdf" href="http://jmlr.org/papers/volume14/sun13a/sun13a.pdf">pdf</a></p><p>Author: Tingni Sun, Cun-Hui Zhang</p><p>Abstract: We propose a new method of learning a sparse nonnegative-deﬁnite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm ﬁrst estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other ℓ1 regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the ℓ1 and spectrum norms of the target inverse matrix diverges to inﬁnity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso. Keywords: precision matrix, concentration matrix, inverse matrix, graphical model, scaled Lasso, linear regression, spectrum norm</p><p>Reference: <a title="jmlr-2013-102-reference" href="../jmlr2013_reference/jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The algorithm ﬁrst estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. [sent-7, score-0.563]
</p><p>2 The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. [sent-8, score-0.566]
</p><p>3 In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso. [sent-12, score-0.6]
</p><p>4 Keywords: precision matrix, concentration matrix, inverse matrix, graphical model, scaled Lasso, linear regression, spectrum norm  1. [sent-13, score-0.783]
</p><p>5 The inverse covariance matrix is also called precision matrix or concentration matrix. [sent-16, score-0.438]
</p><p>6 In such cases, the sample covariance matrix is always singular and a certain type of sparsity condition is typically imposed for proper estimation of the precision matrix and for theoretical investigation of the problem. [sent-18, score-0.471]
</p><p>7 (2008) provides the convergence rate {((p + s)/n) log p}1/2 in the Frobenius norm and {(s/n) log p}1/2 in the spectrum norm, where s is the number of nonzero off-diagonal entries in the precision matrix. [sent-30, score-0.624]
</p><p>8 Since the spectrum norm can be controlled via the Frobenius norm, this provides a sufﬁcient condition (s/n) log p → 0 for the convergence to the unknown precision matrix under the spectrum norm. [sent-34, score-0.76]
</p><p>9 A potentially faster rate d (log p)/n can be achieved by ℓ1 regularized estimation of individual columns of the precision matrix, where d, the matrix degree, is the largest number of nonzero entries in a column. [sent-36, score-0.323]
</p><p>10 When the ℓ1 operator norm of the precision matrix is bounded, this method achieves the convergence rate d (log p)/n in ℓq matrix operator norms. [sent-38, score-0.413]
</p><p>11 In this paper, we propose to apply the scaled Lasso (Sun and Zhang, 2012) column-by-column to estimate a precision matrix in the high dimensional setting. [sent-46, score-0.462]
</p><p>12 Based on the connection of precision matrix estimation to linear regression, we construct a column estimator with the scaled Lasso, a joint estimator for the regression coefﬁcients and noise level. [sent-47, score-0.812]
</p><p>13 Since we only need a sample covariance matrix as input, this estimator could be extended to generate an approximate inverse of a nonnegative-deﬁnite data matrix in a more general setting. [sent-48, score-0.344]
</p><p>14 For each column, the penalty level of the scaled Lasso is determined by data via convex minimization, without using cross-validation. [sent-50, score-0.533]
</p><p>15 Theorem 1 Let Θ and Ω be the scaled Lasso estimators deﬁned in (4), (7) and (9) below with penalty level λ0 = A 4(log p)/n, A > 1, based on n iid observations from N(0, Σ∗ ). [sent-57, score-0.637]
</p><p>16 Then, Ω − Ω∗  2  = OP (1)d  (log p)/n = o(1),  where · 2 is the spectrum norm (the ℓ2 matrix operator norm). [sent-59, score-0.326]
</p><p>17 Theorem 1 provides a simple boundedness condition on the spectrum norm of Ω∗ for the convergence of Ω in spectrum norm with sample size n ≫ d 2 log p. [sent-61, score-0.71]
</p><p>18 The boundedness condition on the spectrum norm of (diagΣ∗ )1/2 Θ∗ (diagΣ∗ )1/2 and the diagonal of Θ∗ is weaker than the boundedness of the ℓ1 operator norm assumed in Yuan (2010) and Cai et al. [sent-63, score-0.54]
</p><p>19 When the ratio of the ℓ1 operator norm and spectrum norm of the precision matrix diverges to inﬁnity, the proposed estimator has a faster proven convergence rate. [sent-65, score-0.665]
</p><p>20 This sharper result is a direct consequence of the faster convergence rate of the scaled Lasso estimator of the noise level in linear regression. [sent-66, score-0.488]
</p><p>21 An important advantage of the scaled Lasso is that the penalty level is automatically set to achieve the optimal convergence rate in the regression model for the estimation of each column of the inverse matrix. [sent-69, score-0.734]
</p><p>22 This raises the possibility for the scaled Lasso to outperform methods using a single unscaled penalty level for the estimation of all columns such as the GLasso and CLIME. [sent-70, score-0.618]
</p><p>23 Another contribution of this paper is to study the scaled Lasso at a smaller penalty level than those based on ℓ∞ bounds of the noise. [sent-72, score-0.569]
</p><p>24 For A ≈ 1 and ε = po(1) , this penalty level is comparable to the universal penalty level (2/n) log p. [sent-74, score-0.66]
</p><p>25 However, ε = o(1/p), or equivalently λ0 ≈ (4/n) log p, is required if the union bound is used to simultaneously control the error of p applications of the scaled Lasso in the estimation of individual columns of a precision matrix. [sent-75, score-0.577]
</p><p>26 We close this gap by providing a theory based on a sparse ℓ2 measure of the noise, corresponding to a penalty level satisfying P{N(0, 1/n) > λ0 /A} = k/p with A > 1 and a potentially large k. [sent-77, score-0.349]
</p><p>27 This penalty level provides a faster convergence rate than the universal penalty level in linear regression when log(p/k) ≈ log(p/ β 0 ) ≪ log p. [sent-78, score-0.722]
</p><p>28 Moreover, the new analysis provides a higher concentration of the error so that the same penalty level λ0 ≈ (2/n) log(p/k) can be used to simultaneously control the estimation error in p applications of the scaled Lasso for the estimation of a precision matrix. [sent-79, score-0.839]
</p><p>29 In Section 2, we present the scaled Lasso method for the estimation of the inversion of a nonnegative deﬁnite matrix. [sent-81, score-0.342]
</p><p>30 In Section 4, we provide a theory for the Lasso and its scaled version with higher proven concentration at a smaller, practical penalty level. [sent-83, score-0.516]
</p><p>31 In Section 7, we discuss the beneﬁts of using the scaled penalty levels for the estimation of different columns of the precision matrix, compared with an optimal ﬁxed penalty level for all columns. [sent-86, score-0.958]
</p><p>32 In this section, we describe the relationship between positive-deﬁnite matrix inversion and linear regression and propose an estimator for Θ∗ via scaled Lasso, a joint convex minimization for the estimation of regression coefﬁcients and noise level. [sent-102, score-0.607]
</p><p>33 A solution to resolve these two issues is the scaled Lasso (Sun and Zhang, 2012): {β∗, j , σ j } = arg min b,σ  p bT Σb σ 1/2 + + λ0 ∑ Σkk |bk | : b j = −1 2σ 2 k=1  3388  (4)  S PARSE M ATRIX I NVERSION WITH S CALED L ASSO  with λ0 ≈ (2/n) log p. [sent-116, score-0.356]
</p><p>34 j An iterative algorithm has been provided in Sun and Zhang (2012) to compute the scaled Lasso estimator (4). [sent-123, score-0.354]
</p><p>35 Based on the Lasso path β∗, j (λ), the scaled Lasso estimator {β∗, j , σ j } is computed iteratively by T  σ2 ← β∗, j Σβ∗, j , j  λ ← σ j λ0 ,  β∗, j ← β∗, j (λ). [sent-129, score-0.354]
</p><p>36 We then simply take advantage of the relationship (2) and compute the coefﬁcients and noise levels by the scaled Lasso for each column diagΘ = diag(σ−2 , j = 1, . [sent-131, score-0.316]
</p><p>37 The estimator Ω here is a result of normalizing the precision matrix estimator by the population variances. [sent-143, score-0.467]
</p><p>38 Alternatively, we may estimate the inverse correlation matrix by using the population correlation matrix −1/2  R = (diagΣ)−1/2 Σ(diagΣ)−1/2 = D  −1/2  ΣD  as data matrix. [sent-144, score-0.341]
</p><p>39 Thus, in this scaled Lasso approach, the estimator based on the normalized data matrix is exactly the same as the one based on the original data matrix followed by a normalization step. [sent-155, score-0.498]
</p><p>40 The scaled Lasso methodology is scale-free in the noise level, and as a result, the estimator for inverse correlation matrix is also scale free in diagonal normalization. [sent-156, score-0.577]
</p><p>41 A nice property of symmetric matrices is that the spectrum norm is bounded by the ℓ1 matrix norm. [sent-164, score-0.326]
</p><p>42 The ℓ1 matrix norm can be expressed more explicitly as the maximum ℓ1 norm of the columns, while the ℓ∞ matrix norm is the maximum ℓ1 norm of the rows. [sent-165, score-0.556]
</p><p>43 Hence, for any symmetric matrix, the ℓ1 matrix norm is equivalent to the ℓ∞ matrix norm, and the spectrum norm can be bounded by either of them. [sent-166, score-0.501]
</p><p>44 Since our estimators and target matrices are all symmetric, the error bound based on the spectrum norm could be studied by bounding the ℓ1 error as typically done in the existing literature. [sent-167, score-0.349]
</p><p>45 Then (7) translates the resulting estimators of (6) to column estimators and thus a preliminary matrix estimator is constructed. [sent-171, score-0.331]
</p><p>46 Let Θ∗ = (Σ∗ )−1 be the precision matrix as the inverse of the population covariance matrix. [sent-175, score-0.367]
</p><p>47 Let Θ and Ω be their scaled Lasso estimators deﬁned in (4), (7) and (9) with a penalty level λ0 = A 4(log p)/n, A > 1. [sent-197, score-0.596]
</p><p>48 pdf), we are able to demonstrate good numerical performance of the scaled Lasso estimator with the universal penalty level λ0 = 2(log p)/n, compared with some existing methods, but not the larger penalty level λ0 > 4(log p)/n in Theorems 1 and 2. [sent-207, score-0.912]
</p><p>49 We are able to provide an afﬁrmative answer in this version of the paper by proving a higher concentration of the error of the scaled Lasso at a smaller penalty level as follows. [sent-209, score-0.591]
</p><p>50 Our new analysis of the scaled Lasso allows a threshold level λ∗,0 = Ln−3/2 (k/p) with k ≍ s log(p/s), where s = 1 + max j s∗, j . [sent-213, score-0.421]
</p><p>51 (16)  Theorem 3 Let {Σ, Σ∗ , Θ∗ , Ω∗ } be matrices as in Theorem 2, and Θ and Ω be the scaled Lasso estimators with a penalty level λ0 ≥ Aλ∗,0 where λ∗,0 = Ln−3/2 (k/p). [sent-215, score-0.596]
</p><p>52 The condition max j s∗, j λ0 ≤ c0 on (10), which controls the capped ℓ1 sparsity of the inverse correlation matrix, weakens the ℓ0 sparsity condition d (log p)/n → 0. [sent-218, score-0.353]
</p><p>53 In comparison, the spectrum norm condition is not only j 3392  S PARSE M ATRIX I NVERSION WITH S CALED L ASSO  weaker than the ℓ1 operator norm condition, but also more natural for the convergence in spectrum norm. [sent-225, score-0.553]
</p><p>54 Our sharper theoretical results are consequences of using the scaled Lasso estimator (4) and its fast convergence rate in linear regression. [sent-226, score-0.384]
</p><p>55 In Sun and Zhang (2012), a convergence rate of order s∗ (log p)/n was established for the scaled Lasso estimation of the noise level, compared with an oracle noise level as the moment estimator based on the noise vector. [sent-227, score-0.656]
</p><p>56 In the context of the columnby-column application of the scaled Lasso for precision matrix estimation, the results in Sun and Zhang (2012) can be written as σ∗ j − 1 ≤ C1 s∗, j λ2 , 0 σj  1/2  ∑ Σkk k= j  |βk, j − βk, j |  Θ∗ j ≤ C2 s∗, j λ0 , j  (17)  √ where σ∗ = Xβ∗, j 2 / n. [sent-228, score-0.462]
</p><p>57 While the results in Sun and Zhang (2012) requires a penalty level A (2/n) log(p2 ) to allow simultaneous application of (17) for all j ≤ p via the union bound in proving Theorem 2, Theorem 3 allows a smaller penalty level λ∗,0 = ALn−3/2 (k/p) with A > 1 and a potentially large k ≍ s log(p/s). [sent-236, score-0.558]
</p><p>58 Linear Regression Revisited This section provides certain new error bounds for the Lasso and scaled Lasso in the linear regression model. [sent-239, score-0.322]
</p><p>59 Let λuniv = (2/n) log p be the universal penalty level (Donoho and Johnstone, 1994). [sent-243, score-0.381]
</p><p>60 For the estimation of β and variable selection, existing theoretical results with p ≫ n typically require a penalty level λ = Aσλuniv , with A > 1, to guarantee rate optimality of regularized estimators. [sent-244, score-0.365]
</p><p>61 When the (scaled) Lasso is simultaneously applied to p subproblems as in the case of matrix estimation, the new oracle inequalities allow the use of the union bound to uniformly control the estimation error in subproblems at the same penalty level. [sent-255, score-0.417]
</p><p>62 To bound the effect of the noise when λ < X T ε/n ∞ , we use a certain sparse ℓ2 norm to control the excess of X T ε/n over a threshold level λ∗ . [sent-260, score-0.338]
</p><p>63 This leads to the deﬁnition of uT Σu/A2 : u ∈ U (Σ, S, B; A, A1 , m, m1 ) m1 + |S|  (23)  u q /A : u ∈ U (Σ, S, B; A, A1 , m, m1 ) (m1 + |S|)1/q  (24)  M ∗ = sup pred  as a constant factor for the prediction error of the Lasso and ∗ Mq = sup  for the ℓq estimation error of the Lasso. [sent-280, score-0.315]
</p><p>64 The following theorem provides analytic error bounds for the Lasso prediction and estimation under the sparse ℓ2 norm condition (21) on the noise. [sent-281, score-0.336]
</p><p>65 In the case of Gaussian error, (21) allows a ﬁxed threshold level λ∗ = σ (2/n) log(p/m) to uniformly control the error of p applications of the Lasso for the estimation of a precision matrix. [sent-283, score-0.315]
</p><p>66 Let A > 1, β = β(λ) be the Lasso estimator with penalty level λ ≥ Aλ∗ , h = β − β, and m1 = s∗ − |S|. [sent-286, score-0.379]
</p><p>67 pred ∗ The purpose of including a choice B in (21) is to achieve bounded {M ∗ , M1 } in the presence pred of some highly correlated design vectors outside S ∪ B when ΣS∪B,(S∪B)c is small. [sent-297, score-0.518]
</p><p>68 2 Scaled Lasso with Smaller Penalty: Analytical Bounds The scaled Lasso estimator is deﬁned as {β, σ} = arg min b,σ  y − Xb 2 /(2nσ) + λ0 b 2  1 + σ/2  ,  (26)  where λ0 > 0 is a scale-free penalty level. [sent-306, score-0.558]
</p><p>69 A scaled version of (19) is |S| + ∑ |β j |/(σ∗ λ∗,0 ) = s∗,0 ≤ s∗ ,  (27)  j∈S  √ where σ∗ = ε 2 / n is an oracle estimate of the noise level and λ∗,0 > 0 is a scaled threshold level. [sent-308, score-0.714]
</p><p>70 Let {β, σ} be the scaled Lasso estimator in (26), φ1 = 1/ 1 + η0 , √ √ φ2 = 1/ 1 − η0 , and σ∗ = ε 2 / n. [sent-315, score-0.354]
</p><p>71 q  < (1 + ε0 )  2  M ∗ s∗ (σφ2 λ0 )2 , pred  2 2 /n  Compared with Theorem 6, Theorem 8 requires nearly identical conditions on the design X, the noise and penalty level under proper scale. [sent-323, score-0.567]
</p><p>72 Probabilistic upper pred bounds for the noise and consequences of their combination with Theorems 6 and 8 are discussed ∗ ∗ in Section 4. [sent-326, score-0.324]
</p><p>73 pred Existing analyses of the Lasso and Dantzig selector can be to used ﬁnd upper bounds for ∗ ∗ e {M ∗ , Mq , Mσ } via the sparse eigenvalues (Cand` s and Tao, 2005, 2007; Zhang and Huang, 2008; pred Zhang, 2009; Cai et al. [sent-329, score-0.653]
</p><p>74 The following lemma provide some simple bounds used in our analysis of the scaled Lasso estimation of the precision matrix. [sent-335, score-0.512]
</p><p>75 M ∗ + 2(1 + A1 ) 1 + pred c∗ A A As∗ A s∗  (33)  ∗ M ∗ + M1 1 − pred  and ∗ Mσ ≤ 1 +  Moreover, if in addition B = {1, . [sent-340, score-0.518]
</p><p>76 pred  3397  (34)  S UN AND Z HANG  The main condition of Lemma 9, c∗ ≤ inf  uT Σu : u ∈ U (Σ, S, B; A, A1 , m, m1 ) , uS∪B 2 2  (35)  can be viewed as a restricted eigenvalue condition (Bickel et al. [sent-344, score-0.386]
</p><p>77 For p applications of the scaled Lasso in the estimation of precision matrix, this also provides a more practical penalty level compared with A′ Ln (ε/p2 ) ≈ A′ (4/n) log(p/ε1/2 ), A′ > 1 and ε ≪ 1, √ based on existing results and Theorem 11. [sent-404, score-0.725]
</p><p>78 Estimation after Model Selection We have presented theoretical properties of the scaled Lasso for linear regression and precision matrix estimation. [sent-421, score-0.494]
</p><p>79 In this section, we extend the theory to smaller threshold level and to the estimation of precision matrix. [sent-424, score-0.315]
</p><p>80 J  / J∩S=0,|J|≤m∗ uJ  2 =1  It is proved in Sun and Zhang (2012) that {β, σ} satisﬁes prediction and estimation error bounds of the same order as those for the scaled Lasso (26) under some extra conditions on κ∗ (m∗ , S; Σ). [sent-427, score-0.346]
</p><p>81 Theorem 15 Let (β, σ) be the scaled lasso estimator in (26) and (β, σ) be the least squares estimator (42) in the selected model S = supp(β). [sent-431, score-0.898]
</p><p>82 3401  (44)  S UN AND Z HANG  Theorem 15 asserts that when k ∨ m∗ ≍ s∗ , the least squares estimator {β, σ} after the scaled Lasso selection enjoys estimation and prediction properties comparable to that of the scaled Lasso: λ−2 0  σ/σ∗ − 1 + β − β  2 + 2  2  Xβ − Xβ 2 /n + β  0  = OP (1)s∗ . [sent-436, score-0.695]
</p><p>83 Corollary 16 Under the additional condition R∗ ∗  2  = O(1) on the population correlation matrix LSE  R , Theorems 2 and 3 are applicable to the estimator Θ Ω∗ = (R∗ )−1 with possibly different numerical constants. [sent-443, score-0.32]
</p><p>84 In addition to the proposed estimator (7) and (9) based on the scaled Lasso (4) and the least squares estimation after the scale Lasso (45), the graphical Lasso and CLIME are considered. [sent-446, score-0.441]
</p><p>85 The scaled 3402  S PARSE M ATRIX I NVERSION WITH S CALED L ASSO  Lasso estimators are computed based on the training sample alone with penalty level λ0 = ALn (k/p), √ 4 2 where A = 2 and k is the solution of k = L1 (k/p)+2L1 (k/p). [sent-469, score-0.596]
</p><p>86 The estimation error is measured by three matrix norms: the spectrum norm, the matrix ℓ1 norm and the Frobenius norm. [sent-475, score-0.454]
</p><p>87 The least squares estimator after the scaled Lasso selection outperforms all estimators by large margin in the spectrum and Frobenius losses in Models 1 and 3, but in general underperforms in the ℓ1 operator norm and in Model 2. [sent-478, score-0.671]
</p><p>88 Both the scaled Lasso and the CLIME are resulting from sparse linear regression solutions. [sent-481, score-0.327]
</p><p>89 A main advantage of the scaled Lasso over the CLIME is adaptive choice of the penalty level for the estimation of each column of the precision matrix. [sent-482, score-0.758]
</p><p>90 Discussion Since the scaled Lasso choose penalty levels adaptively in the estimation of each column of the precision matrix, it is expected to outperform methods using a ﬁxed penalty level for all columns in the presence of heterogeneity of the diagonal of the precision matrix. [sent-487, score-1.155]
</p><p>91 (46)  The CLIME is a symmetrization of this estimator Θ(λ) with ﬁxed penalty level for all columns. [sent-492, score-0.418]
</p><p>92 In the following example, the scaled Lasso estimator has a faster convergence rate than (46). [sent-493, score-0.384]
</p><p>93 03)  Table 1: Estimation errors under various matrix norms of scaled Lasso, GLasso and CLIME for three models. [sent-904, score-0.326]
</p><p>94 (i) Let Θ be the scaled Lasso estimator of Θ∗ = (Σ∗ )−1 with penalty level λ0 = A (4/n) log p, ∗ A > 1, as in Theorem 2. [sent-911, score-0.735]
</p><p>95 Thus, the order of the ℓ1 and spectrum norms of the error of (46) for the best data dependent penalty √ level λ is larger than that of the scaled Lasso by a factor m. [sent-914, score-0.684]
</p><p>96 ∗ This and the deﬁnition of {M ∗ , M1 } yield (32) via pred ∗ ξM ∗ + M1 ≤ max pred  Moreover, (52) gives c∗ uS∪B  2 2  1 ∨ (m/s∗ ))(2ξ + ξ1 )2  4 , ξc∗ (1 − |S|/s∗ )/A2 . [sent-960, score-0.562]
</p><p>97 The above inequalities and (55) yield |J| ≤  κ∗ (m∗ , S)M ∗ s∗ λ2 + pred λ − σ∗ λ∗,0 − σ∗ ξ1 (A − 1)λ∗,0  2 +  <  3411  κ∗ (m∗ , S)M ∗ s∗ + pred 1 − 1/A − ξ1 (1 − 1/A)  2 +  ≤ m∗ . [sent-1039, score-0.549]
</p><p>98 Then, P |R jk − R∗ | > x jk  1 − (R∗ )2 ≤ 2P |tn | > n1/2 x jk  where tn has the t-distribution with n degrees of freedom. [sent-1054, score-0.399]
</p><p>99 Thus, jk z jk =  nΣkk (1 − (R∗ )2 )Σ j j jk 3412  1/2  Σ jk Σ jk − Σkk Σkk  S PARSE M ATRIX I NVERSION WITH S CALED L ASSO  =  n 1 − (R∗ )2 jk  1/2  1/2  R jk  1/2  Σjj  Σkk  Σjj  Σkk  − R jk 1/2  1/2  is a N(0, 1) variable independent of Σkk . [sent-1061, score-1.064]
</p><p>100 Consequently, n1/2 |R jk − R jk | |z jk + zk j | ≤ |t jk | ∨ |tk j | = (1 − (R∗ )2 )1/2 Σ1/2 /Σ1/2 + Σ1/2 /Σ1/2 jk jj kk jj kk 1/2  1/2  with t jk = z jk Σkk /Σkk ∼ tn . [sent-1062, score-1.267]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lasso', 0.444), ('pred', 0.259), ('scaled', 0.254), ('clime', 0.232), ('penalty', 0.204), ('glasso', 0.185), ('ln', 0.182), ('asso', 0.174), ('nversion', 0.174), ('spectrum', 0.151), ('caled', 0.149), ('precision', 0.136), ('jk', 0.133), ('kk', 0.132), ('ut', 0.117), ('diag', 0.117), ('slasso', 0.116), ('atrix', 0.116), ('ub', 0.109), ('na', 0.108), ('norm', 0.103), ('log', 0.102), ('estimator', 0.1), ('hang', 0.095), ('parse', 0.094), ('mq', 0.091), ('xh', 0.089), ('un', 0.076), ('level', 0.075), ('zhang', 0.074), ('univ', 0.074), ('sun', 0.073), ('matrix', 0.072), ('estimators', 0.063), ('ye', 0.063), ('dantzig', 0.062), ('xb', 0.062), ('cai', 0.059), ('population', 0.059), ('concentration', 0.058), ('selector', 0.058), ('estimation', 0.056), ('boundedness', 0.055), ('theorem', 0.055), ('proposition', 0.055), ('oracle', 0.054), ('frobenius', 0.05), ('inverse', 0.05), ('covariance', 0.05), ('zb', 0.049), ('threshold', 0.048), ('mlse', 0.046), ('capped', 0.045), ('condition', 0.045), ('correlation', 0.044), ('max', 0.044), ('supp', 0.042), ('excess', 0.042), ('sparse', 0.041), ('ps', 0.041), ('iid', 0.041), ('yuan', 0.04), ('sparsity', 0.04), ('uj', 0.04), ('theorems', 0.039), ('symmetrization', 0.039), ('bc', 0.038), ('ii', 0.038), ('inf', 0.037), ('cand', 0.036), ('jj', 0.036), ('bounds', 0.036), ('aln', 0.035), ('hsc', 0.035), ('mln', 0.035), ('tingni', 0.035), ('geer', 0.033), ('column', 0.033), ('inversion', 0.032), ('regression', 0.032), ('target', 0.032), ('invertibility', 0.031), ('asserts', 0.031), ('graphical', 0.031), ('inequalities', 0.031), ('bickel', 0.03), ('suppose', 0.03), ('let', 0.03), ('rothman', 0.03), ('lse', 0.03), ('raskutti', 0.03), ('lemma', 0.03), ('rate', 0.03), ('columns', 0.029), ('noise', 0.029), ('alt', 0.029), ('op', 0.029), ('sm', 0.029), ('satisfying', 0.029), ('diagonal', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="102-tfidf-1" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>Author: Tingni Sun, Cun-Hui Zhang</p><p>Abstract: We propose a new method of learning a sparse nonnegative-deﬁnite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm ﬁrst estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other ℓ1 regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the ℓ1 and spectrum norms of the target inverse matrix diverges to inﬁnity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso. Keywords: precision matrix, concentration matrix, inverse matrix, graphical model, scaled Lasso, linear regression, spectrum norm</p><p>2 0.18443657 <a title="102-tfidf-2" href="./jmlr-2013-Consistent_Selection_of_Tuning_Parameters_via_Variable_Selection_Stability.html">27 jmlr-2013-Consistent Selection of Tuning Parameters via Variable Selection Stability</a></p>
<p>Author: Wei Sun, Junhui Wang, Yixin Fang</p><p>Abstract: Penalized regression models are popularly used in high-dimensional data analysis to conduct variable selection and model ﬁtting simultaneously. Whereas success has been widely reported in literature, their performances largely depend on the tuning parameters that balance the trade-off between model ﬁtting and model sparsity. Existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability, such as cross validation, AIC and BIC. This article introduces a general tuning parameter selection criterion based on variable selection stability. The key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection. The asymptotic selection consistency is established for both ﬁxed and diverging dimensions. Its effectiveness is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data. Keywords: kappa coefﬁcient, penalized regression, selection consistency, stability, tuning</p><p>3 0.11222787 <a title="102-tfidf-3" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>Author: Indraneel Mukherjee, Cynthia Rudin, Robert E. Schapire</p><p>Abstract: The AdaBoost algorithm was designed to combine many “weak” hypotheses that perform slightly better than random guessing into a “strong” hypothesis that has very low error. We study the rate at which AdaBoost iteratively converges to the minimum of the “exponential loss.” Unlike previous work, our proofs do not require a weak-learning assumption, nor do they require that minimizers of the exponential loss are ﬁnite. Our ﬁrst result shows that the exponential loss of AdaBoost’s computed parameter vector will be at most ε more than that of any parameter vector of ℓ1 -norm bounded by B in a number of rounds that is at most a polynomial in B and 1/ε. We also provide lower bounds showing that a polynomial dependence is necessary. Our second result is that within C/ε iterations, AdaBoost achieves a value of the exponential loss that is at most ε more than the best possible value, where C depends on the data set. We show that this dependence of the rate on ε is optimal up to constant factors, that is, at least Ω(1/ε) rounds are necessary to achieve within ε of the optimal exponential loss. Keywords: AdaBoost, optimization, coordinate descent, convergence rate</p><p>4 0.10124438 <a title="102-tfidf-4" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>Author: Pierre Alquier, Gérard Biau</p><p>Abstract: Let (X,Y ) be a random pair taking values in R p × R. In the so-called single-index model, one has Y = f ⋆ (θ⋆T X) +W , where f ⋆ is an unknown univariate measurable function, θ⋆ is an unknown vector in Rd , and W denotes a random noise satisfying E[W |X] = 0. The single-index model is known to offer a ﬂexible way to model a variety of high-dimensional real-world phenomena. However, despite its relative simplicity, this dimension reduction scheme is faced with severe complications as soon as the underlying dimension becomes larger than the number of observations (“p larger than n” paradigm). To circumvent this difﬁculty, we consider the single-index model estimation problem from a sparsity perspective using a PAC-Bayesian approach. On the theoretical side, we offer a sharp oracle inequality, which is more powerful than the best known oracle inequalities for other common procedures of single-index recovery. The proposed method is implemented by means of the reversible jump Markov chain Monte Carlo technique and its performance is compared with that of standard procedures. Keywords: single-index model, sparsity, regression estimation, PAC-Bayesian, oracle inequality, reversible jump Markov chain Monte Carlo method</p><p>5 0.095658153 <a title="102-tfidf-5" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>Author: Tony Cai, Wen-Xin Zhou</p><p>Abstract: We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed. Keywords: 1-bit matrix completion, low-rank matrix, max-norm, trace-norm, constrained optimization, maximum likelihood estimate, optimal rate of convergence</p><p>6 0.095488034 <a title="102-tfidf-6" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>7 0.093896009 <a title="102-tfidf-7" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>8 0.09196604 <a title="102-tfidf-8" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>9 0.09080667 <a title="102-tfidf-9" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>10 0.090095922 <a title="102-tfidf-10" href="./jmlr-2013-Variable_Selection_in_High-Dimension_with_Random_Designs_and_Orthogonal_Matching_Pursuit.html">119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</a></p>
<p>11 0.085158519 <a title="102-tfidf-11" href="./jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning.html">35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</a></p>
<p>12 0.083615817 <a title="102-tfidf-12" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>13 0.079401754 <a title="102-tfidf-13" href="./jmlr-2013-CODA%3A_High_Dimensional_Copula_Discriminant_Analysis.html">20 jmlr-2013-CODA: High Dimensional Copula Discriminant Analysis</a></p>
<p>14 0.079078652 <a title="102-tfidf-14" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>15 0.071651272 <a title="102-tfidf-15" href="./jmlr-2013-Risk_Bounds_of_Learning_Processes_for_L%C3%A9vy_Processes.html">97 jmlr-2013-Risk Bounds of Learning Processes for Lévy Processes</a></p>
<p>16 0.06882266 <a title="102-tfidf-16" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>17 0.066890836 <a title="102-tfidf-17" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>18 0.059612632 <a title="102-tfidf-18" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>19 0.057109442 <a title="102-tfidf-19" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>20 0.053793032 <a title="102-tfidf-20" href="./jmlr-2013-Truncated_Power_Method_for_Sparse_Eigenvalue_Problems.html">116 jmlr-2013-Truncated Power Method for Sparse Eigenvalue Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.281), (1, 0.078), (2, 0.109), (3, 0.146), (4, 0.187), (5, -0.191), (6, 0.049), (7, -0.099), (8, 0.3), (9, 0.049), (10, -0.012), (11, -0.043), (12, -0.159), (13, 0.121), (14, 0.013), (15, -0.134), (16, 0.054), (17, 0.072), (18, 0.101), (19, -0.091), (20, 0.041), (21, -0.003), (22, 0.165), (23, -0.023), (24, 0.035), (25, -0.06), (26, -0.037), (27, -0.08), (28, -0.007), (29, -0.087), (30, -0.075), (31, 0.05), (32, -0.018), (33, 0.068), (34, -0.0), (35, 0.003), (36, -0.013), (37, 0.075), (38, -0.015), (39, -0.014), (40, -0.019), (41, 0.037), (42, 0.036), (43, 0.001), (44, 0.009), (45, 0.033), (46, 0.009), (47, 0.045), (48, -0.029), (49, 0.004)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95761782 <a title="102-lsi-1" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>Author: Tingni Sun, Cun-Hui Zhang</p><p>Abstract: We propose a new method of learning a sparse nonnegative-deﬁnite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm ﬁrst estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other ℓ1 regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the ℓ1 and spectrum norms of the target inverse matrix diverges to inﬁnity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso. Keywords: precision matrix, concentration matrix, inverse matrix, graphical model, scaled Lasso, linear regression, spectrum norm</p><p>2 0.7657631 <a title="102-lsi-2" href="./jmlr-2013-Consistent_Selection_of_Tuning_Parameters_via_Variable_Selection_Stability.html">27 jmlr-2013-Consistent Selection of Tuning Parameters via Variable Selection Stability</a></p>
<p>Author: Wei Sun, Junhui Wang, Yixin Fang</p><p>Abstract: Penalized regression models are popularly used in high-dimensional data analysis to conduct variable selection and model ﬁtting simultaneously. Whereas success has been widely reported in literature, their performances largely depend on the tuning parameters that balance the trade-off between model ﬁtting and model sparsity. Existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability, such as cross validation, AIC and BIC. This article introduces a general tuning parameter selection criterion based on variable selection stability. The key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection. The asymptotic selection consistency is established for both ﬁxed and diverging dimensions. Its effectiveness is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data. Keywords: kappa coefﬁcient, penalized regression, selection consistency, stability, tuning</p><p>3 0.60783958 <a title="102-lsi-3" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>Author: Yuejia He, Yiyuan She, Dapeng Wu</p><p>Abstract: Recently, researchers have proposed penalized maximum likelihood to identify network topology underlying a dynamical system modeled by multivariate time series. The time series of interest are assumed to be stationary, but this restriction is never taken into consideration by existing estimation methods. Moreover, practical problems of interest may have ultra-high dimensionality and obvious node collinearity. In addition, none of the available algorithms provides a probabilistic measure of the uncertainty for the obtained network topology which is informative in reliable network identiﬁcation. The main purpose of this paper is to tackle these challenging issues. We propose the S2 learning framework, which stands for stationary-sparse network learning. We propose a novel algorithm referred to as the Berhu iterative sparsity pursuit with stationarity (BISPS), where the Berhu regularization can improve the Lasso in detection and estimation. The algorithm is extremely easy to implement, efﬁcient in computation and has a theoretical guarantee to converge to a global optimum. We also incorporate a screening technique into BISPS to tackle ultra-high dimensional problems and enhance computational efﬁciency. Furthermore, a stationary bootstrap technique is applied to provide connection occurring frequency for reliable topology learning. Experiments show that our method can achieve stationary and sparse causality network learning and is scalable for high-dimensional problems. Keywords: stationarity, sparsity, Berhu, screening, bootstrap</p><p>4 0.54635251 <a title="102-lsi-4" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>Author: Pinghua Gong, Jieping Ye, Changshui Zhang</p><p>Abstract: Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an ℓ0 -type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel non-convex regularizer. To solve the non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm; we also provide intuitive interpretations, detailed convergence and reproducibility analysis for the proposed algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms. Keywords: multi-task learning, multi-stage, non-convex, sparse learning</p><p>5 0.52546299 <a title="102-lsi-5" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>Author: Pierre Alquier, Gérard Biau</p><p>Abstract: Let (X,Y ) be a random pair taking values in R p × R. In the so-called single-index model, one has Y = f ⋆ (θ⋆T X) +W , where f ⋆ is an unknown univariate measurable function, θ⋆ is an unknown vector in Rd , and W denotes a random noise satisfying E[W |X] = 0. The single-index model is known to offer a ﬂexible way to model a variety of high-dimensional real-world phenomena. However, despite its relative simplicity, this dimension reduction scheme is faced with severe complications as soon as the underlying dimension becomes larger than the number of observations (“p larger than n” paradigm). To circumvent this difﬁculty, we consider the single-index model estimation problem from a sparsity perspective using a PAC-Bayesian approach. On the theoretical side, we offer a sharp oracle inequality, which is more powerful than the best known oracle inequalities for other common procedures of single-index recovery. The proposed method is implemented by means of the reversible jump Markov chain Monte Carlo technique and its performance is compared with that of standard procedures. Keywords: single-index model, sparsity, regression estimation, PAC-Bayesian, oracle inequality, reversible jump Markov chain Monte Carlo method</p><p>6 0.48071149 <a title="102-lsi-6" href="./jmlr-2013-Variable_Selection_in_High-Dimension_with_Random_Designs_and_Orthogonal_Matching_Pursuit.html">119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</a></p>
<p>7 0.46381864 <a title="102-lsi-7" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>8 0.45415246 <a title="102-lsi-8" href="./jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning.html">35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</a></p>
<p>9 0.4211185 <a title="102-lsi-9" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>10 0.41473851 <a title="102-lsi-10" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>11 0.41443411 <a title="102-lsi-11" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<p>12 0.41361505 <a title="102-lsi-12" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>13 0.40691224 <a title="102-lsi-13" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>14 0.39728299 <a title="102-lsi-14" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>15 0.3928906 <a title="102-lsi-15" href="./jmlr-2013-CODA%3A_High_Dimensional_Copula_Discriminant_Analysis.html">20 jmlr-2013-CODA: High Dimensional Copula Discriminant Analysis</a></p>
<p>16 0.3788029 <a title="102-lsi-16" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>17 0.3767978 <a title="102-lsi-17" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>18 0.37485066 <a title="102-lsi-18" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>19 0.36899623 <a title="102-lsi-19" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>20 0.35535991 <a title="102-lsi-20" href="./jmlr-2013-Risk_Bounds_of_Learning_Processes_for_L%C3%A9vy_Processes.html">97 jmlr-2013-Risk Bounds of Learning Processes for Lévy Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.03), (5, 0.129), (6, 0.033), (10, 0.064), (14, 0.011), (20, 0.021), (23, 0.052), (51, 0.316), (68, 0.043), (70, 0.046), (75, 0.029), (85, 0.07), (87, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71907306 <a title="102-lda-1" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>Author: Tingni Sun, Cun-Hui Zhang</p><p>Abstract: We propose a new method of learning a sparse nonnegative-deﬁnite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm ﬁrst estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other ℓ1 regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the ℓ1 and spectrum norms of the target inverse matrix diverges to inﬁnity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso. Keywords: precision matrix, concentration matrix, inverse matrix, graphical model, scaled Lasso, linear regression, spectrum norm</p><p>2 0.49367994 <a title="102-lda-2" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>Author: Sébastien Gerchinovitz</p><p>Abstract: We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T . We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same ﬂavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with ﬁxed design. Keywords: sparsity, online linear regression, individual sequences, adaptive regret bounds</p><p>3 0.48364568 <a title="102-lda-3" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>Author: Yuchen Zhang, John C. Duchi, Martin J. Wainwright</p><p>Abstract: We analyze two communication-efﬁcient algorithms for distributed optimization in statistical settings involving large-scale data sets. The ﬁrst algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves √ mean-squared error (MSE) that decays as O (N −1 + (N/m)−2 ). Whenever m ≤ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O (N −1 + (N/m)−3 ), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O (N −1 + (N/m)−3/2 ), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efﬁciently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 108 samples and d ≈ 740,000 covariates. Keywords: distributed learning, stochastic optimization, averaging, subsampling</p><p>4 0.47334471 <a title="102-lda-4" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>Author: Takafumi Kanamori, Akiko Takeda, Taiji Suzuki</p><p>Abstract: There are two main approaches to binary classiÄ?Ĺš cation problems: the loss function approach and the uncertainty set approach. The loss function approach is widely used in real-world data analysis. Statistical decision theory has been used to elucidate its properties such as statistical consistency. Conditional probabilities can also be estimated by using the minimum solution of the loss function. In the uncertainty set approach, an uncertainty set is deÄ?Ĺš ned for each binary label from training samples. The best separating hyperplane between the two uncertainty sets is used as the decision function. Although the uncertainty set approach provides an intuitive understanding of learning algorithms, its statistical properties have not been sufÄ?Ĺš ciently studied. In this paper, we show that the uncertainty set is deeply connected with the convex conjugate of a loss function. On the basis of the conjugate relation, we propose a way of revising the uncertainty set approach so that it will have good statistical properties such as statistical consistency. We also introduce statistical models corresponding to uncertainty sets in order to estimate conditional probabilities. Finally, we present numerical experiments, verifying that the learning with revised uncertainty sets improves the prediction accuracy. Keywords: loss function, uncertainty set, convex conjugate, consistency</p><p>5 0.4710364 <a title="102-lda-5" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>Author: Tony Cai, Wen-Xin Zhou</p><p>Abstract: We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed. Keywords: 1-bit matrix completion, low-rank matrix, max-norm, trace-norm, constrained optimization, maximum likelihood estimate, optimal rate of convergence</p><p>6 0.46895033 <a title="102-lda-6" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>7 0.46810952 <a title="102-lda-7" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>8 0.46499884 <a title="102-lda-8" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>9 0.46453857 <a title="102-lda-9" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>10 0.46202314 <a title="102-lda-10" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>11 0.46154198 <a title="102-lda-11" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>12 0.45976633 <a title="102-lda-12" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>13 0.45919096 <a title="102-lda-13" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>14 0.45911607 <a title="102-lda-14" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>15 0.45768228 <a title="102-lda-15" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>16 0.45736286 <a title="102-lda-16" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>17 0.45624328 <a title="102-lda-17" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>18 0.45618832 <a title="102-lda-18" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>19 0.45538864 <a title="102-lda-19" href="./jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications.html">18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</a></p>
<p>20 0.45463797 <a title="102-lda-20" href="./jmlr-2013-Stochastic_Dual_Coordinate_Ascent_Methods_for_Regularized_Loss_Minimization.html">107 jmlr-2013-Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
