<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-30" href="#">jmlr2013-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</h1>
<br/><p>Source: <a title="jmlr-2013-30-pdf" href="http://jmlr.org/papers/volume14/bottou13a/bottou13a.pdf">pdf</a></p><p>Author: Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X. Charles, D. Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson</p><p>Abstract: This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine. Keywords: causation, counterfactual reasoning, computational advertising</p><p>Reference: <a title="jmlr-2013-30-reference" href="../jmlr2013_reference/jmlr-2013-Counterfactual_Reasoning_and_Learning_Systems%3A_The_Example_of_Computational_Advertising_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 For instance, the placement of advertisement on the result pages of Internet search engines depend on the bids of advertisers and on scores computed by statistical machine learning systems. [sent-21, score-0.684]
</p><p>2 Using the ad placement example as a model of our problem class, we therefore argue that the language and the methods of causal inference provide ﬂexible means to describe such complex machine learning systems and give sound answers to the practical questions facing the designer of such a system. [sent-41, score-0.647]
</p><p>3 Section 4 centers on formulating and answering counterfactual questions such as “how would the system have performed during the data collection period if certain interventions had been carried out on the system ? [sent-53, score-0.621]
</p><p>4 Whenever a user visits a publisher web page, an advertisement placement engine runs an auction in real time in order to select winning ads, determine where to display them in the page, and compute the prices charged to advertisers, should the user click on their ad. [sent-78, score-0.919]
</p><p>5 In the case of the ad placement problem, the publisher runs multiple auctions and sells opportunities to receive a click. [sent-82, score-0.578]
</p><p>6 Ads placed in the mainline are more likely to be noticed, increasing both the chances of a click if the ad is relevant and the risk of annoying the user if the ad is not relevant. [sent-87, score-0.943]
</p><p>7 The ad placement engine ﬁrst determines all eligible ads a1 . [sent-89, score-0.666]
</p><p>8 ) • The advertiser payment associated with a user click is computed using the generalized second price (GSP) rule: the advertiser pays the smallest bid that it could have entered without changing the solution of the discrete maximization problem, all other bids remaining equal. [sent-104, score-0.749]
</p><p>9 The ad placement engine should therefore be viewed as a complex learning system interacting with both users and advertisers. [sent-116, score-0.567]
</p><p>10 2 Controlled Experiments The designer of such an ad placement engine faces the fundamental question of testing whether a proposed modiﬁcation of the ad placement engine results in an improvement of the operational performance of the system. [sent-118, score-0.982]
</p><p>11 Whereas it is easy to split users into treatment and control groups, splitting advertisers into treatment and control groups demands special attention because each auction involves multiple advertisers (Charles et al. [sent-126, score-0.672]
</p><p>12 2) randomly distribute users between treatment and control groups, and this is also why, in the case of an ad placement engine, we should be somehow concerned by the practical impossibility to randomly distribute both users and advertisers. [sent-166, score-0.602]
</p><p>13 The overall counts suggest that the click-through rate of the second mainline ad increases when the click probability estimate q1 of the top ad is high. [sent-175, score-0.901]
</p><p>14 4 Confounding Data in Ad Placement Let us return to the question of assessing the value of passing a new input signal to the ad placement engine click prediction model. [sent-180, score-0.667]
</p><p>15 1 outlines a placement method where the click probability estimates qi,p (x) depend on the ad and the position we consider, but do not depend on other ads displayed on the page. [sent-182, score-0.783]
</p><p>16 We now consider replacing this model by a new model that additionally uses the estimated click probability of the top mainline ad to estimate the click probability of the second mainline ad (Figure 1). [sent-183, score-1.394]
</p><p>17 Let q1 and q2 denote the click probability estimates computed by the existing model for respectively the top mainline ad and the second mainline ad. [sent-186, score-1.001]
</p><p>18 4 reports the click counts and frequencies observed on the second mainline ad in each group. [sent-191, score-0.697]
</p><p>19 Although the overall numbers show that users click more often on the second mainline ad when the top mainline ad has a high click probability estimate q1 , this conclusion is reversed when we further split the data according to the click probability estimate q2 of the second mainline ad. [sent-192, score-1.943]
</p><p>20 The overall click counts show that the actual click-through rate of the second mainline ad is positively correlated with the click probability estimate on the top mainline ad. [sent-194, score-1.19]
</p><p>21 Meanwhile, the click probability estimate q2 returned by the current model for the second mainline ad also depend on the query and therefore the user intention. [sent-199, score-0.777]
</p><p>22 suggests that a frequently clicked top mainline ad has a negative impact on the click-through rate of the second mainline ad. [sent-202, score-0.812]
</p><p>23 This would decrease the click probability estimates for ads placed in the second mainline position on commercial search pages. [sent-204, score-0.653]
</p><p>24 Although we could tune the reserve prices to compensate this unfortunate effect, nothing in this data tells us where the performance of the ad placement engine will land. [sent-207, score-0.664]
</p><p>25 The set of eligible ads a and the corresponding bids b are then derived from the query x and the ad inventory v supplied by the advertisers. [sent-248, score-0.585]
</p><p>26 For instance, the user intent u and the ad inventory v in Figure 3 have temporal correlations because both users and advertisers worry about their budgets when the end of the month approaches. [sent-286, score-0.507]
</p><p>27 The ad placement structural equation model shown in Figure 2 only describes the causal dependencies for a single page and therefore cannot account for such effects. [sent-289, score-0.778]
</p><p>28 For instance, the ad placement structural equation model and the corresponding causal graph (ﬁgures 2 and 3) do not take user feedback or advertiser feedback into account. [sent-344, score-0.977]
</p><p>29 For instance, assuming that the prices c are discrete, the ad placement structural equation model shown in Figure 2 reduces to a contextual bandit problem with context (u, v), actions (s, c) and reward z. [sent-401, score-0.753]
</p><p>30 ” The answer of this counterfactual question is of course a counterfactual statement that describes the system performance subject to a condition that did not happen. [sent-434, score-0.981]
</p><p>31 2) in a counterfactual question: “How would the system have performed if, when the data was collected, we had replaced model M by model M ∗ without incurring user or advertiser reactions? [sent-440, score-0.698]
</p><p>32 The remaining text in this section explains how we can answer certain counterfactual questions using data collected in the past. [sent-449, score-0.531]
</p><p>33 More precisely, we seek to estimate performance metrics that can be expressed as expectations with respect to the distribution that would have been observed if the counterfactual conditions had been in force. [sent-450, score-0.534]
</p><p>34 More generally, to estimate a counterfactual by replaying a data set, we need to know all the functional dependencies associated with all causal paths connecting the intervention point to the measurement point. [sent-460, score-0.766]
</p><p>35 Although counterfactual expectations can be viewed as expectations of unit-level counterfactuals (Pearl, 2009, Definition 4), they elude the semantic subtleties of unit-level counterfactuals and can be measured with randomized experiments (see Section 4. [sent-463, score-0.588]
</p><p>36 The average number of ad clicks per page is often called click yield. [sent-480, score-0.556]
</p><p>37 Increasing the click yield usually beneﬁts both the advertiser and the publisher, whereas increasing the revenue per page often beneﬁts the publisher at the expense of the advertiser. [sent-481, score-0.557]
</p><p>38 Let ω be a shorthand for all variables appearing in the Markov factorization of the ad placement structural equation model, P(ω) = P(u, v) P( x | u) P( a | x, v) P( b | x, v) P( q | x, a) × P( s | a, q, b) P( c | a, q, b) P( y | s, u) P( z | y, c) . [sent-483, score-0.6]
</p><p>39 This intervention amounts to replacing the actual factor P( q | x, a) by a counterfactual factor P∗ ( q | x, a) in the Markov factorization. [sent-489, score-0.539]
</p><p>40 In general, we can use importance sampling to estimate the counterfactual expectation of any quantity ℓ(ω) : Y∗ =  ω  ℓ(ω) P∗ (ω) =  ω  ℓ(ω)  P∗ (ω) 1 n P(ω) ≈ ∑ ℓ(ωi ) wi P(ω) n i=1  (5)  with weights wi = w(ωi ) =  factors appearing in P∗ (ωi ) but not in P(ωi ) P∗ (ωi ) = . [sent-500, score-0.525]
</p><p>41 We must therefore design cost-effective randomized experiments that yield enough information to estimate many interesting counterfactual expectations with sufﬁcient accuracy. [sent-509, score-0.506]
</p><p>42 This problem cannot be solved without answering the conﬁdence interval question: given data collected with a certain level of randomization, with which accuracy can we estimate a given counterfactual expectation? [sent-510, score-0.57]
</p><p>43 5 Interpreting the Conﬁdence Intervals The estimation of the counterfactual expectation Y ∗ can be inaccurate because the sample size is insufﬁcient or because the sampling distribution P(ω) does not sufﬁciently explore the counterfactual conditions of interest. [sent-548, score-0.948]
</p><p>44 A large inner conﬁdence interval suggests that the most practical way to improve the estimate is to adjust the data collection experiment in order to obtain a better coverage of the counterfactual conditions of interest. [sent-553, score-0.546]
</p><p>45 6 Experimenting with Mainline Reserves We return to the ad placement problem to illustrate the reweighting approach and the interpretation of the conﬁdence intervals. [sent-558, score-0.526]
</p><p>46 Manipulating the reserves R p (x) associated with the mainline positions (Figure 1) controls which ads are prominently displayed in the mainline or displaced into the sidebar. [sent-559, score-0.873]
</p><p>47 We seek in this section to answer counterfactual questions of the form: “How would the ad placement system have performed if we had scaled the mainline reserves by a constant factor ρ, without incurring user or advertiser reactions? [sent-560, score-1.541]
</p><p>48 The curves bound 95% conﬁdence intervals on the variations of the average number of mainline ads displayed per page, the average number of ad clicks per page, 8. [sent-579, score-0.868]
</p><p>49 The curves delimit 95% conﬁdence intervals for the metrics we would have observed if we had increased the mainline reserves by the percentage shown on the horizontal axis. [sent-583, score-0.518]
</p><p>50 In order to validate the accuracy of these counterfactual estimates, a second trafﬁc bucket of equal size was conﬁgured with mainline reserves reduced by about 18%. [sent-591, score-0.883]
</p><p>51 7 More on Mainline Reserves The main beneﬁt of the counterfactual estimation approach is the ability to use the same data to answer a broad range of counterfactual questions. [sent-601, score-0.948]
</p><p>52 The data collected using the simple mainline reserve randomization can also be used to estimate what would have been measured if we had increased all the mainline reserves by a query-dependent multiplier ρ∗ (x). [sent-613, score-0.961]
</p><p>53 = P( qi | xi , ai ) p(mi ; µ, σ) Considerably broader ranges of counterfactual questions can be answered when data is collected using randomization schemes that explore more dimensions. [sent-615, score-0.562]
</p><p>54 For instance, in the case of the ad placement problem, we could apply an independent random multiplier for each score instead of applying a single random multiplier to the mainline reserves only. [sent-616, score-0.923]
</p><p>55 There are many practical situations in which one is only interested in limited aspects of the ad placement policy involving continuous parameters such as click prices or reserves. [sent-635, score-0.728]
</p><p>56 3230  C OUNTERFACTUAL R EASONING AND L EARNING S YSTEMS  Finally, the causal framework allows us to easily formulate counterfactual questions that pertain to the practical ad placement problem and yet differ considerably in complexity and exploration requirements. [sent-639, score-1.121]
</p><p>57 Structure This section shows how the structure of the causal graph reveals many ways to leverage a priori knowledge and improve the accuracy of our counterfactual estimates. [sent-643, score-0.685]
</p><p>58 We can exploit this knowledge by pretending that the reserve was drawn from the counterfactual distribution P∗ ( q | xi , ai ) instead of the actual distribution P( q | xi , ai ). [sent-652, score-0.594]
</p><p>59 For instance, we know that users make click decisions without knowing which scores were computed by the ad placement engine, and without knowing the prices charged to advertisers. [sent-658, score-0.77]
</p><p>60 The ad placement causal graph encodes this knowledge by showing the clicks y as direct effects of the user intent u and the ad slate s. [sent-659, score-1.067]
</p><p>61 Because the causal graph has this special structure, we can simplify both the actual and counterfactual Markov factorizations (2) (3) without eliminating the variable y whose expectation is sought. [sent-661, score-0.661]
</p><p>62 We can estimate the counterfactual click yield Y ∗ using these simpliﬁed factorizations: Y∗ = ≈  y P∗ (u, v, x, a, b, s, y) =  y  P∗ ( s | x, a, b) P(u, v, x, a, b, s, y) P( s | x, a, b)  1 n P∗ ( si | xi , ai , bi ) yi . [sent-670, score-0.663]
</p><p>63 inner conﬁdence intervals signiﬁcantly extends the range of mainline reserve multipliers for which we can compute accurate counterfactual expectations using this same data. [sent-678, score-1.044]
</p><p>64 The weight w(ωi ) is the ratio of the probabilities of the observed ad slate si under the counterfactual and actual multiplier distributions. [sent-690, score-0.773]
</p><p>65 Therefore the values υi of the invariant variables sampled during the actual experiment are also representative of the distribution of the invariant variables under the counterfactual conditions. [sent-698, score-0.53]
</p><p>66 We can leverage a priori knowledge to construct a predictor ζ(ω) of the quantity ℓ(ω) whose counterfactual expectation Y ∗ is sought. [sent-699, score-0.54]
</p><p>67 n i=1 i=1  (17)  The ﬁrst term in this sum represents the counterfactual expectation of the predictor and can be accurately estimated by averaging the simulated counterfactual samples ζ∗ without resorting to poi tentially large importance weights. [sent-703, score-1.016]
</p><p>68 For instance, in order to estimate the counterfactual performance of the ad placement system, we can easily use a predictor that runs the ad auction and simulate the user clicks using a click probability model trained ofﬂine. [sent-712, score-1.598]
</p><p>69 We can estimate the counterfactual expectation Y ∗ of the number of clicks per page as the sum of the counterfactual expectations of a predictor ζ, which is easy to estimate by replaying empirical data, and y − ζ, which has to be estimated by importance sampling but has reduced variance. [sent-715, score-1.251]
</p><p>70 Figure 18: The two plots show the hourly click yield for two variants of the ad placement engine. [sent-716, score-0.623]
</p><p>71 Appendix C provide details on the computation of conﬁdence intervals for estimators of the counterfactual differences. [sent-727, score-0.555]
</p><p>72 Appendix D shows how the same approach can be used to compute counterfactual derivatives that describe the response of the system to very small interventions. [sent-728, score-0.534]
</p><p>73 For instance, we might want to know what the performance of the ad placement engine would have been if we had used different values for the parameter θ of the click scoring model. [sent-735, score-0.667]
</p><p>74 3 Tuning Ad Placement Auctions We now present an application of this learning principle to the optimization of auction tuning parameters in the ad placement engine. [sent-784, score-0.528]
</p><p>75 K}, and, in order to avoid negative user or advertiser reactions, we seek the auction tuning parameters αk and ρk that maximize an estimate of the advertisement value10 subject to a global constraint on the average number of ads displayed in the mainline. [sent-799, score-0.514]
</p><p>76 We then use the collected data to estimate bounds on the counterfactual expectations of the advertiser value and the counterfactual expectation of the number of mainline ads per page. [sent-802, score-1.65]
</p><p>77 The counterfactual approach described here avoids the problem because it does not rely on a click prediction model to simulate users. [sent-808, score-0.663]
</p><p>78 Instead it estimates the counterfactual performance of the system using the actual behavior of the users collected under moderate randomization. [sent-809, score-0.62]
</p><p>79 For instance, in the contextual bandit formulation of the ad placement problem outlined in Section 3. [sent-827, score-0.537]
</p><p>80 5, actions are pairs (s, c) describing the ad slate s and the corresponding click prices c, policies select actions by combining individual ad scores in very speciﬁc ways, and actions determine the rewards through very speciﬁc mechanisms. [sent-828, score-0.743]
</p><p>81 The isolation assumption is in fact a component of the counterfactual conditions under investigation. [sent-838, score-0.53]
</p><p>82 determine how the ad placement system would have performed if we had changed the mainline reserves without incurring a reaction from the users or the advertisers. [sent-841, score-0.932]
</p><p>83 1 Rational Advertisers The ad placement system is an example of game where each actor furthers his or her interests by controlling some aspects of the system: the publisher controls the placement engine parameters, the advertisers control the bids, and the users control the clicks. [sent-856, score-1.104]
</p><p>84 As an example of the general quasi-static approach, this section focuses on the reaction of rational advertisers to small changes of the scoring functions driving the ad placement system. [sent-857, score-0.639]
</p><p>85 ables ya in the structural equation model represents the number of clicks received by ads associated with bid ba . [sent-873, score-0.61]
</p><p>86 The advertisers select their bids ba according to their anticipated impact on the number of resulting clicks ya and on their cost za . [sent-875, score-0.685]
</p><p>87 However, after observing the operation of the stationary ad placement system for a sufﬁciently long time, it is reasonable to assume that the most active advertisers have tried small bid variations and have chosen locally optimal ones. [sent-899, score-0.737]
</p><p>88 Such quantities can be estimated by randomizing the bids and computing on-policy counterfactual derivatives as explained in appendix D. [sent-906, score-0.656]
</p><p>89 Unfortunately, the publisher is not allowed to directly randomize the bids because the advertisers expect to pay prices computed using the bid they have speciﬁed and not the potentially higher bids resulting from the randomization. [sent-908, score-0.748]
</p><p>90 • Some advertisers attempt to capture all the available ad opportunities by placing extremely high bids and hoping to pay reasonable prices thanks to the generalized second price rule. [sent-920, score-0.63]
</p><p>91 Similarly, advertisers that place extremely high bids are probably underestimating the risk to occasionally experience a very high click price. [sent-927, score-0.549]
</p><p>92 Assuming that the other advertisers leave their bids unchanged, we can estimate how the active advertisers adjust their bids in response to an inﬁnitesimal change dθ of the scoring model parameters. [sent-931, score-0.72]
</p><p>93 This expression can then be used to estimate how any counterfactual expectation Y of interest changes when the publisher applies an inﬁnitesimal change dθ to the scoring parameter θ and the active advertisers A rationally adjust their bids ba in response: dY =  ∂Y ∂Y + ∑ Ξa ∂θ ∂ba a  dθ . [sent-939, score-1.021]
</p><p>94 4 Discussion The rational advertiser assumption is the cornerstone of seminal works describing simpliﬁed variants of the ad placement problem using auction theory (Varian, 2007; Edelman et al. [sent-951, score-0.677]
</p><p>95 We believe that our counterfactual reasoning framework is best viewed as a modular toolkit that lets us apply insights from auction theory and machine learning to problems that are far more complex than those studied in any single paper. [sent-955, score-0.568]
</p><p>96 Conclusion Using the ad placement example, this work demonstrates the central role of causal inference (Pearl, 2000; Spirtes et al. [sent-964, score-0.621]
</p><p>97 2 Inner Conﬁdence Interval Inner conﬁdence intervals are derived from inequality (14) which bounds the difference between the ¯ counterfactual expectation Y ∗ and the clipped expectation Y ∗ : ¯ ¯ 0 ≤ Y ∗ − Y ∗ ≤ M (1 − W ∗ ) . [sent-1074, score-0.611]
</p><p>98 Let ζ(υ) be a known function believed to be a good predictor of the quantity ℓ(ω) whose counterfactual expectation is sought. [sent-1089, score-0.516]
</p><p>99 The rest of this appendix describes how to construct conﬁdence intervals for the estimation of counterfactual differences. [sent-1096, score-0.555]
</p><p>100 2 Conﬁdence Intervals for Counterfactual Differences We now describe how to leverage invariant predictors in order to construct tighter conﬁdence intervals for the difference of two counterfactual expectations. [sent-1110, score-0.579]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('counterfactual', 0.474), ('mainline', 0.304), ('placement', 0.23), ('advertisers', 0.205), ('ad', 0.204), ('click', 0.189), ('causal', 0.187), ('ads', 0.16), ('bids', 0.155), ('advertiser', 0.149), ('ottou', 0.135), ('easoning', 0.13), ('ounterfactual', 0.13), ('ystems', 0.13), ('reserve', 0.12), ('clicks', 0.119), ('dence', 0.119), ('reserves', 0.105), ('eters', 0.104), ('publisher', 0.102), ('auction', 0.094), ('reweighting', 0.092), ('equilibrium', 0.089), ('ba', 0.085), ('structural', 0.084), ('intervals', 0.081), ('interventions', 0.081), ('revenue', 0.073), ('advertisement', 0.069), ('ya', 0.068), ('prices', 0.066), ('intervention', 0.065), ('bid', 0.065), ('al', 0.063), ('exogenous', 0.062), ('bandit', 0.06), ('confounding', 0.06), ('collected', 0.057), ('clipped', 0.056), ('isolation', 0.056), ('con', 0.056), ('treatment', 0.056), ('users', 0.056), ('slate', 0.055), ('za', 0.053), ('kidney', 0.05), ('earning', 0.045), ('reactions', 0.045), ('engine', 0.044), ('page', 0.044), ('contextual', 0.043), ('auctions', 0.042), ('user', 0.042), ('outer', 0.042), ('predictor', 0.042), ('web', 0.041), ('replaying', 0.04), ('squashing', 0.04), ('stones', 0.04), ('multiplier', 0.04), ('policy', 0.039), ('interval', 0.039), ('satisfaction', 0.038), ('query', 0.038), ('reward', 0.037), ('bandits', 0.035), ('athey', 0.035), ('va', 0.034), ('system', 0.033), ('inner', 0.033), ('expectations', 0.032), ('randomization', 0.031), ('bernstein', 0.03), ('maurer', 0.03), ('reinforcement', 0.03), ('patients', 0.03), ('equation', 0.029), ('eligible', 0.028), ('metrics', 0.028), ('instance', 0.028), ('variables', 0.028), ('outcome', 0.027), ('derivatives', 0.027), ('importance', 0.026), ('exploration', 0.026), ('feedback', 0.026), ('effects', 0.026), ('designer', 0.026), ('yc', 0.026), ('scores', 0.025), ('causation', 0.025), ('bhi', 0.025), ('blo', 0.025), ('counterfactuals', 0.025), ('edelman', 0.025), ('nekipelov', 0.025), ('traf', 0.025), ('appearing', 0.025), ('action', 0.024), ('leverage', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="30-tfidf-1" href="./jmlr-2013-Counterfactual_Reasoning_and_Learning_Systems%3A_The_Example_of_Computational_Advertising.html">30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</a></p>
<p>Author: Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X. Charles, D. Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson</p><p>Abstract: This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine. Keywords: causation, counterfactual reasoning, computational advertising</p><p>2 0.12608305 <a title="30-tfidf-2" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>Author: Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer</p><p>Abstract: Randomized controlled experiments are often described as the most reliable tool available to scientists for discovering causal relationships among quantities of interest. However, it is often unclear how many and which different experiments are needed to identify the full (possibly cyclic) causal structure among some given (possibly causally insufﬁcient) set of variables. Recent results in the causal discovery literature have explored various identiﬁability criteria that depend on the assumptions one is able to make about the underlying causal process, but these criteria are not directly constructive for selecting the optimal set of experiments. Fortunately, many of the needed constructions already exist in the combinatorics literature, albeit under terminology which is unfamiliar to most of the causal discovery community. In this paper we translate the theoretical results and apply them to the concrete problem of experiment selection. For a variety of settings we give explicit constructions of the optimal set of experiments and adapt some of the general combinatorics results to answer questions relating to the problem of experiment selection. Keywords: causality, randomized experiments, experiment selection, separating systems, completely separating systems, cut-coverings</p><p>3 0.082129583 <a title="30-tfidf-3" href="./jmlr-2013-Ranked_Bandits_in_Metric_Spaces%3A_Learning_Diverse_Rankings_over_Large_Document_Collections.html">94 jmlr-2013-Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections</a></p>
<p>Author: Aleksandrs Slivkins, Filip Radlinski, Sreenivas Gollapudi</p><p>Abstract: Most learning to rank research has assumed that the utility of different documents is independent, which results in learned ranking functions that return redundant results. The few approaches that avoid this have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisﬁed users, with several scalable algorithms that explicitly takes document similarity and ranking context into account. Our formulation is a non-trivial common generalization of two multi-armed bandit models from the literature: ranked bandits (Radlinski et al., 2008) and Lipschitz bandits (Kleinberg et al., 2008b). We present theoretical justiﬁcations for this approach, as well as a near-optimal algorithm. Our evaluation adds optimizations that improve empirical performance, and shows that our algorithms learn orders of magnitude more quickly than previous approaches. Keywords: online learning, clickthrough data, diversity, multi-armed bandits, contextual bandits, regret, metric spaces</p><p>4 0.050152663 <a title="30-tfidf-4" href="./jmlr-2013-Lower_Bounds_and_Selectivity_of_Weak-Consistent_Policies_in_Stochastic_Multi-Armed_Bandit_Problem.html">65 jmlr-2013-Lower Bounds and Selectivity of Weak-Consistent Policies in Stochastic Multi-Armed Bandit Problem</a></p>
<p>Author: Antoine Salomon, Jean-Yves Audibert, Issam El Alaoui</p><p>Abstract: This paper is devoted to regret lower bounds in the classical model of stochastic multi-armed bandit. A well-known result of Lai and Robbins, which has then been extended by Burnetas and Katehakis, has established the presence of a logarithmic bound for all consistent policies. We relax the notion of consistency, and exhibit a generalisation of the bound. We also study the existence of logarithmic bounds in general and in the case of Hannan consistency. Moreover, we prove that it is impossible to design an adaptive policy that would select the best of two algorithms by taking advantage of the properties of the environment. To get these results, we study variants of popular Upper Conﬁdence Bounds (UCB) policies. Keywords: stochastic bandits, regret lower bounds, consistency, selectivity, UCB policies 1. Introduction and Notations Multi-armed bandits are a classical way to illustrate the difﬁculty of decision making in the case of a dilemma between exploration and exploitation. The denomination of these models comes from an analogy with playing a slot machine with more than one arm. Each arm has a given (and unknown) reward distribution and, for a given number of rounds, the agent has to choose one of them. As the goal is to maximize the sum of rewards, each round decision consists in a trade-off between exploitation (i.e., playing the arm that has been the more lucrative so far) and exploration (i.e., testing another arm, hoping to discover an alternative that beats the current best choice). One possible application is clinical trial: a doctor wants to heal as many patients as possible, the patients arrive sequentially, and the effectiveness of each treatment is initially unknown (Thompson, 1933). Bandit problems have initially been studied by Robbins (1952), and since then they have been applied to many ﬁelds such as economics (Lamberton et al., 2004; Bergemann and Valimaki, 2008), games (Gelly and Wang, 2006), and optimisation (Kleinberg, 2005; Coquelin and Munos, 2007; Kleinberg et al., 2008; Bubeck et al., 2009). ∗. Also at Willow, CNRS/ENS/INRIA - UMR 8548. c 2013 Antoine Salomon, Jean-Yves Audibert and Issam El Alaoui. S ALOMON , AUDIBERT AND E L A LAOUI 1.1 Setting In this paper, we consider the following model. A stochastic multi-armed bandit problem is deﬁned by: • a number of rounds n, • a number of arms K ≥ 2, • an environment θ = (ν1 , · · · , νK ), where each νk (k ∈ {1, · · · , K}) is a real-valued measure that represents the distribution reward of arm k. The number of rounds n may or may not be known by the agent, but this will not affect the present study. We assume that rewards are bounded. Thus, for simplicity, each νk is a probability on [0, 1]. Environment θ is initially unknown by the agent but lies in some known set Θ. For the problem to be interesting, the agent should not have great knowledges of its environment, so that Θ should not be too small and/or only contain too trivial distributions such as Dirac measures. To make it simple, we may assume that Θ contains all environments where each reward distribution is a Dirac distribution or a Bernoulli distribution. This will be acknowledged as Θ having the Dirac/Bernoulli property. For technical reason, we may also assume that Θ is of the form Θ1 × . . . × ΘK , meaning that Θk is the set of possible reward distributions of arm k. This will be acknowledged as Θ having the product property. The game is as follows. At each round (or time step) t = 1, · · · , n, the agent has to choose an arm It in the set {1, · · · , K}. This decision is based on past actions and observations, and the agent may also randomize his choice. Once the decision is made, the agent gets and observes a reward that is drawn from νIt independently from the past. Thus a policy (or strategy) can be described by a sequence (σt )t≥1 (or (σt )1≤t≤n if the number of rounds n is known) such that each σt is a mapping from the set {1, . . . , K}t−1 × [0, 1]t−1 of past decisions and rewards into the set of arm {1, . . . , K} (or into the set of probabilities on {1, . . . , K}, in case the agent randomizes his choices). For each arm k and all time step t, let Tk (t) = ∑ts=1 ½Is =k denote the sampling time, that is, the number of times arm k was pulled from round 1 to round t, and Xk,1 , Xk,2 , . . . , Xk,Tk (t) the corresponding sequence of rewards. We denote by Pθ the distribution on the probability space such that for any k ∈ {1, . . . , K}, the random variables Xk,1 , Xk,2 , . . . , Xk,n are i.i.d. realizations of νk , and such that these K sequences of random variables are independent. Let Eθ denote the associated expectation. Let µk = xdνk (x) be the mean reward of arm k. Introduce µ∗ = maxk∈{1,...,K} µk and ﬁx an arm ∗ ∈ argmax ∗ k k∈{1,...,K} µk , that is, k has the best expected reward. The agent aims at minimizing its regret, deﬁned as the difference between the cumulative reward he would have obtained by always drawing the best arm and the cumulative reward he actually received. Its regret is thus n n Rn = ∑ Xk∗ ,t − ∑ XIt ,TIt (t) . t=1 t=1 As most of the publications on this topic, we focus on the expected regret, for which one can check that: K E θ Rn = ∑ ∆k Eθ [Tk (n)], k=1 188 (1) L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS where ∆k is the optimality gap of arm k, deﬁned by ∆k = µ∗ − µk . We also deﬁne ∆ as the gap between the best arm and the second best arm, that is, ∆ := mink:∆k >0 ∆k . Other notions of regret exist in the literature. One of them is the quantity n max ∑ Xk,t − XIt ,TIt (t) , k t=1 which is mostly used in adversarial settings. Results and ideas we want to convey here are more suited to expected regret, and considering other deﬁnitions of regret would only bring some more technical intricacies. 1.2 Consistency and Regret Lower Bounds Former works have shown the existence of lower bounds on the expected regret of a large class of policies: intuitively, to perform well the agent has to explore all arms, and this requires a signiﬁcant amount of suboptimal choices. In this way, Lai and Robbins (1985) proved a lower bound of order log n in a particular parametric framework, and they also exhibited optimal policies. This work has then been extended by Burnetas and Katehakis (1996). Both papers deal with consistent policies, meaning that they only consider policies such that: ∀a > 0, ∀θ ∈ Θ, Eθ [Rn ] = o(na ). (2) Let us detail the bound of Burnetas and Katehakis, which is valid when Θ has the product property. Given an environment θ = (ν1 , · · · , νK ) and an arm k ∈ {1, . . . , K}, deﬁne: Dk (θ) := inf ˜ νk ∈Θk :E[˜ k ]>µ∗ ν ˜ KL(νk , νk ), where KL(ν, µ) denotes the Kullback-Leibler divergence of measures ν and µ. Now ﬁx a consistent policy and an environment θ ∈ Θ. If k is a suboptimal arm (i.e., µk = µ∗ ) such that 0 < Dk (θ) < ∞, then (1 − ε) log n ∀ε > 0, lim P Tk (n) ≥ = 1. n→+∞ Dk (θ) This readily implies that: lim inf n→+∞ Eθ [Tk (n)] 1 ≥ . log n Dk (θ) Thanks to Formula (1), it is then easy to deduce a lower bound of the expected regret. One contribution of this paper is to generalize the study of regret lower bounds, by considering weaker notions of consistency: α-consistency and Hannan consistency. We will deﬁne αconsistency (α ∈ [0, 1)) as a variant of Equation (2), where equality Eθ [Rn ] = o(na ) only holds for all a > α. We show that the logarithmic bound of Burnetas and Katehakis still holds, but coefﬁcient 1−α 1 Dk (θ) is turned into Dk (θ) . We also prove that the dependence of this new bound with respect to the term 1 − α is asymptotically optimal when n → +∞ (up to a constant). We will also consider the case of Hannan consistency. Indeed, any policy achieves at most an expected regret of order n: because of the equality ∑K Tk (n) = n and thanks to Equation (1), one k=1 can show that Eθ Rn ≤ n maxk ∆k . More intuitively, this comes from the fact that the average cost of pulling an arm k is a constant ∆k . As a consequence, it is natural to wonder what happens when 189 S ALOMON , AUDIBERT AND E L A LAOUI dealing with policies whose expected regret is only required to be o(n), which is equivalent to Hannan consistency. This condition is less restrictive than any of the previous notions of consistency. In this larger class of policy, we show that the lower bounds on the expected regret are no longer logarithmic, but can be much smaller. Finally, even if no logarithmic lower bound holds on the whole set Θ, we show that there necessarily exist some environments θ for which the expected regret is at least logarithmic. The latter result is actually valid without any assumptions on the considered policies, and only requires a simple property on Θ. 1.3 Selectivity As we exhibit new lower bounds, we want to know if it is possible to provide optimal policies that achieve these lower bounds, as it is the case in the classical class of consistent policies. We answer negatively to this question, and for this we solve the more general problem of selectivity. Given a set of policies, we deﬁne selectivity as the ability to perform at least as good as the policy that is best suited to the current environment θ. Still, such an ability can not be implemented. As a by-product it is not possible to design a procedure that would speciﬁcally adapt to some kinds of environments, for example by selecting a particular policy. This contribution is linked with selectivity in on-line learning problem with perfect information, commonly addressed by prediction with expert advice (see, e.g., Cesa-Bianchi et al., 1997). In this spirit, a closely related problem is the one of regret against the best strategy from a pool studied by Auer et al. (2003). The latter designed an algorithm in the context of adversarial/nonstochastic bandit whose decisions are based on a given number of recommendations (experts), which are themselves possibly the rewards received by a set of given policies. To a larger extent, model selection has been intensively studied in statistics, and is commonly solved by penalization methods (Mallows, 1973; Akaike, 1973; Schwarz, 1978). 1.4 UCB Policies Some of our results are obtained using particular Upper Conﬁdence Bound algorithms. These algorithms were introduced by Lai and Robbins (1985): they basically consist in computing an index for each arm, and selecting the arm with the greatest index. A simple and efﬁcient way to design such policies is as follows: choose each index as low as possible such that, conditional to past observations, it is an upper bound of the mean reward of the considered arm with high probability (or, say, with high conﬁdence level). This idea can be traced back to Agrawal (1995), and has been popularized by Auer et al. (2002), who notably described a policy called UCB1. In this policy, each index Bk,s,t is deﬁned by an arm k, a time step t, an integer s that indicates the number of times arm k has been pulled before round t, and is given by: ˆ Bk,s,t = Xk,s + 2 logt , s ˆ ˆ where Xk,s is the empirical mean of arm k after s pulls, that is, Xk,s = 1 ∑s Xk,u . s u=1 To summarize, UCB1 policy ﬁrst pulls each arm once and then, at each round t > K, selects an arm k that maximizes Bk,Tk (t−1),t . Note that, by means of Hoeffding’s inequality, the index Bk,Tk (t−1),t is indeed an upper bound of µk with high probability (i.e., the probability is greater than 1 − 1/t 4 ). ˆ Another way to understand this index is to interpret the empiric mean Xk,Tk (t−1) as an ”exploitation” 190 L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS term, and the square root 2 logt/s as an ”exploration” term (because the latter gradually increases when arm k is not selected). Policy UCB1 achieves the logarithmic bound (up to a multiplicative constant), as it was shown that: ∀θ ∈ Θ, ∀n ≥ 3, Eθ [Tk (n)] ≤ 12 K log n log n log n ≤ 12K . and Eθ Rn ≤ 12 ∑ 2 ∆ ∆k k=1 ∆k Audibert et al. (2009) studied some variants of UCB1 policy. Among them, one consists in changing the 2 logt in the exploration term into ρ logt, where ρ > 0. This can be interpreted as a way to tune exploration: the smaller ρ is, the better the policy will perform in simple environments where information is disclosed easily (for example when all reward distributions are Dirac measures). On the contrary, ρ has to be greater to face more challenging environments (typically when reward distributions are Bernoulli laws with close parameters). This policy, that we denote UCB(ρ), was proven by Audibert et al. to achieve the logarithmic bound when ρ > 1, and the optimality was also obtained when ρ > 1 for a variant of UCB(ρ). 2 Bubeck (2010) showed in his PhD dissertation that their ideas actually enable to prove optimality 1 of UCB(ρ) for ρ > 1 . Moreover, the case ρ = 2 corresponds to a conﬁdence level of 1 (because 2 t of Hoeffding’s inequality, as above), and several studies (Lai and Robbins, 1985; Agrawal, 1995; Burnetas and Katehakis, 1996; Audibert et al., 2009; Honda and Takemura, 2010) have shown that this level is critical. We complete these works by a precise study of UCB(ρ) when ρ ≤ 1 . We prove that UCB(ρ) 2 is (1 − 2ρ)-consistent and that it is not α-consistent for any α < 1 − 2ρ (in view of the deﬁnition above, this means that the expected regret is roughly of order n1−2ρ ). Thus it does not achieve the logarithmic bound, but it performs well in simple environments, for example, environments where all reward distributions are Dirac measures. Moreover, we exhibit expected regret bounds of general UCB policies, with the 2 logt in the exploration term of UCB1 replaced by an arbitrary function. We give sufﬁcient conditions for such policies to be Hannan consistent and, as mentioned before, ﬁnd that lower bounds need not be logarithmic any more. 1.5 Outline The paper is organized as follows: in Section 2, we give bounds on the expected regret of general 1 UCB policies and of UCB (ρ) (ρ ≤ 2 ), as preliminary results. In Section 3, we focus on α-consistent policies. Then, in Section 4, we study the problem of selectivity, and we conclude in Section 5 by general results on the existence of logarithmic lower bounds. Throughout the paper ⌈x⌉ denotes the smallest integer not less than x whereas ⌊x⌋ denotes the largest integer not greater than x, ½A stands for the indicator function of event A, Ber(p) is the Bernoulli law with parameter p, and δx is the Dirac measure centred on x. 2. Preliminary Results In this section, we estimate the expected regret of the paper. UCB 191 policies. This will be useful for the rest of S ALOMON , AUDIBERT AND E L A LAOUI 2.1 Bounds on the Expected Regret of General UCB Policies We ﬁrst study general UCB policies, deﬁned by: • Draw each arm once, • then, at each round t, draw an arm It ∈ argmax Bk,Tk (t−1),t , k∈{1,...,K} ˆ where Bk,s,t is deﬁned by Bk,s,t = Xk,s + creasing. fk (t) s and where functions fk (1 ≤ k ≤ K) are in- This deﬁnition is inspired by popular UCB1 algorithm, for which fk (t) = 2 logt for all k. The following lemma estimates the performances of UCB policies in simple environments, for which reward distributions are Dirac measures. Lemma 1 Let 0 ≤ b < a ≤ 1 and n ≥ 1. For θ = (δa , δb ), the random variable T2 (n) is uniformly 1 upper bounded by ∆2 f2 (n) + 1. Consequently, the expected regret of UCB is upper bounded by 1 ∆ f 2 (n) + 1. Proof In environment θ, best arm is arm 1 and ∆ = ∆2 = a − b. Let us ﬁrst prove the upper bound of the sampling time. The assertion is true for n = 1 and n = 2: the ﬁrst two rounds consists in 1 drawing each arm once, so that T2 (n) ≤ 1 ≤ ∆2 f2 (n) + 1 for n ∈ {1, 2}. If, by contradiction, the as1 1 sertion is false, then there exists t ≥ 3 such that T2 (t) > ∆2 f2 (t) + 1 and T2 (t − 1) ≤ ∆2 f2 (t − 1) + 1. Since f2 (t) ≥ f2 (t − 1), this leads to T2 (t) > T2 (t − 1), meaning that arm 2 is drawn at round t. Therefore, we have a + f1 (t) T1 (t−1) ≤ b+ f2 (t) T2 (t−1) , hence a − b = ∆ ≤ f2 (t) T2 (t−1) , which implies 1 1 T2 (t − 1) ≤ ∆2 f2 (t) and thus T2 (t) ≤ ∆2 f2 (t) + 1. This contradicts the deﬁnition of t, and this ends the proof of the ﬁrst statement. The second statement is a direct consequence of Formula (1). Remark: throughout the paper, we will often use environments with K = 2 arms to provide bounds on expected regrets. However, we do not lose generality by doing so, because all corresponding proofs can be written almost identically to suit to any K ≥ 2, by simply assuming that the distribution of each arm k ≥ 3 is δ0 . We now give an upper bound of the expected sampling time of any arm such that ∆k > 0. This bound is valid in any environment, and not only those of the form (δa , δb ). Lemma 2 For any θ ∈ Θ and any β ∈ (0, 1), if ∆k > 0 the following upper bound holds: n Eθ [Tk (n)] ≤ u + where u = 4 fk (n) ∆2 k ∑ t=u+1 1+ logt 1 log( β ) . 192 e−2β fk (t) + e−2β fk∗ (t) , L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS An upper bound of the expected regret can be deduced from this lemma thanks to Formula 1. Proof The core of the proof is a peeling argument and the use of Hoeffding’s maximal inequality (see, e.g., Cesa-Bianchi and Lugosi, 2006, section A.1.3 for details). The idea is originally taken from Audibert et al. (2009), and the following is an adaptation of the proof of an upper bound of UCB (ρ) in the case ρ > 1 which can be found in S. Bubeck’s PhD dissertation. 2 First, let us notice that the policy selects an arm k such that ∆k > 0 at time step t ≤ n only if at least one of the three following equations holds: Bk∗ ,Tk∗ (t−1),t ≤ µ∗ , (3) fk (t) , Tk (t − 1) (4) ˆ Xk,t ≥ µk + Tk (t − 1) < 4 fk (n) . ∆2 k (5) Indeed, if none of the equations is true, then: fk (n) ˆ > Xk,t + Tk (t − 1) Bk∗ ,Tk∗ (t−1),t > µ∗ = µk + ∆k ≥ µk + 2 fk (t) = Bk,Tk (t−1),t , Tk (t − 1) which implies that arm k can not be chosen at time step t. We denote respectively by ξ1,t , ξ2,t and ξ3,t the events corresponding to Equations (3), (4) and (5). We have: n ∑ ½I =k Eθ [Tk (n)] = Eθ t n n ∑ ½{I =k}∩ξ = Eθ t t=1 + Eθ 3,t ∑ ½{I =k}\ξ t 3,t . t=1 t=1 n Let us show that the sum ∑t=1 ½{It =k}∩ξ3,t is almost surely lower than u := ⌈4 fk (n)/∆2 ⌉. We assume k m−1 n by contradiction that ∑t=1 ½{It =k}∩ξ3,t > u. Then there exists m < n such that ∑t=1 ½{It =k}∩ξ3,t < m 4 fk (n)/∆2 and ∑t=1 ½{It =k}∩ξ3,t = ⌈4 fk (n)/∆2 ⌉. Therefore, for any s > m, we have: k k m m t=1 t=1 Tk (s − 1) ≥ Tk (m) = ∑ ½{It =k} ≥ ∑ ½{It =k}∩ξ3,t = 4 fk (n) 4 fk (n) ≥ , 2 ∆k ∆2 k so that ½{Is =k}∩ξ3,s = 0. But then m n ∑ ½{I =k}∩ξ t t=1 3,t = ∑ ½{It =k}∩ξ3,t = t=1 4 fk (n) ≤ u, ∆2 k which is the contradiction expected. n n We also have ∑t=1 ½{It =k}\ξ3,t = ∑t=u+1 ½{It =k}\ξ3,t : since Tk (t − 1) ≤ t − 1, event ξ3,t always happens at time step t ∈ {1, . . . , u}. And then, since event {It = k} is included in ξ1,t ∪ ξ2,t ∪ ξ3,t : n Eθ ∑ ½{It =k}\ξ3,t ≤ Eθ t=u+1 n n t=u+1 t=u+1 ∑ ½ξ1,t ∪ξ2,t ≤ ∑ Pθ (ξ1,t ) + Pθ (ξ2,t ). 193 S ALOMON , AUDIBERT AND E L A LAOUI It remains to ﬁnd upper bounds of Pθ (ξ1,t ) and Pθ (ξ2,t ). To this aim, we apply the peeling argument with a geometric grid over the time interval [1,t]: fk∗ (t) ≤ µ∗ Tk∗ (t − 1) ˆ Pθ (ξ1,t ) = Pθ Bk∗ ,Tk∗ (t−1),t ≤ µ∗ = Pθ Xk∗ ,Tk∗ (t−1) + ˆ ≤ Pθ ∃s ∈ {1, · · · ,t}, Xk∗ ,s + fk∗ (t) ≤ µ∗ s logt log(1/β) ≤ ∑ j=0 ˆ Pθ ∃s : {β j+1t < s ≤ β j t}, Xk∗ ,s + logt log(1/β) ≤ ∑ j=0 s Pθ ∃s : {β j+1t < s ≤ β j t}, logt log(1/β) ≤ ∑ j=0 ∑ j=0 ∑ (Xk ,l − µ∗ ) ≤ − ∗ s fk∗ (t) β j+1t fk∗ (t) l=1 ∑ (µ∗ − Xk ,l ) ≥ t < s ≤ β j t}, logt log(1/β) = fk∗ (t) ≤ µ∗ s j ∗ β j+1t fk∗ (t) l=1 s Pθ max ∑ (µ∗ − Xk∗ ,l ) ≥ s≤β j t l=1 β j+1t fk∗ (t) . As the range of the random variables (Xk∗ ,l )1≤l≤s is [0, 1], Hoeffding’s maximal inequality gives:   2 logt log(1/β) β j+1t fk∗ (t)  2 logt  Pθ (ξ1,t ) ≤ + 1 e−2β fk∗ (t) . ≤ ∑ exp − jt β log(1/β) j=0 Similarly, we have: logt + 1 e−2β fk (t) , log(1/β) and the result follows from the combination of previous inequalities. Pθ (ξ2,t ) ≤ 2.2 Bounds on the Expected Regret of UCB(ρ), ρ ≤ We study the performances of UCB (ρ) 1 2 1 policy, with ρ ∈ (0, 2 ]. We recall that ρ logt s . UCB (ρ) is the UCB ˆ policy deﬁned by fk (t) = ρ log(t) for all k, that is, Bk,s,t = Xk,s + Small values of ρ can be interpreted as a low level of experimentation in the balance between exploration and exploitation. 1 Precise regret bound orders of UCB(ρ) when ρ ∈ (0, 2 ] are not documented in the literature. We ﬁrst give an upper bound of expected regret in simple environments, where it is supposed to perform well. As stated in the following proposition (which is a direct consequence of Lemma 1), the order of the bound is ρ log n . ∆ 194 L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS Lemma 3 Let 0 ≤ b < a ≤ 1 and n ≥ 1. For θ = (δa , δb ), the random variable T2 (n) is uniformly ρ upper bounded by ∆2 log(n) + 1. Consequently, the expected regret of UCB(ρ) is upper bounded by ρ ∆ log(n) + 1. One can show that the expected regret of UCB(ρ) is actually equivalent to ρ log n as n goes to ∆ inﬁnity. These good performances are compensated by poor results in more complex environments, as showed in the following theorem. We exhibit an expected regret upper bound which is valid for any θ ∈ Θ, and which is roughly of order n1−2ρ . We also show that this upper bound is asymptot1 ically optimal. Thus, with ρ ∈ (0, 2 ), UCB(ρ) does not perform enough exploration to achieve the logarithmic bound, as opposed to UCB(ρ) with ρ ∈ ( 1 , +∞). 2 1 Theorem 4 For any ρ ∈ (0, 2 ], any θ ∈ Θ and any β ∈ (0, 1), one has Eθ [Rn ] ≤ 4ρ log n ∑ ∆k + ∆k + 2∆k k:∆k >0 log n n1−2ρβ +1 . log(1/β) 1 − 2ρβ Moreover, if Θ has the Dirac/Bernoulli property, then for any ε > 0 there exists θ ∈ Θ such that Eθ [Rn ] lim n→+∞ n1−2ρ−ε = +∞. 1 1 The value ρ = 2 is critical, but we can deduce from the upper bound of this theorem that UCB( 2 ) is consistent in the classical sense of Lai and Robbins (1985) and of Burnetas and Katehakis (1996). log Proof We set u = 4ρ∆2 n . By Lemma 2 we get: k n Eθ [Tk (n)] ≤ u + 2 = u+2 ∑ logt + 1 e−2βρ log(t) log(1/β) ∑ logt 1 + 1 2ρβ log(1/β) t t=u+1 n t=u+1 n 1 ≤ u+2 log n +1 log(1/β) ≤ u+2 log n +1 log(1/β) 1+ ∑ ≤ u+2 log n +1 log(1/β) 1+ ≤ u+2 log n +1 . log(1/β) 1 − 2ρβ ∑ t 2ρβ t=1 n 1 t 2ρβ t=2 n−1 1 1−2ρβ n 1 t 2ρβ dt As usual, the upper bound of the expected regret follows from Formula (1). Now, let us show the lower bound. The result is obtained by considering an environment θ of the √ 1 form Ber( 1 ), δ 1 −∆ , where ∆ lies in (0, 2 ) and is such that 2ρ(1 + ∆)2 < 2ρ + ε. This notation is 2 2 obviously consistent with the deﬁnition of ∆ as an optimality gap. We set Tn := ⌈ ρ log n ⌉, and deﬁne ∆ the event ξn by: 1 1 ˆ ξn = X1,Tn < − (1 + √ )∆ . 2 ∆ 195 S ALOMON , AUDIBERT AND E L A LAOUI When event ξn occurs, one has for any t ∈ {Tn , . . . , n} ˆ X1,Tn + ρ logt Tn ˆ ≤ X1,Tn + ≤ √ ρ log n 1 1 < − (1 + √ )∆ + ∆ Tn 2 ∆ 1 − ∆, 2 so that arm 1 is chosen no more than Tn times by UCB(ρ) policy. Consequently: Eθ [T2 (n)] ≥ Pθ (ξn )(n − Tn ). We will now ﬁnd a lower bound of the probability of ξn thanks to Berry-Esseen inequality. We denote by C the corresponding constant, and by Φ the c.d.f. of the standard normal distribution. For convenience, we also deﬁne the following quantities: σ := E X1,1 − Using the fact that Φ(−x) = e− √ 2 β(x) 2πx 1 2 2 1 = , M3 := E 2 X1,1 − 1 2 3 1 = . 8 x2 with β(x) − − → 1, we have: −− x→+∞ ˆ √ X1,Tn − 1 √ 1 2 Tn ≤ −2 1 + √ ∆ Tn σ ∆ √ √ CM3 Φ −2(∆ + ∆) Tn − 3 √ σ Tn √ 2 exp −2(∆ + ∆) Tn √ √ CM3 √ √ √ β 2(∆ + ∆) Tn − 3 √ σ Tn 2 2π(∆ + ∆) Tn √ 2 ρ log n exp −2(∆ + ∆) ( ∆ + 1) √ √ CM3 √ √ √ β 2(∆ + ∆) Tn − 3 √ σ Tn 2 2π(∆ + ∆) Tn √ √ −2ρ(1+ ∆)2 exp −2(∆ + ∆)2 √ √ CM3 n √ √ √ β 2(∆ + ∆) Tn − 3 √ . Tn σ Tn 2 2π(∆ + ∆) Pθ (ξn ) = Pθ ≥ ≥ ≥ ≥ Previous calculations and Formula (1) give Eθ [Rn ] = ∆Eθ [T2 (n)] ≥ ∆Pθ (ξn )(n − Tn ), √ 1−2ρ(1+ ∆)2 so that we ﬁnally obtain a lower bound of Eθ [Rn ] of order n √log n . Therefore, nEθ [Rn ] is at least 1−2ρ−ε √ 2 √ 2 n2ρ+ε−2ρ(1+ ∆) √ of order . Since 2ρ + ε − 2ρ(1 + ∆) > 0, the numerator goes to inﬁnity, faster than log n √ log n. This concludes the proof. 196 L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS 3. Bounds on the Class α-consistent Policies In this section, our aim is to ﬁnd how the classical results of Lai and Robbins (1985) and of Burnetas and Katehakis (1996) can be generalised if we do not restrict the study to consistent policies. As a by-product, we will adapt their results to the present setting, which is simpler than their parametric frameworks. We recall that a policy is consistent if its expected regret is o(na ) for all a > 0 in all environments θ ∈ Θ. A natural way to relax this deﬁnition is the following. Deﬁnition 5 A policy is α-consistent if ∀a > α, ∀θ ∈ Θ, Eθ [Rn ] = o(na ). For example, we showed in the previous section that, for any ρ ∈ (0, 1 ], UCB(ρ) is (1−2ρ)-consistent 2 and not α-consistent if α < 1 − 2ρ. Note that the relevant range of α in this deﬁnition is [0, 1): the case α = 0 corresponds to the standard deﬁnition of consistency (so that throughout the paper the term ”consistent” also means ”0-consistent”), and any value α ≥ 1 is pointless as any policy is then α-consistent. Indeed, the expected regret of any policy is at most of order n. This also lead us to wonder what happens if we only require the expected regret to be o(n): ∀θ ∈ Θ, Eθ [Rn ] = o(n). This requirement corresponds to the deﬁnition of Hannan consistency. The class of Hannan consistent policies includes consistent policies and α-consistent policies for any α ∈ [0, 1). Some results about this class will be obtained in Section 5. We focus on regret lower bounds on α-consistent policies. We ﬁrst show that the main result of Burnetas and Katehakis can be extended in the following way. Theorem 6 Assume that Θ has the product property. Fix an α-consistent policy and θ ∈ Θ. If ∆k > 0 and if 0 < Dk (θ) < ∞, then ∀ε > 0, lim Pθ Tk (n) ≥ (1 − ε) n→+∞ (1 − α) log n = 1. Dk (θ) Consequently lim inf n→+∞ 1−α Eθ [Tk (n)] ≥ . log n Dk (θ) Remind that the lower bound of the expected regret is then deduced from Formula (1), and that coefﬁcient Dk (θ) is deﬁned by: Dk (θ) := inf ˜ νk ∈Θk :E[˜ k ]>µ∗ ν ˜ KL(νk , νk ), where KL(ν, µ) denotes the Kullback-Leibler divergence of measures ν and µ. Note that, as opposed to Burnetas and Katehakis (1996), there is no optimal policy in general (i.e., a policy that would achieve the lower bound in all environment θ). This can be explained intuitively as follows. If by contradiction there existed such a policy, its expected regret would be of order log n and consequently it would be (0-)consistent. Then the lower bounds in the case of 197 S ALOMON , AUDIBERT AND E L A LAOUI 1−α 0-consistency would necessarily hold. This can not happen if α > 0 because Dk (θ) < Dk1 . (θ) Nevertheless, this argument is not rigorous because the fact that the regret would be of order log n is only valid for environments θ such that 0 < Dk (θ) < ∞. The non-existence of optimal policies is implied by a stronger result of the next section (yet, only if α > 0.2). Proof We adapt Proposition 1 in Burnetas and Katehakis (1996) and its proof. Let us denote θ = (ν1 , . . . , νK ). We ﬁx ε > 0, and we want to show that: lim Pθ n→+∞ Set δ > 0 and δ′ > α such that ˜ that E[νk ] > µ∗ and 1−δ′ 1+δ Tk (n) (1 − ε)(1 − α) < log n Dk (θ) = 0. ˜ > (1 − ε)(1 − α). By deﬁnition of Dk (θ), there exists νk such ˜ Dk (θ) < KL(νk , νk ) < (1 + δ)Dk (θ). ˜ ˜ ˜ Let us set θ = (ν1 , . . . , νk−1 , νk , νk+1 , . . . , νK ). Environment θ lies in Θ by the product property and δ = KL(ν , ν ) and arm k is its best arm. Deﬁne I k ˜k ′ Aδ := n Tk (n) 1 − δ′ < δ log n I ′′ δ , Cn := log LTk (n) ≤ 1 − δ′′ log n , where δ′′ is such that α < δ′′ < δ′ and Lt is deﬁned by log Lt = ∑ts=1 log δ′ δ′ δ′′ δ′ dνk ˜ d νk (Xk,s ) . δ′′ Now, we show that Pθ (An ) = Pθ (An ∩Cn ) + Pθ (An \Cn ) − − → 0. −− n→+∞ On the one hand, one has: ′′ ′′ ′ ′′ ′ δ δ Pθ (Aδ ∩Cn ) ≤ n1−δ Pθ (Aδ ∩Cn ) ˜ n n ′′ ′ (6) ′′ ≤ n1−δ Pθ (Aδ ) = n1−δ Pθ n − Tk (n) > n − ˜ ˜ n 1 − δ′ Iδ log n ′′ ≤ n1−δ Eθ [n − Tk (n)] ˜ (7) ′ n − 1−δ log n Iδ ′′ = n−δ Eθ ∑K Tℓ (n) − Tk (n) ˜ l=1 ′ n − 1−δ Iδ log n n ′′ ≤ ∑ℓ=k n−δ Eθ [Tℓ (n)] ˜ ′ 1 − 1−δ Iδ log n n − − → 0. −− (8) n→+∞ ′ Equation (6) results from a partition of Aδ into events {Tk (n) = a}, 0 ≤ a < n ′′ 1−δ′ Iδ log n . Each event ′′ δ {Tk (n) = a} ∩ Cn equals {Tk (n) = a} ∩ ∏a dνk (Xk,s ) ≤ n1−δ and is measurable with respect s=1 d νk ˜ to Xk,1 , . . . , Xk,a and to Xℓ,1 , . . . , Xℓ,n (ℓ = k). Thus, ½{Tk (n)=a}∩Cn ′′ can be written as a function f of δ the latter r.v. and we have: ′′ δ Pθ {Tk (n) = a} ∩Cn = f (xk,s )1≤s≤a , (xℓ,s )ℓ=k,1≤s≤n ∏ ℓ=k 1≤s≤n ≤ f (xk,s )1≤s≤a , (xℓ,s )ℓ=k,1≤s≤n ∏ ℓ=k 1≤s≤n ′′ ′′ δ = n1−δ Pθ {Tk (n) = a} ∩Cn ˜ 198 . dνℓ (xℓ,s ) ∏ dνk (xk,s ) 1≤s≤a ′′ dνℓ (xℓ,s )n1−δ ∏ 1≤s≤a ˜ d νk (xk,s ) L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS Equation (7) is a consequence of Markov’s inequality, and the limit in (8) is a consequence of α-consistency. ′ On the other hand, we set bn := 1−δ log n, so that Iδ ′ ′′ δ Pθ (Aδ \Cn ) ≤ P n ≤ P max log L j > (1 − δ′′ ) log n j≤⌊bn ⌋ 1 1 − δ′′ max log L j > I δ bn j≤⌊bn ⌋ 1 − δ′ . This term tends to zero, as a consequence of the law of large numbers. ′ Now that Pθ (Aδ ) tends to zero, the conclusion results from n 1 − δ′ 1 − δ′ (1 − ε)(1 − α) > ≥ . δ (1 + δ)Dk (θ) Dk (θ) I The previous lower bound is asymptotically optimal with respect to its dependence in α, as claimed in the following proposition. Proposition 7 Assume that Θ has the Dirac/Bernoulli property. There exist θ ∈ Θ and a constant c > 0 such that, for any α ∈ [0, 1), there exists an α-consistent policy such that: lim inf n→+∞ Eθ [Tk (n)] ≤ c, (1 − α) log n for any k satisfying ∆k > 0. Proof In any environment of the form θ = (δa , δb ) with a = b, Lemma 3 implies the following estimate for UCB(ρ): Eθ Tk (n) ρ lim inf ≤ 2, n→+∞ log n ∆ where k = k∗ . Because 1−α ∈ (0, 1 ) and since UCB(ρ) is (1 − 2ρ)-consistent for any ρ ∈ (0, 1 ] (Theorem 4), we 2 2 2 1 obtain the result by choosing the α-consistent policy UCB( 1−α ) and by setting c = 2∆2 . 2 4. Selectivity In this section, we address the problem of selectivity. By selectivity, we mean the ability to adapt to the environment as and when rewards are observed. More precisely, a set of two (or more) policies is given. The one that performs the best depends on environment θ. We wonder if there exists an adaptive procedure that, given any environment θ, would be as good as any policy in the given set. Two major reasons motivate this study. On the one hand this question was answered by Burnetas and Katehakis within the class of consistent policies. They exhibits an asymptotically optimal policy, that is, that achieves the regret 199 S ALOMON , AUDIBERT AND E L A LAOUI lower bounds they have proven. The fact that a policy performs as best as any other one obviously solves the problem of selectivity. On the other hand, this problem has already been studied in the context of adversarial bandit by Auer et al. (2003). Their setting differs from our not only because their bandits are nonstochastic, but also because their adaptive procedure takes only into account a given number of recommendations, whereas in our setting the adaptation is supposed to come from observing rewards of the chosen arms (only one per time step). Nevertheless, one can wonder if an ”exponentially weighted forecasters” procedure like E XP 4 could be transposed to our context. The answer is negative, as stated in the following theorem. To avoid confusion, we make the notations of the regret and of sampling time more precise by adding the considered policy: under policy A , Rn and Tk (n) will be respectively denoted Rn (A ) and Tk (n, A ). ˜ Theorem 8 Let A be a consistent policy and let ρ be a real number in (0, 0.4). If Θ has the ˜ Dirac/Bernoulli property and the product property, there is no policy which can both beat A and UCB (ρ), that is: ∀A , ∃θ ∈ Θ, lim sup n→+∞ Eθ [Rn (A )] > 1. ˜ min(Eθ [Rn (A )], Eθ [Rn (UCB(ρ))]) Thus the existence of optimal policies does not hold when we extend the notion of consistency. Precisely, as UCB(ρ) is (1 − 2ρ)-consistent, we have shown that there is no optimal policy within the class of α-consistent policies, with α > 0.2. Consequently, there do not exist optimal policies in the class of Hannan consistent policies either. Moreover, Theorem 8 shows that methods that would be inspired by related literature in adversarial bandit can not apply to our framework. As we said, this impossibility may come from the fact that we can not observe at each time step the decisions and rewards of more than one algorithm. If we were able to observe a given set of policies from step to step, then it would be easy to beat them all: it would be sufﬁcient to aggregate all the observations and simply pull the arm with the greater empiric mean. The case where we only observe decisions (and not rewards) of a set of policies may be interesting, but is left outside of the scope of this paper. Proof Assume by contradiction that ∃A , ∀θ ∈ Θ, lim sup un,θ ≤ 1, n→+∞ [Rn where un,θ = min(E [R (Eθ)],E(A )](UCB(ρ))]) . ˜ θ n A θ [Rn For any θ, we have Eθ [Rn (A )] = Eθ [Rn (A )] ˜ ˜ Eθ [Rn (A )] ≤ un,θ Eθ [Rn (A )], ˜ Eθ [Rn (A )] (9) ˜ so that the fact that A is a consistent policy implies that A is also consistent. Consequently the lower bound of Theorem 6 also holds for policy A . For the rest of the proof, we focus on environments of the form θ = (δ0 , δ∆ ) with ∆ > 0. In this case, arm 2 is the best arm, so that we have to compute D1 (θ). On the one hand, we have: D1 (θ) = inf ˜ ν1 ∈Θ1 :E[˜ 1 ν ]>µ∗ ˜ KL(ν1 , ν1 ) = inf ˜ ν1 ∈Θ1 :E[˜ 1 ]>∆ ν 200 ˜ KL(δ0 , ν1 ) = inf ˜ ν1 ∈Θ1 :E[˜ 1 ]>∆ ν log 1 . ˜ ν1 (0) L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS ˜ ˜ As E[ν1 ] ≤ 1 − ν1 (0), we get: D1 (θ) ≥ inf ˜ ν1 ∈Θ1 :1−˜ 1 (0)≥∆ ν log 1 ˜ ν1 (0) ≥ log 1 . 1−∆ One the other hand, we have for any ε > 0: D1 (θ) ≤ KL(δ0 , Ber(∆ + ε)) = log Consequently D1 (θ) = log 1 1−∆ 1 1−∆−ε , and the lower bound of Theorem 6 reads: lim inf n→+∞ 1 Eθ [T1 (n, A )] . ≥ 1 log n log 1−∆ Just like Equation (9), we have: Eθ [Rn (A )] ≤ un,θ Eθ [Rn (UCB(ρ))]. Moreover, Lemma 3 provides: Eθ [Rn (UCB(ρ))] ≤ 1 + ρ log n . ∆ Now, by gathering the three previous inequalities and Formula (1), we get: 1 log 1 1−∆ ≤ lim inf n→+∞ Eθ [T1 (n, A )] Eθ [Rn (A )] = lim inf n→+∞ log n ∆ log n un,θ Eθ [Rn (UCB(ρ))] un,θ ρ log n 1+ ≤ lim inf n→+∞ ∆ log n ∆ log n ∆ ρun,θ un,θ ρ ρ + lim inf 2 = 2 lim inf un,θ ≤ 2 lim sup un,θ ≤ lim inf n→+∞ ∆ n→+∞ ∆ log n ∆ n→+∞ ∆ n→+∞ ρ . ≤ ∆2 ≤ lim inf n→+∞ This means that ρ has to be lower bounded by ∆2 , 1 log( 1−∆ ) but this is greater than 0.4 if ∆ = 0.75, hence the contradiction. Note that this proof gives a simple alternative to Theorem 4 to show that UCB(ρ) is not consistent (if ρ ≤ 0.4). Indeed if it were consistent, then in environment θ = (δ0 , δ∆ ) the same contradiction between the lower bound of Theorem 6 and the upper bound of Lemma 3 would hold. 5. General Bounds In this section, we study lower bounds on the expected regret with few requirements on Θ and on the class of policies. With a simple property on Θ but without any assumption on the policy, we show that there always exist logarithmic lower bounds for some environments θ. Then, still with a 201 S ALOMON , AUDIBERT AND E L A LAOUI simple property on Θ, we show that there exists a Hannan consistent policy for which the expected regret is sub-logarithmic for some environment θ. Note that the policy that always pulls arm 1 has a 0 expected regret in environments where arm 1 has the best mean reward, and an expected regret of order n in other environments. So, for this policy, expected regret is sub-logarithmic in some environments. Nevertheless, this policy is not Hannan consistent because its expected regret is not always o(n). 5.1 The Necessity of a Logarithmic Regret in Some Environments The necessity of a logarithmic regret in some environments can be explained by a simple sketch proof. Assume that the agent knows the number of rounds n, and that he balances exploration and exploitation in the following way: he ﬁrst pulls each arm s(n) times, and then selects the arm that has obtained the best empiric mean for the rest of the game. Denote by ps(n) the probability that the best arm does not have the best empiric mean after the exploration phase (i.e., after the ﬁrst Ks(n) rounds). The expected regret is then of the form c1 (1 − ps(n) )s(n) + c2 ps(n) n. (10) Indeed, if the agent manages to match the best arm then he only suffers the pulls of suboptimal arms during the exploration phase. That represents an expected regret of order s(n). If not, the number of pulls of suboptimal arms is of order n, and so is the expected regret. Now, let us approximate ps(n) . It has the same order as the probability that the best arm gets X ∗ −µ∗ an empiric mean lower than the second best mean reward. Moreover, k ,s(n) s(n) (where σ is σ ∗ ,1 ) has approximately a standard normal distribution by the central limit theorem. the variance of Xk Therefore, we have: ps(n) ≈ Pθ (Xk∗ ,s(n) ≤ µ∗ − ∆) = Pθ ≈ ≈  σ 1 1 √ exp − 2 2π ∆ s(n) Xk∗ ,s(n) − µ∗ σ  2 ∆ s(n)  σ s(n) ≤ − ∆ s(n) σ 1 σ ∆2 s(n) √ . exp − 2σ2 2π ∆ s(n) It follows that the expected regret has to be at least logarithmic. Indeed, to ensure that the second term c2 ps(n) n of Equation (10) is sub-logarithmic, s(n) has to be greater than log n. But then ﬁrst term c1 (1 − ps(n) )s(n) is greater than log n. Actually, the necessity of a logarithmic regret can be written as a consequence of Theorem 6. n Indeed, if we assume by contradiction that lim supn→+∞ Eθ Rn = 0 for all θ (i.e., Eθ Rn = o(log n)), log the considered policy is consistent. Consequently, Theorem 6 implies that lim sup n→+∞ E θ Rn E θ Rn ≥ lim inf > 0. n→+∞ log n log n Yet, this reasoning needs Θ having the product property, and conditions of the form 0 < Dk (θ) < ∞ also have to hold. The following proposition is a rigorous version of our sketch, and it shows that the necessity of a logarithmic lower bound can be based on a simple property on Θ. 202 L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS ˜ ˜ ˜ Proposition 9 Assume that there exist two environments θ = (ν1 , . . . , νK ) ∈ Θ, θ = (ν1 , . . . , νK ) ∈ Θ, and an arm k ∈ {1, . . . , K} such that 1. k has the best mean reward in environment θ, ˜ 2. k is not the winning arm in environment θ, ˜ 3. νk = νk and there exists η ∈ (0, 1) such that dνℓ ∏ d νℓ (Xℓ,1 ) ≥ η ˜ ℓ=k Pθ − a.s. ˜ (11) ˆ Then, for any policy, there exists θ ∈ Θ such that lim sup n→+∞ E θ Rn ˆ > 0. log n ˜ Let us explain the logic of the three conditions of the proposition. If νk = νk , and in case νk seems to be the reward distribution of arm k, then arm k has to be pulled often enough for the regret to be small if the environment is θ. Nevertheless, one has to explore other arms to know ˜ whether the environment is actually θ. Moreover, Inequality (11) makes sure that the distinction ˜ is tough to make: it ensures that pulling any arm ℓ = k gives a reward which is between θ and θ likely in both environments. Without such an assumption the problem may be very simple, and providing a logarithmic lower bound is hopeless. Indeed, the distinction between any pair of tricky ˜ environments (θ, θ) may be solved in only one pull of a given arm ℓ = k, that would almost surely give a reward that is possible in only one of the two environments. The third condition can be seen as an alternate version of condition 0 < Dk (θ) < ∞ in Theorem 6, though there is no logical connection with it. Finally, let us remark that one can check that any set Θ that has the Dirac/Bernoulli property satisﬁes the conditions of Proposition 9. Proof The proof consists in writing a proper version of Expression (10). To this aim we compute a lower bound of Eθ Rn , expressed as a function of Eθ Rn and of an arbitrary function g(n). ˜ ˜ ˜ In the following, ∆k denotes the optimality gap of arm k in environment θ. As event ∑ℓ=k Tℓ (n) ≤ g(n) is measurable with respect to Xℓ,1 , . . . , Xℓ,⌊g(n)⌋ (ℓ = k) and to Xk,1 , . . . , Xk,n , we also introduce the function q such that ½{∑ℓ=k Tℓ (n)≤g(n)} = q (Xℓ,s )ℓ=k, s=1..⌊g(n)⌋ , (Xk,s )s=1..n . 203 S ALOMON , AUDIBERT AND E L A LAOUI We have: ˜ ˜ ˜ Eθ Rn ≥ ∆k Eθ [Tk (n)] ≥ ∆k (n − g(n))Pθ (Tk (n) ≥ n − g(n)) ˜ ˜ (12) ˜ = ∆k (n − g(n))Pθ n − ∑ Tℓ (n) ≥ n − g(n) ˜ ℓ=k ˜ = ∆k (n − g(n))Pθ ˜ ˜ = ∆k (n − g(n)) ∑ Tℓ (n) ≤ g(n) ℓ=k q (xℓ,s )ℓ=k, s=1..⌊g(n)⌋ , (xk,s )s=1..n ˜ ˜ ∏ d νℓ (xℓ,s )∏d νk (xk,s ) ℓ=k s = 1..⌊g(n)⌋ s=1..n ˜ ≥ ∆k (n − g(n)) q (xℓ,s )ℓ=k, s=1..⌊g(n)⌋ , (xk,s )s=1..n η⌊g(n)⌋∏ dνℓ (xℓ,s )∏dνk (xk,s ) ℓ=k s = 1..⌊g(n)⌋ ˜ ≥ ∆k (n − g(n))ηg(n) q (xℓ,s )ℓ=k, s=1..⌊g(n)⌋ , (xk,s )s=1..n ∏ dνℓ (xℓ,s )∏dνk (xk,s ) ℓ=k s = 1..⌊g(n)⌋ ˜ = ∆k (n − g(n))ηg(n) Pθ (13) s=1..n s=1..n ∑ Tℓ (n) ≤ g(n) ℓ=k ˜ = ∆k (n − g(n))ηg(n) 1 − Pθ ∑ Tℓ (n) > g(n) ℓ=k ˜ ≥ ∆k (n − g(n))ηg(n) 1 − Eθ ∑ℓ=k Tℓ (n) g(n) (14) ˜ ≥ ∆k (n − g(n))ηg(n) 1 − Eθ ∑ℓ=k ∆ℓ Tℓ (n) ∆g(n) (15) E θ Rn ˜ ≥ ∆k (n − g(n))ηg(n) 1 − , ∆g(n) where the ﬁrst inequality of (12) is a consequence of Formula (1), the second inequality of (12) and inequality (14) come from Markov’s inequality, Inequality (13) is a consequence of (11), and Inequality (15) results from the fact that ∆ℓ ≥ ∆ for all ℓ. n θ −− Now, let us conclude. If Eθ Rn − − → 0, we set g(n) = 2E∆Rn , so that log n→+∞ g(n) ≤ min n − log n 2 , 2 log η for n large enough. Then, we have: √ − log n ˜ k n − g(n) ηg(n) ≥ ∆k n η 2 log η = ∆k n . ˜ ˜ E θ Rn ≥ ∆ ˜ 2 4 4 In particular, Eθ Rn ˜ −− log n − − → n→+∞ +∞, and the result follows. 204 L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS 5.2 Hannan Consistency We will prove that there exists a Hannan consistent policy such that there can not be a logarithmic lower bound for every environment θ of Θ. To this aim, we make use of general UCB policies again (cf. Section 2.1). Let us ﬁrst give sufﬁcient conditions on the fk for UCB policy to be Hannan consistent. Proposition 10 Assume that fk (n) = o(n) for all k ∈ {1, . . . , K}. Assume also that there exist γ > 1 2 and N ≥ 3 such that fk (n) ≥ γ log log n for all k ∈ {1, . . . , K} and for all n ≥ N. Then UCB is Hannan consistent. Proof Fix an arm k such that ∆k > 0 and choose β ∈ (0, 1) such that 2βγ > 1. By means of Lemma 2, we have for n large enough: n Eθ [Tk (n)] ≤ u + 2 ∑ 1+ t=u+1 logt 1 log( β ) e−2βγ log logt , k where u = 4 f∆(n) . 2 k Consequently, we have: n Eθ [Tk (n)] ≤ u + 2 ∑ t=2 1 1 1 + 1 (logt)2βγ−1 2βγ (logt) log( β ) . (16) n n 1 Sums of the form ∑t=2 (logt)c with c > 0 are equivalent to (log n)c as n goes to inﬁnity. Indeed, on the one hand we have n n n 1 dx 1 ∑ (logt)c ≤ 2 (log x)c ≤ ∑ (logt)c , t=2 t=3 n 1 so that ∑t=2 (logt)c ∼ n dx 2 (log x)c . n 2 On the other hand, we have n x dx = c (log x) (log x)c n dx 2 (log x)c+1 n 1 n ∑t=2 (logt)c ∼ (log n)c n +c 2 2 dx . (log x)c+1 n dx 2 (log x)c n dx 2 (log x)c n (log n)c . As both integrals are divergent we have =o Combining the fact that constant C > 0 such that with Equation (16), we get the existence of a Eθ [Tk (n)] ≤ , so that ∼ Cn 4 fk (n) + . 2 ∆ (log n)2βγ−1 Since fk (n) = o(n) and 2βγ − 1 > 0, the latter inequality shows that Eθ [Tk (n)] = o(n). The result follows. We are now in the position to prove the main result of this section. Theorem 11 If Θ has the Dirac/Bernoulli property, there exist Hannan consistent policies for which the expected regret can not be lower bounded by a logarithmic function in all environments θ. 205 S ALOMON , AUDIBERT AND E L A LAOUI Proof If f1 (n) = f2 (n) = log log n for n ≥ 3, UCB is Hannan consistent by Proposition 10. According to Lemma 1, the expected regret is then of order log log n in environments of the form (δa , δb ), a = b. Hence the conclusion on the non-existence of logarithmic lower bounds. Thus we have obtained a lower bound of order log log n. This order is critical regarding the methods we used. Yet, we do not know if this order is optimal. Acknowledgments This work has been supported by the French National Research Agency (ANR) through the COSINUS program (ANR-08-COSI-004: EXPLO-RA project). References R. Agrawal. Sample mean based index policies with o(log n) regret for the multi-armed bandit problem. Advances in Applied Mathematics, 27:1054–1078, 1995. H. Akaike. Information theory and an extension of the maximum likelihood principle. In Second International Symposium on Information Theory, volume 1, pages 267–281. Springer Verlag, 1973. J.-Y. Audibert, R. Munos, and C. Szepesv´ ri. Exploration-exploitation tradeoff using variance estia mates in multi-armed bandits. Theoretical Computer Science, 410(19):1876–1902, 2009. P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235–256, 2002. P. Auer, N. Cesa-Bianchi, Y. Freund, and R.E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48–77, 2003. D. Bergemann and J. Valimaki. Bandit problems. In The New Palgrave Dictionary of Economics, 2nd ed. Macmillan Press, 2008. S. Bubeck. Bandits Games and Clustering Foundations. PhD thesis, Universit´ Lille 1, France, e 2010. S. Bubeck, R. Munos, G. Stoltz, and C. Szepesvari. Online optimization in X-armed bandits. In Advances in Neural Information Processing Systems 21, pages 201–208. 2009. A.N. Burnetas and M.N. Katehakis. Optimal adaptive policies for sequential allocation problems. Advances in Applied Mathematics, 17(2):122–142, 1996. N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, 2006. N. Cesa-Bianchi, Y. Freund, D. Haussler, D.P. Helmbold, R.E. Schapire, and M.K. Warmuth. How to use expert advice. Journal of the ACM (JACM), 44(3):427–485, 1997. 206 L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS P.A. Coquelin and R. Munos. Bandit algorithms for tree search. In Uncertainty in Artiﬁcial Intelligence, 2007. S. Gelly and Y. Wang. Exploration exploitation in go: UCT for Monte-Carlo go. In Online Trading between Exploration and Exploitation Workshop, Twentieth Annual Conference on Neural Information Processing Systems (NIPS 2006), 2006. J. Honda and A. Takemura. An asymptotically optimal bandit algorithm for bounded support models. In Proceedings of the Twenty-Third Annual Conference on Learning Theory (COLT), 2010. R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandits in metric spaces. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing, pages 681–690, 2008. R. D. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances in Neural Information Processing Systems 17, pages 697–704. 2005. T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Mathematics, 6:4–22, 1985. D. Lamberton, G. Pag` s, and P. Tarr` s. When can the two-armed bandit algorithm be trusted? e e Annals of Applied Probability, 14(3):1424–1454, 2004. C.L. Mallows. Some comments on cp. Technometrics, pages 661–675, 1973. H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematics Society, 58:527–535, 1952. G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6(2):461–464, 1978. W.R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933. 207</p><p>5 0.046953492 <a title="30-tfidf-5" href="./jmlr-2013-Pairwise_Likelihood_Ratios_for_Estimation_of_Non-Gaussian_Structural_Equation_Models.html">85 jmlr-2013-Pairwise Likelihood Ratios for Estimation of Non-Gaussian Structural Equation Models</a></p>
<p>Author: Aapo Hyvärinen, Stephen M. Smith</p><p>Abstract: We present new measures of the causal direction, or direction of effect, between two non-Gaussian random variables. They are based on the likelihood ratio under the linear non-Gaussian acyclic model (LiNGAM). We also develop simple ﬁrst-order approximations of the likelihood ratio and analyze them based on related cumulant-based measures, which can be shown to ﬁnd the correct causal directions. We show how to apply these measures to estimate LiNGAM for more than two variables, and even in the case of more variables than observations. We further extend the method to cyclic and nonlinear models. The proposed framework is statistically at least as good as existing ones in the cases of few data points or noisy data, and it is computationally and conceptually very simple. Results on simulated fMRI data indicate that the method may be useful in neuroimaging where the number of time points is typically quite small. Keywords: structural equation model, Bayesian network, non-Gaussianity, causality, independent component analysis</p><p>6 0.046060756 <a title="30-tfidf-6" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>7 0.044351708 <a title="30-tfidf-7" href="./jmlr-2013-Performance_Bounds_for_%CE%BB_Policy_Iteration_and_Application_to_the_Game_of_Tetris.html">87 jmlr-2013-Performance Bounds for λ Policy Iteration and Application to the Game of Tetris</a></p>
<p>8 0.041911744 <a title="30-tfidf-8" href="./jmlr-2013-Algorithms_for_Discovery_of_Multiple_Markov_Boundaries.html">11 jmlr-2013-Algorithms for Discovery of Multiple Markov Boundaries</a></p>
<p>9 0.040077921 <a title="30-tfidf-9" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>10 0.039404396 <a title="30-tfidf-10" href="./jmlr-2013-Learning_Theory_Analysis_for_Association_Rules_and_Sequential_Event_Prediction.html">61 jmlr-2013-Learning Theory Analysis for Association Rules and Sequential Event Prediction</a></p>
<p>11 0.03279563 <a title="30-tfidf-11" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>12 0.031425096 <a title="30-tfidf-12" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>13 0.030900449 <a title="30-tfidf-13" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>14 0.030671306 <a title="30-tfidf-14" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>15 0.030032564 <a title="30-tfidf-15" href="./jmlr-2013-Keep_It_Simple_And_Sparse%3A_Real-Time_Action_Recognition.html">56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</a></p>
<p>16 0.029857771 <a title="30-tfidf-16" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>17 0.029699925 <a title="30-tfidf-17" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>18 0.029499816 <a title="30-tfidf-18" href="./jmlr-2013-Optimal_Discovery_with_Probabilistic_Expert_Advice%3A_Finite_Time_Analysis_and_Macroscopic_Optimality.html">81 jmlr-2013-Optimal Discovery with Probabilistic Expert Advice: Finite Time Analysis and Macroscopic Optimality</a></p>
<p>19 0.029171096 <a title="30-tfidf-19" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>20 0.028001819 <a title="30-tfidf-20" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.157), (1, 0.011), (2, -0.031), (3, 0.103), (4, -0.038), (5, 0.108), (6, -0.075), (7, 0.029), (8, -0.016), (9, 0.097), (10, -0.043), (11, -0.173), (12, -0.016), (13, -0.147), (14, 0.087), (15, 0.175), (16, -0.019), (17, 0.056), (18, 0.034), (19, 0.244), (20, 0.166), (21, -0.002), (22, 0.211), (23, 0.184), (24, -0.052), (25, 0.036), (26, 0.18), (27, 0.087), (28, -0.09), (29, -0.113), (30, 0.101), (31, 0.058), (32, 0.156), (33, 0.029), (34, 0.032), (35, 0.081), (36, -0.005), (37, 0.032), (38, -0.001), (39, -0.128), (40, 0.006), (41, 0.061), (42, -0.059), (43, 0.01), (44, -0.066), (45, -0.082), (46, 0.064), (47, 0.075), (48, -0.083), (49, 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93329972 <a title="30-lsi-1" href="./jmlr-2013-Counterfactual_Reasoning_and_Learning_Systems%3A_The_Example_of_Computational_Advertising.html">30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</a></p>
<p>Author: Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X. Charles, D. Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson</p><p>Abstract: This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine. Keywords: causation, counterfactual reasoning, computational advertising</p><p>2 0.81303293 <a title="30-lsi-2" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>Author: Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer</p><p>Abstract: Randomized controlled experiments are often described as the most reliable tool available to scientists for discovering causal relationships among quantities of interest. However, it is often unclear how many and which different experiments are needed to identify the full (possibly cyclic) causal structure among some given (possibly causally insufﬁcient) set of variables. Recent results in the causal discovery literature have explored various identiﬁability criteria that depend on the assumptions one is able to make about the underlying causal process, but these criteria are not directly constructive for selecting the optimal set of experiments. Fortunately, many of the needed constructions already exist in the combinatorics literature, albeit under terminology which is unfamiliar to most of the causal discovery community. In this paper we translate the theoretical results and apply them to the concrete problem of experiment selection. For a variety of settings we give explicit constructions of the optimal set of experiments and adapt some of the general combinatorics results to answer questions relating to the problem of experiment selection. Keywords: causality, randomized experiments, experiment selection, separating systems, completely separating systems, cut-coverings</p><p>3 0.41389123 <a title="30-lsi-3" href="./jmlr-2013-Ranked_Bandits_in_Metric_Spaces%3A_Learning_Diverse_Rankings_over_Large_Document_Collections.html">94 jmlr-2013-Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections</a></p>
<p>Author: Aleksandrs Slivkins, Filip Radlinski, Sreenivas Gollapudi</p><p>Abstract: Most learning to rank research has assumed that the utility of different documents is independent, which results in learned ranking functions that return redundant results. The few approaches that avoid this have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisﬁed users, with several scalable algorithms that explicitly takes document similarity and ranking context into account. Our formulation is a non-trivial common generalization of two multi-armed bandit models from the literature: ranked bandits (Radlinski et al., 2008) and Lipschitz bandits (Kleinberg et al., 2008b). We present theoretical justiﬁcations for this approach, as well as a near-optimal algorithm. Our evaluation adds optimizations that improve empirical performance, and shows that our algorithms learn orders of magnitude more quickly than previous approaches. Keywords: online learning, clickthrough data, diversity, multi-armed bandits, contextual bandits, regret, metric spaces</p><p>4 0.3489331 <a title="30-lsi-4" href="./jmlr-2013-Algorithms_for_Discovery_of_Multiple_Markov_Boundaries.html">11 jmlr-2013-Algorithms for Discovery of Multiple Markov Boundaries</a></p>
<p>Author: Alexander Statnikov, Nikita I. Lytkin, Jan Lemeire, Constantin F. Aliferis</p><p>Abstract: Algorithms for Markov boundary discovery from data constitute an important recent development in machine learning, primarily because they offer a principled solution to the variable/feature selection problem and give insight on local causal structure. Over the last decade many sound algorithms have been proposed to identify a single Markov boundary of the response variable. Even though faithful distributions and, more broadly, distributions that satisfy the intersection property always have a single Markov boundary, other distributions/data sets may have multiple Markov boundaries of the response variable. The latter distributions/data sets are common in practical data-analytic applications, and there are several reasons why it is important to induce multiple Markov boundaries from such data. However, there are currently no sound and efﬁcient algorithms that can accomplish this task. This paper describes a family of algorithms TIE* that can discover all Markov boundaries in a distribution. The broad applicability as well as efﬁciency of the new algorithmic family is demonstrated in an extensive benchmarking study that involved comparison with 26 state-of-the-art algorithms/variants in 15 data sets from a diversity of application domains. Keywords: Markov boundary discovery, variable/feature selection, information equivalence, violations of faithfulness</p><p>5 0.3449297 <a title="30-lsi-5" href="./jmlr-2013-Pairwise_Likelihood_Ratios_for_Estimation_of_Non-Gaussian_Structural_Equation_Models.html">85 jmlr-2013-Pairwise Likelihood Ratios for Estimation of Non-Gaussian Structural Equation Models</a></p>
<p>Author: Aapo Hyvärinen, Stephen M. Smith</p><p>Abstract: We present new measures of the causal direction, or direction of effect, between two non-Gaussian random variables. They are based on the likelihood ratio under the linear non-Gaussian acyclic model (LiNGAM). We also develop simple ﬁrst-order approximations of the likelihood ratio and analyze them based on related cumulant-based measures, which can be shown to ﬁnd the correct causal directions. We show how to apply these measures to estimate LiNGAM for more than two variables, and even in the case of more variables than observations. We further extend the method to cyclic and nonlinear models. The proposed framework is statistically at least as good as existing ones in the cases of few data points or noisy data, and it is computationally and conceptually very simple. Results on simulated fMRI data indicate that the method may be useful in neuroimaging where the number of time points is typically quite small. Keywords: structural equation model, Bayesian network, non-Gaussianity, causality, independent component analysis</p><p>6 0.28806803 <a title="30-lsi-6" href="./jmlr-2013-Learning_Theory_Analysis_for_Association_Rules_and_Sequential_Event_Prediction.html">61 jmlr-2013-Learning Theory Analysis for Association Rules and Sequential Event Prediction</a></p>
<p>7 0.23980564 <a title="30-lsi-7" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>8 0.21489325 <a title="30-lsi-8" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>9 0.19839345 <a title="30-lsi-9" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>10 0.19699547 <a title="30-lsi-10" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<p>11 0.19685233 <a title="30-lsi-11" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<p>12 0.18738286 <a title="30-lsi-12" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>13 0.18498638 <a title="30-lsi-13" href="./jmlr-2013-Lower_Bounds_and_Selectivity_of_Weak-Consistent_Policies_in_Stochastic_Multi-Armed_Bandit_Problem.html">65 jmlr-2013-Lower Bounds and Selectivity of Weak-Consistent Policies in Stochastic Multi-Armed Bandit Problem</a></p>
<p>14 0.18347026 <a title="30-lsi-14" href="./jmlr-2013-AC%2B%2BTemplate-Based_Reinforcement_Learning_Library%3A_Fitting_the_Code_to_the_Mathematics.html">1 jmlr-2013-AC++Template-Based Reinforcement Learning Library: Fitting the Code to the Mathematics</a></p>
<p>15 0.18058766 <a title="30-lsi-15" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>16 0.17021148 <a title="30-lsi-16" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>17 0.16986117 <a title="30-lsi-17" href="./jmlr-2013-Learning_Bilinear_Model_for_Matching_Queries_and_Documents.html">60 jmlr-2013-Learning Bilinear Model for Matching Queries and Documents</a></p>
<p>18 0.16750725 <a title="30-lsi-18" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>19 0.16739477 <a title="30-lsi-19" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>20 0.16268973 <a title="30-lsi-20" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.023), (5, 0.11), (6, 0.029), (10, 0.045), (20, 0.017), (23, 0.032), (68, 0.022), (70, 0.526), (75, 0.037), (85, 0.017), (87, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89018679 <a title="30-lda-1" href="./jmlr-2013-Risk_Bounds_of_Learning_Processes_for_L%C3%A9vy_Processes.html">97 jmlr-2013-Risk Bounds of Learning Processes for Lévy Processes</a></p>
<p>Author: Chao Zhang, Dacheng Tao</p><p>Abstract: L´ vy processes refer to a class of stochastic processes, for example, Poisson processes and Browe nian motions, and play an important role in stochastic processes and machine learning. Therefore, it is essential to study risk bounds of the learning process for time-dependent samples drawn from a L´ vy process (or brieﬂy called learning process for L´ vy process). It is noteworthy that samples e e in this learning process are not independently and identically distributed (i.i.d.). Therefore, results in traditional statistical learning theory are not applicable (or at least cannot be applied directly), because they are obtained under the sample-i.i.d. assumption. In this paper, we study risk bounds of the learning process for time-dependent samples drawn from a L´ vy process, and then analyze the e asymptotical behavior of the learning process. In particular, we ﬁrst develop the deviation inequalities and the symmetrization inequality for the learning process. By using the resultant inequalities, we then obtain the risk bounds based on the covering number. Finally, based on the resulting risk bounds, we study the asymptotic convergence and the rate of convergence of the learning process for L´ vy process. Meanwhile, we also give a comparison to the related results under the samplee i.i.d. assumption. Keywords: L´ vy process, risk bound, deviation inequality, symmetrization inequality, statistical e learning theory, time-dependent</p><p>2 0.87770432 <a title="30-lda-2" href="./jmlr-2013-Lower_Bounds_and_Selectivity_of_Weak-Consistent_Policies_in_Stochastic_Multi-Armed_Bandit_Problem.html">65 jmlr-2013-Lower Bounds and Selectivity of Weak-Consistent Policies in Stochastic Multi-Armed Bandit Problem</a></p>
<p>Author: Antoine Salomon, Jean-Yves Audibert, Issam El Alaoui</p><p>Abstract: This paper is devoted to regret lower bounds in the classical model of stochastic multi-armed bandit. A well-known result of Lai and Robbins, which has then been extended by Burnetas and Katehakis, has established the presence of a logarithmic bound for all consistent policies. We relax the notion of consistency, and exhibit a generalisation of the bound. We also study the existence of logarithmic bounds in general and in the case of Hannan consistency. Moreover, we prove that it is impossible to design an adaptive policy that would select the best of two algorithms by taking advantage of the properties of the environment. To get these results, we study variants of popular Upper Conﬁdence Bounds (UCB) policies. Keywords: stochastic bandits, regret lower bounds, consistency, selectivity, UCB policies 1. Introduction and Notations Multi-armed bandits are a classical way to illustrate the difﬁculty of decision making in the case of a dilemma between exploration and exploitation. The denomination of these models comes from an analogy with playing a slot machine with more than one arm. Each arm has a given (and unknown) reward distribution and, for a given number of rounds, the agent has to choose one of them. As the goal is to maximize the sum of rewards, each round decision consists in a trade-off between exploitation (i.e., playing the arm that has been the more lucrative so far) and exploration (i.e., testing another arm, hoping to discover an alternative that beats the current best choice). One possible application is clinical trial: a doctor wants to heal as many patients as possible, the patients arrive sequentially, and the effectiveness of each treatment is initially unknown (Thompson, 1933). Bandit problems have initially been studied by Robbins (1952), and since then they have been applied to many ﬁelds such as economics (Lamberton et al., 2004; Bergemann and Valimaki, 2008), games (Gelly and Wang, 2006), and optimisation (Kleinberg, 2005; Coquelin and Munos, 2007; Kleinberg et al., 2008; Bubeck et al., 2009). ∗. Also at Willow, CNRS/ENS/INRIA - UMR 8548. c 2013 Antoine Salomon, Jean-Yves Audibert and Issam El Alaoui. S ALOMON , AUDIBERT AND E L A LAOUI 1.1 Setting In this paper, we consider the following model. A stochastic multi-armed bandit problem is deﬁned by: • a number of rounds n, • a number of arms K ≥ 2, • an environment θ = (ν1 , · · · , νK ), where each νk (k ∈ {1, · · · , K}) is a real-valued measure that represents the distribution reward of arm k. The number of rounds n may or may not be known by the agent, but this will not affect the present study. We assume that rewards are bounded. Thus, for simplicity, each νk is a probability on [0, 1]. Environment θ is initially unknown by the agent but lies in some known set Θ. For the problem to be interesting, the agent should not have great knowledges of its environment, so that Θ should not be too small and/or only contain too trivial distributions such as Dirac measures. To make it simple, we may assume that Θ contains all environments where each reward distribution is a Dirac distribution or a Bernoulli distribution. This will be acknowledged as Θ having the Dirac/Bernoulli property. For technical reason, we may also assume that Θ is of the form Θ1 × . . . × ΘK , meaning that Θk is the set of possible reward distributions of arm k. This will be acknowledged as Θ having the product property. The game is as follows. At each round (or time step) t = 1, · · · , n, the agent has to choose an arm It in the set {1, · · · , K}. This decision is based on past actions and observations, and the agent may also randomize his choice. Once the decision is made, the agent gets and observes a reward that is drawn from νIt independently from the past. Thus a policy (or strategy) can be described by a sequence (σt )t≥1 (or (σt )1≤t≤n if the number of rounds n is known) such that each σt is a mapping from the set {1, . . . , K}t−1 × [0, 1]t−1 of past decisions and rewards into the set of arm {1, . . . , K} (or into the set of probabilities on {1, . . . , K}, in case the agent randomizes his choices). For each arm k and all time step t, let Tk (t) = ∑ts=1 ½Is =k denote the sampling time, that is, the number of times arm k was pulled from round 1 to round t, and Xk,1 , Xk,2 , . . . , Xk,Tk (t) the corresponding sequence of rewards. We denote by Pθ the distribution on the probability space such that for any k ∈ {1, . . . , K}, the random variables Xk,1 , Xk,2 , . . . , Xk,n are i.i.d. realizations of νk , and such that these K sequences of random variables are independent. Let Eθ denote the associated expectation. Let µk = xdνk (x) be the mean reward of arm k. Introduce µ∗ = maxk∈{1,...,K} µk and ﬁx an arm ∗ ∈ argmax ∗ k k∈{1,...,K} µk , that is, k has the best expected reward. The agent aims at minimizing its regret, deﬁned as the difference between the cumulative reward he would have obtained by always drawing the best arm and the cumulative reward he actually received. Its regret is thus n n Rn = ∑ Xk∗ ,t − ∑ XIt ,TIt (t) . t=1 t=1 As most of the publications on this topic, we focus on the expected regret, for which one can check that: K E θ Rn = ∑ ∆k Eθ [Tk (n)], k=1 188 (1) L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS where ∆k is the optimality gap of arm k, deﬁned by ∆k = µ∗ − µk . We also deﬁne ∆ as the gap between the best arm and the second best arm, that is, ∆ := mink:∆k >0 ∆k . Other notions of regret exist in the literature. One of them is the quantity n max ∑ Xk,t − XIt ,TIt (t) , k t=1 which is mostly used in adversarial settings. Results and ideas we want to convey here are more suited to expected regret, and considering other deﬁnitions of regret would only bring some more technical intricacies. 1.2 Consistency and Regret Lower Bounds Former works have shown the existence of lower bounds on the expected regret of a large class of policies: intuitively, to perform well the agent has to explore all arms, and this requires a signiﬁcant amount of suboptimal choices. In this way, Lai and Robbins (1985) proved a lower bound of order log n in a particular parametric framework, and they also exhibited optimal policies. This work has then been extended by Burnetas and Katehakis (1996). Both papers deal with consistent policies, meaning that they only consider policies such that: ∀a > 0, ∀θ ∈ Θ, Eθ [Rn ] = o(na ). (2) Let us detail the bound of Burnetas and Katehakis, which is valid when Θ has the product property. Given an environment θ = (ν1 , · · · , νK ) and an arm k ∈ {1, . . . , K}, deﬁne: Dk (θ) := inf ˜ νk ∈Θk :E[˜ k ]>µ∗ ν ˜ KL(νk , νk ), where KL(ν, µ) denotes the Kullback-Leibler divergence of measures ν and µ. Now ﬁx a consistent policy and an environment θ ∈ Θ. If k is a suboptimal arm (i.e., µk = µ∗ ) such that 0 < Dk (θ) < ∞, then (1 − ε) log n ∀ε > 0, lim P Tk (n) ≥ = 1. n→+∞ Dk (θ) This readily implies that: lim inf n→+∞ Eθ [Tk (n)] 1 ≥ . log n Dk (θ) Thanks to Formula (1), it is then easy to deduce a lower bound of the expected regret. One contribution of this paper is to generalize the study of regret lower bounds, by considering weaker notions of consistency: α-consistency and Hannan consistency. We will deﬁne αconsistency (α ∈ [0, 1)) as a variant of Equation (2), where equality Eθ [Rn ] = o(na ) only holds for all a > α. We show that the logarithmic bound of Burnetas and Katehakis still holds, but coefﬁcient 1−α 1 Dk (θ) is turned into Dk (θ) . We also prove that the dependence of this new bound with respect to the term 1 − α is asymptotically optimal when n → +∞ (up to a constant). We will also consider the case of Hannan consistency. Indeed, any policy achieves at most an expected regret of order n: because of the equality ∑K Tk (n) = n and thanks to Equation (1), one k=1 can show that Eθ Rn ≤ n maxk ∆k . More intuitively, this comes from the fact that the average cost of pulling an arm k is a constant ∆k . As a consequence, it is natural to wonder what happens when 189 S ALOMON , AUDIBERT AND E L A LAOUI dealing with policies whose expected regret is only required to be o(n), which is equivalent to Hannan consistency. This condition is less restrictive than any of the previous notions of consistency. In this larger class of policy, we show that the lower bounds on the expected regret are no longer logarithmic, but can be much smaller. Finally, even if no logarithmic lower bound holds on the whole set Θ, we show that there necessarily exist some environments θ for which the expected regret is at least logarithmic. The latter result is actually valid without any assumptions on the considered policies, and only requires a simple property on Θ. 1.3 Selectivity As we exhibit new lower bounds, we want to know if it is possible to provide optimal policies that achieve these lower bounds, as it is the case in the classical class of consistent policies. We answer negatively to this question, and for this we solve the more general problem of selectivity. Given a set of policies, we deﬁne selectivity as the ability to perform at least as good as the policy that is best suited to the current environment θ. Still, such an ability can not be implemented. As a by-product it is not possible to design a procedure that would speciﬁcally adapt to some kinds of environments, for example by selecting a particular policy. This contribution is linked with selectivity in on-line learning problem with perfect information, commonly addressed by prediction with expert advice (see, e.g., Cesa-Bianchi et al., 1997). In this spirit, a closely related problem is the one of regret against the best strategy from a pool studied by Auer et al. (2003). The latter designed an algorithm in the context of adversarial/nonstochastic bandit whose decisions are based on a given number of recommendations (experts), which are themselves possibly the rewards received by a set of given policies. To a larger extent, model selection has been intensively studied in statistics, and is commonly solved by penalization methods (Mallows, 1973; Akaike, 1973; Schwarz, 1978). 1.4 UCB Policies Some of our results are obtained using particular Upper Conﬁdence Bound algorithms. These algorithms were introduced by Lai and Robbins (1985): they basically consist in computing an index for each arm, and selecting the arm with the greatest index. A simple and efﬁcient way to design such policies is as follows: choose each index as low as possible such that, conditional to past observations, it is an upper bound of the mean reward of the considered arm with high probability (or, say, with high conﬁdence level). This idea can be traced back to Agrawal (1995), and has been popularized by Auer et al. (2002), who notably described a policy called UCB1. In this policy, each index Bk,s,t is deﬁned by an arm k, a time step t, an integer s that indicates the number of times arm k has been pulled before round t, and is given by: ˆ Bk,s,t = Xk,s + 2 logt , s ˆ ˆ where Xk,s is the empirical mean of arm k after s pulls, that is, Xk,s = 1 ∑s Xk,u . s u=1 To summarize, UCB1 policy ﬁrst pulls each arm once and then, at each round t > K, selects an arm k that maximizes Bk,Tk (t−1),t . Note that, by means of Hoeffding’s inequality, the index Bk,Tk (t−1),t is indeed an upper bound of µk with high probability (i.e., the probability is greater than 1 − 1/t 4 ). ˆ Another way to understand this index is to interpret the empiric mean Xk,Tk (t−1) as an ”exploitation” 190 L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS term, and the square root 2 logt/s as an ”exploration” term (because the latter gradually increases when arm k is not selected). Policy UCB1 achieves the logarithmic bound (up to a multiplicative constant), as it was shown that: ∀θ ∈ Θ, ∀n ≥ 3, Eθ [Tk (n)] ≤ 12 K log n log n log n ≤ 12K . and Eθ Rn ≤ 12 ∑ 2 ∆ ∆k k=1 ∆k Audibert et al. (2009) studied some variants of UCB1 policy. Among them, one consists in changing the 2 logt in the exploration term into ρ logt, where ρ > 0. This can be interpreted as a way to tune exploration: the smaller ρ is, the better the policy will perform in simple environments where information is disclosed easily (for example when all reward distributions are Dirac measures). On the contrary, ρ has to be greater to face more challenging environments (typically when reward distributions are Bernoulli laws with close parameters). This policy, that we denote UCB(ρ), was proven by Audibert et al. to achieve the logarithmic bound when ρ > 1, and the optimality was also obtained when ρ > 1 for a variant of UCB(ρ). 2 Bubeck (2010) showed in his PhD dissertation that their ideas actually enable to prove optimality 1 of UCB(ρ) for ρ > 1 . Moreover, the case ρ = 2 corresponds to a conﬁdence level of 1 (because 2 t of Hoeffding’s inequality, as above), and several studies (Lai and Robbins, 1985; Agrawal, 1995; Burnetas and Katehakis, 1996; Audibert et al., 2009; Honda and Takemura, 2010) have shown that this level is critical. We complete these works by a precise study of UCB(ρ) when ρ ≤ 1 . We prove that UCB(ρ) 2 is (1 − 2ρ)-consistent and that it is not α-consistent for any α < 1 − 2ρ (in view of the deﬁnition above, this means that the expected regret is roughly of order n1−2ρ ). Thus it does not achieve the logarithmic bound, but it performs well in simple environments, for example, environments where all reward distributions are Dirac measures. Moreover, we exhibit expected regret bounds of general UCB policies, with the 2 logt in the exploration term of UCB1 replaced by an arbitrary function. We give sufﬁcient conditions for such policies to be Hannan consistent and, as mentioned before, ﬁnd that lower bounds need not be logarithmic any more. 1.5 Outline The paper is organized as follows: in Section 2, we give bounds on the expected regret of general 1 UCB policies and of UCB (ρ) (ρ ≤ 2 ), as preliminary results. In Section 3, we focus on α-consistent policies. Then, in Section 4, we study the problem of selectivity, and we conclude in Section 5 by general results on the existence of logarithmic lower bounds. Throughout the paper ⌈x⌉ denotes the smallest integer not less than x whereas ⌊x⌋ denotes the largest integer not greater than x, ½A stands for the indicator function of event A, Ber(p) is the Bernoulli law with parameter p, and δx is the Dirac measure centred on x. 2. Preliminary Results In this section, we estimate the expected regret of the paper. UCB 191 policies. This will be useful for the rest of S ALOMON , AUDIBERT AND E L A LAOUI 2.1 Bounds on the Expected Regret of General UCB Policies We ﬁrst study general UCB policies, deﬁned by: • Draw each arm once, • then, at each round t, draw an arm It ∈ argmax Bk,Tk (t−1),t , k∈{1,...,K} ˆ where Bk,s,t is deﬁned by Bk,s,t = Xk,s + creasing. fk (t) s and where functions fk (1 ≤ k ≤ K) are in- This deﬁnition is inspired by popular UCB1 algorithm, for which fk (t) = 2 logt for all k. The following lemma estimates the performances of UCB policies in simple environments, for which reward distributions are Dirac measures. Lemma 1 Let 0 ≤ b < a ≤ 1 and n ≥ 1. For θ = (δa , δb ), the random variable T2 (n) is uniformly 1 upper bounded by ∆2 f2 (n) + 1. Consequently, the expected regret of UCB is upper bounded by 1 ∆ f 2 (n) + 1. Proof In environment θ, best arm is arm 1 and ∆ = ∆2 = a − b. Let us ﬁrst prove the upper bound of the sampling time. The assertion is true for n = 1 and n = 2: the ﬁrst two rounds consists in 1 drawing each arm once, so that T2 (n) ≤ 1 ≤ ∆2 f2 (n) + 1 for n ∈ {1, 2}. If, by contradiction, the as1 1 sertion is false, then there exists t ≥ 3 such that T2 (t) > ∆2 f2 (t) + 1 and T2 (t − 1) ≤ ∆2 f2 (t − 1) + 1. Since f2 (t) ≥ f2 (t − 1), this leads to T2 (t) > T2 (t − 1), meaning that arm 2 is drawn at round t. Therefore, we have a + f1 (t) T1 (t−1) ≤ b+ f2 (t) T2 (t−1) , hence a − b = ∆ ≤ f2 (t) T2 (t−1) , which implies 1 1 T2 (t − 1) ≤ ∆2 f2 (t) and thus T2 (t) ≤ ∆2 f2 (t) + 1. This contradicts the deﬁnition of t, and this ends the proof of the ﬁrst statement. The second statement is a direct consequence of Formula (1). Remark: throughout the paper, we will often use environments with K = 2 arms to provide bounds on expected regrets. However, we do not lose generality by doing so, because all corresponding proofs can be written almost identically to suit to any K ≥ 2, by simply assuming that the distribution of each arm k ≥ 3 is δ0 . We now give an upper bound of the expected sampling time of any arm such that ∆k > 0. This bound is valid in any environment, and not only those of the form (δa , δb ). Lemma 2 For any θ ∈ Θ and any β ∈ (0, 1), if ∆k > 0 the following upper bound holds: n Eθ [Tk (n)] ≤ u + where u = 4 fk (n) ∆2 k ∑ t=u+1 1+ logt 1 log( β ) . 192 e−2β fk (t) + e−2β fk∗ (t) , L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS An upper bound of the expected regret can be deduced from this lemma thanks to Formula 1. Proof The core of the proof is a peeling argument and the use of Hoeffding’s maximal inequality (see, e.g., Cesa-Bianchi and Lugosi, 2006, section A.1.3 for details). The idea is originally taken from Audibert et al. (2009), and the following is an adaptation of the proof of an upper bound of UCB (ρ) in the case ρ > 1 which can be found in S. Bubeck’s PhD dissertation. 2 First, let us notice that the policy selects an arm k such that ∆k > 0 at time step t ≤ n only if at least one of the three following equations holds: Bk∗ ,Tk∗ (t−1),t ≤ µ∗ , (3) fk (t) , Tk (t − 1) (4) ˆ Xk,t ≥ µk + Tk (t − 1) < 4 fk (n) . ∆2 k (5) Indeed, if none of the equations is true, then: fk (n) ˆ > Xk,t + Tk (t − 1) Bk∗ ,Tk∗ (t−1),t > µ∗ = µk + ∆k ≥ µk + 2 fk (t) = Bk,Tk (t−1),t , Tk (t − 1) which implies that arm k can not be chosen at time step t. We denote respectively by ξ1,t , ξ2,t and ξ3,t the events corresponding to Equations (3), (4) and (5). We have: n ∑ ½I =k Eθ [Tk (n)] = Eθ t n n ∑ ½{I =k}∩ξ = Eθ t t=1 + Eθ 3,t ∑ ½{I =k}\ξ t 3,t . t=1 t=1 n Let us show that the sum ∑t=1 ½{It =k}∩ξ3,t is almost surely lower than u := ⌈4 fk (n)/∆2 ⌉. We assume k m−1 n by contradiction that ∑t=1 ½{It =k}∩ξ3,t > u. Then there exists m < n such that ∑t=1 ½{It =k}∩ξ3,t < m 4 fk (n)/∆2 and ∑t=1 ½{It =k}∩ξ3,t = ⌈4 fk (n)/∆2 ⌉. Therefore, for any s > m, we have: k k m m t=1 t=1 Tk (s − 1) ≥ Tk (m) = ∑ ½{It =k} ≥ ∑ ½{It =k}∩ξ3,t = 4 fk (n) 4 fk (n) ≥ , 2 ∆k ∆2 k so that ½{Is =k}∩ξ3,s = 0. But then m n ∑ ½{I =k}∩ξ t t=1 3,t = ∑ ½{It =k}∩ξ3,t = t=1 4 fk (n) ≤ u, ∆2 k which is the contradiction expected. n n We also have ∑t=1 ½{It =k}\ξ3,t = ∑t=u+1 ½{It =k}\ξ3,t : since Tk (t − 1) ≤ t − 1, event ξ3,t always happens at time step t ∈ {1, . . . , u}. And then, since event {It = k} is included in ξ1,t ∪ ξ2,t ∪ ξ3,t : n Eθ ∑ ½{It =k}\ξ3,t ≤ Eθ t=u+1 n n t=u+1 t=u+1 ∑ ½ξ1,t ∪ξ2,t ≤ ∑ Pθ (ξ1,t ) + Pθ (ξ2,t ). 193 S ALOMON , AUDIBERT AND E L A LAOUI It remains to ﬁnd upper bounds of Pθ (ξ1,t ) and Pθ (ξ2,t ). To this aim, we apply the peeling argument with a geometric grid over the time interval [1,t]: fk∗ (t) ≤ µ∗ Tk∗ (t − 1) ˆ Pθ (ξ1,t ) = Pθ Bk∗ ,Tk∗ (t−1),t ≤ µ∗ = Pθ Xk∗ ,Tk∗ (t−1) + ˆ ≤ Pθ ∃s ∈ {1, · · · ,t}, Xk∗ ,s + fk∗ (t) ≤ µ∗ s logt log(1/β) ≤ ∑ j=0 ˆ Pθ ∃s : {β j+1t < s ≤ β j t}, Xk∗ ,s + logt log(1/β) ≤ ∑ j=0 s Pθ ∃s : {β j+1t < s ≤ β j t}, logt log(1/β) ≤ ∑ j=0 ∑ j=0 ∑ (Xk ,l − µ∗ ) ≤ − ∗ s fk∗ (t) β j+1t fk∗ (t) l=1 ∑ (µ∗ − Xk ,l ) ≥ t < s ≤ β j t}, logt log(1/β) = fk∗ (t) ≤ µ∗ s j ∗ β j+1t fk∗ (t) l=1 s Pθ max ∑ (µ∗ − Xk∗ ,l ) ≥ s≤β j t l=1 β j+1t fk∗ (t) . As the range of the random variables (Xk∗ ,l )1≤l≤s is [0, 1], Hoeffding’s maximal inequality gives:   2 logt log(1/β) β j+1t fk∗ (t)  2 logt  Pθ (ξ1,t ) ≤ + 1 e−2β fk∗ (t) . ≤ ∑ exp − jt β log(1/β) j=0 Similarly, we have: logt + 1 e−2β fk (t) , log(1/β) and the result follows from the combination of previous inequalities. Pθ (ξ2,t ) ≤ 2.2 Bounds on the Expected Regret of UCB(ρ), ρ ≤ We study the performances of UCB (ρ) 1 2 1 policy, with ρ ∈ (0, 2 ]. We recall that ρ logt s . UCB (ρ) is the UCB ˆ policy deﬁned by fk (t) = ρ log(t) for all k, that is, Bk,s,t = Xk,s + Small values of ρ can be interpreted as a low level of experimentation in the balance between exploration and exploitation. 1 Precise regret bound orders of UCB(ρ) when ρ ∈ (0, 2 ] are not documented in the literature. We ﬁrst give an upper bound of expected regret in simple environments, where it is supposed to perform well. As stated in the following proposition (which is a direct consequence of Lemma 1), the order of the bound is ρ log n . ∆ 194 L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS Lemma 3 Let 0 ≤ b < a ≤ 1 and n ≥ 1. For θ = (δa , δb ), the random variable T2 (n) is uniformly ρ upper bounded by ∆2 log(n) + 1. Consequently, the expected regret of UCB(ρ) is upper bounded by ρ ∆ log(n) + 1. One can show that the expected regret of UCB(ρ) is actually equivalent to ρ log n as n goes to ∆ inﬁnity. These good performances are compensated by poor results in more complex environments, as showed in the following theorem. We exhibit an expected regret upper bound which is valid for any θ ∈ Θ, and which is roughly of order n1−2ρ . We also show that this upper bound is asymptot1 ically optimal. Thus, with ρ ∈ (0, 2 ), UCB(ρ) does not perform enough exploration to achieve the logarithmic bound, as opposed to UCB(ρ) with ρ ∈ ( 1 , +∞). 2 1 Theorem 4 For any ρ ∈ (0, 2 ], any θ ∈ Θ and any β ∈ (0, 1), one has Eθ [Rn ] ≤ 4ρ log n ∑ ∆k + ∆k + 2∆k k:∆k >0 log n n1−2ρβ +1 . log(1/β) 1 − 2ρβ Moreover, if Θ has the Dirac/Bernoulli property, then for any ε > 0 there exists θ ∈ Θ such that Eθ [Rn ] lim n→+∞ n1−2ρ−ε = +∞. 1 1 The value ρ = 2 is critical, but we can deduce from the upper bound of this theorem that UCB( 2 ) is consistent in the classical sense of Lai and Robbins (1985) and of Burnetas and Katehakis (1996). log Proof We set u = 4ρ∆2 n . By Lemma 2 we get: k n Eθ [Tk (n)] ≤ u + 2 = u+2 ∑ logt + 1 e−2βρ log(t) log(1/β) ∑ logt 1 + 1 2ρβ log(1/β) t t=u+1 n t=u+1 n 1 ≤ u+2 log n +1 log(1/β) ≤ u+2 log n +1 log(1/β) 1+ ∑ ≤ u+2 log n +1 log(1/β) 1+ ≤ u+2 log n +1 . log(1/β) 1 − 2ρβ ∑ t 2ρβ t=1 n 1 t 2ρβ t=2 n−1 1 1−2ρβ n 1 t 2ρβ dt As usual, the upper bound of the expected regret follows from Formula (1). Now, let us show the lower bound. The result is obtained by considering an environment θ of the √ 1 form Ber( 1 ), δ 1 −∆ , where ∆ lies in (0, 2 ) and is such that 2ρ(1 + ∆)2 < 2ρ + ε. This notation is 2 2 obviously consistent with the deﬁnition of ∆ as an optimality gap. We set Tn := ⌈ ρ log n ⌉, and deﬁne ∆ the event ξn by: 1 1 ˆ ξn = X1,Tn < − (1 + √ )∆ . 2 ∆ 195 S ALOMON , AUDIBERT AND E L A LAOUI When event ξn occurs, one has for any t ∈ {Tn , . . . , n} ˆ X1,Tn + ρ logt Tn ˆ ≤ X1,Tn + ≤ √ ρ log n 1 1 < − (1 + √ )∆ + ∆ Tn 2 ∆ 1 − ∆, 2 so that arm 1 is chosen no more than Tn times by UCB(ρ) policy. Consequently: Eθ [T2 (n)] ≥ Pθ (ξn )(n − Tn ). We will now ﬁnd a lower bound of the probability of ξn thanks to Berry-Esseen inequality. We denote by C the corresponding constant, and by Φ the c.d.f. of the standard normal distribution. For convenience, we also deﬁne the following quantities: σ := E X1,1 − Using the fact that Φ(−x) = e− √ 2 β(x) 2πx 1 2 2 1 = , M3 := E 2 X1,1 − 1 2 3 1 = . 8 x2 with β(x) − − → 1, we have: −− x→+∞ ˆ √ X1,Tn − 1 √ 1 2 Tn ≤ −2 1 + √ ∆ Tn σ ∆ √ √ CM3 Φ −2(∆ + ∆) Tn − 3 √ σ Tn √ 2 exp −2(∆ + ∆) Tn √ √ CM3 √ √ √ β 2(∆ + ∆) Tn − 3 √ σ Tn 2 2π(∆ + ∆) Tn √ 2 ρ log n exp −2(∆ + ∆) ( ∆ + 1) √ √ CM3 √ √ √ β 2(∆ + ∆) Tn − 3 √ σ Tn 2 2π(∆ + ∆) Tn √ √ −2ρ(1+ ∆)2 exp −2(∆ + ∆)2 √ √ CM3 n √ √ √ β 2(∆ + ∆) Tn − 3 √ . Tn σ Tn 2 2π(∆ + ∆) Pθ (ξn ) = Pθ ≥ ≥ ≥ ≥ Previous calculations and Formula (1) give Eθ [Rn ] = ∆Eθ [T2 (n)] ≥ ∆Pθ (ξn )(n − Tn ), √ 1−2ρ(1+ ∆)2 so that we ﬁnally obtain a lower bound of Eθ [Rn ] of order n √log n . Therefore, nEθ [Rn ] is at least 1−2ρ−ε √ 2 √ 2 n2ρ+ε−2ρ(1+ ∆) √ of order . Since 2ρ + ε − 2ρ(1 + ∆) > 0, the numerator goes to inﬁnity, faster than log n √ log n. This concludes the proof. 196 L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS 3. Bounds on the Class α-consistent Policies In this section, our aim is to ﬁnd how the classical results of Lai and Robbins (1985) and of Burnetas and Katehakis (1996) can be generalised if we do not restrict the study to consistent policies. As a by-product, we will adapt their results to the present setting, which is simpler than their parametric frameworks. We recall that a policy is consistent if its expected regret is o(na ) for all a > 0 in all environments θ ∈ Θ. A natural way to relax this deﬁnition is the following. Deﬁnition 5 A policy is α-consistent if ∀a > α, ∀θ ∈ Θ, Eθ [Rn ] = o(na ). For example, we showed in the previous section that, for any ρ ∈ (0, 1 ], UCB(ρ) is (1−2ρ)-consistent 2 and not α-consistent if α < 1 − 2ρ. Note that the relevant range of α in this deﬁnition is [0, 1): the case α = 0 corresponds to the standard deﬁnition of consistency (so that throughout the paper the term ”consistent” also means ”0-consistent”), and any value α ≥ 1 is pointless as any policy is then α-consistent. Indeed, the expected regret of any policy is at most of order n. This also lead us to wonder what happens if we only require the expected regret to be o(n): ∀θ ∈ Θ, Eθ [Rn ] = o(n). This requirement corresponds to the deﬁnition of Hannan consistency. The class of Hannan consistent policies includes consistent policies and α-consistent policies for any α ∈ [0, 1). Some results about this class will be obtained in Section 5. We focus on regret lower bounds on α-consistent policies. We ﬁrst show that the main result of Burnetas and Katehakis can be extended in the following way. Theorem 6 Assume that Θ has the product property. Fix an α-consistent policy and θ ∈ Θ. If ∆k > 0 and if 0 < Dk (θ) < ∞, then ∀ε > 0, lim Pθ Tk (n) ≥ (1 − ε) n→+∞ (1 − α) log n = 1. Dk (θ) Consequently lim inf n→+∞ 1−α Eθ [Tk (n)] ≥ . log n Dk (θ) Remind that the lower bound of the expected regret is then deduced from Formula (1), and that coefﬁcient Dk (θ) is deﬁned by: Dk (θ) := inf ˜ νk ∈Θk :E[˜ k ]>µ∗ ν ˜ KL(νk , νk ), where KL(ν, µ) denotes the Kullback-Leibler divergence of measures ν and µ. Note that, as opposed to Burnetas and Katehakis (1996), there is no optimal policy in general (i.e., a policy that would achieve the lower bound in all environment θ). This can be explained intuitively as follows. If by contradiction there existed such a policy, its expected regret would be of order log n and consequently it would be (0-)consistent. Then the lower bounds in the case of 197 S ALOMON , AUDIBERT AND E L A LAOUI 1−α 0-consistency would necessarily hold. This can not happen if α > 0 because Dk (θ) < Dk1 . (θ) Nevertheless, this argument is not rigorous because the fact that the regret would be of order log n is only valid for environments θ such that 0 < Dk (θ) < ∞. The non-existence of optimal policies is implied by a stronger result of the next section (yet, only if α > 0.2). Proof We adapt Proposition 1 in Burnetas and Katehakis (1996) and its proof. Let us denote θ = (ν1 , . . . , νK ). We ﬁx ε > 0, and we want to show that: lim Pθ n→+∞ Set δ > 0 and δ′ > α such that ˜ that E[νk ] > µ∗ and 1−δ′ 1+δ Tk (n) (1 − ε)(1 − α) < log n Dk (θ) = 0. ˜ > (1 − ε)(1 − α). By deﬁnition of Dk (θ), there exists νk such ˜ Dk (θ) < KL(νk , νk ) < (1 + δ)Dk (θ). ˜ ˜ ˜ Let us set θ = (ν1 , . . . , νk−1 , νk , νk+1 , . . . , νK ). Environment θ lies in Θ by the product property and δ = KL(ν , ν ) and arm k is its best arm. Deﬁne I k ˜k ′ Aδ := n Tk (n) 1 − δ′ < δ log n I ′′ δ , Cn := log LTk (n) ≤ 1 − δ′′ log n , where δ′′ is such that α < δ′′ < δ′ and Lt is deﬁned by log Lt = ∑ts=1 log δ′ δ′ δ′′ δ′ dνk ˜ d νk (Xk,s ) . δ′′ Now, we show that Pθ (An ) = Pθ (An ∩Cn ) + Pθ (An \Cn ) − − → 0. −− n→+∞ On the one hand, one has: ′′ ′′ ′ ′′ ′ δ δ Pθ (Aδ ∩Cn ) ≤ n1−δ Pθ (Aδ ∩Cn ) ˜ n n ′′ ′ (6) ′′ ≤ n1−δ Pθ (Aδ ) = n1−δ Pθ n − Tk (n) > n − ˜ ˜ n 1 − δ′ Iδ log n ′′ ≤ n1−δ Eθ [n − Tk (n)] ˜ (7) ′ n − 1−δ log n Iδ ′′ = n−δ Eθ ∑K Tℓ (n) − Tk (n) ˜ l=1 ′ n − 1−δ Iδ log n n ′′ ≤ ∑ℓ=k n−δ Eθ [Tℓ (n)] ˜ ′ 1 − 1−δ Iδ log n n − − → 0. −− (8) n→+∞ ′ Equation (6) results from a partition of Aδ into events {Tk (n) = a}, 0 ≤ a < n ′′ 1−δ′ Iδ log n . Each event ′′ δ {Tk (n) = a} ∩ Cn equals {Tk (n) = a} ∩ ∏a dνk (Xk,s ) ≤ n1−δ and is measurable with respect s=1 d νk ˜ to Xk,1 , . . . , Xk,a and to Xℓ,1 , . . . , Xℓ,n (ℓ = k). Thus, ½{Tk (n)=a}∩Cn ′′ can be written as a function f of δ the latter r.v. and we have: ′′ δ Pθ {Tk (n) = a} ∩Cn = f (xk,s )1≤s≤a , (xℓ,s )ℓ=k,1≤s≤n ∏ ℓ=k 1≤s≤n ≤ f (xk,s )1≤s≤a , (xℓ,s )ℓ=k,1≤s≤n ∏ ℓ=k 1≤s≤n ′′ ′′ δ = n1−δ Pθ {Tk (n) = a} ∩Cn ˜ 198 . dνℓ (xℓ,s ) ∏ dνk (xk,s ) 1≤s≤a ′′ dνℓ (xℓ,s )n1−δ ∏ 1≤s≤a ˜ d νk (xk,s ) L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS Equation (7) is a consequence of Markov’s inequality, and the limit in (8) is a consequence of α-consistency. ′ On the other hand, we set bn := 1−δ log n, so that Iδ ′ ′′ δ Pθ (Aδ \Cn ) ≤ P n ≤ P max log L j > (1 − δ′′ ) log n j≤⌊bn ⌋ 1 1 − δ′′ max log L j > I δ bn j≤⌊bn ⌋ 1 − δ′ . This term tends to zero, as a consequence of the law of large numbers. ′ Now that Pθ (Aδ ) tends to zero, the conclusion results from n 1 − δ′ 1 − δ′ (1 − ε)(1 − α) > ≥ . δ (1 + δ)Dk (θ) Dk (θ) I The previous lower bound is asymptotically optimal with respect to its dependence in α, as claimed in the following proposition. Proposition 7 Assume that Θ has the Dirac/Bernoulli property. There exist θ ∈ Θ and a constant c > 0 such that, for any α ∈ [0, 1), there exists an α-consistent policy such that: lim inf n→+∞ Eθ [Tk (n)] ≤ c, (1 − α) log n for any k satisfying ∆k > 0. Proof In any environment of the form θ = (δa , δb ) with a = b, Lemma 3 implies the following estimate for UCB(ρ): Eθ Tk (n) ρ lim inf ≤ 2, n→+∞ log n ∆ where k = k∗ . Because 1−α ∈ (0, 1 ) and since UCB(ρ) is (1 − 2ρ)-consistent for any ρ ∈ (0, 1 ] (Theorem 4), we 2 2 2 1 obtain the result by choosing the α-consistent policy UCB( 1−α ) and by setting c = 2∆2 . 2 4. Selectivity In this section, we address the problem of selectivity. By selectivity, we mean the ability to adapt to the environment as and when rewards are observed. More precisely, a set of two (or more) policies is given. The one that performs the best depends on environment θ. We wonder if there exists an adaptive procedure that, given any environment θ, would be as good as any policy in the given set. Two major reasons motivate this study. On the one hand this question was answered by Burnetas and Katehakis within the class of consistent policies. They exhibits an asymptotically optimal policy, that is, that achieves the regret 199 S ALOMON , AUDIBERT AND E L A LAOUI lower bounds they have proven. The fact that a policy performs as best as any other one obviously solves the problem of selectivity. On the other hand, this problem has already been studied in the context of adversarial bandit by Auer et al. (2003). Their setting differs from our not only because their bandits are nonstochastic, but also because their adaptive procedure takes only into account a given number of recommendations, whereas in our setting the adaptation is supposed to come from observing rewards of the chosen arms (only one per time step). Nevertheless, one can wonder if an ”exponentially weighted forecasters” procedure like E XP 4 could be transposed to our context. The answer is negative, as stated in the following theorem. To avoid confusion, we make the notations of the regret and of sampling time more precise by adding the considered policy: under policy A , Rn and Tk (n) will be respectively denoted Rn (A ) and Tk (n, A ). ˜ Theorem 8 Let A be a consistent policy and let ρ be a real number in (0, 0.4). If Θ has the ˜ Dirac/Bernoulli property and the product property, there is no policy which can both beat A and UCB (ρ), that is: ∀A , ∃θ ∈ Θ, lim sup n→+∞ Eθ [Rn (A )] > 1. ˜ min(Eθ [Rn (A )], Eθ [Rn (UCB(ρ))]) Thus the existence of optimal policies does not hold when we extend the notion of consistency. Precisely, as UCB(ρ) is (1 − 2ρ)-consistent, we have shown that there is no optimal policy within the class of α-consistent policies, with α > 0.2. Consequently, there do not exist optimal policies in the class of Hannan consistent policies either. Moreover, Theorem 8 shows that methods that would be inspired by related literature in adversarial bandit can not apply to our framework. As we said, this impossibility may come from the fact that we can not observe at each time step the decisions and rewards of more than one algorithm. If we were able to observe a given set of policies from step to step, then it would be easy to beat them all: it would be sufﬁcient to aggregate all the observations and simply pull the arm with the greater empiric mean. The case where we only observe decisions (and not rewards) of a set of policies may be interesting, but is left outside of the scope of this paper. Proof Assume by contradiction that ∃A , ∀θ ∈ Θ, lim sup un,θ ≤ 1, n→+∞ [Rn where un,θ = min(E [R (Eθ)],E(A )](UCB(ρ))]) . ˜ θ n A θ [Rn For any θ, we have Eθ [Rn (A )] = Eθ [Rn (A )] ˜ ˜ Eθ [Rn (A )] ≤ un,θ Eθ [Rn (A )], ˜ Eθ [Rn (A )] (9) ˜ so that the fact that A is a consistent policy implies that A is also consistent. Consequently the lower bound of Theorem 6 also holds for policy A . For the rest of the proof, we focus on environments of the form θ = (δ0 , δ∆ ) with ∆ > 0. In this case, arm 2 is the best arm, so that we have to compute D1 (θ). On the one hand, we have: D1 (θ) = inf ˜ ν1 ∈Θ1 :E[˜ 1 ν ]>µ∗ ˜ KL(ν1 , ν1 ) = inf ˜ ν1 ∈Θ1 :E[˜ 1 ]>∆ ν 200 ˜ KL(δ0 , ν1 ) = inf ˜ ν1 ∈Θ1 :E[˜ 1 ]>∆ ν log 1 . ˜ ν1 (0) L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS ˜ ˜ As E[ν1 ] ≤ 1 − ν1 (0), we get: D1 (θ) ≥ inf ˜ ν1 ∈Θ1 :1−˜ 1 (0)≥∆ ν log 1 ˜ ν1 (0) ≥ log 1 . 1−∆ One the other hand, we have for any ε > 0: D1 (θ) ≤ KL(δ0 , Ber(∆ + ε)) = log Consequently D1 (θ) = log 1 1−∆ 1 1−∆−ε , and the lower bound of Theorem 6 reads: lim inf n→+∞ 1 Eθ [T1 (n, A )] . ≥ 1 log n log 1−∆ Just like Equation (9), we have: Eθ [Rn (A )] ≤ un,θ Eθ [Rn (UCB(ρ))]. Moreover, Lemma 3 provides: Eθ [Rn (UCB(ρ))] ≤ 1 + ρ log n . ∆ Now, by gathering the three previous inequalities and Formula (1), we get: 1 log 1 1−∆ ≤ lim inf n→+∞ Eθ [T1 (n, A )] Eθ [Rn (A )] = lim inf n→+∞ log n ∆ log n un,θ Eθ [Rn (UCB(ρ))] un,θ ρ log n 1+ ≤ lim inf n→+∞ ∆ log n ∆ log n ∆ ρun,θ un,θ ρ ρ + lim inf 2 = 2 lim inf un,θ ≤ 2 lim sup un,θ ≤ lim inf n→+∞ ∆ n→+∞ ∆ log n ∆ n→+∞ ∆ n→+∞ ρ . ≤ ∆2 ≤ lim inf n→+∞ This means that ρ has to be lower bounded by ∆2 , 1 log( 1−∆ ) but this is greater than 0.4 if ∆ = 0.75, hence the contradiction. Note that this proof gives a simple alternative to Theorem 4 to show that UCB(ρ) is not consistent (if ρ ≤ 0.4). Indeed if it were consistent, then in environment θ = (δ0 , δ∆ ) the same contradiction between the lower bound of Theorem 6 and the upper bound of Lemma 3 would hold. 5. General Bounds In this section, we study lower bounds on the expected regret with few requirements on Θ and on the class of policies. With a simple property on Θ but without any assumption on the policy, we show that there always exist logarithmic lower bounds for some environments θ. Then, still with a 201 S ALOMON , AUDIBERT AND E L A LAOUI simple property on Θ, we show that there exists a Hannan consistent policy for which the expected regret is sub-logarithmic for some environment θ. Note that the policy that always pulls arm 1 has a 0 expected regret in environments where arm 1 has the best mean reward, and an expected regret of order n in other environments. So, for this policy, expected regret is sub-logarithmic in some environments. Nevertheless, this policy is not Hannan consistent because its expected regret is not always o(n). 5.1 The Necessity of a Logarithmic Regret in Some Environments The necessity of a logarithmic regret in some environments can be explained by a simple sketch proof. Assume that the agent knows the number of rounds n, and that he balances exploration and exploitation in the following way: he ﬁrst pulls each arm s(n) times, and then selects the arm that has obtained the best empiric mean for the rest of the game. Denote by ps(n) the probability that the best arm does not have the best empiric mean after the exploration phase (i.e., after the ﬁrst Ks(n) rounds). The expected regret is then of the form c1 (1 − ps(n) )s(n) + c2 ps(n) n. (10) Indeed, if the agent manages to match the best arm then he only suffers the pulls of suboptimal arms during the exploration phase. That represents an expected regret of order s(n). If not, the number of pulls of suboptimal arms is of order n, and so is the expected regret. Now, let us approximate ps(n) . It has the same order as the probability that the best arm gets X ∗ −µ∗ an empiric mean lower than the second best mean reward. Moreover, k ,s(n) s(n) (where σ is σ ∗ ,1 ) has approximately a standard normal distribution by the central limit theorem. the variance of Xk Therefore, we have: ps(n) ≈ Pθ (Xk∗ ,s(n) ≤ µ∗ − ∆) = Pθ ≈ ≈  σ 1 1 √ exp − 2 2π ∆ s(n) Xk∗ ,s(n) − µ∗ σ  2 ∆ s(n)  σ s(n) ≤ − ∆ s(n) σ 1 σ ∆2 s(n) √ . exp − 2σ2 2π ∆ s(n) It follows that the expected regret has to be at least logarithmic. Indeed, to ensure that the second term c2 ps(n) n of Equation (10) is sub-logarithmic, s(n) has to be greater than log n. But then ﬁrst term c1 (1 − ps(n) )s(n) is greater than log n. Actually, the necessity of a logarithmic regret can be written as a consequence of Theorem 6. n Indeed, if we assume by contradiction that lim supn→+∞ Eθ Rn = 0 for all θ (i.e., Eθ Rn = o(log n)), log the considered policy is consistent. Consequently, Theorem 6 implies that lim sup n→+∞ E θ Rn E θ Rn ≥ lim inf > 0. n→+∞ log n log n Yet, this reasoning needs Θ having the product property, and conditions of the form 0 < Dk (θ) < ∞ also have to hold. The following proposition is a rigorous version of our sketch, and it shows that the necessity of a logarithmic lower bound can be based on a simple property on Θ. 202 L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS ˜ ˜ ˜ Proposition 9 Assume that there exist two environments θ = (ν1 , . . . , νK ) ∈ Θ, θ = (ν1 , . . . , νK ) ∈ Θ, and an arm k ∈ {1, . . . , K} such that 1. k has the best mean reward in environment θ, ˜ 2. k is not the winning arm in environment θ, ˜ 3. νk = νk and there exists η ∈ (0, 1) such that dνℓ ∏ d νℓ (Xℓ,1 ) ≥ η ˜ ℓ=k Pθ − a.s. ˜ (11) ˆ Then, for any policy, there exists θ ∈ Θ such that lim sup n→+∞ E θ Rn ˆ > 0. log n ˜ Let us explain the logic of the three conditions of the proposition. If νk = νk , and in case νk seems to be the reward distribution of arm k, then arm k has to be pulled often enough for the regret to be small if the environment is θ. Nevertheless, one has to explore other arms to know ˜ whether the environment is actually θ. Moreover, Inequality (11) makes sure that the distinction ˜ is tough to make: it ensures that pulling any arm ℓ = k gives a reward which is between θ and θ likely in both environments. Without such an assumption the problem may be very simple, and providing a logarithmic lower bound is hopeless. Indeed, the distinction between any pair of tricky ˜ environments (θ, θ) may be solved in only one pull of a given arm ℓ = k, that would almost surely give a reward that is possible in only one of the two environments. The third condition can be seen as an alternate version of condition 0 < Dk (θ) < ∞ in Theorem 6, though there is no logical connection with it. Finally, let us remark that one can check that any set Θ that has the Dirac/Bernoulli property satisﬁes the conditions of Proposition 9. Proof The proof consists in writing a proper version of Expression (10). To this aim we compute a lower bound of Eθ Rn , expressed as a function of Eθ Rn and of an arbitrary function g(n). ˜ ˜ ˜ In the following, ∆k denotes the optimality gap of arm k in environment θ. As event ∑ℓ=k Tℓ (n) ≤ g(n) is measurable with respect to Xℓ,1 , . . . , Xℓ,⌊g(n)⌋ (ℓ = k) and to Xk,1 , . . . , Xk,n , we also introduce the function q such that ½{∑ℓ=k Tℓ (n)≤g(n)} = q (Xℓ,s )ℓ=k, s=1..⌊g(n)⌋ , (Xk,s )s=1..n . 203 S ALOMON , AUDIBERT AND E L A LAOUI We have: ˜ ˜ ˜ Eθ Rn ≥ ∆k Eθ [Tk (n)] ≥ ∆k (n − g(n))Pθ (Tk (n) ≥ n − g(n)) ˜ ˜ (12) ˜ = ∆k (n − g(n))Pθ n − ∑ Tℓ (n) ≥ n − g(n) ˜ ℓ=k ˜ = ∆k (n − g(n))Pθ ˜ ˜ = ∆k (n − g(n)) ∑ Tℓ (n) ≤ g(n) ℓ=k q (xℓ,s )ℓ=k, s=1..⌊g(n)⌋ , (xk,s )s=1..n ˜ ˜ ∏ d νℓ (xℓ,s )∏d νk (xk,s ) ℓ=k s = 1..⌊g(n)⌋ s=1..n ˜ ≥ ∆k (n − g(n)) q (xℓ,s )ℓ=k, s=1..⌊g(n)⌋ , (xk,s )s=1..n η⌊g(n)⌋∏ dνℓ (xℓ,s )∏dνk (xk,s ) ℓ=k s = 1..⌊g(n)⌋ ˜ ≥ ∆k (n − g(n))ηg(n) q (xℓ,s )ℓ=k, s=1..⌊g(n)⌋ , (xk,s )s=1..n ∏ dνℓ (xℓ,s )∏dνk (xk,s ) ℓ=k s = 1..⌊g(n)⌋ ˜ = ∆k (n − g(n))ηg(n) Pθ (13) s=1..n s=1..n ∑ Tℓ (n) ≤ g(n) ℓ=k ˜ = ∆k (n − g(n))ηg(n) 1 − Pθ ∑ Tℓ (n) > g(n) ℓ=k ˜ ≥ ∆k (n − g(n))ηg(n) 1 − Eθ ∑ℓ=k Tℓ (n) g(n) (14) ˜ ≥ ∆k (n − g(n))ηg(n) 1 − Eθ ∑ℓ=k ∆ℓ Tℓ (n) ∆g(n) (15) E θ Rn ˜ ≥ ∆k (n − g(n))ηg(n) 1 − , ∆g(n) where the ﬁrst inequality of (12) is a consequence of Formula (1), the second inequality of (12) and inequality (14) come from Markov’s inequality, Inequality (13) is a consequence of (11), and Inequality (15) results from the fact that ∆ℓ ≥ ∆ for all ℓ. n θ −− Now, let us conclude. If Eθ Rn − − → 0, we set g(n) = 2E∆Rn , so that log n→+∞ g(n) ≤ min n − log n 2 , 2 log η for n large enough. Then, we have: √ − log n ˜ k n − g(n) ηg(n) ≥ ∆k n η 2 log η = ∆k n . ˜ ˜ E θ Rn ≥ ∆ ˜ 2 4 4 In particular, Eθ Rn ˜ −− log n − − → n→+∞ +∞, and the result follows. 204 L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS 5.2 Hannan Consistency We will prove that there exists a Hannan consistent policy such that there can not be a logarithmic lower bound for every environment θ of Θ. To this aim, we make use of general UCB policies again (cf. Section 2.1). Let us ﬁrst give sufﬁcient conditions on the fk for UCB policy to be Hannan consistent. Proposition 10 Assume that fk (n) = o(n) for all k ∈ {1, . . . , K}. Assume also that there exist γ > 1 2 and N ≥ 3 such that fk (n) ≥ γ log log n for all k ∈ {1, . . . , K} and for all n ≥ N. Then UCB is Hannan consistent. Proof Fix an arm k such that ∆k > 0 and choose β ∈ (0, 1) such that 2βγ > 1. By means of Lemma 2, we have for n large enough: n Eθ [Tk (n)] ≤ u + 2 ∑ 1+ t=u+1 logt 1 log( β ) e−2βγ log logt , k where u = 4 f∆(n) . 2 k Consequently, we have: n Eθ [Tk (n)] ≤ u + 2 ∑ t=2 1 1 1 + 1 (logt)2βγ−1 2βγ (logt) log( β ) . (16) n n 1 Sums of the form ∑t=2 (logt)c with c > 0 are equivalent to (log n)c as n goes to inﬁnity. Indeed, on the one hand we have n n n 1 dx 1 ∑ (logt)c ≤ 2 (log x)c ≤ ∑ (logt)c , t=2 t=3 n 1 so that ∑t=2 (logt)c ∼ n dx 2 (log x)c . n 2 On the other hand, we have n x dx = c (log x) (log x)c n dx 2 (log x)c+1 n 1 n ∑t=2 (logt)c ∼ (log n)c n +c 2 2 dx . (log x)c+1 n dx 2 (log x)c n dx 2 (log x)c n (log n)c . As both integrals are divergent we have =o Combining the fact that constant C > 0 such that with Equation (16), we get the existence of a Eθ [Tk (n)] ≤ , so that ∼ Cn 4 fk (n) + . 2 ∆ (log n)2βγ−1 Since fk (n) = o(n) and 2βγ − 1 > 0, the latter inequality shows that Eθ [Tk (n)] = o(n). The result follows. We are now in the position to prove the main result of this section. Theorem 11 If Θ has the Dirac/Bernoulli property, there exist Hannan consistent policies for which the expected regret can not be lower bounded by a logarithmic function in all environments θ. 205 S ALOMON , AUDIBERT AND E L A LAOUI Proof If f1 (n) = f2 (n) = log log n for n ≥ 3, UCB is Hannan consistent by Proposition 10. According to Lemma 1, the expected regret is then of order log log n in environments of the form (δa , δb ), a = b. Hence the conclusion on the non-existence of logarithmic lower bounds. Thus we have obtained a lower bound of order log log n. This order is critical regarding the methods we used. Yet, we do not know if this order is optimal. Acknowledgments This work has been supported by the French National Research Agency (ANR) through the COSINUS program (ANR-08-COSI-004: EXPLO-RA project). References R. Agrawal. Sample mean based index policies with o(log n) regret for the multi-armed bandit problem. Advances in Applied Mathematics, 27:1054–1078, 1995. H. Akaike. Information theory and an extension of the maximum likelihood principle. In Second International Symposium on Information Theory, volume 1, pages 267–281. Springer Verlag, 1973. J.-Y. Audibert, R. Munos, and C. Szepesv´ ri. Exploration-exploitation tradeoff using variance estia mates in multi-armed bandits. Theoretical Computer Science, 410(19):1876–1902, 2009. P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47(2-3):235–256, 2002. P. Auer, N. Cesa-Bianchi, Y. Freund, and R.E. Schapire. The nonstochastic multiarmed bandit problem. SIAM Journal on Computing, 32(1):48–77, 2003. D. Bergemann and J. Valimaki. Bandit problems. In The New Palgrave Dictionary of Economics, 2nd ed. Macmillan Press, 2008. S. Bubeck. Bandits Games and Clustering Foundations. PhD thesis, Universit´ Lille 1, France, e 2010. S. Bubeck, R. Munos, G. Stoltz, and C. Szepesvari. Online optimization in X-armed bandits. In Advances in Neural Information Processing Systems 21, pages 201–208. 2009. A.N. Burnetas and M.N. Katehakis. Optimal adaptive policies for sequential allocation problems. Advances in Applied Mathematics, 17(2):122–142, 1996. N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, New York, NY, 2006. N. Cesa-Bianchi, Y. Freund, D. Haussler, D.P. Helmbold, R.E. Schapire, and M.K. Warmuth. How to use expert advice. Journal of the ACM (JACM), 44(3):427–485, 1997. 206 L OWER B OUNDS , W EAK C ONSISTENCY AND S ELECTIVITY IN BANDIT P ROBLEMS P.A. Coquelin and R. Munos. Bandit algorithms for tree search. In Uncertainty in Artiﬁcial Intelligence, 2007. S. Gelly and Y. Wang. Exploration exploitation in go: UCT for Monte-Carlo go. In Online Trading between Exploration and Exploitation Workshop, Twentieth Annual Conference on Neural Information Processing Systems (NIPS 2006), 2006. J. Honda and A. Takemura. An asymptotically optimal bandit algorithm for bounded support models. In Proceedings of the Twenty-Third Annual Conference on Learning Theory (COLT), 2010. R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandits in metric spaces. In Proceedings of the 40th Annual ACM Symposium on Theory of Computing, pages 681–690, 2008. R. D. Kleinberg. Nearly tight bounds for the continuum-armed bandit problem. In Advances in Neural Information Processing Systems 17, pages 697–704. 2005. T. L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Mathematics, 6:4–22, 1985. D. Lamberton, G. Pag` s, and P. Tarr` s. When can the two-armed bandit algorithm be trusted? e e Annals of Applied Probability, 14(3):1424–1454, 2004. C.L. Mallows. Some comments on cp. Technometrics, pages 661–675, 1973. H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematics Society, 58:527–535, 1952. G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6(2):461–464, 1978. W.R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933. 207</p><p>same-paper 3 0.87361264 <a title="30-lda-3" href="./jmlr-2013-Counterfactual_Reasoning_and_Learning_Systems%3A_The_Example_of_Computational_Advertising.html">30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</a></p>
<p>Author: Léon Bottou, Jonas Peters, Joaquin Quiñonero-Candela, Denis X. Charles, D. Max Chickering, Elon Portugaly, Dipankar Ray, Patrice Simard, Ed Snelson</p><p>Abstract: This work shows how to leverage causal inference to understand the behavior of complex learning systems interacting with their environment and predict the consequences of changes to the system. Such predictions allow both humans and algorithms to select the changes that would have improved the system performance. This work is illustrated by experiments on the ad placement system associated with the Bing search engine. Keywords: causation, counterfactual reasoning, computational advertising</p><p>4 0.44935805 <a title="30-lda-4" href="./jmlr-2013-Ranked_Bandits_in_Metric_Spaces%3A_Learning_Diverse_Rankings_over_Large_Document_Collections.html">94 jmlr-2013-Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections</a></p>
<p>Author: Aleksandrs Slivkins, Filip Radlinski, Sreenivas Gollapudi</p><p>Abstract: Most learning to rank research has assumed that the utility of different documents is independent, which results in learned ranking functions that return redundant results. The few approaches that avoid this have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisﬁed users, with several scalable algorithms that explicitly takes document similarity and ranking context into account. Our formulation is a non-trivial common generalization of two multi-armed bandit models from the literature: ranked bandits (Radlinski et al., 2008) and Lipschitz bandits (Kleinberg et al., 2008b). We present theoretical justiﬁcations for this approach, as well as a near-optimal algorithm. Our evaluation adds optimizations that improve empirical performance, and shows that our algorithms learn orders of magnitude more quickly than previous approaches. Keywords: online learning, clickthrough data, diversity, multi-armed bandits, contextual bandits, regret, metric spaces</p><p>5 0.41533178 <a title="30-lda-5" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>Author: Sébastien Gerchinovitz</p><p>Abstract: We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T . We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same ﬂavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with ﬁxed design. Keywords: sparsity, online linear regression, individual sequences, adaptive regret bounds</p><p>6 0.40768456 <a title="30-lda-6" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>7 0.40204385 <a title="30-lda-7" href="./jmlr-2013-Derivative_Estimation_with_Local_Polynomial_Fitting.html">31 jmlr-2013-Derivative Estimation with Local Polynomial Fitting</a></p>
<p>8 0.39107725 <a title="30-lda-8" href="./jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications.html">18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</a></p>
<p>9 0.39029938 <a title="30-lda-9" href="./jmlr-2013-Learning_Theory_Approach_to_Minimum_Error_Entropy_Criterion.html">62 jmlr-2013-Learning Theory Approach to Minimum Error Entropy Criterion</a></p>
<p>10 0.38941184 <a title="30-lda-10" href="./jmlr-2013-Learning_Theory_Analysis_for_Association_Rules_and_Sequential_Event_Prediction.html">61 jmlr-2013-Learning Theory Analysis for Association Rules and Sequential Event Prediction</a></p>
<p>11 0.38485205 <a title="30-lda-11" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>12 0.38140073 <a title="30-lda-12" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>13 0.37312928 <a title="30-lda-13" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>14 0.36365163 <a title="30-lda-14" href="./jmlr-2013-Optimal_Discovery_with_Probabilistic_Expert_Advice%3A_Finite_Time_Analysis_and_Macroscopic_Optimality.html">81 jmlr-2013-Optimal Discovery with Probabilistic Expert Advice: Finite Time Analysis and Macroscopic Optimality</a></p>
<p>15 0.35956413 <a title="30-lda-15" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>16 0.3566083 <a title="30-lda-16" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>17 0.35031179 <a title="30-lda-17" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>18 0.34738117 <a title="30-lda-18" href="./jmlr-2013-Fast_MCMC_Sampling_for_Markov_Jump_Processes_and_Extensions.html">43 jmlr-2013-Fast MCMC Sampling for Markov Jump Processes and Extensions</a></p>
<p>19 0.34264129 <a title="30-lda-19" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>20 0.3406918 <a title="30-lda-20" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
