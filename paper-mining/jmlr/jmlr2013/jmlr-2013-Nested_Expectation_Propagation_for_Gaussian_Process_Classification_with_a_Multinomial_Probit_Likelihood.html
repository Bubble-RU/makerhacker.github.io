<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-75" href="#">jmlr2013-75</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</h1>
<br/><p>Source: <a title="jmlr-2013-75-pdf" href="http://jmlr.org/papers/volume14/riihimaki13a/riihimaki13a.pdf">pdf</a></p><p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>Reference: <a title="jmlr-2013-75-reference" href="../jmlr2013_reference/jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. [sent-10, score-0.282]
</p><p>2 In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. [sent-12, score-0.363]
</p><p>3 The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. [sent-13, score-0.242]
</p><p>4 In multiclass GP classiﬁcation, the posterior inference is challenging because each target class increases the number of unknown latent variables by the number of observations n. [sent-17, score-0.282]
</p><p>5 a a  ¨ ¨ R IIHIM AKI , J YL ANKI AND V EHTARI  As an additional challenge, the posterior inference is analytically intractable because the likelihood term related to each observation is non-Gaussian and depends on multiple latent values (one for each class). [sent-23, score-0.3]
</p><p>6 Later Girolami and Rogers (2006) proposed an alternative approach based on the multinomial probit likelihood which can be augmented with auxiliary latent variables. [sent-26, score-0.371]
</p><p>7 To speed up the inference, Williams and Barber (1998) used the Laplace approximation (LA) to approximate the non-Gaussian posterior distribution of the latent function values with a tractable Gaussian distribution. [sent-29, score-0.281]
</p><p>8 Assuming the latent values and the auxiliary variables a posteriori independent, a computationally efﬁcient posterior approximation scheme is obtained. [sent-32, score-0.281]
</p><p>9 Incorporating the full posterior couplings requires evaluating the nonanalytical moments of c-dimensional tilted distributions which Girolami and Zhong (2007) approximated with Laplace’s method resulting in an approximation scheme known as Laplace propagation described by Smola et al. [sent-39, score-0.394]
</p><p>10 Earlier Seeger and Jordan (2004) proposed an alternative approach where the full posterior dependencies were approximated by enforcing a similar structure for the posterior covariance as in LA using the softmax likelihood. [sent-41, score-0.36]
</p><p>11 This enables a posterior representation scaling as O ((c + 1)n3 ) but the proposed implementation requires a c-dimensional numerical quadrature and double-loop optimization to obtain a restricted-form site covariance approximation for each likelihood term (Seeger and Jordan, 2004). [sent-42, score-0.474]
</p><p>12 Both approaches omit the between-class posterior dependencies of the latent values which results in a posterior representation scaling as O (cn3 ). [sent-47, score-0.385]
</p><p>13 The approaches rely on numerical two-dimensional quadratures for evaluating the moments of the tilted distributions with the main difference being that Seeger et al. [sent-48, score-0.277]
</p><p>14 Contrary to the usual EP approach of maximizing the marginal likelihood approximation, Kim and Ghahramani (2006) determined the hyperparameters by maximizing a lower bound on the log marginal likelihood in a similar way as is done in the expectation maximization (EM) algorithm. [sent-54, score-0.315]
</p><p>15 In this paper, we focus on the multinomial probit model and describe an efﬁcient quadraturefree nested EP approach for multiclass GP classiﬁcation that scales as O ((c + 1)n3 ). [sent-58, score-0.328]
</p><p>16 The proposed EP method takes into account all the posterior covariances between the latent variables, and the posterior computations scale as efﬁciently as in the LA approximation. [sent-59, score-0.36]
</p><p>17 We visualize the accuracy of the approximate marginal distributions with respect to MCMC, illustrate the suitability of the respective marginal likelihood approximations for type-II MAP estimation of the covariance function hyperparameters, and discuss their computational complexities. [sent-64, score-0.29]
</p><p>18 Element Ki, j of the k’th covariance matrix deﬁnes the prior covariance between the function values fik and f jk , which is deﬁned by the covariance function κ(xi , x j ), that k is, Ki, j = κ(xi , x j ) = Cov fik , f jk for the latent values related to class k. [sent-102, score-0.285]
</p><p>19 The softmax and multinomial probit models are multiclass generalizations of the logistic and the probit models respectively. [sent-113, score-0.36]
</p><p>20 Both i=1 observation models result in an analytically intractable posterior distribution and therefore approximate methods are needed for integration over the latent variables. [sent-115, score-0.262]
</p><p>21 Then we present a novel nested EP approach for the multinomial probit model. [sent-119, score-0.289]
</p><p>22 In the ˜ algorithm, ﬁrst the site approximations are initialized, and then each site is updated in turns. [sent-123, score-0.287]
</p><p>23 The update for the i’th site is done by ﬁrst removing the site term from the marginal posterior which gives the cavity distribution ˜ q−i (fi ) = N (fi |µ−i , Σ−i ) ∝ q(fi |D , θ)t (fi )−1 . [sent-124, score-0.464]
</p><p>24 The cavity distribution is then combined with the exact i’th likelihood term p(yi |fi ) to form the non-Gaussian tilted distribution ˆ p(fi ) = Zi−1 q−i (fi )p(yi |fi ), ˆ  (5)  which is assumed to encompass more information about the true marginal distribution. [sent-125, score-0.302]
</p><p>25 After updating the ˆ site parameters, the posterior distribution (4) is updated. [sent-128, score-0.242]
</p><p>26 This can be done either in a sequential way, where immediately after each site update the posterior is refreshed using a rank-c update, or in a parallel way (see, for example, van Gerven et al. [sent-129, score-0.242]
</p><p>27 , 2009), where the posterior is refreshed only after all the site approximations have been updated once. [sent-130, score-0.279]
</p><p>28 In the multiclass setting, the problem is how to evaluate the multi-dimensional integrals which are required to determine the moments of the tilted distributions (5). [sent-133, score-0.241]
</p><p>29 Because of the skewness of the tilted distribution caused by the likelihood function, the LA method can lead to inaccurate mean and covariance estimates in which case the resulting posterior approximation does not correspond to the full EP solution. [sent-137, score-0.412]
</p><p>30 The existing approaches for the multinomial probit rely on multiple numerical quadratures for each site update; the implementation of Girolami and Zhong (2007) requires a total of 2c+1 two-dimensional numerical quadratures for each likelihood term, whereas Seeger et al. [sent-141, score-0.501]
</p><p>31 Later, we will demonstrate that compared to the full EP approximation, IEP underestimates the uncertainty on the latent values and in practice it may require more iterations than full EP for convergence especially if the hyperparameter setting results in strong between-class posterior couplings. [sent-143, score-0.349]
</p><p>32 2 Efﬁciently Scaling Quadrature-Free Implementation In this section, we present a novel nested EP approach for multinomial probit classiﬁcation that does not require numerical quadratures or sampling for estimation of the tilted moments and predictive probabilities. [sent-145, score-0.651]
</p><p>33 The method also leads simultaneously to low-rank site approximations which retain all posterior couplings but results in linear computational scaling with respect to the number of target classes c. [sent-146, score-0.323]
</p><p>34 1 Q UADRATURE -F REE N ESTED E XPECTATION P ROPAGATION Here we use the multinomial probit as the likelihood function because its product form consisting of cumulative Gaussian factors is computationally more suitable for EP than the sum of exponential terms in the softmax likelihood. [sent-150, score-0.281]
</p><p>35 i To form a computationally efﬁcient EP algorithm for approximating the tilted moments, it is helpful to consider the joint distribution of fi and the auxiliary variable ui arising from (6). [sent-157, score-0.306]
</p><p>36 Deﬁning wi = [fT , ui ]T and removing the marginalization over ui results in the following augmented tilted i distribution: ˆ p(wi ) = Zi−1 N (wi |µwi , Σwi ) ˆ  c  ∏  ˜ Φ(wT bi, j ), i  (7)  j=1, j=yi  where µwi = [µT , 0]T and Σwi is a block-diagonal matrix formed from Σ−i and 1. [sent-158, score-0.275]
</p><p>37 ˆ The augmented distribution (7) is of similar functional form as the posterior distribution resulting from a linear binary classiﬁer with a multivariate Gaussian prior on the weights wi and a probit likelihood function. [sent-162, score-0.34]
</p><p>38 ˜ j=y Note that the probit terms in Equation (7) depend on the unknown latents fi only through the j j ˜ ˜ ˜i linear transformation gi = BT wi , where Bi = [bi, j ] j=yi , that is gi = fiyi − fi +ui . [sent-168, score-0.57]
</p><p>39 This relation implies that the likelihood of fi increases as the latent value associated with the correct class yi increases compared to the latents associated with the other classes. [sent-169, score-0.341]
</p><p>40 Integration over the auxiliary variable ui results from the conic truncation of the latent variable representation of the multinomial probit model (see, for example, Girolami and Rogers, 2006). [sent-170, score-0.343]
</p><p>41 First, the fully-coupled nested EP solution can be computed j by propagating scalar moments of gi which requires solving only one-dimensional integrals because j each probit factor in the augmented tilted distribution depends only on the scalar gi (see Appendix A and references therein). [sent-172, score-0.545]
</p><p>42 Second, it can be shown that the exact mean and covariance of wi ∼ p(wi ) can be solved from the respective moments of gi whose distribution is obtained by gi = ˆ ˜i BT wi on Equation (7). [sent-173, score-0.371]
</p><p>43 Because the dimension of gi is c − 1 we can form computationally cheaper quadrature-based estimates of the tilted moments as described in Section 3. [sent-174, score-0.265]
</p><p>44 We will also use the approximate marginal moments of gi to visualize differences in the predictive accuracy of EP and IEP approximations in Section 5. [sent-176, score-0.338]
</p><p>45 The approximate marginal covariance of fi derived from (8) is given by ˆ ˜ ˜ ˜i Σi = H T Σ−1 + Bi Ti BT wi  −1  H,  (9)  ˜ where the matrix Ti = diag(αi ) is diagonal,2 and H T = Ic 0 picks up the desired components ˜ T w . [sent-182, score-0.313]
</p><p>46 The marginal mean of fi with respect to q(wi ) is given by ˜ ˜ ˆ i ˜ ˜ ˜i µi = HiT Σ−1 + Bi Ti BT ˆ wi  −1  ˜ ˜ Σ−1 µwi + Bi βi , wi  (13)  2. [sent-194, score-0.31]
</p><p>47 It follows that the posterior ˜ (predictive) means and covariances as well as the marginal likelihood can be evaluated with similar computational complexity as with the Laplace approximation (Williams and Barber, 1998; Rasmussen and Williams, 2006). [sent-200, score-0.255]
</p><p>48 3 E FFICIENT I MPLEMENTATION Approximating the tilted moments using inner EP for each site may appear too slow for larger problems because typically several iterations are required to achieve convergence. [sent-205, score-0.347]
</p><p>49 Apart from the integration over the auxiliary variables ui , Equation (16) j resembles an EP approximation where n(c − 1) probit terms of the form Φ(ui + fiyi − fi ) are approximated with Gaussian site functions. [sent-209, score-0.43]
</p><p>50 In our experiments, only one inner-loop iteration per site was found sufﬁcient for convergence with comparable number of outer-loop iterations, which results in signiﬁcant computational savings in the tilted moment evaluations. [sent-213, score-0.273]
</p><p>51 Damping cannot be directly applied on the site precision matrix Πi = Σ−1 i because the constrained form of the site precision (12) is lost. [sent-216, score-0.298]
</p><p>52 Convergence of the nested EP algorithm with full posterior couplings using this scheme is illustrated with different damping levels in Section 5. [sent-219, score-0.318]
</p><p>53 3 Quadrature-Based Full EP Implementation A challenge in forming the fully-coupled EP approximation using numerical quadratures is how to obtain a site precision structure, which results in efﬁciently scaling posterior computations. [sent-222, score-0.385]
</p><p>54 2 to form a simpler fully-coupled EP algorithm that uses similar approximate site precision structures determined directly using (c − 1)-dimensional quadratures instead of separate optimizations. [sent-225, score-0.243]
</p><p>55 ˜i ˆ We use the previously deﬁned transformation gi = BT wi , where wi ∼ p(wi ), and denote the tilted ˆ mean vector and covariance matrix of wi with µwi and Σwi . [sent-226, score-0.471]
</p><p>56 i ˆ The marginal tilted covariance of fi can be computed from Σwi similarly as in Equations (9) and ˜ −1 can be computed as in Equation (11) with Λi (10), and the corresponding site precision matrix Σi ˜ now in place of Ti . [sent-230, score-0.522]
</p><p>57 The form of the site precision is similar to nested EP, except that now Λi is a full matrix, which would result in the unfavorable O ((c − 1)3 n3 ) posterior scaling. [sent-232, score-0.404]
</p><p>58 This results in posterior computations scaling linearly in c similarly as with the full nested EP approach. [sent-234, score-0.28]
</p><p>59 To estimate the site location parameter νi using quadratures, we proceed in the same way as for ˜ the site precision. [sent-235, score-0.25]
</p><p>60 This site location structure is similar to nested EP (15) with ξi ˜ i . [sent-240, score-0.245]
</p><p>61 1, we validate this approximate (c − 1)dimensional quadrature approach by comparing the tilted moments to those of a more expensive straightforward (c + 1)-dimensional full quadrature solution. [sent-243, score-0.395]
</p><p>62 1 I MPROVING M ARGINAL P OSTERIOR D ISTRIBUTIONS In Gaussian process classiﬁcation, the LA and EP methods can be used to efﬁciently form a multivariate Gaussian approximation for the posterior distribution of the latent values. [sent-255, score-0.262]
</p><p>63 Recently, motivated by the earlier ideas of Tierney and Kadane (1986), two methods have been proposed for improving the marginal posterior distributions in latent Gaussian models; one based on subsequent use of Laplace’s method (Rue et al. [sent-256, score-0.305]
</p><p>64 Given the hyperparameter values, the latent values are drawn from the conditional posterior 86  G AUSSIAN P ROCESS C LASSIFICATION WITH A M ULTINOMIAL P ROBIT L IKELIHOOD  p(f|D , θ) using the scaled Metropolis-Hastings sampling (Neal, 1998). [sent-282, score-0.29]
</p><p>65 The multinomial probit likelihood (2) can be written in the form c  p(yi |fi ) =  j  j  ψ(vyi > vk ∀k = yi ) ∏ N (vi | fi , 1)dvi , i i  (24)  j=1  where vi = [v1 , . [sent-288, score-0.357]
</p><p>66 3, we compare visually the quality of the approximate marginal distributions of f, the marginal likelihood approximations and the predictive class probabilities between EP, IEP, VB and LA using a three-class real-world data set. [sent-308, score-0.345]
</p><p>67 In the ﬁrst experiment, we examine the tilted moments after two parallel EP outer-loop iterations when the parameters of the cavity distributions are clearly different from their initialized values for all site terms. [sent-328, score-0.362]
</p><p>68 Because QD10 agrees well with QF10, from now on, we use it to compute the tilted moments in the full quadrature solution, and refer to this algorithm as QEP. [sent-346, score-0.298]
</p><p>69 Both nested EP algorithms are implemented incrementally, so that only one inner-loop iteration per site is done at each outer-loop iteration step, which results in computational savings (see Section 3. [sent-349, score-0.245]
</p><p>70 The marginal distribution is non-Gaussian, and the latent values are more likely larger for class 1 than for class 2 indicating a larger predictive probability for class 1. [sent-534, score-0.273]
</p><p>71 To illustrate the effect of these differences on the predictive probabilities, we show the unnormalized tilted distributions c  p(gi |D , xi ) = q(gi |D , xi ) ˆ  ∏  Φ(gk ), i  (25)  k=1,k=yi  where the random vector gi is deﬁned in Section 3. [sent-537, score-0.314]
</p><p>72 Note that the marginal predictive probability for class label yi with the multinomial probit model (2) can be obtained by appropriately forming the transformation Bi and calculating the integral over gi in (25). [sent-540, score-0.4]
</p><p>73 91  ¨ ¨ R IIHIM AKI , J YL ANKI AND V EHTARI  MCMC  EP  IEP  10  −10  latent fi  0  2  10  2 i  20  latent f  latent fi  20  10  2  20  0 −10  −20  −10  −20  −10  0 10 1 20 latent f  30  −20 −10  i  (a)  30  −10  40 g3 i  g3 i  0  20 0  20 g2 i  (d)  40  30  IEP, p=0. [sent-548, score-0.724]
</p><p>74 95  0  0  20 0  0  20 g2 i  40  0  (e)  20 g2 i  40  (f)  Figure 3: An example of a non-Gaussian marginal posterior distribution for the latent values related to the input xi in the synthetic example shown in Figure 2. [sent-551, score-0.305]
</p><p>75 The second location x j is near the class boundary, where all the methods give similar predictive probabilities, although the latent approximations can differ notably as shown in Figures 4(a)-(c), which visualize the marginal approximations for f j2 and f j3 . [sent-561, score-0.347]
</p><p>76 3 Approximate Marginal Densities with Digit Classiﬁcation Data In this section, we compare the predictive performances and marginal likelihood approximations of EP, IEP, VB and LA using the USPS 3 vs. [sent-567, score-0.241]
</p><p>77 We ﬁxed the hyperparameter values at log(σ2 ) = 4 and log(l) = 2 which leads to skewed non-Gaussian marginal posterior distributions as will be illustrated shortly. [sent-581, score-0.248]
</p><p>78 Figure 6 shows an example of the latent marginal posterior distributions for one training point with the correct class label being 2. [sent-587, score-0.305]
</p><p>79 The LA approximation captures some of the dependencies between the latent variables associated with different classes, but the joint mode of f is a poor estimate for the true mean, which causes inaccurate predictive probabilities (plots (d) and (i) in Figure 5). [sent-622, score-0.253]
</p><p>80 They compared the calibration of predictive performance and the marginal likelihood estimates on a grid of hyperparameter values. [sent-638, score-0.251]
</p><p>81 The ﬁrst row shows the log marginal likelihood approximations, the second row shows the log predictive densities in a test set, and the third row shows the classiﬁcation accuracies in a test set. [sent-715, score-0.31]
</p><p>82 The log predictive densities of VB and LA are small where log(σ2 ) is large (regions where q(f|D , θ) is likely to be non-Gaussian), but on the other hand, also the marginal likelihood approximations favor the areas of smaller log(σ2 ) values. [sent-720, score-0.29]
</p><p>83 The column Likelihood complexity of Table 2 approximates the scaling of the number of calculations that are required besides the posterior mean and covariance evaluations (mainly likelihood related computations for one iteration). [sent-735, score-0.252]
</p><p>84 The cubic scaling in c − 1 of the tilted moment evaluations in the nested EP and IEP algorithms can be alleviated by reducing the number of inner-loop iterations nin as discussed in Section 3. [sent-743, score-0.293]
</p><p>85 In the CPU time comparisons across the range of hyperparameter values producing variety of skewed and non-isotropic posterior distributions, fully-coupled nested EP converged in fewer outerloop iterations than nested IEP if the same convergence criteria were used. [sent-760, score-0.426]
</p><p>86 For both methods, the negative log marginal likelihood approximation − log ZEP and the mean log predictive density (mlpd) in the test data set are shown after each iteration. [sent-768, score-0.304]
</p><p>87 With the fully-coupled nested EP algorithm the damping ˜ is applied on the inner-EP site parameters αi and βi , whereas with IEP the damping is applied on ˜ ˜ −1 . [sent-773, score-0.333]
</p><p>88 The ﬁrst two rows show the negative log marginal likelihood estimates − log ZEP as a function of iterations for two different damping factors δ, and the bottom two rows show the corresponding mean log predictive density (mlpd) evaluated using a separate test data set. [sent-786, score-0.329]
</p><p>89 In the columns denoted Standard the inner-loops of the nested EP and IEP algorithms are run until convergence at each outer-loop iteration, whereas in the rest of the columns (Incremental) only one inner-loop iteration per site is done at each outer-loop iteration. [sent-787, score-0.245]
</p><p>90 We compare the performances of nested EP and IEP, VB, LA, LA-TKP, and Gibbs sampling with the multinomial probit model (MCMC) on various benchmark data sets. [sent-797, score-0.289]
</p><p>91 To highlight the differences between the methods more clearly, we compute the pairwise differences of the log posterior predictive densities with respect to EP. [sent-813, score-0.316]
</p><p>92 In this paper, we have complemented their work with a novel quadrature-free nested EP algorithm that maintains all between-class posterior dependencies but still scales linearly in the number of classes. [sent-943, score-0.237]
</p><p>93 Our comparisons with ﬁxed hyperparameters show that compared to quadrature-based EP algorithms, nested EP achieves similar accuracy, and its computational cost is comparable with a class-independent approximation whereas with full posterior couplings nested EP scales more efﬁciently. [sent-944, score-0.463]
</p><p>94 In terms of predictive density, nested EP is close to MCMC, and more accurate compared to VB and LA, but if only the classiﬁcation accuracy is concerned, all the approximations perform similarly. [sent-966, score-0.242]
</p><p>95 In our comparisons the predictive accuracies of the full EP and IEP solutions obtained using the nested EP algorithm are similar for practical purposes. [sent-968, score-0.253]
</p><p>96 However, our visualizations show that the approximate marginal posterior distributions of the latent values provided by full EP are clearly more accurate, although the full nested EP solution can be calculated with similar computational burden as nested IEP. [sent-969, score-0.6]
</p><p>97 With the same hyperparameter values, nested IEP converged more slowly and required more damping than full nested EP. [sent-971, score-0.349]
</p><p>98 Approximating Tilted Moments Using EP For convenience, we summarize the inner EP algorithm for approximating the tilted moments resulting from a multinomial probit likelihood. [sent-995, score-0.391]
</p><p>99 Given E[f∗ ] and Cov[f∗ ], the integration over the posterior uncertainty of f∗ required to compute the predictive class probabilities, is equivalent to the tilted moment evaluation, and can be approximated using the algorithm described in Appendix A. [sent-1039, score-0.35]
</p><p>100 Approximate Bayesian inference for latent Gausa sian models by using integrated nested Laplace approximations. [sent-1139, score-0.246]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ep', 0.606), ('iep', 0.497), ('vb', 0.179), ('la', 0.17), ('tilted', 0.148), ('latent', 0.126), ('site', 0.125), ('nested', 0.12), ('mcmc', 0.118), ('posterior', 0.117), ('fi', 0.11), ('latkp', 0.107), ('usps', 0.103), ('probit', 0.097), ('predictive', 0.085), ('robit', 0.08), ('ultinomial', 0.08), ('quadrature', 0.078), ('quadratures', 0.075), ('aki', 0.072), ('multinomial', 0.072), ('wi', 0.069), ('anki', 0.065), ('ehtari', 0.065), ('iihim', 0.065), ('ikelihood', 0.065), ('glass', 0.064), ('gi', 0.063), ('qep', 0.062), ('marginal', 0.062), ('rocess', 0.058), ('qiep', 0.058), ('likelihood', 0.057), ('girolami', 0.056), ('softmax', 0.055), ('moments', 0.054), ('seeger', 0.053), ('mc', 0.053), ('zqi', 0.053), ('covariance', 0.053), ('gp', 0.053), ('hyperparameters', 0.05), ('aussian', 0.05), ('hyperparameter', 0.047), ('lassification', 0.045), ('damping', 0.044), ('yl', 0.043), ('bi', 0.041), ('multiclass', 0.039), ('approximations', 0.037), ('contour', 0.036), ('laplace', 0.035), ('bt', 0.035), ('cavity', 0.035), ('teaching', 0.034), ('zhong', 0.034), ('fiyi', 0.031), ('ln', 0.031), ('rasmussen', 0.031), ('accuracies', 0.03), ('ui', 0.029), ('pairwise', 0.029), ('credible', 0.028), ('log', 0.027), ('qi', 0.027), ('latents', 0.027), ('eyi', 0.027), ('classi', 0.026), ('scaling', 0.025), ('gibbs', 0.025), ('williams', 0.024), ('precision', 0.024), ('zi', 0.024), ('rogers', 0.024), ('probabilities', 0.023), ('underestimates', 0.023), ('tierney', 0.023), ('zep', 0.023), ('kadane', 0.022), ('mlpd', 0.022), ('skewed', 0.022), ('densities', 0.022), ('yi', 0.021), ('inner', 0.02), ('barber', 0.019), ('couplings', 0.019), ('ti', 0.019), ('cseke', 0.019), ('propagation', 0.019), ('auxiliary', 0.019), ('approximation', 0.019), ('approximate', 0.019), ('wine', 0.018), ('minka', 0.018), ('jordan', 0.018), ('differences', 0.018), ('segmentation', 0.018), ('full', 0.018), ('gaussian', 0.018), ('icn', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="75-tfidf-1" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>2 0.27968559 <a title="75-tfidf-2" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>Author: Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Pierre Dupont</p><p>Abstract: We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efﬁciently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about speciﬁc groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy. Keywords: group feature selection, generalized spike-and-slab priors, expectation propagation, sparse linear model, approximate inference, sequential experimental design, signal reconstruction</p><p>3 0.27807823 <a title="75-tfidf-3" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian ﬁeld, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model’s partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. Through the expansion, we see that EP is correct to ﬁrst order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. Keywords: expectation consistent inference, expectation propagation, perturbation correction, Wick expansions, Ising model, Gaussian process</p><p>4 0.23465608 <a title="75-tfidf-4" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>5 0.15393129 <a title="75-tfidf-5" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>6 0.077534661 <a title="75-tfidf-6" href="./jmlr-2013-Global_Analytic_Solution_of_Fully-observed_Variational_Bayesian_Matrix_Factorization.html">49 jmlr-2013-Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization</a></p>
<p>7 0.061840344 <a title="75-tfidf-7" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>8 0.058700081 <a title="75-tfidf-8" href="./jmlr-2013-Variational_Inference_in_Nonconjugate_Models.html">121 jmlr-2013-Variational Inference in Nonconjugate Models</a></p>
<p>9 0.048981477 <a title="75-tfidf-9" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>10 0.047159623 <a title="75-tfidf-10" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>11 0.043339789 <a title="75-tfidf-11" href="./jmlr-2013-Fast_MCMC_Sampling_for_Markov_Jump_Processes_and_Extensions.html">43 jmlr-2013-Fast MCMC Sampling for Markov Jump Processes and Extensions</a></p>
<p>12 0.040599771 <a title="75-tfidf-12" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>13 0.039343495 <a title="75-tfidf-13" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>14 0.037921757 <a title="75-tfidf-14" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>15 0.034916166 <a title="75-tfidf-15" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>16 0.033246063 <a title="75-tfidf-16" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>17 0.031335346 <a title="75-tfidf-17" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>18 0.030079642 <a title="75-tfidf-18" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>19 0.026260549 <a title="75-tfidf-19" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>20 0.025465544 <a title="75-tfidf-20" href="./jmlr-2013-Pairwise_Likelihood_Ratios_for_Estimation_of_Non-Gaussian_Structural_Equation_Models.html">85 jmlr-2013-Pairwise Likelihood Ratios for Estimation of Non-Gaussian Structural Equation Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.208), (1, -0.379), (2, 0.135), (3, -0.072), (4, 0.185), (5, -0.241), (6, -0.372), (7, -0.18), (8, -0.284), (9, 0.074), (10, 0.009), (11, 0.033), (12, 0.01), (13, -0.004), (14, 0.018), (15, -0.001), (16, 0.028), (17, -0.022), (18, -0.061), (19, -0.021), (20, -0.043), (21, 0.038), (22, 0.048), (23, 0.042), (24, 0.019), (25, 0.039), (26, 0.001), (27, 0.061), (28, 0.036), (29, -0.05), (30, 0.005), (31, 0.004), (32, -0.029), (33, -0.03), (34, 0.049), (35, -0.044), (36, -0.003), (37, -0.002), (38, 0.03), (39, 0.019), (40, -0.017), (41, 0.015), (42, -0.007), (43, -0.003), (44, -0.052), (45, -0.057), (46, 0.014), (47, -0.002), (48, 0.027), (49, -0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9644708 <a title="75-lsi-1" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>2 0.78812706 <a title="75-lsi-2" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian ﬁeld, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model’s partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. Through the expansion, we see that EP is correct to ﬁrst order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. Keywords: expectation consistent inference, expectation propagation, perturbation correction, Wick expansions, Ising model, Gaussian process</p><p>3 0.74321049 <a title="75-lsi-3" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>Author: Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Pierre Dupont</p><p>Abstract: We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efﬁciently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about speciﬁc groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy. Keywords: group feature selection, generalized spike-and-slab priors, expectation propagation, sparse linear model, approximate inference, sequential experimental design, signal reconstruction</p><p>4 0.7283529 <a title="75-lsi-4" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>5 0.45207089 <a title="75-lsi-5" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>6 0.29299089 <a title="75-lsi-6" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>7 0.25839308 <a title="75-lsi-7" href="./jmlr-2013-Global_Analytic_Solution_of_Fully-observed_Variational_Bayesian_Matrix_Factorization.html">49 jmlr-2013-Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization</a></p>
<p>8 0.20372458 <a title="75-lsi-8" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>9 0.19448899 <a title="75-lsi-9" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>10 0.18426627 <a title="75-lsi-10" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>11 0.1686098 <a title="75-lsi-11" href="./jmlr-2013-Fast_MCMC_Sampling_for_Markov_Jump_Processes_and_Extensions.html">43 jmlr-2013-Fast MCMC Sampling for Markov Jump Processes and Extensions</a></p>
<p>12 0.16372228 <a title="75-lsi-12" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>13 0.14459778 <a title="75-lsi-13" href="./jmlr-2013-Variational_Inference_in_Nonconjugate_Models.html">121 jmlr-2013-Variational Inference in Nonconjugate Models</a></p>
<p>14 0.14252041 <a title="75-lsi-14" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>15 0.13975886 <a title="75-lsi-15" href="./jmlr-2013-Pairwise_Likelihood_Ratios_for_Estimation_of_Non-Gaussian_Structural_Equation_Models.html">85 jmlr-2013-Pairwise Likelihood Ratios for Estimation of Non-Gaussian Structural Equation Models</a></p>
<p>16 0.13862519 <a title="75-lsi-16" href="./jmlr-2013-Bayesian_Nonparametric_Hidden_Semi-Markov_Models.html">16 jmlr-2013-Bayesian Nonparametric Hidden Semi-Markov Models</a></p>
<p>17 0.13848197 <a title="75-lsi-17" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>18 0.13470086 <a title="75-lsi-18" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>19 0.13090628 <a title="75-lsi-19" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>20 0.12588839 <a title="75-lsi-20" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.045), (5, 0.105), (6, 0.038), (10, 0.073), (22, 0.247), (23, 0.029), (44, 0.019), (68, 0.011), (70, 0.021), (72, 0.051), (75, 0.168), (85, 0.026), (87, 0.014), (93, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.79245692 <a title="75-lda-1" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>2 0.78026485 <a title="75-lda-2" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>Author: Nathan Parrish, Hyrum S. Anderson, Maya R. Gupta, Dun Yu Hsiao</p><p>Abstract: We consider the problem of classifying a test sample given incomplete information. This problem arises naturally when data about a test sample is collected over time, or when costs must be incurred to compute the classiﬁcation features. For example, in a distributed sensor network only a fraction of the sensors may have reported measurements at a certain time, and additional time, power, and bandwidth is needed to collect the complete data to classify. A practical goal is to assign a class label as soon as enough data is available to make a good decision. We formalize this goal through the notion of reliability—the probability that a label assigned given incomplete data would be the same as the label assigned given the complete data, and we propose a method to classify incomplete data only if some reliability threshold is met. Our approach models the complete data as a random variable whose distribution is dependent on the current incomplete data and the (complete) training data. The method differs from standard imputation strategies in that our focus is on determining the reliability of the classiﬁcation decision, rather than just the class label. We show that the method provides useful reliability estimates of the correctness of the imputed class labels on a set of experiments on time-series data sets, where the goal is to classify the time-series as early as possible while still guaranteeing that the reliability threshold is met. Keywords: classiﬁcation, sensor networks, signals, reliability</p><p>3 0.6521315 <a title="75-lda-3" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>Author: Philémon Brakel, Dirk Stroobandt, Benjamin Schrauwen</p><p>Abstract: Imputing missing values in high dimensional time-series is a difﬁcult problem. This paper presents a strategy for training energy-based graphical models for imputation directly, bypassing difﬁculties probabilistic approaches would face. The training strategy is inspired by recent work on optimization-based learning (Domke, 2012) and allows complex neural models with convolutional and recurrent structures to be trained for imputation tasks. In this work, we use this training strategy to derive learning rules for three substantially different neural architectures. Inference in these models is done by either truncated gradient descent or variational mean-ﬁeld iterations. In our experiments, we found that the training methods outperform the Contrastive Divergence learning algorithm. Moreover, the training methods can easily handle missing values in the training data itself during learning. We demonstrate the performance of this learning scheme and the three models we introduce on one artiﬁcial and two real-world data sets. Keywords: neural networks, energy-based models, time-series, missing values, optimization</p><p>4 0.6489765 <a title="75-lda-4" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>Author: Wei Pan, Xiaotong Shen, Binghui Liu</p><p>Abstract: Clustering analysis is widely used in many ﬁelds. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classiﬁcation and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classiﬁcation and regression, such as model selection criteria to select the number of clusters, a difﬁcult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method’s promising performance. Keywords: generalized degrees of freedom, grouping, K-means clustering, Lasso, penalized regression, truncated Lasso penalty (TLP)</p><p>5 0.6423384 <a title="75-lda-5" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>Author: Lisha Chen, Andreas Buja</p><p>Abstract: Multidimensional scaling (MDS) is the art of reconstructing pointsets (embeddings) from pairwise distance data, and as such it is at the basis of several approaches to nonlinear dimension reduction and manifold learning. At present, MDS lacks a unifying methodology as it consists of a discrete collection of proposals that differ in their optimization criteria, called “stress functions”. To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. This methodology yields the following beneﬁts and problem solutions: (a) It provides guidance in tailoring stress functions to a given data situation, responding to the fact that no single stress function dominates all others across all data situations; (b) the methodology enriches the supply of available stress functions; (c) it helps our understanding of stress functions by replacing the comparison of discrete proposals with a characterization of the effect of parameters on embeddings; (d) it builds a bridge to graph drawing, which is the related but not identical art of constructing embeddings from graphs. Keywords: multidimensional scaling, force-directed layout, cluster analysis, clustering strength, unsupervised learning, Box-Cox transformations</p><p>6 0.6391322 <a title="75-lda-6" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>7 0.61663789 <a title="75-lda-7" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>8 0.60363525 <a title="75-lda-8" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>9 0.56365627 <a title="75-lda-9" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>10 0.5420534 <a title="75-lda-10" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>11 0.5273807 <a title="75-lda-11" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>12 0.51847547 <a title="75-lda-12" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>13 0.51541781 <a title="75-lda-13" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>14 0.5111351 <a title="75-lda-14" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>15 0.51017624 <a title="75-lda-15" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>16 0.50979221 <a title="75-lda-16" href="./jmlr-2013-Using_Symmetry_and_Evolutionary_Search_to_Minimize_Sorting_Networks.html">118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</a></p>
<p>17 0.50952339 <a title="75-lda-17" href="./jmlr-2013-Differential_Privacy_for_Functions_and_Functional_Data.html">32 jmlr-2013-Differential Privacy for Functions and Functional Data</a></p>
<p>18 0.50949675 <a title="75-lda-18" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>19 0.50804198 <a title="75-lda-19" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>20 0.50541604 <a title="75-lda-20" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
