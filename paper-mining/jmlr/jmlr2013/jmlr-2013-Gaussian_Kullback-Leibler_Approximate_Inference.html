<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-47" href="#">jmlr2013-47</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</h1>
<br/><p>Source: <a title="jmlr-2013-47-pdf" href="http://jmlr.org/papers/volume14/challis13a/challis13a.pdf">pdf</a></p><p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>Reference: <a title="jmlr-2013-47-reference" href="../jmlr2013_reference/jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 UK  Department of Computer Science University College London London, WC1E 6BT, UK  Editor: Manfred Opper  Abstract We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. [sent-9, score-0.384]
</p><p>2 1 Overview In Section 2 we provide an introduction and overview of Gaussian Kullback-Leibler (G-KL) approximate inference methods for problems of the form of Equation (2) and describe a large class of models for which G-KL inference is feasible. [sent-34, score-0.382]
</p><p>3 To make G-KL approximate inference scalable we present constrained parameterisations of covariance. [sent-39, score-0.481]
</p><p>4 In Section 5 we compare G-KL approximate inference to other Gaussian approximate inference methods. [sent-40, score-0.426]
</p><p>5 Many approximate inference methods make this assumption, for example the Laplace approximation (see Barber, 2012 for a recent introduction), expectation propagation with an assumed Gaussian approximating density (Minka, 2001) and local variational bounding methods (Jaakkola and Jordan, 1997). [sent-51, score-0.533]
</p><p>6 Whilst the site potentials are not, in full generality, easy to evaluate in the next section we describe a large class of models for which they can be computed efﬁciently and so for which the G-KL bound is tractable. [sent-62, score-0.436]
</p><p>7 However, in many practical problems of interest each potential function φn takes the form φn (w) = φn (wT hn ), (5) for ﬁxed vectors hn . [sent-66, score-0.402]
</p><p>8 1 N ON -G AUSSIAN M ODELS G-KL approximate inference is not limited to models where the target density has a Gaussian potential N (w|µ, Σ). [sent-75, score-0.386]
</p><p>9 G-KL Bound Optimisation G-KL approximate inference proceeds to obtain the tightest lower-bound to log Z and the ‘closest’ Gaussian approximation to p(w) by maximising BKL (m, S) with respect to the moments m and S of the variational Gaussian density. [sent-81, score-0.488]
</p><p>10 2 G-KL Bound Concavity If each site potential {φn }N is log-concave then the G-KL bound BKL (m, S) is jointly concave with n=1 respect to the variational Gaussian mean m and C the upper triangular Cholesky decomposition of covariance such that S = CT C. [sent-105, score-0.705]
</p><p>11 Thus the G-KL bound is jointly concave in m, C provided all site potentials {φn }N are logn=1 concave. [sent-117, score-0.498]
</p><p>12 Another deterministic Gaussian approximate inference procedure for models of the form of Equation (2) are local variational bounding methods (discussed at further length in Section 5. [sent-143, score-0.475]
</p><p>13 For log-concave potentials local variational bounding methods, which optimise a different criterion with a different parameterisation to the G-KL bound, have also been shown to result in a convex optimisation problem (Seeger and Nickisch, 2011b). [sent-146, score-0.792]
</p><p>14 To the best of our knowledge, local variational bounding and G-KL approximate inference methods are the only known concave variational inference procedures for models of the form of Equation (2). [sent-147, score-0.903]
</p><p>15 Complexity : G-KL Bound and Gradient Computations In the previous section we provided conditions for which the G-KL bound is strongly concave and differentiable and so provided conditions for which G-KL bound optimisation using quasiNewton methods will exhibit super-linear convergence rates. [sent-157, score-0.415]
</p><p>16 An important practical consideration is the numerical complexity of the bound and gradient computations required by any gradient ascent optimisation procedure. [sent-159, score-0.39]
</p><p>17 We consider models where the covariance of the Gaussian potential in Equation (2) is spherical, Σ = ν2 I, and each potential function is a site projection, φn (w) = φn (wT hn ). [sent-161, score-0.67]
</p><p>18 For models that do not satisfy this assumption, in Appendix D we present a full breakdown of the complexity of bound and gradient 2245  C HALLIS AND BARBER  computations for each G-KL covariance parameterisation presented in Section 4. [sent-162, score-0.447]
</p><p>19 The computational bottleneck arises from the projected variational variances s2 = CT hn 2 required to compute each log φn (wT hn ) term. [sent-170, score-0.549]
</p><p>20 Thus for a broad class of models the G-KL bound and gradient computations scale O ND2 for general parameterisations of the covariance S = CT C. [sent-178, score-0.526]
</p><p>21 To this end we now consider constrained parameterisations of covariance that reduce both the time and space complexity of G-KL procedures. [sent-185, score-0.409]
</p><p>22 What is more, the complexity of computing the φn site potential contributions to the bound increases for the precision parameterised G-KL bound. [sent-189, score-0.403]
</p><p>23 We note that since a Gaussian potential, N (w|µ, Σ), can be written as a product over D site projection potentials computing log N (w|µ, Σ) will in general scale O D3 —see Appendix B. [sent-192, score-0.454]
</p><p>24 However the site projection potentials are not concave with respect to (Γnn )−1 thus the bound is neither concave nor convex for this parameterisation resulting in convergence to a possibly local optimum. [sent-209, score-0.797]
</p><p>25 In Appendix B we provide equations for each term of the G-KL bound and its gradient for each of the covariance parameterisations considered below. [sent-211, score-0.497]
</p><p>26 3 C ONSTRAINED C ONCAVE PARAMETERISATIONS Below we present constrained parameterisations of covariance which reduce both the space and time complexity of G-KL bound optimisation whilst preserving concavity. [sent-222, score-0.783]
</p><p>27 The constrained parameterisations below have different qualities regarding the expressiveness of the variational Gaussian approximation. [sent-225, score-0.41]
</p><p>28 Such a parameterisation describes a sparse covariance matrix and assumes zero covariance between variables that are indexed out of bandwidth. [sent-232, score-0.461]
</p><p>29 Constraining C such that C = blkdiag (C1 , cIL×L ), with C1 a K × K Cholesky matrix we have that s2 = CT ET hn n 1 1  2  + c2 ( hn  2  − ET hn 2 ), 1  N  meaning that only the K subspace vectors in E1 are needed to compute s2 n=1 . [sent-250, score-0.536]
</p><p>30 For models of the form of Equation (2) three popular, deterministic, Gaussian, approximate inference techniques are local variational bounding, Laplace approximations, and expectation propagation with an assumed Gaussian density. [sent-268, score-0.449]
</p><p>31 Of the three Gaussian approximate inference methods listed above only one, local variational bounding, provides a lower-bound to the normalisation constant Z. [sent-270, score-0.389]
</p><p>32 Local variational bounding procedures that use exponentiated quadratic site n=1 bounds return a Gaussian approximation to the target density p(w). [sent-300, score-0.533]
</p><p>33 Each site projection potential function is lower-bounded by an exponentiated quadratic parameterised in w and a variational parameter ξn . [sent-307, score-0.555]
</p><p>34 Thus we can obtain a bound on Z by substituting Equation (12) into Equation (2): N  Z=  N (w|µ, Σ) ∏ φn (wT hn )dw n=1  ≥  1  T  N (w|µ, Σ) c(ξ)e− 2 w 1  T  −1  e− 2 µ Σ µ = c(ξ) det (2πΣ)  1  F(ξ)w+wT f(ξ)  T  e− 2 w  Aw+wT b  dw (13)  dw,  where A := Σ−1 + F(ξ) and b := Σ−1 µ + f(ξ). [sent-313, score-0.387]
</p><p>35 Completing the square in Equation (13) and integrating, we have log Z ≥ B(ξ), where 1 2  1 2  1 2  1 2  B (ξ) = log c(ξ) − µT Σ−1 µ + bT A−1 b − log det (2πΣ) − log det (2πA) . [sent-316, score-0.578]
</p><p>36 However, many problems have more site potentials 1 φn than Gaussian moment parameters, that is N > 2 D(D + 3), and the local bound in such cases has a richer parameterisation than the G-KL. [sent-324, score-0.585]
</p><p>37 We ﬁrst substitute the local bound on ∏N φn (wT hn ), Equation (12), into Equan=1 tion (4) to obtain a new bound ˜ BKL (m, S) ≥ BKL (m, S, ξ), where ˜ 2BKL = −2 log q(w) − log det (2πΣ) + 2 log c(ξ) − (w − µ)T Σ−1 (w − µ) − wT F(ξ)w + 2 wT f(ξ) . [sent-326, score-0.725]
</p><p>38 Using Equation (14) this can be written as 1 2  1 2  ˜ BKL = − log q(w) − log det (2πΣ) + log c(ξ) − µT Σ−1 µ − 2251  1 T w Aw + wT b . [sent-327, score-0.382]
</p><p>39 By deﬁning q(w) = N w|A−1 b, A−1 we obtain ˜ 1 2  1 2  ˜ BKL = −KL( q(w)| q(w)) − log det (2πΣ) + log c(ξ) − µT Σ−1 µ ˜ 1 1 + bT A−1 b − log det (2πA) . [sent-332, score-0.485]
</p><p>40 m,S  Thus optimal G-KL bounds are provably tighter than both the local variational bound and the G-KL bound calculated using the optimal local bound moments mξ and Sξ . [sent-338, score-0.478]
</p><p>41 Furthermore, constrained parameterisations of covariance, introduced in Section 4, which are required when D ≫ 1, are also frequently observed to outperform local variational solutions despite the fact that they are not provably guaranteed to do so. [sent-341, score-0.444]
</p><p>42 2 Complexity and Model Suitability Comparison We brieﬂy review the core computational bottlenecks and the conditions placed on the potential functions by the local variational bounding, the Laplace approximation and the Gaussian expectation propagation approximate inference methods. [sent-343, score-0.508]
</p><p>43 1, have N free variational parameters— one for each site potential φn . [sent-357, score-0.397]
</p><p>44 1, local variational bounding procedures are applicable provided tight exponentiated quadratic lower-bounds to the site projection potentials {φn }N exist— n=1 that is each site potential is required to be super-Gaussian (Palmer et al. [sent-363, score-0.96]
</p><p>45 For example, the log det (A) term is no longer exactly computed and a lower-bound on log Z is no longer maintained—only an estimate of log Z is provided. [sent-374, score-0.382]
</p><p>46 4 G-KL G-KL approximate inference methods require that each site projection potential has unbounded support on R. [sent-389, score-0.498]
</p><p>47 Unlike local variational bounding procedures G-KL does not require the site potentials to be super-Gaussian. [sent-391, score-0.619]
</p><p>48 Importantly, G-KL procedures can be made scalable by using constrained parameterisations of covariance that do not require making a priori factorisation assumptions for the approximate posterior density. [sent-401, score-0.64]
</p><p>49 Scalable covariance decompositions for G-KL inference maintain a strict lowerbound on log Z whereas approximate local bound optimisers do not. [sent-402, score-0.557]
</p><p>50 2 we compare the performance of the constrained parameterisations of G-KL covariance that we presented in Section 4. [sent-409, score-0.409]
</p><p>51 Z  The likelihood factorises over data instances, p(y|w) = ∏N φ(wn ), thus the GP posterior is of the n=1 form of Equation (1) with site projection potentials of the form of Equation (5). [sent-420, score-0.508]
</p><p>52 In this section we compare G-KL approximate inference to other deterministic Gaussian approximate inference methods, namely: the Laplace approximation (Lap), local variational bounding (VB) and Gaussian expectation propagation (G-EP). [sent-462, score-0.69]
</p><p>53 Second column section: Student’s t likelihood results with G-KL, local variational bounding (VB) and Laplace (Lap) approximate inference. [sent-548, score-0.384]
</p><p>54 4 G-KL approximate inference is straightforward, for the G-KL approximate posterior q(w) = N (w|m, S) the likelihood’s contribution to the bound is log p(y|w)  q(w)  = ∑ log φn (mn + z Snn ) n  N (z|0,1)  . [sent-556, score-0.617]
</p><p>55 The expectations for the Laplace likelihood site potentials have simple analytic forms—see Appendix B. [sent-558, score-0.409]
</p><p>56 We follow the evidence maximisation or maximum likelihood two (ML-II) procedure to estimate the covariance hyperparameters, that is we set covariance hyperparameters to maximise p(y|X, θ). [sent-579, score-0.394]
</p><p>57 The results show that both G-KL bound optimisation and G-KL hyperparameter optimisation is numerically stable. [sent-646, score-0.452]
</p><p>58 G-KL approximate inference appears more robust than G-EP and VB—G-KL hyperparameter optimisation always converged, often to a better local optima. [sent-647, score-0.451]
</p><p>59 Our aim is not make a comparison of deterministic approximate inference methods for Bayesian logistic regression models, see Nickisch and Rasmussen (2008) to that end, but to investigate the time accuracy trade-offs of each of the constrained G-KL covariance parameterisations. [sent-659, score-0.469]
</p><p>60 The expression above is of the form of Equation (2) with logconcave site projection potentials φn (x) = σ(x) and hn = yn xn . [sent-666, score-0.518]
</p><p>61 For local variational bounding (VB) approximate inference K refers to the number of Lanczos vectors used to update the variational parameters. [sent-681, score-0.588]
</p><p>62 Since the G-KL bound is strongly concave we performed G-KL bound optimisation using Hessian free Newton methods for all the Cholesky parameterised covariance experiments. [sent-685, score-0.628]
</p><p>63 G-KL results obtained using chevron Cholesky (Chev), banded Cholesky (Band), subspace Cholesky (Sub) and factor analysis (FA) constrained parameterisations of covariance. [sent-916, score-0.524]
</p><p>64 In the smaller problems considered, the best G-KL times were 2262  G AUSSIAN KL A PPROXIMATE I NFERENCE  achieved by the chevron Cholesky covariance followed by the banded, the subspace and the FA parameterisations in that order. [sent-937, score-0.534]
</p><p>65 Whilst chevron and banded parameterisations both scale O (NDK) they access and compute different elements of the data and Cholesky matrices. [sent-940, score-0.414]
</p><p>66 The G-KL banded covariance parameterisation achieves the strongest bound value with the chevron and factor analysis parameterisations a close second place. [sent-950, score-0.775]
</p><p>67 The results show that the the G-KL mean is broadly invariant to the G-KL covariance parameterisations used. [sent-962, score-0.395]
</p><p>68 3 S UMMARY The results support the use of the constrained Cholesky covariance parameterisations to drive scalable G-KL approximate inference procedures. [sent-973, score-0.622]
</p><p>69 Whilst neither the banded nor the chevron Cholesky parameterisations are invariant to permutations of the index set they both achieved the strongest bound values and test set performance. [sent-974, score-0.49]
</p><p>70 In this larger setting we apply G-KL and VB approximate inference methods only and make use of approximate covariance decompositions. [sent-1106, score-0.427]
</p><p>71 These results highlight a general distinction between the two methods, VB optimisation is an approximate EM algorithm whilst GKL optimisation in this setting is implemented using an approximate second order gradient ascent procedure. [sent-1125, score-0.701]
</p><p>72 All that is required to apply G-KL to a model of the form of Equation (2) is the pointwise evaluation of the univariate site projection potentials and that each of these potentials has unbounded support on R. [sent-1147, score-0.525]
</p><p>73 Unlike other deterministic Gaussian approximate inference methods G-KL does not require the site potentials to be differentiable, super-Gaussian or log-concave. [sent-1148, score-0.544]
</p><p>74 2270  G AUSSIAN KL A PPROXIMATE I NFERENCE  A long perceived disadvantage of G-KL approximate inference is the difﬁculty of optimising the bound with respect to the O D2 parameters needed to specify the G-KL covariance matrix. [sent-1150, score-0.52]
</p><p>75 We have shown, however, that whilst O D2 parameters are required in full generality, the computations needed for bound optimisation compare favourably with other deterministic Gaussian approximate inference procedures. [sent-1151, score-0.587]
</p><p>76 n=1  For larger problems we provided concave constrained parameterisations of covariance that allow G-KL methods to be applied to larger problems without imposing a priori factorisation assumptions on the approximate posterior density. [sent-1153, score-0.676]
</p><p>77 The results presented in Section 6 show that such constrained covariance parameterisations are at least as good as other widely used deterministic methods at capturing posterior covariance. [sent-1154, score-0.478]
</p><p>78 G-KL approximate inference using constrained concave covariance parameterisations have optimisation convergence times comparable to fast approximate variational local bound methods whilst maintaining a strict lower-bound on log Z. [sent-1155, score-1.429]
</p><p>79 The vgai package implements G-KL approximate inference for models of the form of Equation (2) where each potential function is a site projection φn (w) = φ(wT hn ). [sent-1160, score-0.684]
</p><p>80 Generic site projection potentials are supported if an implementation of ψ := log φ : R → R is provided. [sent-1162, score-0.454]
</p><p>81 The package implements the unconstrained Cholesky, constrained Cholesky and factor analysis parameterisations of covariance discussed in Section 4. [sent-1163, score-0.409]
</p><p>82 G-KL bound optimisation is achieved in the vgai package using Mark Schmidt’s minFunc optimisation package. [sent-1165, score-0.42]
</p><p>83 G-KL Bound Gradients We present the G-KL bound and its gradient for Gaussian and generic site projection potentials with full Cholesky and factor analysis parameterisations of G-KL covariance. [sent-1183, score-0.717]
</p><p>84 Gradients for the chevron, banded and sparse Cholesky covariance parameterisations are implemented simply by placing that Cholesky parameterisation’s sparsity mask on the full Cholesky gradient matrix. [sent-1184, score-0.542]
</p><p>85 2 Site Projection Potentials Each site projection potential’s contribution to the G-KL bound can be expressed as In = log φn (wT hn ) = log φ(y) N (y|mn ,s2 ) = log φ(mn + zsn ) N (z|0,1) , n where mn = hT m and s2 = hT Shn . [sent-1193, score-0.918]
</p><p>86 The expectations and their derivatives are given by: In = ∂In = ∂mn ∂In = ∂s2 n  N (z|0, 1) log φn (mn + zsn )dz, log φn (mn + zsn ) dz, sn log φn (mn + zsn ) z2 − 1 N (z|0, 1) dz. [sent-1196, score-0.612]
</p><p>87 We consider the case of a zero mean Laplace density, p(wT hn |τ) = T e−|w hn |/τ /2τ, giving log p(mn + zsn ) z = − log(2τ) −  1 |mn + zsn | z . [sent-1204, score-0.629]
</p><p>88 3 Gaussian Potentials For a Gaussian potential N (w|µ, Σ) the log expectation is given by log N (w|µ, Σ)  q(w)  =−  1 log det (2πΣ) + (m − µ)T Σ−1 (m − µ) + trace Σ−1 S 2  . [sent-1209, score-0.531]
</p><p>89 ν =  For the FA parameterised covariance we have y − HT w  T  y − HT w  2  = yT y − 2yT HT m + ∑ HT m i + ∑ ΘT HT i  ij  2 + ij  ∑ ∑ H2 ji j  d2 j  i  with corresponding gradients: ∂ log N y|HT w, ν2 I ∂d j  =−  1 ν2  ∂ log N y|Mw, ν2 I ∂Θ  =−  1 HHT Θ. [sent-1218, score-0.399]
</p><p>90 4 Subspace Covariance Decomposition We consider optimising the G-KL bound with respect to a covariance matrix parameterised on a subspace of the parameters w ∈ RD . [sent-1226, score-0.444]
</p><p>91 1 2 2275  C HALLIS AND BARBER  Since E is orthonormal it does not effect the value or gradient of the entropy’s contribution to the bound since log det (S) = log det (S′ ). [sent-1229, score-0.553]
</p><p>92 For S block diagonal with the second block component spherical, S2 = c2 I, the orthonormal basis vectors E2 do not need to be computed or maintained since s2 = hT S′ hn = hT ET S1 E1 hn + c2 hT ET E2 hn = hT ET S1 E1 hn + c2 n n n 1 n 2 n 1  hn  2 2−  E1 hn  2  . [sent-1232, score-0.97]
</p><p>93 to the subspace parameterised variational Gaussian by iterating between optimising the bound with respect to the parameters {m, C1 , c} and updating the subspace basis vectors E1 . [sent-1236, score-0.51]
</p><p>94 The G-KL bound for the subspace Cholesky covariance parameterisation is given by  BKL (m, C, c, E) =  K D D log (2π) + + ∑ log (Ckk ) + L log(c) 2 2 k=1 D 1 − log 2πν2 − 2 m − µ 2 ν  2 2 + trace  CT C + Lc2  N  + ∑ log φn (mn + zsn ) N (z|0,1) . [sent-1251, score-0.939]
</p><p>95 As described above, when Σ = ν2 I, the only term in the G-KL bound that depends on E are the site projection potential functions log φn (wT hn ) . [sent-1263, score-0.611]
</p><p>96 We consider G-KL inference problems of the form of Equation (2) where {φn }N are site n=1 projection potentials that are piecewise exponentiated quadratics, log-concave and have unbounded support on R. [sent-1295, score-0.557]
</p><p>97 We consider each term that depends on the variational parameters m and S separately, namely: log det (S) from the entropy’s contribution, trace Σ−1 S and mT Σ−1 m from N the Gaussian potential’s contribution, and mn , s2 n=1 from the product of site projection potential’s n contribution. [sent-1336, score-0.663]
</p><p>98 Note that this procedure can speed up G-KL bound optimisation only in settings where hn are not sparse. [sent-1354, score-0.405]
</p><p>99 2 Hyperparameter Optimisation For a general likelihood p(y|w) = ∏N φn (wn ) and GP prior N (w|0, Σ) with covariance function n=1 Σmn = k(xm , xn ) we get the G-KL bound  BKL (m, C) =  D 1 1 1 + ∑ logCnn − log det (Σ) − mT Σ−1 m − trace Σ−1 S 2 2 2 2 n + ∑ log φ(mn + z Snn ) n  N (z|0,1)  . [sent-1364, score-0.644]
</p><p>100 Concave gaussian variational approximations for inference in large-scale bayesian linear models. [sent-1619, score-0.436]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bkl', 0.302), ('vb', 0.227), ('parameterisations', 0.223), ('cholesky', 0.195), ('optimisation', 0.172), ('laplace', 0.168), ('site', 0.167), ('potentials', 0.164), ('ht', 0.158), ('hallis', 0.157), ('hn', 0.157), ('student', 0.149), ('parameterisation', 0.144), ('variational', 0.142), ('covariance', 0.141), ('inference', 0.14), ('barber', 0.14), ('pproximate', 0.135), ('whilst', 0.126), ('wt', 0.119), ('aussian', 0.118), ('zsn', 0.111), ('kl', 0.106), ('chevron', 0.105), ('nference', 0.105), ('det', 0.103), ('seeger', 0.102), ('gaussian', 0.1), ('mn', 0.098), ('sed', 0.098), ('nickisch', 0.094), ('log', 0.093), ('ntrn', 0.092), ('concave', 0.091), ('optimising', 0.09), ('potential', 0.088), ('banded', 0.086), ('fa', 0.086), ('chev', 0.085), ('gp', 0.082), ('optimise', 0.079), ('likelihood', 0.078), ('lanczos', 0.076), ('bound', 0.076), ('approximate', 0.073), ('parameterised', 0.072), ('band', 0.07), ('posterior', 0.069), ('mt', 0.066), ('lml', 0.066), ('subspace', 0.065), ('wtr', 0.059), ('bounding', 0.057), ('gradients', 0.057), ('gradient', 0.057), ('exponentiated', 0.056), ('density', 0.056), ('procedures', 0.055), ('bayesian', 0.054), ('ntst', 0.052), ('ct', 0.051), ('dw', 0.051), ('tlp', 0.05), ('ch', 0.047), ('ndk', 0.046), ('optimised', 0.046), ('constrained', 0.045), ('sub', 0.044), ('logistic', 0.041), ('moments', 0.04), ('challis', 0.039), ('sublevel', 0.039), ('equation', 0.039), ('normalised', 0.035), ('sigmoid', 0.035), ('sparse', 0.035), ('hyperparameters', 0.034), ('local', 0.034), ('factorisation', 0.034), ('rd', 0.033), ('initialised', 0.033), ('reconstruction', 0.032), ('hyperparameter', 0.032), ('expectation', 0.031), ('broadly', 0.031), ('projection', 0.03), ('trace', 0.03), ('prior', 0.03), ('hessian', 0.03), ('measurement', 0.029), ('models', 0.029), ('regression', 0.029), ('dk', 0.029), ('likelihoods', 0.029), ('orthonormal', 0.028), ('ascent', 0.028), ('vanhatalo', 0.028), ('rasmussen', 0.027), ('measurements', 0.027), ('covariates', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="47-tfidf-1" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>2 0.22411659 <a title="47-tfidf-2" href="./jmlr-2013-Variational_Inference_in_Nonconjugate_Models.html">121 jmlr-2013-Variational Inference in Nonconjugate Models</a></p>
<p>Author: Chong Wang, David M. Blei</p><p>Abstract: Mean-ﬁeld variational methods are widely used for approximate posterior inference in many probabilistic models. In a typical application, mean-ﬁeld methods approximately compute the posterior with a coordinate-ascent optimization algorithm. When the model is conditionally conjugate, the coordinate updates are easily derived and in closed form. However, many models of interest—like the correlated topic model and Bayesian logistic regression—are nonconjugate. In these models, mean-ﬁeld methods cannot be directly applied and practitioners have had to develop variational algorithms on a case-by-case basis. In this paper, we develop two generic methods for nonconjugate models, Laplace variational inference and delta method variational inference. Our methods have several advantages: they allow for easily derived variational algorithms with a wide class of nonconjugate models; they extend and unify some of the existing algorithms that have been derived for speciﬁc models; and they work well on real-world data sets. We studied our methods on the correlated topic model, Bayesian logistic regression, and hierarchical Bayesian logistic regression. Keywords: variational inference, nonconjugate models, Laplace approximations, the multivariate delta method</p><p>3 0.16681068 <a title="47-tfidf-3" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>Author: Matthew D. Hoffman, David M. Blei, Chong Wang, John Paisley</p><p>Abstract: We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets. Keywords: Bayesian inference, variational inference, stochastic optimization, topic models, Bayesian nonparametrics</p><p>4 0.15393129 <a title="47-tfidf-4" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>5 0.1264182 <a title="47-tfidf-5" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>6 0.11452314 <a title="47-tfidf-6" href="./jmlr-2013-Global_Analytic_Solution_of_Fully-observed_Variational_Bayesian_Matrix_Factorization.html">49 jmlr-2013-Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization</a></p>
<p>7 0.1012472 <a title="47-tfidf-7" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>8 0.10023095 <a title="47-tfidf-8" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>9 0.09083122 <a title="47-tfidf-9" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>10 0.088666216 <a title="47-tfidf-10" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>11 0.067797601 <a title="47-tfidf-11" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>12 0.06604667 <a title="47-tfidf-12" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>13 0.060311273 <a title="47-tfidf-13" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>14 0.058704253 <a title="47-tfidf-14" href="./jmlr-2013-On_the_Mutual_Nearest_Neighbors_Estimate_in_Regression.html">79 jmlr-2013-On the Mutual Nearest Neighbors Estimate in Regression</a></p>
<p>15 0.057235669 <a title="47-tfidf-15" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>16 0.056872658 <a title="47-tfidf-16" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>17 0.055732016 <a title="47-tfidf-17" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>18 0.055096608 <a title="47-tfidf-18" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>19 0.054703705 <a title="47-tfidf-19" href="./jmlr-2013-A_Theory_of_Multiclass_Boosting.html">8 jmlr-2013-A Theory of Multiclass Boosting</a></p>
<p>20 0.052919939 <a title="47-tfidf-20" href="./jmlr-2013-Derivative_Estimation_with_Local_Polynomial_Fitting.html">31 jmlr-2013-Derivative Estimation with Local Polynomial Fitting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.297), (1, -0.38), (2, 0.122), (3, -0.058), (4, -0.061), (5, -0.058), (6, -0.004), (7, -0.047), (8, -0.01), (9, -0.034), (10, 0.046), (11, 0.061), (12, 0.047), (13, -0.026), (14, -0.05), (15, 0.069), (16, 0.004), (17, -0.063), (18, -0.06), (19, -0.035), (20, 0.091), (21, -0.113), (22, 0.082), (23, -0.142), (24, -0.075), (25, -0.134), (26, 0.004), (27, -0.031), (28, 0.027), (29, -0.003), (30, 0.046), (31, 0.054), (32, 0.086), (33, 0.014), (34, -0.091), (35, 0.052), (36, -0.046), (37, -0.066), (38, 0.064), (39, 0.072), (40, -0.041), (41, -0.015), (42, -0.006), (43, 0.055), (44, -0.073), (45, -0.011), (46, 0.028), (47, 0.017), (48, -0.031), (49, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94567114 <a title="47-lsi-1" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>2 0.68442273 <a title="47-lsi-2" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>3 0.63717866 <a title="47-lsi-3" href="./jmlr-2013-Global_Analytic_Solution_of_Fully-observed_Variational_Bayesian_Matrix_Factorization.html">49 jmlr-2013-Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama, S. Derin Babacan, Ryota Tomioka</p><p>Abstract: The variational Bayesian (VB) approximation is known to be a promising approach to Bayesian estimation, when the rigorous calculation of the Bayes posterior is intractable. The VB approximation has been successfully applied to matrix factorization (MF), offering automatic dimensionality selection for principal component analysis. Generally, ﬁnding the VB solution is a non-convex problem, and most methods rely on a local search algorithm derived through a standard procedure for the VB approximation. In this paper, we show that a better option is available for fully-observed VBMF—the global solution can be analytically computed. More speciﬁcally, the global solution is a reweighted SVD of the observed matrix, and each weight can be obtained by solving a quartic equation with its coefﬁcients being functions of the observed singular value. We further show that the global optimal solution of empirical VBMF (where hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments in multi-variate analysis. Keywords: variational Bayes, matrix factorization, empirical Bayes, model-induced regularization, probabilistic PCA</p><p>4 0.63318765 <a title="47-lsi-4" href="./jmlr-2013-Variational_Inference_in_Nonconjugate_Models.html">121 jmlr-2013-Variational Inference in Nonconjugate Models</a></p>
<p>Author: Chong Wang, David M. Blei</p><p>Abstract: Mean-ﬁeld variational methods are widely used for approximate posterior inference in many probabilistic models. In a typical application, mean-ﬁeld methods approximately compute the posterior with a coordinate-ascent optimization algorithm. When the model is conditionally conjugate, the coordinate updates are easily derived and in closed form. However, many models of interest—like the correlated topic model and Bayesian logistic regression—are nonconjugate. In these models, mean-ﬁeld methods cannot be directly applied and practitioners have had to develop variational algorithms on a case-by-case basis. In this paper, we develop two generic methods for nonconjugate models, Laplace variational inference and delta method variational inference. Our methods have several advantages: they allow for easily derived variational algorithms with a wide class of nonconjugate models; they extend and unify some of the existing algorithms that have been derived for speciﬁc models; and they work well on real-world data sets. We studied our methods on the correlated topic model, Bayesian logistic regression, and hierarchical Bayesian logistic regression. Keywords: variational inference, nonconjugate models, Laplace approximations, the multivariate delta method</p><p>5 0.5663892 <a title="47-lsi-5" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>6 0.56241179 <a title="47-lsi-6" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>7 0.48393297 <a title="47-lsi-7" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>8 0.45132366 <a title="47-lsi-8" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>9 0.44253486 <a title="47-lsi-9" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>10 0.43799776 <a title="47-lsi-10" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>11 0.41352189 <a title="47-lsi-11" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>12 0.38000762 <a title="47-lsi-12" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>13 0.37141594 <a title="47-lsi-13" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>14 0.35580978 <a title="47-lsi-14" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>15 0.33062211 <a title="47-lsi-15" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<p>16 0.32703561 <a title="47-lsi-16" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>17 0.32700902 <a title="47-lsi-17" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<p>18 0.31396356 <a title="47-lsi-18" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>19 0.2761713 <a title="47-lsi-19" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>20 0.27327845 <a title="47-lsi-20" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.035), (5, 0.124), (6, 0.051), (10, 0.078), (20, 0.015), (23, 0.033), (41, 0.011), (61, 0.012), (68, 0.016), (70, 0.025), (72, 0.332), (75, 0.081), (85, 0.029), (87, 0.025), (89, 0.013), (93, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74436975 <a title="47-lda-1" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>2 0.5240823 <a title="47-lda-2" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>3 0.4653028 <a title="47-lda-3" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian ﬁeld, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model’s partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. Through the expansion, we see that EP is correct to ﬁrst order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. Keywords: expectation consistent inference, expectation propagation, perturbation correction, Wick expansions, Ising model, Gaussian process</p><p>4 0.46069655 <a title="47-lda-4" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>Author: Lorenzo Rosasco, Silvia Villa, Sofia Mosci, Matteo Santoro, Alessandro Verri</p><p>Abstract: In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose a new notion of nonparametric sparsity and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art methods. Keywords: sparsity, nonparametric, variable selection, regularization, proximal methods, RKHS ∗. Also at Istituto Italiano di Tecnologia, Via Morego 30, 16163 Genova, Italy and Massachusetts Institute of Technology, Bldg. 46-5155, 77 Massachusetts Avenue, Cambridge, MA 02139, USA. c 2013 Lorenzo Rosasco, Silvia Villa, Soﬁa Mosci, Matteo Santoro and Alessandro Verri. ROSASCO , V ILLA , M OSCI , S ANTORO AND V ERRI</p><p>5 0.45515332 <a title="47-lda-5" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>Author: Qiang Liu, Alexander Ihler</p><p>Abstract: The marginal maximum a posteriori probability (MAP) estimation problem, which calculates the mode of the marginal posterior distribution of a subset of variables with the remaining variables marginalized, is an important inference problem in many models, such as those with hidden variables or uncertain parameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has attracted less attention in the literature compared to the joint MAP (maximization) and marginalization problems. We derive a general dual representation for marginal MAP that naturally integrates the marginalization and maximization operations into a joint variational optimization problem, making it possible to easily extend most or all variational-based algorithms to marginal MAP. In particular, we derive a set of “mixed-product” message passing algorithms for marginal MAP, whose form is a hybrid of max-product, sum-product and a novel “argmax-product” message updates. We also derive a class of convergent algorithms based on proximal point methods, including one that transforms the marginal MAP problem into a sequence of standard marginalization problems. Theoretically, we provide guarantees under which our algorithms give globally or locally optimal solutions, and provide novel upper bounds on the optimal objectives. Empirically, we demonstrate that our algorithms signiﬁcantly outperform the existing approaches, including a state-of-the-art algorithm based on local search methods. Keywords: graphical models, message passing, belief propagation, variational methods, maximum a posteriori, marginal-MAP, hidden variable models</p><p>6 0.45481095 <a title="47-lda-6" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>7 0.45447746 <a title="47-lda-7" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>8 0.45421067 <a title="47-lda-8" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>9 0.45338008 <a title="47-lda-9" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>10 0.45228618 <a title="47-lda-10" href="./jmlr-2013-Variational_Inference_in_Nonconjugate_Models.html">121 jmlr-2013-Variational Inference in Nonconjugate Models</a></p>
<p>11 0.45164913 <a title="47-lda-11" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>12 0.45101061 <a title="47-lda-12" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>13 0.45041585 <a title="47-lda-13" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>14 0.44735226 <a title="47-lda-14" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>15 0.44628847 <a title="47-lda-15" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>16 0.44533685 <a title="47-lda-16" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>17 0.44499534 <a title="47-lda-17" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>18 0.44466683 <a title="47-lda-18" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>19 0.44277769 <a title="47-lda-19" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>20 0.44263124 <a title="47-lda-20" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
