<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-93" href="#">jmlr2013-93</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</h1>
<br/><p>Source: <a title="jmlr-2013-93-pdf" href="http://jmlr.org/papers/volume14/urry13a/urry13a.pdf">pdf</a></p><p>Author: Matthew J. Urry, Peter Sollich</p><p>Abstract: We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to signiﬁcant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is signiﬁcantly more accurate than previous approximations and should become exact in the limit of large random graphs. Keywords: Gaussian process, generalisation error, learning curve, cavity method, belief propagation, graph, random walk kernel</p><p>Reference: <a title="jmlr-2013-93-reference" href="../jmlr2013_reference/jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. [sent-16, score-1.13]
</p><p>2 Keywords: Gaussian process, generalisation error, learning curve, cavity method, belief propagation, graph, random walk kernel  1. [sent-19, score-0.791]
</p><p>3 (2009) and Urry and Sollich (2010) and focus on predicting the learning curves of GPs used for regression (where outputs are from the whole real line) on large sparse graphs, using the random walk kernel (Kondor and Lafferty, 2002; Smola and Kondor, 2003). [sent-41, score-0.562]
</p><p>4 With a better understanding of the random walk kernel in hand, we proceed in Section 3 to an analysis of the use of the random walk kernel for GP regression on graphs. [sent-44, score-0.819]
</p><p>5 2 by looking at how kernel normalisation affects the prior probability over functions. [sent-46, score-0.488]
</p><p>6 We show that the more frequently used global normalisation of a kernel by its average prior variance is inappropriate for the highly location dependent random walk kernel, and suggest normalisation to uniform local prior variance as a remedy. [sent-47, score-1.233]
</p><p>7 1, to the case of globally normalised kernels as originally proposed. [sent-53, score-0.597]
</p><p>8 The belief propagation analysis for global normalisation also acts as a useful warm-up for the extension to the prediction of learning curves for the locally normalised kernel setting, which we present in Section 4. [sent-54, score-1.078]
</p><p>9 Finally, to emphasise the distinction between the use of globally and locally normalised kernels in GP regression, we study qualitatively the case of model mismatch, with a GP with a globally normalised kernel as the teacher and a GP with a locally normalised kernel as the student, or visa versa. [sent-57, score-2.005]
</p><p>10 These make use of the normalised graph Laplacian to deﬁne correlations between vertices of a graph. [sent-71, score-0.678]
</p><p>11 The class of kernels created in Kondor and Lafferty (2002) is constructed using the normalised Laplacian, L = I − D −1/2 AD −1/2 (see Chung, 1996) as a replacement for the Laplacian in continuous spaces. [sent-78, score-0.497]
</p><p>12 q  (3)  The matrix AD −1 is a random walk transition matrix: (AD −1 )i j is the probability of being at vertex i after one random walk step starting from vertex j. [sent-88, score-0.8]
</p><p>13 Equivalently one can interpret the random walk kernel as a p-step lazy random walk, where at each step the walker stays at the current vertex with probability (1 − a−1 ) and moves to a neighbouring vertex with probability a−1 . [sent-90, score-0.665]
</p><p>14 Using either interpretation, one sees that p/a is the lengthscale over which the random walk can diffuse along the graph, and hence the lengthscale describing the typical maximum range of the random walk kernel. [sent-91, score-0.705]
</p><p>15 We next consider how random walk kernels on graphs approach the fully correlated case, and show that even for ‘simple’ graphs the convergence to this limit is non-trivial. [sent-100, score-0.639]
</p><p>16 1 The d-Regular Tree: A Concrete Example To begin our discussion of the dependence of the random walk kernel on the lengthscale p/a, we ﬁrst look at how this kernel behaves on a d-regular graph sampled uniformly from the set of all d-regular graphs. [sent-106, score-0.739]
</p><p>17 For a large enough number of vertices V , typical cycles in such a d-regular graph are also large, of length O(logV ), and can be neglected for calculation of the kernel when V → ∞. [sent-108, score-0.483]
</p><p>18 Since all vertices on the tree are equivalent, the random walk kernel Ci j can only depend on the distance between vertices i and j, that is, the smallest number of steps on the graph required to get from one vertex to the other. [sent-111, score-0.999]
</p><p>19 Denoting the value of a p-step lazy random walk kernel for vertices a distance l apart by Cl,p , we can determine these values by recursion over 1804  G AUSSIAN P ROCESSES ON R ANDOM G RAPHS  p as follows: 1 1 C0,p + C1,p , a a 1 d −1 1 γ p+1Cl,p+1 = Cl−1,p + 1 − Cl,p + Cl+1,p l ≥ 1. [sent-112, score-0.54]
</p><p>20 ad a ad Cl,p=0 = δl,0 ,  γ p+1C0,p+1 = 1 −  (4)  Here γ p is chosen to achieve the desired normalisation of the prior variance for every p. [sent-113, score-0.481]
</p><p>21 (5) Cl,p→∞ = 1 + d (d − 1)l/2 Equation (5) can be derived by taking the σ2 → ∞ limit of the integral expression for the diffusion kernel from Chung and Yau (1999) whilst preserving normalisation of the kernel (see Appendix A. [sent-118, score-0.596]
</p><p>22 To summarise thus far, the analysis on a d-regular tree shows that, for large p, the random walk kernel does not approach the expected fully correlated limit: because all vertices have the same degree this limit would correspond to Cl,p→∞ = 1. [sent-126, score-0.639]
</p><p>23 The square root accounts for the fact that local kernel values Cii can vary slightly on a regular graph because of cycles, while they are the same for all vertices of a regular tree. [sent-150, score-0.521]
</p><p>24 To emphasise the issue, Figure 2 shows examples of distributions of local prior variances Cii for random walk kernels globally normalised to an average prior variance of unity. [sent-201, score-1.134]
</p><p>25 Single vertex subgraphs have an atypically small prior variance since, for a single disconnected vertex i, before normalisation Cii = (1 − a−1 ) p which is the q = 0 contribution from Equation (3). [sent-217, score-0.731]
</p><p>26 The maximum values of Cii that we see in these two speciﬁc o e graph instances follow the same trend, with maxi Cii ≈ 40 for the power law graph and maxi Cii ≈ 15 for the Erd˝ s-R´ nyi graph. [sent-237, score-0.491]
</p><p>27 To summarise, Figure 2 shows that after global normalisation a random walk kernel can retain a large spread in the local prior variances, with the latter depending on the graph structure in a complicated manner. [sent-239, score-0.939]
</p><p>28 For a desired ˆ prior variance c this means normalising according to Ci j = cCi j /(κi κ j )1/2 with local normalisation ˆii ; here Ci j is the unnormalised kernel matrix as before. [sent-241, score-0.649]
</p><p>29 This guarantees that all ˆ constants κi = C vertices have exactly equal prior variance as in the Euclidean case, that is, all vertices have a prior variance of c. [sent-242, score-0.51]
</p><p>30 5  0  0 0  1  2  3  4 5 Cii  6  7  8  0  9  1  2  3  7  8  9  Figure 2: (Left) Grey: histogram of prior variances for the globally normalised random walk kernel with a = 2, p = 10 on a single instance of an Erd˝ s-R´ nyi graph with mean degree λ = 3 o e and V = 10000 vertices. [sent-271, score-1.332]
</p><p>31 Section 4 then improves on this using the cavity method to fully exploit the graph structure. [sent-302, score-0.474]
</p><p>32 Learning curve predictions from Equation (14) using numerically computed eigenvalues for the globally normalised random walk kernel are shown in Figure 3 as dotted lines for random regular (left), Erd˝ s-R´ nyi (centre) and power law generalised random graphs (right). [sent-320, score-1.535]
</p><p>33 1 L EARNING C URVES  FOR  L ARGE p  Before moving on to the more accurate cavity prediction of the learning curves, we now look at how the learning curves for GP regression on graphs depend on the kernel lengthscale p/a. [sent-326, score-0.863]
</p><p>34 1, we saw that on a large regular graph the random walk kernel approaches a non-trivial limiting form for large p, as long as one stays below the threshold 4. [sent-329, score-0.567]
</p><p>35 4), solid lines: numerically simulated learning curves for graphs of size V = 500, dashed lines: cavity predictions (see Section 4. [sent-333, score-0.719]
</p><p>36 Explicitly, if λL are the eigenvalues of the normalised graph Laplacian on a tree, α then the kernel eigenvalues are λα = κ−1V −1 (1 − λL /a) p . [sent-355, score-0.663]
</p><p>37 The inverse of the covariance matrix in (12) creates factors linking vertices at arbitrary distances along the graph, and so must be eliminated before the cavity method ˆ can be applied. [sent-396, score-0.512]
</p><p>38 We begin by assuming a general form for the normalisation of C that encompasses −1/2 −1 )I + a−1 D −1/2 AD −1/2 ] p K −1/2 both local and global normalisation and set C = K [(1 − a with Ki j = κi δi j . [sent-397, score-0.618]
</p><p>39 We focus ﬁrst on the simpler case of a globally normalised kernel where κi = κ for all i. [sent-415, score-0.621]
</p><p>40 In terms of the sum-product formulation of belief j j j j propagation, the cavity marginal on the left is the message that vertex j sends to the factor in Z for edge (i, j) (Bishop, 2007). [sent-428, score-0.498]
</p><p>41 ) Using again the locally treelike structure, the incoming ( j) (to vertex j) cavity covariances Vk will be independent and identically distributed samples from W (V ). [sent-473, score-0.571]
</p><p>42 00  (26)  γ  This has a simple interpretation: the cavity marginals of the neighbours provide an effective Gaussian prior for each vertex, whose inverse variance is dκ(M −1 )00 . [sent-492, score-0.507]
</p><p>43 With κ set to this value, one can then run the population dynamics for any ν to obtain the Bayes error prediction for GP regression with a globally normalised kernel. [sent-503, score-0.548]
</p><p>44 2 Predicting Prior Variances As a by-product of the cavity analysis for globally normalised kernels we note that in the cavity form −1 of the Bayes error in Equation (26), the fraction (γ/σ2 + dκ(Md )00 )−1 is the local Bayes error, that is, the local posterior variance. [sent-508, score-1.393]
</p><p>45 2 and compare the cavity predictions to numerically simulated distributions of prior variances. [sent-513, score-0.518]
</p><p>46 3 Local Normalisation We now extend the cavity analysis for the learning curves to the case of locally normalised random walk kernels, which, as argued above, provide more plausible probabilistic models. [sent-519, score-1.199]
</p><p>47 In this case the diagonal entries of the normalisation matrix K are deﬁned as κi =  df fi2 P(f ),  ˆ where P(f ) is the GP prior with the unnormalised kernel C. [sent-520, score-0.595]
</p><p>48 This makes clear why the locally normalised kernel case is more challenging technically: we cannot calculate the normalisation constants once and for all for a given random graph ensemble and set of kernel parameters p and a as we did for κ in the globally normalised scenario. [sent-521, score-1.68]
</p><p>49 One iterates the cavity updates (22) for the unnormalised kernel and without the training data (i. [sent-524, score-0.54]
</p><p>50 1820  G AUSSIAN P ROCESSES ON R ANDOM G RAPHS  Once the κi have been determined in this way, one can use them for predicting the Bayes error for the scenario we really want to study, that is, using a locally normalised kernel and incorporating the training data. [sent-529, score-0.594]
</p><p>51 The outcome of the ﬁrst round of cavity updates, for the unnormalised kernel without training data, is then represented by a distribution of cavity covariances V , while the second one gives a distribution of cavity covariances Vloc for the locally normalised kernel, with training data included. [sent-538, score-1.668]
</p><p>52 Detailed analysis using the replica method (Urry and Sollich, 2012) shows that the correct ﬁxed point equation updates the V -messages as in the globally normalised case with γ = 0. [sent-540, score-0.521]
</p><p>53 (29)    γ  One sees that if one marginalises over Vloc , then one obtains exactly the same condition on W (V ) as before in the globally normalised kernel case (but with κ = 1 and ν = 0), see (24). [sent-543, score-0.652]
</p><p>54 (30) κ= −1 d(Md )00 It may seem unusual that d copies of V enter here; Vd represents the cavity covariance from the ﬁrst set that is received from the vertex to which the new message Vloc is being sent. [sent-547, score-0.502]
</p><p>55 1 to give the prediction for the learning curve for GP regression with a locally normalised kernel. [sent-553, score-0.612]
</p><p>56 γ  Learning curve predictions for GPs with locally normalised kernels as they result from the cavity approach described above are shown in Figure 5. [sent-555, score-1.045]
</p><p>57 The ﬁgure shows numerically simulated learning curves and the cavity prediction, both for Erd˝ s-R´ nyi random graphs (left) and power law genero e alised random graphs (centre) of size V = 500. [sent-556, score-1.008]
</p><p>58 As for the globally normalised case one sees that the cavity predictions are quantitatively very accurate even with the simpliﬁed update Equation (29). [sent-557, score-0.906]
</p><p>59 The fact that the cavity predictions of the learning curve for a locally normalised kernel are indistinguishable from the numerically simulated learning curves in Figure 5 leads us to believe that the simpliﬁcation made by dropping the consistency requirement in (29) is in fact exact. [sent-559, score-1.27]
</p><p>60 o e Solid lines: numerically simulated learning curves for graphs of size V = 500, dashed lines: cavity predictions (see Section 4. [sent-561, score-0.719]
</p><p>61 (Right top) Comparison between learning curves for locally (dashed line) and globally (solid line) normalised kernels for Erd˝ s-R´ nyi o e random graphs. [sent-566, score-0.932]
</p><p>62 A Qualitative Comparison of Learning with Locally and Globally Normalised Kernels The cavity approach we have developed gives very accurate predictions for learning curves for GP regression on graphs using random walk kernels. [sent-572, score-0.95]
</p><p>63 2 that the local normalisation is much more plausible as a probabilistic model, because it avoids variability in the local prior variances that is non-trivially related to the local graph structure and so difﬁcult to justify from prior knowledge. [sent-575, score-0.75]
</p><p>64 It is not a simple matter to say which kernel is ‘better’, the locally or globally normalised one. [sent-577, score-0.694]
</p><p>65 172 for the locally normalised random walk kernel with a = 2, p = 10, averaged over ten samples each of teacher functions, data and Erd˝ s-R´ nyi graphs with mean degree λ = 3 and V = 1000 vertices. [sent-588, score-1.299]
</p><p>66 Black: o e cavity prediction for this distribution in the large graph limit. [sent-589, score-0.502]
</p><p>67 A more deﬁnite answer could be obtained only empirically, by running GP regression with local and global kernel normalisation on the same data sets and comparing the prediction errors and also the marginal data likelihood. [sent-593, score-0.518]
</p><p>68 The same approach could also be tried with synthetic data sets generated from GP priors that are mismatched to both priors we have considered, deﬁned by the globally and locally normalised kernel, though justifying what is a reasonable choice for the prior of the target function would not be easy. [sent-594, score-0.666]
</p><p>69 Figure 5 (right top and bottom) overlays the learning curves for global and local kernel normalisations, for an Erd˝ s-R´ nyi o e and a power law generalised random graph respectively. [sent-596, score-0.733]
</p><p>70 There are qualitative differences in the shapes of the learning curves, with the ones for the locally normalised kernel exhibiting a shoulder around ν = 2. [sent-597, score-0.621]
</p><p>71 This shoulder is due to the proper normalisation of isolated vertices to unit prior variance; by contrast, as shown earlier in Figure 2 (left), global normalisation gives too small a prior variance to such vertices. [sent-598, score-0.927]
</p><p>72 The inset in Figure 5 (left) shows the expected learning curve contributions from all locally normalised isolated vertices (single vertex subgraphs) as dotted lines. [sent-599, score-0.868]
</p><p>73 After the GP learns the rest of the graph to a sufﬁcient accuracy, the single vertex error dominates the learning curve until these vertices have typically seen at least one example. [sent-600, score-0.513]
</p><p>74 Once this point has been passed, the dominant error comes once more from the giant connected component of the graph, and the GP 1824  G AUSSIAN P ROCESSES ON R ANDOM G RAPHS  learns in a similar manner to the globally normalised case. [sent-601, score-0.491]
</p><p>75 We can extend the scope of this qualitative comparison by examining how a student GP with a kernel with one normalisation performs when learning from a teacher with a kernel with the other normalisation. [sent-603, score-0.726]
</p><p>76 Figure 7 (left) shows the case of GP students with a globally normalised kernel learning from a teacher with a locally normalised kernel on an Erd˝ s-R´ nyi graph. [sent-605, score-1.459]
</p><p>77 Similar behaviour can be observed for the case of power law generalised random graphs, shown in Figure 7 (right) and for the case of GP students with a locally normalised kernel learning from a teacher with a globally normalised kernel, shown in Figure 8. [sent-608, score-1.36]
</p><p>78 In all cases close inspection (see Appendix B) shows that the error maximum is caused by ‘dangling edges’ of the graph, that is, chains of vertices (with degree two) extending away from the giant graph component and terminating in a vertex of degree one. [sent-609, score-0.546]
</p><p>79 As a ﬁnal qualitative comparison between globally and locally normalised kernels, Figure 9 shows the variance of local posterior variances. [sent-610, score-0.691]
</p><p>80 As Figure 9 shows, for kernels with local normalisation we ﬁnd exactly this scenario, both for Erd˝ s-R´ nyi and power law random graphs. [sent-615, score-0.644]
</p><p>81 These results can now be contrasted with those for globally normalised kernels, also displayed in Figure 9. [sent-618, score-0.491]
</p><p>82 2, that the more typical approach to normalisation, that is, scaling the kernel globally to a desired average prior variance, results in a large spread of local prior variances that is related in a complicated manner to the graph structure; this is undesirable in a prior. [sent-629, score-0.605]
</p><p>83 The teacher GP has a locally normalised kernel with the same parameters. [sent-631, score-0.714]
</p><p>84 as a simple remedy to perform local normalisation, where the raw kernel is normalised by its local prior variance so that the prior variance becomes the same at every vertex. [sent-634, score-0.829]
</p><p>85 The teacher GP has a globally normalised kernel with the same parameters. [sent-649, score-0.741]
</p><p>86 2  0 10−2  10−1  0 10−2  100 ν  10−1  100  101  ν  Figure 9: (Left) Error variance for GP regression with locally (stars) and globally (circles) normalised kernels against ν, on Erd˝ s-R´ nyi random graphs, and for matched learning with o e p = 10, a = 2, σ2 = 0. [sent-660, score-0.862]
</p><p>87 Sadly, however, since the cavity method requires graphs to be treelike this is not possible. [sent-665, score-0.497]
</p><p>88 Finally in Section 5 we qualitatively compared GPs with kernels that are normalised globally and locally. [sent-666, score-0.597]
</p><p>89 In particular, local normalisation leads to a shoulder in the learning curves owing to the correct normalisation of the single vertex disconnected graph components. [sent-668, score-1.095]
</p><p>90 Lastly we looked at the variance among the local Bayes errors, for GPs with both globally and locally normalised kernels. [sent-671, score-0.647]
</p><p>91 Plausibly, locally normalised kernels lead to this error variance being maximal for intermediate data set sizes. [sent-672, score-0.609]
</p><p>92 For globally normalised kernels, the error variance inherited from the spread of local prior variances is always dominant, obscuring any signatures from the changing ‘coverage’ of the graph by examples. [sent-674, score-0.834]
</p><p>93 In further work we intend to extend the cavity approximation of the learning curves to the case of mismatch, where teacher and student have different kernel hyperparameters. [sent-675, score-0.779]
</p><p>94 It would also be interesting to apply the cavity method to the learning curves of GPs with random walk kernels on more general random graphs, like those considered in Rogers et al. [sent-676, score-0.841]
</p><p>95 Looking further ahead, preliminary work has shown that it should be possible to extend the cavity learning curve approximation to the problem of graph mismatch, where the student has incomplete information about the graph structure of the teacher. [sent-679, score-0.766]
</p><p>96 The heat kernel for a graph with normalised Laplacian L (see Section 2 in the main text) is given by  H = exp (−tL) ,  t > 0,  This is exactly the diffusion kernel given in (1) with t = 1 σ2 . [sent-687, score-0.842]
</p><p>97 If we call the unnormalised p ˆ form of the random walk kernels we considered in the main text again C = I − a−1 L , then we can directly use the results of Chung and Yau (1999), simply by modifying the function that is applied to each eigenvalue λ of L from exp(−tλ) to (1 − λ/a) p . [sent-692, score-0.484]
</p><p>98 Red lines show zz T for the case where student is globally normalised and the teacher is locally normalised. [sent-778, score-0.81]
</p><p>99 For the case of mismatch, for example, because student and teacher use differently normalised kernels, the only change is in the statistics of z. [sent-782, score-0.57]
</p><p>100 2  0  50  100 150 example  200  Figure 12: Analogue of Figure 11 for a locally normalised student and globally normalised teacher  References S. [sent-796, score-1.134]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('normalised', 0.391), ('cavity', 0.332), ('normalisation', 0.287), ('walk', 0.265), ('erd', 0.167), ('sollich', 0.16), ('gp', 0.158), ('vertices', 0.145), ('graph', 0.142), ('cii', 0.14), ('curves', 0.138), ('vertex', 0.135), ('graphs', 0.134), ('ollich', 0.132), ('rry', 0.132), ('kernel', 0.13), ('nyi', 0.124), ('teacher', 0.12), ('kernels', 0.106), ('rocesses', 0.102), ('malzahn', 0.101), ('globally', 0.1), ('andom', 0.094), ('gps', 0.093), ('curve', 0.091), ('aussian', 0.088), ('raphs', 0.088), ('vloc', 0.086), ('unnormalised', 0.078), ('locally', 0.073), ('lengthscale', 0.072), ('generalised', 0.072), ('prior', 0.071), ('zz', 0.067), ('cycles', 0.066), ('opper', 0.065), ('numerically', 0.063), ('hi', 0.063), ('xvk', 0.062), ('degree', 0.062), ('mismatch', 0.061), ('cutoff', 0.06), ('hq', 0.06), ('student', 0.059), ('kondor', 0.055), ('bayes', 0.054), ('urry', 0.054), ('predictions', 0.052), ('law', 0.05), ('diffusion', 0.049), ('variances', 0.047), ('dhq', 0.047), ('cq', 0.047), ('zard', 0.047), ('posterior', 0.044), ('local', 0.044), ('di', 0.042), ('ad', 0.042), ('variance', 0.039), ('dangling', 0.039), ('dvk', 0.039), ('fia', 0.039), ('normalisations', 0.039), ('vl', 0.039), ('tree', 0.037), ('ensemble', 0.036), ('gaussian', 0.035), ('disconnected', 0.035), ('covariance', 0.035), ('marginals', 0.035), ('eigenvalue', 0.035), ('power', 0.033), ('inset', 0.033), ('generalisation', 0.033), ('williams', 0.032), ('belief', 0.031), ('md', 0.031), ('mismatched', 0.031), ('treelike', 0.031), ('vivarelli', 0.031), ('yau', 0.031), ('unity', 0.031), ('chung', 0.031), ('sees', 0.031), ('exponent', 0.03), ('replica', 0.03), ('neighbours', 0.03), ('regular', 0.03), ('spike', 0.029), ('df', 0.029), ('subgraphs', 0.029), ('regression', 0.029), ('prediction', 0.028), ('rescaled', 0.028), ('hk', 0.027), ('dhk', 0.027), ('shoulder', 0.027), ('master', 0.026), ('analogue', 0.026), ('centre', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="93-tfidf-1" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>Author: Matthew J. Urry, Peter Sollich</p><p>Abstract: We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to signiﬁcant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is signiﬁcantly more accurate than previous approximations and should become exact in the limit of large random graphs. Keywords: Gaussian process, generalisation error, learning curve, cavity method, belief propagation, graph, random walk kernel</p><p>2 0.10023095 <a title="93-tfidf-2" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>3 0.094528399 <a title="93-tfidf-3" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>Author: Vinay Jethava, Anders Martinsson, Chiranjib Bhattacharyya, Devdatt Dubhashi</p><p>Abstract: In this paper we establish that the Lov´ sz ϑ function on a graph can be restated as a kernel learning a problem. We introduce the notion of SVM − ϑ graphs, on which Lov´ sz ϑ function can be apa proximated well by a Support vector machine (SVM). We show that Erd¨ s-R´ nyi random G(n, p) o e log4 n np graphs are SVM − ϑ graphs for n ≤ p < 1. Even if we embed a large clique of size Θ 1−p in a G(n, p) graph the resultant graph still remains a SVM − ϑ graph. This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. Associated with the ϑ function is the notion of orthogonal labellings. We introduce common orthogonal labellings which extends the idea of orthogonal labellings to multiple graphs. This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude. Keywords: orthogonal labellings of graphs, planted cliques, random graphs, common dense subgraph</p><p>4 0.093943052 <a title="93-tfidf-4" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>5 0.091341257 <a title="93-tfidf-5" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>Author: Jun Wang, Tony Jebara, Shih-Fu Chang</p><p>Abstract: Graph-based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems, particularly in agnostic settings when no parametric information or other prior knowledge is available about the data distribution. Given the constructed graph represented by a weight matrix, transductive inference is used to propagate known labels to predict the values of all unlabeled vertices. Designing a robust label diffusion algorithm for such graphs is a widely studied problem and various methods have recently been suggested. Many of these can be formalized as regularized function estimation through the minimization of a quadratic cost. However, most existing label diffusion methods minimize a univariate cost with the classiﬁcation function as the only variable of interest. Since the observed labels seed the diffusion process, such univariate frameworks are extremely sensitive to the initial label choice and any label noise. To alleviate the dependency on the initial observed labels, this article proposes a bivariate formulation for graph-based SSL, where both the binary label information and a continuous classiﬁcation function are arguments of the optimization. This bivariate formulation is shown to be equivalent to a linearly constrained Max-Cut problem. Finally an efﬁcient solution via greedy gradient Max-Cut (GGMC) is derived which gradually assigns unlabeled vertices to each class with minimum connectivity. Once convergence guarantees are established, this greedy Max-Cut based SSL is applied on both artiﬁcial and standard benchmark data sets where it obtains superior classiﬁcation accuracy compared to existing state-of-the-art SSL methods. Moreover, GGMC shows robustness with respect to the graph construction method and maintains high accuracy over extensive experiments with various edge linking and weighting schemes. Keywords: graph transduction, semi-supervised learning, bivariate formulation, mixed integer programming, greedy</p><p>6 0.081551298 <a title="93-tfidf-6" href="./jmlr-2013-Regularization-Free_Principal_Curve_Estimation.html">96 jmlr-2013-Regularization-Free Principal Curve Estimation</a></p>
<p>7 0.079258017 <a title="93-tfidf-7" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>8 0.071795836 <a title="93-tfidf-8" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>9 0.071172342 <a title="93-tfidf-9" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>10 0.062983707 <a title="93-tfidf-10" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>11 0.056217011 <a title="93-tfidf-11" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>12 0.054011911 <a title="93-tfidf-12" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>13 0.051898088 <a title="93-tfidf-13" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>14 0.050723847 <a title="93-tfidf-14" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>15 0.049690884 <a title="93-tfidf-15" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>16 0.048981477 <a title="93-tfidf-16" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>17 0.047883976 <a title="93-tfidf-17" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>18 0.045668449 <a title="93-tfidf-18" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>19 0.044502873 <a title="93-tfidf-19" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>20 0.040787764 <a title="93-tfidf-20" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.22), (1, -0.076), (2, 0.034), (3, -0.061), (4, 0.139), (5, 0.159), (6, -0.098), (7, -0.0), (8, -0.112), (9, -0.141), (10, -0.002), (11, 0.068), (12, 0.021), (13, 0.016), (14, 0.096), (15, 0.209), (16, 0.041), (17, -0.053), (18, 0.02), (19, -0.019), (20, -0.02), (21, -0.184), (22, 0.062), (23, -0.298), (24, -0.069), (25, 0.023), (26, 0.011), (27, -0.099), (28, -0.082), (29, 0.09), (30, -0.05), (31, 0.022), (32, 0.085), (33, 0.131), (34, -0.042), (35, 0.099), (36, -0.038), (37, 0.068), (38, -0.037), (39, -0.019), (40, 0.055), (41, 0.015), (42, 0.058), (43, -0.068), (44, 0.098), (45, 0.044), (46, 0.074), (47, 0.037), (48, 0.095), (49, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95471787 <a title="93-lsi-1" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>Author: Matthew J. Urry, Peter Sollich</p><p>Abstract: We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to signiﬁcant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is signiﬁcantly more accurate than previous approximations and should become exact in the limit of large random graphs. Keywords: Gaussian process, generalisation error, learning curve, cavity method, belief propagation, graph, random walk kernel</p><p>2 0.54505014 <a title="93-lsi-2" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>Author: Vinay Jethava, Anders Martinsson, Chiranjib Bhattacharyya, Devdatt Dubhashi</p><p>Abstract: In this paper we establish that the Lov´ sz ϑ function on a graph can be restated as a kernel learning a problem. We introduce the notion of SVM − ϑ graphs, on which Lov´ sz ϑ function can be apa proximated well by a Support vector machine (SVM). We show that Erd¨ s-R´ nyi random G(n, p) o e log4 n np graphs are SVM − ϑ graphs for n ≤ p < 1. Even if we embed a large clique of size Θ 1−p in a G(n, p) graph the resultant graph still remains a SVM − ϑ graph. This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. Associated with the ϑ function is the notion of orthogonal labellings. We introduce common orthogonal labellings which extends the idea of orthogonal labellings to multiple graphs. This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude. Keywords: orthogonal labellings of graphs, planted cliques, random graphs, common dense subgraph</p><p>3 0.53144217 <a title="93-lsi-3" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>Author: Nicolò Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella</p><p>Abstract: We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world data sets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice. Keywords: online learning, learning on graphs, graph prediction, random spanning trees</p><p>4 0.48969367 <a title="93-lsi-4" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>5 0.44567397 <a title="93-lsi-5" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>Author: Jun Wang, Tony Jebara, Shih-Fu Chang</p><p>Abstract: Graph-based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems, particularly in agnostic settings when no parametric information or other prior knowledge is available about the data distribution. Given the constructed graph represented by a weight matrix, transductive inference is used to propagate known labels to predict the values of all unlabeled vertices. Designing a robust label diffusion algorithm for such graphs is a widely studied problem and various methods have recently been suggested. Many of these can be formalized as regularized function estimation through the minimization of a quadratic cost. However, most existing label diffusion methods minimize a univariate cost with the classiﬁcation function as the only variable of interest. Since the observed labels seed the diffusion process, such univariate frameworks are extremely sensitive to the initial label choice and any label noise. To alleviate the dependency on the initial observed labels, this article proposes a bivariate formulation for graph-based SSL, where both the binary label information and a continuous classiﬁcation function are arguments of the optimization. This bivariate formulation is shown to be equivalent to a linearly constrained Max-Cut problem. Finally an efﬁcient solution via greedy gradient Max-Cut (GGMC) is derived which gradually assigns unlabeled vertices to each class with minimum connectivity. Once convergence guarantees are established, this greedy Max-Cut based SSL is applied on both artiﬁcial and standard benchmark data sets where it obtains superior classiﬁcation accuracy compared to existing state-of-the-art SSL methods. Moreover, GGMC shows robustness with respect to the graph construction method and maintains high accuracy over extensive experiments with various edge linking and weighting schemes. Keywords: graph transduction, semi-supervised learning, bivariate formulation, mixed integer programming, greedy</p><p>6 0.41806391 <a title="93-lsi-6" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>7 0.40913054 <a title="93-lsi-7" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>8 0.39200068 <a title="93-lsi-8" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>9 0.36630467 <a title="93-lsi-9" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>10 0.35075915 <a title="93-lsi-10" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>11 0.34698224 <a title="93-lsi-11" href="./jmlr-2013-Regularization-Free_Principal_Curve_Estimation.html">96 jmlr-2013-Regularization-Free Principal Curve Estimation</a></p>
<p>12 0.31002331 <a title="93-lsi-12" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>13 0.30233905 <a title="93-lsi-13" href="./jmlr-2013-Kernel_Bayes%27_Rule%3A_Bayesian_Inference_with_Positive_Definite_Kernels.html">57 jmlr-2013-Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels</a></p>
<p>14 0.2692582 <a title="93-lsi-14" href="./jmlr-2013-Derivative_Estimation_with_Local_Polynomial_Fitting.html">31 jmlr-2013-Derivative Estimation with Local Polynomial Fitting</a></p>
<p>15 0.26322776 <a title="93-lsi-15" href="./jmlr-2013-Segregating_Event_Streams_and_Noise_with_a_Markov_Renewal_Process_Model.html">98 jmlr-2013-Segregating Event Streams and Noise with a Markov Renewal Process Model</a></p>
<p>16 0.24005859 <a title="93-lsi-16" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>17 0.23211746 <a title="93-lsi-17" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>18 0.2301496 <a title="93-lsi-18" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>19 0.22979246 <a title="93-lsi-19" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>20 0.22912967 <a title="93-lsi-20" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.029), (5, 0.136), (6, 0.062), (10, 0.091), (14, 0.015), (20, 0.017), (23, 0.018), (53, 0.013), (61, 0.365), (68, 0.035), (70, 0.03), (75, 0.066), (85, 0.011), (87, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78542006 <a title="93-lda-1" href="./jmlr-2013-Global_Analytic_Solution_of_Fully-observed_Variational_Bayesian_Matrix_Factorization.html">49 jmlr-2013-Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama, S. Derin Babacan, Ryota Tomioka</p><p>Abstract: The variational Bayesian (VB) approximation is known to be a promising approach to Bayesian estimation, when the rigorous calculation of the Bayes posterior is intractable. The VB approximation has been successfully applied to matrix factorization (MF), offering automatic dimensionality selection for principal component analysis. Generally, ﬁnding the VB solution is a non-convex problem, and most methods rely on a local search algorithm derived through a standard procedure for the VB approximation. In this paper, we show that a better option is available for fully-observed VBMF—the global solution can be analytically computed. More speciﬁcally, the global solution is a reweighted SVD of the observed matrix, and each weight can be obtained by solving a quartic equation with its coefﬁcients being functions of the observed singular value. We further show that the global optimal solution of empirical VBMF (where hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments in multi-variate analysis. Keywords: variational Bayes, matrix factorization, empirical Bayes, model-induced regularization, probabilistic PCA</p><p>same-paper 2 0.72341484 <a title="93-lda-2" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>Author: Matthew J. Urry, Peter Sollich</p><p>Abstract: We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to signiﬁcant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is signiﬁcantly more accurate than previous approximations and should become exact in the limit of large random graphs. Keywords: Gaussian process, generalisation error, learning curve, cavity method, belief propagation, graph, random walk kernel</p><p>3 0.44663322 <a title="93-lda-3" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>Author: Alberto N. Escalante-B., Laurenz Wiskott</p><p>Abstract: Supervised learning from high-dimensional data, for example, multimedia data, is a challenging task. We propose an extension of slow feature analysis (SFA) for supervised dimensionality reduction called graph-based SFA (GSFA). The algorithm extracts a label-predictive low-dimensional set of features that can be post-processed by typical supervised algorithms to generate the ﬁnal label or class estimation. GSFA is trained with a so-called training graph, in which the vertices are the samples and the edges represent similarities of the corresponding labels. A new weighted SFA optimization problem is introduced, generalizing the notion of slowness from sequences of samples to such training graphs. We show that GSFA computes an optimal solution to this problem in the considered function space and propose several types of training graphs. For classiﬁcation, the most straightforward graph yields features equivalent to those of (nonlinear) Fisher discriminant analysis. Emphasis is on regression, where four different graphs were evaluated experimentally with a subproblem of face detection on photographs. The method proposed is promising particularly when linear models are insufﬁcient as well as when feature selection is difﬁcult. Keywords: slow feature analysis, feature extraction, classiﬁcation, regression, pattern recognition, training graphs, nonlinear dimensionality reduction, supervised learning, implicitly supervised, high-dimensional data, image analysis</p><p>4 0.44587892 <a title="93-lda-4" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>Author: Wendelin Böhmer, Steffen Grünewälder, Yun Shen, Marek Musial, Klaus Obermayer</p><p>Abstract: Linear reinforcement learning (RL) algorithms like least-squares temporal difference learning (LSTD) require basis functions that span approximation spaces of potential value functions. This article investigates methods to construct these bases from samples. We hypothesize that an ideal approximation spaces should encode diffusion distances and that slow feature analysis (SFA) constructs such spaces. To validate our hypothesis we provide theoretical statements about the LSTD value approximation error and induced metric of approximation spaces constructed by SFA and the state-of-the-art methods Krylov bases and proto-value functions (PVF). In particular, we prove that SFA minimizes the average (over all tasks in the same environment) bound on the above approximation error. Compared to other methods, SFA is very sensitive to sampling and can sometimes fail to encode the whole state space. We derive a novel importance sampling modiﬁcation to compensate for this effect. Finally, the LSTD and least squares policy iteration (LSPI) performance of approximation spaces constructed by Krylov bases, PVF, SFA and PCA is compared in benchmark tasks and a visual robot navigation experiment (both in a realistic simulation and with a robot). The results support our hypothesis and suggest that (i) SFA provides subspace-invariant features for MDPs with self-adjoint transition operators, which allows strong guarantees on the approximation error, (ii) the modiﬁed SFA algorithm is best suited for LSPI in both discrete and continuous state spaces and (iii) approximation spaces encoding diffusion distances facilitate LSPI performance. Keywords: reinforcement learning, diffusion distance, proto value functions, slow feature analysis, least-squares policy iteration, visual robot navigation c 2013 Wendelin B¨ hmer, Steffen Gr¨ new¨ lder, Yun Shen, Marek Musial and Klaus Obermayer. o u a ¨ ¨ ¨ B OHMER , G R UNEW ALDER , S HEN , M USIAL AND O BERMAYER</p><p>5 0.44412896 <a title="93-lda-5" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>Author: Yuchen Zhang, John C. Duchi, Martin J. Wainwright</p><p>Abstract: We analyze two communication-efﬁcient algorithms for distributed optimization in statistical settings involving large-scale data sets. The ﬁrst algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves √ mean-squared error (MSE) that decays as O (N −1 + (N/m)−2 ). Whenever m ≤ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O (N −1 + (N/m)−3 ), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O (N −1 + (N/m)−3/2 ), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efﬁciently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 108 samples and d ≈ 740,000 covariates. Keywords: distributed learning, stochastic optimization, averaging, subsampling</p><p>6 0.44377822 <a title="93-lda-6" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>7 0.44150952 <a title="93-lda-7" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>8 0.44111127 <a title="93-lda-8" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>9 0.44083878 <a title="93-lda-9" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>10 0.43995601 <a title="93-lda-10" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>11 0.43775699 <a title="93-lda-11" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>12 0.43505335 <a title="93-lda-12" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>13 0.43499595 <a title="93-lda-13" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>14 0.43492869 <a title="93-lda-14" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>15 0.43478301 <a title="93-lda-15" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>16 0.43455163 <a title="93-lda-16" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>17 0.43409437 <a title="93-lda-17" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>18 0.43366814 <a title="93-lda-18" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>19 0.43297634 <a title="93-lda-19" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>20 0.43257216 <a title="93-lda-20" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
