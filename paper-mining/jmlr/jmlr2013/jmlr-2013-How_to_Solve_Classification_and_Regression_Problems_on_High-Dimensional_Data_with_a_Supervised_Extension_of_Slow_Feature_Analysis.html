<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-52" href="#">jmlr2013-52</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</h1>
<br/><p>Source: <a title="jmlr-2013-52-pdf" href="http://jmlr.org/papers/volume14/escalante13a/escalante13a.pdf">pdf</a></p><p>Author: Alberto N. Escalante-B., Laurenz Wiskott</p><p>Abstract: Supervised learning from high-dimensional data, for example, multimedia data, is a challenging task. We propose an extension of slow feature analysis (SFA) for supervised dimensionality reduction called graph-based SFA (GSFA). The algorithm extracts a label-predictive low-dimensional set of features that can be post-processed by typical supervised algorithms to generate the ﬁnal label or class estimation. GSFA is trained with a so-called training graph, in which the vertices are the samples and the edges represent similarities of the corresponding labels. A new weighted SFA optimization problem is introduced, generalizing the notion of slowness from sequences of samples to such training graphs. We show that GSFA computes an optimal solution to this problem in the considered function space and propose several types of training graphs. For classiﬁcation, the most straightforward graph yields features equivalent to those of (nonlinear) Fisher discriminant analysis. Emphasis is on regression, where four different graphs were evaluated experimentally with a subproblem of face detection on photographs. The method proposed is promising particularly when linear models are insufﬁcient as well as when feature selection is difﬁcult. Keywords: slow feature analysis, feature extraction, classiﬁcation, regression, pattern recognition, training graphs, nonlinear dimensionality reduction, supervised learning, implicitly supervised, high-dimensional data, image analysis</p><p>Reference: <a title="jmlr-2013-52-reference" href="../jmlr2013_reference/jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We propose an extension of slow feature analysis (SFA) for supervised dimensionality reduction called graph-based SFA (GSFA). [sent-10, score-0.261]
</p><p>2 The algorithm extracts a label-predictive low-dimensional set of features that can be post-processed by typical supervised algorithms to generate the ﬁnal label or class estimation. [sent-11, score-0.18]
</p><p>3 Keywords: slow feature analysis, feature extraction, classiﬁcation, regression, pattern recognition, training graphs, nonlinear dimensionality reduction, supervised learning, implicitly supervised, high-dimensional data, image analysis  1. [sent-18, score-0.315]
</p><p>4 A widely known algorithm for supervised dimensionality reduction is Fisher discriminant analysis (FDA) (Fisher, 1936). [sent-33, score-0.179]
</p><p>5 In this article, we introduce a supervised extension of SFA called graph-based SFA (GSFA) speciﬁcally designed for supervised dimensionality reduction. [sent-64, score-0.219]
</p><p>6 For instance, features equivalent to those of FDA can be obtained if a particular training graph is given to GSFA. [sent-74, score-0.208]
</p><p>7 One advantage of GSFA over many algorithms for supervised dimensionality reduction is that it is designed for both classiﬁcation and regression (using appropriate training graphs), whereas other algorithms typically focus on classiﬁcation only. [sent-82, score-0.25]
</p><p>8 The unsupervised hierarchical network plus the supervised direct method together constitute the classiﬁer or regression architecture. [sent-98, score-0.178]
</p><p>9 In the case of GSFA, the structured training data is called training graph, a weighted graph that has vertices representing the samples, node weights specifying a priori sample probabilities, and edge weights indicating desired output similarities, as derived from the labels. [sent-99, score-0.475]
</p><p>10 Thereafter, we experimentally evaluate and compare the performance of four training graphs to other common supervised methods (e. [sent-117, score-0.191]
</p><p>11 This construction allows the solution of supervised learning problems on high-dimensional data when the dimensionality and number of samples make the direct application of many conventional supervised algorithms infeasible. [sent-131, score-0.27]
</p><p>12 (a) The input samples are 128×128-pixel images with labels indicating the horizontal position of the center of the face. [sent-157, score-0.184]
</p><p>13 Then, one uses another SFA node in a next layer to extract global slow features from the local slow features. [sent-239, score-0.289]
</p><p>14 In the example above, one can then introduce a weighted edge between any pair of face images according to some similarity measure based on age (or other criteria such as gender, race, or mimic expression), with high similarity resulting in large edge weights. [sent-260, score-0.254]
</p><p>15 The original SFA objective then needs to be adapted such that samples connected by large edge weights yield similar output values. [sent-261, score-0.203]
</p><p>16 In mathematical terms, the training data is represented as a training graph G = (V, E) (illustrated in Figure 4) with a set V of vertices x(n) (each vertex/node being a sample), and a set E of edges (x(n), x(n′ )), which are pairs of samples, with 1 ≤ n, n′ ≤ N. [sent-262, score-0.26]
</p><p>17 This representation includes the standard time series as a special case in which the graph has a linear structure and all node and edge weights are identical (Figure 4. [sent-267, score-0.266]
</p><p>18 2 GSFA Optimization Problem We extend the concept of slowness, originally conceived for sequences of samples, to data structured in a training graph making use of its associated edge weights. [sent-273, score-0.227]
</p><p>19 These factors, however, provide invariance to the scale of the edge weights as well as to the scale of the node weights, and serve a normalization purpose. [sent-279, score-0.213]
</p><p>20 , vN } and the edges E be the set of edges (x(n), x(n′ )) with edge weights γn,n′ . [sent-295, score-0.206]
</p><p>21 The sample covariance matrix CG is deﬁned as: def  CG =  1 1 ˆˆ ˆ ˆ ∑ vn (x(n) − x)(x(n) − x)T = Q ∑ vn x(n)(x(n))T − xxT , Q n n  where def  ˆ x =  1 vn x(n) Q∑ n  (12)  (13)  ˙ is the weighted average of all samples. [sent-298, score-0.521]
</p><p>22 The weighted zero mean constraint (7) holds trivially for any W, because  ∑ vn y(n)  (18)  =  n  ˆ ∑ vn WT (x(n) − x) n  = WT  ˆ ∑ vn x(n) − ∑ vn x ′  n  n′  (13,11)  ˆ ˆ = WT (Q x − Q x) = 0 . [sent-312, score-0.38]
</p><p>23 We also ﬁnd I = RT IR  (since R is a rotation matrix),  (15)  = RT (ST CG S)R ,  (17)  = WT CG W , 1 (12) ˆ ˆ = WT ∑ vn (x(n) − x)(x(n) − x)T W , Q n (18) 1 = vn y(n)(y(n))T , Q∑ n which is equivalent to the normalization constraints (8) and (9). [sent-313, score-0.219]
</p><p>24 Given a training graph, we construct below a Markov chain M for the generation of input data such that training standard SFA with such data yields the same features as GSFA does with the 3694  H OW TO S OLVE C LASSIFICATION AND R EGRESSION P ROBLEMS WITH SFA  graph. [sent-330, score-0.207]
</p><p>25 Contrary to the graph introduced by Klampﬂ and Maass (2010), the formulation here is not restricted to classiﬁcation, accounting for any training graph irrespective of its purpose, and there is one state per sample rather than one state per class. [sent-331, score-0.224]
</p><p>26 , vn ← vn /Q, γn,n′ ← γn,n′ /R) without affecting the ˜ ˜ outputs generated by GSFA. [sent-335, score-0.19]
</p><p>27 Restriction (20) is fundamental because it limits the graph connectivity, and indicates (after multiplying with vn ) that each vertex weight should be equal to the sum of the ˜ weights of all edges originating from such a vertex. [sent-336, score-0.327]
</p><p>28 ˜ It is easy to see that π = {vn }N is indeed a stationary distribution, since for pt (n) = vn ˜ n=1 pt+1 (n) = Pr(Zt+1 = x(n)) =  ∑ Pr(Zt+1 = x(n)|Zt = x(n′ )) Pr(Zt = x(n′ )) n′  (23,24)  =  ˜ ˜ γ ˜ ∑ ((1 − ε) (˜n ,n /vn ) + εvn ) vn ′  ′  ′  n′  (21,19)  = (1 − ε)vn + εvn = vn = pt (n) . [sent-345, score-0.285]
</p><p>29 Therefore, if a graph fulﬁls the normalization restrictions (19)–(22), GSFA yields the same features as standard SFA on the sequence generated by the Markov chain, in the limit ε → 0. [sent-349, score-0.194]
</p><p>30 However, when the goal is to solve a supervised learning task, the graph created should implicitly integrate the label information. [sent-352, score-0.196]
</p><p>31 1 Clustered Training Graph To generate features useful for classiﬁcation, we propose the use of a clustered training graph presented below (Figure 5). [sent-362, score-0.248]
</p><p>32 We deﬁne the clustered training graph as a graph G = (V, E) with vertices V = {xs (n)}, and edges E = {(xs (n), xs (n′ ))} for s = 1, . [sent-370, score-0.405]
</p><p>33 As usual, a trivial scaling of the node and edge weights sufﬁces to fulﬁl restrictions (19) and (22), allowing the probabilistic interpretation of the graph. [sent-382, score-0.201]
</p><p>34 Figure 5: Illustration of a clustered training graph used for a classiﬁcation problem. [sent-384, score-0.182]
</p><p>35 These node and edge weights assume that the classiﬁcation of all samples is equally important. [sent-392, score-0.235]
</p><p>36 , their ∆-values are small), their structure is simple enough that one can use even a modest supervised step after SFA, such as a nearest centroid or a Gaussian classiﬁer (in which a Gaussian distribution is ﬁtted to each class) on S − 1 slow features or less. [sent-406, score-0.245]
</p><p>37 The ﬁrst method is called sample reordering and employs standard SFA, whereas the remaining ones employ GSFA with three different training graphs called sliding window, serial, and mixed (Sections 5. [sent-420, score-0.214]
</p><p>38 Starting from the reordered sequence X as deﬁned above, a training graph is constructed, in which each sample x(n) is connected to its d closest samples to the left and to the right in the order given by X. [sent-454, score-0.235]
</p><p>39 In this graph, the vertex weights are constant, that is, vn = 1, and the edge weights typically depend on the distance of the samples involved, that is, ∀n, n′ : γn,n′ = f (|n′ − n|), for some function f (·) that speciﬁes the shape of a “weight window”. [sent-463, score-0.365]
</p><p>40 For the experiments in this article, we employ a mirrored sliding window with edge weights  γn,n′   2,  = 1,   0,  if n + n′ ≤ d + 1 or n + n′ ≥ 2N − 1 , if |n′ − n| ≤ d, n + n′ > d + 1 and n + n′ < 2N − 1 , otherwise . [sent-465, score-0.237]
</p><p>41 These weights compensate the limited connectivity of the few ﬁrst and last samples (which are connected by d to 2d − 1 edges) in contrast to intermediate samples (connected by 2d edges). [sent-466, score-0.19]
</p><p>42 Figure 7: (a) A mirrored square sliding window training graph with a half-width of d = 2. [sent-468, score-0.247]
</p><p>43 (b) Illustration of the edge weights of an intermediate node x(n) for an arbitrary window half-width d. [sent-470, score-0.226]
</p><p>44 After various experiments, we have found useful to enforce the normalization restriction (20) at least approximately (after node and edge weights have been normalized). [sent-482, score-0.213]
</p><p>45 This is the reason why we make the node weights of the ﬁrst and last groups of samples in the serial training graph weaker, the intra-group connections of the ﬁrst and last groups of samples in the mixed graph stronger, and introduced mirroring for the square sliding window graph. [sent-486, score-0.712]
</p><p>46 ˙ In the sliding window training graph with arbitrary window, the computation of CG and CG requires O (dN) operations. [sent-487, score-0.219]
</p><p>47 3 Serial Training Graph The serial training graph is similar to the clustered training graph used for classiﬁcation in terms of construction and efﬁciency. [sent-492, score-0.399]
</p><p>48 The samples used for training are denoted by xl (n), where the index l (1 ≤ l ≤ L) denotes the group (discrete label) and n (1 ≤ n ≤ Nl ) denotes the sample within such a group. [sent-501, score-0.236]
</p><p>49 The edge weight of the edge (xl (n), xl+1 (n′ )) is denoted by n γl,l+1 , and we use the same edge weight for all connections: ∀n, n′ , l : γl,l+1 = 1. [sent-505, score-0.244]
</p><p>50 Thus, all edges n,n′ n,n′ have a weight of 1, and all samples are assigned a weight of 2 except for the samples in the ﬁrst and last groups, which have a weight of 1 (Figure 8). [sent-506, score-0.217]
</p><p>51 The reason for the different weights in the ﬁrst and last groups is to improve feature quality by enforcing the normalization restriction (20) (after node and edge weight normalization). [sent-507, score-0.285]
</p><p>52 (11)  The sum of vertex weights is Q = Ng + 2Ng (L − 2) + Ng = 2Ng (L − 1) and the sum of edge (10)  weights is R = (L − 1)(Ng )2 , which is also the number of connections considered. [sent-509, score-0.259]
</p><p>53 Similarly to the clustered def ˆ training graph, deﬁne the average of the samples from the group l as xl = ∑n xl (n)/Ng , the sum of the products of samples from group l as Πl = ∑n xl (n)(xl (n))T , and the weighted sample average 3701  E SCALANTE -B. [sent-511, score-0.695]
</p><p>54 AND W ISKOTT  Figure 8: Illustration of a serial training graph with L discrete labels. [sent-512, score-0.217]
</p><p>55 as: L−1 1 x1 (n) + xL (n) + 2 ∑ xl (n) Q∑ n l=2  def  ˆ x =  =  L−1  1 2(L − 1)  ˆ ˆ ˆ x1 + xL + 2 ∑ xl  . [sent-515, score-0.368]
</p><p>56 (33)  l=2  From (12), the sample covariance matrix accounting for the weights vl of the serial training n graph is: L−1  =  1 Q  xx ∑ x1 (n)(x1 (n))T + 2 ∑ ∑ xl (n)(xl (n))T + ∑ xL (n)(xL (n))T − Qˆ (ˆ )T  1 Q  (12,33)  Cser =  Π1 + ΠL + 2 ∑ Πl − Qˆ ′ (ˆ ′ )T x x  l=2 n  n  n  L−1  . [sent-516, score-0.41]
</p><p>57 4 Mixed Training Graph The serial training graph does not have intra-group connections, and therefore the output differences of samples with the same label are not explicitly being minimized. [sent-524, score-0.294]
</p><p>58 We thus conceived the mixed training graph (Figure 9), which is a combination of the serial and clustered training graph and fulﬁls the consistency restriction (20). [sent-527, score-0.452]
</p><p>59 In the mixed training graph, all nodes and edges have a weight of 1, except for the intra-group edges in the ﬁrst and last groups, which have a weight of 2. [sent-528, score-0.241]
</p><p>60 All vertex and edge weights are equal to 1 except for the intra-group edge weights of the ﬁrst and last groups, which are equal to 2 as represented by thick lines. [sent-532, score-0.283]
</p><p>61 5 Supervised Step for Regression Problems There are at least three approaches to implement the supervised step on top of SFA to learn a mapping from slow features to the labels. [sent-534, score-0.228]
</p><p>62 For instance, one can use def  ℓ =  ˜ L  ∑ ℓ˜l · P(Cℓ˜ |y) l  (36)  l=1  if the loss function is the RMSE, where the slow features y might be extracted using any of the four SFA-based methods for regression above. [sent-547, score-0.327]
</p><p>63 We circumvent this by using three efﬁcient dimensionality reduction methods, and by applying supervised processing on the lower-dimensional features extracted. [sent-628, score-0.228]
</p><p>64 • GSFA with a mirrored sliding window graph with d = 32 (MSW32). [sent-645, score-0.187]
</p><p>65 • GSFA with a mirrored sliding window graph with d = 64 (MSW64). [sent-652, score-0.187]
</p><p>66 • GSFA with a serial training graph with L = 50 groups of Ng = 600 images (serial). [sent-653, score-0.275]
</p><p>67 The evolution across the hierarchical network of the two slowest features extracted by HSFA is illustrated in Figure 11. [sent-660, score-0.187]
</p><p>68 AND W ISKOTT  Figure 11: Evolution of the slow features extracted from test data after layers 1, 4, 7 and 11 of a GSFA network trained with the serial training graph. [sent-665, score-0.316]
</p><p>69 4 R ESULTS We evaluated all the combinations of a dimensionality reduction method (reordering, MSW32, MSW64, serial, mixed and HPCA) and a supervised post-processing algorithm (NCC, Soft GC, SVM, LR). [sent-679, score-0.194]
</p><p>70 The labels estimated depend on three parameters: the number of features passed to the supervised postprocessing algorithm, and the parameters C and γ in the case of the SVM. [sent-681, score-0.208]
</p><p>71 The results are presented in Table 2, and analyzed focusing on four aspects: the dimensionalityreduction method, the number of features used, the supervised methods, and the training graphs. [sent-683, score-0.214]
</p><p>72 Thus, the information that encodes the horizontal position of a face is mixed with other information and distributed over many principal components, whereas it is more concentrated in the slowest components of SFA. [sent-727, score-0.177]
</p><p>73 If one focuses on the post-processing methods, one can observe that linear regression performed poorly conﬁrming that a linear supervised step is too weak, particularly when the dimensionality reduction is also linear (e. [sent-728, score-0.19]
</p><p>74 Regarding the training graphs, we expected that the sliding window graphs, MSW32 and MSW64, would be more accurate than the serial and mixed graphs, even when using a square window, because the labels are not discretized. [sent-734, score-0.298]
</p><p>75 The RMSE of the serial graph was smaller than the one of the mixed graph by less than 2% (for Soft GC), making it uncertain for statistical reasons which one of these graphs is better for this problem. [sent-738, score-0.314]
</p><p>76 We call GSFA implicitly supervised because the labels themselves are never provided to it, but only the training graphs, which encode the labels through their structure. [sent-746, score-0.256]
</p><p>77 While the construction of the graph is a supervised operation, GSFA works in an unsupervised fashion on structured data. [sent-747, score-0.192]
</p><p>78 Hence, GSFA does not search for a ﬁt to the labels explicitly but instead fully concentrates on the generation of slow features according to the topology deﬁned by the graph. [sent-748, score-0.194]
</p><p>79 First, determining the edges and edge weights is a trivial operation because they are derived from the labels in a simple manner. [sent-753, score-0.223]
</p><p>80 When solving a concrete supervised learning problem, the features extracted by unsupervised dimensionality reduction algorithms are often suboptimal. [sent-758, score-0.291]
</p><p>81 For instance, PCA does not yield good features for age estimation from adult face photographs because features revealing age (e. [sent-759, score-0.269]
</p><p>82 Supervised dimensionality reduction algorithms, including GSFA, are specially promising when one does not know a good set of features for the particular data and problem at hand, and one wants to improve performance by generating features adapted to the speciﬁc data and labels. [sent-762, score-0.206]
</p><p>83 For this purpose, they use an importance measure p(xt+1 , xt ) ≥ 0 with high values for transitions within the desired subspace and propose the objective function def  min s′ (yi ) =  1 n−1 (yi (t + 1) − yi (t))2 ∑ p(xt+1 , xt ) , n − 1 t=1  def  with yi (t) = φi (x(t)). [sent-782, score-0.236]
</p><p>84 In this case, the elements Dii play the role of the node weights vi , and the elements Wi j play the role of the edge weights γi, j . [sent-806, score-0.252]
</p><p>85 One difference between LPP and GSFA is that the additional normalization factors Q and R of GSFA provide invariance to the scale of the edge and node weights specifying a particular feature and objective function normalization. [sent-807, score-0.238]
</p><p>86 A second difference is that for GSFA the node weights, fundamental for feature normalization, can be chosen independently from the edge weights (unless one explicitly enforces (20)), whereas for LPP the diagonal def elements Dii = ∑ j Wi j are derived from the edge weights. [sent-808, score-0.391]
</p><p>87 Given a similarity matrix def Wi j and diagonal matrix D with diagonal elements Dii = ∑ j Wi j , one solves a GSFA problem deﬁned over a training graph with the same samples, edge weights γi, j = Wi j , and node weights vi = Dii . [sent-810, score-0.512]
</p><p>88 Given a training graph with edge weights γi, j and node weights vi , we can deﬁne the following similarity matrix: Wi j =  2γi, j /R, vi /Q − ∑ j′ =i Wi j′ ,  for i = j, with R as deﬁned in (10) , for i = j, with Q as deﬁned in (11) . [sent-814, score-0.394]
</p><p>89 We show that a few slow features allow for an accurate solution to the supervised learning problem, requiring only a small postprocessing step that maps the features to labels or classes. [sent-819, score-0.348]
</p><p>90 Training with the clustered graph is equivalent to considering all transitions between samples of the same identity and no transition between different identities. [sent-822, score-0.191]
</p><p>91 Thus, the features extracted with GSFA from the clustered graph are not better or worse than those extracted with FDA. [sent-834, score-0.27]
</p><p>92 Early, undocumented versions of the mixed and serial training graphs were employed. [sent-856, score-0.21]
</p><p>93 The serial and mixed training graphs have provided the best accuracy for the experiments in this article, with the serial one being slightly more accurate but not to a signiﬁcant level. [sent-873, score-0.285]
</p><p>94 These graphs have the advantage over the sliding window graph that they are also suitable for cases in which the labels are discrete from the beginning, which occurs frequently due to the ﬁnite resolution in measurements or due to discrete phenomena (e. [sent-874, score-0.256]
</p><p>95 Since the edge weights supported by GSFA are arbitrary, it is tempting to use a complete weight (ℓ −ℓ )2 1 matrix continuous in the labels (e. [sent-877, score-0.212]
</p><p>96 For instance, using another algorithm, the estimation of age from face images has been reported to improve when also gender and race labels are estimated (Guo and Mu, 2011). [sent-886, score-0.209]
</p><p>97 The features extracted by GSFA strongly depend on the labels, even though label information is only provided implicitly by the graph connectivity. [sent-888, score-0.215]
</p><p>98 On the other hand GSFA is motivated from supervised learning, and was conceived as an extension of SFA designed for supervised non-linear dimensionality reduction speciﬁcally targeting regression and classiﬁcation problems. [sent-899, score-0.299]
</p><p>99 Although supervised learning is less biologically plausible, GSFA being implicitly supervised is still closely connected to feasible unsupervised biological models through the probabilistic interpretation of the graph. [sent-902, score-0.218]
</p><p>100 Gender and age estimation from synthetic face images with hierarchical slow feature analysis. [sent-981, score-0.265]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sfa', 0.609), ('gsfa', 0.58), ('xl', 0.125), ('def', 0.118), ('wiskott', 0.111), ('lpp', 0.111), ('ns', 0.108), ('iskott', 0.099), ('olve', 0.099), ('scalante', 0.099), ('vn', 0.095), ('supervised', 0.088), ('xs', 0.083), ('graph', 0.082), ('hsfa', 0.077), ('serial', 0.075), ('slow', 0.074), ('fda', 0.072), ('weights', 0.068), ('cg', 0.067), ('features', 0.066), ('roblems', 0.066), ('egression', 0.066), ('edge', 0.064), ('face', 0.063), ('hpca', 0.061), ('training', 0.06), ('lassification', 0.059), ('labels', 0.054), ('node', 0.052), ('samples', 0.051), ('rmse', 0.051), ('franzius', 0.047), ('reordering', 0.044), ('graphs', 0.043), ('slowness', 0.043), ('dimensionality', 0.043), ('window', 0.042), ('extracted', 0.041), ('slowest', 0.04), ('connections', 0.04), ('clustered', 0.04), ('hierarchical', 0.04), ('klamp', 0.039), ('berkes', 0.038), ('images', 0.037), ('edges', 0.037), ('sliding', 0.035), ('zt', 0.034), ('sh', 0.034), ('mixed', 0.032), ('ow', 0.032), ('reduction', 0.031), ('sprekeler', 0.03), ('classi', 0.029), ('gender', 0.029), ('normalization', 0.029), ('maass', 0.028), ('pca', 0.028), ('regression', 0.028), ('cclus', 0.028), ('mirrored', 0.028), ('label', 0.026), ('age', 0.026), ('gc', 0.026), ('sejnowski', 0.026), ('ng', 0.026), ('weight', 0.026), ('feature', 0.025), ('position', 0.024), ('soft', 0.023), ('layer', 0.023), ('nodes', 0.023), ('unsupervised', 0.022), ('detection', 0.022), ('cser', 0.022), ('lfda', 0.022), ('photographs', 0.022), ('reordered', 0.022), ('chain', 0.021), ('conceived', 0.021), ('groups', 0.021), ('vertices', 0.021), ('ful', 0.02), ('connected', 0.02), ('vertex', 0.019), ('hmer', 0.019), ('dii', 0.019), ('er', 0.019), ('afterwards', 0.018), ('recognition', 0.018), ('identity', 0.018), ('horizontal', 0.018), ('restrictions', 0.017), ('centroid', 0.017), ('discriminant', 0.017), ('wt', 0.017), ('mohamed', 0.017), ('eye', 0.017), ('bray', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="52-tfidf-1" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>Author: Alberto N. Escalante-B., Laurenz Wiskott</p><p>Abstract: Supervised learning from high-dimensional data, for example, multimedia data, is a challenging task. We propose an extension of slow feature analysis (SFA) for supervised dimensionality reduction called graph-based SFA (GSFA). The algorithm extracts a label-predictive low-dimensional set of features that can be post-processed by typical supervised algorithms to generate the ﬁnal label or class estimation. GSFA is trained with a so-called training graph, in which the vertices are the samples and the edges represent similarities of the corresponding labels. A new weighted SFA optimization problem is introduced, generalizing the notion of slowness from sequences of samples to such training graphs. We show that GSFA computes an optimal solution to this problem in the considered function space and propose several types of training graphs. For classiﬁcation, the most straightforward graph yields features equivalent to those of (nonlinear) Fisher discriminant analysis. Emphasis is on regression, where four different graphs were evaluated experimentally with a subproblem of face detection on photographs. The method proposed is promising particularly when linear models are insufﬁcient as well as when feature selection is difﬁcult. Keywords: slow feature analysis, feature extraction, classiﬁcation, regression, pattern recognition, training graphs, nonlinear dimensionality reduction, supervised learning, implicitly supervised, high-dimensional data, image analysis</p><p>2 0.37924325 <a title="52-tfidf-2" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>Author: Wendelin Böhmer, Steffen Grünewälder, Yun Shen, Marek Musial, Klaus Obermayer</p><p>Abstract: Linear reinforcement learning (RL) algorithms like least-squares temporal difference learning (LSTD) require basis functions that span approximation spaces of potential value functions. This article investigates methods to construct these bases from samples. We hypothesize that an ideal approximation spaces should encode diffusion distances and that slow feature analysis (SFA) constructs such spaces. To validate our hypothesis we provide theoretical statements about the LSTD value approximation error and induced metric of approximation spaces constructed by SFA and the state-of-the-art methods Krylov bases and proto-value functions (PVF). In particular, we prove that SFA minimizes the average (over all tasks in the same environment) bound on the above approximation error. Compared to other methods, SFA is very sensitive to sampling and can sometimes fail to encode the whole state space. We derive a novel importance sampling modiﬁcation to compensate for this effect. Finally, the LSTD and least squares policy iteration (LSPI) performance of approximation spaces constructed by Krylov bases, PVF, SFA and PCA is compared in benchmark tasks and a visual robot navigation experiment (both in a realistic simulation and with a robot). The results support our hypothesis and suggest that (i) SFA provides subspace-invariant features for MDPs with self-adjoint transition operators, which allows strong guarantees on the approximation error, (ii) the modiﬁed SFA algorithm is best suited for LSPI in both discrete and continuous state spaces and (iii) approximation spaces encoding diffusion distances facilitate LSPI performance. Keywords: reinforcement learning, diffusion distance, proto value functions, slow feature analysis, least-squares policy iteration, visual robot navigation c 2013 Wendelin B¨ hmer, Steffen Gr¨ new¨ lder, Yun Shen, Marek Musial and Klaus Obermayer. o u a ¨ ¨ ¨ B OHMER , G R UNEW ALDER , S HEN , M USIAL AND O BERMAYER</p><p>3 0.081795074 <a title="52-tfidf-3" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>Author: Jun Wang, Tony Jebara, Shih-Fu Chang</p><p>Abstract: Graph-based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems, particularly in agnostic settings when no parametric information or other prior knowledge is available about the data distribution. Given the constructed graph represented by a weight matrix, transductive inference is used to propagate known labels to predict the values of all unlabeled vertices. Designing a robust label diffusion algorithm for such graphs is a widely studied problem and various methods have recently been suggested. Many of these can be formalized as regularized function estimation through the minimization of a quadratic cost. However, most existing label diffusion methods minimize a univariate cost with the classiﬁcation function as the only variable of interest. Since the observed labels seed the diffusion process, such univariate frameworks are extremely sensitive to the initial label choice and any label noise. To alleviate the dependency on the initial observed labels, this article proposes a bivariate formulation for graph-based SSL, where both the binary label information and a continuous classiﬁcation function are arguments of the optimization. This bivariate formulation is shown to be equivalent to a linearly constrained Max-Cut problem. Finally an efﬁcient solution via greedy gradient Max-Cut (GGMC) is derived which gradually assigns unlabeled vertices to each class with minimum connectivity. Once convergence guarantees are established, this greedy Max-Cut based SSL is applied on both artiﬁcial and standard benchmark data sets where it obtains superior classiﬁcation accuracy compared to existing state-of-the-art SSL methods. Moreover, GGMC shows robustness with respect to the graph construction method and maintains high accuracy over extensive experiments with various edge linking and weighting schemes. Keywords: graph transduction, semi-supervised learning, bivariate formulation, mixed integer programming, greedy</p><p>4 0.054491818 <a title="52-tfidf-4" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>Author: Julien Mairal, Bin Yu</p><p>Abstract: We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called “path coding” penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efﬁciently solve by leveraging network ﬂow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs. Keywords: convex and non-convex optimization, network ﬂow optimization, graph sparsity</p><p>5 0.049031317 <a title="52-tfidf-5" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>Author: Nicolò Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella</p><p>Abstract: We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world data sets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice. Keywords: online learning, learning on graphs, graph prediction, random spanning trees</p><p>6 0.03980913 <a title="52-tfidf-6" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>7 0.037897754 <a title="52-tfidf-7" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>8 0.037817225 <a title="52-tfidf-8" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>9 0.037479013 <a title="52-tfidf-9" href="./jmlr-2013-Language-Motivated_Approaches_to_Action_Recognition.html">58 jmlr-2013-Language-Motivated Approaches to Action Recognition</a></p>
<p>10 0.036449868 <a title="52-tfidf-10" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>11 0.036197342 <a title="52-tfidf-11" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>12 0.035899702 <a title="52-tfidf-12" href="./jmlr-2013-Keep_It_Simple_And_Sparse%3A_Real-Time_Action_Recognition.html">56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</a></p>
<p>13 0.033837371 <a title="52-tfidf-13" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>14 0.032484028 <a title="52-tfidf-14" href="./jmlr-2013-A_Theory_of_Multiclass_Boosting.html">8 jmlr-2013-A Theory of Multiclass Boosting</a></p>
<p>15 0.03178075 <a title="52-tfidf-15" href="./jmlr-2013-One-shot_Learning_Gesture_Recognition_from_RGB-D_Data_Using_Bag_of_Features.html">80 jmlr-2013-One-shot Learning Gesture Recognition from RGB-D Data Using Bag of Features</a></p>
<p>16 0.031608019 <a title="52-tfidf-16" href="./jmlr-2013-Optimally_Fuzzy_Temporal_Memory.html">82 jmlr-2013-Optimally Fuzzy Temporal Memory</a></p>
<p>17 0.030865224 <a title="52-tfidf-17" href="./jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion.html">110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</a></p>
<p>18 0.029415919 <a title="52-tfidf-18" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>19 0.02794495 <a title="52-tfidf-19" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>20 0.027891586 <a title="52-tfidf-20" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.179), (1, 0.063), (2, -0.147), (3, 0.056), (4, -0.074), (5, 0.243), (6, -0.318), (7, 0.024), (8, -0.002), (9, -0.256), (10, 0.141), (11, 0.213), (12, -0.283), (13, 0.025), (14, -0.378), (15, -0.022), (16, -0.123), (17, -0.087), (18, 0.154), (19, -0.1), (20, 0.046), (21, 0.152), (22, 0.087), (23, 0.005), (24, 0.064), (25, -0.005), (26, 0.083), (27, 0.074), (28, 0.017), (29, 0.025), (30, -0.012), (31, -0.025), (32, -0.046), (33, -0.046), (34, 0.007), (35, 0.085), (36, -0.036), (37, -0.064), (38, -0.026), (39, 0.005), (40, -0.012), (41, 0.008), (42, -0.005), (43, 0.085), (44, -0.098), (45, -0.006), (46, -0.024), (47, -0.011), (48, -0.027), (49, -0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93101358 <a title="52-lsi-1" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>Author: Alberto N. Escalante-B., Laurenz Wiskott</p><p>Abstract: Supervised learning from high-dimensional data, for example, multimedia data, is a challenging task. We propose an extension of slow feature analysis (SFA) for supervised dimensionality reduction called graph-based SFA (GSFA). The algorithm extracts a label-predictive low-dimensional set of features that can be post-processed by typical supervised algorithms to generate the ﬁnal label or class estimation. GSFA is trained with a so-called training graph, in which the vertices are the samples and the edges represent similarities of the corresponding labels. A new weighted SFA optimization problem is introduced, generalizing the notion of slowness from sequences of samples to such training graphs. We show that GSFA computes an optimal solution to this problem in the considered function space and propose several types of training graphs. For classiﬁcation, the most straightforward graph yields features equivalent to those of (nonlinear) Fisher discriminant analysis. Emphasis is on regression, where four different graphs were evaluated experimentally with a subproblem of face detection on photographs. The method proposed is promising particularly when linear models are insufﬁcient as well as when feature selection is difﬁcult. Keywords: slow feature analysis, feature extraction, classiﬁcation, regression, pattern recognition, training graphs, nonlinear dimensionality reduction, supervised learning, implicitly supervised, high-dimensional data, image analysis</p><p>2 0.80821532 <a title="52-lsi-2" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>Author: Wendelin Böhmer, Steffen Grünewälder, Yun Shen, Marek Musial, Klaus Obermayer</p><p>Abstract: Linear reinforcement learning (RL) algorithms like least-squares temporal difference learning (LSTD) require basis functions that span approximation spaces of potential value functions. This article investigates methods to construct these bases from samples. We hypothesize that an ideal approximation spaces should encode diffusion distances and that slow feature analysis (SFA) constructs such spaces. To validate our hypothesis we provide theoretical statements about the LSTD value approximation error and induced metric of approximation spaces constructed by SFA and the state-of-the-art methods Krylov bases and proto-value functions (PVF). In particular, we prove that SFA minimizes the average (over all tasks in the same environment) bound on the above approximation error. Compared to other methods, SFA is very sensitive to sampling and can sometimes fail to encode the whole state space. We derive a novel importance sampling modiﬁcation to compensate for this effect. Finally, the LSTD and least squares policy iteration (LSPI) performance of approximation spaces constructed by Krylov bases, PVF, SFA and PCA is compared in benchmark tasks and a visual robot navigation experiment (both in a realistic simulation and with a robot). The results support our hypothesis and suggest that (i) SFA provides subspace-invariant features for MDPs with self-adjoint transition operators, which allows strong guarantees on the approximation error, (ii) the modiﬁed SFA algorithm is best suited for LSPI in both discrete and continuous state spaces and (iii) approximation spaces encoding diffusion distances facilitate LSPI performance. Keywords: reinforcement learning, diffusion distance, proto value functions, slow feature analysis, least-squares policy iteration, visual robot navigation c 2013 Wendelin B¨ hmer, Steffen Gr¨ new¨ lder, Yun Shen, Marek Musial and Klaus Obermayer. o u a ¨ ¨ ¨ B OHMER , G R UNEW ALDER , S HEN , M USIAL AND O BERMAYER</p><p>3 0.24414828 <a title="52-lsi-3" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>Author: Jun Wang, Tony Jebara, Shih-Fu Chang</p><p>Abstract: Graph-based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems, particularly in agnostic settings when no parametric information or other prior knowledge is available about the data distribution. Given the constructed graph represented by a weight matrix, transductive inference is used to propagate known labels to predict the values of all unlabeled vertices. Designing a robust label diffusion algorithm for such graphs is a widely studied problem and various methods have recently been suggested. Many of these can be formalized as regularized function estimation through the minimization of a quadratic cost. However, most existing label diffusion methods minimize a univariate cost with the classiﬁcation function as the only variable of interest. Since the observed labels seed the diffusion process, such univariate frameworks are extremely sensitive to the initial label choice and any label noise. To alleviate the dependency on the initial observed labels, this article proposes a bivariate formulation for graph-based SSL, where both the binary label information and a continuous classiﬁcation function are arguments of the optimization. This bivariate formulation is shown to be equivalent to a linearly constrained Max-Cut problem. Finally an efﬁcient solution via greedy gradient Max-Cut (GGMC) is derived which gradually assigns unlabeled vertices to each class with minimum connectivity. Once convergence guarantees are established, this greedy Max-Cut based SSL is applied on both artiﬁcial and standard benchmark data sets where it obtains superior classiﬁcation accuracy compared to existing state-of-the-art SSL methods. Moreover, GGMC shows robustness with respect to the graph construction method and maintains high accuracy over extensive experiments with various edge linking and weighting schemes. Keywords: graph transduction, semi-supervised learning, bivariate formulation, mixed integer programming, greedy</p><p>4 0.20099254 <a title="52-lsi-4" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>Author: Nicolò Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella</p><p>Abstract: We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world data sets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice. Keywords: online learning, learning on graphs, graph prediction, random spanning trees</p><p>5 0.17507365 <a title="52-lsi-5" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>Author: Anastasios Roussos, Stavros Theodorakis, Vassilis Pitsikalis, Petros Maragos</p><p>Abstract: We propose the novel approach of dynamic afﬁne-invariant shape-appearance model (Aff-SAM) and employ it for handshape classiﬁcation and sign recognition in sign language (SL) videos. AffSAM offers a compact and descriptive representation of hand conﬁgurations as well as regularized model-ﬁtting, assisting hand tracking and extracting handshape features. We construct SA images representing the hand’s shape and appearance without landmark points. We model the variation of the images by linear combinations of eigenimages followed by afﬁne transformations, accounting for 3D hand pose changes and improving model’s compactness. We also incorporate static and dynamic handshape priors, offering robustness in occlusions, which occur often in signing. The approach includes an afﬁne signer adaptation component at the visual level, without requiring training from scratch a new singer-speciﬁc model. We rather employ a short development data set to adapt the models for a new signer. Experiments on the Boston-University-400 continuous SL corpus demonstrate improvements on handshape classiﬁcation when compared to other feature extraction approaches. Supplementary evaluations of sign recognition experiments, are conducted on a multi-signer, 100-sign data set, from the Greek sign language lemmas corpus. These explore the fusion with movement cues as well as signer adaptation of Aff-SAM to multiple signers providing promising results. Keywords: afﬁne-invariant shape-appearance model, landmarks-free shape representation, static and dynamic priors, feature extraction, handshape classiﬁcation</p><p>6 0.15787035 <a title="52-lsi-6" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>7 0.15251951 <a title="52-lsi-7" href="./jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion.html">110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</a></p>
<p>8 0.14709148 <a title="52-lsi-8" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>9 0.14444411 <a title="52-lsi-9" href="./jmlr-2013-Keep_It_Simple_And_Sparse%3A_Real-Time_Action_Recognition.html">56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</a></p>
<p>10 0.14225926 <a title="52-lsi-10" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>11 0.14115953 <a title="52-lsi-11" href="./jmlr-2013-Optimally_Fuzzy_Temporal_Memory.html">82 jmlr-2013-Optimally Fuzzy Temporal Memory</a></p>
<p>12 0.13971834 <a title="52-lsi-12" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>13 0.13798468 <a title="52-lsi-13" href="./jmlr-2013-One-shot_Learning_Gesture_Recognition_from_RGB-D_Data_Using_Bag_of_Features.html">80 jmlr-2013-One-shot Learning Gesture Recognition from RGB-D Data Using Bag of Features</a></p>
<p>14 0.13452125 <a title="52-lsi-14" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>15 0.13429067 <a title="52-lsi-15" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>16 0.13303825 <a title="52-lsi-16" href="./jmlr-2013-Language-Motivated_Approaches_to_Action_Recognition.html">58 jmlr-2013-Language-Motivated Approaches to Action Recognition</a></p>
<p>17 0.13212305 <a title="52-lsi-17" href="./jmlr-2013-GURLS%3A_A_Least_Squares_Library_for_Supervised_Learning.html">46 jmlr-2013-GURLS: A Least Squares Library for Supervised Learning</a></p>
<p>18 0.1275598 <a title="52-lsi-18" href="./jmlr-2013-BudgetedSVM%3A_A_Toolbox_for_Scalable_SVM_Approximations.html">19 jmlr-2013-BudgetedSVM: A Toolbox for Scalable SVM Approximations</a></p>
<p>19 0.12667066 <a title="52-lsi-19" href="./jmlr-2013-Truncated_Power_Method_for_Sparse_Eigenvalue_Problems.html">116 jmlr-2013-Truncated Power Method for Sparse Eigenvalue Problems</a></p>
<p>20 0.12662469 <a title="52-lsi-20" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.021), (5, 0.141), (6, 0.049), (10, 0.086), (20, 0.026), (23, 0.048), (29, 0.319), (44, 0.012), (68, 0.034), (70, 0.022), (75, 0.05), (85, 0.022), (87, 0.047), (89, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.75860447 <a title="52-lda-1" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>Author: Vinay Jethava, Anders Martinsson, Chiranjib Bhattacharyya, Devdatt Dubhashi</p><p>Abstract: In this paper we establish that the Lov´ sz ϑ function on a graph can be restated as a kernel learning a problem. We introduce the notion of SVM − ϑ graphs, on which Lov´ sz ϑ function can be apa proximated well by a Support vector machine (SVM). We show that Erd¨ s-R´ nyi random G(n, p) o e log4 n np graphs are SVM − ϑ graphs for n ≤ p < 1. Even if we embed a large clique of size Θ 1−p in a G(n, p) graph the resultant graph still remains a SVM − ϑ graph. This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. Associated with the ϑ function is the notion of orthogonal labellings. We introduce common orthogonal labellings which extends the idea of orthogonal labellings to multiple graphs. This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude. Keywords: orthogonal labellings of graphs, planted cliques, random graphs, common dense subgraph</p><p>same-paper 2 0.72798383 <a title="52-lda-2" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>Author: Alberto N. Escalante-B., Laurenz Wiskott</p><p>Abstract: Supervised learning from high-dimensional data, for example, multimedia data, is a challenging task. We propose an extension of slow feature analysis (SFA) for supervised dimensionality reduction called graph-based SFA (GSFA). The algorithm extracts a label-predictive low-dimensional set of features that can be post-processed by typical supervised algorithms to generate the ﬁnal label or class estimation. GSFA is trained with a so-called training graph, in which the vertices are the samples and the edges represent similarities of the corresponding labels. A new weighted SFA optimization problem is introduced, generalizing the notion of slowness from sequences of samples to such training graphs. We show that GSFA computes an optimal solution to this problem in the considered function space and propose several types of training graphs. For classiﬁcation, the most straightforward graph yields features equivalent to those of (nonlinear) Fisher discriminant analysis. Emphasis is on regression, where four different graphs were evaluated experimentally with a subproblem of face detection on photographs. The method proposed is promising particularly when linear models are insufﬁcient as well as when feature selection is difﬁcult. Keywords: slow feature analysis, feature extraction, classiﬁcation, regression, pattern recognition, training graphs, nonlinear dimensionality reduction, supervised learning, implicitly supervised, high-dimensional data, image analysis</p><p>3 0.50224549 <a title="52-lda-3" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>Author: Yuchen Zhang, John C. Duchi, Martin J. Wainwright</p><p>Abstract: We analyze two communication-efﬁcient algorithms for distributed optimization in statistical settings involving large-scale data sets. The ﬁrst algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves √ mean-squared error (MSE) that decays as O (N −1 + (N/m)−2 ). Whenever m ≤ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O (N −1 + (N/m)−3 ), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O (N −1 + (N/m)−3/2 ), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efﬁciently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 108 samples and d ≈ 740,000 covariates. Keywords: distributed learning, stochastic optimization, averaging, subsampling</p><p>4 0.49758518 <a title="52-lda-4" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>Author: Wendelin Böhmer, Steffen Grünewälder, Yun Shen, Marek Musial, Klaus Obermayer</p><p>Abstract: Linear reinforcement learning (RL) algorithms like least-squares temporal difference learning (LSTD) require basis functions that span approximation spaces of potential value functions. This article investigates methods to construct these bases from samples. We hypothesize that an ideal approximation spaces should encode diffusion distances and that slow feature analysis (SFA) constructs such spaces. To validate our hypothesis we provide theoretical statements about the LSTD value approximation error and induced metric of approximation spaces constructed by SFA and the state-of-the-art methods Krylov bases and proto-value functions (PVF). In particular, we prove that SFA minimizes the average (over all tasks in the same environment) bound on the above approximation error. Compared to other methods, SFA is very sensitive to sampling and can sometimes fail to encode the whole state space. We derive a novel importance sampling modiﬁcation to compensate for this effect. Finally, the LSTD and least squares policy iteration (LSPI) performance of approximation spaces constructed by Krylov bases, PVF, SFA and PCA is compared in benchmark tasks and a visual robot navigation experiment (both in a realistic simulation and with a robot). The results support our hypothesis and suggest that (i) SFA provides subspace-invariant features for MDPs with self-adjoint transition operators, which allows strong guarantees on the approximation error, (ii) the modiﬁed SFA algorithm is best suited for LSPI in both discrete and continuous state spaces and (iii) approximation spaces encoding diffusion distances facilitate LSPI performance. Keywords: reinforcement learning, diffusion distance, proto value functions, slow feature analysis, least-squares policy iteration, visual robot navigation c 2013 Wendelin B¨ hmer, Steffen Gr¨ new¨ lder, Yun Shen, Marek Musial and Klaus Obermayer. o u a ¨ ¨ ¨ B OHMER , G R UNEW ALDER , S HEN , M USIAL AND O BERMAYER</p><p>5 0.49685344 <a title="52-lda-5" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>Author: Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Pierre Dupont</p><p>Abstract: We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efﬁciently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about speciﬁc groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy. Keywords: group feature selection, generalized spike-and-slab priors, expectation propagation, sparse linear model, approximate inference, sequential experimental design, signal reconstruction</p><p>6 0.49476644 <a title="52-lda-6" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>7 0.49423778 <a title="52-lda-7" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>8 0.49303162 <a title="52-lda-8" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>9 0.49162573 <a title="52-lda-9" href="./jmlr-2013-GURLS%3A_A_Least_Squares_Library_for_Supervised_Learning.html">46 jmlr-2013-GURLS: A Least Squares Library for Supervised Learning</a></p>
<p>10 0.49148378 <a title="52-lda-10" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>11 0.49101532 <a title="52-lda-11" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>12 0.489301 <a title="52-lda-12" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>13 0.48854914 <a title="52-lda-13" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>14 0.48788473 <a title="52-lda-14" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>15 0.48700866 <a title="52-lda-15" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>16 0.48667875 <a title="52-lda-16" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>17 0.48348787 <a title="52-lda-17" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>18 0.48236093 <a title="52-lda-18" href="./jmlr-2013-Convex_and_Scalable_Weakly_Labeled_SVMs.html">29 jmlr-2013-Convex and Scalable Weakly Labeled SVMs</a></p>
<p>19 0.48226005 <a title="52-lda-19" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>20 0.48217434 <a title="52-lda-20" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
