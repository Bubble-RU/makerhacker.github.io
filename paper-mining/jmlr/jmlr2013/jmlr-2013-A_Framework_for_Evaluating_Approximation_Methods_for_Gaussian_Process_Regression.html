<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-3" href="#">jmlr2013-3</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</h1>
<br/><p>Source: <a title="jmlr-2013-3-pdf" href="http://jmlr.org/papers/volume14/chalupka13a/chalupka13a.pdf">pdf</a></p><p>Author: Krzysztof Chalupka, Christopher K. I. Williams, Iain Murray</p><p>Abstract: Gaussian process (GP) predictors are an important component of many Bayesian approaches to machine learning. However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n2 ) space and O(n3 ) time for a data set of n examples. Several approximation methods have been proposed, but there is a lack of understanding of the relative merits of the different approximations, and in what situations they are most useful. We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e.g., Subset of Data and FITC). We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons. Keywords: Gaussian process regression, subset of data, FITC, local GP</p><p>Reference: <a title="jmlr-2013-3-reference" href="../jmlr2013_reference/jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n2 ) space and O(n3 ) time for a data set of n examples. [sent-14, score-0.052]
</p><p>2 We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e. [sent-16, score-0.062]
</p><p>3 We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons. [sent-19, score-0.067]
</p><p>4 Keywords: Gaussian process regression, subset of data, FITC, local GP  1. [sent-20, score-0.053]
</p><p>5 The basic model on which these are based is Gaussian process regression (GPR), for which a standard implementation requires O(n2 ) space and O(n3 ) time for a data set of n examples (e. [sent-26, score-0.052]
</p><p>6 Our key recommendation is to study the quality of the predictions obtained as a function of the compute time taken as m is varied, as times can be compared across different methods. [sent-38, score-0.062]
</p><p>7 The time decomposes into that needed for training the predictor (including setting hyperparameters), and test time; the user needs to understand which will dominate in their application. [sent-42, score-0.135]
</p><p>8 We illustrate this process by studying four different approximation algorithms on four different prediction problems. [sent-43, score-0.068]
</p><p>9 Approximation Algorithms for Gaussian Process Regression (GPR) A regression task has a training set D = {xi , yi }n with D-dimensional inputs xi and scalar outputs i=1 yi . [sent-50, score-0.061]
</p><p>10 Assuming that the outputs are noisy observations of a latent function f at values fi = f (xi ), the goal is to compute a predictive distribution over the latent function value f∗ at a test location x∗ . [sent-51, score-0.055]
</p><p>11 Assuming a Gaussian process prior over functions f with zero mean, and covariance or kernel function k(·, ·), and Gaussian observations, yi = fi + εi where εi ∼ N (0, σ2 ), gives Gaussian predictions p( f∗ | x∗ , D ) = N ( f ∗ , [ f∗ ]), with predictive mean and variance (see, e. [sent-52, score-0.092]
</p><p>12 We identify three computational phases in carrying out GPR: hyperparameter learning: The hyperparameters are learned, by for example maximizing the log marginal likelihood. [sent-62, score-0.219]
</p><p>13 training: Given the hyperparameters, all computations that do not involve test inputs are performed, such as computing α above, and/or computing the Cholesky decomposition of K + n σ2 I. [sent-64, score-0.06]
</p><p>14 Training: the time required for preliminary computations before the test point x∗ is known, for each hyperparameter setting considered. [sent-69, score-0.217]
</p><p>15 variance): the time needed to compute the predictive mean (variance) at test point x∗ . [sent-71, score-0.091]
</p><p>16 testing: Only the computations involving the test inputs are carried out, those which could not have been done previously. [sent-72, score-0.06]
</p><p>17 Hyperparameter learning involves evaluating L for all values of the hyperparameters θ that are searched over, and so is more expensive than training for ﬁxed hyperparameters. [sent-78, score-0.129]
</p><p>18 We use the Fully Independent Training Conditional (FITC) method as it is recommended over other inducing point methods by Qui˜ onero-Candela et al. [sent-86, score-0.052]
</p><p>19 2 Inducing Point Methods: FITC A number of GP approximation algorithms use alternative kernel matrices based on inducing points, u, in the D-dimensional input space (Qui˜ onero-Candela and Rasmussen, 2005). [sent-108, score-0.095]
</p><p>20 Here we restrict n the m inducing points to be a subset of the training inputs. [sent-109, score-0.098]
</p><p>21 a To make predictions with FITC, and to evaluate its marginal likelihood, simply substitute kFITC for the original kernel in Equations 1–3. [sent-120, score-0.051]
</p><p>22 We again choose a set of inducing points of size m from the training inputs either randomly or using FPC, and use the FITC implementation from the gpml toolbox. [sent-126, score-0.154]
</p><p>23 It is possible to “mix and match” the SoD and FITC methods, adapting the hyperparameters to optimize the SoD approximation to the marginal likelihood, then using the FITC algorithm to make predictions using the same data subset and the SoD-trained hyperparameters. [sent-127, score-0.127]
</p><p>24 1 We expect that saving time on the hyperparameter learning phase, O(m3 ) instead of O(m2 n), will come at the cost of reducing the predictive performance of FITC for a given m. [sent-129, score-0.197]
</p><p>25 We n divide the n training points into k = ⌈ m ⌉ clusters each of size m, and run GPR in each cluster, 1. [sent-132, score-0.062]
</p><p>26 At test time we assign a test input x∗ to the closest cluster. [sent-135, score-0.096]
</p><p>27 Now repeat recursively in each cluster until the cluster size is no larger than m. [sent-144, score-0.054]
</p><p>28 A test point x∗ is assigned to the appropriate cluster by descending the tree of splits constructed by RPC. [sent-148, score-0.057]
</p><p>29 We implemented Local GPR using the gpml toolbox with small modiﬁcations to sum gradients for joint training. [sent-153, score-0.057]
</p><p>30 , Golub and Van Loan 1996) can be used at training time to solve the linear system (K + σ2 I)α = y. [sent-157, score-0.082]
</p><p>31 Indeed, all GPR computations can be based on iterative methods (Gibbs, 1997). [sent-158, score-0.051]
</p><p>32 Whether iterative methods can provide a speedup for GPR or not, fast MVM methods will certainly be required to scale to huge data sets. [sent-181, score-0.052]
</p><p>33 Storing the kernel elements on disk, or reproducing the kernel computations on the ﬂy, is prohibitively expensive. [sent-184, score-0.065]
</p><p>34 , many entries near zero) it would be possible to speed up MVMs using sparse matrix techniques, but in the hyperparameter regimes identiﬁed in practice this does not usually occur; 2) the piecewise constant approximations used by simple kd-tree approximations to GPR (Shen et al. [sent-188, score-0.188]
</p><p>35 There are open problems with making iterative methods and fast MVMs for GPR work routinely. [sent-203, score-0.052]
</p><p>36 Firstly, unlike standard dense linear algebra routines, the number of operations depends on the hyperparameter settings. [sent-204, score-0.136]
</p><p>37 The space and time complexities for the SoD, FITC, and Local methods are given in Table 1; as explained above there are open problems with making iterative methods and fast MVMs work routinely for GPR, see also Sections 4. [sent-212, score-0.088]
</p><p>38 Comparing Local to SoD, we might expect that using training points lying nearer to the test point would help, so that for ﬁxed m Local would beat SoD. [sent-217, score-0.076]
</p><p>39 So if equal training time was allowed, a larger m could be afforded for SoD than the others. [sent-219, score-0.082]
</p><p>40 The Hybrid method has the same hyperparameter learning time as SoD by deﬁnition, but the training phase will take longer than SoD with the same m, because of the need for a ﬁnal O(m2 n) phase of FITC training, as compared to the O(m3 ) for SoD. [sent-222, score-0.218]
</p><p>41 However, as per the argument above, we would expect the FITC predictions to be superior to the SoD ones, even if the hyperparameters have not been optimized explicitly for FITC prediction; this is explored experimentally in Section 4. [sent-223, score-0.126]
</p><p>42 At test time Table 1 shows that the SoD, FITC, Hybrid and Local approximations are O(m) for mean prediction, and O(m2 ) for predictive variances. [sent-225, score-0.117]
</p><p>43 The ‘best’ method could be the approximation with best predictions for a given computational cost, or alternatively the smallest computational cost for a given predictive performance. [sent-229, score-0.069]
</p><p>44 It should also be borne in mind that any error measure compresses the predictive mean and variance functions into a single number; for low-dimensional problems visualizing these functions can illustrate the differences between approximations (e. [sent-232, score-0.051]
</p><p>45 n It is rare that the appropriate hyperparameters are known for a given problem, unless it is a synthetic problem drawn from a GP. [sent-237, score-0.083]
</p><p>46 In terms of computational cost we use the CPU time in seconds, based on M ATLAB implementations of the algorithms (except for the IFGT where the Figtree C++ code is used with M ATLAB wrappers). [sent-240, score-0.051]
</p><p>47 Our SoD, FITC, Hybrid and Local GP implementations are all derived from the standard gpml toolbox of Rasmussen and Nickisch. [sent-242, score-0.057]
</p><p>48 , high sampling density); this provides a useful sanity check on the noise level returned during hyperparameter optimization. [sent-255, score-0.136]
</p><p>49 The choice of kernel function: Selecting an appropriate family of kernel functions is an important part of modelling a particular problem. [sent-256, score-0.05]
</p><p>50 (c) Does the method work efﬁciently for a wide range of hyperparameter settings? [sent-267, score-0.136]
</p><p>51 If not, hyperparameter searching must be performed much more carefully and one has to ask if the method will work well on good hyperparameter settings. [sent-268, score-0.272]
</p><p>52 The inputs were drawn from a N(0, I) Gaussian, and the function was drawn from a GP with zero mean and isotropic SE kernel with unit lengthscale. [sent-272, score-0.055]
</p><p>53 There are 30,543 training points and 30,544 test points in each data set. [sent-273, score-0.076]
</p><p>54 3 The input dimensionality is 15, and the data is split into 31,535 training cases and 31,536 test cases. [sent-277, score-0.076]
</p><p>55 It has 21 input dimensions, 44,484 training cases and 4,449 test cases (the split used by Rasmussen and Williams 2006). [sent-282, score-0.076]
</p><p>56 Error measures: We measured the accuracy of the methods’ predictions on the test sets using the Standardized Mean Squared Error (SMSE), and Mean Standardized Log Loss (MSLL), as deﬁned in (Rasmussen and Williams, 2006, Section 2. [sent-287, score-0.056]
</p><p>57 The SMSE is the mean squared error normalized by the MSE of the dumb predictor that always predicts the mean of the training set. [sent-289, score-0.086]
</p><p>58 The MSLL is obtained by averaging − log p(y∗ |D , x∗ ) over the test set and subtracting the same score for a trivial model which always predicts the mean and variance of the training set. [sent-290, score-0.076]
</p><p>59 4 compares predictions made with the learned hyperparameters and the generative hyperparameters on the synthetic data sets. [sent-309, score-0.216]
</p><p>60 The horizontal lines give test performance for SoD with 4,096, 8,192 and 16,384 training points. [sent-330, score-0.076]
</p><p>61 We have compared CG and DD for training a GP mean predictor based on 16,384 points from the SARCOS data, with the same ﬁxed hyperparameters used by Rasmussen and Williams (2006). [sent-338, score-0.152]
</p><p>62 Figure 1b) instead plots test-set SMSE, and adds reference lines for the SMSEs obtained by subsets with 4,096, 8,192 and 16,384 training points. [sent-344, score-0.062]
</p><p>63 Figure 1c) plots the SMSE against computer time taken on our machine. [sent-346, score-0.052]
</p><p>64 Figure 1d) shows the test-set SMSE progression against time for 16,384 points from SYNTH 8 using the true hyperparameters. [sent-349, score-0.053]
</p><p>65 The time per iteration was measured on a separate run that was not slowed down by storing the intermediate results required for these plots. [sent-356, score-0.053]
</p><p>66 We used the isotropic squared-exponential kernel (which has one lengthscale parameter shared over all dimensions). [sent-364, score-0.084]
</p><p>67 For each of the four data sets we randomly chose 5000 datapoints to construct a kernel matrix, and a 5000-element random vector (with elements sampled from U[0, 1]). [sent-365, score-0.06]
</p><p>68 The test times given below include computation of the predictive variances. [sent-374, score-0.055]
</p><p>69 In Figure 3 we plot the test set SMSE against hyperparameter training time (left column), and test time (right column) for the four methods on the four data sets. [sent-382, score-0.348]
</p><p>70 Further details including tables of learned hyperparameters are provided by Chalupka (2011), although the experiments were re-run for this paper, so there are some differences between the two. [sent-387, score-0.107]
</p><p>71 ) Looking at the hyperparameter training plots (left column), it is noticeable that SoD and FITC reduce monotonically with increasing time, and that SoD outperforms FITC on all data sets (i. [sent-393, score-0.198]
</p><p>72 On the test time plots (right column) the pattern between SoD and FITC is reversed, with FITC being superior. [sent-396, score-0.082]
</p><p>73 These results are consistent with theoretical scalings (Table 1): at training time FITC has worse scaling, at test time its scaling is the same,5 and it turns out that its more sophisticated approximation does give better results. [sent-397, score-0.166]
</p><p>74 At test time the Hybrid results are inferior to FITC for the same m as expected, but the faster hyperparameter learning time means that larger subset sizes can be used with Hybrid. [sent-399, score-0.238]
</p><p>75 FPC distributes the inducing points in a more regular fashion in the space, instead of having multiple close by in regions of high density. [sent-406, score-0.052]
</p><p>76 For Local, the joint estimation of hyperparameters was found to be significantly better than separate; this result makes sense as the target function is actually drawn from a single GP. [sent-407, score-0.083]
</p><p>77 For FITC and Hybrid the plots are cut off at m = 128 and m = 256 respectively, as numerical instabilities in the gpml FITC code for larger m values gave larger errors. [sent-408, score-0.086]
</p><p>78 Both SoD and FITC did slightly better when selecting the inducing points randomly. [sent-411, score-0.067]
</p><p>79 For the Local method, again joint estimation of hyperparameters was found to be superior, as for SYNTH 2. [sent-412, score-0.083]
</p><p>80 For both SYNTH 2 and SYNTH 8 we note that the lengthscales learned by the FITC approximation did not converge to the true values even for the largest m, while convergence was observed for SoD and Local; full details are available (Chalupka, 2011, Appendix 1). [sent-413, score-0.059]
</p><p>81 In fact, careful comparison of the test time plots show that FITC takes longer than SoD; this constant-factor performance difference is due to an implementation detail in gpml, which represents the FITC and SoD predictors differently, although they could be manipulated into the same form. [sent-415, score-0.082]
</p><p>82 343  C HALUPKA , W ILLIAMS AND M URRAY  CHEM : Both SoD and FITC did slightly better when selecting the inducing points randomly. [sent-416, score-0.067]
</p><p>83 Local with joint and separate hyperparameter training gave similar results. [sent-417, score-0.196]
</p><p>84 Local with joint hyperparameter training did better than separate training. [sent-420, score-0.182]
</p><p>85 4 Comparison with Prediction using the Generative Hyperparameters For the SYNTH 2 and SYNTH 8 data sets it is possible to compare the results with learned hyperparameters against those obtained with hyperparameters ﬁxed to the true generative values. [sent-422, score-0.19]
</p><p>86 We refer to these as the learned and ﬁxed hyperparameter settings. [sent-423, score-0.16]
</p><p>87 This may suggest that for FITC the hyperparameters that produce optimal performance may not be the generative ones. [sent-429, score-0.083]
</p><p>88 For example, Snelson and Ghahramani (2006) optimized the locations of the m inducing points, potentially improving test-time performance at the expense of a longer training time. [sent-439, score-0.098]
</p><p>89 A potential future area of research is working out how to intelligently balance the computer time spent on selecting and moving inducing points, while performing hyperparameter training, and choosing a subset size. [sent-440, score-0.239]
</p><p>90 2  2  −5  10 Hyperparameter training time [s]  −4  10 10 Test time per datapoint [s]  −3  10  CHEM SoD FITC Local Hybrid  1  0. [sent-454, score-0.201]
</p><p>91 2 0  10  2  −5  10 Hyperparameter training time [s]  −4  10 10 Test time per datapoint [s]  −3  10  SARCOS 0. [sent-462, score-0.201]
</p><p>92 02  0  10  2  10 Hyperparameter training time [s]  4  10  −5  −4  10 10 Test time per datapoint [s]  −3  10  Figure 3: SMSE (log scale) as a function of time (log scale) for the four data sets. [sent-472, score-0.254]
</p><p>93 Right: test time per test point (including variance computations, despite not being needed to report SMSE). [sent-474, score-0.113]
</p><p>94 345  C HALUPKA , W ILLIAMS AND M URRAY  SYNTH 2 0  0 SoD FITC Local Hybrid  −1  −2  −3  MSLL  MSLL  −2  SoD FITC Local Hybrid  −1  −4  −3 −4  −5  −5  −6  −6  −7  0  10  −7 −6 10  2  10 Hyperparameter training time [s]  −4  10 Test time per datapoint [s]  SYNTH 8 1  1 SoD FITC Local Hybrid  0. [sent-476, score-0.201]
</p><p>95 5  −2  2  10 Hyperparameter training time [s]  4  10  −2. [sent-485, score-0.082]
</p><p>96 5  −5  −4  10 10 Test time per datapoint [s]  −3  10  Figure 4: MSLL as a function of time (log scale) for the four data sets. [sent-486, score-0.172]
</p><p>97 We believe that future evaluations of GP approximations should consider these factors (Section 3), and compare error-time curves with standard approximations such as SoD and FITC. [sent-491, score-0.052]
</p><p>98 On the data sets we considered, SoD and Hybrid dominate FITC in terms of hyperparameter learning. [sent-495, score-0.136]
</p><p>99 Assuming that hyperparameter learning is the dominant factor in computation time, the results presented above point to the very simple Subset of Data method (or the Hybrid variant) as being the leading contender. [sent-501, score-0.136]
</p><p>100 Variational learning of inducing variables in sparse Gaussian processes. [sent-781, score-0.052]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sod', 0.573), ('fitc', 0.505), ('gpr', 0.282), ('synth', 0.224), ('smse', 0.199), ('ifgt', 0.158), ('hyperparameter', 0.136), ('mvm', 0.133), ('hybrid', 0.121), ('msll', 0.116), ('sarcos', 0.1), ('cg', 0.095), ('hyperparameters', 0.083), ('fpc', 0.083), ('gp', 0.078), ('rasmussen', 0.077), ('halupka', 0.075), ('illiams', 0.075), ('urray', 0.075), ('qui', 0.07), ('atlab', 0.066), ('datapoint', 0.066), ('valuating', 0.066), ('inducing', 0.052), ('mvms', 0.05), ('pproximation', 0.047), ('training', 0.046), ('lengthscale', 0.044), ('chalupka', 0.042), ('chem', 0.042), ('gpml', 0.041), ('williams', 0.039), ('snelson', 0.038), ('dd', 0.038), ('local', 0.037), ('iterative', 0.036), ('time', 0.036), ('auto', 0.036), ('ethods', 0.035), ('duraiswami', 0.033), ('raykar', 0.033), ('test', 0.03), ('rpc', 0.028), ('cluster', 0.027), ('approximations', 0.026), ('predictions', 0.026), ('kernel', 0.025), ('predictive', 0.025), ('kfitc', 0.025), ('ksor', 0.025), ('morariu', 0.025), ('learned', 0.024), ('predictor', 0.023), ('gaussian', 0.023), ('freitas', 0.022), ('editors', 0.022), ('weiss', 0.02), ('edinburgh', 0.019), ('approximation', 0.018), ('shen', 0.018), ('ard', 0.018), ('datapoints', 0.018), ('ghz', 0.018), ('murray', 0.018), ('powers', 0.018), ('four', 0.017), ('per', 0.017), ('bromide', 0.017), ('carrington', 0.017), ('dumb', 0.017), ('figtree', 0.017), ('fritz', 0.017), ('iain', 0.017), ('lengthscales', 0.017), ('liberty', 0.017), ('malshe', 0.017), ('manzhos', 0.017), ('progression', 0.017), ('raff', 0.017), ('smses', 0.017), ('vinyl', 0.017), ('wikle', 0.017), ('wrt', 0.017), ('gauss', 0.017), ('plots', 0.016), ('platt', 0.016), ('toolbox', 0.016), ('fast', 0.016), ('clusters', 0.016), ('process', 0.016), ('computations', 0.015), ('inputs', 0.015), ('code', 0.015), ('isotropic', 0.015), ('selecting', 0.015), ('bottou', 0.014), ('lawrence', 0.014), ('gave', 0.014), ('diagnose', 0.014), ('gonzales', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="3-tfidf-1" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>Author: Krzysztof Chalupka, Christopher K. I. Williams, Iain Murray</p><p>Abstract: Gaussian process (GP) predictors are an important component of many Bayesian approaches to machine learning. However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n2 ) space and O(n3 ) time for a data set of n examples. Several approximation methods have been proposed, but there is a lack of understanding of the relative merits of the different approximations, and in what situations they are most useful. We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e.g., Subset of Data and FITC). We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons. Keywords: Gaussian process regression, subset of data, FITC, local GP</p><p>2 0.077957898 <a title="3-tfidf-2" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>3 0.045662507 <a title="3-tfidf-3" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>4 0.038049702 <a title="3-tfidf-4" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>Author: Matthew J. Urry, Peter Sollich</p><p>Abstract: We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to signiﬁcant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is signiﬁcantly more accurate than previous approximations and should become exact in the limit of large random graphs. Keywords: Gaussian process, generalisation error, learning curve, cavity method, belief propagation, graph, random walk kernel</p><p>5 0.034916166 <a title="3-tfidf-5" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>6 0.024952902 <a title="3-tfidf-6" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>7 0.022002963 <a title="3-tfidf-7" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>8 0.020965768 <a title="3-tfidf-8" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>9 0.019967981 <a title="3-tfidf-9" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>10 0.018964643 <a title="3-tfidf-10" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>11 0.018546743 <a title="3-tfidf-11" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>12 0.017850626 <a title="3-tfidf-12" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>13 0.017832566 <a title="3-tfidf-13" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>14 0.017800644 <a title="3-tfidf-14" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>15 0.017542085 <a title="3-tfidf-15" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>16 0.017118583 <a title="3-tfidf-16" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>17 0.017104557 <a title="3-tfidf-17" href="./jmlr-2013-Variational_Inference_in_Nonconjugate_Models.html">121 jmlr-2013-Variational Inference in Nonconjugate Models</a></p>
<p>18 0.016916808 <a title="3-tfidf-18" href="./jmlr-2013-Bayesian_Nonparametric_Hidden_Semi-Markov_Models.html">16 jmlr-2013-Bayesian Nonparametric Hidden Semi-Markov Models</a></p>
<p>19 0.016112631 <a title="3-tfidf-19" href="./jmlr-2013-BudgetedSVM%3A_A_Toolbox_for_Scalable_SVM_Approximations.html">19 jmlr-2013-BudgetedSVM: A Toolbox for Scalable SVM Approximations</a></p>
<p>20 0.015736209 <a title="3-tfidf-20" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.093), (1, -0.074), (2, 0.005), (3, -0.028), (4, 0.037), (5, -0.003), (6, -0.085), (7, -0.018), (8, -0.044), (9, -0.029), (10, 0.047), (11, -0.012), (12, 0.02), (13, -0.032), (14, -0.014), (15, 0.016), (16, 0.03), (17, -0.038), (18, -0.038), (19, 0.069), (20, -0.01), (21, -0.043), (22, -0.043), (23, -0.148), (24, -0.019), (25, -0.063), (26, -0.049), (27, 0.012), (28, -0.004), (29, 0.097), (30, -0.006), (31, 0.09), (32, 0.136), (33, 0.311), (34, -0.033), (35, 0.225), (36, -0.05), (37, -0.104), (38, -0.241), (39, -0.084), (40, -0.137), (41, 0.036), (42, -0.172), (43, 0.098), (44, 0.251), (45, 0.232), (46, -0.122), (47, -0.083), (48, -0.231), (49, -0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93578839 <a title="3-lsi-1" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>Author: Krzysztof Chalupka, Christopher K. I. Williams, Iain Murray</p><p>Abstract: Gaussian process (GP) predictors are an important component of many Bayesian approaches to machine learning. However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n2 ) space and O(n3 ) time for a data set of n examples. Several approximation methods have been proposed, but there is a lack of understanding of the relative merits of the different approximations, and in what situations they are most useful. We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e.g., Subset of Data and FITC). We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons. Keywords: Gaussian process regression, subset of data, FITC, local GP</p><p>2 0.57594514 <a title="3-lsi-2" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>3 0.32667139 <a title="3-lsi-3" href="./jmlr-2013-GURLS%3A_A_Least_Squares_Library_for_Supervised_Learning.html">46 jmlr-2013-GURLS: A Least Squares Library for Supervised Learning</a></p>
<p>Author: Andrea Tacchetti, Pavan K. Mallapragada, Matteo Santoro, Lorenzo Rosasco</p><p>Abstract: We present GURLS, a least squares, modular, easy-to-extend software library for efﬁcient supervised learning. GURLS is targeted to machine learning practitioners, as well as non-specialists. It offers a number state-of-the-art training strategies for medium and large-scale learning, and routines for efﬁcient model selection. The library is particularly well suited for multi-output problems (multi-category/multi-label). GURLS is currently available in two independent implementations: Matlab and C++. It takes advantage of the favorable properties of regularized least squares algorithm to exploit advanced tools in linear algebra. Routines to handle computations with very large matrices by means of memory-mapped storage and distributed task execution are available. The package is distributed under the BSD license and is available for download at https://github.com/LCSL/GURLS. Keywords: regularized least squares, big data, linear algebra 1. Introduction and Design Supervised learning has become a fundamental tool for the design of intelligent systems and the analysis of high dimensional data. Key to this success has been the availability of efﬁcient, easy-touse software packages. New data collection technologies make it easy to gather high dimensional, multi-output data sets of increasing size. This trend calls for new software solutions for the automatic training, tuning and testing of supervised learning methods. These observations motivated the design of GURLS (Grand Uniﬁed Regularized Least Squares). The package was developed to pursue the following goals: Speed: Fast training/testing procedures for learning problems with potentially large/huge number of points, features and especially outputs (e.g., classes). Memory: Flexible data management to work with large data sets by means of memory-mapped storage. Performance: ∗. Also in the Laboratory for Computational and Statistical Learning, Istituto Italiano di Tecnologia and Massachusetts Institute of Technology c 2013 Andrea Tacchetti, Pavan K. Mallapragada, Matteo Santoro and Lorenzo Rosasco. TACCHETTI , M ALLAPRAGADA , S ANTORO AND ROSASCO State of the art results in high-dimensional multi-output problems. Usability and modularity: Easy to use and to expand. GURLS is based on Regularized Least Squares (RLS) and takes advantage of all the favorable properties of these methods (Rifkin et al., 2003). Since the algorithm reduces to solving a linear system, GURLS is set up to exploit the powerful tools, and recent advances, of linear algebra (including randomized solver, ﬁrst order methods, etc.). Second, it makes use of RLS properties which are particularly suited for high dimensional learning. For example: (1) RLS has natural primal and dual formulation (hence having complexity which is the smallest between number of examples and features); (2) efﬁcient parameter selection (closed form expression of the leave one out error and efﬁcient computations of regularization path); (3) natural and efﬁcient extension to multiple outputs. Speciﬁc attention has been devoted to handle large high dimensional data sets. We rely on data structures that can be serialized using memory-mapped ﬁles, and on a distributed task manager to perform a number of key steps (such as matrix multiplication) without loading the whole data set in memory. Efforts were devoted to to provide a lean API and an exhaustive documentation. GURLS has been deployed and tested successfully on Linux, MacOS and Windows. The library is distributed under the simpliﬁed BSD license, and can be downloaded from https://github.com/LCSL/GURLS. 2. Description of the Library The library comprises four main modules. GURLS and bGURLS—both implemented in Matlab— are aimed at solving learning problems with small/medium and large-scale data sets respectively. GURLS++ and bGURLS++ are their C++ counterparts. The Matlab and C++ versions share the same design, but the C++ modules have signiﬁcant improvements, which make them faster and more ﬂexible. The speciﬁcation of the desired machine learning experiment in the library is straightforward. Basically, it is a formal description of a pipeline, that is, an ordered sequence of steps. Each step identiﬁes an actual learning task, and belongs to a predeﬁned category. The core of the library is a method (a class in the C++ implementation) called GURLScore, which is responsible for processing the sequence of tasks in the proper order and for linking the output of the former task to the input of the subsequent one. A key role is played by the additional “options” structure, referred to as OPT. OPT is used to store all conﬁguration parameters required to customize the behavior of individual tasks in the pipeline. Tasks receive conﬁguration parameters from OPT in read-only mode and—upon termination—the results are appended to the structure by GURLScore in order to make them available to subsequent tasks. This allows the user to skip the execution of some tasks in a pipeline, by simply inserting the desired results directly into the options structure. Currently, we identify six different task categories: data set splitting, kernel computation, model selection, training, evaluation and testing and performance assessment and analysis. Tasks belonging to the same category may be interchanged with each other. 2.1 Learning From Large Data Sets Two modules in GURLS have been speciﬁcally designed to deal with big data scenarios. The approach we adopted is mainly based on a memory-mapped abstraction of matrix and vector data structures, and on a distributed computation of a number of standard problems in linear algebra. For learning on big data, we decided to focus speciﬁcally on those situations where one seeks a linear model on a large set of (possibly non linear) features. A more accurate speciﬁcation of what “large” means in GURLS is related to the number of features d and the number of training 3202 GURLS: A L EAST S QUARES L IBRARY FOR S UPERVISED L EARNING data set optdigit landast pendigit letter isolet # of samples 3800 4400 7400 10000 6200 # of classes 10 6 10 26 26 # of variables 64 36 16 16 600 Table 1: Data sets description. examples n: we require it must be possible to store a min(d, n) × min(d, n) matrix in memory. In practice, this roughly means we can train models with up-to 25k features on machines with 8Gb of RAM, and up-to 50k features on machines with 36Gb of RAM. We do not require the data matrix itself to be stored in memory: within GURLS it is possible to manage an arbitrarily large set of training examples. We distinguish two different scenarios. Data sets that can fully reside in RAM without any memory mapping techniques—such as swapping—are considered to be small/medium. Larger data sets are considered to be “big” and learning must be performed using either bGURLS or bGURLS++ . These two modules include all the design patterns described above, and have been complemented with additional big data and distributed computation capabilities. Big data support is obtained using a data structure called bigarray, which allows to handle data matrices as large as the space available on the hard drive: we store the entire data set on disk and load only small chunks in memory when required. There are some differences between the Matlab and C++ implementations. bGURLS relies on a simple, ad hoc interface, called GURLS Distributed Manager (GDM), to distribute matrix-matrix multiplications, thus allowing users to perform the important task of kernel matrix computation on a distributed network of computing nodes. After this step, the subsequent tasks behave as in GURLS. bGURLS++ (currently in active development) offers more interesting features because it is based on the MPI libraries. Therefore, it allows for a full distribution within every single task of the pipeline. All the processes read the input data from a shared ﬁlesystem over the network and then start executing the same pipeline. During execution, each process’ task communicates with the corresponding ones. Every process maintains its local copy of the options. Once the same task is completed by all processes, the local copies of the options are synchronized. This architecture allows for the creation of hybrid pipelines comprising serial one-process-based tasks from GURLS++ . 3. Experiments We decided to focus the experimental analysis in the paper to the assessment of GURLS’ performance both in terms of accuracy and time. In our experiments we considered 5 popular data sets, brieﬂy described in Table 1. Experiments were run on a Intel Xeon 5140 @ 2.33GHz processor with 8GB of RAM, and running Ubuntu 8.10 Server (64 bit). optdigit accuracy (%) GURLS (linear primal) GURLS (linear dual) LS-SVM linear GURLS (500 random features) GURLS (1000 random features) GURLS (Gaussian kernel) LS-SVM (Gaussian kernel) time (s) landsat accuracy (%) time (s) pendigit accuracy (%) time (s) 92.3 92.3 92.3 96.8 97.5 98.3 98.3 0.49 726 7190 25.6 207 13500 26100 63.68 66.3 64.6 63.5 63.5 90.4 90.51 0.22 1148 6526 28.0 187 20796 18430 82.24 82.46 82.3 96.7 95.8 98.4 98.36 0.23 5590 46240 31.6 199 100600 120170 Table 2: Comparison between GURLS and LS-SVM. 3203 TACCHETTI , M ALLAPRAGADA , S ANTORO AND ROSASCO Performance (%) 1 0.95 0.9 0.85 isolet(∇) letter(×) 0.8 pendigit(∆) 0.75 landsat(♦) optdigit(◦) 0.7 LIBSVM:rbf 0.65 GURLS++:rbf GURLS:randomfeatures-1000 0.6 GURLS:randomfeatures-500 0.55 0.5 0 10 GURLS:rbf 1 10 2 10 3 10 4 Time (s) 10 Figure 1: Prediction accuracy vs. computing time. The color represents the training method and the library used. In blue: the Matlab implementation of RLS with RBF kernel, in red: its C++ counterpart. In dark red: results of LIBSVM with RBF kernel. In yellow and green: results obtained using a linear kernel on 500 and 1000 random features respectively. We set up different pipelines and compared the performance to SVM, for which we used the python modular interface to LIBSVM (Chang and Lin, 2011). Automatic selection of the optimal regularization parameter is implemented identically in all experiments: (i) split the data; (ii) deﬁne a set of regularization parameter on a regular grid; (iii) perform hold-out validation. The variance of the Gaussian kernel has been ﬁxed by looking at the statistics of the pairwise distances among training examples. The prediction accuracy of GURLS and GURLS++ is identical—as expected—but the implementation in C++ is signiﬁcantly faster. The prediction accuracy of standard RLS-based methods is in many cases higher than SVM. Exploiting the primal formulation of RLS, we further ran experiments with the random features approximation (Rahimi and Recht, 2008). As show in Figure 1, the performance of this method is comparable to that of SVM at a much lower computational cost in the majority of the tested data sets. We further compared GURLS with another available least squares based toolbox: the LS-SVM toolbox (Suykens et al., 2001), which includes routines for parameter selection such as coupled simulated annealing and line/grid search. The goal of this experiment is to benchmark the performance of the parameter selection with random data splitting included in GURLS. For a fair comparison, we considered only the Matlab implementation of GURLS. Results are reported in Table 2. As expected, using the linear kernel with the primal formulation—not available in LS-SVM—is the fastest approach since it leverages the lower dimensionality of the input space. When the Gaussian kernel is used, GURLS and LS-SVM have comparable computing time and classiﬁcation performance. Note, however, that in GURLS the number of parameter in the grid search is ﬁxed to 400, while in LS-SVM it may vary and is limited to 70. The interesting results obtained with the random features implementation in GURLS, make it an interesting choice in many applications. Finally, all GURLS pipelines, in their Matlab implementation, are faster than LS-SVM and further improvements can be achieved with GURLS++ . Acknowledgments We thank Tomaso Poggio, Zak Stone, Nicolas Pinto, Hristo S. Paskov and CBCL for comments and insights. 3204 GURLS: A L EAST S QUARES L IBRARY FOR S UPERVISED L EARNING References C.-C. Chang and C.-J. Lin. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http://www. csie.ntu.edu.tw/˜cjlin/libsvm. A. Rahimi and B. Recht. Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Advances in Neural Information Processing Systems, volume 21, pages 1313–1320, 2008. R. Rifkin, G. Yeo, and T. Poggio. Regularized least-squares classiﬁcation. Nato Science Series Sub Series III Computer and Systems Sciences, 190:131–154, 2003. J. Suykens, T. V. Gestel, J. D. Brabanter, B. D. Moor, and J. Vandewalle. Least Squares Support Vector Machines. World Scientiﬁc, 2001. ISBN 981-238-151-1. 3205</p><p>4 0.29469609 <a title="3-lsi-4" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>Author: Matthew J. Urry, Peter Sollich</p><p>Abstract: We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to signiﬁcant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is signiﬁcantly more accurate than previous approximations and should become exact in the limit of large random graphs. Keywords: Gaussian process, generalisation error, learning curve, cavity method, belief propagation, graph, random walk kernel</p><p>5 0.23105986 <a title="3-lsi-5" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>6 0.21891749 <a title="3-lsi-6" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<p>7 0.15959266 <a title="3-lsi-7" href="./jmlr-2013-BudgetedSVM%3A_A_Toolbox_for_Scalable_SVM_Approximations.html">19 jmlr-2013-BudgetedSVM: A Toolbox for Scalable SVM Approximations</a></p>
<p>8 0.15349166 <a title="3-lsi-8" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>9 0.14311714 <a title="3-lsi-9" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>10 0.14270918 <a title="3-lsi-10" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>11 0.14202119 <a title="3-lsi-11" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>12 0.13798276 <a title="3-lsi-12" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>13 0.13714302 <a title="3-lsi-13" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>14 0.13207889 <a title="3-lsi-14" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>15 0.13046028 <a title="3-lsi-15" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>16 0.12721989 <a title="3-lsi-16" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>17 0.12277826 <a title="3-lsi-17" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>18 0.12180571 <a title="3-lsi-18" href="./jmlr-2013-Similarity-based_Clustering_by_Left-Stochastic_Matrix_Factorization.html">100 jmlr-2013-Similarity-based Clustering by Left-Stochastic Matrix Factorization</a></p>
<p>19 0.12090656 <a title="3-lsi-19" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>20 0.10925105 <a title="3-lsi-20" href="./jmlr-2013-Truncated_Power_Method_for_Sparse_Eigenvalue_Problems.html">116 jmlr-2013-Truncated Power Method for Sparse Eigenvalue Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.037), (5, 0.108), (6, 0.036), (10, 0.082), (20, 0.016), (23, 0.024), (36, 0.374), (44, 0.011), (68, 0.018), (70, 0.021), (75, 0.101), (87, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70115286 <a title="3-lda-1" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>Author: Krzysztof Chalupka, Christopher K. I. Williams, Iain Murray</p><p>Abstract: Gaussian process (GP) predictors are an important component of many Bayesian approaches to machine learning. However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n2 ) space and O(n3 ) time for a data set of n examples. Several approximation methods have been proposed, but there is a lack of understanding of the relative merits of the different approximations, and in what situations they are most useful. We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e.g., Subset of Data and FITC). We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons. Keywords: Gaussian process regression, subset of data, FITC, local GP</p><p>2 0.41981784 <a title="3-lda-2" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>Author: Wei Pan, Xiaotong Shen, Binghui Liu</p><p>Abstract: Clustering analysis is widely used in many ﬁelds. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classiﬁcation and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classiﬁcation and regression, such as model selection criteria to select the number of clusters, a difﬁcult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method’s promising performance. Keywords: generalized degrees of freedom, grouping, K-means clustering, Lasso, penalized regression, truncated Lasso penalty (TLP)</p><p>3 0.41337398 <a title="3-lda-3" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>Author: Philémon Brakel, Dirk Stroobandt, Benjamin Schrauwen</p><p>Abstract: Imputing missing values in high dimensional time-series is a difﬁcult problem. This paper presents a strategy for training energy-based graphical models for imputation directly, bypassing difﬁculties probabilistic approaches would face. The training strategy is inspired by recent work on optimization-based learning (Domke, 2012) and allows complex neural models with convolutional and recurrent structures to be trained for imputation tasks. In this work, we use this training strategy to derive learning rules for three substantially different neural architectures. Inference in these models is done by either truncated gradient descent or variational mean-ﬁeld iterations. In our experiments, we found that the training methods outperform the Contrastive Divergence learning algorithm. Moreover, the training methods can easily handle missing values in the training data itself during learning. We demonstrate the performance of this learning scheme and the three models we introduce on one artiﬁcial and two real-world data sets. Keywords: neural networks, energy-based models, time-series, missing values, optimization</p><p>4 0.40870637 <a title="3-lda-4" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>5 0.40301946 <a title="3-lda-5" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>Author: Ran Gilad-Bachrach, Christopher J.C. Burges</p><p>Abstract: Typically, one approaches a supervised machine learning problem by writing down an objective function and ﬁnding a hypothesis that minimizes it. This is equivalent to ﬁnding the Maximum A Posteriori (MAP) hypothesis for a Boltzmann distribution. However, MAP is not a robust statistic. We present an alternative approach by deﬁning a median of the distribution, which we show is both more robust, and has good generalization guarantees. We present algorithms to approximate this median. One contribution of this work is an efﬁcient method for approximating the Tukey median. The Tukey median, which is often used for data visualization and outlier detection, is a special case of the family of medians we deﬁne: however, computing it exactly is exponentially slow in the dimension. Our algorithm approximates such medians in polynomial time while making weaker assumptions than those required by previous work. Keywords: classiﬁcation, estimation, median, Tukey depth</p><p>6 0.39492425 <a title="3-lda-6" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>7 0.39110947 <a title="3-lda-7" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>8 0.38929254 <a title="3-lda-8" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>9 0.38864538 <a title="3-lda-9" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>10 0.38792634 <a title="3-lda-10" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>11 0.38783935 <a title="3-lda-11" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>12 0.38687479 <a title="3-lda-12" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>13 0.38569939 <a title="3-lda-13" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>14 0.38283086 <a title="3-lda-14" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>15 0.37775281 <a title="3-lda-15" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>16 0.37767458 <a title="3-lda-16" href="./jmlr-2013-GURLS%3A_A_Least_Squares_Library_for_Supervised_Learning.html">46 jmlr-2013-GURLS: A Least Squares Library for Supervised Learning</a></p>
<p>17 0.37760481 <a title="3-lda-17" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>18 0.37574688 <a title="3-lda-18" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>19 0.37560457 <a title="3-lda-19" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>20 0.37530324 <a title="3-lda-20" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
