<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-64" href="#">jmlr2013-64</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</h1>
<br/><p>Source: <a title="jmlr-2013-64-pdf" href="http://jmlr.org/papers/volume14/jethava13a/jethava13a.pdf">pdf</a></p><p>Author: Vinay Jethava, Anders Martinsson, Chiranjib Bhattacharyya, Devdatt Dubhashi</p><p>Abstract: In this paper we establish that the Lov´ sz ϑ function on a graph can be restated as a kernel learning a problem. We introduce the notion of SVM − ϑ graphs, on which Lov´ sz ϑ function can be apa proximated well by a Support vector machine (SVM). We show that Erd¨ s-R´ nyi random G(n, p) o e log4 n np graphs are SVM − ϑ graphs for n ≤ p < 1. Even if we embed a large clique of size Θ 1−p in a G(n, p) graph the resultant graph still remains a SVM − ϑ graph. This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. Associated with the ϑ function is the notion of orthogonal labellings. We introduce common orthogonal labellings which extends the idea of orthogonal labellings to multiple graphs. This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude. Keywords: orthogonal labellings of graphs, planted cliques, random graphs, common dense subgraph</p><p>Reference: <a title="jmlr-2013-64-reference" href="../jmlr2013_reference/jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We show that Erd¨ s-R´ nyi random G(n, p) o e log4 n np graphs are SVM − ϑ graphs for n ≤ p < 1. [sent-10, score-0.56]
</p><p>2 Even if we embed a large clique of size Θ 1−p  in a G(n, p) graph the resultant graph still remains a SVM − ϑ graph. [sent-11, score-0.641]
</p><p>3 This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. [sent-12, score-0.61]
</p><p>4 This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. [sent-15, score-0.48]
</p><p>5 Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude. [sent-16, score-0.953]
</p><p>6 Keywords: orthogonal labellings of graphs, planted cliques, random graphs, common dense subgraph  1. [sent-17, score-0.965]
</p><p>7 Introduction In a general graph many problems, such as computing the size of the largest clique or determining the chromatic number, are NP hard (Garey and Johnson, 1979). [sent-18, score-0.455]
</p><p>8 Consider the problem of ﬁnding a large planted clique in a random graph. [sent-26, score-0.61]
</p><p>9 In particular we study the problem of ﬁnding a common dense subgraph in multiple graphs (Pardalos and Rebennack, 2010), a computationally challenging problem for large graphs. [sent-34, score-0.689]
</p><p>10 We also study the problem of ﬁnding a hidden planted clique in a random graph. [sent-35, score-0.636]
</p><p>11 Many problems in social network analysis can also be posed as dense subgraph discovery problem (Newman et al. [sent-46, score-0.44]
</p><p>12 In this paper we target two difﬁcult versions of dense subgraph recovery problem. [sent-54, score-0.44]
</p><p>13 The problem of planted clique in a random graph is an instance of dense subgraph discovery in a random graph. [sent-55, score-1.236]
</p><p>14 To cite an example recently the problem of correlation detection was formulated as that of ﬁnding a planted clique in a large random graph (Devroye et al. [sent-58, score-0.796]
</p><p>15 , Podolyan and Karypis, 2009), we study the problem of ﬁnding a large common dense subgraph in multiple graphs. [sent-62, score-0.48]
</p><p>16 One of the main contribution of this paper is to show how the connection of ϑ to SVMs can be exploited to ﬁnd a dense subgraph in multiple graphs. [sent-70, score-0.44]
</p><p>17 We extend the idea of orthogonal labelling to multiple graphs by introducing the notion of common orthogonal labelling. [sent-71, score-0.495]
</p><p>18 An extremely difﬁcult instance of dense subgraph recovery problem is to pose the question of ﬁnding a hidden clique in a random graph. [sent-79, score-0.735]
</p><p>19 Another key contribution of this paper is show that one can ﬁnd a planted clique by solving an SVM. [sent-81, score-0.61]
</p><p>20 In particular, we show that in a G(n, 1 − p) graph even if we embed a clique of size k = Θ n(1 − p)/p , the resultant graph is a SVM − ϑ graph. [sent-82, score-0.641]
</p><p>21 Furthermore, even if embed a sparse random subgraph in a large random graph, the resultant graph turns out to be SVM − ϑ graph. [sent-83, score-0.489]
</p><p>22 Furthermore we show that the the graph associated with the planted clique problem also satisﬁes this property. [sent-93, score-0.796]
</p><p>23 Let GS = (S, ES ) denote 2 ¯ the subgraph induced by S ⊆ V in graph G. [sent-141, score-0.538]
</p><p>24 The study of this problem was initiated by Lov´ sz (1979) who introduced the idea of orthogonal labelling: a Deﬁnition 1 (Lov´ sz, 1979) An orthogonal labelling of graph G = (V, E) with |V | = n, is a matrix a U = [u1 , . [sent-151, score-0.648]
</p><p>25 Deﬁnition 4 (Luz, 1995) A graph G, with adjacency matrix A, is called a Q graph whenever ALPHA(G) = v(G, A) where v(G, A) is deﬁned in Theorem 2. [sent-215, score-0.524]
</p><p>26 In Section 4 we will use this characterization to show that random graphs with planted cliques are not Q graphs. [sent-224, score-0.6]
</p><p>27 Before we discuss the applicability of the results obtained here to random graphs we study the interesting problem of ﬁnding a dense common subgraph in multiple graphs. [sent-245, score-0.689]
</p><p>28 Finding Large Dense Regions in Multiple Graphs The problem of ﬁnding a dense subgraph in a single graph is a computationally challenging problem (Pardalos and Rebennack, 2010). [sent-247, score-0.626]
</p><p>29 An interesting method was proposed by Jiang and Pei (2009), which uses an enumerative strategy for ﬁnding a common dense subgraph in multiple graphs. [sent-251, score-0.512]
</p><p>30 Find a common subgraph which is dense in all the graphs. [sent-287, score-0.48]
</p><p>31 2 presents our algorithm for recovery of large common dense subgraph from multiple graphs. [sent-293, score-0.48]
</p><p>32 1 C OMMON O RTHOGONAL L ABELLING AND MKL F ORMULATION We begin by deﬁning common orthogonal labelling below: Deﬁnition 9 Given set of simple undirected graphs G on a common vertex set V = {1, . [sent-296, score-0.519]
</p><p>33 Let ϒ(G) denote the size of the maximum common independent set, that is, subset of vertices CS ⊆ V (l) of maximum possible cardinality for which the subgraph GCS induced by CS in graph G(l) is an independent set for all G(l) ∈ G. [sent-309, score-0.619]
</p><p>34 However, the relationship between minimum eigenvalue ρ(l) = −λn (A(l) ) of original graphs G(l) ∈ G, and minimum eigenvalue ρ∪ = −λn (A∪ ) of union graph G∪ is not clear, that is, ρ∪ can be greater or less than ρ(l) (see, e. [sent-365, score-0.471]
</p><p>35 2 S UBGRAPH D ETECTION BASED  ON  M ULTIPLE G RAPHS  In the remainder of this section, we relate the optimal solution (support vectors) of ω(K) and the density of related induced subgraph for the single graph case; which we later extend to multiple graphs. [sent-388, score-0.586]
</p><p>36 We extend this notion to general graphs by relating the density of the induced subgraph obtained by choosing vertices having “high” support through the KKT conditions. [sent-394, score-0.65]
</p><p>37 i ¯i Let α∗ (S) denote the average of the support vectors α∗ over the neighbourhood Ni (GS ) of node j ¯i ¯S i in subgraph GS induced by S ⊆ V in graph G; and α∗ be the minimum α∗ (S) over all i ∈ S, that is, ¯i α∗ (S) =  ∑ j∈S Ai j α∗ i , di (GS )  and  3506  ¯S ¯i α∗ = min α∗ (S). [sent-397, score-0.592]
</p><p>38 This result provides an upper bound on the density γ(GSc ) of the subgraph induced by set Sc in graph G for general c. [sent-405, score-0.586]
</p><p>39 γ(GSV ) ≤ ∗ ¯ αSV (nSV − 1) This provides a simple procedure for ﬁnding a sparse subgraph by selection the subgraph GSV induced by the set of non-zero support vectors SV . [sent-408, score-0.655]
</p><p>40 We now consider the problem of common dense subgraph recovery from multiple graphs based on the MKL formulation in (7). [sent-412, score-0.689]
</p><p>41 The set T with cardinality nT = |T | induces a subgraph (l) (l) GT in graph G(l) ∈ G having density at most γ(GT ) given by (l)  γ(GT ) ≤  (1 − αmin )ρ(l) ¯ (l) αSV (nT − 1)  ∀ G(l) ∈ G. [sent-424, score-0.537]
</p><p>42 The above result allows us to build a parameter-less common sparse subgraph (CSS) algorithm shown in Algorithm 1 having following advantages: it provides a theoretical bound on subgraph density; and, it requires no parameters from the user beyond the set of graphs G. [sent-435, score-0.855]
</p><p>43 However, if nT is very large, that is, nT /N ≃ 1, the density of the induced subgraph is close to the average graph density. [sent-438, score-0.586]
</p><p>44 More generally, one might be interested in a trade-off between the subgraph (l) size nT and subgraph density γ(GT ). [sent-439, score-0.654]
</p><p>45 Analogous to the simple graph case, we can improve the subgraph density is obtained by choosing smaller region nodes Tc := {i ∈ T : α∗ > c} ⊆ T . [sent-440, score-0.537]
</p><p>46 Subsequently we show that G(n, p) graphs and G(n, p) graphs with planted cliques are SVM − ϑ graphs. [sent-453, score-0.809]
</p><p>47 This results immediately show that one can identify planted cliques or planted subgraphs. [sent-454, score-0.732]
</p><p>48 Deﬁnition 16 A family of graphs G = {G = (V, E)} is said to be SVM − ϑ graph family if there exist a constant γ, such that for any graph G ∈ G with |V | ≥ n0 , the following holds: ω(K) ≤ γϑ(G), where ω(K) is deﬁned in (1) and K is deﬁned on G by (2). [sent-456, score-0.581]
</p><p>49 We will demonstrate examples of such families of random graphs: the Erd¨ s–R´ nyi random graph G(n, p) o e and a planted variation. [sent-459, score-0.582]
</p><p>50 This property can be very useful in detecting planted cliques in dense graphs. [sent-509, score-0.528]
</p><p>51 Deﬁnition 20 (Hidden Planted Clique) A random G(n, q) graph is chosen ﬁrst and then a clique of size k is introduced in the ﬁrst 1, . [sent-514, score-0.455]
</p><p>52 In this paper we establish that for large planted 2 ¯ clique G(n, 1− p, k) is indeed a SVM− ϑ graph. [sent-538, score-0.61]
</p><p>53 We study the planted clique problem where k is in the above regime. [sent-548, score-0.61]
</p><p>54 This motivates Algorithm 2 for ﬁnding planted clique in a random graph. [sent-611, score-0.61]
</p><p>55 3 Finding Planted Subgraphs in Random Graphs The above results show that the SVM procedure can recover a planted independent set in a sparse random graph, which is later exploited to solve the planted clique problem in a dense graph. [sent-620, score-1.088]
</p><p>56 Let G(n, p, p′ , k) be a graph which has a planted G(k, p′ ) graph on the ﬁrst k vertices of a random G(n, p) graph. [sent-623, score-0.754]
</p><p>57 Indeed one can show that the graph with a  planted subgraph is also a SVM − ϑ graph. [sent-628, score-0.83]
</p><p>58 As in the planted clique case the Algorithm 2 recovers the planted subgraph. [sent-646, score-0.951]
</p><p>59 For the planted clique case one needs to consider d = − random variables. [sent-682, score-0.61]
</p><p>60 Experimental Evaluation In Section 3 an algorithm was proposed which was capable of discovering a large common dense subgraph in a collection of graphs. [sent-690, score-0.48]
</p><p>61 2 an algorithm for discovering a large planted clique in a single graph was discussed. [sent-692, score-0.796]
</p><p>62 We also investigate a thresholding heuristic which improves induced subgraph density at the cost of subgraph size. [sent-703, score-0.703]
</p><p>63 We evaluate the algorithm on following class of graphs c-fat graphs which are based on fault diagnosis problems and are relatively sparse; and, p hat graphs which are generalizations of Erd¨ so R´ nyi random graphs; and are characerized by wider degree spread compared to classical G(n, p) e 2. [sent-709, score-0.689]
</p><p>64 For the families of dense graphs (brock, san, sanr), we focus on ﬁnding large dense region in the complement of the original graphs. [sent-714, score-0.508]
</p><p>65 In order to evaluate the performance of our algorithm, we compute a = maxl a(l) and a = ¯ (l) where a(l) = γ(G(l) )/γ(G(l) ) is relative density of induced subgraph (compared to original minl a T graph density); and nT /N is relative size of induced subgraph compared to original graph size. [sent-718, score-1.124]
</p><p>66 0017  Table 1: Common dense subgraph recovery on multiple graphs in DIMACS data set. [sent-748, score-0.649]
</p><p>67 Here a and ¯ a denote the maximum and minimum relative density of the induced subgraph (relative to density of the original graph); nSV and nT denotes the number of support vectors and size of subset T ⊆ SV returned by Algorithm 1. [sent-749, score-0.448]
</p><p>68 We note that our algorithm ﬁnds a large subgraph (large nT /N) with higher density compared to original graph in all of DIMACS graph classes making it suitable for ﬁnding large dense regions in multiple graphs. [sent-752, score-0.86]
</p><p>69 We note that traditional set enumerative methods fail to handle dense subgraph recovery for the case when nT /N is large. [sent-753, score-0.472]
</p><p>70 The results in Table 1 show that in case of c-fat and p hat graph families, the induced subgraph density is signiﬁcantly improved (evidenced by high a and a); and, the number of support vectors ¯ nSV is a large fraction of N (nSV /N ≃ 1/2). [sent-756, score-0.618]
</p><p>71 Here a and a denote ¯ the maximum and minimum relative density of the induced subgraph (relative to density of the original graph). [sent-793, score-0.448]
</p><p>72 ¯ ¯  On the other hand, for brock and san graph families, the number of support vectors is equal to (l) the overall graph size nSV ≃ N; and consequently the relative density is 1, that is, γ(GSV ) = γ(G(l) ) which is not interesting. [sent-795, score-0.446]
</p><p>73 (l) Figure 2 shows the variation in density of the induced subgraph γ(GSc ) relative to original graph density γ(G(l) ) for all graphs G(l) ∈ G at increasing subgraph sizes for the largest graph (resp. [sent-800, score-1.332]
</p><p>74 Notice that in case of c-fat and p hat graph families (Figures 2(a) and 2(c)), one can further improve graph densities across all graphs G(l) ∈ G by choosing a higher value of c (and correspondingly, a smaller induced subgraph |Sc |). [sent-804, score-0.99]
</p><p>75 We note that minimum and maximum induced subgraph density improves by choosing Sc instead of T , that is, a(Sc ) ≥ a(T ) and a(Sc ) ≥ a(T ) for ¯ ¯ all graph families. [sent-810, score-0.586]
</p><p>76 It can be seen from Figure 2 that the induced subgraph density is not strictly monotonic with induced subgraph size |Sc |. [sent-811, score-0.752]
</p><p>77 Figures (a) − (e) show the densities of the induced (l) subgraph γ(GSc ) relative to original graph density γ(G(l) ) for all graphs G(l) ∈ G at different values of c ∈ [0, 1] (i. [sent-831, score-0.795]
</p><p>78 , different subgraph sizes |Sc |) for different DIMACS graph families. [sent-833, score-0.489]
</p><p>79 Thus, one can choose a smaller induced subgraph Sc having higher induced subgraph density by selecting higher value of threshold c. [sent-835, score-0.752]
</p><p>80 It is instructive to note that in all graph families, the graph with maximum relative density, for example, c-fat500 in Figure 2(a) is the graph with minimum average density among all graph G. [sent-836, score-0.792]
</p><p>81 In other words, MKL-based approach tries to ﬁnd a dense region in the sparsest subgraph G(l) ∈ G while making sure it is compatible with remaining graphs in G. [sent-837, score-0.649]
</p><p>82 3 Planted Clique Recovery on Random Graphs We consider the case for Erdos-Renyi graph with general p = 1/2 and planted clique of size k, that is, G(n, p, k). [sent-839, score-0.796]
</p><p>83 1 DATA S ET ¯ We generate ns = 100 random graphs based on G(n, 1 − p, k) with planted independent set of size 1 −α k = 2t n(1 − p)/p and p = 2 n where n = 20000 and α ≥ 0. [sent-842, score-0.55]
</p><p>84 Note that the case of α = 0 yields √ the familiar planted clique model G(n, 1/2, k) with k = 2t n. [sent-847, score-0.61]
</p><p>85 5  4  (b)  Figure 3: (a) shows fr the fraction of graphs for which the hidden clique is recovered exactly; and, the average F1 -score measuring quality of recovered subset over ns trials at each t √ (k = 2t n). [sent-856, score-0.504]
</p><p>86 3 D ISCUSSION As predicted by Theorem 23, there exists some t0 > 0 for which Lov´ sz ϑ function is bounded a by ω(K); and the planted clique can be recovered perfectly by selecting the top k support vectors in sorted descending order of α∗ . [sent-892, score-0.803]
</p><p>87 We ﬁnd experimentally that this approach recovers the planted i ¯ clique exactly for t ≥ 2 for all c ∈ {0, . [sent-893, score-0.61]
</p><p>88 , 10}, that is, random graph G(n, 1 − p, k) with p = 1 n−α 2 and planted independent set of size k = 2t n(1 − p)/p. [sent-896, score-0.527]
</p><p>89 In particular we discuss the case c = 0 which yields the Erd¨ s-R´ nyi graph with p = 1/2 and o e √ planted clique of size k = 2t n. [sent-897, score-0.826]
</p><p>90 Figure 3(a) shows the fraction of graphs for which the hidden clique is recovered exactly using above procedure. [sent-898, score-0.504]
</p><p>91 Extended Results on DIMACS Graph Families (l)  Figure 5 shows the densities of the induced subgraph γ(GSc ) relative to original graph density γ(G(l) ) for all graphs G(l) ∈ G at different values of c ∈ [0, 1] (i. [sent-912, score-0.795]
</p><p>92 , different subgraph sizes |Sc |) for remaining graphs (other than those presented in Figure 2) in different DIMACS graph families. [sent-914, score-0.698]
</p><p>93 Some Results Related to G(n, 1 − p, k) In this section we derive two results related to the planted clique problem. [sent-916, score-0.61]
</p><p>94 Figures (a) − (i) show the densities of the induced (l) subgraph γ(GSc ) relative to original graph density γ(G(l) ) for all graphs G(l) ∈ G at different values of c ∈ [0, 1] (i. [sent-956, score-0.795]
</p><p>95 , different subgraph sizes |Sc |) for remaining graphs in different DIMACS graph families. [sent-958, score-0.698]
</p><p>96 Using this together with Lemma 31, we get that G is a Q graph with probability 1 − O( 1 ) if n A − EA 2 √ 9k2 + + ln n + p ≤ kp − 6kp ln n. [sent-995, score-0.494]
</p><p>97 kp − 16n kp Application of Lemma 33 yields A − EA graph if the following holds, 9k2 p ≥ 16n  2  = O (np(1 − p)). [sent-996, score-0.518]
</p><p>98 k  (32)  Next we note that for k ≥ 8 n2/3 p−1/3 (ln n)1/3 the following holds 3 √ n 6kp ln n + ln n + O = k ≤  n 1 + k1/2 p1/2 k3/2 p1/2 1 1 6kp ln n 1 + O( 1/3 1/3 + ) 1/6 n p (ln n) (ln n)1/2  =  6kp ln n (1 + o(1))  6kp ln n 1 + O  where np = Ω(log4 n) by assumption in the theorem. [sent-998, score-0.467]
</p><p>99 This means that, almost surely √ O kp ln n kp √ √ − 1| = =O max |pri − 1| = | i kp + O kp ln n kp + O kp ln n  where we use that ln n ≪ kp as noted above. [sent-1035, score-1.47]
</p><p>100 High-dimensional random geometric graphs and o their clique number. [sent-1136, score-0.478]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('planted', 0.341), ('subgraph', 0.303), ('clique', 0.269), ('graphs', 0.209), ('lov', 0.193), ('sz', 0.193), ('graph', 0.186), ('kp', 0.166), ('artinsson', 0.159), ('ethava', 0.159), ('hattacharyya', 0.159), ('ubhashi', 0.159), ('labelling', 0.156), ('asz', 0.152), ('ense', 0.152), ('ubgraphs', 0.152), ('dense', 0.137), ('svm', 0.135), ('sc', 0.134), ('ov', 0.13), ('sv', 0.126), ('inding', 0.117), ('np', 0.112), ('labellings', 0.099), ('nsv', 0.099), ('alpha', 0.097), ('kkt', 0.095), ('adjacency', 0.095), ('gsc', 0.091), ('luz', 0.083), ('nt', 0.08), ('pri', 0.076), ('dimacs', 0.076), ('ln', 0.071), ('mkl', 0.07), ('gsv', 0.068), ('subgraphs', 0.066), ('ea', 0.064), ('gs', 0.058), ('feige', 0.058), ('sdp', 0.057), ('ai', 0.054), ('krauthgamer', 0.052), ('ni', 0.051), ('cliques', 0.05), ('induced', 0.049), ('density', 0.048), ('pei', 0.047), ('cqp', 0.045), ('deg', 0.045), ('orthogonal', 0.045), ('ls', 0.043), ('gt', 0.043), ('vertices', 0.041), ('ri', 0.04), ('common', 0.04), ('erd', 0.039), ('chalmers', 0.038), ('dubhashi', 0.038), ('gq', 0.038), ('eigenvalue', 0.038), ('jiang', 0.037), ('whenever', 0.034), ('ui', 0.033), ('schrijver', 0.032), ('enumerative', 0.032), ('hat', 0.032), ('pr', 0.032), ('lemma', 0.031), ('nc', 0.03), ('lab', 0.03), ('nyi', 0.03), ('theorem', 0.03), ('di', 0.029), ('vertex', 0.029), ('hush', 0.029), ('ek', 0.029), ('mini', 0.028), ('nding', 0.027), ('conv', 0.027), ('mcdiarmid', 0.027), ('brock', 0.026), ('bollob', 0.026), ('hidden', 0.026), ('regime', 0.026), ('finding', 0.026), ('johnson', 0.026), ('families', 0.025), ('min', 0.025), ('hoffman', 0.025), ('surely', 0.024), ('establishes', 0.024), ('frieze', 0.023), ('matrix', 0.023), ('ij', 0.023), ('css', 0.023), ('garey', 0.023), ('jethava', 0.023), ('juh', 0.023), ('kucera', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="64-tfidf-1" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>Author: Vinay Jethava, Anders Martinsson, Chiranjib Bhattacharyya, Devdatt Dubhashi</p><p>Abstract: In this paper we establish that the Lov´ sz ϑ function on a graph can be restated as a kernel learning a problem. We introduce the notion of SVM − ϑ graphs, on which Lov´ sz ϑ function can be apa proximated well by a Support vector machine (SVM). We show that Erd¨ s-R´ nyi random G(n, p) o e log4 n np graphs are SVM − ϑ graphs for n ≤ p < 1. Even if we embed a large clique of size Θ 1−p in a G(n, p) graph the resultant graph still remains a SVM − ϑ graph. This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. Associated with the ϑ function is the notion of orthogonal labellings. We introduce common orthogonal labellings which extends the idea of orthogonal labellings to multiple graphs. This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude. Keywords: orthogonal labellings of graphs, planted cliques, random graphs, common dense subgraph</p><p>2 0.10767374 <a title="64-tfidf-2" href="./jmlr-2013-Variable_Selection_in_High-Dimension_with_Random_Designs_and_Orthogonal_Matching_Pursuit.html">119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</a></p>
<p>Author: Antony Joseph</p><p>Abstract: The performance of orthogonal matching pursuit (OMP) for variable selection is analyzed for random designs. When contrasted with the deterministic case, since the performance is here measured after averaging over the distribution of the design matrix, one can have far less stringent sparsity constraints on the coefﬁcient vector. We demonstrate that for exact sparse vectors, the performance of the OMP is similar to known results on the Lasso algorithm (Wainwright, 2009). Moreover, variable selection under a more relaxed sparsity assumption on the coefﬁcient vector, whereby one has only control on the ℓ1 norm of the smaller coefﬁcients, is also analyzed. As consequence of these results, we also show that the coefﬁcient estimate satisﬁes strong oracle type inequalities. Keywords: high dimensional regression, greedy algorithms, Lasso, compressed sensing</p><p>3 0.094528399 <a title="64-tfidf-3" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>Author: Matthew J. Urry, Peter Sollich</p><p>Abstract: We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to signiﬁcant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is signiﬁcantly more accurate than previous approximations and should become exact in the limit of large random graphs. Keywords: Gaussian process, generalisation error, learning curve, cavity method, belief propagation, graph, random walk kernel</p><p>4 0.080822796 <a title="64-tfidf-4" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>Author: Julien Mairal, Bin Yu</p><p>Abstract: We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called “path coding” penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efﬁciently solve by leveraging network ﬂow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs. Keywords: convex and non-convex optimization, network ﬂow optimization, graph sparsity</p><p>5 0.077714287 <a title="64-tfidf-5" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>Author: Jun Wang, Tony Jebara, Shih-Fu Chang</p><p>Abstract: Graph-based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems, particularly in agnostic settings when no parametric information or other prior knowledge is available about the data distribution. Given the constructed graph represented by a weight matrix, transductive inference is used to propagate known labels to predict the values of all unlabeled vertices. Designing a robust label diffusion algorithm for such graphs is a widely studied problem and various methods have recently been suggested. Many of these can be formalized as regularized function estimation through the minimization of a quadratic cost. However, most existing label diffusion methods minimize a univariate cost with the classiﬁcation function as the only variable of interest. Since the observed labels seed the diffusion process, such univariate frameworks are extremely sensitive to the initial label choice and any label noise. To alleviate the dependency on the initial observed labels, this article proposes a bivariate formulation for graph-based SSL, where both the binary label information and a continuous classiﬁcation function are arguments of the optimization. This bivariate formulation is shown to be equivalent to a linearly constrained Max-Cut problem. Finally an efﬁcient solution via greedy gradient Max-Cut (GGMC) is derived which gradually assigns unlabeled vertices to each class with minimum connectivity. Once convergence guarantees are established, this greedy Max-Cut based SSL is applied on both artiﬁcial and standard benchmark data sets where it obtains superior classiﬁcation accuracy compared to existing state-of-the-art SSL methods. Moreover, GGMC shows robustness with respect to the graph construction method and maintains high accuracy over extensive experiments with various edge linking and weighting schemes. Keywords: graph transduction, semi-supervised learning, bivariate formulation, mixed integer programming, greedy</p><p>6 0.074855 <a title="64-tfidf-6" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>7 0.069553405 <a title="64-tfidf-7" href="./jmlr-2013-Convex_and_Scalable_Weakly_Labeled_SVMs.html">29 jmlr-2013-Convex and Scalable Weakly Labeled SVMs</a></p>
<p>8 0.056112073 <a title="64-tfidf-8" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>9 0.05183699 <a title="64-tfidf-9" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>10 0.048611209 <a title="64-tfidf-10" href="./jmlr-2013-Truncated_Power_Method_for_Sparse_Eigenvalue_Problems.html">116 jmlr-2013-Truncated Power Method for Sparse Eigenvalue Problems</a></p>
<p>11 0.048088886 <a title="64-tfidf-11" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>12 0.046492621 <a title="64-tfidf-12" href="./jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning.html">35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</a></p>
<p>13 0.046106987 <a title="64-tfidf-13" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>14 0.044757541 <a title="64-tfidf-14" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>15 0.041999754 <a title="64-tfidf-15" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>16 0.040137663 <a title="64-tfidf-16" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>17 0.040104285 <a title="64-tfidf-17" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>18 0.03980913 <a title="64-tfidf-18" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>19 0.039754175 <a title="64-tfidf-19" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>20 0.039661285 <a title="64-tfidf-20" href="./jmlr-2013-BudgetedSVM%3A_A_Toolbox_for_Scalable_SVM_Approximations.html">19 jmlr-2013-BudgetedSVM: A Toolbox for Scalable SVM Approximations</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.193), (1, 0.08), (2, 0.014), (3, 0.034), (4, 0.157), (5, 0.131), (6, 0.047), (7, 0.025), (8, -0.013), (9, -0.143), (10, 0.049), (11, 0.019), (12, 0.006), (13, 0.119), (14, 0.105), (15, 0.254), (16, 0.059), (17, -0.065), (18, 0.024), (19, -0.27), (20, 0.042), (21, -0.097), (22, -0.038), (23, -0.043), (24, -0.038), (25, 0.069), (26, -0.001), (27, 0.077), (28, -0.098), (29, -0.085), (30, -0.064), (31, 0.034), (32, 0.064), (33, 0.035), (34, 0.155), (35, 0.082), (36, 0.177), (37, 0.052), (38, -0.116), (39, 0.043), (40, 0.059), (41, -0.066), (42, 0.064), (43, -0.014), (44, -0.034), (45, -0.01), (46, 0.045), (47, -0.026), (48, 0.119), (49, -0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93692422 <a title="64-lsi-1" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>Author: Vinay Jethava, Anders Martinsson, Chiranjib Bhattacharyya, Devdatt Dubhashi</p><p>Abstract: In this paper we establish that the Lov´ sz ϑ function on a graph can be restated as a kernel learning a problem. We introduce the notion of SVM − ϑ graphs, on which Lov´ sz ϑ function can be apa proximated well by a Support vector machine (SVM). We show that Erd¨ s-R´ nyi random G(n, p) o e log4 n np graphs are SVM − ϑ graphs for n ≤ p < 1. Even if we embed a large clique of size Θ 1−p in a G(n, p) graph the resultant graph still remains a SVM − ϑ graph. This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. Associated with the ϑ function is the notion of orthogonal labellings. We introduce common orthogonal labellings which extends the idea of orthogonal labellings to multiple graphs. This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude. Keywords: orthogonal labellings of graphs, planted cliques, random graphs, common dense subgraph</p><p>2 0.5399313 <a title="64-lsi-2" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>Author: Matthew J. Urry, Peter Sollich</p><p>Abstract: We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to signiﬁcant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is signiﬁcantly more accurate than previous approximations and should become exact in the limit of large random graphs. Keywords: Gaussian process, generalisation error, learning curve, cavity method, belief propagation, graph, random walk kernel</p><p>3 0.52924246 <a title="64-lsi-3" href="./jmlr-2013-Variable_Selection_in_High-Dimension_with_Random_Designs_and_Orthogonal_Matching_Pursuit.html">119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</a></p>
<p>Author: Antony Joseph</p><p>Abstract: The performance of orthogonal matching pursuit (OMP) for variable selection is analyzed for random designs. When contrasted with the deterministic case, since the performance is here measured after averaging over the distribution of the design matrix, one can have far less stringent sparsity constraints on the coefﬁcient vector. We demonstrate that for exact sparse vectors, the performance of the OMP is similar to known results on the Lasso algorithm (Wainwright, 2009). Moreover, variable selection under a more relaxed sparsity assumption on the coefﬁcient vector, whereby one has only control on the ℓ1 norm of the smaller coefﬁcients, is also analyzed. As consequence of these results, we also show that the coefﬁcient estimate satisﬁes strong oracle type inequalities. Keywords: high dimensional regression, greedy algorithms, Lasso, compressed sensing</p><p>4 0.50250584 <a title="64-lsi-4" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>Author: Nicolò Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella</p><p>Abstract: We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world data sets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice. Keywords: online learning, learning on graphs, graph prediction, random spanning trees</p><p>5 0.4318957 <a title="64-lsi-5" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>Author: Jun Wang, Tony Jebara, Shih-Fu Chang</p><p>Abstract: Graph-based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems, particularly in agnostic settings when no parametric information or other prior knowledge is available about the data distribution. Given the constructed graph represented by a weight matrix, transductive inference is used to propagate known labels to predict the values of all unlabeled vertices. Designing a robust label diffusion algorithm for such graphs is a widely studied problem and various methods have recently been suggested. Many of these can be formalized as regularized function estimation through the minimization of a quadratic cost. However, most existing label diffusion methods minimize a univariate cost with the classiﬁcation function as the only variable of interest. Since the observed labels seed the diffusion process, such univariate frameworks are extremely sensitive to the initial label choice and any label noise. To alleviate the dependency on the initial observed labels, this article proposes a bivariate formulation for graph-based SSL, where both the binary label information and a continuous classiﬁcation function are arguments of the optimization. This bivariate formulation is shown to be equivalent to a linearly constrained Max-Cut problem. Finally an efﬁcient solution via greedy gradient Max-Cut (GGMC) is derived which gradually assigns unlabeled vertices to each class with minimum connectivity. Once convergence guarantees are established, this greedy Max-Cut based SSL is applied on both artiﬁcial and standard benchmark data sets where it obtains superior classiﬁcation accuracy compared to existing state-of-the-art SSL methods. Moreover, GGMC shows robustness with respect to the graph construction method and maintains high accuracy over extensive experiments with various edge linking and weighting schemes. Keywords: graph transduction, semi-supervised learning, bivariate formulation, mixed integer programming, greedy</p><p>6 0.40642506 <a title="64-lsi-6" href="./jmlr-2013-Convex_and_Scalable_Weakly_Labeled_SVMs.html">29 jmlr-2013-Convex and Scalable Weakly Labeled SVMs</a></p>
<p>7 0.38045698 <a title="64-lsi-7" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>8 0.34639165 <a title="64-lsi-8" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>9 0.32934132 <a title="64-lsi-9" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>10 0.30676746 <a title="64-lsi-10" href="./jmlr-2013-BudgetedSVM%3A_A_Toolbox_for_Scalable_SVM_Approximations.html">19 jmlr-2013-BudgetedSVM: A Toolbox for Scalable SVM Approximations</a></p>
<p>11 0.29782432 <a title="64-lsi-11" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>12 0.29549232 <a title="64-lsi-12" href="./jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion.html">110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</a></p>
<p>13 0.28958911 <a title="64-lsi-13" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>14 0.28003904 <a title="64-lsi-14" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>15 0.26847792 <a title="64-lsi-15" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>16 0.26588565 <a title="64-lsi-16" href="./jmlr-2013-Truncated_Power_Method_for_Sparse_Eigenvalue_Problems.html">116 jmlr-2013-Truncated Power Method for Sparse Eigenvalue Problems</a></p>
<p>17 0.24917772 <a title="64-lsi-17" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>18 0.24682193 <a title="64-lsi-18" href="./jmlr-2013-Learning_Theory_Analysis_for_Association_Rules_and_Sequential_Event_Prediction.html">61 jmlr-2013-Learning Theory Analysis for Association Rules and Sequential Event Prediction</a></p>
<p>19 0.23127402 <a title="64-lsi-19" href="./jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning.html">35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</a></p>
<p>20 0.22367467 <a title="64-lsi-20" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.024), (5, 0.13), (6, 0.036), (10, 0.056), (14, 0.026), (20, 0.01), (23, 0.037), (29, 0.409), (68, 0.039), (70, 0.021), (75, 0.024), (85, 0.04), (87, 0.023), (89, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.66649425 <a title="64-lda-1" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>Author: Vinay Jethava, Anders Martinsson, Chiranjib Bhattacharyya, Devdatt Dubhashi</p><p>Abstract: In this paper we establish that the Lov´ sz ϑ function on a graph can be restated as a kernel learning a problem. We introduce the notion of SVM − ϑ graphs, on which Lov´ sz ϑ function can be apa proximated well by a Support vector machine (SVM). We show that Erd¨ s-R´ nyi random G(n, p) o e log4 n np graphs are SVM − ϑ graphs for n ≤ p < 1. Even if we embed a large clique of size Θ 1−p in a G(n, p) graph the resultant graph still remains a SVM − ϑ graph. This immediately suggests an SVM based algorithm for recovering a large planted clique in random graphs. Associated with the ϑ function is the notion of orthogonal labellings. We introduce common orthogonal labellings which extends the idea of orthogonal labellings to multiple graphs. This allows us to propose a Multiple Kernel learning (MKL) based solution which is capable of identifying a large common dense subgraph in multiple graphs. Both in the planted clique case and common subgraph detection problem the proposed solutions beat the state of the art by an order of magnitude. Keywords: orthogonal labellings of graphs, planted cliques, random graphs, common dense subgraph</p><p>2 0.59991771 <a title="64-lda-2" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>Author: Alberto N. Escalante-B., Laurenz Wiskott</p><p>Abstract: Supervised learning from high-dimensional data, for example, multimedia data, is a challenging task. We propose an extension of slow feature analysis (SFA) for supervised dimensionality reduction called graph-based SFA (GSFA). The algorithm extracts a label-predictive low-dimensional set of features that can be post-processed by typical supervised algorithms to generate the ﬁnal label or class estimation. GSFA is trained with a so-called training graph, in which the vertices are the samples and the edges represent similarities of the corresponding labels. A new weighted SFA optimization problem is introduced, generalizing the notion of slowness from sequences of samples to such training graphs. We show that GSFA computes an optimal solution to this problem in the considered function space and propose several types of training graphs. For classiﬁcation, the most straightforward graph yields features equivalent to those of (nonlinear) Fisher discriminant analysis. Emphasis is on regression, where four different graphs were evaluated experimentally with a subproblem of face detection on photographs. The method proposed is promising particularly when linear models are insufﬁcient as well as when feature selection is difﬁcult. Keywords: slow feature analysis, feature extraction, classiﬁcation, regression, pattern recognition, training graphs, nonlinear dimensionality reduction, supervised learning, implicitly supervised, high-dimensional data, image analysis</p><p>3 0.36682138 <a title="64-lda-3" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>Author: Yuchen Zhang, John C. Duchi, Martin J. Wainwright</p><p>Abstract: We analyze two communication-efﬁcient algorithms for distributed optimization in statistical settings involving large-scale data sets. The ﬁrst algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves √ mean-squared error (MSE) that decays as O (N −1 + (N/m)−2 ). Whenever m ≤ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O (N −1 + (N/m)−3 ), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O (N −1 + (N/m)−3/2 ), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efﬁciently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 108 samples and d ≈ 740,000 covariates. Keywords: distributed learning, stochastic optimization, averaging, subsampling</p><p>4 0.36347219 <a title="64-lda-4" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>Author: Sébastien Gerchinovitz</p><p>Abstract: We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T . We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same ﬂavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with ﬁxed design. Keywords: sparsity, online linear regression, individual sequences, adaptive regret bounds</p><p>5 0.3622604 <a title="64-lda-5" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>Author: Tony Cai, Wen-Xin Zhou</p><p>Abstract: We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed. Keywords: 1-bit matrix completion, low-rank matrix, max-norm, trace-norm, constrained optimization, maximum likelihood estimate, optimal rate of convergence</p><p>6 0.36061615 <a title="64-lda-6" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>7 0.359703 <a title="64-lda-7" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>8 0.35766977 <a title="64-lda-8" href="./jmlr-2013-Variable_Selection_in_High-Dimension_with_Random_Designs_and_Orthogonal_Matching_Pursuit.html">119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</a></p>
<p>9 0.35734066 <a title="64-lda-9" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>10 0.35733464 <a title="64-lda-10" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>11 0.35717192 <a title="64-lda-11" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>12 0.35685861 <a title="64-lda-12" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>13 0.35587224 <a title="64-lda-13" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>14 0.35521942 <a title="64-lda-14" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>15 0.35377496 <a title="64-lda-15" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>16 0.35300371 <a title="64-lda-16" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>17 0.35222179 <a title="64-lda-17" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>18 0.35194719 <a title="64-lda-18" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>19 0.3517929 <a title="64-lda-19" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>20 0.35047677 <a title="64-lda-20" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
