<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-88" href="#">jmlr2013-88</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</h1>
<br/><p>Source: <a title="jmlr-2013-88-pdf" href="http://jmlr.org/papers/volume14/opper13a/opper13a.pdf">pdf</a></p><p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian ﬁeld, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model’s partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. Through the expansion, we see that EP is correct to ﬁrst order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. Keywords: expectation consistent inference, expectation propagation, perturbation correction, Wick expansions, Ising model, Gaussian process</p><p>Reference: <a title="jmlr-2013-88-reference" href="../jmlr2013_reference/jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. [sent-9, score-0.487]
</p><p>2 In the experiments, corrections to the marginal and predictive means and variances are also shown, although the technical details for correcting moments beyond the partition function are relegated to Appendix D. [sent-39, score-0.335]
</p><p>3 This is done by considering the cumulants of both: as EP locally matches lower-order cumulants like means and variances, the “intractable part” exists as an expression over the higher-order cumulants which are neglected by EP. [sent-42, score-0.753]
</p><p>4 The main body of the paper deals with corrections to the partition function, while corrections to marginal moments are left to Appendix D. [sent-52, score-0.404]
</p><p>5 In the case of GP classiﬁcation with a class label yn ∈ {−1, +1} on a latent function evaluation xn , the terms are typically probit link functions, for example p(x) =  1 N ∏ Φ(yn xn ) N (x ; 0, K) . [sent-62, score-0.317]
</p><p>6 To therefore allow for regrouping, combining, splitting, and dividing terms, a power Da is associated with each fa , such that p(x) =  1 Z  ∏ fa (x)D  a  a  2859  (4)  O PPER , PAQUET AND W INTHER  with intractable normalization (or partition function) Z = ∏a fa (x)Da dx. [sent-77, score-0.282]
</p><p>7 Although not shown explicitly, fa and ga have a dependence on the same subset of variables xa . [sent-80, score-0.316]
</p><p>8 The optimal parameters of the ga -term approximations are found through a set of auxiliary tilted distributions, deﬁned by qa (x) =  q(x) fa (x) ga (x)  1 Za  . [sent-81, score-0.425]
</p><p>9 Assuming that this replacement leaves qa still tractable, the parameters in ga are determined by the condition that q(x) and all qa (x) should be made as similar as possible. [sent-83, score-0.394]
</p><p>10 The EP algorithm iteratively updates the ga -terms by enforcing q to share moments with each of the tilted distributions qa ; on reaching a ﬁxed point all moments match according to Equation (7) (Minka, 2001a,b). [sent-90, score-0.491]
</p><p>11 The tilted distribution may be decomposed into a Gaussian and a discrete part as qn (x) = qn (x\n |xn )qn (xn ), where the vector x\n consists of all variables apart from xn . [sent-111, score-0.38]
</p><p>12 In Section 4 we investigate how cumulant corrections can correct for this discrepancy. [sent-132, score-0.31]
</p><p>13 Since this is not possible—it would require the computation of exact moments—we instead iteratively minimize “local” KL-divergences KL(qa q), between the tilted distribution qa and q, with respect to ga (appearing in q). [sent-138, score-0.307]
</p><p>14 Integrating out the Gaussian random variable hn gives the Gaussian cavity ﬁeld approximation to the marginal distribution: 2  pn (xn ) ≈ const · tn (xn ) e−Jnn xn /2  e−xn h N (h ; γn ,Vn ) dh  1 2 = const · tn (xn ) exp −xn γn − (Jnn −Vn )xn 2  . [sent-163, score-0.346]
</p><p>15 This correction factor R encapsulates the intractability in the model, and contains a “local marginal” contribution by each fa (see Section 4. [sent-176, score-0.283]
</p><p>16 As qa (x) and q(x) share their two ﬁrst cumulants, the mean and covariance from the moment matching condition in Equation (7), a cumulant expansion of R will be in terms of higher-order cumulants (see Section 4. [sent-183, score-0.798]
</p><p>17 Each factor fa contributes a complex random variable ka in this average (see Section 4. [sent-187, score-0.579]
</p><p>18 Finally, the cumulant differences are used as “small quantities” in a Taylor series expansion of R, and the leading terms are kept (see Sections 5 and 7). [sent-190, score-0.279]
</p><p>19 We can derive a useful expression for R in a few steps as follows: First we solve for fa in Equation (6), and substitute this into Equation (4) to obtain qa (x) Da Za qa (x)ga (x) Da = ZEP q(x) ∏ . [sent-199, score-0.424]
</p><p>20 (11) fa (x)Da = ∏ ∏ q(x) q(x) a a a We introduce F(x) F(x) ≡ ∏ a  qa (x) q(x)  Da  to derive the expression for the correction R = Z/ZEP by integrating Equation (11): R=  q(x)F(x) dx ,  (12)  where we have used Z = ∏a fa (x)Da dx. [sent-200, score-0.532]
</p><p>21 The distributions in Equations (5) and (6) differ only with respect to their marginals on xa , qa (xa ) and q(xa ), and therefore qa (x) q(x\a |xa )qa (xa ) qa (xa ) = = . [sent-205, score-0.766]
</p><p>22 This simple observation has a crucial consequence: As the q(xa )’s are Gaussian and do not contain any higher order cumulants (three and above), F can be expressed in terms of the higher cumulants of the marginals qa (xa ). [sent-212, score-0.72]
</p><p>23 When the term-product approximation is fully factorized, these are simply cumulants of one-dimensional distributions. [sent-213, score-0.274]
</p><p>24 Furthermore, let ka be an Na -dimensional vector ka = (k1 , . [sent-216, score-0.958]
</p><p>25 The characteristic function of qa is T  T  eika xa qa (xa ) dxa = eika xa  χa (ka ) =  qa  ,  (15)  and is obtained through the Fourier transform of the density. [sent-220, score-0.965]
</p><p>26 Inversely, qa (xa ) =  1 (2π)Na  T  e−ika xa χa (ka ) dka . [sent-221, score-0.395]
</p><p>27 (16)  The cumulants cαa of qa are the coefﬁcients that appear in the Taylor expansion of log χa (ka ) around the zero vector, ∂ α . [sent-222, score-0.572]
</p><p>28 log χa (ka ) cαa = (−i)l ∂ka ka =0 By this deﬁnition of cαa , the Taylor expansion of log χa (ka ) is ∞  log χa (ka ) = ∑ il l=1  cαa α k . [sent-223, score-0.779]
</p><p>29 , αNa ), with α j ∈ N0 , denotes a multi-index on the elements of ka . [sent-229, score-0.479]
</p><p>30 Other notational conventions that employ α (writing k j instead of ka j ) are: |α| = ∑ α j , j  α  kα = ∏ k j j , a j  α! [sent-230, score-0.479]
</p><p>31 The ﬁrst is that of the tilted distribution, log χa (ka ), and the other is the characteristic function of the 2865  O PPER , PAQUET AND W INTHER  T  EP marginal q(xa ), deﬁned as χ(ka ) = eika xa q . [sent-235, score-0.418]
</p><p>32 By virtue of matching the ﬁrst two moments, and q(xa ) being Gaussian with cumulants c′ , αa cαa − c′ αa α ka α! [sent-236, score-0.757]
</p><p>33 ra (ka ) = log χa (ka ) − log χ(ka ) = ∑ il  ∑  (17)  contains the remaining higher-order cumulants where the tilted and approximate distributions differ. [sent-238, score-0.608]
</p><p>34 1 I SING M ODEL E XAMPLE The cumulant expansion for the discrete distribution in Equation (10) becomes 1 + m ikn 1 − m −ikn e + e 2 2 1 i 1 2 3 4 = imkn − (1 − m2 )kn − (−2m + 2m3 )kn + (−2 + 8m2 − 6m4 )kn + · · · 2! [sent-243, score-0.279]
</p><p>35 log χn (kn ) = log  dxn eikn xn qn (xn ) = log  (we’re compactly writing m for mn ), from which the cumulants are obtained as c4n = −2 + 8m2 − 6m4 ,  c1n = m , c2n = 1 − m2 ,  c3n = −2m + 2m3 ,  c5n = 16m − 40m3 + 24m5 ,  c6n = 16 − 136m2 + 240m4 − 120m6 . [sent-246, score-0.666]
</p><p>36 Firstly, the ratio qa /q will be written as a single quantity that depends on ra , which was introduced above in Equation (17). [sent-250, score-0.29]
</p><p>37 1 C OMPLEX E XPECTATIONS Assume that q(xa ) = N (xa ; µa , Σa ) and qa (xa ) share the same mean and covariance, and substitute log χa (ka ) = ra (ka ) + log χ(ka ) in the deﬁnition of qa in Equation (16) to give qa (xa ) = q(xa )  T  e−ika xa +ra (ka ) χ(ka ) dka . [sent-256, score-0.982]
</p><p>38 T e−ika xa χ(ka ) dka  (18)  Although the ka variables have not been introduced as random variables, we ﬁnd it natural to interpret them as such, because the rules of expectations over Gaussian random variables will be 2866  P ERTURBATIVE C ORRECTIONS FOR A PPROXIMATE I NFERENCE  1 Im(k)  0. [sent-257, score-0.722]
</p><p>39 5 −1 −1  0  1  2  3  x  1  4  0  −1 Re(k)  Figure 1: Equation (20) shifts ka to the complex plane. [sent-259, score-0.505]
</p><p>40 We will therefore write qa (xa )/q(xa ) T as an expectation of exp ra (ka ) over a density p(ka |xa ) ∝ e−ika xa χ(ka ): qa (xa ) = exp ra (ka ) q(xa )  ka |xa  . [sent-266, score-1.333]
</p><p>41 (19)  By substituting log χ(ka ) = iµT ka − kT Σa ka /2 into Equation (18), we see that p(ka |xa ) can be a a viewed as Gaussian, but not for real random variables! [sent-267, score-1.019]
</p><p>42 We have to consider ka as Gaussian random variables with a real and an imaginary part with ℜ(ka ) ∼ N ℜ(ka ) ; 0, Σ−1 , a  ℑ(ka ) = −Σ−1 (xa − µa ) . [sent-268, score-0.501]
</p><p>43 a  For the purpose of computing the expectation in Equation (19), ka |xa is a degenerate complex Gaussian that shifts the coefﬁcients ka into the complex plane. [sent-269, score-1.04]
</p><p>44 As shorthand, we write p(ka |xa ) = N ka ; −iΣ−1 (xa − µa ) , Σ−1 . [sent-271, score-0.479]
</p><p>45 Once xa is averaged out of the joint density p(ka |xa ) q(xa ), a circularly symmetric complex Gaussian distribution over ka remains. [sent-273, score-0.703]
</p><p>46 It is circularly symmetric as ka = 0, reT lation matrix ka kT = 0, and covariance matrix ka ka = 2Σ−1 (notation k indicates the complex a a conjugate of k). [sent-274, score-1.977]
</p><p>47 4 below), we only need the relations ka kT for pairs of factors a and b. [sent-276, score-0.502]
</p><p>48 All of these b will be derived next: According to Equation (12), a further expectation over q(x) is needed, after integrating over ka , to determine R. [sent-277, score-0.509]
</p><p>49 By substituting Equation (19) into Equation (12), R is equal to R = F(x)  = x∼q(x)  ∏  exp ra (ka )  a  Da ka |xa  . [sent-279, score-0.617]
</p><p>50 (21)  x  When x is given, the ka -variables are independent. [sent-280, score-0.479]
</p><p>51 However, when they are averaged over q(x), the ka -variables become coupled. [sent-281, score-0.479]
</p><p>52 They are zero-mean complex Gaussians ka =  ka  ka |xa  x  = −iΣ−1 (xa − µa ) a  x  =0  and are coupled with a zero self-relation matrix! [sent-282, score-1.463]
</p><p>53 In other words, if Σab = cov(xa , xb ), the expected values ka kT between the variables in the set {ka } are b ka kT = b =  ka kT b  ka,b |x x  + i2 Σ−1 (xa − µa )(xb − µb )T a  x  Σ−1 b  0 if a = b . [sent-283, score-1.437]
</p><p>54 −Σ−1 Σab Σ−1 if a = b a b  (22) T  Complex Gaussian random variables are additionally characterized by ka kb . [sent-284, score-0.501]
</p><p>55 Figure 2 illustrates the structure of the resulting relation matrix ka kT for two different factorizations of the same distribution. [sent-286, score-0.479]
</p><p>56 Each factor fa b contributes a ka variable, such that the tree-structured approximation’s relation matrix will be larger than that of the fully factorized one. [sent-287, score-0.58]
</p><p>57 The white squares indicate a zero relation matrix ka kT , with the diagonal being zero. [sent-299, score-0.479]
</p><p>58 Following Equation (21), the correction to the free energy R=  ∏ n  exp rn (kn )  kn |xn  = exp x  ∑ rn (kn ) n  (24) k  is taken directly over the centered complex-valued Gaussian random variables k = (k1 , . [sent-319, score-0.475]
</p><p>59 The single marginal terms also vanish (and hence EP is correct to ﬁrst order) because both kn = 0 2 and kn = 0. [sent-333, score-0.373]
</p><p>60 This result can give us a hint in which situations the corrections are expected to be small: • Firstly, the rn could be small for values of kn where the density of k is not small. [sent-334, score-0.307]
</p><p>61 The expectation rm rn , as it appears in Equation (26), is treated by substituting rn with its cumulant l expansion rn (kn ) = ∑l≥3 il cln kn /l! [sent-342, score-0.671]
</p><p>62 Wick’s theorem now plays a pivotal role in evaluating the expectations that appear in the expansion: cln csm s l km kn l! [sent-344, score-0.301]
</p><p>63 All the self-pairing terms, when for example one of the l kn ’s is paired with another kn in Equation (23), are zero because 2870  P ERTURBATIVE C ORRECTIONS FOR A PPROXIMATE I NFERENCE  s l 2 kn = 0. [sent-351, score-0.486]
</p><p>64 To therefore get a non-zero result for km kn , using Equation (23), each factor kn has to be paired with some factor km , and this is possible only when l = s. [sent-352, score-0.394]
</p><p>65 Finally, plugging Equation (27) into Equation (26) gives the second order correction  log R =  1 clm cln ∑ ∑ l! [sent-355, score-0.351]
</p><p>66 The uneven terms in the cumulant expansion derived in Section 4. [sent-362, score-0.279]
</p><p>67 The cumulant corrections for the marginal moments are derived in Appendix D; for example, the correction to the marginal mean µi of an approximation q(x) = N (x; µ, Σ) is xi  Σi j cl+1, j cln ∑ Σ j j l! [sent-373, score-0.813]
</p><p>68 l≥3 j=n  p(x) − µi = ∑  Σ jn Σ j j Σnn  l  +··· ,  (29)  while the correction to the marginal covariance is (xi − µi )(xi′ − µi′ )  Σi j Σi′ j cl+2, j cln 2 l! [sent-374, score-0.398]
</p><p>69 3 Edgeworth-Type Expansions To simplify the expansion of Equation (24), we integrated (combined) degenerate complex Gaussians kn |xn over q(x) to obtain fully complex Gaussian random variables {kn }. [sent-378, score-0.299]
</p><p>70 We’ve then relied on 2 kn = 0 to simplify the expansion of log R. [sent-379, score-0.308]
</p><p>71 The series that describes the tilted distribution qn (xn ) is equal to the product of q(xn ) and an expansion of polynomials for the higher-cumulant deviation 2871  O PPER , PAQUET AND W INTHER  from a Gaussian density. [sent-384, score-0.303]
</p><p>72 We prove here that a cumulant expansion for R will converge when the eigenvalues of D−1/2 ΣD−1/2 —which has diagonal values of one—are bounded between zero and two. [sent-396, score-0.279]
</p><p>73 Hence, we deﬁne R(λ) = exp  ∑ rn (λkn ) n  = exp k  2872  ∑ rn (kn ) n  k′  P ERTURBATIVE C ORRECTIONS FOR A PPROXIMATE I NFERENCE  where ′ ′ km kn =  0 if m = n . [sent-409, score-0.301]
</p><p>74 2873  O PPER , PAQUET AND W INTHER  1 c with c = c(J) ∈] − 1, 1[ which has eigenvalues 1 − c and c 1 1 + c, meaning that cumulant expansion for R(λ) is convergent for the N = 2 Ising model. [sent-429, score-0.279]
</p><p>75 Consider R in Equation (21): Its inner expectations are over ka |x, and outer expectations are over x. [sent-440, score-0.525]
</p><p>76 First take the binomial expansion of the inner expectation, and keep it to second order in ra : era (ka )  Da ka |x  = 1 + ra +  Da  1 2 r +··· 2 a  Da (Da − 1) 1 2 1 2 r +··· + ra + ra + · · · 2 a 2 2 Da (Da − 1) Da 2 ra + ra 2 + · · · . [sent-441, score-1.254]
</p><p>77 = 1 + Da ra + 2 2 = 1 + Da  ra +  2  +···  Notice that ra (ka ) can be complex, but ra (ka )  above expansion, is real-  valued. [sent-442, score-0.46]
</p><p>78 Using this result, again expand is  log R, up to second order,  log R =  1 ∑ Da Db 2 a=b +  ka |x , as it appears in the ra Da ∏a e ka |x x . [sent-443, score-1.195]
</p><p>79 The correction to  ra (ka )  ka |x  rb (kb )  1 Da (Da − 1) 2∑ a  kb |x  ra (ka )  x  2 ka |x  x  +··· . [sent-444, score-1.419]
</p><p>80 Terms involving ra (ka )2 = 0 similarly disappear, as every polynomial in the expansion ra (ka )2 averages to zero. [sent-446, score-0.315]
</p><p>81 1  0 2  3  4  5  2  log lengthscale, log(l)  3  4  5  log lengthscale, log(l)  Figure 3: A comparison of log R using a perturbation expansion of Equation (28) against Monte Carlo estimates of log R, using the USPS data set from Kuss and Rasmussen (2005). [sent-501, score-0.329]
</p><p>82 The second order correction to log R, with l = 3, 4, is used on the left; the right plot uses a Monte Carlo estimate of log R. [sent-502, score-0.331]
</p><p>83 To simplify the exposition of the predictive marginal, we follow the notation of Rasmussen and Williams (2005, Chapter 3) and let λn = (τn , νn ), so 1 2 that the ﬁnal EP approximation multiplies gn terms ∏n exp{− 2 τn xn + νn xn } into a joint Gaussian N (x ; 0, K). [sent-511, score-0.3]
</p><p>84 The correction is x∗  Σ∗ j cl+1, j cln ∑ Σ j j l! [sent-524, score-0.29]
</p><p>85 The cumulants for this problem, used both for EP and correcting it, are derived in Appendix E. [sent-527, score-0.274]
</p><p>86 (The cumulants that are required for the correction in Equation (28), and recipes for deriving them, are given in Appendix E. [sent-614, score-0.46]
</p><p>87 0002  Table 1: Average absolute deviation (AAD) of marginals in a Wainwright-Jordan set-up, comparing loopy belief propagation (LBP), log-determinant relaxation (LD), EC, EC with l = 4 second order correction (EC c), and an EC tree (EC t). [sent-770, score-0.299]
</p><p>88 In Figure 8 we vary the coupling strength for a speciﬁc set-up (Grid Mixed) and observe a cross-over between the correction and original for the error on marginals as the coupling strength increases. [sent-778, score-0.348]
</p><p>89 We conjecture that when the error of the original solution is high then the number of terms needed in the cumulant correction increases. [sent-779, score-0.403]
</p><p>90 The tree approximation is very precise for the whole coupling strength interval considered and the fourth order cumulant in the second order expansion is therefore sufﬁcient to get often quite large improvements over the original tree approximation. [sent-781, score-0.402]
</p><p>91 0433  Table 2: Absolute deviation log partition function in a Wainwright-Jordan set-up, comparing EC, EC with l = 4 second order correction (EC c), EC with a full second order ε expansion (EC εc), EC tree (EC t) and EC tree with l = 4 second order correction (EC tc). [sent-857, score-0.647]
</p><p>92 Finally, by substituting Equation (43) into (42), the correction to the mean is Σi j cl+1, j cln yi p(y) = ∑ ∑ l! [sent-882, score-0.29]
</p><p>93 2 The Marginal Covariance The correction to the second moments follow the same recipe as that of the marginal mean in Appendix D. [sent-885, score-0.35]
</p><p>94 One might also directly take derivatives of the cumulant generating function, and the cumulants of a Probittimes-Gaussian distribution, common to GP classiﬁcation models, are derived this way in Appendix E. [sent-898, score-0.445]
</p><p>95 The odd moments of this n 5 3 tilted distributions are, by symmetry, xn = xn = xn = 0. [sent-908, score-0.528]
</p><p>96 6  Figure 10: The third and fourth cumulants of the density qn (x) ∝ Φ((x − m)/v) N (x; µ, σ2 ) in Appendix E. [sent-968, score-0.339]
</p><p>97 The third cumulant is always positive, while the fourth cumulant is positive only when σ > µ. [sent-971, score-0.388]
</p><p>98 v2 + σ2 v2 + σ2 The cumulants cln are determined from the derivatives of log χn (k) at zero; a lengthy calculation shows that they are c3n = α3 β 2β2 + 3zβ + z2 − 1 ,  c4n = −α4 β 6β3 + 12zβ2 + 7z2 β + z3 − 4β − 3z ,  √ where α = σ2 / v2 + σ2 and β = N (z; 0, 1)/Φ(z). [sent-978, score-0.393]
</p><p>99 We can generate this cumulant from derivatives of log χa (ka ): c(l,l ′ ) =  ∂ ∂ik1  l  ∂ ∂ik2 2896  l′  log χa (ka )  . [sent-984, score-0.316]
</p><p>100 The fact that we can write c(2,0) in terms of the ﬁrst order cumulant shows that we can express all order cumulants in terms of the ﬁrst and second order cumulant for example: c(2,1) =  ∂ c (k) ∂ik2 (2,0)  = k=0  ∂ (1 − c2 (k)) (1,0) ∂ik2  k=0  = −2c(1,0) c(1,1) . [sent-987, score-0.639]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ka', 0.479), ('ep', 0.345), ('cumulants', 0.251), ('correction', 0.209), ('xa', 0.198), ('cumulant', 0.194), ('paquet', 0.176), ('qa', 0.175), ('ising', 0.175), ('kn', 0.162), ('ec', 0.138), ('orrections', 0.125), ('erturbative', 0.117), ('inther', 0.117), ('corrections', 0.116), ('xn', 0.116), ('ra', 0.115), ('zep', 0.113), ('zmin', 0.103), ('pper', 0.1), ('pproximate', 0.1), ('da', 0.095), ('wick', 0.095), ('zmax', 0.095), ('moments', 0.092), ('qn', 0.088), ('tilted', 0.088), ('expansion', 0.085), ('cln', 0.081), ('nference', 0.078), ('gp', 0.076), ('fa', 0.074), ('kt', 0.068), ('log', 0.061), ('opper', 0.055), ('gaussian', 0.051), ('equation', 0.05), ('marginal', 0.049), ('coupling', 0.048), ('cavity', 0.047), ('tn', 0.044), ('mcmc', 0.044), ('ga', 0.044), ('dev', 0.044), ('repulsive', 0.044), ('zn', 0.044), ('marginals', 0.043), ('polynomials', 0.042), ('winther', 0.04), ('couplings', 0.038), ('nn', 0.037), ('dcoup', 0.037), ('jnn', 0.037), ('km', 0.035), ('covariance', 0.035), ('kuss', 0.034), ('yn', 0.034), ('il', 0.032), ('moment', 0.031), ('partition', 0.031), ('minka', 0.03), ('kl', 0.03), ('expectation', 0.03), ('aad', 0.029), ('ika', 0.029), ('rn', 0.029), ('std', 0.029), ('corr', 0.029), ('intractable', 0.029), ('latent', 0.028), ('zl', 0.028), ('expansions', 0.028), ('mn', 0.028), ('rasmussen', 0.027), ('matching', 0.027), ('factorized', 0.027), ('tree', 0.026), ('complex', 0.026), ('cl', 0.025), ('predictive', 0.024), ('jn', 0.024), ('exp', 0.023), ('expectations', 0.023), ('approximation', 0.023), ('probit', 0.023), ('correcting', 0.023), ('diverges', 0.023), ('na', 0.023), ('factors', 0.023), ('attractive', 0.022), ('box', 0.022), ('dka', 0.022), ('dobs', 0.022), ('eika', 0.022), ('imaginary', 0.022), ('kb', 0.022), ('inference', 0.022), ('mixed', 0.021), ('propagation', 0.021), ('gn', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="88-tfidf-1" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian ﬁeld, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model’s partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. Through the expansion, we see that EP is correct to ﬁrst order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. Keywords: expectation consistent inference, expectation propagation, perturbation correction, Wick expansions, Ising model, Gaussian process</p><p>2 0.27807823 <a title="88-tfidf-2" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>3 0.15930177 <a title="88-tfidf-3" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>Author: Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Pierre Dupont</p><p>Abstract: We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efﬁciently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about speciﬁc groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy. Keywords: group feature selection, generalized spike-and-slab priors, expectation propagation, sparse linear model, approximate inference, sequential experimental design, signal reconstruction</p><p>4 0.10839439 <a title="88-tfidf-4" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>5 0.10736075 <a title="88-tfidf-5" href="./jmlr-2013-Universal_Consistency_of_Localized_Versions_of_Regularized_Kernel_Methods.html">117 jmlr-2013-Universal Consistency of Localized Versions of Regularized Kernel Methods</a></p>
<p>Author: Robert Hable</p><p>Abstract: In supervised learning problems, global and local learning algorithms are used. In contrast to global learning algorithms, the prediction of a local learning algorithm in a testing point is only based on training data which are close to the testing point. Every global algorithm such as support vector machines (SVM) can be localized in the following way: in every testing point, the (global) learning algorithm is not applied to the whole training data but only to the k nearest neighbors (kNN) of the testing point. In case of support vector machines, the success of such mixtures of SVM and kNN (called SVM-KNN) has been shown in extensive simulation studies and also for real data sets but only little has been known on theoretical properties so far. In the present article, it is shown how a large class of regularized kernel methods (including SVM) can be localized in order to get a universally consistent learning algorithm. Keywords: machine learning, regularized kernel methods, localization, SVM, k-nearest neighbors, SVM-KNN</p><p>6 0.1012472 <a title="88-tfidf-6" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>7 0.095222235 <a title="88-tfidf-7" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>8 0.084333539 <a title="88-tfidf-8" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>9 0.062983707 <a title="88-tfidf-9" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>10 0.05027514 <a title="88-tfidf-10" href="./jmlr-2013-Variational_Inference_in_Nonconjugate_Models.html">121 jmlr-2013-Variational Inference in Nonconjugate Models</a></p>
<p>11 0.049334012 <a title="88-tfidf-11" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>12 0.048470661 <a title="88-tfidf-12" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>13 0.047769319 <a title="88-tfidf-13" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>14 0.044432059 <a title="88-tfidf-14" href="./jmlr-2013-On_the_Convergence_of_Maximum_Variance_Unfolding.html">77 jmlr-2013-On the Convergence of Maximum Variance Unfolding</a></p>
<p>15 0.043552916 <a title="88-tfidf-15" href="./jmlr-2013-On_the_Mutual_Nearest_Neighbors_Estimate_in_Regression.html">79 jmlr-2013-On the Mutual Nearest Neighbors Estimate in Regression</a></p>
<p>16 0.04125208 <a title="88-tfidf-16" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>17 0.03869009 <a title="88-tfidf-17" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<p>18 0.038629018 <a title="88-tfidf-18" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>19 0.037884831 <a title="88-tfidf-19" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>20 0.036816109 <a title="88-tfidf-20" href="./jmlr-2013-Optimal_Discovery_with_Probabilistic_Expert_Advice%3A_Finite_Time_Analysis_and_Macroscopic_Optimality.html">81 jmlr-2013-Optimal Discovery with Probabilistic Expert Advice: Finite Time Analysis and Macroscopic Optimality</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.217), (1, -0.265), (2, 0.137), (3, -0.049), (4, 0.155), (5, -0.179), (6, -0.294), (7, -0.065), (8, -0.222), (9, 0.066), (10, -0.117), (11, 0.061), (12, 0.159), (13, 0.025), (14, -0.036), (15, -0.002), (16, 0.009), (17, 0.076), (18, 0.037), (19, -0.01), (20, -0.005), (21, 0.084), (22, 0.015), (23, 0.082), (24, -0.011), (25, 0.148), (26, 0.034), (27, 0.03), (28, 0.062), (29, 0.103), (30, -0.035), (31, -0.102), (32, -0.055), (33, -0.079), (34, 0.118), (35, -0.057), (36, -0.037), (37, -0.027), (38, -0.057), (39, -0.034), (40, -0.018), (41, -0.058), (42, 0.027), (43, -0.081), (44, -0.061), (45, -0.089), (46, -0.0), (47, 0.073), (48, 0.05), (49, -0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94144142 <a title="88-lsi-1" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian ﬁeld, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model’s partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. Through the expansion, we see that EP is correct to ﬁrst order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. Keywords: expectation consistent inference, expectation propagation, perturbation correction, Wick expansions, Ising model, Gaussian process</p><p>2 0.79185021 <a title="88-lsi-2" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>3 0.57835841 <a title="88-lsi-3" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>Author: Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Pierre Dupont</p><p>Abstract: We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efﬁciently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about speciﬁc groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy. Keywords: group feature selection, generalized spike-and-slab priors, expectation propagation, sparse linear model, approximate inference, sequential experimental design, signal reconstruction</p><p>4 0.48460731 <a title="88-lsi-4" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>5 0.35977808 <a title="88-lsi-5" href="./jmlr-2013-Universal_Consistency_of_Localized_Versions_of_Regularized_Kernel_Methods.html">117 jmlr-2013-Universal Consistency of Localized Versions of Regularized Kernel Methods</a></p>
<p>Author: Robert Hable</p><p>Abstract: In supervised learning problems, global and local learning algorithms are used. In contrast to global learning algorithms, the prediction of a local learning algorithm in a testing point is only based on training data which are close to the testing point. Every global algorithm such as support vector machines (SVM) can be localized in the following way: in every testing point, the (global) learning algorithm is not applied to the whole training data but only to the k nearest neighbors (kNN) of the testing point. In case of support vector machines, the success of such mixtures of SVM and kNN (called SVM-KNN) has been shown in extensive simulation studies and also for real data sets but only little has been known on theoretical properties so far. In the present article, it is shown how a large class of regularized kernel methods (including SVM) can be localized in order to get a universally consistent learning algorithm. Keywords: machine learning, regularized kernel methods, localization, SVM, k-nearest neighbors, SVM-KNN</p><p>6 0.32105789 <a title="88-lsi-6" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>7 0.31612062 <a title="88-lsi-7" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>8 0.30748564 <a title="88-lsi-8" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>9 0.25593776 <a title="88-lsi-9" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>10 0.23915902 <a title="88-lsi-10" href="./jmlr-2013-On_the_Mutual_Nearest_Neighbors_Estimate_in_Regression.html">79 jmlr-2013-On the Mutual Nearest Neighbors Estimate in Regression</a></p>
<p>11 0.22940639 <a title="88-lsi-11" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>12 0.22867593 <a title="88-lsi-12" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>13 0.20206532 <a title="88-lsi-13" href="./jmlr-2013-Optimal_Discovery_with_Probabilistic_Expert_Advice%3A_Finite_Time_Analysis_and_Macroscopic_Optimality.html">81 jmlr-2013-Optimal Discovery with Probabilistic Expert Advice: Finite Time Analysis and Macroscopic Optimality</a></p>
<p>14 0.19871674 <a title="88-lsi-14" href="./jmlr-2013-Approximating_the_Permanent_with_Fractional_Belief_Propagation.html">13 jmlr-2013-Approximating the Permanent with Fractional Belief Propagation</a></p>
<p>15 0.1837206 <a title="88-lsi-15" href="./jmlr-2013-Pairwise_Likelihood_Ratios_for_Estimation_of_Non-Gaussian_Structural_Equation_Models.html">85 jmlr-2013-Pairwise Likelihood Ratios for Estimation of Non-Gaussian Structural Equation Models</a></p>
<p>16 0.17970262 <a title="88-lsi-16" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>17 0.17688942 <a title="88-lsi-17" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>18 0.17670856 <a title="88-lsi-18" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>19 0.16428863 <a title="88-lsi-19" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>20 0.15290463 <a title="88-lsi-20" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.028), (5, 0.118), (6, 0.033), (10, 0.067), (14, 0.016), (23, 0.038), (26, 0.344), (61, 0.019), (68, 0.012), (70, 0.032), (71, 0.015), (72, 0.026), (75, 0.066), (85, 0.021), (87, 0.017), (93, 0.018), (96, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7402516 <a title="88-lda-1" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian ﬁeld, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model’s partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. Through the expansion, we see that EP is correct to ﬁrst order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. Keywords: expectation consistent inference, expectation propagation, perturbation correction, Wick expansions, Ising model, Gaussian process</p><p>2 0.57698864 <a title="88-lda-2" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>Author: Sébastien Gerchinovitz</p><p>Abstract: We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T . We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same ﬂavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with ﬁxed design. Keywords: sparsity, online linear regression, individual sequences, adaptive regret bounds</p><p>3 0.42874452 <a title="88-lda-3" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>4 0.41572315 <a title="88-lda-4" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>5 0.41404307 <a title="88-lda-5" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>Author: Qiang Liu, Alexander Ihler</p><p>Abstract: The marginal maximum a posteriori probability (MAP) estimation problem, which calculates the mode of the marginal posterior distribution of a subset of variables with the remaining variables marginalized, is an important inference problem in many models, such as those with hidden variables or uncertain parameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has attracted less attention in the literature compared to the joint MAP (maximization) and marginalization problems. We derive a general dual representation for marginal MAP that naturally integrates the marginalization and maximization operations into a joint variational optimization problem, making it possible to easily extend most or all variational-based algorithms to marginal MAP. In particular, we derive a set of “mixed-product” message passing algorithms for marginal MAP, whose form is a hybrid of max-product, sum-product and a novel “argmax-product” message updates. We also derive a class of convergent algorithms based on proximal point methods, including one that transforms the marginal MAP problem into a sequence of standard marginalization problems. Theoretically, we provide guarantees under which our algorithms give globally or locally optimal solutions, and provide novel upper bounds on the optimal objectives. Empirically, we demonstrate that our algorithms signiﬁcantly outperform the existing approaches, including a state-of-the-art algorithm based on local search methods. Keywords: graphical models, message passing, belief propagation, variational methods, maximum a posteriori, marginal-MAP, hidden variable models</p><p>6 0.41176417 <a title="88-lda-6" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>7 0.40815726 <a title="88-lda-7" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>8 0.40530068 <a title="88-lda-8" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>9 0.40425059 <a title="88-lda-9" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>10 0.40289465 <a title="88-lda-10" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>11 0.4017894 <a title="88-lda-11" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>12 0.40140837 <a title="88-lda-12" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>13 0.40126672 <a title="88-lda-13" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>14 0.39888194 <a title="88-lda-14" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>15 0.3981697 <a title="88-lda-15" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>16 0.39801589 <a title="88-lda-16" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>17 0.39783096 <a title="88-lda-17" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>18 0.39753723 <a title="88-lda-18" href="./jmlr-2013-Similarity-based_Clustering_by_Left-Stochastic_Matrix_Factorization.html">100 jmlr-2013-Similarity-based Clustering by Left-Stochastic Matrix Factorization</a></p>
<p>19 0.39612564 <a title="88-lda-19" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>20 0.39495239 <a title="88-lda-20" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
