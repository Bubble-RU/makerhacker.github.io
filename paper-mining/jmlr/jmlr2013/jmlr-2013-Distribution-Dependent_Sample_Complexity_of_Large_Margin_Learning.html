<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-35" href="#">jmlr2013-35</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</h1>
<br/><p>Source: <a title="jmlr-2013-35-pdf" href="http://jmlr.org/papers/volume14/sabato13a/sabato13a.pdf">pdf</a></p><p>Author: Sivan Sabato, Nathan Srebro, Naftali Tishby</p><p>Abstract: We obtain a tight distribution-speciﬁc characterization of the sample complexity of large-margin classiﬁcation with L2 regularization: We introduce the margin-adapted dimension, which is a simple function of the second order statistics of the data distribution, and show distribution-speciﬁc upper and lower bounds on the sample complexity, both governed by the margin-adapted dimension of the data distribution. The upper bounds are universal, and the lower bounds hold for the rich family of sub-Gaussian distributions with independent features. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classiﬁcation. To prove the lower bound, we develop several new tools of independent interest. These include new connections between shattering and hardness of learning, new properties of shattering with linear classiﬁers, and a new lower bound on the smallest eigenvalue of a random Gram matrix generated by sub-Gaussian variables. Our results can be used to quantitatively compare large margin learning to other learning rules, and to improve the effectiveness of methods that use sample complexity bounds, such as active learning. Keywords: supervised learning, sample complexity, linear classiﬁers, distribution-dependence</p><p>Reference: <a title="jmlr-2013-35-reference" href="../jmlr2013_reference/jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The upper bounds are universal, and the lower bounds hold for the rich family of sub-Gaussian distributions with independent features. [sent-10, score-0.227]
</p><p>2 These include new connections between shattering and hardness of learning, new properties of shattering with linear classiﬁers, and a new lower bound on the smallest eigenvalue of a random Gram matrix generated by sub-Gaussian variables. [sent-13, score-0.378]
</p><p>3 Our results can be used to quantitatively compare large margin learning to other learning rules, and to improve the effectiveness of methods that use sample complexity bounds, such as active learning. [sent-14, score-0.257]
</p><p>4 Introduction In this paper we pursue a tight characterization of the sample complexity of learning a classiﬁer, under a particular data distribution, and using a particular learning rule. [sent-16, score-0.196]
</p><p>5 If the VCdimension of the hypothesis class, when restricted to this subset, is smaller than d, then learning with respect to this distribution will require less examples than the upper bound predicts. [sent-24, score-0.249]
</p><p>6 Of course, some sample complexity upper bounds are known to be tight or to have an almostmatching lower bound. [sent-25, score-0.335]
</p><p>7 For instance, the VC-dimension upper bound is tight (Vapnik and Chervonenkis, 1974). [sent-26, score-0.203]
</p><p>8 This means that there exists some data distribution in the class covered by the upper bound, for which this bound cannot be improved. [sent-27, score-0.205]
</p><p>9 But it does not imply that the upper bound characterizes the true sample complexity for every speciﬁc distribution in the class. [sent-29, score-0.297]
</p><p>10 d This implies a sample complexity upper bound of O ε2 using any MEM algorithm, where ε is the excess error relative to the optimal margin error. [sent-34, score-0.427]
</p><p>11 1 We also have that the sample complexity of 2 any MEM algorithm is at most O γBε2 , where B2 is the average squared norm of the data and γ 2 is the size of the margin (Bartlett and Mendelson, 2002). [sent-35, score-0.252]
</p><p>12 2 However, the VC-dimension upper bound indicates, for instance, that if a distribution induces a large average norm but is supported by a low-dimensional sub-space, then the true number of examples required to reach a low error is much smaller. [sent-40, score-0.198]
</p><p>13 Thus, neither of these upper bounds fully describes the sample complexity of MEM for a speciﬁc distribution. [sent-41, score-0.226]
</p><p>14 We obtain a tight distribution-speciﬁc characterization of the sample complexity of large-margin learning for a rich class of distributions. [sent-42, score-0.233]
</p><p>15 The upper bound is universal, and the lower bound holds for a rich class of distributions with independent features. [sent-44, score-0.342]
</p><p>16 The margin-adapted dimension reﬁnes both the dimension and the average norm of the data distribution, and can be easily calculated from the covariance matrix and the mean of the distribution. [sent-45, score-0.2]
</p><p>17 Our sample-complexity upper bound shows ˜ kγ that O( ε2 ) examples sufﬁce in order to learn any distribution with a margin-adapted dimension of kγ using a MEM algorithm with margin γ. [sent-47, score-0.298]
</p><p>18 Our main result shows the following matching distribution-speciﬁc upper and lower bounds on the sample complexity of MEM: ˜ kγ (D) . [sent-53, score-0.268]
</p><p>19 Ω(kγ (D)) ≤ m(ε, γ, D) ≤ O ε2  (1)  Our tight characterization, and in particular the distribution-speciﬁc lower bound on the sample complexity that we establish, can be used to compare large-margin (L2 regularized) learning to other learning rules. [sent-54, score-0.322]
</p><p>20 We provide two such examples: we use our lower bound to rigorously establish a sample complexity gap between L1 and L2 regularization previously studied in Ng (2004), and to show a large gap between discriminative and generative learning on a Gaussian-mixture distribution. [sent-55, score-0.255]
</p><p>21 The tight bounds can also be used for active learning algorithms in which sample-complexity bounds are used to decide on the next label to query. [sent-56, score-0.192]
</p><p>22 • Providing a new lower bound for the smallest eigenvalue of a random Gram matrix generated by sub-Gaussian variables. [sent-63, score-0.192]
</p><p>23 In Section 8 we show that any nontrivial sample-complexity lower bound for more general distributions must employ properties other than the covariance matrix of the distribution. [sent-71, score-0.265]
</p><p>24 This type of a lower bound does not, however, indicate much on the sample complexity of other distributions under the same set of assumptions. [sent-76, score-0.298]
</p><p>25 The essential condition is that the metric entropy of the hypothesis class with respect to the distribution be sub-linear in the limit of an inﬁnite sample size. [sent-79, score-0.206]
</p><p>26 Benedek and Itai (1991) show that if the distribution is known to the learner, a speciﬁc hypothesis class is learnable if and only if there is a ﬁnite ε-cover of this hypothesis class with respect to the distribution. [sent-83, score-0.268]
</p><p>27 (2008) consider a similar setting, and prove sample complexity lower bounds for learning with any data distribution, for some binary hypothesis classes on the real line. [sent-85, score-0.297]
</p><p>28 Vayatis and Azencott (1999) provide distribution-speciﬁc sample complexity upper bounds for hypothesis classes with a limited VC-dimension, as a function of how balanced the hypotheses are with respect to the considered distributions. [sent-86, score-0.307]
</p><p>29 As can be seen in Equation (1), we do not tightly characterize the dependence of the sample complexity on the desired error (as done, for example, in Steinwart and Scovel, 2007), thus our bounds are not tight for asymptotically small error levels. [sent-88, score-0.241]
</p><p>30 The margin error of a classiﬁer w with respect to a margin γ > 0 on D is ℓγ (h, D)  P(X,Y )∼D [Y · h(X) ≤ γ]. [sent-96, score-0.186]
</p><p>31 For a given hypothesis class H ⊆ {±1}X , the best achievable margin error on D is ℓ∗ (H , D) γ  inf ℓγ (h, D). [sent-97, score-0.211]
</p><p>32 h∈H  The distribution-speciﬁc sample complexity for MEM algorithms is the sample size required to guarantee low excess error for the given distribution. [sent-111, score-0.216]
</p><p>33 Preliminaries As mentioned above, for the hypothesis class of linear classiﬁers W , one can derive a samplecomplexity upper bound of the form O(B2 /γ2 ε2 ), where B2 = EX∼D [ X 2 ] and ε is the excess error relative to the γ-margin loss. [sent-136, score-0.323]
</p><p>34 m  (2)  To get the desired upper bound for linear classiﬁers we use the ramp loss, which is deﬁned as follows. [sent-149, score-0.612]
</p><p>35 γ2 m  Combining this with Proposition 3 we can conclude a sample complexity upper bound of O(B2 /γ2 ε2 ). [sent-159, score-0.265]
</p><p>36 The Margin-Adapted Dimension When considering learning of linear classiﬁers using MEM, the dimension-based upper bound and the norm-based upper bound are both tight in the worst-case sense, that is, they are the best bounds that rely only on the dimensionality or only on the norm respectively. [sent-175, score-0.414]
</p><p>37 Nonetheless, neither is tight in a distribution-speciﬁc sense: If the average norm is unbounded while the dimension is small, then there can be an arbitrarily large gap between the true distribution-dependent sample complexity and the bound that depends on the average norm. [sent-176, score-0.347]
</p><p>38 Trivially, this is an upper bound on the sample complexity as well. [sent-179, score-0.265]
</p><p>39 We will show that in such situations the sample complexity is characterized not by the minimum of dimension and norm, but by the sum of the number of high-variance dimensions and the average squared norm in the other directions. [sent-181, score-0.196]
</p><p>40 The eigenvalues of the empirical covariance matrix were used to provide sample complexity bounds, for instance in Sch¨ lkopf et al. [sent-195, score-0.253]
</p><p>41 A Distribution-Dependent Upper Bound In this section we prove an upper bound on the sample complexity of learning with MEM, using the margin-adapted dimension. [sent-202, score-0.265]
</p><p>42 We do this by providing a tighter upper bound for the Rademacher complexity of RAMPγ . [sent-203, score-0.209]
</p><p>43 left: norm bound is tight; middle: dimension bound is tight; right: neither bound is tight. [sent-215, score-0.319]
</p><p>44 The ﬁrst function class will be bounded because of the norm bound on the subspace V used in Deﬁnition 6, and the second function class will have a bounded pseudo-dimension. [sent-220, score-0.188]
</p><p>45 One should note that a similar upper bound can be obtained much more easily under a uniform upper bound on the eigenvalues of the uncentered covariance matrix. [sent-304, score-0.4]
</p><p>46 2 However, such an upper bound would not capture the fact that a ﬁnite dimension implies a ﬁnite sample complexity, regardless of the size of the covariance. [sent-305, score-0.229]
</p><p>47 If one wants to estimate the sample complexity, then large covariance matrix eigenvalues imply that more examples are required to estimate the covariance matrix from a sample. [sent-306, score-0.276]
</p><p>48 Moreover, estimating the covariance matrix is not necessary to achieve the sample complexity, since the upper bound holds for any margin-error minimization algorithm. [sent-308, score-0.288]
</p><p>49 A Distribution-Dependent Lower Bound The new upper bound presented in Corollary 12 can be tighter than both the norm-only and the dimension-only upper bounds. [sent-310, score-0.188]
</p><p>50 But does the margin-adapted dimension characterize the true sample complexity of the distribution, or is it just another upper bound? [sent-311, score-0.218]
</p><p>51 1 relates fat-shattering with a lower bound on sample complexity. [sent-314, score-0.182]
</p><p>52 2 we use this result to relate the smallest eigenvalue of a Gram-matrix to a lower bound on sample complexity. [sent-316, score-0.212]
</p><p>53 In contrast, the average Rademacher complexity cannot be used to derive general lower bounds for MEM algorithms, since it is related to the rate of uniform convergence of the entire hypothesis class, while MEM algorithms choose low-error hypotheses (see, e. [sent-354, score-0.241]
</p><p>54 However, any learning algorithm that returns a hypothesis from the hypothesis class will incur zero error on this distribution. [sent-369, score-0.199]
</p><p>55 Thus 1 by Lemma 16 there exists a wy such that Ywy = y and wy ≤ wy ≤ 1. [sent-416, score-0.198]
</p><p>56 Corollary 17 generalizes the requirement of linear independence for shattering with no margin: A set of vectors is shattered with no margin if the vectors are linearly independent, that is if λmin > 0. [sent-426, score-0.236]
</p><p>57 3 Sub-Gaussian Distributions In order to derive a lower bound on distribution-speciﬁc sample complexity in terms of the covariance of X ∼ DX , we must assume that X is not too heavy-tailed. [sent-440, score-0.315]
</p><p>58 In this work we further say that X is sub-Gaussian with relative moment ρ > 0 if X is sub-Gaussian with moment ρ E[X 2 ], that is, ∀t ∈ R,  E[exp(tX)] ≤ exp(t 2 ρ2 E[X 2 ]/2). [sent-449, score-0.324]
</p><p>59 Note that a sub-Gaussian variable with moment B and relative moment ρ is also sub-Gaussian with moment B′ and relative moment ρ′ for any B′ ≥ B and ρ′ ≥ ρ. [sent-450, score-0.648]
</p><p>60 Speciﬁcally, if X is a mean-zero Gaussian random variable, X ∼ N(0, σ2 ), then X is sub-Gaussian with relative moment 1 and the inequalities in the deﬁnition above hold with equality. [sent-452, score-0.181]
</p><p>61 As another example, if X is a uniform random variable over {±b} for some b ≥ 0, then X is sub-Gaussian with relative moment 1, since 1 E[exp(tX)] = (exp(tb) + exp(−tb)) ≤ exp(t 2 b2 /2) = exp(t 2 E[X 2 ]/2). [sent-453, score-0.181]
</p><p>62 The following lemma provides a useful connection between the trace of the sub-Gaussian moment matrix and the moment-generating function of the squared norm of the random vector. [sent-456, score-0.37]
</p><p>63 Deﬁnition 21 (Sub-Gaussian product distributions) A distribution DX over Rd is a sub-Gaussian product distribution with moment B and relative moment ρ if there exists some orthonormal basis a1 , . [sent-462, score-0.388]
</p><p>64 , ad ∈ Rd , such that for X ∼ DX , ai , X are independent sub-Gaussian random variables, each with moment B and relative moment ρ. [sent-465, score-0.324]
</p><p>65 Note that a sub-Gaussian product distribution has mean zero, thus its covariance matrix is equal to its sg uncentered covariance matrix. [sent-466, score-0.308]
</p><p>66 For any ﬁxed ρ ≥ 0, we denote by Dρ the family of all sub-Gaussian product distributions with relative moment ρ, in arbitrary dimension. [sent-467, score-0.224]
</p><p>67 For instance, all multivariate Gaussian distributions and all uniform distributions on the corners of a centered hyper-rectangle sg sg are in D1 . [sent-468, score-0.246]
</p><p>68 sg We will provide a lower bound for all distributions in Dρ . [sent-471, score-0.249]
</p><p>69 This lower bound is linear in the margin-adapted dimension of the distribution, thus it matches the upper bound provided in Corollary 12. [sent-472, score-0.299]
</p><p>70 2, to obtain a sample complexity lower bound it sufﬁces to have a lower bound on the value of the smallest eigenvalue of a random Gram matrix. [sent-476, score-0.411]
</p><p>71 In this case, then, the sample complexity lower bound is indeed the same order as kγ , which controls also the upper bound in Corollary 12. [sent-494, score-0.391]
</p><p>72 sg For any DX ∈ Dρ with covariance matrix Σ ≤ I, and for any m ≤ β · trace(Σ) −C, if X is the m × d matrix of a sample drawn from Dm , then X P[λmin (XXT ) ≥ m] ≥ δ. [sent-504, score-0.268]
</p><p>73 sg Theorem 24 (Sample complexity lower bound for distributions in Dρ ) For any ρ > 0 there are sg constants β > 0,C ≥ 0 such that for any D with DX ∈ Dρ , for any γ > 0 and for any ε < 1 − ℓ∗ (D), γ 2  m(ε, γ, D, 1/4) ≥ βkγ (DX ) −C. [sent-505, score-0.402]
</p><p>74 By our assumptions on DX , for all i ∈ [d] the random variable X[i] is sub-Gaussian with relative moment ρ. [sent-535, score-0.181]
</p><p>75 Z[i] is also sub-Gaussian with relative moment ρ, and E[Z[i]2 ] = 1. [sent-537, score-0.181]
</p><p>76 Z[i] is sub-Gaussian with relative moment ρ and E[Z[i]2 ] ≤ 1. [sent-561, score-0.181]
</p><p>77 On the Limitations of the Covariance Matrix We have shown matching upper and lower bounds for the sample complexity of learning with MEM, for any sub-Gaussian product distribution with a bounded relative moment. [sent-581, score-0.338]
</p><p>78 Both DX and PX are sub-Gaussian random vectors, with a relative moment of 2 in all directions. [sent-586, score-0.181]
</p><p>79 2 By Equation (9), PX , Da and Db are all sub-Gaussian product distribution with relative moment √ 1, thus also with moment 2 > 1. [sent-595, score-0.356]
</p><p>80 Conclusions Corollary 12 and Theorem 24 together provide a tight characterization of the sample complexity of any sub-Gaussian product distribution with a bounded relative moment. [sent-604, score-0.266]
</p><p>81 ε2  (10)  The upper bound holds uniformly for all distributions, and the constants in the lower bound depend only on ρ. [sent-607, score-0.262]
</p><p>82 An interesting conclusion can be drawn as to the inﬂuence of the conditional distribution of labels DY |X : Since Equation (10) holds for any DY |X , the effect of the direction of the best separator on the sample complexity is bounded, even for highly non-spherical distributions. [sent-609, score-0.206]
</p><p>83 There are upper bounds that depend on the margin alone and on the dimension alone without logarithmic factors. [sent-611, score-0.227]
</p><p>84 Equation (10) can be used to easily characterize the sample complexity behavior for interesting distributions, to compare L2 margin minimization to other learning methods, and to improve certain active learning strategies. [sent-614, score-0.257]
</p><p>85 When X ∞ ≤ 1, upper bounds on learning with L1 regularization guarantee a sample complexity of O(ln(d)) for an L1 -based learning rule (Zhang, 2002). [sent-617, score-0.226]
</p><p>86 In order to compare this with the sample complexity of L2 regularized learning and establish a gap, one must use a lower bound on the L2 sample complexity. [sent-618, score-0.311]
</p><p>87 For instance, if each coordinate is a bounded independent sub-Gaussian random variable with a bounded relative moment, we have k1 = ⌈d/2⌉ and Theorem 24 implies a lower bound of Ω(d) on the L2 sample complexity. [sent-621, score-0.22]
</p><p>88 A popular approach to active learning involves estimating the current set of possible classiﬁers using sample complexity upper bounds (see, e. [sent-632, score-0.261]
</p><p>89 Thus, our sample complexity upper bounds can be used to improve the active learner’s label complexity. [sent-640, score-0.261]
</p><p>90 Moreover, the lower bound suggests that any further improvement of such active learning strategies would require more information other than the distribution’s covariance matrix. [sent-641, score-0.221]
</p><p>91 The ﬁrst inequality follows since the ramp loss is upper bounded by the margin loss. [sent-657, score-0.621]
</p><p>92 Therefore for all y ∈ {±1}k there exists a zy ∈ Z such that ∀i ∈ [k], sign( f (xi ) + zy (xi ) − f (xi ) − r[i]) = y[i]. [sent-678, score-0.238]
</p><p>93 If y[i] = 1 then f (xi ) + zy (xi ) − f (xi ) − r[i] > 0 It follows that f (xi ) + zy (xi ) > f (xi ) + r[i] > 0, thus f (xi ) + zy (xi ) > f (xi ) + r[i]. [sent-683, score-0.357]
</p><p>94 It follows that f (xi ) + zy (xi ) < f (xi ) + r[i] < 1, thus f (xi ) + zy (xi ) < f (xi ) + r[i]. [sent-686, score-0.238]
</p><p>95 For each y ∈ {±1}m , select ry ∈ Rm such that for all i ∈ [m], ry [i]y[i] ≥ γ. [sent-697, score-0.238]
</p><p>96 Thus, for all y ∈ {±1} there exist αy , βy ≥ 0 such that ¯ ¯ ∑y∈Y+ αy = ∑y∈Y− βy = 1, and z= ¯  ¯ ¯ ∑ αy ry = ∑ βy ry . [sent-715, score-0.238]
</p><p>97 y∈Y−  y∈Y+  Let za = ∑y∈Y+ αy ry and zb = ∑y∈Y− βy ry We have that ∀y ∈ Y+ , ry [m] ≥ γ, and ∀y ∈ Y− , ry [m] ≤ −γ. [sent-716, score-0.571]
</p><p>98 Since S is shattered, for any y ∈ {±1}m there is an ry ∈ L such that ∀i ∈ [m], ry [i]y[i] ≥ γ. [sent-726, score-0.238]
</p><p>99 5 Proof of Lemma 20 Proof [of Lemma 20] It sufﬁces to consider diagonal moment matrices: If B is not diagonal, let V ∈ Rd×d be an orthogonal matrix such that VBVT is diagonal, and let Y = VX. [sent-731, score-0.257]
</p><p>100 2  (16)  To bound the second term in line (15), since Yi j are sub-Gaussian with moment ρ, E[Y4j ] ≤ 5ρ4 i (Buldygin and Kozachenko, 1998, Lemma 1. [sent-876, score-0.227]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ramp', 0.476), ('mem', 0.298), ('xxt', 0.254), ('ishby', 0.174), ('istribution', 0.174), ('rebro', 0.174), ('ln', 0.165), ('ependent', 0.149), ('argin', 0.149), ('abato', 0.149), ('omplexity', 0.149), ('dx', 0.145), ('moment', 0.143), ('ry', 0.119), ('zy', 0.119), ('arge', 0.109), ('ample', 0.109), ('exp', 0.109), ('trace', 0.099), ('yx', 0.099), ('rd', 0.098), ('margin', 0.093), ('shattering', 0.093), ('rx', 0.089), ('bound', 0.084), ('hypothesis', 0.081), ('sg', 0.08), ('ov', 0.079), ('cm', 0.079), ('sm', 0.075), ('conv', 0.074), ('complexity', 0.073), ('sx', 0.073), ('tight', 0.067), ('rademacher', 0.066), ('bd', 0.066), ('wy', 0.066), ('lemma', 0.062), ('ex', 0.061), ('earning', 0.06), ('covariance', 0.06), ('sabato', 0.06), ('rm', 0.059), ('yt', 0.058), ('rudelson', 0.058), ('sample', 0.056), ('upper', 0.052), ('shattered', 0.05), ('wb', 0.05), ('yyt', 0.049), ('zb', 0.049), ('gram', 0.049), ('dm', 0.048), ('argminh', 0.046), ('vbvt', 0.046), ('za', 0.046), ('bounds', 0.045), ('xt', 0.045), ('origin', 0.045), ('separator', 0.045), ('distributions', 0.043), ('lower', 0.042), ('bartlett', 0.041), ('hausdorff', 0.041), ('xi', 0.04), ('uncentered', 0.04), ('witness', 0.04), ('nm', 0.039), ('let', 0.039), ('psd', 0.039), ('relative', 0.038), ('vapnik', 0.038), ('ers', 0.038), ('dimension', 0.037), ('class', 0.037), ('theorem', 0.037), ('fg', 0.036), ('learnability', 0.036), ('db', 0.036), ('sivan', 0.036), ('matrix', 0.036), ('active', 0.035), ('ps', 0.035), ('buldygin', 0.035), ('yw', 0.035), ('mendelson', 0.034), ('invertible', 0.034), ('covering', 0.032), ('xm', 0.032), ('distribution', 0.032), ('excess', 0.031), ('px', 0.031), ('argmin', 0.03), ('norm', 0.03), ('eigenvalue', 0.03), ('min', 0.03), ('sst', 0.03), ('proof', 0.029), ('classi', 0.029), ('eigenvalues', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="35-tfidf-1" href="./jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning.html">35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</a></p>
<p>Author: Sivan Sabato, Nathan Srebro, Naftali Tishby</p><p>Abstract: We obtain a tight distribution-speciﬁc characterization of the sample complexity of large-margin classiﬁcation with L2 regularization: We introduce the margin-adapted dimension, which is a simple function of the second order statistics of the data distribution, and show distribution-speciﬁc upper and lower bounds on the sample complexity, both governed by the margin-adapted dimension of the data distribution. The upper bounds are universal, and the lower bounds hold for the rich family of sub-Gaussian distributions with independent features. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classiﬁcation. To prove the lower bound, we develop several new tools of independent interest. These include new connections between shattering and hardness of learning, new properties of shattering with linear classiﬁers, and a new lower bound on the smallest eigenvalue of a random Gram matrix generated by sub-Gaussian variables. Our results can be used to quantitatively compare large margin learning to other learning rules, and to improve the effectiveness of methods that use sample complexity bounds, such as active learning. Keywords: supervised learning, sample complexity, linear classiﬁers, distribution-dependence</p><p>2 0.13139461 <a title="35-tfidf-2" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>Author: Sébastien Gerchinovitz</p><p>Abstract: We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T . We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same ﬂavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with ﬁxed design. Keywords: sparsity, online linear regression, individual sequences, adaptive regret bounds</p><p>3 0.12812057 <a title="35-tfidf-3" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>Author: Indraneel Mukherjee, Cynthia Rudin, Robert E. Schapire</p><p>Abstract: The AdaBoost algorithm was designed to combine many “weak” hypotheses that perform slightly better than random guessing into a “strong” hypothesis that has very low error. We study the rate at which AdaBoost iteratively converges to the minimum of the “exponential loss.” Unlike previous work, our proofs do not require a weak-learning assumption, nor do they require that minimizers of the exponential loss are ﬁnite. Our ﬁrst result shows that the exponential loss of AdaBoost’s computed parameter vector will be at most ε more than that of any parameter vector of ℓ1 -norm bounded by B in a number of rounds that is at most a polynomial in B and 1/ε. We also provide lower bounds showing that a polynomial dependence is necessary. Our second result is that within C/ε iterations, AdaBoost achieves a value of the exponential loss that is at most ε more than the best possible value, where C depends on the data set. We show that this dependence of the rate on ε is optimal up to constant factors, that is, at least Ω(1/ε) rounds are necessary to achieve within ε of the optimal exponential loss. Keywords: AdaBoost, optimization, coordinate descent, convergence rate</p><p>4 0.10678474 <a title="35-tfidf-4" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>Author: Alon Gonen, Sivan Sabato, Shai Shalev-Shwartz</p><p>Abstract: We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efﬁcient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches. Our efﬁcient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in signiﬁcantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings. Keywords: active learning, linear classiﬁers, margin, adaptive sub-modularity</p><p>5 0.085158519 <a title="35-tfidf-5" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>Author: Tingni Sun, Cun-Hui Zhang</p><p>Abstract: We propose a new method of learning a sparse nonnegative-deﬁnite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm ﬁrst estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other ℓ1 regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the ℓ1 and spectrum norms of the target inverse matrix diverges to inﬁnity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso. Keywords: precision matrix, concentration matrix, inverse matrix, graphical model, scaled Lasso, linear regression, spectrum norm</p><p>6 0.078946285 <a title="35-tfidf-6" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>7 0.071074739 <a title="35-tfidf-7" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>8 0.061164327 <a title="35-tfidf-8" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>9 0.058266461 <a title="35-tfidf-9" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>10 0.057138912 <a title="35-tfidf-10" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>11 0.053801466 <a title="35-tfidf-11" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>12 0.052981269 <a title="35-tfidf-12" href="./jmlr-2013-Risk_Bounds_of_Learning_Processes_for_L%C3%A9vy_Processes.html">97 jmlr-2013-Risk Bounds of Learning Processes for Lévy Processes</a></p>
<p>13 0.051370848 <a title="35-tfidf-13" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>14 0.049396474 <a title="35-tfidf-14" href="./jmlr-2013-On_the_Convergence_of_Maximum_Variance_Unfolding.html">77 jmlr-2013-On the Convergence of Maximum Variance Unfolding</a></p>
<p>15 0.048604209 <a title="35-tfidf-15" href="./jmlr-2013-Manifold_Regularization_and_Semi-supervised_Learning%3A_Some_Theoretical_Analyses.html">69 jmlr-2013-Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses</a></p>
<p>16 0.047048487 <a title="35-tfidf-16" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>17 0.046492621 <a title="35-tfidf-17" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>18 0.04588747 <a title="35-tfidf-18" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>19 0.044266209 <a title="35-tfidf-19" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>20 0.043854021 <a title="35-tfidf-20" href="./jmlr-2013-A_Theory_of_Multiclass_Boosting.html">8 jmlr-2013-A Theory of Multiclass Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.236), (1, 0.127), (2, 0.084), (3, 0.096), (4, -0.043), (5, -0.179), (6, 0.114), (7, -0.081), (8, -0.039), (9, -0.047), (10, 0.06), (11, 0.08), (12, -0.019), (13, -0.019), (14, -0.017), (15, 0.08), (16, 0.003), (17, -0.086), (18, 0.039), (19, -0.025), (20, -0.011), (21, 0.089), (22, 0.027), (23, 0.158), (24, 0.021), (25, 0.055), (26, -0.118), (27, -0.26), (28, -0.028), (29, -0.024), (30, 0.178), (31, 0.022), (32, 0.071), (33, 0.141), (34, 0.119), (35, -0.026), (36, -0.06), (37, 0.145), (38, -0.13), (39, 0.084), (40, 0.064), (41, 0.036), (42, 0.03), (43, 0.136), (44, -0.08), (45, 0.07), (46, -0.068), (47, 0.079), (48, 0.009), (49, 0.11)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92630476 <a title="35-lsi-1" href="./jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning.html">35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</a></p>
<p>Author: Sivan Sabato, Nathan Srebro, Naftali Tishby</p><p>Abstract: We obtain a tight distribution-speciﬁc characterization of the sample complexity of large-margin classiﬁcation with L2 regularization: We introduce the margin-adapted dimension, which is a simple function of the second order statistics of the data distribution, and show distribution-speciﬁc upper and lower bounds on the sample complexity, both governed by the margin-adapted dimension of the data distribution. The upper bounds are universal, and the lower bounds hold for the rich family of sub-Gaussian distributions with independent features. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classiﬁcation. To prove the lower bound, we develop several new tools of independent interest. These include new connections between shattering and hardness of learning, new properties of shattering with linear classiﬁers, and a new lower bound on the smallest eigenvalue of a random Gram matrix generated by sub-Gaussian variables. Our results can be used to quantitatively compare large margin learning to other learning rules, and to improve the effectiveness of methods that use sample complexity bounds, such as active learning. Keywords: supervised learning, sample complexity, linear classiﬁers, distribution-dependence</p><p>2 0.72586828 <a title="35-lsi-2" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>Author: Alon Gonen, Sivan Sabato, Shai Shalev-Shwartz</p><p>Abstract: We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efﬁcient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches. Our efﬁcient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in signiﬁcantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings. Keywords: active learning, linear classiﬁers, margin, adaptive sub-modularity</p><p>3 0.56338292 <a title="35-lsi-3" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>Author: Sébastien Gerchinovitz</p><p>Abstract: We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T . We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same ﬂavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with ﬁxed design. Keywords: sparsity, online linear regression, individual sequences, adaptive regret bounds</p><p>4 0.52947432 <a title="35-lsi-4" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>Author: Indraneel Mukherjee, Cynthia Rudin, Robert E. Schapire</p><p>Abstract: The AdaBoost algorithm was designed to combine many “weak” hypotheses that perform slightly better than random guessing into a “strong” hypothesis that has very low error. We study the rate at which AdaBoost iteratively converges to the minimum of the “exponential loss.” Unlike previous work, our proofs do not require a weak-learning assumption, nor do they require that minimizers of the exponential loss are ﬁnite. Our ﬁrst result shows that the exponential loss of AdaBoost’s computed parameter vector will be at most ε more than that of any parameter vector of ℓ1 -norm bounded by B in a number of rounds that is at most a polynomial in B and 1/ε. We also provide lower bounds showing that a polynomial dependence is necessary. Our second result is that within C/ε iterations, AdaBoost achieves a value of the exponential loss that is at most ε more than the best possible value, where C depends on the data set. We show that this dependence of the rate on ε is optimal up to constant factors, that is, at least Ω(1/ε) rounds are necessary to achieve within ε of the optimal exponential loss. Keywords: AdaBoost, optimization, coordinate descent, convergence rate</p><p>5 0.41735244 <a title="35-lsi-5" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>Author: Tingni Sun, Cun-Hui Zhang</p><p>Abstract: We propose a new method of learning a sparse nonnegative-deﬁnite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm ﬁrst estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other ℓ1 regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the ℓ1 and spectrum norms of the target inverse matrix diverges to inﬁnity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso. Keywords: precision matrix, concentration matrix, inverse matrix, graphical model, scaled Lasso, linear regression, spectrum norm</p><p>6 0.4078896 <a title="35-lsi-6" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>7 0.3940388 <a title="35-lsi-7" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>8 0.38829949 <a title="35-lsi-8" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>9 0.36602953 <a title="35-lsi-9" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>10 0.35232311 <a title="35-lsi-10" href="./jmlr-2013-Risk_Bounds_of_Learning_Processes_for_L%C3%A9vy_Processes.html">97 jmlr-2013-Risk Bounds of Learning Processes for Lévy Processes</a></p>
<p>11 0.34960356 <a title="35-lsi-11" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>12 0.34071577 <a title="35-lsi-12" href="./jmlr-2013-Kernel_Bayes%27_Rule%3A_Bayesian_Inference_with_Positive_Definite_Kernels.html">57 jmlr-2013-Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels</a></p>
<p>13 0.32319745 <a title="35-lsi-13" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<p>14 0.32211724 <a title="35-lsi-14" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>15 0.31379828 <a title="35-lsi-15" href="./jmlr-2013-Learning_Theory_Analysis_for_Association_Rules_and_Sequential_Event_Prediction.html">61 jmlr-2013-Learning Theory Analysis for Association Rules and Sequential Event Prediction</a></p>
<p>16 0.30772287 <a title="35-lsi-16" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>17 0.28718624 <a title="35-lsi-17" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>18 0.27770624 <a title="35-lsi-18" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>19 0.27714932 <a title="35-lsi-19" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>20 0.2769942 <a title="35-lsi-20" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.012), (5, 0.119), (6, 0.021), (10, 0.05), (20, 0.01), (23, 0.021), (68, 0.012), (70, 0.034), (75, 0.03), (85, 0.558), (87, 0.019), (93, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89353848 <a title="35-lda-1" href="./jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning.html">35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</a></p>
<p>Author: Sivan Sabato, Nathan Srebro, Naftali Tishby</p><p>Abstract: We obtain a tight distribution-speciﬁc characterization of the sample complexity of large-margin classiﬁcation with L2 regularization: We introduce the margin-adapted dimension, which is a simple function of the second order statistics of the data distribution, and show distribution-speciﬁc upper and lower bounds on the sample complexity, both governed by the margin-adapted dimension of the data distribution. The upper bounds are universal, and the lower bounds hold for the rich family of sub-Gaussian distributions with independent features. We conclude that this new quantity tightly characterizes the true sample complexity of large-margin classiﬁcation. To prove the lower bound, we develop several new tools of independent interest. These include new connections between shattering and hardness of learning, new properties of shattering with linear classiﬁers, and a new lower bound on the smallest eigenvalue of a random Gram matrix generated by sub-Gaussian variables. Our results can be used to quantitatively compare large margin learning to other learning rules, and to improve the effectiveness of methods that use sample complexity bounds, such as active learning. Keywords: supervised learning, sample complexity, linear classiﬁers, distribution-dependence</p><p>2 0.88839531 <a title="35-lda-2" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>Author: Xin Tong</p><p>Abstract: The Neyman-Pearson (NP) paradigm in binary classiﬁcation treats type I and type II errors with different priorities. It seeks classiﬁers that minimize type II error, subject to a type I error constraint under a user speciﬁed level α. In this paper, plug-in classiﬁers are developed under the NP paradigm. Based on the fundamental Neyman-Pearson Lemma, we propose two related plug-in classiﬁers which amount to thresholding respectively the class conditional density ratio and the regression function. These two classiﬁers handle different sampling schemes. This work focuses on theoretical properties of the proposed classiﬁers; in particular, we derive oracle inequalities that can be viewed as ﬁnite sample versions of risk bounds. NP classiﬁcation can be used to address anomaly detection problems, where asymmetry in errors is an intrinsic property. As opposed to a common practice in anomaly detection that consists of thresholding normal class density, our approach does not assume a speciﬁc form for anomaly distributions. Such consideration is particularly necessary when the anomaly class density is far from uniformly distributed. Keywords: plug-in approach, Neyman-Pearson paradigm, nonparametric statistics, oracle inequality, anomaly detection</p><p>3 0.53578305 <a title="35-lda-3" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>Author: Alon Gonen, Sivan Sabato, Shai Shalev-Shwartz</p><p>Abstract: We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efﬁcient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches. Our efﬁcient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in signiﬁcantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings. Keywords: active learning, linear classiﬁers, margin, adaptive sub-modularity</p><p>4 0.51346713 <a title="35-lda-4" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>Author: Sébastien Gerchinovitz</p><p>Abstract: We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T . We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same ﬂavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with ﬁxed design. Keywords: sparsity, online linear regression, individual sequences, adaptive regret bounds</p><p>5 0.48720926 <a title="35-lda-5" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>Author: Tingni Sun, Cun-Hui Zhang</p><p>Abstract: We propose a new method of learning a sparse nonnegative-deﬁnite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm ﬁrst estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other ℓ1 regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the ℓ1 and spectrum norms of the target inverse matrix diverges to inﬁnity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso. Keywords: precision matrix, concentration matrix, inverse matrix, graphical model, scaled Lasso, linear regression, spectrum norm</p><p>6 0.47407249 <a title="35-lda-6" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>7 0.44484532 <a title="35-lda-7" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<p>8 0.44309419 <a title="35-lda-8" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>9 0.43471611 <a title="35-lda-9" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>10 0.42415702 <a title="35-lda-10" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>11 0.4234243 <a title="35-lda-11" href="./jmlr-2013-On_the_Mutual_Nearest_Neighbors_Estimate_in_Regression.html">79 jmlr-2013-On the Mutual Nearest Neighbors Estimate in Regression</a></p>
<p>12 0.41764316 <a title="35-lda-12" href="./jmlr-2013-On_the_Convergence_of_Maximum_Variance_Unfolding.html">77 jmlr-2013-On the Convergence of Maximum Variance Unfolding</a></p>
<p>13 0.41155946 <a title="35-lda-13" href="./jmlr-2013-Differential_Privacy_for_Functions_and_Functional_Data.html">32 jmlr-2013-Differential Privacy for Functions and Functional Data</a></p>
<p>14 0.40665522 <a title="35-lda-14" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>15 0.40532723 <a title="35-lda-15" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>16 0.40118411 <a title="35-lda-16" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>17 0.39707994 <a title="35-lda-17" href="./jmlr-2013-Learning_Bilinear_Model_for_Matching_Queries_and_Documents.html">60 jmlr-2013-Learning Bilinear Model for Matching Queries and Documents</a></p>
<p>18 0.39616519 <a title="35-lda-18" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>19 0.39310789 <a title="35-lda-19" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>20 0.39177772 <a title="35-lda-20" href="./jmlr-2013-Learning_Theory_Approach_to_Minimum_Error_Entropy_Criterion.html">62 jmlr-2013-Learning Theory Approach to Minimum Error Entropy Criterion</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
