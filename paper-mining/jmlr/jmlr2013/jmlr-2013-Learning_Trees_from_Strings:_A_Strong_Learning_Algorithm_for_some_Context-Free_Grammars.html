<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 jmlr-2013-Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-63" href="#">jmlr2013-63</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>63 jmlr-2013-Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars</h1>
<br/><p>Source: <a title="jmlr-2013-63-pdf" href="http://jmlr.org/papers/volume14/clark13a/clark13a.pdf">pdf</a></p><p>Author: Alexander Clark</p><p>Abstract: Standard models of language learning are concerned with weak learning: the learner, receiving as input only information about the strings in the language, must learn to generalise and to generate the correct, potentially inﬁnite, set of strings generated by some target grammar. Here we deﬁne the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees. We formalise this using a modiﬁcation of Gold’s identiﬁcation in the limit model, requiring convergence to a grammar that is isomorphic to the target grammar. We take as our starting point a simple learning algorithm for substitutable context-free languages, based on principles of distributional learning, and modify it so that it will converge to a canonical grammar for each language. We prove a corresponding strong learning result for a subclass of context-free grammars. Keywords: context-free grammars, grammatical inference, identiﬁcation in the limit, structure learning</p><p>Reference: <a title="jmlr-2013-63-reference" href="../jmlr2013_reference/jmlr-2013-Learning_Trees_from_Strings%3A_A_Strong_Learning_Algorithm_for_some_Context-Free_Grammars_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Here we deﬁne the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees. [sent-5, score-0.73]
</p><p>2 We take as our starting point a simple learning algorithm for substitutable context-free languages, based on principles of distributional learning, and modify it so that it will converge to a canonical grammar for each language. [sent-7, score-0.544]
</p><p>3 1 Weak learning involves merely learning a grammar that generates the right set of strings; strong learning involves learning a grammar that generates the right set of structures (Wexler and Culicover, 1980, p. [sent-30, score-0.817]
</p><p>4 We consider only the problem where the learner has access to the ﬂat strings alone, but must infer an appropriate set of trees for each string in the language. [sent-35, score-0.532]
</p><p>5 Strong learning is obviously impossible for the full class of context-free grammars since there are an inﬁnite number of structurally different context-free grammars that generate a given context-free language. [sent-40, score-0.541]
</p><p>6 If we have a set of strings, X, that are all congruent then we write [X] for the congruence class containing them. [sent-73, score-0.573]
</p><p>7 Note that for any strings u, v, [uv] ⊇ [u][v] so if X,Y are congruence classes we can write [XY ] and the result is well deﬁned. [sent-74, score-0.809]
</p><p>8 / The unique congruence class [λ] is called the unit congruence class. [sent-75, score-1.064]
</p><p>9 The set {u | DL (u) = 0} if it is non-empty is a congruence class, which is called the zero congruence class. [sent-76, score-1.03]
</p><p>10 A congruence class in a language L is non-zero iff it is a subset of Sub(L). [sent-77, score-0.723]
</p><p>11 We are mostly concerned with non-zero non-unit congruence classes in this paper. [sent-78, score-0.566]
</p><p>12 Deﬁnition 1 We will be considering sequences of congruence classes: so if α is a sequence X1 , . [sent-79, score-0.533]
</p><p>13 , Xn ¯ where each of the Xi is a congruence class, then we write α for the set of strings formed by concatenating all of the Xi . [sent-82, score-0.758]
</p><p>14 We can therefore write without ambiguity ¯ ¯ [α] for the congruence class of the strings in α. [sent-85, score-0.792]
</p><p>15 Consider a target grammar G∗ ∈ G , and a sequence of hypothesized grammars G1 , G2 , . [sent-132, score-0.606]
</p><p>16 That is to say we might have a learner which on presentation T ′ of grammar G produces a grammar G′ and on presentation T ′′ of the same grammar, produces a grammar G′′ , where G′ and G′′ are weakly equivalent but not isomorphic. [sent-153, score-1.308]
</p><p>17 Deﬁnition 5 We say that a class of context-free grammars is redundant if it contains two grammars G1 , G2 such that G1 ∼ G2 and L (G1 ) = L (G2 ). [sent-154, score-0.537]
</p><p>18 Suppose A is some learning algorithm that outputs grammars in a hypothesis class HA ⊆ GCFG , and suppose that it can GOLD-learn the class of grammars G ⊆ HA . [sent-163, score-0.598]
</p><p>19 The ﬁrst is that each string in the language has exactly one labelled structure, and the other is that every structure is uniformly right branching, whereas we are interested in learning grammars which may assign more than one different structure to a given string. [sent-171, score-0.549]
</p><p>20 We can enumerate the strings in the language and apply the learner to them, and the limit of the hypothesis grammars will then satisfy the conditions given above, though this function may not be computable. [sent-173, score-0.741]
</p><p>21 This grammar is weakly equivalent to the former grammar, but it is not isomorphic or structurally equivalent, as it will assign a larger set of parses to strings like aacbb. [sent-205, score-0.715]
</p><p>22 Indeed it is easy to see that this grammar will assign every possible binary branching structure to any string that is part of the set that the grammar is constructed from. [sent-207, score-0.923]
</p><p>23 Each nonterminal/syntactic category will correspond to a congruence class. [sent-211, score-0.515]
</p><p>24 We start with the following deﬁnition: Deﬁnition 7 A congruence class X is prime if it is non-zero and non-unit and for any two congruence classes Y, Z such that X = Y · Z then either Y or Z is the unit. [sent-213, score-1.491]
</p><p>25 If a non-zero non-unit congruence class is not prime then we say it is composite. [sent-214, score-0.92]
</p><p>26 3543  C LARK  In other words a class is not prime if it can be decomposed into the concatenation of two other congruence classes. [sent-215, score-0.925]
</p><p>27 The stipulation that the unit and zero congruence classes are not prime is analogous to the stipulation that 1 is not a prime number. [sent-216, score-1.274]
</p><p>28 We will not give a detailed exposition of why the concept of a prime congruence class is important, but one intuitive reason is this. [sent-217, score-0.903]
</p><p>29 If we have nonterminals that correspond to congruence classes, and a congruence class N is composite, then that means that we can decompose N into two classes P, Q such that N = PQ. [sent-218, score-1.219]
</p><p>30 Thus non-prime congruence classes can always be replaced by a sequence of prime congruence classes, and we can limit our attention to the primes which informally are those where “the whole is greater than the sum of the parts”. [sent-220, score-1.608]
</p><p>31 This language is not regular and therefore has an inﬁnite number of congruence classes of which three are prime. [sent-223, score-0.703]
</p><p>32 The congruence classes are as follows: • {λ} is a congruence class with just one element; this is the unit congruence class which is not prime. [sent-224, score-1.664]
</p><p>33 • The zero congruence class which consists of all strings that have empty distribution. [sent-225, score-0.792]
</p><p>34 • We also have an inﬁnite number of congruence classes of the form {ai } for any i > 1. [sent-228, score-0.566]
</p><p>35 This language has 5 congruence classes: [a], [b], [ab], [λ] and the zero congruence class. [sent-233, score-1.167]
</p><p>36 Proof Let a be some letter in a language L and let [a] be its congruence class. [sent-237, score-0.652]
</p><p>37 Suppose there are two congruence classes X,Y such that XY = [a]. [sent-238, score-0.566]
</p><p>38 Deﬁnition 9 Let Lsc be the set of all languages which are substitutable, non-empty, do not contain λ and have a ﬁnite number of prime congruence classes. [sent-242, score-0.988]
</p><p>39 Here we consider only languages where there are a ﬁnite number of prime congruence classes. [sent-244, score-0.988]
</p><p>40 Every regular language of course has a ﬁnite number of primes as it has a ﬁnite number of congruence classes. [sent-246, score-0.825]
</p><p>41 Therefore we have an inﬁnite number of congruence classes of the form {bai b, dei d}, each of which is prime. [sent-251, score-0.566]
</p><p>42 Deﬁnition 10 A prime decomposition of a congruence class X is a ﬁnite sequence of one or more ¯ prime congruence classes α = X1 , . [sent-252, score-1.823]
</p><p>43 Clearly any prime congruence class X has a trivial prime decomposition of length one, namely X . [sent-256, score-1.28]
</p><p>44 We have a prime factorization lemma for substitutable languages; we can rather pompously call this the ‘fundamental lemma’ by analogy with the fundamental lemma of arithmetic. [sent-257, score-0.555]
</p><p>45 This lemma means that we can represent all of the congruence classes exactly using just concatenations of the prime congruence classes. [sent-258, score-1.463]
</p><p>46 Lemma 11 Every non-zero non-unit congruence class of a language in Lsc has a unique prime factorisation. [sent-259, score-1.04]
</p><p>47 Among the congruence classes are {a}, {b}, {c} {ab, ap}, {bc, pc} and {abc, apc}. [sent-264, score-0.566]
</p><p>48 Clearly {ab, ap}, {bc, pc} are both prime but {abc, apc} is composite and has the two distinct prime decompositions {ab, ap} · {c} and {a} · {bc, pc}. [sent-265, score-0.708]
</p><p>49 If we restrict ourselves to languages in Lsc then we can assume without loss of generality that the nonterminals of the generating grammar correspond to congruence classes. [sent-266, score-1.101]
</p><p>50 In a substitutable language, a trim CFG cannot have a nonterminal that generates two strings that are not congruent. [sent-267, score-0.494]
</p><p>51 Given that non-regular languages will have an inﬁnite number of congruence classes, and that CFG s have by deﬁnition only a ﬁnite number of nonterminals, we cannot have one nonterminal for every congruence class. [sent-269, score-1.211]
</p><p>52 However in languages in Lsc there are only ﬁnitely many prime congruence classes, and since every other congruence class can be represented perfectly as a sequence of primes, it is sufﬁcient to consider a grammar which has nonterminals that correspond to the primes. [sent-270, score-2.004]
</p><p>53 Therefore we will consider grammars whose nonterminals correspond only to the prime congruence classes of the grammar: we add one extra nonterminal S, a start symbol, which will not appear on the right hand side of any rule. [sent-271, score-1.351]
</p><p>54 1 Productions We now consider an abstract notion of a production where the nonterminals are the prime congruence classes. [sent-273, score-1.096]
</p><p>55 ¯ Deﬁnition 12 A correct branching production is of the form [α] → α where α is a sequence of at ¯ least 2 primes and [α] is a prime congruence class. [sent-274, score-1.229]
</p><p>56 Informally we say that the right hand side of the production [a][a][c][b][b] is too long since there is a proper subsequence [a][c][b] which generates strings in a prime congruence class, and should be represented just by the prime [c]. [sent-282, score-1.629]
</p><p>57 For any string w in a prime congruence class where w = a1 . [sent-287, score-1.072]
</p><p>58 Such productions may in general be pleonastic because there may be substrings that can be represented by prime congruence classes. [sent-294, score-1.07]
</p><p>59 Note that for every language in Lsc , L is a congruence class. [sent-299, score-0.652]
</p><p>60 The nonterminals are the prime congruence classes of L, together with an additional symbol S, which is the start symbol. [sent-304, score-1.071]
</p><p>61 Lemma 17 If L ∈ Lsc is a substitutable language, then for any prime congruence class X,  L (G∗ (L), X) ⊆ X. [sent-309, score-1.048]
</p><p>62 Using this observed data, together with the grammar which is used for parsing, we can then compute the canonical grammar for the language as follows. [sent-365, score-0.899]
</p><p>63 We partition Sub(D) into congruence classes, with respect to our learned grammar Gw . [sent-367, score-0.878]
</p><p>64 We then test to see which of the congruence classes are prime. [sent-371, score-0.566]
</p><p>65 A set of valid rules is constructed from the strings in the prime congruence classes. [sent-375, score-1.141]
</p><p>66 If there is some string that cannot be split, then we know that the congruence class must be prime. [sent-396, score-0.718]
</p><p>67 For all of the non-prime congruence classes, we now want to compute the unique decomposition into primes. [sent-399, score-0.515]
</p><p>68 We then greedily convert this into a unique shortest sequence of primes by checking every proper subsequence of length at least 2, and seeing if that string is in a prime congruence class. [sent-409, score-1.307]
</p><p>69 We then take the shortest path from 0 to n; and read off the sequence of 3548  L EARNING T REES FROM S TRINGS  Algorithm 2 Testing for primality Data: A set of strings X Data: A partition of strings X = {X1 , . [sent-418, score-0.559]
</p><p>70 Note that since the lexical congruence classes are all prime, we know there will be at least one such path; since the language is substitutable we know this will be unique. [sent-422, score-0.903]
</p><p>71 Every valid production will be of the form N → Mα where N, M are primes and α is a prime decomposition of length at least 1. [sent-424, score-0.702]
</p><p>72 Accordingly we loop through all triples of N and M, Q as follows: for each prime N, for each prime M, for each class Q, take α to be the prime decomposition of Q, and test to see if N → Mα is valid. [sent-426, score-1.096]
</p><p>73 We can test if it is correct easily by taking any shortest string u from M and any shortest string v from α and seeing if uv ∈ N; if it does then the rule is correct. [sent-427, score-0.548]
</p><p>74 For the initial symbol S, we identify the unique congruence class of strings in the language X. [sent-431, score-0.954]
</p><p>75 In order to prove this we will show that for any presentation of a grammar in the class we will converge strongly to a grammar isomorphic to the canonical grammar. [sent-440, score-0.858]
</p><p>76 For a grammar G∗ ∈ Gsc , we deﬁne χ(G∗ ) to be 3549  C LARK  Algorithm 3 ASGOLD Strong Gold Learning Algorithm Data: A sequence of strings w1 , w2 , . [sent-442, score-0.606]
</p><p>77 First, if Gw is correct, then the partition of Sub(D) into congruence classes will be correct in the sense that two strings of Sub(D) will be in the same class iff they are congruent. [sent-458, score-0.916]
</p><p>78 , Xn is a correct partition of Sub(D) into congruence classes. [sent-462, score-0.551]
</p><p>79 Proof Suppose [Xi ] is not prime: then there are two congruence classes Y, Z such that [Xi ] = Y Z. [sent-464, score-0.566]
</p><p>80 , Xn is a correct partition of Sub(D) into congruence classes, and D ⊇ χ(G∗ ). [sent-473, score-0.551]
</p><p>81 Then by the construction of the characteristic set we will have a unique congruence class in the grammar corresponding to [X2 · · · Xn ]. [sent-487, score-0.932]
</p><p>82 We know that it will be correct, by the correctness of the weak learner and the fact that the congruence classes are correctly divided. [sent-501, score-0.719]
</p><p>83 To conclude the proof of Theorem 23, we just need to observe that since the characteristic set includes the shortest element of each prime congruence class, and so the labels for each nonterminal will not change which means that the output grammars will converge exactly. [sent-503, score-1.283]
</p><p>84 Nonterminals in the output grammar are either S for the start symbol or NT followed by a digit for the congruence classes that correspond to primes. [sent-512, score-0.976]
</p><p>85 This grammar is unambiguous so every string has only one tree. [sent-518, score-0.532]
</p><p>86 Since (¬A) and (A∨A) are both in the language, ¬ ∼ A∨, so the parse tree for (A∨B) will look a = little strange: the canonical grammar has pulled out some more structure than the textbook grammar does: see Figure 2 for example trees. [sent-534, score-0.799]
</p><p>87 In this case the grammar is ambiguous and the number of parses for each string varies, depending on properties of the string that are more complex than just the length. [sent-543, score-0.756]
</p><p>88 For example, the string abab has 5 parses, the string abba has 3 and the string aabb has only 2. [sent-544, score-0.507]
</p><p>89 In general it will not be possible to compute a canonical form for an arbitrary grammar as this will be undecidable; however we may be able to do this for the grammars output by weak learners which will typically produce grammars in a restricted class. [sent-556, score-0.923]
</p><p>90 Nonetheless we can extend the notion of a prime congruence class naturally to the richer mathematical structures that we need to model the more complex grammar formalisms required for natural language syntax. [sent-564, score-1.403]
</p><p>91 This appendix contains the proofs of some technical lemmas that we use earlier that are not important from a learning theoretic point of view, but merely concern the algebraic properties of substitutable languages and their congruence classes. [sent-569, score-0.796]
</p><p>92 Lemma 28 If X is a prime, and Y is a congruence class which is not equal to X, then there is a string in X which does not start with an element of Y . [sent-571, score-0.756]
</p><p>93 Since we have some context (l, r) such that lbr ∈ L therefore lcbr ∈ L by substitutability we will have b1 ≡ cb1 so cb1 ∈ B1 since it is a congruence class. [sent-618, score-0.577]
</p><p>94 ¯ ¯ ¯ Lemma 32 If α and β are non-empty sequences of prime congruence classes such that α = β = [α], ¯ and α ⊆ Sub(L), then α = β. [sent-622, score-0.938]
</p><p>95 Lemma 33 Every non-zero non-unit congruence class has a unique prime factorisation. [sent-729, score-0.903]
</p><p>96 Proof We show that every congruence class can be represented as a product of primes; uniqueness then follows immediately by Lemma 32. [sent-730, score-0.549]
</p><p>97 Inductive step: suppose X is a congruence class whose shortest string is of length k. [sent-734, score-0.839]
</p><p>98 If X is prime, then again it is uniquely representable so suppose it is not prime, and there 3556  L EARNING T REES FROM S TRINGS  is at least one decomposition into two congruence classes Y, Z. [sent-735, score-0.591]
</p><p>99 Y, Z must contain strings of length less than k and so by the inductive hypothesis, Y and Z are both decomposable into sequences of prime congruence classes, Y = Y1 . [sent-736, score-1.176]
</p><p>100 Lemma 34 Suppose N is a prime and α, γ are nonempty sequences of primes such that N → γα is ¯ a valid production. [sent-749, score-0.574]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('congruence', 0.515), ('grammar', 0.363), ('prime', 0.354), ('strings', 0.243), ('grammars', 0.243), ('primes', 0.173), ('string', 0.169), ('substitutable', 0.145), ('language', 0.137), ('cfg', 0.125), ('productions', 0.125), ('production', 0.123), ('languages', 0.119), ('nt', 0.107), ('nonterminals', 0.104), ('learner', 0.099), ('lsc', 0.097), ('sgold', 0.097), ('sub', 0.09), ('gold', 0.082), ('cfgs', 0.076), ('lark', 0.076), ('pleonastic', 0.076), ('trings', 0.076), ('shortest', 0.073), ('nonterminal', 0.062), ('substitutability', 0.062), ('acb', 0.059), ('rees', 0.059), ('lexical', 0.055), ('clark', 0.055), ('classes', 0.051), ('gsc', 0.047), ('dl', 0.046), ('cbn', 0.042), ('eyraud', 0.042), ('gcfg', 0.042), ('wbc', 0.042), ('yoshinaka', 0.042), ('sbc', 0.041), ('weak', 0.038), ('parse', 0.037), ('iff', 0.037), ('correct', 0.036), ('canonical', 0.036), ('presentation', 0.035), ('gn', 0.035), ('pb', 0.035), ('lur', 0.035), ('pu', 0.035), ('substring', 0.035), ('pl', 0.034), ('class', 0.034), ('weakly', 0.033), ('linguistics', 0.033), ('syntactic', 0.032), ('gw', 0.032), ('bn', 0.03), ('valid', 0.029), ('cb', 0.029), ('lemma', 0.028), ('uv', 0.028), ('asgold', 0.028), ('kanazawa', 0.028), ('lvr', 0.028), ('parses', 0.028), ('branching', 0.028), ('strong', 0.028), ('isomorphic', 0.027), ('ambiguous', 0.027), ('earning', 0.026), ('symbol', 0.025), ('suppose', 0.025), ('ab', 0.025), ('congruent', 0.024), ('generates', 0.023), ('bi', 0.023), ('inductive', 0.023), ('length', 0.023), ('concatenation', 0.022), ('start', 0.022), ('structurally', 0.021), ('trees', 0.021), ('behaviorally', 0.021), ('seki', 0.021), ('sentential', 0.021), ('trim', 0.021), ('pre', 0.021), ('characteristic', 0.02), ('logic', 0.02), ('hypothesis', 0.019), ('sequences', 0.018), ('neg', 0.018), ('lexicographically', 0.018), ('polynomial', 0.018), ('merely', 0.017), ('polynomially', 0.017), ('say', 0.017), ('correctness', 0.016), ('element', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="63-tfidf-1" href="./jmlr-2013-Learning_Trees_from_Strings%3A_A_Strong_Learning_Algorithm_for_some_Context-Free_Grammars.html">63 jmlr-2013-Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars</a></p>
<p>Author: Alexander Clark</p><p>Abstract: Standard models of language learning are concerned with weak learning: the learner, receiving as input only information about the strings in the language, must learn to generalise and to generate the correct, potentially inﬁnite, set of strings generated by some target grammar. Here we deﬁne the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees. We formalise this using a modiﬁcation of Gold’s identiﬁcation in the limit model, requiring convergence to a grammar that is isomorphic to the target grammar. We take as our starting point a simple learning algorithm for substitutable context-free languages, based on principles of distributional learning, and modify it so that it will converge to a canonical grammar for each language. We prove a corresponding strong learning result for a subclass of context-free grammars. Keywords: context-free grammars, grammatical inference, identiﬁcation in the limit, structure learning</p><p>2 0.18735556 <a title="63-tfidf-2" href="./jmlr-2013-On_the_Learnability_of_Shuffle_Ideals.html">78 jmlr-2013-On the Learnability of Shuffle Ideals</a></p>
<p>Author: Dana Angluin, James Aspnes, Sarah Eisenstat, Aryeh Kontorovich</p><p>Abstract: PAC learning of unrestricted regular languages is long known to be a difﬁcult problem. The class of shufﬂe ideals is a very restricted subclass of regular languages, where the shufﬂe ideal generated by a string u is the collection of all strings containing u as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shufﬂe ideals appears quite difﬁcult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP = NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efﬁcient algorithm for properly learning shufﬂe ideals in the statistical query (and therefore also PAC) model under the uniform distribution. Keywords: PAC learning, statistical queries, regular languages, deterministic ﬁnite automata, shufﬂe ideals, subsequences</p><p>3 0.041244917 <a title="63-tfidf-3" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>Author: Philip M. Long, Rocco A. Servedio</p><p>Abstract: We consider the problem of learning an unknown large-margin halfspace in the context of parallel computation, giving both positive and negative results. As our main positive result, we give a parallel algorithm for learning a large-margin halfspace, based on an algorithm of Nesterov’s that performs gradient descent with a momentum term. We show that this algorithm can learn an unknown γ-margin halfspace over n dimensions using ˜ n · poly(1/γ) processors and running in time O(1/γ) + O(log n). In contrast, naive parallel algorithms that learn a γ-margin halfspace in time that depends polylogarithmically on n have an inverse quadratic running time dependence on the margin parameter γ. Our negative result deals with boosting, which is a standard approach to learning large-margin halfspaces. We prove that in the original PAC framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized. More precisely, we show that, if the algorithm is allowed to call the weak learner multiple times in parallel within a single boosting stage, this ability does not reduce the overall number of successive stages of boosting needed for learning by even a single stage. Our proof is information-theoretic and does not rely on unproven assumptions. Keywords: PAC learning, parallel learning algorithms, halfspace learning, linear classiﬁers</p><p>4 0.03316509 <a title="63-tfidf-4" href="./jmlr-2013-Query_Induction_with_Schema-Guided_Pruning_Strategies.html">91 jmlr-2013-Query Induction with Schema-Guided Pruning Strategies</a></p>
<p>Author: Joachim Niehren, Jérôme Champavère, Aurélien Lemay, Rémi Gilleron</p><p>Abstract: Inference algorithms for tree automata that deﬁne node selecting queries in unranked trees rely on tree pruning strategies. These impose additional assumptions on node selection that are needed to compensate for small numbers of annotated examples. Pruning-based heuristics in query learning algorithms for Web information extraction often boost the learning quality and speed up the learning process. We will distinguish the class of regular queries that are stable under a given schemaguided pruning strategy, and show that this class is learnable with polynomial time and data. Our learning algorithm is obtained by adding pruning heuristics to the traditional learning algorithm for tree automata from positive and negative examples. While justiﬁed by a formal learning model, our learning algorithm for stable queries also performs very well in practice of XML information extraction. Keywords: XML information extraction, XML schemas, interactive learning, tree automata, grammatical inference</p><p>5 0.032265015 <a title="63-tfidf-5" href="./jmlr-2013-A_Theory_of_Multiclass_Boosting.html">8 jmlr-2013-A Theory of Multiclass Boosting</a></p>
<p>Author: Indraneel Mukherjee, Robert E. Schapire</p><p>Abstract: Boosting combines weak classiﬁers to form highly accurate predictors. Although the case of binary classiﬁcation is well understood, in the multiclass setting, the “correct” requirements on the weak classiﬁer, or the notion of the most efﬁcient boosting algorithms are missing. In this paper, we create a broad and general framework, within which we make precise and identify the optimal requirements on the weak-classiﬁer, as well as design the most effective, in a certain sense, boosting algorithms that assume such requirements. Keywords: multiclass, boosting, weak learning condition, drifting games</p><p>6 0.023446191 <a title="63-tfidf-6" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>7 0.02331439 <a title="63-tfidf-7" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>8 0.022967134 <a title="63-tfidf-8" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>9 0.022840697 <a title="63-tfidf-9" href="./jmlr-2013-Manifold_Regularization_and_Semi-supervised_Learning%3A_Some_Theoretical_Analyses.html">69 jmlr-2013-Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses</a></p>
<p>10 0.021634616 <a title="63-tfidf-10" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>11 0.021061117 <a title="63-tfidf-11" href="./jmlr-2013-Efficient_Program_Synthesis_Using_Constraint_Satisfaction_in_Inductive_Logic_Programming.html">40 jmlr-2013-Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming</a></p>
<p>12 0.020699918 <a title="63-tfidf-12" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>13 0.019096773 <a title="63-tfidf-13" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>14 0.019013006 <a title="63-tfidf-14" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>15 0.019007925 <a title="63-tfidf-15" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>16 0.018495046 <a title="63-tfidf-16" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>17 0.017159861 <a title="63-tfidf-17" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>18 0.01628479 <a title="63-tfidf-18" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>19 0.016256392 <a title="63-tfidf-19" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<p>20 0.01588477 <a title="63-tfidf-20" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.093), (1, 0.042), (2, -0.012), (3, 0.023), (4, -0.013), (5, 0.018), (6, 0.103), (7, 0.005), (8, -0.22), (9, 0.131), (10, 0.001), (11, -0.189), (12, -0.077), (13, 0.231), (14, -0.279), (15, -0.017), (16, 0.0), (17, 0.001), (18, -0.043), (19, 0.001), (20, -0.031), (21, -0.172), (22, -0.014), (23, 0.007), (24, -0.099), (25, -0.032), (26, -0.099), (27, -0.076), (28, -0.142), (29, -0.016), (30, -0.173), (31, 0.047), (32, 0.041), (33, -0.141), (34, 0.182), (35, 0.047), (36, 0.012), (37, -0.123), (38, -0.085), (39, -0.101), (40, -0.047), (41, -0.035), (42, 0.137), (43, -0.191), (44, -0.007), (45, 0.064), (46, 0.048), (47, 0.011), (48, -0.041), (49, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96861005 <a title="63-lsi-1" href="./jmlr-2013-Learning_Trees_from_Strings%3A_A_Strong_Learning_Algorithm_for_some_Context-Free_Grammars.html">63 jmlr-2013-Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars</a></p>
<p>Author: Alexander Clark</p><p>Abstract: Standard models of language learning are concerned with weak learning: the learner, receiving as input only information about the strings in the language, must learn to generalise and to generate the correct, potentially inﬁnite, set of strings generated by some target grammar. Here we deﬁne the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees. We formalise this using a modiﬁcation of Gold’s identiﬁcation in the limit model, requiring convergence to a grammar that is isomorphic to the target grammar. We take as our starting point a simple learning algorithm for substitutable context-free languages, based on principles of distributional learning, and modify it so that it will converge to a canonical grammar for each language. We prove a corresponding strong learning result for a subclass of context-free grammars. Keywords: context-free grammars, grammatical inference, identiﬁcation in the limit, structure learning</p><p>2 0.77699494 <a title="63-lsi-2" href="./jmlr-2013-On_the_Learnability_of_Shuffle_Ideals.html">78 jmlr-2013-On the Learnability of Shuffle Ideals</a></p>
<p>Author: Dana Angluin, James Aspnes, Sarah Eisenstat, Aryeh Kontorovich</p><p>Abstract: PAC learning of unrestricted regular languages is long known to be a difﬁcult problem. The class of shufﬂe ideals is a very restricted subclass of regular languages, where the shufﬂe ideal generated by a string u is the collection of all strings containing u as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shufﬂe ideals appears quite difﬁcult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP = NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efﬁcient algorithm for properly learning shufﬂe ideals in the statistical query (and therefore also PAC) model under the uniform distribution. Keywords: PAC learning, statistical queries, regular languages, deterministic ﬁnite automata, shufﬂe ideals, subsequences</p><p>3 0.28294048 <a title="63-lsi-3" href="./jmlr-2013-Efficient_Program_Synthesis_Using_Constraint_Satisfaction_in_Inductive_Logic_Programming.html">40 jmlr-2013-Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming</a></p>
<p>Author: John Ahlgren, Shiu Yin Yuen</p><p>Abstract: We present NrSample, a framework for program synthesis in inductive logic programming. NrSample uses propositional logic constraints to exclude undesirable candidates from the search. This is achieved by representing constraints as propositional formulae and solving the associated constraint satisfaction problem. We present a variety of such constraints: pruning, input-output, functional (arithmetic), and variable splitting. NrSample is also capable of detecting search space exhaustion, leading to further speedups in clause induction and optimality. We benchmark NrSample against enumeration search (Aleph’s default) and Progol’s A∗ search in the context of program synthesis. The results show that, on large program synthesis problems, NrSample induces between 1 and 1358 times faster than enumeration (236 times faster on average), always with similar or better accuracy. Compared to Progol A∗ , NrSample is 18 times faster on average with similar or better accuracy except for two problems: one in which Progol A∗ substantially sacriﬁced accuracy to induce faster, and one in which Progol A∗ was a clear winner. Functional constraints provide a speedup of up to 53 times (21 times on average) with similar or better accuracy. We also benchmark using a few concept learning (non-program synthesis) problems. The results indicate that without strong constraints, the overhead of solving constraints is not compensated for. Keywords: inductive logic programming, program synthesis, theory induction, constraint satisfaction, Boolean satisﬁability problem</p><p>4 0.20137642 <a title="63-lsi-4" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<p>Author: Reza Bosagh Zadeh, Ashish Goel</p><p>Abstract: We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high-dimensional sparse vectors. All of our results are provably independent of dimension, meaning that apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension; thus the dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard similarity measures. For Jaccard similarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems with large scale experiments using data from the social networking site Twitter. At time of writing, our algorithms are live in production at twitter.com. Keywords: cosine, Jaccard, overlap, dice, similarity, MapReduce, dimension independent</p><p>5 0.18880433 <a title="63-lsi-5" href="./jmlr-2013-Query_Induction_with_Schema-Guided_Pruning_Strategies.html">91 jmlr-2013-Query Induction with Schema-Guided Pruning Strategies</a></p>
<p>Author: Joachim Niehren, Jérôme Champavère, Aurélien Lemay, Rémi Gilleron</p><p>Abstract: Inference algorithms for tree automata that deﬁne node selecting queries in unranked trees rely on tree pruning strategies. These impose additional assumptions on node selection that are needed to compensate for small numbers of annotated examples. Pruning-based heuristics in query learning algorithms for Web information extraction often boost the learning quality and speed up the learning process. We will distinguish the class of regular queries that are stable under a given schemaguided pruning strategy, and show that this class is learnable with polynomial time and data. Our learning algorithm is obtained by adding pruning heuristics to the traditional learning algorithm for tree automata from positive and negative examples. While justiﬁed by a formal learning model, our learning algorithm for stable queries also performs very well in practice of XML information extraction. Keywords: XML information extraction, XML schemas, interactive learning, tree automata, grammatical inference</p><p>6 0.15993969 <a title="63-lsi-6" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>7 0.15794978 <a title="63-lsi-7" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>8 0.15059732 <a title="63-lsi-8" href="./jmlr-2013-Orange%3A_Data_Mining_Toolbox_in_Python.html">83 jmlr-2013-Orange: Data Mining Toolbox in Python</a></p>
<p>9 0.14080073 <a title="63-lsi-9" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>10 0.13738319 <a title="63-lsi-10" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>11 0.12860121 <a title="63-lsi-11" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>12 0.1272255 <a title="63-lsi-12" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>13 0.12211715 <a title="63-lsi-13" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>14 0.10290967 <a title="63-lsi-14" href="./jmlr-2013-Manifold_Regularization_and_Semi-supervised_Learning%3A_Some_Theoretical_Analyses.html">69 jmlr-2013-Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses</a></p>
<p>15 0.10047924 <a title="63-lsi-15" href="./jmlr-2013-Truncated_Power_Method_for_Sparse_Eigenvalue_Problems.html">116 jmlr-2013-Truncated Power Method for Sparse Eigenvalue Problems</a></p>
<p>16 0.09688352 <a title="63-lsi-16" href="./jmlr-2013-Universal_Consistency_of_Localized_Versions_of_Regularized_Kernel_Methods.html">117 jmlr-2013-Universal Consistency of Localized Versions of Regularized Kernel Methods</a></p>
<p>17 0.094788298 <a title="63-lsi-17" href="./jmlr-2013-Learning_Theory_Approach_to_Minimum_Error_Entropy_Criterion.html">62 jmlr-2013-Learning Theory Approach to Minimum Error Entropy Criterion</a></p>
<p>18 0.094145603 <a title="63-lsi-18" href="./jmlr-2013-Ranking_Forests.html">95 jmlr-2013-Ranking Forests</a></p>
<p>19 0.093626469 <a title="63-lsi-19" href="./jmlr-2013-Learning_Theory_Analysis_for_Association_Rules_and_Sequential_Event_Prediction.html">61 jmlr-2013-Learning Theory Analysis for Association Rules and Sequential Event Prediction</a></p>
<p>20 0.09339904 <a title="63-lsi-20" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.019), (5, 0.104), (6, 0.038), (9, 0.047), (10, 0.035), (20, 0.012), (23, 0.018), (68, 0.015), (70, 0.018), (75, 0.018), (85, 0.022), (87, 0.011), (89, 0.533)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81586814 <a title="63-lda-1" href="./jmlr-2013-Alleviating_Naive_Bayes_Attribute_Independence_Assumption_by_Attribute_Weighting.html">12 jmlr-2013-Alleviating Naive Bayes Attribute Independence Assumption by Attribute Weighting</a></p>
<p>Author: Nayyar A. Zaidi, Jesús Cerquides, Mark J. Carman, Geoffrey I. Webb</p><p>Abstract: Despite the simplicity of the Naive Bayes classiﬁer, it has continued to perform well against more sophisticated newcomers and has remained, therefore, of great interest to the machine learning community. Of numerous approaches to reﬁning the naive Bayes classiﬁer, attribute weighting has received less attention than it warrants. Most approaches, perhaps inﬂuenced by attribute weighting in other machine learning algorithms, use weighting to place more emphasis on highly predictive attributes than those that are less predictive. In this paper, we argue that for naive Bayes attribute weighting should instead be used to alleviate the conditional independence assumption. Based on this premise, we propose a weighted naive Bayes algorithm, called WANBIA, that selects weights to minimize either the negative conditional log likelihood or the mean squared error objective functions. We perform extensive evaluations and ﬁnd that WANBIA is a competitive alternative to state of the art classiﬁers like Random Forest, Logistic Regression and A1DE. Keywords: classiﬁcation, naive Bayes, attribute independence assumption, weighted naive Bayes classiﬁcation</p><p>same-paper 2 0.77605903 <a title="63-lda-2" href="./jmlr-2013-Learning_Trees_from_Strings%3A_A_Strong_Learning_Algorithm_for_some_Context-Free_Grammars.html">63 jmlr-2013-Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars</a></p>
<p>Author: Alexander Clark</p><p>Abstract: Standard models of language learning are concerned with weak learning: the learner, receiving as input only information about the strings in the language, must learn to generalise and to generate the correct, potentially inﬁnite, set of strings generated by some target grammar. Here we deﬁne the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees. We formalise this using a modiﬁcation of Gold’s identiﬁcation in the limit model, requiring convergence to a grammar that is isomorphic to the target grammar. We take as our starting point a simple learning algorithm for substitutable context-free languages, based on principles of distributional learning, and modify it so that it will converge to a canonical grammar for each language. We prove a corresponding strong learning result for a subclass of context-free grammars. Keywords: context-free grammars, grammatical inference, identiﬁcation in the limit, structure learning</p><p>3 0.64676863 <a title="63-lda-3" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>Author: Markus Thom, Günther Palm</p><p>Abstract: Sparseness is a useful regularizer for learning in a wide range of applications, in particular in neural networks. This paper proposes a model targeted at classiﬁcation tasks, where sparse activity and sparse connectivity are used to enhance classiﬁcation capabilities. The tool for achieving this is a sparseness-enforcing projection operator which ﬁnds the closest vector with a pre-deﬁned sparseness for any given vector. In the theoretical part of this paper, a comprehensive theory for such a projection is developed. In conclusion, it is shown that the projection is differentiable almost everywhere and can thus be implemented as a smooth neuronal transfer function. The entire model can hence be tuned end-to-end using gradient-based methods. Experiments on the MNIST database of handwritten digits show that classiﬁcation performance can be boosted by sparse activity or sparse connectivity. With a combination of both, performance can be signiﬁcantly better compared to classical non-sparse approaches. Keywords: supervised learning, sparseness projection, sparse activity, sparse connectivity</p><p>4 0.26306871 <a title="63-lda-4" href="./jmlr-2013-On_the_Learnability_of_Shuffle_Ideals.html">78 jmlr-2013-On the Learnability of Shuffle Ideals</a></p>
<p>Author: Dana Angluin, James Aspnes, Sarah Eisenstat, Aryeh Kontorovich</p><p>Abstract: PAC learning of unrestricted regular languages is long known to be a difﬁcult problem. The class of shufﬂe ideals is a very restricted subclass of regular languages, where the shufﬂe ideal generated by a string u is the collection of all strings containing u as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shufﬂe ideals appears quite difﬁcult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP = NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efﬁcient algorithm for properly learning shufﬂe ideals in the statistical query (and therefore also PAC) model under the uniform distribution. Keywords: PAC learning, statistical queries, regular languages, deterministic ﬁnite automata, shufﬂe ideals, subsequences</p><p>5 0.25744775 <a title="63-lda-5" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>Author: Tony Cai, Wen-Xin Zhou</p><p>Abstract: We consider in this paper the problem of noisy 1-bit matrix completion under a general non-uniform sampling distribution using the max-norm as a convex relaxation for the rank. A max-norm constrained maximum likelihood estimate is introduced and studied. The rate of convergence for the estimate is obtained. Information-theoretical methods are used to establish a minimax lower bound under the general sampling model. The minimax upper and lower bounds together yield the optimal rate of convergence for the Frobenius norm loss. Computational algorithms and numerical performance are also discussed. Keywords: 1-bit matrix completion, low-rank matrix, max-norm, trace-norm, constrained optimization, maximum likelihood estimate, optimal rate of convergence</p><p>6 0.24998742 <a title="63-lda-6" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>7 0.24925394 <a title="63-lda-7" href="./jmlr-2013-Efficient_Program_Synthesis_Using_Constraint_Satisfaction_in_Inductive_Logic_Programming.html">40 jmlr-2013-Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming</a></p>
<p>8 0.24823116 <a title="63-lda-8" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<p>9 0.24584067 <a title="63-lda-9" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>10 0.24548367 <a title="63-lda-10" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>11 0.24404243 <a title="63-lda-11" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>12 0.24236187 <a title="63-lda-12" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>13 0.2422481 <a title="63-lda-13" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>14 0.24109085 <a title="63-lda-14" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<p>15 0.24069306 <a title="63-lda-15" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>16 0.2403519 <a title="63-lda-16" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>17 0.24032095 <a title="63-lda-17" href="./jmlr-2013-CODA%3A_High_Dimensional_Copula_Discriminant_Analysis.html">20 jmlr-2013-CODA: High Dimensional Copula Discriminant Analysis</a></p>
<p>18 0.23902512 <a title="63-lda-18" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>19 0.23818597 <a title="63-lda-19" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>20 0.23176622 <a title="63-lda-20" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
