<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-18" href="#">jmlr2013-18</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</h1>
<br/><p>Source: <a title="jmlr-2013-18-pdf" href="http://jmlr.org/papers/volume14/zhao13a/zhao13a.pdf">pdf</a></p><p>Author: Ming-Jie Zhao, Narayanan Edakunni, Adam Pocock, Gavin Brown</p><p>Abstract: Fano’s inequality lower bounds the probability of transmission error through a communication channel. Applied to classiﬁcation problems, it provides a lower bound on the Bayes error rate and motivates the widely used Infomax principle. In modern machine learning, we are often interested in more than just the error rate. In medical diagnosis, different errors incur different cost; hence, the overall risk is cost-sensitive. Two other popular criteria are balanced error rate (BER) and F-score. In this work, we focus on the two-class problem and use a general deﬁnition of conditional entropy (including Shannon’s as a special case) to derive upper/lower bounds on the optimal F-score, BER and cost-sensitive risk, extending Fano’s result. As a consequence, we show that Infomax is not suitable for optimizing F-score or cost-sensitive risk, in that it can potentially lead to low F-score and high risk. For cost-sensitive risk, we propose a new conditional entropy formulation which avoids this inconsistency. In addition, we consider the common practice of using a threshold on the posterior probability to tune performance of a classiﬁer. As is widely known, a threshold of 0.5, where the posteriors cross, minimizes error rate—we derive similar optimal thresholds for F-score and BER. Keywords: balanced error rate, F-score (Fβ -measure), cost-sensitive risk, conditional entropy, lower/upper bound</p><p>Reference: <a title="jmlr-2013-18-reference" href="../jmlr2013_reference/jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 9 But hbin (η) is a symmetric ˜ function, that is, hbin (η) = hbin (η), by the deﬁnition of e(η) we know the curve ℓ is in fact the left half of hbin (η), as depicted in Figure 2-a. [sent-277, score-0.61]
</p><p>2 We then construct the convex hull of the curve ℓ, which for this example is the bow shape OABCO bounded by the curve OCB (i. [sent-279, score-0.36]
</p><p>3 The convex hull of ℓ, denoted co ℓ, can then be expressed as10 (recall that e = 1 − e) ˜ co ℓ = {(e, h) | e ∈ [0, 0. [sent-303, score-1.132]
</p><p>4 We are now ready to “read” the lower and upper bounds of H(y|x) from the set co ℓ, as Theorem 1 has already told us that [ERR, H(y|x)] ∈ co ℓ. [sent-305, score-1.012]
</p><p>5 Furthermore, from Theorem 1 and the deﬁnition of the convex hull of a set, we can easily see the two bounds of H(y|x) given by Equation (24) are tight, in the sense that for any concave function h(·) and any given value of ERR, both bounds are reachable by some task (µ, η). [sent-362, score-0.368]
</p><p>6 Theorem 5 For any concave function h : [0, 1] → R with h(0) = h(1) = 0, and any point [e0 , h0 ] inside the convex set co ℓ as given by Equation (23), there exists a task (µ, η) for which it holds that ERR = e0 and H(y|x) = h0 . [sent-364, score-0.666]
</p><p>7 The curve ℓ is then the union of 0 the curve OHEC and the curve OC, that is, the closed curve OHECO. [sent-464, score-0.372]
</p><p>8 For such a curve simple geometry tells us that its convex hull is bounded by the triangle OAB, the two bow shapes OAO and ABA, and the minimal “concave” curved surface OAB bordered by the curve ℓ and the line segment OB—see Appendix B. [sent-615, score-0.414]
</p><p>9 Consequently, its convex hull is bounded by three ﬂat facets and one curved surface OAB (O is the origin) which is the minimum concave surface with line segment OB and curve ℓ as its boundary. [sent-770, score-0.438]
</p><p>10 The curve ℓ = {[η, u(η), h(η)] | η ∈ [0, 1]} and its convex hull in the η-u-h space; ODE is the intersection of the plane u = 0 and the convex hull, in which we are interested. [sent-794, score-0.38]
</p><p>11 It follows that line NE, the intersection line of the two planes KMN and u = 0, is above the curve OE (the one in Figure 7-a), the intersection of plane u = 0 and the convex hull of ℓ. [sent-872, score-0.394]
</p><p>12 6211; h(η2 ) = hbin (c1 η2 ) = hbin (0) = 0; and h(η3 ) = ˜ hbin (c0 η3 ) = hbin (0) = 0. [sent-1115, score-0.476]
</p><p>13 Therefore, the convex hull of {[e(η), h(η)] | η ∈ Pm } is bounded by the curve OD and the broken line OABCD, which represent the tight lower (Fano) and upper (Tebbe) bounds on the Bayes error rate, ERR, in terms of the conditional entropy, H(y|x). [sent-1261, score-0.466]
</p><p>14 For any D ⊆ Rm , its convex hull, denoted as co D in the paper, is deﬁned as the set of all (ﬁnite) convex combinations of points in D, co D := {∑n αi ui | n ∈ N, ui ∈ D, αi 0, ∑n αi = 1} . [sent-1353, score-1.174]
</p><p>15 i=1 i=1 Another equivalent deﬁnition of co D is that it is the smallest convex set that contains D as a subset. [sent-1354, score-0.546]
</p><p>16 Notice that a vector u ∈ co D if and only if it has at i=1 i=1 least one convex decomposition in D. [sent-1357, score-0.562]
</p><p>17 Moreover, a ∈ co D iff a ∈ D and b ∈ co D iff b ∈ D. [sent-1359, score-0.958]
</p><p>18 But co D is the smallest convex set that contains D, thus co D ⊆ [a, b). [sent-1362, score-1.025]
</p><p>19 It now remains to show that [a, b) ⊆ co D (hence co D = [a, b) and we are done). [sent-1363, score-0.958]
</p><p>20 In particular, replacing the set E by its convex hull in this identity, we obtain  co E = {[u, s] | u ∈ (co E)↓ , s ∈ (co E)↑ } . [sent-1379, score-0.653]
</p><p>21 u  (55)  Lemma 22 For any E ⊆ Rm+1 , it holds that (co E)↓ = co E ↓ . [sent-1380, score-0.479]
</p><p>22 Proof The set co E is convex, so is its projection (co E)↓ —see, for example, Rockafellar (1970, p. [sent-1381, score-0.479]
</p><p>23 It then follows from the minimality of co E ↓ that co E ↓ ⊆ (co E)↓ . [sent-1386, score-0.958]
</p><p>24 Let u ∈ (co E)↓ , then [u, s] ∈ co E for some s ∈ R. [sent-1388, score-0.479]
</p><p>25 It follows from [ui , si ] ∈ E that ui ∈ E ↓ and so u = ∑n αi ui ∈ co E ↓ . [sent-1390, score-0.561]
</p><p>26 We now analyze its second ingredient, (co E)↑ with u ∈ u (co E)↓ = co E ↓ . [sent-1393, score-0.479]
</p><p>27 First of all, since co E is a convex set, so is (co E)↑ . [sent-1394, score-0.546]
</p><p>28 To see this, let s1 , s2 ∈ u (co E)↑ , then [u, s1 ], [u, s2 ] ∈ co E. [sent-1395, score-0.479]
</p><p>29 But co E is a convex set, so for any α ∈ [0, 1] it holds that u ˜ ˜ ˜ α[u, s1 ] + α[u, s2 ] = [u, αs1 + αs2 ] ∈ co E, that is, αs1 + αs2 ∈ (co E)↑ . [sent-1396, score-1.025]
</p><p>30 The notation g(·|·) and g(·|·) allows us to rewrite the supremum of the set (co E)↑ as g(u| co E) and its u inﬁmum as g(u| co E). [sent-1404, score-0.958]
</p><p>31 Here the functions g(·| co E) and g(·| co E) are also deﬁned by Equation (56), but with the set E replaced by co E. [sent-1405, score-1.437]
</p><p>32 That is, g(·| co E) : (co E)↓ → R, u → sup (co E)↑ , u g(·| co E) : (co E)↓ → R, u → inf (co E)↑ . [sent-1406, score-0.976]
</p><p>33 By this lemma, we at once see that the function g(·| co E) is convex and g(·| co E) concave. [sent-1419, score-1.025]
</p><p>34 ↑ Moreover, as E ⊆ co E and hence Eu ⊆ (co E)↑ for any u ∈ Rm , we know from Equations (56)– u (57) that ∀ u ∈ E↓ . [sent-1420, score-0.499]
</p><p>35 g(u| co E) g(u|E) g(u|E) g(u| co E) , Here the domain of g(·|E) and g(·|E), E ↓ , does not need to be convex; and the domain of g(·| co E) and g(·| co E), (co E)↓ = co E ↓ , is the convex hull of the domain of g(·|E) and g(·|E). [sent-1421, score-2.569]
</p><p>36 The concave hull of f is the smallest concave function f ⌢ : co D → R such that f ⌢ (u) f (u) for all u ∈ D; and the convex hull of f is the greatest convex function f⌣ : co D → R with f⌣ (u) f (u) for u ∈ D. [sent-1424, score-1.504]
</p><p>37 In particular, if the domain D is itself a convex set, then co D = D and our deﬁnition of f⌣ and f ⌢ degenerates into the standard deﬁnition. [sent-1425, score-0.546]
</p><p>38 1077  Z HAO , E DAKUNNI , P OCOCK AND B ROWN  Lemma 24 For any bounded subset E ⊆ Rm+1 , the function g(·| co E) is the convex hull of g(·|E); and g(·| co E) is the concave hull of g(·|E). [sent-1429, score-1.338]
</p><p>39 Proof We have shown that g(·| co E) : co E ↓ → R is a convex function which for any u ∈ E ↓ satisﬁes g(u| co E) g(u|E). [sent-1430, score-1.504]
</p><p>40 It thus remains to show that g(·| co E) f (·) for any convex function f : co E ↓ → R satisfying the same condition. [sent-1431, score-1.025]
</p><p>41 Let u ∈ co E ↓ , by deﬁnition, g(u| co E) = inf (co E)↑ . [sent-1432, score-0.976]
</p><p>42 Thus, for any ε > 0, there is an s ∈ u ↑ ↑ (co E)u such that s < g(u| co E) + ε. [sent-1433, score-0.479]
</p><p>43 By s ∈ (co E)u we know [u, s] ∈ co E, so it has a convex ↑ decomposition in E, say [u, s] = ∑n αi · [ui , si ]. [sent-1434, score-0.582]
</p><p>44 Since f : co E ↓ → R is a convex function and since f (·) g(·|E) on E ↓ , by Jensen’s inequality we have f (u) = f (∑n αi ui ) i=1 . [sent-1436, score-0.603]
</p><p>45 i=1  As ε > 0 can be arbitrarily small, the above inequality implies f (u) g(u| co E). [sent-1443, score-0.495]
</p><p>46 We thus have proved that g(·| co E) is the convex hull of g(·|E). [sent-1444, score-0.653]
</p><p>47 By the similar argument, we can prove g(·| co E) is the concave hull of g(·|E). [sent-1445, score-0.685]
</p><p>48 Then any lower (upper) bounded function f : D → R allows for a convex (concave) hull f⌣ ( f ⌢ ) : co D → R. [sent-1447, score-0.653]
</p><p>49 Proof On the set co D deﬁne two functions f ∗ (u) and f∗ (u) by f ∗ (u) := sup{∑n αi · f (ui ) | {(αi , ui )}n a conv. [sent-1448, score-0.52]
</p><p>50 i=1 i=1  (60)  As any u ∈ co D allows for at least one convex decomposition in D, the above set {∑ . [sent-1453, score-0.562]
</p><p>51 By the deﬁnition of f⌣ , to see that f⌣ = f∗ it sufﬁces to show (a) f∗ (·) is a convex function on co D: Let u, v ∈ co D and t ∈ [0, 1], we need to prove f∗ (tu + ˜ ˜ t v) t · f∗ (u) + t · f∗ (v). [sent-1458, score-1.025]
</p><p>52 (c) g(u) f∗ (u) for any g : co D → R satisfying the above conditions (a) and (b), and any u ∈ co D: For any ε > 0, by the deﬁnition of f∗ (u), there is a convex decomposition of u in D, {(αi , ui )}n , such that f∗ (u) > ∑n αi · f (ui ) − ε. [sent-1466, score-1.082]
</p><p>53 The above two lemmas enable us to describe the functions g(·| co E) and g(·| co E) in terms of g(·|E) and g(·|E), respectively. [sent-1470, score-0.958]
</p><p>54 In fact, by putting f (·) = g(·|E) in Equation (59) and f (·) = g(·|E) in Equation (60), we get g(u| co E) = sup{∑n αi g(ui |E) | {(αi , ui )}n a conv. [sent-1471, score-0.52]
</p><p>55 of u in E ↓ } , i=1 i=1  (61)  g(u| co E) = inf{∑n αi g(ui |E) | {(αi , ui )}n a conv. [sent-1473, score-0.52]
</p><p>56 i=1 i=1  (62)  Now let us return to the expression (55), co E = {[u, s] | u ∈ (co E)↓ , s ∈ (co E)↑ }. [sent-1476, score-0.479]
</p><p>57 As has u been pointed out earlier, for any u ∈ (co E)↓ = co E ↓ , the set (co E)↑ is an interval in R with the u two endpoints g(u| co E), g(u| co E) determined respectively by Equation (61) and Equation (62). [sent-1477, score-1.475]
</p><p>58 Then their convex hull co E are also bounded and closed—see, for example, Aliprantis and Border (2006, p. [sent-1480, score-0.653]
</p><p>59 33), which in turn implies the set (co E)↑ can only be a closed u interval, (co E)↑ = [g(u| co E), g(u| co E)]. [sent-1482, score-0.958]
</p><p>60 Equation (55) can thus be rewritten as u co E = {[u, s] | u ∈ co E ↓ , g(u| co E)  s  g(u| co E)} . [sent-1483, score-1.916]
</p><p>61 (63)  The projection E ↓ of a bounded closed set E is also bounded and closed, so the above expression of co E gives naturally rise to a recursive algorithm to construct the convex hull of any bounded and closed set E, as follows. [sent-1484, score-0.653]
</p><p>62 To get co E we need only to ﬁnd co E ↓ and the functions g(·| co E) and g(·| co E) as given by Equations (56), (61) and (62); to get co E ↓ we need to ﬁnd co E ↓↓ and the functions g(·| co E ↓ ) and g(·| co E ↓ ); and so forth. [sent-1485, score-3.832]
</p><p>63 1 T HE C ONVEX H ULL OF ℓCSR The curve ℓCSR lies in the e-h plane; and, by Equation (63), its convex hull can be expressed as co ℓCSR = {[e0 , h0 ] | e0 ∈ co ℓ↓ , g(e0 | co ℓCSR ) CSR  g(e0 | co ℓCSR )} . [sent-1493, score-2.183]
</p><p>64 5] CSR 2 and the above expressions of g(e0 | co ℓCSR ) and g(e0 | co ℓCSR ) into Equation (67), we obtain 1 co ℓCSR = {[e0 , h0 ] | e0 ∈ [0, 0. [sent-1507, score-1.437]
</p><p>65 CSR  As before, to derive the convex hull of the curve ℓFSC , we use Equation (63) and obtain co ℓFSC = {[η, u0 , h0 ] | [η, u0 ] ∈ co ℓ↓ , g(η, u0 | co ℓFSC ) FSC  h0  g(η, u0 | co ℓFSC )}. [sent-1548, score-2.183]
</p><p>66 (71)  The set co ℓ↓ is already known, so it remains to ﬁnd the expressions of g(η, u0 | co ℓFSC ) and FSC g(η, u0 | co ℓFSC ), for which we need ﬁrst to determine the values of g(η, u0 |ℓFSC ) and g(η, u0 |ℓFSC ) for [η, u0 ] ∈ ℓ↓ —see Equation (61) and Equation (62). [sent-1549, score-1.437]
</p><p>67 Therefore, Equation (73) can be simpliﬁed to g(η, u0 | co ℓFSC ) = inf{α1 · h(0) + α2 · h(θ∗ ) + α3 · h(1) | condition on α1,2,3 } . [sent-1573, score-0.479]
</p><p>68 In the above expression, αi 0 are the coefﬁcients occurred when [η, u0 ] ∈ co ℓ↓ is written as the FSC (unique) convex combination of the three points [0, u(0)], [θ∗ , u(θ∗ )] and [1, u(1)]. [sent-1574, score-0.546]
</p><p>69 Therefore, ˜ [u0 + η(θ ˜ ˜ g(η, u0 | co ℓFSC ) = (θ∗ θ∗ )−1 · h(θ∗ ) · [u0 + η(θ∗ − θ∗ )] ,  ∀ [η, u0 ] ∈ co ℓ↓ . [sent-1579, score-0.958]
</p><p>70 FSC  (75)  The expression of g(η, r0 | co ℓBER ) can be derived in a similar way, yielding g(η, r0 | co ℓBER ) = r0 · h(π) ,  ∀ [η, r0 ] ∈ co ℓ↓ . [sent-1580, score-1.437]
</p><p>71 BER  (76)  Note that both g(η, u0 | co ℓFSC ) and g(η, r0 | co ℓBER ) are afﬁne functions. [sent-1581, score-0.958]
</p><p>72 We now study the expression of g(η, u0 | co ℓFSC ), Equation (72). [sent-1586, score-0.479]
</p><p>73 This enables us to consider only convex decompositions i=1 with at most two items when dealing with the function g(η, u0 | co ℓFSC ). [sent-1593, score-0.546]
</p><p>74 That is, Equation (72) can now be simpliﬁed to ˜ g(η, u0 | co ℓFSC ) = sup{t · h(η′ ) + t · h(η′′ ) | condition on t, η′ and η′′ } ,  (77)  ˜ where t, η′ and η′′ should be such that η′ < θ∗ η′′ and {(t , η′ , u(η′ )), (t, η′′ , u(η′′ ))} forms a convex decomposition of [η, u0 ]. [sent-1594, score-0.562]
</p><p>75 The value of g(η, u0 | co ℓFSC ) is ′ ) + t · h(η′′ ) over all such pairs (M, N). [sent-1596, score-0.479]
</p><p>76 The similar method can be used to simplify the expression of g(η, u0 | co ℓFSC ) to the maximum of a function of t, like Equation (41). [sent-1599, score-0.495]
</p><p>77 For the purpose of deriving Theorem 11 and Theorem 15, we will focus only on the case of u0 = 0 for g(η, u0 | co ℓFSC ) and the case of η = π for g(η, r0 | co ℓBER ). [sent-1600, score-0.958]
</p><p>78 Then by Lemma 21 we have co u(A) = [a, b); so it sufﬁces to show a EA [u] < b. [sent-1627, score-0.479]
</p><p>79 If this is not the case, then there exist a random vector u : Ω → Rm and a set A ∈ A such that P(A) > 0 and EA [u] ∈ co u(A). [sent-1636, score-0.479]
</p><p>80 As EA [u] = 0 is a point not in the convex set co u(A), there is a hyperplane separating the two— see for example, Boyd and Vandenberghe (2004, Chapter 2. [sent-1638, score-0.546]
</p><p>81 That is, there exist w ∈ Rm and c ∈ R such that w ·u+c 0 for all u ∈ co u(A) and that w ·0+c = c 0, where w ·u denotes the standard inner product of w and u. [sent-1640, score-0.479]
</p><p>82 A side remark: intuitively, the above argument says that, since co u(A) is convex and EA [u] ∈ / co u(A), we can ﬁrst move the origin to the point EA [u]; then rotate the axes so that co u(A) lies in the half space H 0 := {u = [u1 , . [sent-1650, score-1.504]
</p><p>83 Furthermore, from u(A) ⊆ co u(A) ⊆ H It hence follows from EA [u] = 0 that 0 = P(A) · EA [u] =  A u(ω)dP  =  0  we know A0 ∪ A1 = A. [sent-1657, score-0.499]
</p><p>84 On the other hand, A0 ⊆ A implies co u(A0 ) ⊆ co u(A). [sent-1663, score-0.958]
</p><p>85 So it follows from the assumptions EA [u] ∈ co u(A) and EA [u] = 0 that 0 ∈ co u(A0 ). [sent-1664, score-0.958]
</p><p>86 As u1 (ω) = 0 for all ω ∈ A0 , we have the following “decomposition”: u(A0 ) = {[u1 (ω), v(ω)] | ω ∈ A0 } = {(0, v(ω)) | ω ∈ A0 } = {0} × {v(ω) | ω ∈ A0 } = {0} × v(A0 ) , and hence co u(A0 ) = {0} × co v(A0 ). [sent-1672, score-0.958]
</p><p>87 This fact together with 0 ∈ co u(A0 ) implies that 0 ∈ co v(A0 ). [sent-1673, score-0.958]
</p><p>88 / / For the (m − 1)-dimensional random vector v, the induction hypothesis gives EA0 [v] ∈ co v(A0 ). [sent-1674, score-0.479]
</p><p>89 We have proved both EA0 [u] = 0 and EA0 [u] = 0; this contradiction reveals that the assumption EA [u] ∈ co u(A) must not be true. [sent-1676, score-0.479]
</p><p>90 1 Proof to Theorem 7 By Equations (28), (64) and Theorem 1, we have [CSR, H(y|x)] ∈ co ℓCSR . [sent-1682, score-0.479]
</p><p>91 2 Proof to Theorem 11 By Equations (34), (65) and Theorem 1, we know [π, 2BER, H(y|x)] is in the set co ℓBER , which, by Equation (63), can be written as co ℓBER = {[η, r0 , h0 ] | [η, r0 ] ∈ co ℓ↓ , g(η, r0 | co ℓBER ) BER  h0  g(η, r0 | co ℓBER )}. [sent-1687, score-2.415]
</p><p>92 It follows from Equation (66) and Theorem 1 that [π, 0, H(y|x)] is in the set co ℓFSC . [sent-1715, score-0.479]
</p><p>93 So by Equation (71) we know g(π, 0| co ℓFSC ) H(y|x) g(π, 0| co ℓFSC ), where the range of π is determined by the condition ˜ [π, 0] ∈ co ℓ↓ , which by Equation (70) implies π ∈ [0, θ∗ /θ∗ ]. [sent-1716, score-1.457]
</p><p>94 It thus follows that FSC infπ∈[0,θ∗ /θ∗ ] g(π, 0| co ℓFSC ) ˜  H(y|x)  supπ∈[0,θ∗ /θ∗ ] g(π, 0| co ℓFSC ) . [sent-1717, score-0.958]
</p><p>95 ˜  (81)  ˜ ˜ By Equation (75), we have g(π, 0| co ℓFSC ) = (θ∗ θ∗ )−1 · h(θ∗ ) · π(θ∗ − θ∗ ), so the inﬁmum in Equation (81) is 0, which is obtained at π = 0. [sent-1718, score-0.479]
</p><p>96 In Equation (78) let η = π and relax the resulting expression to g(π, 0| co ℓFSC )  ˜ θ∗ θ∗ ˜ ˜ ˜ sup{t · f2 (t −1 (πθ∗ − tθ∗ )) + t · f1 (θ∗ + t −1 πθ∗ ) | t ∈ [π θ∗ , π θ∗ ]} ˜ ˜ = f1 (θ∗ ) − s(η2 ) · θ∗ + π · [s(η1 )θ∗ + s(η2 )θ∗ ] =: f0 (π) . [sent-1730, score-0.479]
</p><p>97 ˜ Since π ∈ [0, θ∗ /θ∗ ] and f0 (π) is an afﬁne function, the above inequality further implies g(π, 0| co ℓFSC ) As s(η2 )  ˜ max{ f0 (0), f0 (θ∗ /θ∗ )} . [sent-1731, score-0.495]
</p><p>98 1087  Z HAO , E DAKUNNI , P OCOCK AND B ROWN  ˜ ˜ It thus follows that f0 (0) f0 (θ∗ /θ∗ ) and so g(π, 0| co ℓFSC ) h(η1 ) = h(θ∗ /θ∗ ) for any π ∈ ∗ /θ∗ ). [sent-1734, score-0.479]
</p><p>99 Thus, sup h(θ ˜ [0, θ ˜ ˜ π∈[0,θ∗ /θ∗ ] g(π, 0| co ℓFSC ) ∗ /θ∗ in Equation (78), we obtain ˜ On the other hand, let η = π and t = π · θ g(π, 0| co ℓFSC )  ˜ ˜ ˜ ˜ t · h(0) + t · h(θ∗ /θ∗ ) = π · θ∗ /θ∗ · h(θ∗ /θ∗ ). [sent-1736, score-0.958]
</p><p>100 ˜ ˜ Thus, supπ∈[0,θ∗ /θ∗ ] g(π, 0| co ℓFSC ) g(θ∗ /θ∗ , 0| co ℓFSC ) h(θ∗ /θ∗ ). [sent-1737, score-0.958]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fsc', 0.526), ('co', 0.479), ('csr', 0.384), ('ber', 0.262), ('err', 0.139), ('hbin', 0.119), ('dakunni', 0.108), ('ocock', 0.108), ('rown', 0.108), ('entropy', 0.107), ('hull', 0.107), ('ensitive', 0.104), ('onditional', 0.104), ('concave', 0.099), ('hellman', 0.097), ('fano', 0.095), ('curve', 0.093), ('ntropy', 0.089), ('pr', 0.086), ('hao', 0.083), ('isk', 0.08), ('shannon', 0.071), ('infomax', 0.067), ('convex', 0.067), ('balanced', 0.061), ('equation', 0.055), ('ea', 0.053), ('kh', 0.05), ('conditional', 0.05), ('fp', 0.05), ('tp', 0.049), ('risk', 0.047), ('tight', 0.044), ('classi', 0.042), ('oab', 0.041), ('ui', 0.041), ('endpoints', 0.038), ('bounds', 0.037), ('fn', 0.035), ('eu', 0.035), ('rm', 0.034), ('hcs', 0.034), ('rate', 0.032), ('line', 0.032), ('er', 0.031), ('object', 0.03), ('tebbe', 0.03), ('plane', 0.029), ('core', 0.028), ('dp', 0.027), ('hmax', 0.026), ('hmin', 0.026), ('ex', 0.026), ('feature', 0.026), ('tangent', 0.024), ('hs', 0.024), ('shall', 0.023), ('objects', 0.023), ('oc', 0.022), ('prec', 0.022), ('ers', 0.022), ('segment', 0.022), ('symmetric', 0.021), ('task', 0.021), ('equations', 0.021), ('documents', 0.021), ('pm', 0.021), ('mutual', 0.02), ('know', 0.02), ('false', 0.02), ('principle', 0.02), ('manchester', 0.019), ('error', 0.019), ('oe', 0.019), ('ohec', 0.019), ('pocock', 0.019), ('inf', 0.018), ('bayes', 0.018), ('monotonically', 0.018), ('minimum', 0.018), ('intersection', 0.017), ('concerned', 0.017), ('upper', 0.017), ('tn', 0.017), ('retrieved', 0.016), ('inequality', 0.016), ('maximum', 0.016), ('decomposition', 0.016), ('rec', 0.016), ('um', 0.016), ('label', 0.016), ('graph', 0.016), ('lemma', 0.016), ('proper', 0.016), ('dwyer', 0.015), ('edakunni', 0.015), ('linsker', 0.015), ('raviv', 0.015), ('class', 0.015), ('elkan', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="18-tfidf-1" href="./jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications.html">18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</a></p>
<p>Author: Ming-Jie Zhao, Narayanan Edakunni, Adam Pocock, Gavin Brown</p><p>Abstract: Fano’s inequality lower bounds the probability of transmission error through a communication channel. Applied to classiﬁcation problems, it provides a lower bound on the Bayes error rate and motivates the widely used Infomax principle. In modern machine learning, we are often interested in more than just the error rate. In medical diagnosis, different errors incur different cost; hence, the overall risk is cost-sensitive. Two other popular criteria are balanced error rate (BER) and F-score. In this work, we focus on the two-class problem and use a general deﬁnition of conditional entropy (including Shannon’s as a special case) to derive upper/lower bounds on the optimal F-score, BER and cost-sensitive risk, extending Fano’s result. As a consequence, we show that Infomax is not suitable for optimizing F-score or cost-sensitive risk, in that it can potentially lead to low F-score and high risk. For cost-sensitive risk, we propose a new conditional entropy formulation which avoids this inconsistency. In addition, we consider the common practice of using a threshold on the posterior probability to tune performance of a classiﬁer. As is widely known, a threshold of 0.5, where the posteriors cross, minimizes error rate—we derive similar optimal thresholds for F-score and BER. Keywords: balanced error rate, F-score (Fβ -measure), cost-sensitive risk, conditional entropy, lower/upper bound</p><p>2 0.044021029 <a title="18-tfidf-2" href="./jmlr-2013-Regularization-Free_Principal_Curve_Estimation.html">96 jmlr-2013-Regularization-Free Principal Curve Estimation</a></p>
<p>Author: Samuel Gerber, Ross Whitaker</p><p>Abstract: Principal curves and manifolds provide a framework to formulate manifold learning within a statistical context. Principal curves deﬁne the notion of a curve passing through the middle of a distribution. While the intuition is clear, the formal deﬁnition leads to some technical and practical difﬁculties. In particular, principal curves are saddle points of the mean-squared projection distance, which poses severe challenges for estimation and model selection. This paper demonstrates that the difﬁculties in model selection associated with the saddle point property of principal curves are intrinsically tied to the minimization of the mean-squared projection distance. We introduce a new objective function, facilitated through a modiﬁcation of the principal curve estimation approach, for which all critical points are principal curves and minima. Thus, the new formulation removes the fundamental issue for model selection in principal curve estimation. A gradient-descent-based estimator demonstrates the effectiveness of the new formulation for controlling model complexity on numerical experiments with synthetic and real data. Keywords: principal curve, manifold estimation, unsupervised learning, model complexity, model selection</p><p>3 0.041719265 <a title="18-tfidf-3" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>Author: Takafumi Kanamori, Akiko Takeda, Taiji Suzuki</p><p>Abstract: There are two main approaches to binary classiÄ?Ĺš cation problems: the loss function approach and the uncertainty set approach. The loss function approach is widely used in real-world data analysis. Statistical decision theory has been used to elucidate its properties such as statistical consistency. Conditional probabilities can also be estimated by using the minimum solution of the loss function. In the uncertainty set approach, an uncertainty set is deÄ?Ĺš ned for each binary label from training samples. The best separating hyperplane between the two uncertainty sets is used as the decision function. Although the uncertainty set approach provides an intuitive understanding of learning algorithms, its statistical properties have not been sufÄ?Ĺš ciently studied. In this paper, we show that the uncertainty set is deeply connected with the convex conjugate of a loss function. On the basis of the conjugate relation, we propose a way of revising the uncertainty set approach so that it will have good statistical properties such as statistical consistency. We also introduce statistical models corresponding to uncertainty sets in order to estimate conditional probabilities. Finally, we present numerical experiments, verifying that the learning with revised uncertainty sets improves the prediction accuracy. Keywords: loss function, uncertainty set, convex conjugate, consistency</p><p>4 0.039303187 <a title="18-tfidf-4" href="./jmlr-2013-Risk_Bounds_of_Learning_Processes_for_L%C3%A9vy_Processes.html">97 jmlr-2013-Risk Bounds of Learning Processes for Lévy Processes</a></p>
<p>Author: Chao Zhang, Dacheng Tao</p><p>Abstract: L´ vy processes refer to a class of stochastic processes, for example, Poisson processes and Browe nian motions, and play an important role in stochastic processes and machine learning. Therefore, it is essential to study risk bounds of the learning process for time-dependent samples drawn from a L´ vy process (or brieﬂy called learning process for L´ vy process). It is noteworthy that samples e e in this learning process are not independently and identically distributed (i.i.d.). Therefore, results in traditional statistical learning theory are not applicable (or at least cannot be applied directly), because they are obtained under the sample-i.i.d. assumption. In this paper, we study risk bounds of the learning process for time-dependent samples drawn from a L´ vy process, and then analyze the e asymptotical behavior of the learning process. In particular, we ﬁrst develop the deviation inequalities and the symmetrization inequality for the learning process. By using the resultant inequalities, we then obtain the risk bounds based on the covering number. Finally, based on the resulting risk bounds, we study the asymptotic convergence and the rate of convergence of the learning process for L´ vy process. Meanwhile, we also give a comparison to the related results under the samplee i.i.d. assumption. Keywords: L´ vy process, risk bound, deviation inequality, symmetrization inequality, statistical e learning theory, time-dependent</p><p>5 0.037899416 <a title="18-tfidf-5" href="./jmlr-2013-Ranked_Bandits_in_Metric_Spaces%3A_Learning_Diverse_Rankings_over_Large_Document_Collections.html">94 jmlr-2013-Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections</a></p>
<p>Author: Aleksandrs Slivkins, Filip Radlinski, Sreenivas Gollapudi</p><p>Abstract: Most learning to rank research has assumed that the utility of different documents is independent, which results in learned ranking functions that return redundant results. The few approaches that avoid this have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisﬁed users, with several scalable algorithms that explicitly takes document similarity and ranking context into account. Our formulation is a non-trivial common generalization of two multi-armed bandit models from the literature: ranked bandits (Radlinski et al., 2008) and Lipschitz bandits (Kleinberg et al., 2008b). We present theoretical justiﬁcations for this approach, as well as a near-optimal algorithm. Our evaluation adds optimizations that improve empirical performance, and shows that our algorithms learn orders of magnitude more quickly than previous approaches. Keywords: online learning, clickthrough data, diversity, multi-armed bandits, contextual bandits, regret, metric spaces</p><p>6 0.035707217 <a title="18-tfidf-6" href="./jmlr-2013-A_Risk_Comparison_of_Ordinary_Least_Squares_vs_Ridge_Regression.html">7 jmlr-2013-A Risk Comparison of Ordinary Least Squares vs Ridge Regression</a></p>
<p>7 0.035096217 <a title="18-tfidf-7" href="./jmlr-2013-Learning_Theory_Approach_to_Minimum_Error_Entropy_Criterion.html">62 jmlr-2013-Learning Theory Approach to Minimum Error Entropy Criterion</a></p>
<p>8 0.030519292 <a title="18-tfidf-8" href="./jmlr-2013-A_Theory_of_Multiclass_Boosting.html">8 jmlr-2013-A Theory of Multiclass Boosting</a></p>
<p>9 0.030345488 <a title="18-tfidf-9" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>10 0.030248499 <a title="18-tfidf-10" href="./jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning.html">35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</a></p>
<p>11 0.02759248 <a title="18-tfidf-11" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<p>12 0.027069921 <a title="18-tfidf-12" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>13 0.026634449 <a title="18-tfidf-13" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>14 0.025667116 <a title="18-tfidf-14" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>15 0.025633695 <a title="18-tfidf-15" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>16 0.025413176 <a title="18-tfidf-16" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<p>17 0.024714204 <a title="18-tfidf-17" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>18 0.024505666 <a title="18-tfidf-18" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>19 0.024314826 <a title="18-tfidf-19" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>20 0.024313468 <a title="18-tfidf-20" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.129), (1, 0.04), (2, 0.021), (3, 0.019), (4, -0.011), (5, -0.019), (6, 0.038), (7, 0.001), (8, -0.01), (9, -0.032), (10, -0.035), (11, -0.045), (12, 0.027), (13, -0.012), (14, -0.023), (15, 0.089), (16, -0.088), (17, -0.122), (18, -0.066), (19, 0.075), (20, -0.031), (21, -0.058), (22, 0.081), (23, -0.047), (24, 0.117), (25, 0.128), (26, 0.013), (27, 0.087), (28, 0.063), (29, -0.046), (30, -0.112), (31, -0.096), (32, -0.122), (33, -0.041), (34, -0.272), (35, 0.064), (36, 0.19), (37, 0.104), (38, -0.034), (39, 0.073), (40, 0.125), (41, 0.152), (42, 0.112), (43, -0.048), (44, -0.067), (45, 0.02), (46, -0.184), (47, -0.014), (48, -0.121), (49, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91468114 <a title="18-lsi-1" href="./jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications.html">18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</a></p>
<p>Author: Ming-Jie Zhao, Narayanan Edakunni, Adam Pocock, Gavin Brown</p><p>Abstract: Fano’s inequality lower bounds the probability of transmission error through a communication channel. Applied to classiﬁcation problems, it provides a lower bound on the Bayes error rate and motivates the widely used Infomax principle. In modern machine learning, we are often interested in more than just the error rate. In medical diagnosis, different errors incur different cost; hence, the overall risk is cost-sensitive. Two other popular criteria are balanced error rate (BER) and F-score. In this work, we focus on the two-class problem and use a general deﬁnition of conditional entropy (including Shannon’s as a special case) to derive upper/lower bounds on the optimal F-score, BER and cost-sensitive risk, extending Fano’s result. As a consequence, we show that Infomax is not suitable for optimizing F-score or cost-sensitive risk, in that it can potentially lead to low F-score and high risk. For cost-sensitive risk, we propose a new conditional entropy formulation which avoids this inconsistency. In addition, we consider the common practice of using a threshold on the posterior probability to tune performance of a classiﬁer. As is widely known, a threshold of 0.5, where the posteriors cross, minimizes error rate—we derive similar optimal thresholds for F-score and BER. Keywords: balanced error rate, F-score (Fβ -measure), cost-sensitive risk, conditional entropy, lower/upper bound</p><p>2 0.42588285 <a title="18-lsi-2" href="./jmlr-2013-Learning_Theory_Approach_to_Minimum_Error_Entropy_Criterion.html">62 jmlr-2013-Learning Theory Approach to Minimum Error Entropy Criterion</a></p>
<p>Author: Ting Hu, Jun Fan, Qiang Wu, Ding-Xuan Zhou</p><p>Abstract: We consider the minimum error entropy (MEE) criterion and an empirical risk minimization learning algorithm when an approximation of R´ nyi’s entropy (of order 2) by Parzen windowing is e minimized. This learning algorithm involves a Parzen windowing scaling parameter. We present a learning theory approach for this MEE algorithm in a regression setting when the scaling parameter is large. Consistency and explicit convergence rates are provided in terms of the approximation ability and capacity of the involved hypothesis space. Novel analysis is carried out for the generalization error associated with R´ nyi’s entropy and a Parzen windowing function, to overcome e technical difﬁculties arising from the essential differences between the classical least squares problems and the MEE setting. An involved symmetrized least squares error is introduced and analyzed, which is related to some ranking algorithms. Keywords: minimum error entropy, learning theory, R´ nyi’s entropy, empirical risk minimization, e approximation error</p><p>3 0.40380359 <a title="18-lsi-3" href="./jmlr-2013-A_Risk_Comparison_of_Ordinary_Least_Squares_vs_Ridge_Regression.html">7 jmlr-2013-A Risk Comparison of Ordinary Least Squares vs Ridge Regression</a></p>
<p>Author: Paramveer S. Dhillon, Dean P.  Foster, Sham M.  Kakade, Lyle H. Ungar</p><p>Abstract: We compare the risk of ridge regression to a simple variant of ordinary least squares, in which one simply projects the data onto a ﬁnite dimensional subspace (as speciﬁed by a principal component analysis) and then performs an ordinary (un-regularized) least squares regression in this subspace. This note shows that the risk of this ordinary least squares method (PCA-OLS) is within a constant factor (namely 4) of the risk of ridge regression (RR). Keywords: risk inﬂation, ridge regression, pca</p><p>4 0.38566735 <a title="18-lsi-4" href="./jmlr-2013-Risk_Bounds_of_Learning_Processes_for_L%C3%A9vy_Processes.html">97 jmlr-2013-Risk Bounds of Learning Processes for Lévy Processes</a></p>
<p>Author: Chao Zhang, Dacheng Tao</p><p>Abstract: L´ vy processes refer to a class of stochastic processes, for example, Poisson processes and Browe nian motions, and play an important role in stochastic processes and machine learning. Therefore, it is essential to study risk bounds of the learning process for time-dependent samples drawn from a L´ vy process (or brieﬂy called learning process for L´ vy process). It is noteworthy that samples e e in this learning process are not independently and identically distributed (i.i.d.). Therefore, results in traditional statistical learning theory are not applicable (or at least cannot be applied directly), because they are obtained under the sample-i.i.d. assumption. In this paper, we study risk bounds of the learning process for time-dependent samples drawn from a L´ vy process, and then analyze the e asymptotical behavior of the learning process. In particular, we ﬁrst develop the deviation inequalities and the symmetrization inequality for the learning process. By using the resultant inequalities, we then obtain the risk bounds based on the covering number. Finally, based on the resulting risk bounds, we study the asymptotic convergence and the rate of convergence of the learning process for L´ vy process. Meanwhile, we also give a comparison to the related results under the samplee i.i.d. assumption. Keywords: L´ vy process, risk bound, deviation inequality, symmetrization inequality, statistical e learning theory, time-dependent</p><p>5 0.3642064 <a title="18-lsi-5" href="./jmlr-2013-Ranked_Bandits_in_Metric_Spaces%3A_Learning_Diverse_Rankings_over_Large_Document_Collections.html">94 jmlr-2013-Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections</a></p>
<p>Author: Aleksandrs Slivkins, Filip Radlinski, Sreenivas Gollapudi</p><p>Abstract: Most learning to rank research has assumed that the utility of different documents is independent, which results in learned ranking functions that return redundant results. The few approaches that avoid this have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisﬁed users, with several scalable algorithms that explicitly takes document similarity and ranking context into account. Our formulation is a non-trivial common generalization of two multi-armed bandit models from the literature: ranked bandits (Radlinski et al., 2008) and Lipschitz bandits (Kleinberg et al., 2008b). We present theoretical justiﬁcations for this approach, as well as a near-optimal algorithm. Our evaluation adds optimizations that improve empirical performance, and shows that our algorithms learn orders of magnitude more quickly than previous approaches. Keywords: online learning, clickthrough data, diversity, multi-armed bandits, contextual bandits, regret, metric spaces</p><p>6 0.30830356 <a title="18-lsi-6" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>7 0.29792625 <a title="18-lsi-7" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<p>8 0.28676075 <a title="18-lsi-8" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>9 0.26928693 <a title="18-lsi-9" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>10 0.23881941 <a title="18-lsi-10" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>11 0.23800813 <a title="18-lsi-11" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>12 0.23714268 <a title="18-lsi-12" href="./jmlr-2013-Regularization-Free_Principal_Curve_Estimation.html">96 jmlr-2013-Regularization-Free Principal Curve Estimation</a></p>
<p>13 0.2304126 <a title="18-lsi-13" href="./jmlr-2013-Convex_and_Scalable_Weakly_Labeled_SVMs.html">29 jmlr-2013-Convex and Scalable Weakly Labeled SVMs</a></p>
<p>14 0.22831552 <a title="18-lsi-14" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>15 0.21810301 <a title="18-lsi-15" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>16 0.20819646 <a title="18-lsi-16" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>17 0.20600715 <a title="18-lsi-17" href="./jmlr-2013-A_Theory_of_Multiclass_Boosting.html">8 jmlr-2013-A Theory of Multiclass Boosting</a></p>
<p>18 0.20102674 <a title="18-lsi-18" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<p>19 0.19347586 <a title="18-lsi-19" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>20 0.18867591 <a title="18-lsi-20" href="./jmlr-2013-Approximating_the_Permanent_with_Fractional_Belief_Propagation.html">13 jmlr-2013-Approximating the Permanent with Fractional Belief Propagation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.03), (5, 0.125), (6, 0.02), (10, 0.079), (14, 0.012), (20, 0.015), (23, 0.036), (46, 0.389), (53, 0.01), (68, 0.03), (70, 0.054), (75, 0.033), (85, 0.034), (87, 0.011), (89, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.6808548 <a title="18-lda-1" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>Author: Binbin Lin, Xiaofei He, Chiyuan Zhang, Ming Ji</p><p>Abstract: We propose a novel local isometry based dimensionality reduction method from the perspective of vector ﬁelds, which is called parallel vector ﬁeld embedding (PFE). We ﬁrst give a discussion on local isometry and global isometry to show the intrinsic connection between parallel vector ﬁelds and isometry. The problem of ﬁnding an isometry turns out to be equivalent to ﬁnding orthonormal parallel vector ﬁelds on the data manifold. Therefore, we ﬁrst ﬁnd orthonormal parallel vector ﬁelds by solving a variational problem on the manifold. Then each embedding function can be obtained by requiring its gradient ﬁeld to be as close to the corresponding parallel vector ﬁeld as possible. Theoretical results show that our method can precisely recover the manifold if it is isometric to a connected open subset of Euclidean space. Both synthetic and real data examples demonstrate the effectiveness of our method even if there is heavy noise and high curvature. Keywords: manifold learning, isometry, vector ﬁeld, covariant derivative, out-of-sample extension</p><p>same-paper 2 0.68077093 <a title="18-lda-2" href="./jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications.html">18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</a></p>
<p>Author: Ming-Jie Zhao, Narayanan Edakunni, Adam Pocock, Gavin Brown</p><p>Abstract: Fano’s inequality lower bounds the probability of transmission error through a communication channel. Applied to classiﬁcation problems, it provides a lower bound on the Bayes error rate and motivates the widely used Infomax principle. In modern machine learning, we are often interested in more than just the error rate. In medical diagnosis, different errors incur different cost; hence, the overall risk is cost-sensitive. Two other popular criteria are balanced error rate (BER) and F-score. In this work, we focus on the two-class problem and use a general deﬁnition of conditional entropy (including Shannon’s as a special case) to derive upper/lower bounds on the optimal F-score, BER and cost-sensitive risk, extending Fano’s result. As a consequence, we show that Infomax is not suitable for optimizing F-score or cost-sensitive risk, in that it can potentially lead to low F-score and high risk. For cost-sensitive risk, we propose a new conditional entropy formulation which avoids this inconsistency. In addition, we consider the common practice of using a threshold on the posterior probability to tune performance of a classiﬁer. As is widely known, a threshold of 0.5, where the posteriors cross, minimizes error rate—we derive similar optimal thresholds for F-score and BER. Keywords: balanced error rate, F-score (Fβ -measure), cost-sensitive risk, conditional entropy, lower/upper bound</p><p>3 0.41022056 <a title="18-lda-3" href="./jmlr-2013-On_the_Convergence_of_Maximum_Variance_Unfolding.html">77 jmlr-2013-On the Convergence of Maximum Variance Unfolding</a></p>
<p>Author: Ery Arias-Castro, Bruno Pelletier</p><p>Abstract: Maximum Variance Unfolding is one of the main methods for (nonlinear) dimensionality reduction. We study its large sample limit, providing speciﬁc rates of convergence under standard assumptions. We ﬁnd that it is consistent when the underlying submanifold is isometric to a convex subset, and we provide some simple examples where it fails to be consistent. Keywords: maximum variance unfolding, isometric embedding, U-processes, empirical processes, proximity graphs.</p><p>4 0.39261937 <a title="18-lda-4" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>Author: Yuchen Zhang, John C. Duchi, Martin J. Wainwright</p><p>Abstract: We analyze two communication-efﬁcient algorithms for distributed optimization in statistical settings involving large-scale data sets. The ﬁrst algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves √ mean-squared error (MSE) that decays as O (N −1 + (N/m)−2 ). Whenever m ≤ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O (N −1 + (N/m)−3 ), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O (N −1 + (N/m)−3/2 ), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efﬁciently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 108 samples and d ≈ 740,000 covariates. Keywords: distributed learning, stochastic optimization, averaging, subsampling</p><p>5 0.38597992 <a title="18-lda-5" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>Author: Daniil Ryabko, Jérémie Mary</p><p>Abstract: A metric between time-series distributions is proposed that can be evaluated using binary classiﬁcation methods, which were originally developed to work on i.i.d. data. It is shown how this metric can be used for solving statistical problems that are seemingly unrelated to classiﬁcation and concern highly dependent time series. Speciﬁcally, the problems of time-series clustering, homogeneity testing and the three-sample problem are addressed. Universal consistency of the resulting algorithms is proven under most general assumptions. The theoretical results are illustrated with experiments on synthetic and real-world data. Keywords: time series, reductions, stationary ergodic, clustering, metrics between probability distributions</p><p>6 0.38519716 <a title="18-lda-6" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>7 0.38419762 <a title="18-lda-7" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>8 0.38283518 <a title="18-lda-8" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>9 0.38251665 <a title="18-lda-9" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>10 0.38006568 <a title="18-lda-10" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>11 0.37798795 <a title="18-lda-11" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>12 0.37782443 <a title="18-lda-12" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>13 0.37679875 <a title="18-lda-13" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>14 0.37655887 <a title="18-lda-14" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>15 0.37644577 <a title="18-lda-15" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>16 0.37509209 <a title="18-lda-16" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>17 0.37471271 <a title="18-lda-17" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>18 0.3724072 <a title="18-lda-18" href="./jmlr-2013-Manifold_Regularization_and_Semi-supervised_Learning%3A_Some_Theoretical_Analyses.html">69 jmlr-2013-Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses</a></p>
<p>19 0.37230322 <a title="18-lda-19" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>20 0.37186596 <a title="18-lda-20" href="./jmlr-2013-Convex_and_Scalable_Weakly_Labeled_SVMs.html">29 jmlr-2013-Convex and Scalable Weakly Labeled SVMs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
