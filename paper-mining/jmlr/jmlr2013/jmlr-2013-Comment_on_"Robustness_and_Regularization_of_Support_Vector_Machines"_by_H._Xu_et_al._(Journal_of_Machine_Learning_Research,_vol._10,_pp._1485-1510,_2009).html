<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 jmlr-2013-Comment on "Robustness and Regularization of Support Vector Machines" by H. Xu et al. (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-24" href="#">jmlr2013-24</a> knowledge-graph by maker-knowledge-mining</p><h1>24 jmlr-2013-Comment on "Robustness and Regularization of Support Vector Machines" by H. Xu et al. (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009)</h1>
<br/><p>Source: <a title="jmlr-2013-24-pdf" href="http://jmlr.org/papers/volume14/forghani13a/forghani13a.pdf">pdf</a></p><p>Author: Yahya Forghani, Hadi Sadoghi</p><p>Abstract: This paper comments on the published work dealing with robustness and regularization of support vector machines (Journal of Machine Learning Research, Vol. 10, pp. 1485-1510, 2009) by H. Xu et al. They proposed a theorem to show that it is possible to relate robustness in the feature space and robustness in the sample space directly. In this paper, we propose a counter example that rejects their theorem. Keywords: kernel, robustness, support vector machine 1. Comment Firstly, it must be stated that Xu et al. (2009) made a good study of robustness and regularization of support vector machines. They proposed the following theorem to show that it is possible to relate robustness in the feature space and robustness in the sample space directly: Theorem (Xu et al., 2009) Suppose that the kernel function has the form k(x, x′ ) = f ( x − x′ ), with f : R+ → R a decreasing function. Denote by H the RKHS space of k(., .) and φ(.) the corresponding feature mapping. Then we have any x ∈ Rn , w ∈ H and c > 0, sup w, φ(x − δ) = δ ≤c δφ H≤ sup √ 2 f (0)−2 f (c) w, φ(x) − δφ . The following counter example rejects the mentioned theorem. However, this theorem is a standalone result in the appendix of the paper of Xu et al. (2009), which is not used anywhere else in the paper of Xu et al. (2009). Thus, the main result and all other results of Xu et al. (2009) are not affected in any way. Counter example. Let φ(.) be the feature mapping of Gaussian kernel function. We have φ(x) H = 1. Let w = φ(x). Therefore, w, φ(x) = w H , and sup w, φ(x − δ) = w δ ≤c ∗. Also at Islamic Azad University, Mashhad Branch, Mashhad, IRAN. c 2013 Yahya Forghani and Hadi Sadoghi. H. (1) F ORGHANI AND S ADOGHI Moreover, δφ δφ H≤ H≤ w, φ(x) − δφ = sup √ 2 f (0)−2 f (c) w, φ(x) + sup √ 2 f (0)−2 f (c) w H δφ + δφ w H H≤ + w H≤ w, δφ = sup √ 2 f (0)−2 f (c) w, δφ = sup √ 2 f (0)−2 f (c) H 2 f (0) − 2 f (c). (2) According to Equation (1) and (2), and since f is a decreasing function, for any c > 0, we have sup w, φ(x − δ) ≤ δ ≤c δφ H≤ sup √ 2 f (0)−2 f (c) w, φ(x) − δφ . End of counter example. The exact spot that the error has been occurred in the mentioned theorem is Equation (19) of the paper of Xu et al. (2009). There it has been claimed that the image of the RKHS feature mapping is dense, which unfortunately is not true. Indeed, because φ(x), φ(x) = K(0) where K(.) is the kernel function, the image of the feature mapping is in a ball of radius K(0). References Huan Xu, Shie Mannor, and Constantine Caramanis. Robustness and regularization of support vector machines. Journal of Machine Learning Research, 10:1485–1510, 2009. 3494</p><p>Reference: <a title="jmlr-2013-24-reference" href="../jmlr2013_reference/jmlr-2013-Comment_on_%22Robustness_and_Regularization_of_Support_Vector_Machines%22_by_H._Xu_et_al._%28Journal_of_Machine_Learning_Research%2C_vol._10%2C_pp._1485-1510%2C_2009%29_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mashhad', 0.512), ('xu', 0.433), ('azad', 0.256), ('forghan', 0.256), ('had', 0.256), ('sadogh', 0.256), ('yahy', 0.256), ('sup', 0.185), ('robust', 0.153), ('rkhs', 0.139), ('reject', 0.111), ('huan', 0.11), ('standalon', 0.11), ('ir', 0.105), ('constantin', 0.099), ('anywh', 0.099), ('ingo', 0.099), ('yahoo', 0.099), ('count', 0.088), ('spot', 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="24-tfidf-1" href="./jmlr-2013-Comment_on_%22Robustness_and_Regularization_of_Support_Vector_Machines%22_by_H._Xu_et_al._%28Journal_of_Machine_Learning_Research%2C_vol._10%2C_pp._1485-1510%2C_2009%29.html">24 jmlr-2013-Comment on "Robustness and Regularization of Support Vector Machines" by H. Xu et al. (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009)</a></p>
<p>2 0.073018931 <a title="24-tfidf-2" href="./jmlr-2013-Convex_and_Scalable_Weakly_Labeled_SVMs.html">29 jmlr-2013-Convex and Scalable Weakly Labeled SVMs</a></p>
<p>3 0.06238161 <a title="24-tfidf-3" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>4 0.047735382 <a title="24-tfidf-4" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>5 0.040571876 <a title="24-tfidf-5" href="./jmlr-2013-On_the_Convergence_of_Maximum_Variance_Unfolding.html">77 jmlr-2013-On the Convergence of Maximum Variance Unfolding</a></p>
<p>6 0.030699857 <a title="24-tfidf-6" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>7 0.027418725 <a title="24-tfidf-7" href="./jmlr-2013-Language-Motivated_Approaches_to_Action_Recognition.html">58 jmlr-2013-Language-Motivated Approaches to Action Recognition</a></p>
<p>8 0.026910936 <a title="24-tfidf-8" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>9 0.026876183 <a title="24-tfidf-9" href="./jmlr-2013-Universal_Consistency_of_Localized_Versions_of_Regularized_Kernel_Methods.html">117 jmlr-2013-Universal Consistency of Localized Versions of Regularized Kernel Methods</a></p>
<p>10 0.024604611 <a title="24-tfidf-10" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>11 0.024500016 <a title="24-tfidf-11" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>12 0.022934852 <a title="24-tfidf-12" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>13 0.022006297 <a title="24-tfidf-13" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>14 0.021199569 <a title="24-tfidf-14" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>15 0.021084376 <a title="24-tfidf-15" href="./jmlr-2013-Kernel_Bayes%27_Rule%3A_Bayesian_Inference_with_Positive_Definite_Kernels.html">57 jmlr-2013-Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels</a></p>
<p>16 0.020668281 <a title="24-tfidf-16" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>17 0.020485194 <a title="24-tfidf-17" href="./jmlr-2013-Differential_Privacy_for_Functions_and_Functional_Data.html">32 jmlr-2013-Differential Privacy for Functions and Functional Data</a></p>
<p>18 0.019729048 <a title="24-tfidf-18" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>19 0.019410532 <a title="24-tfidf-19" href="./jmlr-2013-Risk_Bounds_of_Learning_Processes_for_L%C3%A9vy_Processes.html">97 jmlr-2013-Risk Bounds of Learning Processes for Lévy Processes</a></p>
<p>20 0.017393081 <a title="24-tfidf-20" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.081), (1, -0.006), (2, -0.0), (3, -0.018), (4, -0.04), (5, -0.019), (6, 0.066), (7, -0.029), (8, 0.086), (9, 0.035), (10, 0.053), (11, -0.051), (12, -0.085), (13, -0.046), (14, -0.02), (15, -0.001), (16, 0.095), (17, 0.027), (18, 0.036), (19, -0.142), (20, 0.206), (21, 0.052), (22, 0.063), (23, -0.008), (24, 0.068), (25, -0.001), (26, -0.085), (27, -0.043), (28, 0.059), (29, 0.044), (30, -0.043), (31, 0.06), (32, -0.403), (33, 0.02), (34, 0.028), (35, 0.235), (36, 0.064), (37, 0.006), (38, 0.025), (39, 0.226), (40, 0.034), (41, 0.138), (42, -0.177), (43, 0.112), (44, -0.07), (45, -0.162), (46, 0.04), (47, 0.026), (48, -0.028), (49, 0.196)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97002649 <a title="24-lsi-1" href="./jmlr-2013-Comment_on_%22Robustness_and_Regularization_of_Support_Vector_Machines%22_by_H._Xu_et_al._%28Journal_of_Machine_Learning_Research%2C_vol._10%2C_pp._1485-1510%2C_2009%29.html">24 jmlr-2013-Comment on "Robustness and Regularization of Support Vector Machines" by H. Xu et al. (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009)</a></p>
<p>2 0.33232492 <a title="24-lsi-2" href="./jmlr-2013-On_the_Convergence_of_Maximum_Variance_Unfolding.html">77 jmlr-2013-On the Convergence of Maximum Variance Unfolding</a></p>
<p>3 0.32182443 <a title="24-lsi-3" href="./jmlr-2013-Convex_and_Scalable_Weakly_Labeled_SVMs.html">29 jmlr-2013-Convex and Scalable Weakly Labeled SVMs</a></p>
<p>4 0.27887434 <a title="24-lsi-4" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>5 0.24741022 <a title="24-lsi-5" href="./jmlr-2013-Kernel_Bayes%27_Rule%3A_Bayesian_Inference_with_Positive_Definite_Kernels.html">57 jmlr-2013-Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels</a></p>
<p>6 0.23618923 <a title="24-lsi-6" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>7 0.22393802 <a title="24-lsi-7" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>8 0.19199184 <a title="24-lsi-8" href="./jmlr-2013-Risk_Bounds_of_Learning_Processes_for_L%C3%A9vy_Processes.html">97 jmlr-2013-Risk Bounds of Learning Processes for Lévy Processes</a></p>
<p>9 0.16355208 <a title="24-lsi-9" href="./jmlr-2013-Learning_Bilinear_Model_for_Matching_Queries_and_Documents.html">60 jmlr-2013-Learning Bilinear Model for Matching Queries and Documents</a></p>
<p>10 0.15556055 <a title="24-lsi-10" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>11 0.14860407 <a title="24-lsi-11" href="./jmlr-2013-Universal_Consistency_of_Localized_Versions_of_Regularized_Kernel_Methods.html">117 jmlr-2013-Universal Consistency of Localized Versions of Regularized Kernel Methods</a></p>
<p>12 0.14829937 <a title="24-lsi-12" href="./jmlr-2013-QuantMiner_for_Mining_Quantitative_Association_Rules.html">89 jmlr-2013-QuantMiner for Mining Quantitative Association Rules</a></p>
<p>13 0.13358729 <a title="24-lsi-13" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>14 0.12891309 <a title="24-lsi-14" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>15 0.12745628 <a title="24-lsi-15" href="./jmlr-2013-Consistent_Selection_of_Tuning_Parameters_via_Variable_Selection_Stability.html">27 jmlr-2013-Consistent Selection of Tuning Parameters via Variable Selection Stability</a></p>
<p>16 0.1251277 <a title="24-lsi-16" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>17 0.12235381 <a title="24-lsi-17" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>18 0.11879823 <a title="24-lsi-18" href="./jmlr-2013-BudgetedSVM%3A_A_Toolbox_for_Scalable_SVM_Approximations.html">19 jmlr-2013-BudgetedSVM: A Toolbox for Scalable SVM Approximations</a></p>
<p>19 0.11419946 <a title="24-lsi-19" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>20 0.11315355 <a title="24-lsi-20" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(21, 0.025), (34, 0.022), (58, 0.049), (61, 0.014), (62, 0.018), (81, 0.021), (83, 0.666), (98, 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.50553423 <a title="24-lda-1" href="./jmlr-2013-Comment_on_%22Robustness_and_Regularization_of_Support_Vector_Machines%22_by_H._Xu_et_al._%28Journal_of_Machine_Learning_Research%2C_vol._10%2C_pp._1485-1510%2C_2009%29.html">24 jmlr-2013-Comment on "Robustness and Regularization of Support Vector Machines" by H. Xu et al. (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009)</a></p>
<p>2 0.10167841 <a title="24-lda-2" href="./jmlr-2013-On_the_Learnability_of_Shuffle_Ideals.html">78 jmlr-2013-On the Learnability of Shuffle Ideals</a></p>
<p>3 0.1000077 <a title="24-lda-3" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>4 0.08986564 <a title="24-lda-4" href="./jmlr-2013-The_CAM_Software_for_Nonnegative_Blind_Source_Separation_in_R-Java.html">113 jmlr-2013-The CAM Software for Nonnegative Blind Source Separation in R-Java</a></p>
<p>5 0.089484684 <a title="24-lda-5" href="./jmlr-2013-Learning_Trees_from_Strings%3A_A_Strong_Learning_Algorithm_for_some_Context-Free_Grammars.html">63 jmlr-2013-Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars</a></p>
<p>6 0.086289175 <a title="24-lda-6" href="./jmlr-2013-Variable_Selection_in_High-Dimension_with_Random_Designs_and_Orthogonal_Matching_Pursuit.html">119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</a></p>
<p>7 0.086173728 <a title="24-lda-7" href="./jmlr-2013-CODA%3A_High_Dimensional_Copula_Discriminant_Analysis.html">20 jmlr-2013-CODA: High Dimensional Copula Discriminant Analysis</a></p>
<p>8 0.086005569 <a title="24-lda-8" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>9 0.084353618 <a title="24-lda-9" href="./jmlr-2013-GURLS%3A_A_Least_Squares_Library_for_Supervised_Learning.html">46 jmlr-2013-GURLS: A Least Squares Library for Supervised Learning</a></p>
<p>10 0.084201783 <a title="24-lda-10" href="./jmlr-2013-Improving_CUR_Matrix_Decomposition_and_the_Nystrom_Approximation_via_Adaptive_Sampling.html">53 jmlr-2013-Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling</a></p>
<p>11 0.084170938 <a title="24-lda-11" href="./jmlr-2013-Efficient_Program_Synthesis_Using_Constraint_Satisfaction_in_Inductive_Logic_Programming.html">40 jmlr-2013-Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming</a></p>
<p>12 0.083701402 <a title="24-lda-12" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>13 0.083576143 <a title="24-lda-13" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<p>14 0.082467228 <a title="24-lda-14" href="./jmlr-2013-Similarity-based_Clustering_by_Left-Stochastic_Matrix_Factorization.html">100 jmlr-2013-Similarity-based Clustering by Left-Stochastic Matrix Factorization</a></p>
<p>15 0.082113162 <a title="24-lda-15" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>16 0.081885308 <a title="24-lda-16" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>17 0.081848867 <a title="24-lda-17" href="./jmlr-2013-Keep_It_Simple_And_Sparse%3A_Real-Time_Action_Recognition.html">56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</a></p>
<p>18 0.081706107 <a title="24-lda-18" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>19 0.08139237 <a title="24-lda-19" href="./jmlr-2013-Truncated_Power_Method_for_Sparse_Eigenvalue_Problems.html">116 jmlr-2013-Truncated Power Method for Sparse Eigenvalue Problems</a></p>
<p>20 0.081226781 <a title="24-lda-20" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
