<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-118" href="#">jmlr2013-118</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</h1>
<br/><p>Source: <a title="jmlr-2013-118-pdf" href="http://jmlr.org/papers/volume14/valsalam13a/valsalam13a.pdf">pdf</a></p><p>Author: Vinod K. Valsalam, Risto Miikkulainen</p><p>Abstract: Sorting networks are an interesting class of parallel sorting algorithms with applications in multiprocessor computers and switching networks. They are built by cascading a series of comparisonexchange units called comparators. Minimizing the number of comparators for a given number of inputs is a challenging optimization problem. This paper presents a two-pronged approach called Symmetry and Evolution based Network Sort Optimization (SENSO) that makes it possible to scale the solutions to networks with a larger number of inputs than previously possible. First, it uses the symmetry of the problem to decompose the minimization goal into subgoals that are easier to solve. Second, it minimizes the resulting greedy solutions further by using an evolutionary algorithm to learn the statistical distribution of comparators in minimal networks. The ﬁnal solutions improve upon half-century of results published in patents, books, and peer-reviewed literature, demonstrating the potential of the SENSO approach for solving difﬁcult combinatorial problems. Keywords: symmetry, evolution, estimation of distribution algorithms, sorting networks, combinatorial optimization</p><p>Reference: <a title="jmlr-2013-118-reference" href="../jmlr2013_reference/jmlr-2013-Using_Symmetry_and_Evolutionary_Search_to_Minimize_Sorting_Networks_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Minimizing the number of comparators for a given number of inputs is a challenging optimization problem. [sent-8, score-0.805]
</p><p>2 Second, it minimizes the resulting greedy solutions further by using an evolutionary algorithm to learn the statistical distribution of comparators in minimal networks. [sent-11, score-0.922]
</p><p>3 Introduction A sorting network of n inputs is a ﬁxed sequence of comparison-exchange operations (comparators) that sorts all inputs of size n (Knuth, 1998). [sent-14, score-0.457]
</p><p>4 Since the same ﬁxed sequence of comparators can sort any input, it represents an oblivious or data-independent sorting algorithm, that is, the sequence of comparisons does not depend on the input data. [sent-15, score-1.086]
</p><p>5 Their networks had the minimal number of comparators for 4, 5, 6, and 8 inputs, but required two extra comparators for 7 inputs. [sent-24, score-1.676]
</p><p>6 Finding the minimum number of comparators for n > 8 is thus a challenging open problem. [sent-34, score-0.776]
</p><p>7 Each such solution is generated by sampling comparators from the previous distribution such that the required network symmetry is built step-by-step, thereby focusing evolution on more likely candidates and making search more effective. [sent-41, score-1.04]
</p><p>8 The number of comparators has been proven to be minimal only for n ≤ 8 (Knuth, 1998). [sent-60, score-0.802]
</p><p>9 Finding the minimum number of comparators required for n > 8 remains an open problem. [sent-65, score-0.776]
</p><p>10 The results in Table 1, for these values of n, improve on the number of comparators used by Batcher’s method. [sent-66, score-0.776]
</p><p>11 Soon afterwards, Green (1972) found a network with 60 comparators (Figure 2), which still remains the best in terms of the number of comparators. [sent-68, score-0.9]
</p><p>12 For such values, Batcher’s method can be extended with more complex merging strategies to produce signiﬁcant savings in the number of comparators (Van Voorhis, 1974; Drysdale and Young, 1975). [sent-72, score-0.813]
</p><p>13 For example, the best known 256-input sorting network due to Van Voorhis requires only 3651 comparators, compared to 3839 comparators required by Batcher’s method (Drysdale and Young, 1975; Knuth, 1998). [sent-73, score-1.139]
</p><p>14 Asymptotically, the methods based on merging require O(n log2 n) comparators (Van Voorhis, 1974). [sent-74, score-0.813]
</p><p>15 The comparators in such hand-designed networks are often symmetrically arranged about a horizontal axis through the middle of the network. [sent-85, score-0.894]
</p><p>16 n Best  17 73  18 79  19 88  20 93  21 103  22 110  23 118  24 123  25 133  26 140  27 150  28 156  29 166  30 172  31 180  32 185  Table 2: The fewest number of comparators known to date for sorting networks of input sizes 17 ≤ n ≤ 32. [sent-87, score-1.201]
</p><p>17 The difﬁculty of ﬁnding such minimal sorting networks prompted researchers to attack the problem using evolutionary techniques. [sent-91, score-0.438]
</p><p>18 In one such study by Hillis (1991), a 16-input network having 61 comparators was evolved. [sent-92, score-0.9]
</p><p>19 This (host) population of sorting networks was co-evolved with a (parasite) population of test cases that were scored based on how well they made the sorting networks fail. [sent-94, score-0.748]
</p><p>20 This method found good networks with the same number of comparators as in Table 1 for all 9 ≤ n ≤ 16. [sent-102, score-0.874]
</p><p>21 Motivated by observations of symmetric arrangement of comparators in many sorting networks (Figure 2), Graham and Oppacher (2006) used symmetry explicitly to bias evolutionary search. [sent-103, score-1.258]
</p><p>22 The symmetric networks were produced using symmetric comparator pairs, that is, pairs of comparators that are vertical mirror images of each other. [sent-105, score-1.136]
</p><p>23 Approach This section presents the new SENSO approach based on symmetry and evolutionary search to minimize the number of comparators in sorting networks. [sent-118, score-1.184]
</p><p>24 It begins with a description of how the sorting network outputs can be represented as monotone Boolean functions, exposing the symmetries of the network. [sent-119, score-0.532]
</p><p>25 Each subgoal constitutes a step in building the symmetries of the network with as few comparators as possible. [sent-121, score-1.174]
</p><p>26 The resulting greedy solutions are optimized further by using an evolutionary algorithm to learn the distribution of comparators that produce minimal networks. [sent-122, score-0.922]
</p><p>27 Since these functions are implemented by the comparators in the network, the problem of designing a sorting network can be restated as the problem of ﬁnding a sequence of comparators that compute its output functions. [sent-131, score-1.995]
</p><p>28 As a result, a sequence of comparators computes Boolean functions 307  VALSALAM AND M IIKKULAINEN  Figure 3: Boolean output functions of a 4-input sorting network. [sent-133, score-1.092]
</p><p>29 Therefore, a sorting network is a sequence of comparators that compute all its output functions from its input variables. [sent-138, score-1.218]
</p><p>30 Since the number of terms in these functions can grow combinatorially as comparators are added, it is necessary to use a representation that makes it efﬁcient to compute them and to determine whether all output functions have been computed. [sent-141, score-0.865]
</p><p>31 Moreover, efﬁcient algorithms for bit-counting can be used to determine if a given sorting network is valid by checking if its function at output i has the value 1 at all level i nodes for 1 ≤ i ≤ n, which is the case when all output functions fi are computed correctly. [sent-180, score-0.574]
</p><p>32 Writing the Boolean output functions of the sorting network in both the disjunctive normal form (DNF) and in the conjunctive normal form (CNF) is a good way to visualize the symmetries of the output functions. [sent-194, score-0.586]
</p><p>33 These symmetries can be used to minimize the number of comparators in the network. [sent-199, score-0.873]
</p><p>34 Finding a minimum sequence of comparators that computes all the output functions is a challenging combinatorial problem. [sent-200, score-0.838]
</p><p>35 2 Sorting Network Symmetries A sorting network symmetry is an operation on the ordered set of network output functions that leaves the functions invariant, that is, the resulting network outputs remain unchanged. [sent-203, score-0.763]
</p><p>36 For example, swapping the outputs of all comparators of a network to reverse its sorting order and then ﬂipping the network vertically to restore its original sorting order is a symmetry operation. [sent-204, score-1.635]
</p><p>37 310  M INIMIZING S ORTING N ETWORKS  Similarly, the subgroups of Σ , that is, subsets that satisfy the group axioms, can be used to represent the symmetries of partial networks created in the process of constructing a full sorting network. [sent-224, score-0.453]
</p><p>38 Initially, before any comparators have been added, each line i in the network has the trivial monotone Boolean function xi . [sent-227, score-0.949]
</p><p>39 Adding comparators to compute the output function fi and its dual fn+1−i yields Γ = {σi } for the resulting partial network. [sent-229, score-0.89]
</p><p>40 Adding more comparators to compute both f j and its dual fn+1− j creates a new partial network with Γ = {σi , σ j }, that is, the new partial network is more symmetric. [sent-230, score-1.056]
</p><p>41 Continuing to add comparators until all output functions have been constructed produces a complete sorting network with Γ = Σ. [sent-231, score-1.197]
</p><p>42 Thus adding comparators to the network in a particular sequence builds its symmetry in a corresponding sequence of increasingly larger subgroups. [sent-232, score-1.026]
</p><p>43 In particular, a sequence of subgroups can represent a sequence of subgoals for minimizing the number of comparators in the network. [sent-235, score-0.888]
</p><p>44 Each subgoal in this sequence is deﬁned as the subgroup that can be produced from the previous subgoal by adding the fewest number of comparators. [sent-236, score-0.463]
</p><p>45 Applying this heuristic to the initial network with symmetry Γ = {}, the ﬁrst subgoal is deﬁned as the symmetry that can be produced from the input variables by computing a pair of dual output functions with the fewest number of comparators (Figure 6). [sent-237, score-1.359]
</p><p>46 ∨ xn have the fewest number of variable combinations and can therefore be computed by adding fewer comparators than any other pair of dual output functions. [sent-244, score-0.906]
</p><p>47 Thus the ﬁrst subgoal is to produce the symmetry Γ = {σ1 } using as few comparators as possible. [sent-245, score-1.023]
</p><p>48 Therefore, from any subgoal that adds the symmetry σk to Γ, the next subgoal adds the symmetry σk+1 to Γ. [sent-249, score-0.494]
</p><p>49 2  Although this subgoal sequence speciﬁes the order in which to compute the output functions, it does not specify an optimal combination of comparators for each subgoal. [sent-254, score-1.0]
</p><p>50 However, it is easier to minimize the number of comparators required for each subgoal than for the entire network, as will be described next. [sent-255, score-0.953]
</p><p>51 Sharing the same comparator to compute dual functions in this manner reduces the number of comparators required in the network. [sent-258, score-1.085]
</p><p>52 The numbers below the comparators indicate the sequence in which the comparators are added during network construction. [sent-262, score-1.712]
</p><p>53 In network (a), adding comparator 3 completes computing f1 and when comparator 4 is added to complete computing its dual f4 , the network gets the symmetry σ1 . [sent-265, score-0.912]
</p><p>54 Adding comparator 5 then completes computing both f2 and its dual f3 , giving the network its second symmetry σ2 . [sent-266, score-0.488]
</p><p>55 Thus the sequence in which the comparators are added determines the sequence in which the network gets its symmetries. [sent-271, score-0.954]
</p><p>56 Conversely, a preferred sequence of symmetries can be speciﬁed to constrain the sequence in which comparators are added and to minimize the number of comparators required. [sent-272, score-1.703]
</p><p>57 minimizing the number of comparators requires determining which comparators can be shared and then adding those comparators that maximize sharing. [sent-273, score-2.348]
</p><p>58 The dual functions f1 and f4 are the easiest to compute, having fewer variable combinations and therefore requiring fewer comparators than f2 and f3 . [sent-279, score-0.823]
</p><p>59 Notice that comparators 1 and 2 compute parts of both f1 and f4 to achieve this subgoal with the minimum number of comparators. [sent-281, score-0.953]
</p><p>60 Adding comparator 5 completes this subgoal since comparators 3 and 4 have already computed f2 and f3 partially. [sent-283, score-1.215]
</p><p>61 Optimizing the number of comparators required to reach each subgoal separately in this way makes it possible to scale the approach to networks with more inputs. [sent-284, score-1.051]
</p><p>62 If that was not the case, it will be impossible to compute at least one of the remaining output functions by adding more comparators since conjunctions preserve 0s and disjunctions preserve 1s of the intermediate functions they combine. [sent-291, score-0.953]
</p><p>63 Thus, comparator 1 contributes to computing both f1 and f4 by setting the values at two nodes in level 2 of its conjunction to 0 and the values at two nodes in level 4 of its disjunction to 1. [sent-302, score-0.539]
</p><p>64 Sharing comparators in this manner reduces the number of comparators required to construct the sorting network. [sent-303, score-1.791]
</p><p>65 The disjunction that this comparator also computes has fewer 0-valued nodes than either of its input functions and is therefore not useful for computing fi . [sent-305, score-0.5]
</p><p>66 The leaves of the resulting binary recursion tree for fi are the functions f j′ that have 0-valued nodes in level i + 1 and its internal nodes are the conjunctive comparator outputs. [sent-312, score-0.538]
</p><p>67 However, the recursion trees for fi and fn+1−i may have common leaves, making it possible to use the same comparator to compute a conjunction for fi and a disjunction for fn+1−i . [sent-314, score-0.489]
</p><p>68 Maximizing such sharing of comparators between the two recursion trees minimizes the number of comparators required for the current subgoal. [sent-315, score-1.595]
</p><p>69 In order to prioritize subgoals 2 and determine which comparators maximize sharing, each pair of lines where a comparator can potentially be added is assigned a utility. [sent-317, score-1.128]
</p><p>70 Similarly, other comparators are also assigned utilities based on the output functions to which they contribute and the subgoals to which those output functions belong. [sent-320, score-0.921]
</p><p>71 Many comparators may have the same highest utility; therefore, one comparator is chosen randomly from that set and it is added to the network. [sent-321, score-1.072]
</p><p>72 Repeating this process produces a sequence of comparators that optimizes sharing within the current subgoal and between the current subgoal and later subgoals. [sent-322, score-1.191]
</p><p>73 The ﬁtness of each solution is the negative of its number of comparators so that improving ﬁtness will minimize the number of comparators. [sent-328, score-0.776]
</p><p>74 Since the greedy algorithm chooses a comparator with the highest utility randomly, this mutation explores a new combination of comparators that might be more optimal than the parent. [sent-331, score-1.099]
</p><p>75 cases, however, the globally minimal networks may use comparators that are different from those suggested by the greedy algorithm. [sent-337, score-0.945]
</p><p>76 The idea is to estimate the probability distribution of comparator u o combinations in the smallest networks evolved thus far and to use this distribution to generate comparator suggestions for the next generation. [sent-341, score-0.724]
</p><p>77 These networks are used in three ways: (1) to estimate the distribution of comparators for a generative model of small networks, (2) as elite networks, passed unmodiﬁed to the next generation, and (3) as parent networks, from which new offspring networks are created for the next generation. [sent-344, score-1.015]
</p><p>78 The state of a partial network is deﬁned in terms of the n Boolean functions that its comparators compute. [sent-346, score-0.915]
</p><p>79 These functions determine the remaining comparators that are needed to ﬁnish computing the output functions, making them a good representation of the partial network. [sent-347, score-0.837]
</p><p>80 In this step, some comparators are also chosen randomly from the set of all potential comparators to encourage exploration of comparator combinations outside the model. [sent-353, score-1.814]
</p><p>81 Moreover, if the model does not generate any comparators for the current state, then the reconstruction step falls back to the greedy algorithm for adding a comparator. [sent-354, score-0.841]
</p><p>82 3, the greedy algorithm chooses the comparator to be added to the network randomly from those that have the highest utility (Variant 1). [sent-356, score-0.465]
</p><p>83 This random choice can be modiﬁed slightly to prefer comparators that are symmetric with respect to another comparator that is already in the network (Variant 2). [sent-357, score-1.162]
</p><p>84 Doing so makes the arrangement of comparators more bilaterally symmetric about a horizontal axis through the middle of the network. [sent-358, score-0.796]
</p><p>85 5) or was selected randomly from the set of all potential comparators (with probability 0. [sent-372, score-0.776]
</p><p>86 According to the Mann-Whitney U-test, the median number of comparators in the smallest networks found by variant 2 was signiﬁcantly fewer for input sizes 13, 15, 18, 20, 22 (p < 0. [sent-383, score-0.913]
</p><p>87 The fewest number of comparators found for each input size is listed in Table 4. [sent-387, score-0.842]
</p><p>88 For 15 inputs, networks matching previous best results were obtained indirectly by removing the bottom line of the evolved 16-input networks and all comparators touching that line (Knuth, 1998). [sent-397, score-1.087]
</p><p>89 For example, since the 22-input network has been improved by two comparators, two copies of it can be merged to construct a 44-input network with four fewer comparators than previous results. [sent-420, score-1.024]
</p><p>90 First, the greedy algorithm for adding comparators can be improved by evaluating the sharing utility of groups of one or more comparators instead of single comparators. [sent-424, score-1.646]
</p><p>91 Second, the greedy algorithm can be made less greedy by considering the impact of current comparator choices on the number of comparators that will be required for later subgoals. [sent-426, score-1.128]
</p><p>92 Fourth, in some cases, good n-input networks can be obtained from n + 1-input networks by simply removing its bottom line and all comparators touching that line (Knuth, 1998), as was done in the 15-input case in this paper. [sent-430, score-0.985]
</p><p>93 Fifth, the EDA generates comparators to add to the network only if the state of the network matches a state in the generative model exactly. [sent-432, score-1.024]
</p><p>94 Instead of minimizing the number of comparators, it would now minimize the number of parallel steps into which the comparators are grouped. [sent-443, score-0.793]
</p><p>95 Conclusion Minimizing the number of comparators in a sorting network is a challenging optimization problem. [sent-455, score-1.139]
</p><p>96 The resulting structure makes it possible to construct the network in steps and to minimize the number of comparators required for each step separately. [sent-457, score-0.9]
</p><p>97 However, the networks constructed in this manner may be sub-optimal greedy solutions, and they are optimized further by an evolutionary algorithm that learns to anticipate the distribution of comparators in minimal networks. [sent-458, score-1.02]
</p><p>98 Evolved Minimal-Size Sorting Networks This appendix lists examples of minimal-size sorting networks evolved by SENSO. [sent-463, score-0.439]
</p><p>99 For each example, the sequence of comparators is illustrated in a ﬁgure and also listed as pairs of horizontal lines numbered from top to bottom. [sent-464, score-0.851]
</p><p>100 Symmetric comparator pairs in the initialization of genetic algorithm populations for sorting networks. [sent-583, score-0.517]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('comparators', 0.776), ('comparator', 0.262), ('sorting', 0.239), ('subgoal', 0.177), ('senso', 0.145), ('network', 0.124), ('boolean', 0.109), ('evolved', 0.102), ('valsalam', 0.101), ('networks', 0.098), ('symmetries', 0.097), ('batcher', 0.088), ('iikkulainen', 0.088), ('inimizing', 0.088), ('orting', 0.088), ('disjunction', 0.082), ('etworks', 0.076), ('evolutionary', 0.075), ('nodes', 0.071), ('symmetry', 0.07), ('voorhis', 0.069), ('knuth', 0.063), ('lattice', 0.057), ('subgoals', 0.057), ('fi', 0.053), ('eda', 0.05), ('monotone', 0.049), ('fewest', 0.049), ('conjunctions', 0.049), ('disjunctions', 0.049), ('fn', 0.048), ('evolution', 0.046), ('greedy', 0.045), ('juill', 0.044), ('tness', 0.042), ('swapping', 0.04), ('conjunctive', 0.038), ('merging', 0.037), ('doi', 0.037), ('population', 0.037), ('antichain', 0.032), ('dual', 0.032), ('baddar', 0.032), ('koza', 0.032), ('sharing', 0.029), ('inputs', 0.029), ('output', 0.029), ('massively', 0.027), ('offspring', 0.027), ('hardware', 0.027), ('minimal', 0.026), ('conjunction', 0.025), ('generations', 0.025), ('kipfer', 0.025), ('search', 0.024), ('levels', 0.024), ('outputs', 0.023), ('sizes', 0.022), ('numbered', 0.022), ('subgroup', 0.022), ('adding', 0.02), ('horizontal', 0.02), ('evolving', 0.02), ('graham', 0.019), ('drysdale', 0.019), ('hiasat', 0.019), ('multiprocessor', 0.019), ('oppacher', 0.019), ('rearrangeable', 0.019), ('risto', 0.019), ('subgroups', 0.019), ('van', 0.018), ('designing', 0.018), ('sequence', 0.018), ('sorts', 0.018), ('added', 0.018), ('sort', 0.018), ('node', 0.018), ('parallel', 0.017), ('representation', 0.017), ('input', 0.017), ('austin', 0.016), ('compositions', 0.016), ('texas', 0.016), ('dnf', 0.016), ('parent', 0.016), ('genetic', 0.016), ('highest', 0.016), ('lters', 0.015), ('switching', 0.015), ('functions', 0.015), ('cnf', 0.015), ('disjunctive', 0.015), ('lines', 0.015), ('level', 0.014), ('recursion', 0.014), ('produces', 0.014), ('touching', 0.013), ('reproduction', 0.013), ('combinatorially', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="118-tfidf-1" href="./jmlr-2013-Using_Symmetry_and_Evolutionary_Search_to_Minimize_Sorting_Networks.html">118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</a></p>
<p>Author: Vinod K. Valsalam, Risto Miikkulainen</p><p>Abstract: Sorting networks are an interesting class of parallel sorting algorithms with applications in multiprocessor computers and switching networks. They are built by cascading a series of comparisonexchange units called comparators. Minimizing the number of comparators for a given number of inputs is a challenging optimization problem. This paper presents a two-pronged approach called Symmetry and Evolution based Network Sort Optimization (SENSO) that makes it possible to scale the solutions to networks with a larger number of inputs than previously possible. First, it uses the symmetry of the problem to decompose the minimization goal into subgoals that are easier to solve. Second, it minimizes the resulting greedy solutions further by using an evolutionary algorithm to learn the statistical distribution of comparators in minimal networks. The ﬁnal solutions improve upon half-century of results published in patents, books, and peer-reviewed literature, demonstrating the potential of the SENSO approach for solving difﬁcult combinatorial problems. Keywords: symmetry, evolution, estimation of distribution algorithms, sorting networks, combinatorial optimization</p><p>2 0.033174943 <a title="118-tfidf-2" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>Author: Pekka Parviainen, Mikko Koivisto</p><p>Abstract: We consider the problem of ﬁnding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space 2n , to within a factor polynomial in the number of nodes n. In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: ﬁrst, the user may trade space against time; second, the proposed algorithms easily and efﬁciently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order P on the nodes, an optimal DAG compatible with P can be found in time and space roughly proportional to the number of ideals of P , which can be signiﬁcantly less than 2n . Considering sufﬁciently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders. Keywords: exact algorithm, parallelization, partial order, space-time tradeoff, structure learning</p><p>3 0.027879151 <a title="118-tfidf-3" href="./jmlr-2013-Optimally_Fuzzy_Temporal_Memory.html">82 jmlr-2013-Optimally Fuzzy Temporal Memory</a></p>
<p>Author: Karthik H. Shankar, Marc W. Howard</p><p>Abstract: Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register—a moving window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an independent general purpose learner cannot a priori know the characteristic prediction-relevant timescale of the signal. Moreover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by resource availability. Here we construct a fuzzy memory system that optimally sacriﬁces the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from exponentially long timescales. Using several illustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better off committing to such a fuzzy memory system. Keywords: temporal information compression, forecasting long range correlated time series</p><p>4 0.024347208 <a title="118-tfidf-4" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>Author: Yuejia He, Yiyuan She, Dapeng Wu</p><p>Abstract: Recently, researchers have proposed penalized maximum likelihood to identify network topology underlying a dynamical system modeled by multivariate time series. The time series of interest are assumed to be stationary, but this restriction is never taken into consideration by existing estimation methods. Moreover, practical problems of interest may have ultra-high dimensionality and obvious node collinearity. In addition, none of the available algorithms provides a probabilistic measure of the uncertainty for the obtained network topology which is informative in reliable network identiﬁcation. The main purpose of this paper is to tackle these challenging issues. We propose the S2 learning framework, which stands for stationary-sparse network learning. We propose a novel algorithm referred to as the Berhu iterative sparsity pursuit with stationarity (BISPS), where the Berhu regularization can improve the Lasso in detection and estimation. The algorithm is extremely easy to implement, efﬁcient in computation and has a theoretical guarantee to converge to a global optimum. We also incorporate a screening technique into BISPS to tackle ultra-high dimensional problems and enhance computational efﬁciency. Furthermore, a stationary bootstrap technique is applied to provide connection occurring frequency for reliable topology learning. Experiments show that our method can achieve stationary and sparse causality network learning and is scalable for high-dimensional problems. Keywords: stationarity, sparsity, Berhu, screening, bootstrap</p><p>5 0.024130337 <a title="118-tfidf-5" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>Author: Julien Mairal, Bin Yu</p><p>Abstract: We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called “path coding” penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efﬁciently solve by leveraging network ﬂow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs. Keywords: convex and non-convex optimization, network ﬂow optimization, graph sparsity</p><p>6 0.022956787 <a title="118-tfidf-6" href="./jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion.html">110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</a></p>
<p>7 0.022803957 <a title="118-tfidf-7" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>8 0.021366239 <a title="118-tfidf-8" href="./jmlr-2013-Segregating_Event_Streams_and_Noise_with_a_Markov_Renewal_Process_Model.html">98 jmlr-2013-Segregating Event Streams and Noise with a Markov Renewal Process Model</a></p>
<p>9 0.019899946 <a title="118-tfidf-9" href="./jmlr-2013-QuantMiner_for_Mining_Quantitative_Association_Rules.html">89 jmlr-2013-QuantMiner for Mining Quantitative Association Rules</a></p>
<p>10 0.018511504 <a title="118-tfidf-10" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>11 0.017968198 <a title="118-tfidf-11" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>12 0.017017312 <a title="118-tfidf-12" href="./jmlr-2013-On_the_Learnability_of_Shuffle_Ideals.html">78 jmlr-2013-On the Learnability of Shuffle Ideals</a></p>
<p>13 0.016081428 <a title="118-tfidf-13" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>14 0.016062532 <a title="118-tfidf-14" href="./jmlr-2013-Algorithms_for_Discovery_of_Multiple_Markov_Boundaries.html">11 jmlr-2013-Algorithms for Discovery of Multiple Markov Boundaries</a></p>
<p>15 0.015149326 <a title="118-tfidf-15" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>16 0.014731104 <a title="118-tfidf-16" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>17 0.014625109 <a title="118-tfidf-17" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>18 0.014505414 <a title="118-tfidf-18" href="./jmlr-2013-Fast_Generalized_Subset_Scan_for_Anomalous_Pattern_Detection.html">42 jmlr-2013-Fast Generalized Subset Scan for Anomalous Pattern Detection</a></p>
<p>19 0.014437704 <a title="118-tfidf-19" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>20 0.013882096 <a title="118-tfidf-20" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.069), (1, 0.004), (2, -0.019), (3, 0.013), (4, 0.039), (5, 0.053), (6, 0.006), (7, 0.024), (8, -0.05), (9, 0.05), (10, 0.006), (11, -0.06), (12, -0.069), (13, -0.033), (14, 0.011), (15, -0.018), (16, -0.038), (17, 0.055), (18, 0.019), (19, 0.035), (20, 0.035), (21, 0.004), (22, -0.11), (23, 0.009), (24, -0.003), (25, -0.072), (26, -0.111), (27, 0.037), (28, 0.232), (29, -0.034), (30, -0.066), (31, -0.133), (32, 0.035), (33, 0.085), (34, -0.001), (35, 0.045), (36, 0.007), (37, 0.058), (38, -0.003), (39, 0.243), (40, 0.008), (41, 0.174), (42, -0.304), (43, 0.116), (44, 0.051), (45, -0.248), (46, 0.01), (47, -0.054), (48, 0.067), (49, 0.278)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96736205 <a title="118-lsi-1" href="./jmlr-2013-Using_Symmetry_and_Evolutionary_Search_to_Minimize_Sorting_Networks.html">118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</a></p>
<p>Author: Vinod K. Valsalam, Risto Miikkulainen</p><p>Abstract: Sorting networks are an interesting class of parallel sorting algorithms with applications in multiprocessor computers and switching networks. They are built by cascading a series of comparisonexchange units called comparators. Minimizing the number of comparators for a given number of inputs is a challenging optimization problem. This paper presents a two-pronged approach called Symmetry and Evolution based Network Sort Optimization (SENSO) that makes it possible to scale the solutions to networks with a larger number of inputs than previously possible. First, it uses the symmetry of the problem to decompose the minimization goal into subgoals that are easier to solve. Second, it minimizes the resulting greedy solutions further by using an evolutionary algorithm to learn the statistical distribution of comparators in minimal networks. The ﬁnal solutions improve upon half-century of results published in patents, books, and peer-reviewed literature, demonstrating the potential of the SENSO approach for solving difﬁcult combinatorial problems. Keywords: symmetry, evolution, estimation of distribution algorithms, sorting networks, combinatorial optimization</p><p>2 0.46168771 <a title="118-lsi-2" href="./jmlr-2013-Optimally_Fuzzy_Temporal_Memory.html">82 jmlr-2013-Optimally Fuzzy Temporal Memory</a></p>
<p>Author: Karthik H. Shankar, Marc W. Howard</p><p>Abstract: Any learner with the ability to predict the future of a structured time-varying signal must maintain a memory of the recent past. If the signal has a characteristic timescale relevant to future prediction, the memory can be a simple shift register—a moving window extending into the past, requiring storage resources that linearly grows with the timescale to be represented. However, an independent general purpose learner cannot a priori know the characteristic prediction-relevant timescale of the signal. Moreover, many naturally occurring signals show scale-free long range correlations implying that the natural prediction-relevant timescale is essentially unbounded. Hence the learner should maintain information from the longest possible timescale allowed by resource availability. Here we construct a fuzzy memory system that optimally sacriﬁces the temporal accuracy of information in a scale-free fashion in order to represent prediction-relevant information from exponentially long timescales. Using several illustrative examples, we demonstrate the advantage of the fuzzy memory system over a shift register in time series forecasting of natural signals. When the available storage resources are limited, we suggest that a general purpose learner would be better off committing to such a fuzzy memory system. Keywords: temporal information compression, forecasting long range correlated time series</p><p>3 0.31102458 <a title="118-lsi-3" href="./jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion.html">110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</a></p>
<p>Author: Rami Mahdi, Jason Mezey</p><p>Abstract: Constraint-based learning of Bayesian networks (BN) from limited data can lead to multiple testing problems when recovering dense areas of the skeleton and to conﬂicting results in the orientation of edges. In this paper, we present a new constraint-based algorithm, light mutual min (LMM) for improved accuracy of BN learning from small sample data. LMM improves the assessment of candidate edges by using a ranking criterion that considers conditional independence on neighboring variables at both sides of an edge simultaneously. The algorithm also employs an adaptive relaxation of constraints that, selectively, allows some nodes not to condition on some neighbors. This relaxation aims at reducing the incorrect rejection of true edges connecting high degree nodes due to multiple testing. LMM additionally incorporates a new criterion for ranking v-structures that is used to recover the completed partially directed acyclic graph (CPDAG) and to resolve conﬂicting v-structures, a common problem in small sample constraint-based learning. Using simulated data, each of these components of LMM is shown to signiﬁcantly improve network inference compared to commonly applied methods when learning from limited data, including more accurate recovery of skeletons and CPDAGs compared to the PC, MaxMin, and MaxMin hill climbing algorithms. A proof of asymptotic correctness is also provided for LMM for recovering the correct skeleton and CPDAG. Keywords: Bayesian networks, skeleton, constraint-based learning, mutual min</p><p>4 0.30860072 <a title="118-lsi-4" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>Author: Pekka Parviainen, Mikko Koivisto</p><p>Abstract: We consider the problem of ﬁnding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space 2n , to within a factor polynomial in the number of nodes n. In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: ﬁrst, the user may trade space against time; second, the proposed algorithms easily and efﬁciently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order P on the nodes, an optimal DAG compatible with P can be found in time and space roughly proportional to the number of ideals of P , which can be signiﬁcantly less than 2n . Considering sufﬁciently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders. Keywords: exact algorithm, parallelization, partial order, space-time tradeoff, structure learning</p><p>5 0.30049309 <a title="118-lsi-5" href="./jmlr-2013-Efficient_Program_Synthesis_Using_Constraint_Satisfaction_in_Inductive_Logic_Programming.html">40 jmlr-2013-Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming</a></p>
<p>Author: John Ahlgren, Shiu Yin Yuen</p><p>Abstract: We present NrSample, a framework for program synthesis in inductive logic programming. NrSample uses propositional logic constraints to exclude undesirable candidates from the search. This is achieved by representing constraints as propositional formulae and solving the associated constraint satisfaction problem. We present a variety of such constraints: pruning, input-output, functional (arithmetic), and variable splitting. NrSample is also capable of detecting search space exhaustion, leading to further speedups in clause induction and optimality. We benchmark NrSample against enumeration search (Aleph’s default) and Progol’s A∗ search in the context of program synthesis. The results show that, on large program synthesis problems, NrSample induces between 1 and 1358 times faster than enumeration (236 times faster on average), always with similar or better accuracy. Compared to Progol A∗ , NrSample is 18 times faster on average with similar or better accuracy except for two problems: one in which Progol A∗ substantially sacriﬁced accuracy to induce faster, and one in which Progol A∗ was a clear winner. Functional constraints provide a speedup of up to 53 times (21 times on average) with similar or better accuracy. We also benchmark using a few concept learning (non-program synthesis) problems. The results indicate that without strong constraints, the overhead of solving constraints is not compensated for. Keywords: inductive logic programming, program synthesis, theory induction, constraint satisfaction, Boolean satisﬁability problem</p><p>6 0.24291429 <a title="118-lsi-6" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>7 0.23863819 <a title="118-lsi-7" href="./jmlr-2013-Fast_Generalized_Subset_Scan_for_Anomalous_Pattern_Detection.html">42 jmlr-2013-Fast Generalized Subset Scan for Anomalous Pattern Detection</a></p>
<p>8 0.19130453 <a title="118-lsi-8" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>9 0.16503602 <a title="118-lsi-9" href="./jmlr-2013-Segregating_Event_Streams_and_Noise_with_a_Markov_Renewal_Process_Model.html">98 jmlr-2013-Segregating Event Streams and Noise with a Markov Renewal Process Model</a></p>
<p>10 0.16395251 <a title="118-lsi-10" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>11 0.16249147 <a title="118-lsi-11" href="./jmlr-2013-Truncated_Power_Method_for_Sparse_Eigenvalue_Problems.html">116 jmlr-2013-Truncated Power Method for Sparse Eigenvalue Problems</a></p>
<p>12 0.16226423 <a title="118-lsi-12" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>13 0.15793453 <a title="118-lsi-13" href="./jmlr-2013-Divvy%3A_Fast_and_Intuitive_Exploratory_Data_Analysis.html">37 jmlr-2013-Divvy: Fast and Intuitive Exploratory Data Analysis</a></p>
<p>14 0.14467131 <a title="118-lsi-14" href="./jmlr-2013-Algorithms_for_Discovery_of_Multiple_Markov_Boundaries.html">11 jmlr-2013-Algorithms for Discovery of Multiple Markov Boundaries</a></p>
<p>15 0.14057443 <a title="118-lsi-15" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>16 0.13529506 <a title="118-lsi-16" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>17 0.12528825 <a title="118-lsi-17" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>18 0.12467781 <a title="118-lsi-18" href="./jmlr-2013-QuantMiner_for_Mining_Quantitative_Association_Rules.html">89 jmlr-2013-QuantMiner for Mining Quantitative Association Rules</a></p>
<p>19 0.11486883 <a title="118-lsi-19" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>20 0.11329627 <a title="118-lsi-20" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.06), (5, 0.069), (6, 0.07), (9, 0.017), (10, 0.047), (20, 0.025), (23, 0.03), (41, 0.012), (68, 0.024), (70, 0.015), (73, 0.431), (75, 0.048), (87, 0.02), (89, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.68694848 <a title="118-lda-1" href="./jmlr-2013-Using_Symmetry_and_Evolutionary_Search_to_Minimize_Sorting_Networks.html">118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</a></p>
<p>Author: Vinod K. Valsalam, Risto Miikkulainen</p><p>Abstract: Sorting networks are an interesting class of parallel sorting algorithms with applications in multiprocessor computers and switching networks. They are built by cascading a series of comparisonexchange units called comparators. Minimizing the number of comparators for a given number of inputs is a challenging optimization problem. This paper presents a two-pronged approach called Symmetry and Evolution based Network Sort Optimization (SENSO) that makes it possible to scale the solutions to networks with a larger number of inputs than previously possible. First, it uses the symmetry of the problem to decompose the minimization goal into subgoals that are easier to solve. Second, it minimizes the resulting greedy solutions further by using an evolutionary algorithm to learn the statistical distribution of comparators in minimal networks. The ﬁnal solutions improve upon half-century of results published in patents, books, and peer-reviewed literature, demonstrating the potential of the SENSO approach for solving difﬁcult combinatorial problems. Keywords: symmetry, evolution, estimation of distribution algorithms, sorting networks, combinatorial optimization</p><p>2 0.66875958 <a title="118-lda-2" href="./jmlr-2013-Kernel_Bayes%27_Rule%3A_Bayesian_Inference_with_Positive_Definite_Kernels.html">57 jmlr-2013-Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels</a></p>
<p>Author: Kenji Fukumizu, Le Song, Arthur Gretton</p><p>Abstract: A kernel method for realizing Bayes’ rule is proposed, based on representations of probabilities in reproducing kernel Hilbert spaces. Probabilities are uniquely characterized by the mean of the canonical map to the RKHS. The prior and conditional probabilities are expressed in terms of RKHS functions of an empirical sample: no explicit parametric model is needed for these quantities. The posterior is likewise an RKHS mean of a weighted sample. The estimator for the expectation of a function of the posterior is derived, and rates of consistency are shown. Some representative applications of the kernel Bayes’ rule are presented, including Bayesian computation without likelihood and ﬁltering with a nonparametric state-space model. Keywords: kernel method, Bayes’ rule, reproducing kernel Hilbert space</p><p>3 0.270704 <a title="118-lda-3" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>Author: Yuejia He, Yiyuan She, Dapeng Wu</p><p>Abstract: Recently, researchers have proposed penalized maximum likelihood to identify network topology underlying a dynamical system modeled by multivariate time series. The time series of interest are assumed to be stationary, but this restriction is never taken into consideration by existing estimation methods. Moreover, practical problems of interest may have ultra-high dimensionality and obvious node collinearity. In addition, none of the available algorithms provides a probabilistic measure of the uncertainty for the obtained network topology which is informative in reliable network identiﬁcation. The main purpose of this paper is to tackle these challenging issues. We propose the S2 learning framework, which stands for stationary-sparse network learning. We propose a novel algorithm referred to as the Berhu iterative sparsity pursuit with stationarity (BISPS), where the Berhu regularization can improve the Lasso in detection and estimation. The algorithm is extremely easy to implement, efﬁcient in computation and has a theoretical guarantee to converge to a global optimum. We also incorporate a screening technique into BISPS to tackle ultra-high dimensional problems and enhance computational efﬁciency. Furthermore, a stationary bootstrap technique is applied to provide connection occurring frequency for reliable topology learning. Experiments show that our method can achieve stationary and sparse causality network learning and is scalable for high-dimensional problems. Keywords: stationarity, sparsity, Berhu, screening, bootstrap</p><p>4 0.26767921 <a title="118-lda-4" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>Author: Qiang Liu, Alexander Ihler</p><p>Abstract: The marginal maximum a posteriori probability (MAP) estimation problem, which calculates the mode of the marginal posterior distribution of a subset of variables with the remaining variables marginalized, is an important inference problem in many models, such as those with hidden variables or uncertain parameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has attracted less attention in the literature compared to the joint MAP (maximization) and marginalization problems. We derive a general dual representation for marginal MAP that naturally integrates the marginalization and maximization operations into a joint variational optimization problem, making it possible to easily extend most or all variational-based algorithms to marginal MAP. In particular, we derive a set of “mixed-product” message passing algorithms for marginal MAP, whose form is a hybrid of max-product, sum-product and a novel “argmax-product” message updates. We also derive a class of convergent algorithms based on proximal point methods, including one that transforms the marginal MAP problem into a sequence of standard marginalization problems. Theoretically, we provide guarantees under which our algorithms give globally or locally optimal solutions, and provide novel upper bounds on the optimal objectives. Empirically, we demonstrate that our algorithms signiﬁcantly outperform the existing approaches, including a state-of-the-art algorithm based on local search methods. Keywords: graphical models, message passing, belief propagation, variational methods, maximum a posteriori, marginal-MAP, hidden variable models</p><p>5 0.26699659 <a title="118-lda-5" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>Author: Philipp Hennig, Martin Kiefel</p><p>Abstract: Four decades after their invention, quasi-Newton methods are still state of the art in unconstrained numerical optimization. Although not usually interpreted thus, these are learning algorithms that ﬁt a local quadratic approximation to the objective function. We show that many, including the most popular, quasi-Newton methods can be interpreted as approximations of Bayesian linear regression under varying prior assumptions. This new notion elucidates some shortcomings of classical algorithms, and lights the way to a novel nonparametric quasi-Newton method, which is able to make more efﬁcient use of available information at computational cost similar to its predecessors. Keywords: optimization, numerical analysis, probability, Gaussian processes</p><p>6 0.26553139 <a title="118-lda-6" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>7 0.26330844 <a title="118-lda-7" href="./jmlr-2013-Segregating_Event_Streams_and_Noise_with_a_Markov_Renewal_Process_Model.html">98 jmlr-2013-Segregating Event Streams and Noise with a Markov Renewal Process Model</a></p>
<p>8 0.25749525 <a title="118-lda-8" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>9 0.25592545 <a title="118-lda-9" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>10 0.255294 <a title="118-lda-10" href="./jmlr-2013-Optimally_Fuzzy_Temporal_Memory.html">82 jmlr-2013-Optimally Fuzzy Temporal Memory</a></p>
<p>11 0.25520468 <a title="118-lda-11" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<p>12 0.2543326 <a title="118-lda-12" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>13 0.25344095 <a title="118-lda-13" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>14 0.25222299 <a title="118-lda-14" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>15 0.2506676 <a title="118-lda-15" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>16 0.25065476 <a title="118-lda-16" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>17 0.24979684 <a title="118-lda-17" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>18 0.24948645 <a title="118-lda-18" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>19 0.24868652 <a title="118-lda-19" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<p>20 0.24769007 <a title="118-lda-20" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
