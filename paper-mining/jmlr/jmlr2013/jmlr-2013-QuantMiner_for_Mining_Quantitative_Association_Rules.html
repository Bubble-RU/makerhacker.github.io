<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 jmlr-2013-QuantMiner for Mining Quantitative Association Rules</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-89" href="#">jmlr2013-89</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>89 jmlr-2013-QuantMiner for Mining Quantitative Association Rules</h1>
<br/><p>Source: <a title="jmlr-2013-89-pdf" href="http://jmlr.org/papers/volume14/salleb-aouissi13a/salleb-aouissi13a.pdf">pdf</a></p><p>Author: Ansaf Salleb-Aouissi, Christel Vrain, Cyril Nortet, Xiangrong Kong, Vivek Rathod, Daniel Cassard</p><p>Abstract: In this paper, we propose Q UANT M INER, a mining quantitative association rules system. This system is based on a genetic algorithm that dynamically discovers “good” intervals in association rules by optimizing both the support and the conﬁdence. The experiments on real and artiﬁcial databases have shown the usefulness of Q UANT M INER as an interactive, exploratory data mining tool. Keywords: association rules, numerical and categorical attributes, unsupervised discretization, genetic algorithm, simulated annealing</p><p>Reference: <a title="jmlr-2013-89-reference" href="../jmlr2013_reference/jmlr-2013-QuantMiner_for_Mining_Quantitative_Association_Rules_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 FR  Laboratoire d’Informatique Fondamentale d’Orl´ ans (LIFO) e Universit´ d’Orl´ ans e e BP 6759, 45067 Orl´ ans Cedex 2, France e  Xiangrong Kong Vivek Rathod  SHARONXKONG @ GMAIL . [sent-7, score-0.468]
</p><p>2 FR  French Geological Survey (BRGM) 3, avenue Claude Guillemin BP 6009, Orl´ ans Cedex 2, France e  Editor: Mikio Braun  Abstract In this paper, we propose Q UANT M INER, a mining quantitative association rules system. [sent-11, score-0.762]
</p><p>3 This system is based on a genetic algorithm that dynamically discovers “good” intervals in association rules by optimizing both the support and the conﬁdence. [sent-12, score-0.557]
</p><p>4 The experiments on real and artiﬁcial databases have shown the usefulness of Q UANT M INER as an interactive, exploratory data mining tool. [sent-13, score-0.172]
</p><p>5 Keywords: association rules, numerical and categorical attributes, unsupervised discretization, genetic algorithm, simulated annealing  1. [sent-14, score-0.505]
</p><p>6 Introduction In this paper, we propose a software for mining quantitative and categorical rules, that implements the work proposed in Salleb-Aouissi et al. [sent-15, score-0.52]
</p><p>7 Given a set of categorical and quantitative attributes and a data set, the aim is to ﬁnd rules built on these attributes that optimize given criteria. [sent-17, score-1.23]
</p><p>8 Expressions occurring in the rules are either A = v for a categorical attribute, or A ∈ [l, u] for a quantitative one. [sent-18, score-0.551]
</p><p>9 Mining quantitative association rules cannot be considered as a direct extension of mining categorical rules. [sent-20, score-0.799]
</p><p>10 While this task has received less attention than mining Boolean association rules (Agrawal et al. [sent-21, score-0.451]
</p><p>11 S ALLEB -AOUISSI ,V RAIN , N ORTET, KONG , R ATHOD AND C ASSARD  numeric attributes into intervals is performed before the mining task in Srikant and Agrawal (1996). [sent-25, score-0.952]
</p><p>12 , Aumann and Lindell, 1999) restrict learning to special kind of rules: the right-hand side of a rule expresses the distribution (e. [sent-28, score-0.18]
</p><p>13 , mean, variance) of numeric attributes, the left-hand side is composed either of a set of categorical attributes or of a single discretized numeric attribute. [sent-30, score-1.216]
</p><p>14 Optimization-based approaches handle numeric attributes during the mining process. [sent-31, score-0.851]
</p><p>15 (2003) but the forms of the rules remain restricted to one or two numeric attributes. [sent-35, score-0.469]
</p><p>16 A genetic algorithm is also proposed in Mata et al. [sent-36, score-0.146]
</p><p>17 (2002) to optimize the support of itemsets deﬁned on uninstantiated intervals of numeric attributes. [sent-37, score-0.475]
</p><p>18 This approach is limited to numeric attributes and optimizes only the support before mining association rules. [sent-38, score-1.021]
</p><p>19 QuantMiner handles both categorical and numerical attributes and optimizes the intervals of numeric attributes during the process of mining association rules. [sent-39, score-1.679]
</p><p>20 It is based on a genetic algorithm, the ﬁtness function aims at maximizing the gain of an association rule while penalizing the attributes with large intervals. [sent-40, score-0.882]
</p><p>21 , 2010) show the interest of evolutionary algorithms for such a task. [sent-44, score-0.048]
</p><p>22 QuantMiner In the following, an item is either an expression A = v, where A is a categorical (also called qualitative) attribute and v is a value from its domain, or an expression A ∈ [l, u] where A is a numerical (also called quantitative) attribute. [sent-46, score-0.387]
</p><p>23 QuantMiner optimizes a set of rule patterns produced from a user-speciﬁed rule template. [sent-47, score-0.347]
</p><p>24 Rule templates: A rule template is a preset format of a quantitative association rule used as a starting point for the quantitative mining process. [sent-48, score-1.113]
</p><p>25 It is deﬁned by the set of attributes occurring in the left hand side and the right hand side or both sides of the rule. [sent-49, score-0.471]
</p><p>26 Categorical attributes may or may not have speciﬁc values. [sent-50, score-0.364]
</p><p>27 Furthermore, an attribute may be mandatory or optional, thus allowing to generate rules of different lengths from the same rule template. [sent-51, score-0.527]
</p><p>28 Given a rule template, ﬁrst for each unspeciﬁed categorical attribute in the template, the frequent values are computed. [sent-52, score-0.57]
</p><p>29 Then, a set of rule patterns verifying the speciﬁcations (position of the attributes, mandatory/optional presence of the attributes, values for categorical attributes either provided, or computed as frequent) are built. [sent-53, score-0.708]
</p><p>30 For each rule pattern, the algorithm looks for the “best” intervals for the numeric attributes occurring in that template, relying on a genetic algorithm. [sent-54, score-1.126]
</p><p>31 1 An example of rule template and a speciﬁc example of rule are given in Figure 1 and in Figure 2 respectively. [sent-56, score-0.454]
</p><p>32 Population: An individual is a set of items of the form attributei ∈ [li , ui ], where attributei is the ith numeric attribute in the rule template from the left to the right. [sent-57, score-0.992]
</p><p>33 The process for generating the population is described in Salleb-Aouissi et al. [sent-58, score-0.027]
</p><p>34 Genetic operators: Mutation and crossover are both used in QuantMiner. [sent-60, score-0.062]
</p><p>35 For the crossover operator, for each attribute the interval is either inherited from one of the parents or formed by mixing the bounds of the two parents. [sent-61, score-0.308]
</p><p>36 For an individual, mutation increases or decreases the lower or upper bound of its intervals. [sent-62, score-0.052]
</p><p>37 Moving interval bounds is done so as to discard/involve no more than 10% of tuples already covered by the interval. [sent-63, score-0.052]
</p><p>38 3154  Q UANT M INER  Figure 1: An example of rule template exploring petal attributes for 2 categories of iris. [sent-69, score-0.79]
</p><p>39 petal length and petal width are chosen to be on the right side of the rules template. [sent-70, score-0.371]
</p><p>40 (1)  Let Anum the set of numerical attributes present in the rule A ⇒ B. [sent-73, score-0.515]
</p><p>41 Let Ia denote the interval of the attribute a ∈ Anum . [sent-74, score-0.246]
</p><p>42 In the following, size(a) denotes the length of the smallest interval which contains all the data for the attribute a and size(Ia ) denotes the length of the interval Ia . [sent-75, score-0.298]
</p><p>43 If Gain(A ⇒ B) is negative, then the ﬁtness of the rule is set to the gain. [sent-76, score-0.151]
</p><p>44 If it is positive (the conﬁdence of the rule exceeds the minimum conﬁdence threshold), the proportions of the intervals (deﬁned as the ratios between the sizes and the domains) is taken into account, so as to favor those with small sizes as shown in Equation 2. [sent-77, score-0.252]
</p><p>45 Moreover, rules with low supports are penalized by decreasing drastically their ﬁtness values. [sent-78, score-0.154]
</p><p>46 Implementation We developed Q UANT M INER in JAVA as a 5-step GUI wizard allowing an interactive mining process. [sent-81, score-0.215]
</p><p>47 2 After opening a data set, the user can choose attributes, a rule template, the optimization technique and set its parameters, launch the process, and ﬁnally display the rules with various sorting options: support, conﬁdence or rule length. [sent-82, score-0.482]
</p><p>48 3155  S ALLEB -AOUISSI ,V RAIN , N ORTET, KONG , R ATHOD AND C ASSARD  Figure 2: Example of rule visualization in QuantMiner. [sent-87, score-0.151]
</p><p>49 The top part shows ﬁltering criteria that facilitate exploring the rules. [sent-88, score-0.029]
</p><p>50 The bottom part shows a speciﬁc rule followed by the proportion of each interval in its corresponding domain. [sent-89, score-0.203]
</p><p>51 More measures are given to assess the quality of the rule, for example, con f idence(¬A ⇒ B). [sent-90, score-0.028]
</p><p>52 previous steps, change the method, parameters, templates and restart the learning. [sent-91, score-0.08]
</p><p>53 Note that simulated annealing is implemented in QuantMiner as an alternative optimization method. [sent-92, score-0.041]
</p><p>54 A tentative for mining rules with disjunctive intervals is also implemented. [sent-93, score-0.455]
</p><p>55 Acknowledgments This project has been supported by the BRGM-French Geological Survey, LIFO-Universit´ d’Orl´ ans e e and CCLS-Columbia University. [sent-95, score-0.156]
</p><p>56 Many thanks to everyone who contributed to QuantMiner with support, ideas, data and feedback. [sent-96, score-0.031]
</p><p>57 Mining association rules between sets of items in large databases. [sent-102, score-0.313]
</p><p>58 Analysis of the effectiveness of the a e genetic algorithms based on extraction of association rules. [sent-109, score-0.271]
</p><p>59 Quantminer: A genetic algorithm for mining quantitative association rules. [sent-148, score-0.598]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('attributes', 0.364), ('numeric', 0.315), ('quantminer', 0.255), ('attribute', 0.194), ('categorical', 0.193), ('iner', 0.182), ('orl', 0.182), ('uant', 0.182), ('mining', 0.172), ('ans', 0.156), ('quantitative', 0.155), ('rules', 0.154), ('template', 0.152), ('rule', 0.151), ('genetic', 0.146), ('association', 0.125), ('ia', 0.125), ('anum', 0.109), ('ccls', 0.109), ('fukuda', 0.109), ('ortet', 0.109), ('vrain', 0.109), ('intervals', 0.101), ('kong', 0.097), ('gain', 0.096), ('petal', 0.094), ('rain', 0.094), ('ansaf', 0.084), ('alcal', 0.073), ('alleb', 0.073), ('assard', 0.073), ('athod', 0.073), ('attributei', 0.073), ('aumann', 0.073), ('brgm', 0.073), ('brin', 0.073), ('cassard', 0.073), ('christel', 0.073), ('cyril', 0.073), ('fitness', 0.073), ('geological', 0.073), ('mata', 0.073), ('nortet', 0.073), ('rathod', 0.073), ('vivek', 0.073), ('xiangrong', 0.073), ('tness', 0.073), ('agrawal', 0.065), ('columbia', 0.065), ('orleans', 0.062), ('srikant', 0.062), ('riverside', 0.062), ('crossover', 0.062), ('templates', 0.052), ('sigmod', 0.052), ('mutation', 0.052), ('interval', 0.052), ('occurring', 0.049), ('bp', 0.048), ('cedex', 0.048), ('evolutionary', 0.048), ('optimizes', 0.045), ('interactive', 0.043), ('annealing', 0.041), ('dence', 0.04), ('fr', 0.04), ('daniel', 0.039), ('supp', 0.038), ('drive', 0.034), ('gmail', 0.034), ('items', 0.034), ('frequent', 0.032), ('mikio', 0.031), ('discovers', 0.031), ('everyone', 0.031), ('uninstantiated', 0.031), ('alvarez', 0.031), ('claude', 0.031), ('binning', 0.031), ('gui', 0.031), ('france', 0.031), ('exploring', 0.029), ('side', 0.029), ('con', 0.028), ('braun', 0.028), ('pods', 0.028), ('laboratoire', 0.028), ('preset', 0.028), ('disjunctive', 0.028), ('itemsets', 0.028), ('mandatory', 0.028), ('optional', 0.028), ('restart', 0.028), ('acm', 0.028), ('population', 0.027), ('functionality', 0.026), ('opening', 0.026), ('unspeci', 0.026), ('relational', 0.024), ('format', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="89-tfidf-1" href="./jmlr-2013-QuantMiner_for_Mining_Quantitative_Association_Rules.html">89 jmlr-2013-QuantMiner for Mining Quantitative Association Rules</a></p>
<p>Author: Ansaf Salleb-Aouissi, Christel Vrain, Cyril Nortet, Xiangrong Kong, Vivek Rathod, Daniel Cassard</p><p>Abstract: In this paper, we propose Q UANT M INER, a mining quantitative association rules system. This system is based on a genetic algorithm that dynamically discovers “good” intervals in association rules by optimizing both the support and the conﬁdence. The experiments on real and artiﬁcial databases have shown the usefulness of Q UANT M INER as an interactive, exploratory data mining tool. Keywords: association rules, numerical and categorical attributes, unsupervised discretization, genetic algorithm, simulated annealing</p><p>2 0.12692004 <a title="89-tfidf-2" href="./jmlr-2013-Fast_Generalized_Subset_Scan_for_Anomalous_Pattern_Detection.html">42 jmlr-2013-Fast Generalized Subset Scan for Anomalous Pattern Detection</a></p>
<p>Author: Edward McFowland III, Skyler Speakman, Daniel B. Neill</p><p>Abstract: We propose Fast Generalized Subset Scan (FGSS), a new method for detecting anomalous patterns in general categorical data sets. We frame the pattern detection problem as a search over subsets of data records and attributes, maximizing a nonparametric scan statistic over all such subsets. We prove that the nonparametric scan statistics possess a novel property that allows for efﬁcient optimization over the exponentially many subsets of the data without an exhaustive search, enabling FGSS to scale to massive and high-dimensional data sets. We evaluate the performance of FGSS in three real-world application domains (customs monitoring, disease surveillance, and network intrusion detection), and demonstrate that FGSS can successfully detect and characterize relevant patterns in each domain. As compared to three other recently proposed detection algorithms, FGSS substantially decreased run time and improved detection power for massive multivariate data sets. Keywords: pattern detection, anomaly detection, knowledge discovery, Bayesian networks, scan statistics</p><p>3 0.086174041 <a title="89-tfidf-3" href="./jmlr-2013-Alleviating_Naive_Bayes_Attribute_Independence_Assumption_by_Attribute_Weighting.html">12 jmlr-2013-Alleviating Naive Bayes Attribute Independence Assumption by Attribute Weighting</a></p>
<p>Author: Nayyar A. Zaidi, Jesús Cerquides, Mark J. Carman, Geoffrey I. Webb</p><p>Abstract: Despite the simplicity of the Naive Bayes classiﬁer, it has continued to perform well against more sophisticated newcomers and has remained, therefore, of great interest to the machine learning community. Of numerous approaches to reﬁning the naive Bayes classiﬁer, attribute weighting has received less attention than it warrants. Most approaches, perhaps inﬂuenced by attribute weighting in other machine learning algorithms, use weighting to place more emphasis on highly predictive attributes than those that are less predictive. In this paper, we argue that for naive Bayes attribute weighting should instead be used to alleviate the conditional independence assumption. Based on this premise, we propose a weighted naive Bayes algorithm, called WANBIA, that selects weights to minimize either the negative conditional log likelihood or the mean squared error objective functions. We perform extensive evaluations and ﬁnd that WANBIA is a competitive alternative to state of the art classiﬁers like Random Forest, Logistic Regression and A1DE. Keywords: classiﬁcation, naive Bayes, attribute independence assumption, weighted naive Bayes classiﬁcation</p><p>4 0.068960376 <a title="89-tfidf-4" href="./jmlr-2013-Learning_Theory_Analysis_for_Association_Rules_and_Sequential_Event_Prediction.html">61 jmlr-2013-Learning Theory Analysis for Association Rules and Sequential Event Prediction</a></p>
<p>Author: Cynthia Rudin, Benjamin Letham, David Madigan</p><p>Abstract: We present a theoretical analysis for prediction algorithms based on association rules. As part of this analysis, we introduce a problem for which rules are particularly natural, called “sequential event prediction.” In sequential event prediction, events in a sequence are revealed one by one, and the goal is to determine which event will next be revealed. The training set is a collection of past sequences of events. An example application is to predict which item will next be placed into a customer’s online shopping cart, given his/her past purchases. In the context of this problem, algorithms based on association rules have distinct advantages over classical statistical and machine learning methods: they look at correlations based on subsets of co-occurring past events (items a and b imply item c), they can be applied to the sequential event prediction problem in a natural way, they can potentially handle the “cold start” problem where the training set is small, and they yield interpretable predictions. In this work, we present two algorithms that incorporate association rules. These algorithms can be used both for sequential event prediction and for supervised classiﬁcation, and they are simple enough that they can possibly be understood by users, customers, patients, managers, etc. We provide generalization guarantees on these algorithms based on algorithmic stability analysis from statistical learning theory. We include a discussion of the strict minimum support threshold often used in association rule mining, and introduce an “adjusted conﬁdence” measure that provides a weaker minimum support condition that has advantages over the strict minimum support. The paper brings together ideas from statistical learning theory, association rule mining and Bayesian analysis. Keywords: statistical learning theory, algorithmic stability, association rules, sequence prediction, associative classiﬁcation c 2013 Cynthia Rudin, Benjamin Letham and David Madigan. RUDIN , L E</p><p>5 0.034244105 <a title="89-tfidf-5" href="./jmlr-2013-Ranking_Forests.html">95 jmlr-2013-Ranking Forests</a></p>
<p>Author: Stéphan Clémençon, Marine Depecker, Nicolas Vayatis</p><p>Abstract: The present paper examines how the aggregation and feature randomization principles underlying the algorithm R ANDOM F OREST (Breiman, 2001) can be adapted to bipartite ranking. The approach taken here is based on nonparametric scoring and ROC curve optimization in the sense of the AUC criterion. In this problem, aggregation is used to increase the performance of scoring rules produced by ranking trees, as those developed in Cl´ mencon and Vayatis (2009c). The present work e ¸ describes the principles for building median scoring rules based on concepts from rank aggregation. Consistency results are derived for these aggregated scoring rules and an algorithm called R ANK ING F OREST is presented. Furthermore, various strategies for feature randomization are explored through a series of numerical experiments on artiﬁcial data sets. Keywords: bipartite ranking, nonparametric scoring, classiﬁcation data, ROC optimization, AUC criterion, tree-based ranking rules, bootstrap, bagging, rank aggregation, median ranking, feature randomization</p><p>6 0.026915144 <a title="89-tfidf-6" href="./jmlr-2013-The_CAM_Software_for_Nonnegative_Blind_Source_Separation_in_R-Java.html">113 jmlr-2013-The CAM Software for Nonnegative Blind Source Separation in R-Java</a></p>
<p>7 0.023896327 <a title="89-tfidf-7" href="./jmlr-2013-Counterfactual_Reasoning_and_Learning_Systems%3A_The_Example_of_Computational_Advertising.html">30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</a></p>
<p>8 0.022952773 <a title="89-tfidf-8" href="./jmlr-2013-Orange%3A_Data_Mining_Toolbox_in_Python.html">83 jmlr-2013-Orange: Data Mining Toolbox in Python</a></p>
<p>9 0.020296123 <a title="89-tfidf-9" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>10 0.019899946 <a title="89-tfidf-10" href="./jmlr-2013-Using_Symmetry_and_Evolutionary_Search_to_Minimize_Sorting_Networks.html">118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</a></p>
<p>11 0.019644029 <a title="89-tfidf-11" href="./jmlr-2013-AC%2B%2BTemplate-Based_Reinforcement_Learning_Library%3A_Fitting_the_Code_to_the_Mathematics.html">1 jmlr-2013-AC++Template-Based Reinforcement Learning Library: Fitting the Code to the Mathematics</a></p>
<p>12 0.018706897 <a title="89-tfidf-12" href="./jmlr-2013-Approximating_the_Permanent_with_Fractional_Belief_Propagation.html">13 jmlr-2013-Approximating the Permanent with Fractional Belief Propagation</a></p>
<p>13 0.018195007 <a title="89-tfidf-13" href="./jmlr-2013-Tapkee%3A_An_Efficient_Dimension_Reduction_Library.html">112 jmlr-2013-Tapkee: An Efficient Dimension Reduction Library</a></p>
<p>14 0.017862262 <a title="89-tfidf-14" href="./jmlr-2013-Optimal_Discovery_with_Probabilistic_Expert_Advice%3A_Finite_Time_Analysis_and_Macroscopic_Optimality.html">81 jmlr-2013-Optimal Discovery with Probabilistic Expert Advice: Finite Time Analysis and Macroscopic Optimality</a></p>
<p>15 0.016313061 <a title="89-tfidf-15" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>16 0.015940866 <a title="89-tfidf-16" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>17 0.014795123 <a title="89-tfidf-17" href="./jmlr-2013-On_the_Convergence_of_Maximum_Variance_Unfolding.html">77 jmlr-2013-On the Convergence of Maximum Variance Unfolding</a></p>
<p>18 0.014002223 <a title="89-tfidf-18" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>19 0.013217756 <a title="89-tfidf-19" href="./jmlr-2013-Segregating_Event_Streams_and_Noise_with_a_Markov_Renewal_Process_Model.html">98 jmlr-2013-Segregating Event Streams and Noise with a Markov Renewal Process Model</a></p>
<p>20 0.013162604 <a title="89-tfidf-20" href="./jmlr-2013-Ranked_Bandits_in_Metric_Spaces%3A_Learning_Diverse_Rankings_over_Large_Document_Collections.html">94 jmlr-2013-Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.065), (1, 0.006), (2, -0.025), (3, 0.025), (4, 0.015), (5, 0.038), (6, 0.004), (7, 0.038), (8, -0.04), (9, 0.005), (10, 0.007), (11, -0.231), (12, 0.018), (13, -0.315), (14, 0.05), (15, -0.073), (16, -0.404), (17, 0.083), (18, -0.026), (19, -0.265), (20, -0.129), (21, 0.086), (22, 0.007), (23, -0.079), (24, -0.029), (25, 0.036), (26, -0.089), (27, -0.09), (28, 0.016), (29, -0.027), (30, -0.058), (31, 0.075), (32, 0.109), (33, -0.038), (34, 0.061), (35, 0.039), (36, -0.04), (37, 0.021), (38, 0.024), (39, -0.093), (40, -0.023), (41, -0.002), (42, -0.083), (43, -0.044), (44, -0.024), (45, -0.058), (46, 0.009), (47, -0.004), (48, -0.03), (49, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9863807 <a title="89-lsi-1" href="./jmlr-2013-QuantMiner_for_Mining_Quantitative_Association_Rules.html">89 jmlr-2013-QuantMiner for Mining Quantitative Association Rules</a></p>
<p>Author: Ansaf Salleb-Aouissi, Christel Vrain, Cyril Nortet, Xiangrong Kong, Vivek Rathod, Daniel Cassard</p><p>Abstract: In this paper, we propose Q UANT M INER, a mining quantitative association rules system. This system is based on a genetic algorithm that dynamically discovers “good” intervals in association rules by optimizing both the support and the conﬁdence. The experiments on real and artiﬁcial databases have shown the usefulness of Q UANT M INER as an interactive, exploratory data mining tool. Keywords: association rules, numerical and categorical attributes, unsupervised discretization, genetic algorithm, simulated annealing</p><p>2 0.79997838 <a title="89-lsi-2" href="./jmlr-2013-Fast_Generalized_Subset_Scan_for_Anomalous_Pattern_Detection.html">42 jmlr-2013-Fast Generalized Subset Scan for Anomalous Pattern Detection</a></p>
<p>Author: Edward McFowland III, Skyler Speakman, Daniel B. Neill</p><p>Abstract: We propose Fast Generalized Subset Scan (FGSS), a new method for detecting anomalous patterns in general categorical data sets. We frame the pattern detection problem as a search over subsets of data records and attributes, maximizing a nonparametric scan statistic over all such subsets. We prove that the nonparametric scan statistics possess a novel property that allows for efﬁcient optimization over the exponentially many subsets of the data without an exhaustive search, enabling FGSS to scale to massive and high-dimensional data sets. We evaluate the performance of FGSS in three real-world application domains (customs monitoring, disease surveillance, and network intrusion detection), and demonstrate that FGSS can successfully detect and characterize relevant patterns in each domain. As compared to three other recently proposed detection algorithms, FGSS substantially decreased run time and improved detection power for massive multivariate data sets. Keywords: pattern detection, anomaly detection, knowledge discovery, Bayesian networks, scan statistics</p><p>3 0.46372253 <a title="89-lsi-3" href="./jmlr-2013-Alleviating_Naive_Bayes_Attribute_Independence_Assumption_by_Attribute_Weighting.html">12 jmlr-2013-Alleviating Naive Bayes Attribute Independence Assumption by Attribute Weighting</a></p>
<p>Author: Nayyar A. Zaidi, Jesús Cerquides, Mark J. Carman, Geoffrey I. Webb</p><p>Abstract: Despite the simplicity of the Naive Bayes classiﬁer, it has continued to perform well against more sophisticated newcomers and has remained, therefore, of great interest to the machine learning community. Of numerous approaches to reﬁning the naive Bayes classiﬁer, attribute weighting has received less attention than it warrants. Most approaches, perhaps inﬂuenced by attribute weighting in other machine learning algorithms, use weighting to place more emphasis on highly predictive attributes than those that are less predictive. In this paper, we argue that for naive Bayes attribute weighting should instead be used to alleviate the conditional independence assumption. Based on this premise, we propose a weighted naive Bayes algorithm, called WANBIA, that selects weights to minimize either the negative conditional log likelihood or the mean squared error objective functions. We perform extensive evaluations and ﬁnd that WANBIA is a competitive alternative to state of the art classiﬁers like Random Forest, Logistic Regression and A1DE. Keywords: classiﬁcation, naive Bayes, attribute independence assumption, weighted naive Bayes classiﬁcation</p><p>4 0.41894236 <a title="89-lsi-4" href="./jmlr-2013-Learning_Theory_Analysis_for_Association_Rules_and_Sequential_Event_Prediction.html">61 jmlr-2013-Learning Theory Analysis for Association Rules and Sequential Event Prediction</a></p>
<p>Author: Cynthia Rudin, Benjamin Letham, David Madigan</p><p>Abstract: We present a theoretical analysis for prediction algorithms based on association rules. As part of this analysis, we introduce a problem for which rules are particularly natural, called “sequential event prediction.” In sequential event prediction, events in a sequence are revealed one by one, and the goal is to determine which event will next be revealed. The training set is a collection of past sequences of events. An example application is to predict which item will next be placed into a customer’s online shopping cart, given his/her past purchases. In the context of this problem, algorithms based on association rules have distinct advantages over classical statistical and machine learning methods: they look at correlations based on subsets of co-occurring past events (items a and b imply item c), they can be applied to the sequential event prediction problem in a natural way, they can potentially handle the “cold start” problem where the training set is small, and they yield interpretable predictions. In this work, we present two algorithms that incorporate association rules. These algorithms can be used both for sequential event prediction and for supervised classiﬁcation, and they are simple enough that they can possibly be understood by users, customers, patients, managers, etc. We provide generalization guarantees on these algorithms based on algorithmic stability analysis from statistical learning theory. We include a discussion of the strict minimum support threshold often used in association rule mining, and introduce an “adjusted conﬁdence” measure that provides a weaker minimum support condition that has advantages over the strict minimum support. The paper brings together ideas from statistical learning theory, association rule mining and Bayesian analysis. Keywords: statistical learning theory, algorithmic stability, association rules, sequence prediction, associative classiﬁcation c 2013 Cynthia Rudin, Benjamin Letham and David Madigan. RUDIN , L E</p><p>5 0.26431078 <a title="89-lsi-5" href="./jmlr-2013-Ranking_Forests.html">95 jmlr-2013-Ranking Forests</a></p>
<p>Author: Stéphan Clémençon, Marine Depecker, Nicolas Vayatis</p><p>Abstract: The present paper examines how the aggregation and feature randomization principles underlying the algorithm R ANDOM F OREST (Breiman, 2001) can be adapted to bipartite ranking. The approach taken here is based on nonparametric scoring and ROC curve optimization in the sense of the AUC criterion. In this problem, aggregation is used to increase the performance of scoring rules produced by ranking trees, as those developed in Cl´ mencon and Vayatis (2009c). The present work e ¸ describes the principles for building median scoring rules based on concepts from rank aggregation. Consistency results are derived for these aggregated scoring rules and an algorithm called R ANK ING F OREST is presented. Furthermore, various strategies for feature randomization are explored through a series of numerical experiments on artiﬁcial data sets. Keywords: bipartite ranking, nonparametric scoring, classiﬁcation data, ROC optimization, AUC criterion, tree-based ranking rules, bootstrap, bagging, rank aggregation, median ranking, feature randomization</p><p>6 0.18384725 <a title="89-lsi-6" href="./jmlr-2013-The_CAM_Software_for_Nonnegative_Blind_Source_Separation_in_R-Java.html">113 jmlr-2013-The CAM Software for Nonnegative Blind Source Separation in R-Java</a></p>
<p>7 0.15479365 <a title="89-lsi-7" href="./jmlr-2013-AC%2B%2BTemplate-Based_Reinforcement_Learning_Library%3A_Fitting_the_Code_to_the_Mathematics.html">1 jmlr-2013-AC++Template-Based Reinforcement Learning Library: Fitting the Code to the Mathematics</a></p>
<p>8 0.14905542 <a title="89-lsi-8" href="./jmlr-2013-Using_Symmetry_and_Evolutionary_Search_to_Minimize_Sorting_Networks.html">118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</a></p>
<p>9 0.13469294 <a title="89-lsi-9" href="./jmlr-2013-Approximating_the_Permanent_with_Fractional_Belief_Propagation.html">13 jmlr-2013-Approximating the Permanent with Fractional Belief Propagation</a></p>
<p>10 0.12257242 <a title="89-lsi-10" href="./jmlr-2013-Orange%3A_Data_Mining_Toolbox_in_Python.html">83 jmlr-2013-Orange: Data Mining Toolbox in Python</a></p>
<p>11 0.11963557 <a title="89-lsi-11" href="./jmlr-2013-Counterfactual_Reasoning_and_Learning_Systems%3A_The_Example_of_Computational_Advertising.html">30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</a></p>
<p>12 0.11035998 <a title="89-lsi-12" href="./jmlr-2013-Efficient_Program_Synthesis_Using_Constraint_Satisfaction_in_Inductive_Logic_Programming.html">40 jmlr-2013-Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming</a></p>
<p>13 0.10853873 <a title="89-lsi-13" href="./jmlr-2013-Comment_on_%22Robustness_and_Regularization_of_Support_Vector_Machines%22_by_H._Xu_et_al._%28Journal_of_Machine_Learning_Research%2C_vol._10%2C_pp._1485-1510%2C_2009%29.html">24 jmlr-2013-Comment on "Robustness and Regularization of Support Vector Machines" by H. Xu et al. (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009)</a></p>
<p>14 0.10495848 <a title="89-lsi-14" href="./jmlr-2013-Tapkee%3A_An_Efficient_Dimension_Reduction_Library.html">112 jmlr-2013-Tapkee: An Efficient Dimension Reduction Library</a></p>
<p>15 0.097056836 <a title="89-lsi-15" href="./jmlr-2013-Optimal_Discovery_with_Probabilistic_Expert_Advice%3A_Finite_Time_Analysis_and_Macroscopic_Optimality.html">81 jmlr-2013-Optimal Discovery with Probabilistic Expert Advice: Finite Time Analysis and Macroscopic Optimality</a></p>
<p>16 0.088246919 <a title="89-lsi-16" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>17 0.084998608 <a title="89-lsi-17" href="./jmlr-2013-CODA%3A_High_Dimensional_Copula_Discriminant_Analysis.html">20 jmlr-2013-CODA: High Dimensional Copula Discriminant Analysis</a></p>
<p>18 0.084196843 <a title="89-lsi-18" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>19 0.083874576 <a title="89-lsi-19" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<p>20 0.082905874 <a title="89-lsi-20" href="./jmlr-2013-Divvy%3A_Fast_and_Intuitive_Exploratory_Data_Analysis.html">37 jmlr-2013-Divvy: Fast and Intuitive Exploratory Data Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(5, 0.048), (10, 0.018), (20, 0.783), (23, 0.011), (70, 0.014), (75, 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95137268 <a title="89-lda-1" href="./jmlr-2013-QuantMiner_for_Mining_Quantitative_Association_Rules.html">89 jmlr-2013-QuantMiner for Mining Quantitative Association Rules</a></p>
<p>Author: Ansaf Salleb-Aouissi, Christel Vrain, Cyril Nortet, Xiangrong Kong, Vivek Rathod, Daniel Cassard</p><p>Abstract: In this paper, we propose Q UANT M INER, a mining quantitative association rules system. This system is based on a genetic algorithm that dynamically discovers “good” intervals in association rules by optimizing both the support and the conﬁdence. The experiments on real and artiﬁcial databases have shown the usefulness of Q UANT M INER as an interactive, exploratory data mining tool. Keywords: association rules, numerical and categorical attributes, unsupervised discretization, genetic algorithm, simulated annealing</p><p>2 0.73985153 <a title="89-lda-2" href="./jmlr-2013-Language-Motivated_Approaches_to_Action_Recognition.html">58 jmlr-2013-Language-Motivated Approaches to Action Recognition</a></p>
<p>Author: Manavender R. Malgireddy, Ifeoma Nwogu, Venu Govindaraju</p><p>Abstract: We present language-motivated approaches to detecting, localizing and classifying activities and gestures in videos. In order to obtain statistical insight into the underlying patterns of motions in activities, we develop a dynamic, hierarchical Bayesian model which connects low-level visual features in videos with poses, motion patterns and classes of activities. This process is somewhat analogous to the method of detecting topics or categories from documents based on the word content of the documents, except that our documents are dynamic. The proposed generative model harnesses both the temporal ordering power of dynamic Bayesian networks such as hidden Markov models (HMMs) and the automatic clustering power of hierarchical Bayesian models such as the latent Dirichlet allocation (LDA) model. We also introduce a probabilistic framework for detecting and localizing pre-speciﬁed activities (or gestures) in a video sequence, analogous to the use of ﬁller models for keyword detection in speech processing. We demonstrate the robustness of our classiﬁcation model and our spotting framework by recognizing activities in unconstrained real-life video sequences and by spotting gestures via a one-shot-learning approach. Keywords: dynamic hierarchical Bayesian networks, topic models, activity recognition, gesture spotting, generative models</p><p>3 0.51975846 <a title="89-lda-3" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<p>Author: Sohail Bahmani, Bhiksha Raj, Petros T. Boufounos</p><p>Abstract: Sparsity-constrained optimization has wide applicability in machine learning, statistics, and signal processing problems such as feature selection and Compressed Sensing. A vast body of work has studied the sparsity-constrained optimization from theoretical, algorithmic, and application aspects in the context of sparse estimation in linear models where the ﬁdelity of the estimate is measured by the squared error. In contrast, relatively less effort has been made in the study of sparsityconstrained optimization in cases where nonlinear models are involved or the cost function is not quadratic. In this paper we propose a greedy algorithm, Gradient Support Pursuit (GraSP), to approximate sparse minima of cost functions of arbitrary form. Should a cost function have a Stable Restricted Hessian (SRH) or a Stable Restricted Linearization (SRL), both of which are introduced in this paper, our algorithm is guaranteed to produce a sparse vector within a bounded distance from the true sparse optimum. Our approach generalizes known results for quadratic cost functions that arise in sparse linear regression and Compressed Sensing. We also evaluate the performance of GraSP through numerical simulations on synthetic and real data, where the algorithm is employed for sparse logistic regression with and without ℓ2 -regularization. Keywords: sparsity, optimization, compressed sensing, greedy algorithm</p><p>4 0.22412978 <a title="89-lda-4" href="./jmlr-2013-Learning_Theory_Analysis_for_Association_Rules_and_Sequential_Event_Prediction.html">61 jmlr-2013-Learning Theory Analysis for Association Rules and Sequential Event Prediction</a></p>
<p>Author: Cynthia Rudin, Benjamin Letham, David Madigan</p><p>Abstract: We present a theoretical analysis for prediction algorithms based on association rules. As part of this analysis, we introduce a problem for which rules are particularly natural, called “sequential event prediction.” In sequential event prediction, events in a sequence are revealed one by one, and the goal is to determine which event will next be revealed. The training set is a collection of past sequences of events. An example application is to predict which item will next be placed into a customer’s online shopping cart, given his/her past purchases. In the context of this problem, algorithms based on association rules have distinct advantages over classical statistical and machine learning methods: they look at correlations based on subsets of co-occurring past events (items a and b imply item c), they can be applied to the sequential event prediction problem in a natural way, they can potentially handle the “cold start” problem where the training set is small, and they yield interpretable predictions. In this work, we present two algorithms that incorporate association rules. These algorithms can be used both for sequential event prediction and for supervised classiﬁcation, and they are simple enough that they can possibly be understood by users, customers, patients, managers, etc. We provide generalization guarantees on these algorithms based on algorithmic stability analysis from statistical learning theory. We include a discussion of the strict minimum support threshold often used in association rule mining, and introduce an “adjusted conﬁdence” measure that provides a weaker minimum support condition that has advantages over the strict minimum support. The paper brings together ideas from statistical learning theory, association rule mining and Bayesian analysis. Keywords: statistical learning theory, algorithmic stability, association rules, sequence prediction, associative classiﬁcation c 2013 Cynthia Rudin, Benjamin Letham and David Madigan. RUDIN , L E</p><p>5 0.21841209 <a title="89-lda-5" href="./jmlr-2013-One-shot_Learning_Gesture_Recognition_from_RGB-D_Data_Using_Bag_of_Features.html">80 jmlr-2013-One-shot Learning Gesture Recognition from RGB-D Data Using Bag of Features</a></p>
<p>Author: Jun Wan, Qiuqi Ruan, Wei Li, Shuang Deng</p><p>Abstract: For one-shot learning gesture recognition, two important challenges are: how to extract distinctive features and how to learn a discriminative model from only one training sample per gesture class. For feature extraction, a new spatio-temporal feature representation called 3D enhanced motion scale-invariant feature transform (3D EMoSIFT) is proposed, which fuses RGB-D data. Compared with other features, the new feature set is invariant to scale and rotation, and has more compact and richer visual representations. For learning a discriminative model, all features extracted from training samples are clustered with the k-means algorithm to learn a visual codebook. Then, unlike the traditional bag of feature (BoF) models using vector quantization (VQ) to map each feature into a certain visual codeword, a sparse coding method named simulation orthogonal matching pursuit (SOMP) is applied and thus each feature can be represented by some linear combination of a small number of codewords. Compared with VQ, SOMP leads to a much lower reconstruction error and achieves better performance. The proposed approach has been evaluated on ChaLearn gesture database and the result has been ranked amongst the top best performing techniques on ChaLearn gesture challenge (round 2). Keywords: gesture recognition, bag of features (BoF) model, one-shot learning, 3D enhanced motion scale invariant feature transform (3D EMoSIFT), Simulation Orthogonal Matching Pursuit (SOMP)</p><p>6 0.2077805 <a title="89-lda-6" href="./jmlr-2013-Fast_Generalized_Subset_Scan_for_Anomalous_Pattern_Detection.html">42 jmlr-2013-Fast Generalized Subset Scan for Anomalous Pattern Detection</a></p>
<p>7 0.18499027 <a title="89-lda-7" href="./jmlr-2013-Ranking_Forests.html">95 jmlr-2013-Ranking Forests</a></p>
<p>8 0.18455128 <a title="89-lda-8" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>9 0.1662394 <a title="89-lda-9" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>10 0.16435856 <a title="89-lda-10" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>11 0.15782507 <a title="89-lda-11" href="./jmlr-2013-Optimally_Fuzzy_Temporal_Memory.html">82 jmlr-2013-Optimally Fuzzy Temporal Memory</a></p>
<p>12 0.14956991 <a title="89-lda-12" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<p>13 0.14645861 <a title="89-lda-13" href="./jmlr-2013-Convex_and_Scalable_Weakly_Labeled_SVMs.html">29 jmlr-2013-Convex and Scalable Weakly Labeled SVMs</a></p>
<p>14 0.14285004 <a title="89-lda-14" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>15 0.14144194 <a title="89-lda-15" href="./jmlr-2013-Differential_Privacy_for_Functions_and_Functional_Data.html">32 jmlr-2013-Differential Privacy for Functions and Functional Data</a></p>
<p>16 0.14140697 <a title="89-lda-16" href="./jmlr-2013-GURLS%3A_A_Least_Squares_Library_for_Supervised_Learning.html">46 jmlr-2013-GURLS: A Least Squares Library for Supervised Learning</a></p>
<p>17 0.14117102 <a title="89-lda-17" href="./jmlr-2013-BudgetedSVM%3A_A_Toolbox_for_Scalable_SVM_Approximations.html">19 jmlr-2013-BudgetedSVM: A Toolbox for Scalable SVM Approximations</a></p>
<p>18 0.14076252 <a title="89-lda-18" href="./jmlr-2013-Keep_It_Simple_And_Sparse%3A_Real-Time_Action_Recognition.html">56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</a></p>
<p>19 0.13958958 <a title="89-lda-19" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>20 0.1390039 <a title="89-lda-20" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
