<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-48" href="#">jmlr2013-48</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</h1>
<br/><p>Source: <a title="jmlr-2013-48-pdf" href="http://jmlr.org/papers/volume14/hernandez-lobato13a/hernandez-lobato13a.pdf">pdf</a></p><p>Author: Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Pierre Dupont</p><p>Abstract: We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efﬁciently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about speciﬁc groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy. Keywords: group feature selection, generalized spike-and-slab priors, expectation propagation, sparse linear model, approximate inference, sequential experimental design, signal reconstruction</p><p>Reference: <a title="jmlr-2013-48-reference" href="../jmlr2013_reference/jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. [sent-12, score-0.586]
</p><p>2 An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. [sent-14, score-1.058]
</p><p>3 Keywords: group feature selection, generalized spike-and-slab priors, expectation propagation, sparse linear model, approximate inference, sequential experimental design, signal reconstruction  1. [sent-17, score-0.701]
</p><p>4 o The process of inducing the model coefﬁcients under the sparsity assumption can be facilitated when prior information is available about groups of features that are expected to be jointly relevant or jointly irrelevant for prediction (Huang and Zhang, 2010). [sent-34, score-0.519]
</p><p>5 As with the individual sparsity assumption, sparsity at the group level can be introduced in the estimation process of the model coefﬁcients by considering speciﬁc regularization norms or by assuming sparse enforcing priors at the group level (Yuan and Lin, 2006; Ji et al. [sent-52, score-0.896]
</p><p>6 Under the assumption that the grouping information is given beforehand, the proposed prior introduces a set of binary latent variables, one for each different group of features. [sent-56, score-0.601]
</p><p>7 If the latent variable of a particular group is equal to zero, the model coefﬁcients corresponding to that group are set to zero and the features of the group are not used for prediction of the targets. [sent-57, score-1.159]
</p><p>8 The proposed generalized version of the spike-and-slab prior has several practical advantages over other methods for group feature selection. [sent-62, score-0.563]
</p><p>9 The proposed prior also provides a direct estimate of the relative importance of each group for prediction, hence identifying easily the most relevant groups. [sent-69, score-0.516]
</p><p>10 Additionally, a detailed analysis of the sparsity properties of the generalized prior shows that it is adequate for group feature selection. [sent-77, score-0.618]
</p><p>11 Namely, under this prior it is possible to achieve high levels of group sparsity and, at the same time, to avoid shrinking the model coefﬁcients that are truly different from zero. [sent-79, score-0.583]
</p><p>12 The performance of a model based on the generalized spike-and-slab prior and the EP algorithm is evaluated on a collection of benchmark regression problems and compared with other methods from the literature that can also be used for group feature selection. [sent-83, score-0.622]
</p><p>13 , 2009), a modiﬁed version of the horseshoe prior for group feature selection (Carvalho et al. [sent-90, score-0.678]
</p><p>14 Section 2 describes the generalized spikeand-slab prior considered for group feature selection. [sent-101, score-0.563]
</p><p>15 Section 6 introduces a detailed analysis of the group sparsity properties of the generalized spike-and-slab prior and the other methods that can be used for this purpose. [sent-105, score-0.589]
</p><p>16 Finally, the prior for z is a multivariate Bernoulli distribution: G  z  g P (z) = Bern(z|p0 ) = ∏ p0,g (1 − p0,g )(1−zg ) ,  (4)  g=1  where p0,g is the prior probability that the coefﬁcients within the g-th group are different from zero and p0 = (p0,1 , . [sent-136, score-0.649]
</p><p>17 This posterior distribution and the likelihood (2) can be combined to compute a predictive distribution for the target ynew ∈ R associated to a new observation xnew :  P (ynew |xnew , y, X) = ∑ z  P (ynew |w, xnew )P (w, z|y, X) dw . [sent-149, score-0.52]
</p><p>18 1895  (7)  ´ ´ H ERN ANDEZ -L OBATO , H ERN ANDEZ -L OBATO AND D UPONT  Furthermore, one can marginalize (7) over zg′ , with g′ = g, for a speciﬁc latent variable zg to compute P (zg |y, X), that is, the associated posterior probability of using the g-th group of features for prediction. [sent-151, score-0.653]
</p><p>19 1−x  In (22) p0,g is the prior probability of using the g-th group of features for prediction. [sent-283, score-0.537]
</p><p>20 Finally, there exists an equivalent formulation of the group LASSO where the optimization problem is un-constrained, but the loss function is penalized by the sum of the ℓ2 -norms of the group components of w (Yuan and Lin, 2006). [sent-458, score-0.696]
</p><p>21 Consider now for each group of coefﬁcients wg , with g = 1, . [sent-486, score-0.609]
</p><p>22 3 The Group Horseshoe The group horseshoe introduced in this section is a natural extension of the robust horseshoe prior initially proposed to address sparse supervised learning problems (Carvalho et al. [sent-508, score-0.835]
</p><p>23 The prior described in (34) can be easily generalized to address sparsity at the group level. [sent-522, score-0.589]
</p><p>24 Speciﬁcally, we consider for each group of coefﬁcients wg , with g = 1, . [sent-524, score-0.609]
</p><p>25 , G, a multivariate and spherical dg dimensional prior, which can be expressed as a hierarchical normal-half-Cauchy model, as in (34):  P (wg |λg , τ) =  N (wg |0, λ2 τ2 I)C + (λg |0, 1)dλg , g  (35)  where λg is a latent parameter speciﬁc to each group and τ is a shrinkage parameter. [sent-527, score-0.523]
</p><p>26 That is, Cauchy-like tails to allow for large values of each component of wg and an inﬁnitely tall spike at the origin which favors values where all the components of wg are close to zero. [sent-529, score-0.686]
</p><p>27 Following the ARD principle sparsity at the group level can be easily obtained by considering a different hyper-parameter for each group of coefﬁcients. [sent-563, score-0.725]
</p><p>28 (2009) consider the following Gaussian prior distribution for w: G  G  g=1  g=1  P (w) = ∏ P (wg ) = ∏ N (wg |0, α−1 I) , g where αg is the inverse of the prior variances for each component of wg , that is, the vector of model coefﬁcients within the g-th group. [sent-565, score-0.624]
</p><p>29 The group ARD formulation (more precisely the type-II maximum likelihood principle followed by group ARD) can be seen as a Bayesian approach where the posterior distribution of each hyperparameter αg , with j = 1, . [sent-579, score-0.799]
</p><p>30 This improper prior favors solutions with all the components of wg set equal to zero since it has an inﬁnitely tall spike at the origin. [sent-586, score-0.532]
</p><p>31 The posterior distribution of w under the group ARD model is Gaussian since both the likelihood and the prior are Gaussian. [sent-589, score-0.657]
</p><p>32 Finally, note that the group ARD lacks a hyper-parameter to specify the desired level of sparsity at the group level. [sent-591, score-0.725]
</p><p>33 For this, we only have to interpret the prior described in (3) for each group of model coefﬁcients wg as a mixture of two multivariate Gaussians. [sent-596, score-0.802]
</p><p>34 3 for the Bayesian group LASSO and the group horseshoe. [sent-600, score-0.67]
</p><p>35 This prior cannot be used to tackle the group feature selection problem considered here. [sent-628, score-0.521]
</p><p>36 The equivalence holds provided that there is a group per feature and that the g-th group contains the L coefﬁcients associated with the g-th feature, for g = 1, . [sent-640, score-0.699]
</p><p>37 The work of Yen and Yen (2011) describes an alternative generalization of the spike-and-slab prior which considers both sparsity at the group and the feature level. [sent-662, score-0.576]
</p><p>38 A ﬁrst set is used to describe whether or not each group of variables is used for prediction, and a second set is used to describe whether or not each feature within a group is used for prediction. [sent-664, score-0.699]
</p><p>39 Thus, the prior considered in this work can be seen as a particular case of the prior considered by these authors where there is no sparsity at the feature level, but only at the group level. [sent-665, score-0.733]
</p><p>40 Analysis of Group Sparsity In this section we study the properties of the generalized spike-and-slab prior to favor solutions that are sparse at the group level. [sent-673, score-0.563]
</p><p>41 (2009) about the sparsity properties of the standard horseshoe prior and shows interesting insights about the regularization process enforced by each prior distribution and the potential beneﬁts and drawbacks for group feature selection. [sent-676, score-0.89]
</p><p>42 We do not include the prior for the group LASSO since it is identical to the one in the Bayesian group LASSO, as described in Section 5. [sent-680, score-0.827]
</p><p>43 Figure 1 shows the different priors displayed in Table 1 for some values of their hyper-parameters and for a group of size two, that is, dg = 2. [sent-682, score-0.558]
</p><p>44 The only exception is the prior corresponding to the group LASSO, which has a sharp peak at the origin instead. [sent-685, score-0.529]
</p><p>45 By contrast, in the group horseshoe, the group ARD and the Bayesian group LASSO the probability of observing wg at the origin is zero. [sent-688, score-1.316]
</p><p>46 On the other hand, an appealing property of the group ARD and the group horseshoe is that they allow for values of wg located far from the origin since they have heavy tails. [sent-690, score-1.16]
</p><p>47 The group ARD, the spike-and-slab and the group horseshoe are hence expected to be effective for inducing sparsity at the group level. [sent-693, score-1.217]
</p><p>48 The prior corresponding to the Bayesian group LASSO has neither heavy tails nor an inﬁnitely tall spike at the origin. [sent-695, score-0.589]
</p><p>49 g The prior distribution for λ2 determines the resulting family of prior distributions for wg . [sent-702, score-0.588]
</p><p>50 In particular, we assume that there is a single group of dg model coefﬁcients, that is, w = wg . [sent-709, score-0.764]
</p><p>51 Results are displayed for wg = (w1 , w2 )T , that is, a group of size two, and for some particular values of the hyper-parameters of each different prior distribution. [sent-743, score-0.819]
</p><p>52 All priors except the prior for the group LASSO and the Bayesian group LASSO have an inﬁnitely tall spike at the origin. [sent-745, score-0.953]
</p><p>53 g  by the prior distribution for λ2 which can be any of the ones displayed in Table 2, depending on g the actual prior for wg . [sent-754, score-0.641]
</p><p>54 In an ideal situation, P (κ), that is, the prior distribution for κ, should favor the bi-separation of the model coefﬁcients that is characteristic of sparse models at the group level. [sent-755, score-0.557]
</p><p>55 Figure 2 displays for each different prior distribution for wg , the corresponding prior distribution for the shrinkage coefﬁcient κ, P (κ). [sent-759, score-0.623]
</p><p>56 The exception is the prior for κ corresponding to group ARD, which does not have any hyper-parameter to specify the desired level of group sparsity. [sent-768, score-0.827]
</p><p>57 This is an unexpected result which questions the capacity of this prior to provide solutions that are sparse at the group level in a selective manner. [sent-772, score-0.521]
</p><p>58 The other priors, that is, the spike-and-slab, the group horseshoe and the group ARD, do not suffer from the limitations described for the Bayesian group LASSO. [sent-775, score-1.162]
</p><p>59 On the other hand, both the group horseshoe and the group ARD priors are characterized by heavy tails. [sent-779, score-0.9]
</p><p>60 The plots are displayed for a single group of size two, and for some particular values of the hyper-parameters of each prior distribution for wg that give the same inter-quantile range (IQR) for each individual component of this vector. [sent-824, score-0.819]
</p><p>61 Therefore, these three prior distributions are expected to selectively shrink the posterior mean, which is the ideal situation for regression problems which are sparse at the group level. [sent-831, score-0.673]
</p><p>62 Speciﬁcally, we study the behavior of the posterior mean, E[wg ], under the different priors for wg when the targets y are similar to or very different from the prior mean, that is, a vector with all the components equal to zero. [sent-835, score-0.691]
</p><p>63 Figure 3 shows a comparison between these two norms for each different prior distribution for wg and for different values of the prior hyper-parameters. [sent-841, score-0.588]
</p><p>64 We include plots both for the group LASSO and the Bayesian group LASSO to illustrate the differences between the ˆ posterior mean E[wg ] and the MAP estimate wg . [sent-843, score-1.073]
</p><p>65 When ||y||2 > dg γ/2, ˆ ˆ the ℓ2 -norm of wg must satisfy ||wg ||2 = ||y||2 − dg γ/2. [sent-850, score-0.512]
</p><p>66 The consequence is that for high levels of group sparsity, as speciﬁed by γ, the group LASSO regularizes the coefﬁcients that are different from zero and introduces a signiﬁcant bias in their estimation. [sent-852, score-0.67]
</p><p>67 These two methods, that is, the group LASSO and the Bayesian group LASSO, are hence unable to shrink the model coefﬁcients in a selective manner and are expected to lead to an impaired prediction performance in problems that are actually sparse at the group level. [sent-856, score-1.109]
</p><p>68 This is a consequence of the heavy tails of the prior for wg which barely regularizes the model coefﬁcients when these are signiﬁcantly different from zero and strictly required for prediction. [sent-862, score-0.511]
</p><p>69 From this, we conclude that these two prior distributions, the group ARD and the group horseshoe, are expected to be useful to provide the bi-separation of the model coefﬁcients that is characteristic of sparse models at the group level. [sent-869, score-1.227]
</p><p>70 We report results for both the Bayesian group LASSO and for the group LASSO, that is, the MAP estimate in the Bayesian group LASSO. [sent-886, score-1.005]
</p><p>71 More precisely, we generate 100 random sparse signals to be reconstructed from noisy measurements where each signal has d = 512 random components that are codiﬁed using a particular group sparsity pattern. [sent-947, score-0.592]
</p><p>72 The group horseshoe prior does not have deﬁned variances. [sent-981, score-0.649]
</p><p>73 Thus, we ﬁx τ in G-HS so that the marginals under the group horseshoe have the same distance between the percentiles 1% and 99% as the marginals under the generalized spike-and-slab prior. [sent-982, score-0.558]
</p><p>74 In the group horseshoe this behavior can be explained because under this prior the probability of observing a group at the origin is zero. [sent-1042, score-1.021]
</p><p>75 This questions the utility of the Bayesian group LASSO for group feature selection. [sent-1050, score-0.699]
</p><p>76 In G-HS we select τ so that the marginals of the group horseshoe prior have the same distance between the percentiles 1% and 99% as the marginals under the generalized spike-and-slab prior in GSS-EP. [sent-1243, score-0.872]
</p><p>77 In G-HS we ﬁx τ so that the marginals under the group horseshoe prior have the same distance between the percentiles 1% and 99% as the marginals under the generalized spike-and-slab prior. [sent-1410, score-0.715]
</p><p>78 Finally, the image reconstructed by BG-LASSO is not sparse at all which questions again the utility of the Bayesian group LASSO for group feature selection. [sent-1589, score-0.754]
</p><p>79 This approximate distribution is described in detail in (7) and each parameter σ(pg ) estimates the posterior probability of using the g-th group of pixels for prediction. [sent-1595, score-0.524]
</p><p>80 We do not evaluate here the other methods described for group feature selection because they do not allow to introduce this type of prior information during the induction process. [sent-1677, score-0.551]
</p><p>81 More precisely, it is based on a linear model that considers a generalized spike-and-slab prior for group feature selection. [sent-1691, score-0.599]
</p><p>82 Speciﬁcally, under this prior a set of binary latent variables is introduced, one for each different group of features, and each latent variable indicates whether or not the corresponding group is used for prediction. [sent-1692, score-0.895]
</p><p>83 The generalized spike-and-slab prior is also shown to be very useful to introduce prior knowledge about speciﬁc groups of features that are a priori expected to be more relevant or more irrelevant for prediction than the other groups. [sent-1700, score-0.559]
</p><p>84 Furthermore, we consider different cases of prior information: (i) no prior information is used about the relevancy of each group of features (GSS-EP), (ii) the prior information is actually employed (GSSEP PRIOR), and (iii) the prior information is chosen randomly (GSS-EP PRIOR RANDOM). [sent-1726, score-1.008]
</p><p>85 Thus, unlike in other methods for group feature selection, in the generalized spike-and-slab prior it is very easy to specify the expected level of group sparsity and the expected deviation from zero of the relevant coefﬁcients. [sent-1728, score-0.977]
</p><p>86 This analysis shows that the generalized spike-andslab prior is very effective for group feature selection. [sent-1733, score-0.563]
</p><p>87 In summary, under the generalized spike-and-slab prior it is possible to provide solutions that are sparse at the group level in a selective manner. [sent-1741, score-0.563]
</p><p>88 More precisely, under this prior we can model very high levels of sparsity at the group level (small values of p0 ), while at the same time allowing for model coefﬁcients that are signiﬁcantly different from zero (large values of v0 ). [sent-1742, score-0.619]
</p><p>89 This is not possible, for example, in the case of the group LASSO or the Bayesian group LASSO. [sent-1743, score-0.67]
</p><p>90 The group horseshoe also enjoys this selective shrinkage property. [sent-1744, score-0.527]
</p><p>91 An extensive collection of experiments which considers real and synthetic data sets compares the performance of a model based on the generalized spike-and-slab prior and the EP algorithm with the other methods for group feature selection. [sent-1746, score-0.599]
</p><p>92 Our results indicate that when accurate prior information about relevant or irrelevant groups of features for prediction is available, group feature selection signiﬁcantly improves the results of single feature selection. [sent-1749, score-0.753]
</p><p>93 Additionally, the computational cost of EP is signiﬁcantly better than the computational cost of the methods based on Gibbs sampling, including the Bayesian group LASSO and the group horseshoe. [sent-1753, score-0.67]
</p><p>94 The computational cost of the EP algorithm is also similar or better than the computational cost of the methods based on the group ARD principle or the group LASSO. [sent-1754, score-0.67]
</p><p>95 In this section we show how to implement an efﬁcient Gibbs sampler for the regression models based on the generalized spike-and-slab prior, the Bayesian group LASSO and the group horseshoe prior. [sent-1773, score-0.892]
</p><p>96 As described in Table 2, all these models are very similar and only differ in the prior distribution assumed for λ2 , that is, the prior variance for the coefﬁcients wg corresponding to the g-th group g of features. [sent-1774, score-0.923]
</p><p>97 Assume that dg is the size of the g-th group of features and that Xg is a n × dg matrix which contains in each row, for each instance, only the features of the g-th group. [sent-1806, score-0.663]
</p><p>98 g Finally, in the group horseshoe and the Bayesian group LASSO, λ is initialized to a vector whose components are all equal to 1 as described by Scott (2010). [sent-1843, score-0.853]
</p><p>99 In this section we show how to implement the group ARD method for group feature selection. [sent-1854, score-0.699]
</p><p>100 From (38) and (39), it follows that 1 G dg  log P (λ2 |X, y, λ−g ) = g  ∑ L j (λ2 ) + constant , g  (43)  j=1  where dg is the number of features in the g-th group and L j (λ2 ) is deﬁned as in (40) for τ2 = 1. [sent-1876, score-0.618]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ep', 0.375), ('group', 0.335), ('wg', 0.274), ('andez', 0.257), ('obato', 0.257), ('ern', 0.22), ('prior', 0.157), ('horseshoe', 0.157), ('ard', 0.156), ('xnew', 0.143), ('reconstruction', 0.14), ('gss', 0.133), ('posterior', 0.129), ('roup', 0.129), ('upont', 0.129), ('dg', 0.119), ('lasso', 0.118), ('hern', 0.11), ('pike', 0.11), ('zg', 0.11), ('eneralized', 0.11), ('bayesian', 0.106), ('ynew', 0.105), ('cients', 0.1), ('groups', 0.095), ('riors', 0.091), ('measurements', 0.089), ('pg', 0.085), ('coef', 0.081), ('eature', 0.081), ('election', 0.076), ('grouping', 0.075), ('lab', 0.064), ('sentiment', 0.062), ('coefficients', 0.057), ('sparsity', 0.055), ('targets', 0.054), ('displayed', 0.053), ('priors', 0.051), ('slab', 0.051), ('spike', 0.051), ('digit', 0.049), ('ss', 0.047), ('gibbs', 0.047), ('kitchen', 0.045), ('evidence', 0.045), ('features', 0.045), ('sequential', 0.044), ('design', 0.043), ('yen', 0.043), ('generalized', 0.042), ('iqr', 0.041), ('prediction', 0.039), ('origin', 0.037), ('reconstructions', 0.037), ('model', 0.036), ('shrinkage', 0.035), ('latent', 0.034), ('jointly', 0.034), ('pixels', 0.034), ('bern', 0.033), ('instances', 0.032), ('pixel', 0.032), ('signal', 0.032), ('mnist', 0.031), ('training', 0.031), ('books', 0.03), ('induction', 0.03), ('sparse', 0.029), ('images', 0.029), ('feature', 0.029), ('faul', 0.029), ('mcculloch', 0.029), ('vxnew', 0.029), ('carvalho', 0.029), ('raman', 0.029), ('tipping', 0.028), ('minka', 0.028), ('ji', 0.027), ('digits', 0.027), ('inference', 0.026), ('seeger', 0.026), ('approximate', 0.026), ('reconstructed', 0.026), ('components', 0.026), ('iteratively', 0.025), ('tall', 0.024), ('experimental', 0.024), ('fi', 0.024), ('reconstruct', 0.024), ('bishop', 0.024), ('blitzer', 0.024), ('dvds', 0.024), ('percentiles', 0.024), ('pnew', 0.024), ('vnew', 0.024), ('relevant', 0.024), ('regression', 0.023), ('barely', 0.022), ('heavy', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999934 <a title="48-tfidf-1" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>Author: Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Pierre Dupont</p><p>Abstract: We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efﬁciently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about speciﬁc groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy. Keywords: group feature selection, generalized spike-and-slab priors, expectation propagation, sparse linear model, approximate inference, sequential experimental design, signal reconstruction</p><p>2 0.27968559 <a title="48-tfidf-2" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>3 0.15930177 <a title="48-tfidf-3" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian ﬁeld, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model’s partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. Through the expansion, we see that EP is correct to ﬁrst order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. Keywords: expectation consistent inference, expectation propagation, perturbation correction, Wick expansions, Ising model, Gaussian process</p><p>4 0.10006315 <a title="48-tfidf-4" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>Author: Pinghua Gong, Jieping Ye, Changshui Zhang</p><p>Abstract: Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an ℓ0 -type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel non-convex regularizer. To solve the non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm; we also provide intuitive interpretations, detailed convergence and reproducibility analysis for the proposed algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms. Keywords: multi-task learning, multi-stage, non-convex, sparse learning</p><p>5 0.088896491 <a title="48-tfidf-5" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>6 0.088666216 <a title="48-tfidf-6" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>7 0.087955274 <a title="48-tfidf-7" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>8 0.080803059 <a title="48-tfidf-8" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>9 0.079078652 <a title="48-tfidf-9" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>10 0.070477217 <a title="48-tfidf-10" href="./jmlr-2013-Consistent_Selection_of_Tuning_Parameters_via_Variable_Selection_Stability.html">27 jmlr-2013-Consistent Selection of Tuning Parameters via Variable Selection Stability</a></p>
<p>11 0.065796107 <a title="48-tfidf-11" href="./jmlr-2013-Variational_Inference_in_Nonconjugate_Models.html">121 jmlr-2013-Variational Inference in Nonconjugate Models</a></p>
<p>12 0.063219406 <a title="48-tfidf-12" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>13 0.053663332 <a title="48-tfidf-13" href="./jmlr-2013-Bayesian_Nonparametric_Hidden_Semi-Markov_Models.html">16 jmlr-2013-Bayesian Nonparametric Hidden Semi-Markov Models</a></p>
<p>14 0.051059432 <a title="48-tfidf-14" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>15 0.04857482 <a title="48-tfidf-15" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>16 0.046087846 <a title="48-tfidf-16" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>17 0.044315774 <a title="48-tfidf-17" href="./jmlr-2013-Variable_Selection_in_High-Dimension_with_Random_Designs_and_Orthogonal_Matching_Pursuit.html">119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</a></p>
<p>18 0.040787764 <a title="48-tfidf-18" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>19 0.0397329 <a title="48-tfidf-19" href="./jmlr-2013-Fast_Generalized_Subset_Scan_for_Anomalous_Pattern_Detection.html">42 jmlr-2013-Fast Generalized Subset Scan for Anomalous Pattern Detection</a></p>
<p>20 0.037311658 <a title="48-tfidf-20" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.238), (1, -0.276), (2, 0.07), (3, -0.028), (4, 0.208), (5, -0.177), (6, -0.286), (7, -0.165), (8, -0.085), (9, 0.073), (10, 0.004), (11, -0.047), (12, -0.133), (13, 0.035), (14, 0.065), (15, -0.086), (16, -0.003), (17, 0.057), (18, -0.004), (19, -0.065), (20, -0.065), (21, 0.026), (22, -0.015), (23, 0.134), (24, 0.105), (25, 0.073), (26, 0.035), (27, 0.083), (28, 0.019), (29, -0.052), (30, 0.017), (31, 0.001), (32, -0.057), (33, -0.124), (34, 0.028), (35, -0.101), (36, 0.057), (37, 0.049), (38, 0.113), (39, -0.013), (40, 0.083), (41, 0.027), (42, 0.021), (43, 0.027), (44, 0.019), (45, -0.0), (46, 0.052), (47, -0.045), (48, 0.028), (49, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93968356 <a title="48-lsi-1" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>Author: Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Pierre Dupont</p><p>Abstract: We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efﬁciently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about speciﬁc groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy. Keywords: group feature selection, generalized spike-and-slab priors, expectation propagation, sparse linear model, approximate inference, sequential experimental design, signal reconstruction</p><p>2 0.81769621 <a title="48-lsi-2" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>3 0.64236963 <a title="48-lsi-3" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian ﬁeld, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model’s partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. Through the expansion, we see that EP is correct to ﬁrst order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. Keywords: expectation consistent inference, expectation propagation, perturbation correction, Wick expansions, Ising model, Gaussian process</p><p>4 0.49103147 <a title="48-lsi-4" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>Author: Pinghua Gong, Jieping Ye, Changshui Zhang</p><p>Abstract: Multi-task sparse feature learning aims to improve the generalization performance by exploiting the shared features among tasks. It has been successfully applied to many applications including computer vision and biomedical informatics. Most of the existing multi-task sparse feature learning algorithms are formulated as a convex sparse regularization problem, which is usually suboptimal, due to its looseness for approximating an ℓ0 -type regularizer. In this paper, we propose a non-convex formulation for multi-task sparse feature learning based on a novel non-convex regularizer. To solve the non-convex optimization problem, we propose a Multi-Stage Multi-Task Feature Learning (MSMTFL) algorithm; we also provide intuitive interpretations, detailed convergence and reproducibility analysis for the proposed algorithm. Moreover, we present a detailed theoretical analysis showing that MSMTFL achieves a better parameter estimation error bound than the convex formulation. Empirical studies on both synthetic and real-world data sets demonstrate the effectiveness of MSMTFL in comparison with the state of the art multi-task sparse feature learning algorithms. Keywords: multi-task learning, multi-stage, non-convex, sparse learning</p><p>5 0.48533723 <a title="48-lsi-5" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>6 0.45750982 <a title="48-lsi-6" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>7 0.32085195 <a title="48-lsi-7" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>8 0.30856267 <a title="48-lsi-8" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>9 0.29386804 <a title="48-lsi-9" href="./jmlr-2013-Consistent_Selection_of_Tuning_Parameters_via_Variable_Selection_Stability.html">27 jmlr-2013-Consistent Selection of Tuning Parameters via Variable Selection Stability</a></p>
<p>10 0.28061295 <a title="48-lsi-10" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>11 0.27553263 <a title="48-lsi-11" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>12 0.27079242 <a title="48-lsi-12" href="./jmlr-2013-Bayesian_Nonparametric_Hidden_Semi-Markov_Models.html">16 jmlr-2013-Bayesian Nonparametric Hidden Semi-Markov Models</a></p>
<p>13 0.26857978 <a title="48-lsi-13" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>14 0.24410039 <a title="48-lsi-14" href="./jmlr-2013-Fast_MCMC_Sampling_for_Markov_Jump_Processes_and_Extensions.html">43 jmlr-2013-Fast MCMC Sampling for Markov Jump Processes and Extensions</a></p>
<p>15 0.23971912 <a title="48-lsi-15" href="./jmlr-2013-Variable_Selection_in_High-Dimension_with_Random_Designs_and_Orthogonal_Matching_Pursuit.html">119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</a></p>
<p>16 0.23585545 <a title="48-lsi-16" href="./jmlr-2013-Global_Analytic_Solution_of_Fully-observed_Variational_Bayesian_Matrix_Factorization.html">49 jmlr-2013-Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization</a></p>
<p>17 0.22393011 <a title="48-lsi-17" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>18 0.22107826 <a title="48-lsi-18" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>19 0.21121864 <a title="48-lsi-19" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>20 0.20739785 <a title="48-lsi-20" href="./jmlr-2013-Fast_Generalized_Subset_Scan_for_Anomalous_Pattern_Detection.html">42 jmlr-2013-Fast Generalized Subset Scan for Anomalous Pattern Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.05), (5, 0.128), (6, 0.047), (10, 0.071), (20, 0.024), (21, 0.315), (23, 0.052), (68, 0.047), (70, 0.03), (75, 0.059), (85, 0.014), (87, 0.023), (93, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71510625 <a title="48-lda-1" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>Author: Daniel Hernández-Lobato, José Miguel Hernández-Lobato, Pierre Dupont</p><p>Abstract: We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efﬁciently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about speciﬁc groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy. Keywords: group feature selection, generalized spike-and-slab priors, expectation propagation, sparse linear model, approximate inference, sequential experimental design, signal reconstruction</p><p>2 0.48143604 <a title="48-lda-2" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>Author: Yuchen Zhang, John C. Duchi, Martin J. Wainwright</p><p>Abstract: We analyze two communication-efﬁcient algorithms for distributed optimization in statistical settings involving large-scale data sets. The ﬁrst algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves √ mean-squared error (MSE) that decays as O (N −1 + (N/m)−2 ). Whenever m ≤ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O (N −1 + (N/m)−3 ), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O (N −1 + (N/m)−3/2 ), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efﬁciently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 108 samples and d ≈ 740,000 covariates. Keywords: distributed learning, stochastic optimization, averaging, subsampling</p><p>3 0.4789466 <a title="48-lda-3" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>Author: Alberto N. Escalante-B., Laurenz Wiskott</p><p>Abstract: Supervised learning from high-dimensional data, for example, multimedia data, is a challenging task. We propose an extension of slow feature analysis (SFA) for supervised dimensionality reduction called graph-based SFA (GSFA). The algorithm extracts a label-predictive low-dimensional set of features that can be post-processed by typical supervised algorithms to generate the ﬁnal label or class estimation. GSFA is trained with a so-called training graph, in which the vertices are the samples and the edges represent similarities of the corresponding labels. A new weighted SFA optimization problem is introduced, generalizing the notion of slowness from sequences of samples to such training graphs. We show that GSFA computes an optimal solution to this problem in the considered function space and propose several types of training graphs. For classiﬁcation, the most straightforward graph yields features equivalent to those of (nonlinear) Fisher discriminant analysis. Emphasis is on regression, where four different graphs were evaluated experimentally with a subproblem of face detection on photographs. The method proposed is promising particularly when linear models are insufﬁcient as well as when feature selection is difﬁcult. Keywords: slow feature analysis, feature extraction, classiﬁcation, regression, pattern recognition, training graphs, nonlinear dimensionality reduction, supervised learning, implicitly supervised, high-dimensional data, image analysis</p><p>4 0.47651708 <a title="48-lda-4" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>Author: Lorenzo Rosasco, Silvia Villa, Sofia Mosci, Matteo Santoro, Alessandro Verri</p><p>Abstract: In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose a new notion of nonparametric sparsity and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art methods. Keywords: sparsity, nonparametric, variable selection, regularization, proximal methods, RKHS ∗. Also at Istituto Italiano di Tecnologia, Via Morego 30, 16163 Genova, Italy and Massachusetts Institute of Technology, Bldg. 46-5155, 77 Massachusetts Avenue, Cambridge, MA 02139, USA. c 2013 Lorenzo Rosasco, Silvia Villa, Soﬁa Mosci, Matteo Santoro and Alessandro Verri. ROSASCO , V ILLA , M OSCI , S ANTORO AND V ERRI</p><p>5 0.47516564 <a title="48-lda-5" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>Author: Wendelin Böhmer, Steffen Grünewälder, Yun Shen, Marek Musial, Klaus Obermayer</p><p>Abstract: Linear reinforcement learning (RL) algorithms like least-squares temporal difference learning (LSTD) require basis functions that span approximation spaces of potential value functions. This article investigates methods to construct these bases from samples. We hypothesize that an ideal approximation spaces should encode diffusion distances and that slow feature analysis (SFA) constructs such spaces. To validate our hypothesis we provide theoretical statements about the LSTD value approximation error and induced metric of approximation spaces constructed by SFA and the state-of-the-art methods Krylov bases and proto-value functions (PVF). In particular, we prove that SFA minimizes the average (over all tasks in the same environment) bound on the above approximation error. Compared to other methods, SFA is very sensitive to sampling and can sometimes fail to encode the whole state space. We derive a novel importance sampling modiﬁcation to compensate for this effect. Finally, the LSTD and least squares policy iteration (LSPI) performance of approximation spaces constructed by Krylov bases, PVF, SFA and PCA is compared in benchmark tasks and a visual robot navigation experiment (both in a realistic simulation and with a robot). The results support our hypothesis and suggest that (i) SFA provides subspace-invariant features for MDPs with self-adjoint transition operators, which allows strong guarantees on the approximation error, (ii) the modiﬁed SFA algorithm is best suited for LSPI in both discrete and continuous state spaces and (iii) approximation spaces encoding diffusion distances facilitate LSPI performance. Keywords: reinforcement learning, diffusion distance, proto value functions, slow feature analysis, least-squares policy iteration, visual robot navigation c 2013 Wendelin B¨ hmer, Steffen Gr¨ new¨ lder, Yun Shen, Marek Musial and Klaus Obermayer. o u a ¨ ¨ ¨ B OHMER , G R UNEW ALDER , S HEN , M USIAL AND O BERMAYER</p><p>6 0.4750537 <a title="48-lda-6" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>7 0.47280994 <a title="48-lda-7" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>8 0.46980706 <a title="48-lda-8" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>9 0.46963319 <a title="48-lda-9" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>10 0.46952835 <a title="48-lda-10" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>11 0.46779272 <a title="48-lda-11" href="./jmlr-2013-GURLS%3A_A_Least_Squares_Library_for_Supervised_Learning.html">46 jmlr-2013-GURLS: A Least Squares Library for Supervised Learning</a></p>
<p>12 0.46764114 <a title="48-lda-12" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>13 0.46686149 <a title="48-lda-13" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>14 0.46603316 <a title="48-lda-14" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>15 0.46580583 <a title="48-lda-15" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>16 0.46572497 <a title="48-lda-16" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>17 0.46381688 <a title="48-lda-17" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>18 0.46250853 <a title="48-lda-18" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>19 0.46245402 <a title="48-lda-19" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>20 0.46163771 <a title="48-lda-20" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
