<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-45" href="#">jmlr2013-45</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</h1>
<br/><p>Source: <a title="jmlr-2013-45-pdf" href="http://jmlr.org/papers/volume14/vanhatalo13a/vanhatalo13a.pdf">pdf</a></p><p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>Reference: <a title="jmlr-2013-45-reference" href="../jmlr2013_reference/jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Box 65 FI-00014 Helsinki, Finland  Jaakko Riihim¨ ki a Jouni Hartikainen Pasi Jyl¨ nki a Ville Tolvanen Aki Vehtari  JAAKKO . [sent-5, score-0.205]
</p><p>2 Box 12200 FI-00076 Aalto, Finland  Editor: Balazs Kegl  Abstract The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. [sent-17, score-0.176]
</p><p>3 The tools include, among others, various inference methods, sparse approximations and model assessment methods. [sent-18, score-0.169]
</p><p>4 Introduction Gaussian process (GP) prior provides a ﬂexible building block for many hierarchical Bayesian models (Rasmussen and Williams, 2006). [sent-20, score-0.037]
</p><p>5 1) is a versatile collection of computational tools for GP models and it has already been used in several published projects, for example, in epidemiology, species distribution modeling and building energy usage modeling (see Vanhatalo et al. [sent-22, score-0.125]
</p><p>6 GPstuff combines models and inference tools in a modular format. [sent-24, score-0.112]
</p><p>7 It also provides various sparse GP models and methods for model assessment. [sent-25, score-0.081]
</p><p>8 The toolbox is compatible with Unix and Windows Matlab (at least r2009b or later). [sent-26, score-0.051]
</p><p>9 , yn ]T related to inputs (covariates) X = {xi = [xi,1 , . [sent-38, score-0.041]
</p><p>10 xi,d ]T }n are assumed to be conditionally independent given a latent function (or prei=1 dictor) f (x) so that the likelihood p(y|f, γ) = ∏n p(yi | fi , γ), where f = [ f (x1 ), . [sent-41, score-0.145]
</p><p>11 The latent function is given a GP prior, f ∼ GP(m(x|φ), k(x, x′ |θ)) which is deﬁned by the mean and covariance function, m(x|φ) and k(x, x′ |θ) respectively. [sent-48, score-0.114]
</p><p>12 The parameters, ϑ = {γ, φ, θ}, are given a hyperprior after which the posterior p(f|y, X) is approximated and used for prediction. [sent-49, score-0.051]
</p><p>13 Most of the models in GPstuff follow the above single latent dependency, but there are also models where each factor depends on multiple latent values. [sent-50, score-0.17]
</p><p>14 We illustrate the construction and inference of a GP model with a regression example. [sent-51, score-0.049]
</p><p>15 First, we assume yi = f (xi ) + εi , εi ∼ N(0, σ2 ), and give f (x) a GP prior with a squared exponential covariance function, k(x, x′ ) = σ2 exp(||x − x′ ||2 /2l 2 ). [sent-52, score-0.066]
</p><p>16 function gp = gp_set(’lik’, lik, ’cf’, gpcf); % init. [sent-58, score-0.389]
</p><p>17 The structures lik and gpcf contain all the essential information about the likelihood and covariance function such as parameter values and function handles to construct a covariance matrix and its gradient with respect to the parameters. [sent-60, score-0.483]
</p><p>18 All the model blocks are collected into a GP structure constructed by gp set. [sent-61, score-0.433]
</p><p>19 The ﬁrst assumes a Gaussian observation model which enables an analytic solution for the marginal likelihood p(y|X, ϑ) and the conditional posterior p(f|X, y, ϑ). [sent-63, score-0.107]
</p><p>20 Using the relation p(ϑ|y, X) ∝ p(y|X, ϑ)p(ϑ) the parameters, ϑ, can be optimized to the maximum a posterior (MAP) estimate or marginalized over with grid, central composite design (CCD), importance sampling (IS) or Markov chain Monte Carlo (MCMC) integration (Vanhatalo et al. [sent-64, score-0.083]
</p><p>21 With other observation models the marginal likelihood and the conditional posterior have to be approximated either with Laplace’s method (LA) or expectation propagation (EP) (Rasmussen and Williams, 2006). [sent-66, score-0.144]
</p><p>22 An alternative approach is to sample from the joint posterior p(f, ϑ|X, y) with MCMC by alternating sampling from p(f|X, y, ϑ) and p(ϑ|X, y, f). [sent-67, score-0.051]
</p><p>23 Above, gp optim returns a redeﬁned model structure with parameter values optimized to their MAP estimate. [sent-68, score-0.389]
</p><p>24 gp pred returns the conditional posterior predictive mean, E[ f |y, X, ϑ] and variance Var[ f |y, X, ϑ] at the test inputs. [sent-70, score-0.44]
</p><p>25 Many sparse GPs have been proposed to speed up the computations with large data sets. [sent-71, score-0.044]
</p><p>26 GPstuff includes FI(T)C, PIC, SOR, DTC (Qui˜ onero-Candela and Rasmussen, 2005), VAR (Titsias, 2009), n CS+FIC (Vanhatalo and Vehtari, 2008) sparse approximations, and several compactly supported (CS) covariance functions. [sent-72, score-0.11]
</p><p>27 gpcf2 = gpcf_ppcs2(’nin’, nin, ’lengthScale’, 5, ’magnSigma2’, 1); gp = gp_set(’type’,’CS+FIC’,’lik’,lik,’cf’,{gpcf,gpcf2},’X_u’,Xu)  In the ﬁrst line, a CS covariance function, piecewise polynomial of second order, is created. [sent-74, score-0.455]
</p><p>28 It is then given to the GP structure together with inducing inputs (Xu) and sparse GP type deﬁnition. [sent-75, score-0.111]
</p><p>29 We can tailor the above model, for example, by replacing the Gaussian observation model with a more robust Student-t observation model (Jyl¨ nki et al. [sent-76, score-0.137]
</p><p>30 GPstuff has wide variety of observation models (see Table 1) of which we want to highlight implementations of recently proposed multinomial probit with EP (Riihim¨ ki et al. [sent-79, score-0.224]
</p><p>31 , 2013) and logistic a GP density estimation and regression with Laplace approximation (Riihim¨ ki and Vehtari, 2012). [sent-80, score-0.068]
</p><p>32 a The constructed models could be compared, for example, with deviance information criterion (DIC), widely applicable information criterion (WAIC), leave-one-out or k-fold cross-validation (LOO/kf-CV) (Vehtari and Ojanen, 2012) with functions gp dic, gp waic, gp loopred and gp kfcv. [sent-81, score-1.593]
</p><p>33 New models can be implemented by modifying the existing model blocks, such as covariance functions. [sent-82, score-0.103]
</p><p>34 Adding new inference methods is more laborious since they require summaries from model blocks which may not be provided by the current version of GPstuff. [sent-83, score-0.093]
</p><p>35 Related Software Perhaps the best known GP software packages are the Gaussian processes for Machine Learning (GPML) (Rasmussen and Nickisch, 2010) and the ﬂexible Bayesian modelling (FBM) (Neal, 1998). [sent-87, score-0.065]
</p><p>36 Overviews of alternatives are provided by the Gaussian processes website (http://www. [sent-88, score-0.034]
</p><p>37 The main advantage of GPstuff over the other GP software is its versatile collection of models and computational tools. [sent-93, score-0.13]
</p><p>38 In addition, the implementation of sparse matrix routines, used with the CS covariance functions, rely on the SuiteSparse toolbox (Davis, 2005). [sent-100, score-0.161]
</p><p>39 Pieces of code have been written by other people than us. [sent-102, score-0.024]
</p><p>40 We thank them all for sharing their code under a free software license. [sent-130, score-0.031]
</p><p>41 In case of model blocks the notation x means that it can be inferred with any inference method (EP, LA (Laplace), MCMC and in case of GPML also with VB). [sent-136, score-0.093]
</p><p>42 In case of sparse approximations, inference methods and model assessment methods x means that the method is available for all model blocks. [sent-137, score-0.143]
</p><p>43 A unifying view of sparse approximate n Gaussian process regression. [sent-167, score-0.044]
</p><p>44 Nested expectation propagation for Gaussian proa a cess classiﬁcation with a multinomial probit likelihood. [sent-185, score-0.119]
</p><p>45 Approximate Bayesian inference for latent Gausa sian models by using integrated nested Laplace approximations. [sent-188, score-0.16]
</p><p>46 Variational learning of inducing variables in sparse Gaussian processes. [sent-192, score-0.07]
</p><p>47 Modelling local and global phenomena with sparse Gaussian processes. [sent-195, score-0.044]
</p><p>48 Approximate inference for disease mapping a with sparse Gaussian processes. [sent-199, score-0.093]
</p><p>49 Bayesian modeling with Gaussian processes using the GPstuff toolbox. [sent-202, score-0.034]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gp', 0.389), ('gpstuff', 0.32), ('vanhatalo', 0.297), ('aki', 0.247), ('lik', 0.214), ('aalto', 0.206), ('jarno', 0.187), ('riihim', 0.16), ('jyl', 0.137), ('nki', 0.137), ('pasi', 0.137), ('vehtari', 0.137), ('ep', 0.136), ('jaakko', 0.123), ('mcmc', 0.12), ('ville', 0.114), ('gpml', 0.114), ('fbm', 0.107), ('gpcf', 0.107), ('hartikainen', 0.107), ('jouni', 0.107), ('tolvanen', 0.107), ('fic', 0.103), ('hmc', 0.091), ('sls', 0.091), ('rasmussen', 0.087), ('dic', 0.08), ('multinomial', 0.069), ('waic', 0.069), ('ki', 0.068), ('fi', 0.067), ('covariance', 0.066), ('versatile', 0.062), ('cs', 0.061), ('laplace', 0.058), ('bayesian', 0.058), ('cf', 0.057), ('gaussian', 0.055), ('artikainen', 0.053), ('becs', 0.053), ('ccd', 0.053), ('dtc', 0.053), ('masking', 0.053), ('nabney', 0.053), ('netlab', 0.053), ('olvanen', 0.053), ('pietil', 0.053), ('stuff', 0.053), ('weibull', 0.053), ('toolbox', 0.051), ('binomial', 0.051), ('posterior', 0.051), ('probit', 0.05), ('edward', 0.05), ('assessment', 0.05), ('inference', 0.049), ('latent', 0.048), ('helsinki', 0.048), ('sor', 0.046), ('anki', 0.046), ('ehtari', 0.046), ('iihim', 0.046), ('inen', 0.046), ('odeling', 0.046), ('pic', 0.046), ('carl', 0.045), ('finland', 0.045), ('sparse', 0.044), ('blocks', 0.044), ('inputs', 0.041), ('qui', 0.041), ('cox', 0.041), ('nin', 0.041), ('rocesses', 0.041), ('models', 0.037), ('lengthscale', 0.035), ('metropolis', 0.035), ('slice', 0.035), ('aussian', 0.035), ('la', 0.035), ('processes', 0.034), ('opt', 0.033), ('marginalized', 0.032), ('ml', 0.032), ('davis', 0.032), ('software', 0.031), ('yl', 0.03), ('vb', 0.03), ('likelihood', 0.03), ('cv', 0.029), ('rue', 0.028), ('neal', 0.028), ('williams', 0.027), ('priors', 0.027), ('tools', 0.026), ('var', 0.026), ('marginal', 0.026), ('inducing', 0.026), ('nested', 0.026), ('people', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="45-tfidf-1" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>2 0.23465608 <a title="45-tfidf-2" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>3 0.1264182 <a title="45-tfidf-3" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>4 0.10839439 <a title="45-tfidf-4" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian ﬁeld, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model’s partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. Through the expansion, we see that EP is correct to ﬁrst order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. Keywords: expectation consistent inference, expectation propagation, perturbation correction, Wick expansions, Ising model, Gaussian process</p><p>5 0.093943052 <a title="45-tfidf-5" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>Author: Matthew J. Urry, Peter Sollich</p><p>Abstract: We consider learning on graphs, guided by kernels that encode similarity between vertices. Our focus is on random walk kernels, the analogues of squared exponential kernels in Euclidean spaces. We show that on large, locally treelike graphs these have some counter-intuitive properties, specifically in the limit of large kernel lengthscales. We consider using these kernels as covariance functions of Gaussian processes. In this situation one typically scales the prior globally to normalise the average of the prior variance across vertices. We demonstrate that, in contrast to the Euclidean case, this generically leads to signiﬁcant variation in the prior variance across vertices, which is undesirable from a probabilistic modelling point of view. We suggest the random walk kernel should be normalised locally, so that each vertex has the same prior variance, and analyse the consequences of this by studying learning curves for Gaussian process regression. Numerical calculations as well as novel theoretical predictions for the learning curves using belief propagation show that one obtains distinctly different probabilistic models depending on the choice of normalisation. Our method for predicting the learning curves using belief propagation is signiﬁcantly more accurate than previous approximations and should become exact in the limit of large random graphs. Keywords: Gaussian process, generalisation error, learning curve, cavity method, belief propagation, graph, random walk kernel</p><p>6 0.088896491 <a title="45-tfidf-6" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>7 0.077957898 <a title="45-tfidf-7" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>8 0.068840109 <a title="45-tfidf-8" href="./jmlr-2013-Variational_Inference_in_Nonconjugate_Models.html">121 jmlr-2013-Variational Inference in Nonconjugate Models</a></p>
<p>9 0.046679679 <a title="45-tfidf-9" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>10 0.042248331 <a title="45-tfidf-10" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>11 0.041095052 <a title="45-tfidf-11" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>12 0.038056955 <a title="45-tfidf-12" href="./jmlr-2013-Fast_MCMC_Sampling_for_Markov_Jump_Processes_and_Extensions.html">43 jmlr-2013-Fast MCMC Sampling for Markov Jump Processes and Extensions</a></p>
<p>13 0.032275427 <a title="45-tfidf-13" href="./jmlr-2013-Bayesian_Nonparametric_Hidden_Semi-Markov_Models.html">16 jmlr-2013-Bayesian Nonparametric Hidden Semi-Markov Models</a></p>
<p>14 0.025761919 <a title="45-tfidf-14" href="./jmlr-2013-Global_Analytic_Solution_of_Fully-observed_Variational_Bayesian_Matrix_Factorization.html">49 jmlr-2013-Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization</a></p>
<p>15 0.023474185 <a title="45-tfidf-15" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>16 0.022933839 <a title="45-tfidf-16" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>17 0.022238145 <a title="45-tfidf-17" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>18 0.022114256 <a title="45-tfidf-18" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>19 0.018867323 <a title="45-tfidf-19" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>20 0.017900037 <a title="45-tfidf-20" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.147), (1, -0.309), (2, 0.083), (3, -0.073), (4, 0.111), (5, -0.13), (6, -0.249), (7, -0.11), (8, -0.19), (9, 0.013), (10, 0.025), (11, 0.011), (12, -0.001), (13, -0.037), (14, 0.023), (15, 0.031), (16, 0.064), (17, -0.089), (18, -0.022), (19, 0.008), (20, -0.029), (21, -0.074), (22, 0.02), (23, -0.131), (24, -0.017), (25, -0.008), (26, -0.056), (27, 0.01), (28, -0.016), (29, -0.01), (30, -0.021), (31, 0.086), (32, 0.093), (33, 0.228), (34, -0.034), (35, 0.078), (36, -0.059), (37, -0.048), (38, -0.098), (39, -0.014), (40, -0.079), (41, 0.057), (42, -0.071), (43, -0.009), (44, 0.049), (45, 0.049), (46, -0.063), (47, -0.025), (48, -0.058), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95956004 <a title="45-lsi-1" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>2 0.71994984 <a title="45-lsi-2" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>Author: Jaakko Riihimäki, Pasi Jylänki, Aki Vehtari</p><p>Abstract: This paper considers probabilistic multinomial probit classiﬁcation using Gaussian process (GP) priors. Challenges with multiclass GP classiﬁcation are the integration over the non-Gaussian posterior distribution, and the increase of the number of unknown latent variables as the number of target classes grows. Expectation propagation (EP) has proven to be a very accurate method for approximate inference but the existing EP approaches for the multinomial probit GP classiﬁcation rely on numerical quadratures, or independence assumptions between the latent values associated with different classes, to facilitate the computations. In this paper we propose a novel nested EP approach which does not require numerical quadratures, and approximates accurately all betweenclass posterior dependencies of the latent values, but still scales linearly in the number of classes. The predictive accuracy of the nested EP approach is compared to Laplace, variational Bayes, and Markov chain Monte Carlo (MCMC) approximations with various benchmark data sets. In the experiments nested EP was the most consistent method compared to MCMC sampling, but in terms of classiﬁcation accuracy the differences between all the methods were small from a practical point of view. Keywords: Gaussian process, multiclass classiﬁcation, multinomial probit, approximate inference, expectation propagation</p><p>3 0.6734575 <a title="45-lsi-3" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>Author: Krzysztof Chalupka, Christopher K. I. Williams, Iain Murray</p><p>Abstract: Gaussian process (GP) predictors are an important component of many Bayesian approaches to machine learning. However, even a straightforward implementation of Gaussian process regression (GPR) requires O(n2 ) space and O(n3 ) time for a data set of n examples. Several approximation methods have been proposed, but there is a lack of understanding of the relative merits of the different approximations, and in what situations they are most useful. We recommend assessing the quality of the predictions obtained as a function of the compute time taken, and comparing to standard baselines (e.g., Subset of Data and FITC). We empirically investigate four different approximation algorithms on four different prediction problems, and make our code available to encourage future comparisons. Keywords: Gaussian process regression, subset of data, FITC, local GP</p><p>4 0.53184146 <a title="45-lsi-4" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>5 0.49133506 <a title="45-lsi-5" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: Expectation Propagation (EP) provides a framework for approximate inference. When the model under consideration is over a latent Gaussian ﬁeld, with the approximation being Gaussian, we show how these approximations can systematically be corrected. A perturbative expansion is made of the exact but intractable correction, and can be applied to the model’s partition function and other moments of interest. The correction is expressed over the higher-order cumulants which are neglected by EP’s local matching of moments. Through the expansion, we see that EP is correct to ﬁrst order. By considering higher orders, corrections of increasing polynomial complexity can be applied to the approximation. The second order provides a correction in quadratic time, which we apply to an array of Gaussian process and Ising models. The corrections generalize to arbitrarily complex approximating families, which we illustrate on tree-structured Ising model approximations. Furthermore, they provide a polynomial-time assessment of the approximation error. We also provide both theoretical and practical insights on the exactness of the EP solution. Keywords: expectation consistent inference, expectation propagation, perturbation correction, Wick expansions, Ising model, Gaussian process</p><p>6 0.43840829 <a title="45-lsi-6" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>7 0.42046994 <a title="45-lsi-7" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>8 0.27154496 <a title="45-lsi-8" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>9 0.23668005 <a title="45-lsi-9" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>10 0.20406163 <a title="45-lsi-10" href="./jmlr-2013-Fast_MCMC_Sampling_for_Markov_Jump_Processes_and_Extensions.html">43 jmlr-2013-Fast MCMC Sampling for Markov Jump Processes and Extensions</a></p>
<p>11 0.18050367 <a title="45-lsi-11" href="./jmlr-2013-Variational_Inference_in_Nonconjugate_Models.html">121 jmlr-2013-Variational Inference in Nonconjugate Models</a></p>
<p>12 0.15191962 <a title="45-lsi-12" href="./jmlr-2013-Bayesian_Nonparametric_Hidden_Semi-Markov_Models.html">16 jmlr-2013-Bayesian Nonparametric Hidden Semi-Markov Models</a></p>
<p>13 0.14208299 <a title="45-lsi-13" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>14 0.14089425 <a title="45-lsi-14" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>15 0.13615197 <a title="45-lsi-15" href="./jmlr-2013-Global_Analytic_Solution_of_Fully-observed_Variational_Bayesian_Matrix_Factorization.html">49 jmlr-2013-Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization</a></p>
<p>16 0.12927008 <a title="45-lsi-16" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>17 0.12910789 <a title="45-lsi-17" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>18 0.12600943 <a title="45-lsi-18" href="./jmlr-2013-Pairwise_Likelihood_Ratios_for_Estimation_of_Non-Gaussian_Structural_Equation_Models.html">85 jmlr-2013-Pairwise Likelihood Ratios for Estimation of Non-Gaussian Structural Equation Models</a></p>
<p>19 0.12010051 <a title="45-lsi-19" href="./jmlr-2013-BudgetedSVM%3A_A_Toolbox_for_Scalable_SVM_Approximations.html">19 jmlr-2013-BudgetedSVM: A Toolbox for Scalable SVM Approximations</a></p>
<p>20 0.11888875 <a title="45-lsi-20" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.02), (5, 0.055), (6, 0.022), (10, 0.028), (61, 0.015), (70, 0.011), (75, 0.705), (87, 0.025), (93, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96384943 <a title="45-lda-1" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>2 0.91102988 <a title="45-lda-2" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>Author: Lisha Chen, Andreas Buja</p><p>Abstract: Multidimensional scaling (MDS) is the art of reconstructing pointsets (embeddings) from pairwise distance data, and as such it is at the basis of several approaches to nonlinear dimension reduction and manifold learning. At present, MDS lacks a unifying methodology as it consists of a discrete collection of proposals that differ in their optimization criteria, called “stress functions”. To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. This methodology yields the following beneﬁts and problem solutions: (a) It provides guidance in tailoring stress functions to a given data situation, responding to the fact that no single stress function dominates all others across all data situations; (b) the methodology enriches the supply of available stress functions; (c) it helps our understanding of stress functions by replacing the comparison of discrete proposals with a characterization of the effect of parameters on embeddings; (d) it builds a bridge to graph drawing, which is the related but not identical art of constructing embeddings from graphs. Keywords: multidimensional scaling, force-directed layout, cluster analysis, clustering strength, unsupervised learning, Box-Cox transformations</p><p>3 0.82589376 <a title="45-lda-3" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>Author: Philémon Brakel, Dirk Stroobandt, Benjamin Schrauwen</p><p>Abstract: Imputing missing values in high dimensional time-series is a difﬁcult problem. This paper presents a strategy for training energy-based graphical models for imputation directly, bypassing difﬁculties probabilistic approaches would face. The training strategy is inspired by recent work on optimization-based learning (Domke, 2012) and allows complex neural models with convolutional and recurrent structures to be trained for imputation tasks. In this work, we use this training strategy to derive learning rules for three substantially different neural architectures. Inference in these models is done by either truncated gradient descent or variational mean-ﬁeld iterations. In our experiments, we found that the training methods outperform the Contrastive Divergence learning algorithm. Moreover, the training methods can easily handle missing values in the training data itself during learning. We demonstrate the performance of this learning scheme and the three models we introduce on one artiﬁcial and two real-world data sets. Keywords: neural networks, energy-based models, time-series, missing values, optimization</p><p>4 0.7926439 <a title="45-lda-4" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>Author: Wei Pan, Xiaotong Shen, Binghui Liu</p><p>Abstract: Clustering analysis is widely used in many ﬁelds. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classiﬁcation and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classiﬁcation and regression, such as model selection criteria to select the number of clusters, a difﬁcult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method’s promising performance. Keywords: generalized degrees of freedom, grouping, K-means clustering, Lasso, penalized regression, truncated Lasso penalty (TLP)</p><p>5 0.73373383 <a title="45-lda-5" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>Author: Ran Gilad-Bachrach, Christopher J.C. Burges</p><p>Abstract: Typically, one approaches a supervised machine learning problem by writing down an objective function and ﬁnding a hypothesis that minimizes it. This is equivalent to ﬁnding the Maximum A Posteriori (MAP) hypothesis for a Boltzmann distribution. However, MAP is not a robust statistic. We present an alternative approach by deﬁning a median of the distribution, which we show is both more robust, and has good generalization guarantees. We present algorithms to approximate this median. One contribution of this work is an efﬁcient method for approximating the Tukey median. The Tukey median, which is often used for data visualization and outlier detection, is a special case of the family of medians we deﬁne: however, computing it exactly is exponentially slow in the dimension. Our algorithm approximates such medians in polynomial time while making weaker assumptions than those required by previous work. Keywords: classiﬁcation, estimation, median, Tukey depth</p><p>6 0.55115151 <a title="45-lda-6" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>7 0.47932315 <a title="45-lda-7" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>8 0.44554493 <a title="45-lda-8" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>9 0.42223027 <a title="45-lda-9" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>10 0.39082089 <a title="45-lda-10" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>11 0.38763854 <a title="45-lda-11" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>12 0.38602251 <a title="45-lda-12" href="./jmlr-2013-Using_Symmetry_and_Evolutionary_Search_to_Minimize_Sorting_Networks.html">118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</a></p>
<p>13 0.3694379 <a title="45-lda-13" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>14 0.35840765 <a title="45-lda-14" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>15 0.35616741 <a title="45-lda-15" href="./jmlr-2013-Differential_Privacy_for_Functions_and_Functional_Data.html">32 jmlr-2013-Differential Privacy for Functions and Functional Data</a></p>
<p>16 0.34837729 <a title="45-lda-16" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>17 0.34025571 <a title="45-lda-17" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>18 0.33691061 <a title="45-lda-18" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>19 0.33625892 <a title="45-lda-19" href="./jmlr-2013-Variational_Inference_in_Nonconjugate_Models.html">121 jmlr-2013-Variational Inference in Nonconjugate Models</a></p>
<p>20 0.33470964 <a title="45-lda-20" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
