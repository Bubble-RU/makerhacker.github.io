<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-111" href="#">jmlr2013-111</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</h1>
<br/><p>Source: <a title="jmlr-2013-111-pdf" href="http://jmlr.org/papers/volume14/mairal13a/mairal13a.pdf">pdf</a></p><p>Author: Julien Mairal, Bin Yu</p><p>Abstract: We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called “path coding” penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efﬁciently solve by leveraging network ﬂow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs. Keywords: convex and non-convex optimization, network ﬂow optimization, graph sparsity</p><p>Reference: <a title="jmlr-2013-111-reference" href="../jmlr2013_reference/jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We introduce structured sparsity penalties over paths on a DAG called “path coding” penalties. [sent-11, score-0.553]
</p><p>2 Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. [sent-12, score-0.72]
</p><p>3 The penalties and their proximal operators involve path selection problems, which we efﬁciently solve by leveraging network ﬂow optimization. [sent-13, score-0.768]
</p><p>4 Keywords: convex and non-convex optimization, network ﬂow optimization, graph sparsity  1. [sent-15, score-0.417]
</p><p>5 When the solution is known beforehand to be sparse— that is, has only a few non-zero coefﬁcients, regularizing with sparsity-inducing penalties has been shown to provide better prediction and solutions that are easier to interpret. [sent-18, score-0.353]
</p><p>6 To the best of our knowledge, penalties promoting the connectivity of sparsity patterns in a graph can be classiﬁed into two categories. [sent-44, score-0.667]
</p><p>7 The problem of ﬁnding tractable penalties that model long-range interactions is therefore acute. [sent-53, score-0.358]
</p><p>8 Given a pre-deﬁned set of possibly overlapping groups of variables G , these two structured sparsity-inducing regularization functions encourage a sparsity pattern to be in the union of a small number of groups from G . [sent-58, score-0.397]
</p><p>9 Both penalties induce a similar regularization effect and are strongly related to each other. [sent-59, score-0.37]
</p><p>10 Right (b): when the graph is a DAG, the sparsity pattern is covered by two paths (2, 3, 6) and (9, 11, 12) represented by bold arrows. [sent-77, score-0.394]
</p><p>11 (2011) a novel group structure G p that contains all the paths in G; a path is deﬁned as a sequence of vertices (v1 , . [sent-83, score-0.4]
</p><p>12 Even though the number of paths in a DAG is exponential in the graph size, we map the path selection problems our penalties involve to network ﬂow formulations (see Ahuja et al. [sent-92, score-0.845]
</p><p>13 This allows us to efﬁciently compute the penalties and their proximal operators, a key tool to address regularized problems (see Bach et al. [sent-95, score-0.511]
</p><p>14 Therefore, we make in this paper a new link between structured graph penalties in DAGs and network ﬂow optimization. [sent-97, score-0.632]
</p><p>15 This is made possible in the context of path coding penalties thanks to decomposability properties of the path costs, which we make explicit in Section 3. [sent-106, score-0.815]
</p><p>16 The total-variation penalty can be useful to obtain piecewise constant solutions on a graph (see Chen et al. [sent-114, score-0.394]
</p><p>17 (2009) encourages solutions whose sparsity pattern is a union of a few groups, whereas the penalty of Jenatton et al. [sent-123, score-0.385]
</p><p>18 To summarize, we have designed non-convex and convex penalty functions to do feature selection in directed acyclic graphs. [sent-132, score-0.368]
</p><p>19 Because our penalties involve an exponential number of variables, one for every path in the graph, existing optimization techniques cannot be used. [sent-133, score-0.501]
</p><p>20 To deal with this issue, we introduce network ﬂow optimization tools that implicitly handle the exponential number of paths, allowing the penalties and their proximal operators to be computed in polynomial time. [sent-134, score-0.623]
</p><p>21 As a result, our penalties can model long-range interactions in the graph and are tractable. [sent-135, score-0.541]
</p><p>22 In Section 3, we propose the path coding penalties and optimization techniques for solving the corresponding sparse estimation problems. [sent-137, score-0.672]
</p><p>23 Section 4 is devoted to experiments on synthetic, genomic, and image data to demonstrate the beneﬁts of path coding penalties over existing ones and the scalability of our approach. [sent-138, score-0.678]
</p><p>24 Preliminaries As we show later, our path coding penalties are related to the concept of ﬂow in a graph. [sent-141, score-0.636]
</p><p>25 It consists of ﬁnding a ﬂow f in F minimizing 2453  M AIRAL AND Y U  a linear cost ∑(u,v)∈E cuv fuv , where every arc (u, v) in E has a cost cuv in R. [sent-170, score-0.768]
</p><p>26 Either one is looking for the value fuv of a ﬂow on every arc (u, v) of a graph minimizing the cost ∑(u,v)∈E cuv fuv , or one is looking for the quantity of ﬂow that should circulate on every (s,t)-path and cycle ﬂow for minimizing the same cost. [sent-194, score-0.902]
</p><p>27 We will deﬁne ﬂow problems such that selecting a path in the context of our path coding penalties is equivalent to sending some ﬂow along a corresponding (s,t)-path. [sent-196, score-0.874]
</p><p>28 We will also exploit the integrality property to develop tools both adapted to non-convex penalties and convex ones, respectively involving discrete and continuous optimization problems. [sent-197, score-0.379]
</p><p>29 Sparse Estimation in Graphs with Path Coding Penalties We now present our path coding penalties, which exploit the structured sparsity frameworks of Jacob et al. [sent-220, score-0.462]
</p><p>30 3 Now that ϕG and ψG have been introduced, we are interested in automatically selecting a small number of connected subgraphs from a directed acyclic graph G = (V, E). [sent-276, score-0.398]
</p><p>31 As a result, the path coding penalties ϕG p and ψG p encourage solutions that are sparse while forming a subgraph that can be covered by a small number of paths. [sent-278, score-0.71]
</p><p>32 In plain words, the graph G′ , which is a DAG, contains the graph G and two nodes s,t that are linked to every vertices of G. [sent-286, score-0.488]
</p><p>33 Let us also assume that some costs cuv in R are deﬁned for all arcs (u, v) in E ′ . [sent-287, score-0.357]
</p><p>34 , uk ) in G p , we deﬁne the weight ηg as k−1  ηg  csu1 +  ∑ cu u  i i+1  + cuk t =  i=1  ∑  cuv ,  (8)  (u,v)∈(s,g,t)  where the notation (s, g,t) stands for the path (s, u1 , u2 , . [sent-291, score-0.377]
</p><p>35 Designing costs cuv that go beyond the simple choice (7) can be useful whenever one has additional knowledge about the graph structure. [sent-298, score-0.47]
</p><p>36 This is illustrated in Figure 3b where the cost on the arc (s, 1) is much smaller than on the arcs (s, 2), (s, 3), (s, 4), therefore encouraging paths starting from vertex 1. [sent-301, score-0.39]
</p><p>37 At the same time as us, Obozinski and Bach (2012) have studied a larger class of non-convex combinatorial penalties and their corresponding convex relaxations, obtaining in particular a more general result than Lemma 2, showing that ψG is the tightest convex relaxation of ϕG . [sent-303, score-0.436]
</p><p>38 2457  M AIRAL AND Y U  Another interpretation connecting the path-coding penalties with coding lengths and random walks can be drawn using information theoretic arguments derived from Huang et al. [sent-304, score-0.457]
</p><p>39 The second key component of our approach is the fact that the cost of a ﬂow f in F sending one unit from s to t along a path g in G, 2458  F EATURE S ELECTION IN G RAPHS WITH PATH C ODING P ENALTIES  deﬁned as ∑(u,v)∈E ′ fuv cuv = ∑(u,v)∈(s,g,t) cuv is exactly ηg , according to Equation (8). [sent-329, score-0.853]
</p><p>40 We can now formally state the mappings between the penalties ϕG p and ψG p on one hand, and network ﬂows on the other hand. [sent-332, score-0.4]
</p><p>41 u∈V ′ :(u, j)∈E ′  Our formulations involve capacity constraints and costs for s j ( f ), which can be handled by network ﬂow solvers; in fact, a vertex can always be equivalently replaced in the network by two vertices, linked by an arc that carries the ﬂow quantity s j ( f ) (Ahuja et al. [sent-337, score-0.442]
</p><p>42 4 Given the deﬁnition of the penalty ϕG in Equation (5), computing ϕG p seems challenging for two reasons: (i) Equation (5) is for a general group structure G a NP-hard Boolean linear program with |G | variables; (ii) the size of G p is exponential in the graph size. [sent-348, score-0.457]
</p><p>43 This allows us to identify the cost of sending one unit of ﬂow in G′ from s to t along a path g to the cost of selecting the path g in the context of the path coding penalty ϕG p . [sent-351, score-1.054]
</p><p>44 Let us deﬁne p  f ⋆ ∈ arg min f ∈F  ∑  (u,v)∈E ′  1 max u2 (1 − s j ( f )), 0 j 2 j=1  fuv cuv + ∑  ,  (11)  where F is the set of ﬂows on G′ . [sent-381, score-0.361]
</p><p>45 Let us deﬁne p  f ⋆ ∈ arg min f ∈F  ∑  (u,v)∈E ′  1 max |u j | − s j ( f ), 0 j=1 2  fuv cuv + ∑  2  ,  (12)  where F is the set of ﬂows on G′ . [sent-397, score-0.361]
</p><p>46 (2011) that proximal gradient methods for convex optimization are robust to inexact computations of the proximal operator, as long as the precision of these computations iteratively increases with an appropriate rate. [sent-419, score-0.464]
</p><p>47 Using classical Lagrangian duality, it is possible to obtain the following dual formulation max  π∈R p  ∑  quv (πu − πv ), where quv (πu − πv )  (u,v)∈E  min [Cuv ( fuv ) − (πu − πv ) fuv ] . [sent-425, score-0.355]
</p><p>48 1: Choose any path g ∈ G p such that κg = 0; 2: δ ← −∞; 3: while δ < 0 do κ 4: τ ← ηgg 1 ; 5: g ← arg minh∈G p lτ (h); (shortest path problem in a directed acyclic graph); 6: δ ← lτ (g); 7: end while 8: Return: τ = ψ∗ p (κ) (value of the dual norm). [sent-452, score-0.487]
</p><p>49 As far as the choice of the parameters is concerned, we have observed that all penalties we have considered in our experiments are very sensitive to the regularization parameter λ. [sent-501, score-0.37]
</p><p>50 A topological ordering of vertices in a directed graph is such that if there is an arc from vertex u to vertex v, then u ≺ v. [sent-533, score-0.516]
</p><p>51 1 almost all penalties almost perfectly recover the true pattern; • medium SNR: for σ = 0. [sent-544, score-0.354]
</p><p>52 2, where non-convex penalties outperform convex ones according to one performance measure, while being the other way around for another one. [sent-550, score-0.379]
</p><p>53 Whereas OLS does not change the results obtained with the non-convex penalties we consider, it changes signiﬁcantly the ones obtained with the convex ones. [sent-552, score-0.379]
</p><p>54 This produces fairly connected sparsity patterns, but does not exploit arc directions; • the scenario path is similar to graph, but we iteratively add new vertices following single paths in G. [sent-558, score-0.589]
</p><p>55 It exploits arc directions and produces sparsity patterns that can be covered by a small number of paths, which is the sort of patterns that our path-coding penalties encourage. [sent-559, score-0.518]
</p><p>56 (2009) where the groups G are pairs of vertices linked by an arc; • our path-coding penalties ϕG p or ψG p with the weights ηg deﬁned in (7). [sent-564, score-0.565]
</p><p>57 We report the results for the three graphs, three scenarii for generating w0 , three noise levels and the ﬁve penalties in Figure 4. [sent-577, score-0.365]
</p><p>58 The comparisons are the following: • convex vs non-convex (ℓ0 vs ℓ1 and ϕG p vs ψG p ): For high SNR, non-convex penalties do signiﬁcantly better than convex ones, whereas it is the other way around for low SNR. [sent-583, score-0.547]
</p><p>59 • unstructured vs path-coding (ℓ0 vs ϕG p and ℓ1 vs ψG p ): In the structured scenarii graph and path, the structured penalties ϕG p and ψG p respectively do better than ℓ0 and ℓ1 . [sent-586, score-0.757]
</p><p>60 2466  F EATURE S ELECTION IN G RAPHS WITH PATH C ODING P ENALTIES  the best results are shared between StructOMP and our penalties for high and medium SNR, and our penalties do better for low SNR. [sent-601, score-0.676]
</p><p>61 To conclude this experiment, we have shown that our penalties offer a competitive alternative to StructOMP and the penalty of Jacob et al. [sent-603, score-0.533]
</p><p>62 In low SNR, convex penalties are indeed better behaved than non-convex ones, whereas it is the other way around when the SNR is high. [sent-607, score-0.379]
</p><p>63 For a speciﬁc noise level and speciﬁc graph, the results for three scenarii, ﬂat, graph and path are reported. [sent-636, score-0.362]
</p><p>64 We also exploit the variant of our penalties presented in Section 3 that allows us to choose the costs on the arcs of the graph G′ . [sent-645, score-0.664]
</p><p>65 We choose here a small cost on the arc (s, 1) of the graph G′ , and a large one for every arc (s, j), for j in {2, . [sent-646, score-0.433]
</p><p>66 As expected, simple penalties are faster to use: about 65 000 patches per second can be processed using ℓ0 . [sent-658, score-0.396]
</p><p>67 For this denoising task, it is indeed typical to have non-convex penalties outperforming convex ones (see Mairal, 2010, Section 1. [sent-663, score-0.47]
</p><p>68 Interestingly, this superiority of non-convex penalties in this denoising scheme based on overlapping patches is usually only observed after the averaging step 3. [sent-669, score-0.487]
</p><p>69 Note that the bad results obtained by convex penalties after the averaging step are possibly due to the shrinkage effect of these penalties. [sent-675, score-0.379]
</p><p>70 (2009) use their structured sparsity penalty ψG where the groups G are all pairs of genes linked by an arc. [sent-817, score-0.527]
</p><p>71 This pre-processing step is of course questionable since our penalties are originally not designed to deal with the graph G0 . [sent-821, score-0.505]
</p><p>72 We of course do not claim to be able to individually interpret each path selected by our method, but, as we show, it does not prevent our penalties ϕG p and ψG p to achieve their ultimate goal—that is connectivity in the original graph G0 . [sent-822, score-0.747]
</p><p>73 (2009) where the groups G are pairs of vertices linked by an arc; • a variant of the penalty ψG of Jacob et al. [sent-827, score-0.419]
</p><p>74 (2011) given in Appendix A where the groups are all pairs of vertices linked by an arc; • the penalty ζG of Jenatton et al. [sent-829, score-0.419]
</p><p>75 We remark that our penalties ϕG p and ψG p succeed in selecting very few connected components of G0 , on average 1. [sent-846, score-0.378]
</p><p>76 We observe that the outcome G does not signiﬁcantly change the sparsity and connectivity in G0 of the sparsity patterns our penalty selects. [sent-857, score-0.472]
</p><p>77 We have for example tried to use the same graph G, but where we randomly permute the p predictors (genes) at every run, making the graph structure irrelevant to the data. [sent-867, score-0.366]
</p><p>78 For instance, 5 genes are selected by ℓ1 in more than half of the experimental runs, whereas this number is 10 and 14 for the penalties of Jacob et al. [sent-876, score-0.357]
</p><p>79 Conclusion Our paper proposes a new form of structured penalty for supervised learning problems where predicting features are sitting on a DAG, and where one wishes to automatically select a few connected subgraphs of the DAG. [sent-944, score-0.375]
</p><p>80 The computational feasibility of this form of penalty is established by making a new link between supervised path selection problems and network ﬂows. [sent-945, score-0.468]
</p><p>81 Our penalties admit non-convex and convex variants, which can be used within the same network ﬂow optimization framework. [sent-946, score-0.457]
</p><p>82 These penalties are ﬂexible in the sense that they can control the connectivity of a problem solution, whether one wishes to encourage large or small connected components, and are able to model long-range interactions between variables. [sent-947, score-0.477]
</p><p>83 Some of our conclusions show that being able to provide both non-convex and convex variants of the penalties is valuable. [sent-948, score-0.379]
</p><p>84 In mathematical terms, it corresponds to selecting a few paths in a directed acyclic graph in a penalized maximum likelihood formulation. [sent-958, score-0.366]
</p><p>85 This penalty can be interpreted as the ℓ1 -norm of the vector [ηg wg ν ]g∈G , therefore inducing sparsity at the group level. [sent-968, score-0.373]
</p><p>86 (2009) encourages solutions whose set of non-zero coefﬁcients is a union of a few groups, the penalty ζG promotes solutions whose sparsity pattern is in the intersection of some selected groups. [sent-973, score-0.385]
</p><p>87 When the groups are deﬁned as the pairs of vertices linked by an arc, it is indeed not clear that sparsity patterns deﬁned as the intersection of such groups would lead to a well-connected subgraph. [sent-976, score-0.393]
</p><p>88 However, when the graph is a DAG, there exists an appropriate group setting G when the sparsity pattern of the solution is expected to be a single connected component of the DAG. [sent-978, score-0.461]
</p><p>89 Let us indeed deﬁne the groups to be the sets of ancestors, and sets of descendents for every vertex; the set of descendents of a vertex u in a DAG are deﬁned as all vertices v such that there exists a path from u 2475  M AIRAL AND Y U  to v. [sent-979, score-0.393]
</p><p>90 The corresponding penalty ζG encourages sparsity patterns which are intersections of groups in G , which can be shown to be exactly the connected subgraphs of the DAG. [sent-981, score-0.557]
</p><p>91 10 This penalty is tractable since the number of groups is linear in the number of vertices, but as soon as the sparsity pattern of the solution is not connex (contains more than one connected component), it is unable to recover it, making it useful to seek for a more ﬂexible approach. [sent-982, score-0.512]
</p><p>92 (2011) have shown that the penalty ζG with ν = ∞ and any arbitrary group structure G is related to network ﬂows, but for different reasons than the penalties ϕG p and ψG p . [sent-986, score-0.674]
</p><p>93 The penalty ζG is indeed unrelated to the concept of graph sparsity since it does not require the features to have any graph structure. [sent-987, score-0.676]
</p><p>94 the term − log2 π(s, v1 ) represents the number of bits used to indicate that a path g starts with the vertex v1 , whereas the bits corresponding to the terms − log2 π(vi , vi+1 ) indicate that the vertex following vi is vi+1 . [sent-1070, score-0.359]
</p><p>95 We have therefore shown that (i) the different terms composing the weights ηg can be interpreted as the number of bits used to encode the paths in the graph; (ii) it is possible to use probability transition matrices (or random walks) on the graph to design the weights ηg . [sent-1075, score-0.373]
</p><p>96 2 Proof of Proposition 5 Proof Using the deﬁnition of the proximal operator in Equation (3) and the deﬁnition of ϕG in Equation (5), there exists a pattern S in {0, 1} p such that the solution w⋆ of the proximal problem satisﬁes for all j, w⋆ = u j if j is in S, and w⋆ = 0 otherwise. [sent-1101, score-0.438]
</p><p>97 We therefore rewrite Equation (3) by j j using the result of Proposition 3 min p  S∈{0,1} , f ∈F  1 2  ∑ u2j + ∑ j∈S /  fuv cuv s. [sent-1102, score-0.361]
</p><p>98 With this choice for S, we have in addition p ∑ j∈S u2 = ∑ j=1 max u2 (1 − s j ( f )), 0 , and denoting by Fint the set of integer ﬂows, we can equiv/ j j alently rewrite the optimization problem p  min  f ∈Fint  ∑  (u,v)∈E ′  1 max u2 (1 − s j ( f )), 0 j j=1 2  fuv cuv + ∑  . [sent-1111, score-0.361]
</p><p>99 According to Proposition 4, we can write the proximal problem as  min p  w∈R+ , f ∈F  1 2  p  ∑ (u j − w j )2 + ∑  j=1  fuv cuv s. [sent-1119, score-0.55]
</p><p>100 Updating the path g in the algorithm can be done by solving a shortest path problem in the graph G′ , which can be done in O(|E|) operations since the graph is acyclic (Ahuja et al. [sent-1167, score-0.769]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('penalties', 0.322), ('ow', 0.299), ('jacob', 0.267), ('penalty', 0.211), ('cuv', 0.198), ('snr', 0.191), ('proximal', 0.189), ('graph', 0.183), ('path', 0.179), ('fuv', 0.163), ('airal', 0.154), ('enalties', 0.154), ('ows', 0.142), ('coding', 0.135), ('jenatton', 0.124), ('oding', 0.119), ('mairal', 0.117), ('raphs', 0.102), ('sparsity', 0.099), ('ahuja', 0.099), ('arc', 0.097), ('xg', 0.096), ('eature', 0.096), ('election', 0.092), ('dag', 0.091), ('denoising', 0.091), ('costs', 0.089), ('gk', 0.088), ('groups', 0.086), ('structomp', 0.086), ('paths', 0.083), ('network', 0.078), ('vertices', 0.075), ('patches', 0.074), ('arcs', 0.07), ('huang', 0.068), ('group', 0.063), ('connectivity', 0.063), ('flat', 0.059), ('prox', 0.059), ('subgraphs', 0.059), ('sending', 0.059), ('convex', 0.057), ('cost', 0.056), ('connected', 0.056), ('directed', 0.055), ('vertex', 0.053), ('aharon', 0.053), ('clg', 0.051), ('jazz', 0.051), ('pgp', 0.051), ('structured', 0.049), ('supp', 0.049), ('goldberg', 0.048), ('regularization', 0.048), ('linked', 0.047), ('encourages', 0.046), ('elad', 0.046), ('proposition', 0.046), ('acyclic', 0.045), ('bach', 0.044), ('ols', 0.044), ('bertsekas', 0.043), ('boykov', 0.043), ('dabov', 0.043), ('scenarii', 0.043), ('image', 0.042), ('cycle', 0.042), ('obozinski', 0.04), ('subgraph', 0.038), ('nx', 0.038), ('vs', 0.037), ('bits', 0.037), ('interactions', 0.036), ('sparse', 0.036), ('weights', 0.035), ('genes', 0.035), ('cehver', 0.034), ('circulating', 0.034), ('dct', 0.034), ('ford', 0.034), ('portilla', 0.034), ('sink', 0.034), ('email', 0.034), ('polynomial', 0.034), ('medium', 0.032), ('dags', 0.032), ('solution', 0.031), ('encouraging', 0.031), ('dictionary', 0.03), ('dual', 0.029), ('gradient', 0.029), ('julien', 0.029), ('capacities', 0.029), ('gene', 0.029), ('pattern', 0.029), ('sent', 0.028), ('equation', 0.028), ('graphs', 0.028), ('breast', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.000001 <a title="111-tfidf-1" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>Author: Julien Mairal, Bin Yu</p><p>Abstract: We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called “path coding” penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efﬁciently solve by leveraging network ﬂow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs. Keywords: convex and non-convex optimization, network ﬂow optimization, graph sparsity</p><p>2 0.15053272 <a title="111-tfidf-2" href="./jmlr-2013-Segregating_Event_Streams_and_Noise_with_a_Markov_Renewal_Process_Model.html">98 jmlr-2013-Segregating Event Streams and Noise with a Markov Renewal Process Model</a></p>
<p>Author: Dan Stowell, Mark D. Plumbley</p><p>Abstract: We describe an inference task in which a set of timestamped event observations must be clustered into an unknown number of temporal sequences with independent and varying rates of observations. Various existing approaches to multi-object tracking assume a ﬁxed number of sources and/or a ﬁxed observation rate; we develop an approach to inferring structure in timestamped data produced by a mixture of an unknown and varying number of similar Markov renewal processes, plus independent clutter noise. The inference simultaneously distinguishes signal from noise as well as clustering signal observations into separate source streams. We illustrate the technique via synthetic experiments as well as an experiment to track a mixture of singing birds. Source code is available. Keywords: multi-target tracking, clustering, point processes, ﬂow network, sound</p><p>3 0.10206784 <a title="111-tfidf-3" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>Author: Lorenzo Rosasco, Silvia Villa, Sofia Mosci, Matteo Santoro, Alessandro Verri</p><p>Abstract: In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose a new notion of nonparametric sparsity and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art methods. Keywords: sparsity, nonparametric, variable selection, regularization, proximal methods, RKHS ∗. Also at Istituto Italiano di Tecnologia, Via Morego 30, 16163 Genova, Italy and Massachusetts Institute of Technology, Bldg. 46-5155, 77 Massachusetts Avenue, Cambridge, MA 02139, USA. c 2013 Lorenzo Rosasco, Silvia Villa, Soﬁa Mosci, Matteo Santoro and Alessandro Verri. ROSASCO , V ILLA , M OSCI , S ANTORO AND V ERRI</p><p>4 0.09080667 <a title="111-tfidf-4" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>Author: Tingni Sun, Cun-Hui Zhang</p><p>Abstract: We propose a new method of learning a sparse nonnegative-deﬁnite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm ﬁrst estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other ℓ1 regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the ℓ1 and spectrum norms of the target inverse matrix diverges to inﬁnity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso. Keywords: precision matrix, concentration matrix, inverse matrix, graphical model, scaled Lasso, linear regression, spectrum norm</p><p>5 0.090806633 <a title="111-tfidf-5" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>Author: Jun Wang, Tony Jebara, Shih-Fu Chang</p><p>Abstract: Graph-based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems, particularly in agnostic settings when no parametric information or other prior knowledge is available about the data distribution. Given the constructed graph represented by a weight matrix, transductive inference is used to propagate known labels to predict the values of all unlabeled vertices. Designing a robust label diffusion algorithm for such graphs is a widely studied problem and various methods have recently been suggested. Many of these can be formalized as regularized function estimation through the minimization of a quadratic cost. However, most existing label diffusion methods minimize a univariate cost with the classiﬁcation function as the only variable of interest. Since the observed labels seed the diffusion process, such univariate frameworks are extremely sensitive to the initial label choice and any label noise. To alleviate the dependency on the initial observed labels, this article proposes a bivariate formulation for graph-based SSL, where both the binary label information and a continuous classiﬁcation function are arguments of the optimization. This bivariate formulation is shown to be equivalent to a linearly constrained Max-Cut problem. Finally an efﬁcient solution via greedy gradient Max-Cut (GGMC) is derived which gradually assigns unlabeled vertices to each class with minimum connectivity. Once convergence guarantees are established, this greedy Max-Cut based SSL is applied on both artiﬁcial and standard benchmark data sets where it obtains superior classiﬁcation accuracy compared to existing state-of-the-art SSL methods. Moreover, GGMC shows robustness with respect to the graph construction method and maintains high accuracy over extensive experiments with various edge linking and weighting schemes. Keywords: graph transduction, semi-supervised learning, bivariate formulation, mixed integer programming, greedy</p><p>6 0.087955274 <a title="111-tfidf-6" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>7 0.080822796 <a title="111-tfidf-7" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>8 0.0795094 <a title="111-tfidf-8" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>9 0.079258017 <a title="111-tfidf-9" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>10 0.076680057 <a title="111-tfidf-10" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>11 0.071561329 <a title="111-tfidf-11" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>12 0.070911132 <a title="111-tfidf-12" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>13 0.069926068 <a title="111-tfidf-13" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>14 0.067847893 <a title="111-tfidf-14" href="./jmlr-2013-Keep_It_Simple_And_Sparse%3A_Real-Time_Action_Recognition.html">56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</a></p>
<p>15 0.063811623 <a title="111-tfidf-15" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>16 0.057845507 <a title="111-tfidf-16" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>17 0.054491818 <a title="111-tfidf-17" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>18 0.053292576 <a title="111-tfidf-18" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>19 0.051224697 <a title="111-tfidf-19" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>20 0.050822191 <a title="111-tfidf-20" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.257), (1, 0.019), (2, -0.059), (3, 0.063), (4, 0.242), (5, 0.178), (6, -0.015), (7, -0.006), (8, 0.086), (9, -0.032), (10, -0.013), (11, 0.009), (12, -0.172), (13, 0.114), (14, 0.167), (15, -0.036), (16, 0.019), (17, 0.199), (18, 0.048), (19, -0.006), (20, -0.122), (21, -0.209), (22, -0.051), (23, 0.024), (24, -0.153), (25, 0.177), (26, 0.004), (27, -0.043), (28, 0.097), (29, -0.059), (30, 0.103), (31, -0.132), (32, -0.029), (33, -0.074), (34, -0.127), (35, 0.152), (36, -0.108), (37, -0.099), (38, 0.03), (39, 0.051), (40, 0.07), (41, 0.005), (42, 0.057), (43, 0.006), (44, -0.018), (45, 0.007), (46, 0.049), (47, 0.032), (48, 0.039), (49, -0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95515966 <a title="111-lsi-1" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>Author: Julien Mairal, Bin Yu</p><p>Abstract: We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called “path coding” penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efﬁciently solve by leveraging network ﬂow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs. Keywords: convex and non-convex optimization, network ﬂow optimization, graph sparsity</p><p>2 0.75812715 <a title="111-lsi-2" href="./jmlr-2013-Segregating_Event_Streams_and_Noise_with_a_Markov_Renewal_Process_Model.html">98 jmlr-2013-Segregating Event Streams and Noise with a Markov Renewal Process Model</a></p>
<p>Author: Dan Stowell, Mark D. Plumbley</p><p>Abstract: We describe an inference task in which a set of timestamped event observations must be clustered into an unknown number of temporal sequences with independent and varying rates of observations. Various existing approaches to multi-object tracking assume a ﬁxed number of sources and/or a ﬁxed observation rate; we develop an approach to inferring structure in timestamped data produced by a mixture of an unknown and varying number of similar Markov renewal processes, plus independent clutter noise. The inference simultaneously distinguishes signal from noise as well as clustering signal observations into separate source streams. We illustrate the technique via synthetic experiments as well as an experiment to track a mixture of singing birds. Source code is available. Keywords: multi-target tracking, clustering, point processes, ﬂow network, sound</p><p>3 0.46156231 <a title="111-lsi-3" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>Author: Yuejia He, Yiyuan She, Dapeng Wu</p><p>Abstract: Recently, researchers have proposed penalized maximum likelihood to identify network topology underlying a dynamical system modeled by multivariate time series. The time series of interest are assumed to be stationary, but this restriction is never taken into consideration by existing estimation methods. Moreover, practical problems of interest may have ultra-high dimensionality and obvious node collinearity. In addition, none of the available algorithms provides a probabilistic measure of the uncertainty for the obtained network topology which is informative in reliable network identiﬁcation. The main purpose of this paper is to tackle these challenging issues. We propose the S2 learning framework, which stands for stationary-sparse network learning. We propose a novel algorithm referred to as the Berhu iterative sparsity pursuit with stationarity (BISPS), where the Berhu regularization can improve the Lasso in detection and estimation. The algorithm is extremely easy to implement, efﬁcient in computation and has a theoretical guarantee to converge to a global optimum. We also incorporate a screening technique into BISPS to tackle ultra-high dimensional problems and enhance computational efﬁciency. Furthermore, a stationary bootstrap technique is applied to provide connection occurring frequency for reliable topology learning. Experiments show that our method can achieve stationary and sparse causality network learning and is scalable for high-dimensional problems. Keywords: stationarity, sparsity, Berhu, screening, bootstrap</p><p>4 0.44022065 <a title="111-lsi-4" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>Author: Jun Wang, Tony Jebara, Shih-Fu Chang</p><p>Abstract: Graph-based semi-supervised learning (SSL) methods play an increasingly important role in practical machine learning systems, particularly in agnostic settings when no parametric information or other prior knowledge is available about the data distribution. Given the constructed graph represented by a weight matrix, transductive inference is used to propagate known labels to predict the values of all unlabeled vertices. Designing a robust label diffusion algorithm for such graphs is a widely studied problem and various methods have recently been suggested. Many of these can be formalized as regularized function estimation through the minimization of a quadratic cost. However, most existing label diffusion methods minimize a univariate cost with the classiﬁcation function as the only variable of interest. Since the observed labels seed the diffusion process, such univariate frameworks are extremely sensitive to the initial label choice and any label noise. To alleviate the dependency on the initial observed labels, this article proposes a bivariate formulation for graph-based SSL, where both the binary label information and a continuous classiﬁcation function are arguments of the optimization. This bivariate formulation is shown to be equivalent to a linearly constrained Max-Cut problem. Finally an efﬁcient solution via greedy gradient Max-Cut (GGMC) is derived which gradually assigns unlabeled vertices to each class with minimum connectivity. Once convergence guarantees are established, this greedy Max-Cut based SSL is applied on both artiﬁcial and standard benchmark data sets where it obtains superior classiﬁcation accuracy compared to existing state-of-the-art SSL methods. Moreover, GGMC shows robustness with respect to the graph construction method and maintains high accuracy over extensive experiments with various edge linking and weighting schemes. Keywords: graph transduction, semi-supervised learning, bivariate formulation, mixed integer programming, greedy</p><p>5 0.40941113 <a title="111-lsi-5" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>Author: Lorenzo Rosasco, Silvia Villa, Sofia Mosci, Matteo Santoro, Alessandro Verri</p><p>Abstract: In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose a new notion of nonparametric sparsity and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art methods. Keywords: sparsity, nonparametric, variable selection, regularization, proximal methods, RKHS ∗. Also at Istituto Italiano di Tecnologia, Via Morego 30, 16163 Genova, Italy and Massachusetts Institute of Technology, Bldg. 46-5155, 77 Massachusetts Avenue, Cambridge, MA 02139, USA. c 2013 Lorenzo Rosasco, Silvia Villa, Soﬁa Mosci, Matteo Santoro and Alessandro Verri. ROSASCO , V ILLA , M OSCI , S ANTORO AND V ERRI</p><p>6 0.38010243 <a title="111-lsi-6" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>7 0.36926031 <a title="111-lsi-7" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>8 0.36876443 <a title="111-lsi-8" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>9 0.36220184 <a title="111-lsi-9" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>10 0.35778069 <a title="111-lsi-10" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<p>11 0.31197971 <a title="111-lsi-11" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>12 0.28384665 <a title="111-lsi-12" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>13 0.28379941 <a title="111-lsi-13" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>14 0.27594191 <a title="111-lsi-14" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>15 0.26392975 <a title="111-lsi-15" href="./jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion.html">110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</a></p>
<p>16 0.25732225 <a title="111-lsi-16" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>17 0.25455138 <a title="111-lsi-17" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>18 0.2495027 <a title="111-lsi-18" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>19 0.23812543 <a title="111-lsi-19" href="./jmlr-2013-Truncated_Power_Method_for_Sparse_Eigenvalue_Problems.html">116 jmlr-2013-Truncated Power Method for Sparse Eigenvalue Problems</a></p>
<p>20 0.23800206 <a title="111-lsi-20" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.024), (5, 0.132), (6, 0.065), (10, 0.051), (20, 0.018), (23, 0.042), (68, 0.484), (70, 0.016), (75, 0.035), (85, 0.014), (87, 0.026), (89, 0.01), (93, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93259937 <a title="111-lda-1" href="./jmlr-2013-Algorithms_for_Discovery_of_Multiple_Markov_Boundaries.html">11 jmlr-2013-Algorithms for Discovery of Multiple Markov Boundaries</a></p>
<p>Author: Alexander Statnikov, Nikita I. Lytkin, Jan Lemeire, Constantin F. Aliferis</p><p>Abstract: Algorithms for Markov boundary discovery from data constitute an important recent development in machine learning, primarily because they offer a principled solution to the variable/feature selection problem and give insight on local causal structure. Over the last decade many sound algorithms have been proposed to identify a single Markov boundary of the response variable. Even though faithful distributions and, more broadly, distributions that satisfy the intersection property always have a single Markov boundary, other distributions/data sets may have multiple Markov boundaries of the response variable. The latter distributions/data sets are common in practical data-analytic applications, and there are several reasons why it is important to induce multiple Markov boundaries from such data. However, there are currently no sound and efﬁcient algorithms that can accomplish this task. This paper describes a family of algorithms TIE* that can discover all Markov boundaries in a distribution. The broad applicability as well as efﬁciency of the new algorithmic family is demonstrated in an extensive benchmarking study that involved comparison with 26 state-of-the-art algorithms/variants in 15 data sets from a diversity of application domains. Keywords: Markov boundary discovery, variable/feature selection, information equivalence, violations of faithfulness</p><p>same-paper 2 0.87574369 <a title="111-lda-2" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>Author: Julien Mairal, Bin Yu</p><p>Abstract: We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called “path coding” penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efﬁciently solve by leveraging network ﬂow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs. Keywords: convex and non-convex optimization, network ﬂow optimization, graph sparsity</p><p>3 0.54047662 <a title="111-lda-3" href="./jmlr-2013-Consistent_Selection_of_Tuning_Parameters_via_Variable_Selection_Stability.html">27 jmlr-2013-Consistent Selection of Tuning Parameters via Variable Selection Stability</a></p>
<p>Author: Wei Sun, Junhui Wang, Yixin Fang</p><p>Abstract: Penalized regression models are popularly used in high-dimensional data analysis to conduct variable selection and model ﬁtting simultaneously. Whereas success has been widely reported in literature, their performances largely depend on the tuning parameters that balance the trade-off between model ﬁtting and model sparsity. Existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability, such as cross validation, AIC and BIC. This article introduces a general tuning parameter selection criterion based on variable selection stability. The key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection. The asymptotic selection consistency is established for both ﬁxed and diverging dimensions. Its effectiveness is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data. Keywords: kappa coefﬁcient, penalized regression, selection consistency, stability, tuning</p><p>4 0.52009797 <a title="111-lda-4" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>Author: Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer</p><p>Abstract: Randomized controlled experiments are often described as the most reliable tool available to scientists for discovering causal relationships among quantities of interest. However, it is often unclear how many and which different experiments are needed to identify the full (possibly cyclic) causal structure among some given (possibly causally insufﬁcient) set of variables. Recent results in the causal discovery literature have explored various identiﬁability criteria that depend on the assumptions one is able to make about the underlying causal process, but these criteria are not directly constructive for selecting the optimal set of experiments. Fortunately, many of the needed constructions already exist in the combinatorics literature, albeit under terminology which is unfamiliar to most of the causal discovery community. In this paper we translate the theoretical results and apply them to the concrete problem of experiment selection. For a variety of settings we give explicit constructions of the optimal set of experiments and adapt some of the general combinatorics results to answer questions relating to the problem of experiment selection. Keywords: causality, randomized experiments, experiment selection, separating systems, completely separating systems, cut-coverings</p><p>5 0.49571368 <a title="111-lda-5" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>Author: Lorenzo Rosasco, Silvia Villa, Sofia Mosci, Matteo Santoro, Alessandro Verri</p><p>Abstract: In this work we are interested in the problems of supervised learning and variable selection when the input-output dependence is described by a nonlinear function depending on a few variables. Our goal is to consider a sparse nonparametric model, hence avoiding linear or additive models. The key idea is to measure the importance of each variable in the model by making use of partial derivatives. Based on this intuition we propose a new notion of nonparametric sparsity and a corresponding least squares regularization scheme. Using concepts and results from the theory of reproducing kernel Hilbert spaces and proximal methods, we show that the proposed learning algorithm corresponds to a minimization problem which can be provably solved by an iterative procedure. The consistency properties of the obtained estimator are studied both in terms of prediction and selection performance. An extensive empirical analysis shows that the proposed method performs favorably with respect to the state-of-the-art methods. Keywords: sparsity, nonparametric, variable selection, regularization, proximal methods, RKHS ∗. Also at Istituto Italiano di Tecnologia, Via Morego 30, 16163 Genova, Italy and Massachusetts Institute of Technology, Bldg. 46-5155, 77 Massachusetts Avenue, Cambridge, MA 02139, USA. c 2013 Lorenzo Rosasco, Silvia Villa, Soﬁa Mosci, Matteo Santoro and Alessandro Verri. ROSASCO , V ILLA , M OSCI , S ANTORO AND V ERRI</p><p>6 0.48571372 <a title="111-lda-6" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>7 0.47863653 <a title="111-lda-7" href="./jmlr-2013-Fast_Generalized_Subset_Scan_for_Anomalous_Pattern_Detection.html">42 jmlr-2013-Fast Generalized Subset Scan for Anomalous Pattern Detection</a></p>
<p>8 0.47349718 <a title="111-lda-8" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>9 0.4597317 <a title="111-lda-9" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>10 0.45642638 <a title="111-lda-10" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>11 0.45571837 <a title="111-lda-11" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<p>12 0.45417833 <a title="111-lda-12" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>13 0.45401654 <a title="111-lda-13" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>14 0.45206961 <a title="111-lda-14" href="./jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion.html">110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</a></p>
<p>15 0.44987878 <a title="111-lda-15" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>16 0.44827491 <a title="111-lda-16" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>17 0.44711864 <a title="111-lda-17" href="./jmlr-2013-Convex_and_Scalable_Weakly_Labeled_SVMs.html">29 jmlr-2013-Convex and Scalable Weakly Labeled SVMs</a></p>
<p>18 0.44551313 <a title="111-lda-18" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>19 0.44445205 <a title="111-lda-19" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>20 0.44396979 <a title="111-lda-20" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
