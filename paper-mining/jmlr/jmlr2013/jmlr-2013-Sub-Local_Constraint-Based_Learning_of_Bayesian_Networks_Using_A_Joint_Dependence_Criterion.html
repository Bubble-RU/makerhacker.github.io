<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-110" href="#">jmlr2013-110</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</h1>
<br/><p>Source: <a title="jmlr-2013-110-pdf" href="http://jmlr.org/papers/volume14/mahdi13a/mahdi13a.pdf">pdf</a></p><p>Author: Rami Mahdi, Jason Mezey</p><p>Abstract: Constraint-based learning of Bayesian networks (BN) from limited data can lead to multiple testing problems when recovering dense areas of the skeleton and to conﬂicting results in the orientation of edges. In this paper, we present a new constraint-based algorithm, light mutual min (LMM) for improved accuracy of BN learning from small sample data. LMM improves the assessment of candidate edges by using a ranking criterion that considers conditional independence on neighboring variables at both sides of an edge simultaneously. The algorithm also employs an adaptive relaxation of constraints that, selectively, allows some nodes not to condition on some neighbors. This relaxation aims at reducing the incorrect rejection of true edges connecting high degree nodes due to multiple testing. LMM additionally incorporates a new criterion for ranking v-structures that is used to recover the completed partially directed acyclic graph (CPDAG) and to resolve conﬂicting v-structures, a common problem in small sample constraint-based learning. Using simulated data, each of these components of LMM is shown to signiﬁcantly improve network inference compared to commonly applied methods when learning from limited data, including more accurate recovery of skeletons and CPDAGs compared to the PC, MaxMin, and MaxMin hill climbing algorithms. A proof of asymptotic correctness is also provided for LMM for recovering the correct skeleton and CPDAG. Keywords: Bayesian networks, skeleton, constraint-based learning, mutual min</p><p>Reference: <a title="jmlr-2013-110-reference" href="../jmlr2013_reference/jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 LMM improves the assessment of candidate edges by using a ranking criterion that considers conditional independence on neighboring variables at both sides of an edge simultaneously. [sent-5, score-0.572]
</p><p>2 , 2002), ﬁrst recover the skeleton of the network and edges are oriented afterward. [sent-25, score-0.637]
</p><p>3 As will be discussed in this paper, when learning from small sample data, the use of conditional independence testing can lead to multiple testing problems that deteriorate the accuracy of skeleton recovery and to conﬂicting results in the orientation of edges. [sent-37, score-0.539]
</p><p>4 For skeleton recovery, LMM improves the assessment of candidate edges by using a new mutual dependence criterion that considers conditional independence on subsets of neighboring variables at both sides of an edge simultaneously. [sent-41, score-0.83]
</p><p>5 The name light mutual min is motivated by the fact that the algorithm is fast and uses reduced sets of neighbors (light) in addition to ranking edges using a measure that combines the minimal dependence from both sides of an edge (mutual min). [sent-46, score-0.528]
</p><p>6 , p} and a set of edges E ⊆ V ×V where an edge (i, j) ∈ E implies that E(i, j) is an edge pointing from Vi (parent) to V j (child). [sent-73, score-0.434]
</p><p>7 1565  M AHDI AND M EZEY  Deﬁnition 3 (Skeleton) The skeleton of a directed graph G = (V, E) is a graph that contains all the nodes and edges of G such that all the edges have no directions. [sent-78, score-0.968]
</p><p>8 ⊥y Deﬁnition 8 (Blocked Path) In a directed graph G = (V, E), a path Pa of connected edges between two distinct nodes x, y ∈ V is said to be blocked by a set of nodes Z ⊆ V\x,y if one of the following holds: 1. [sent-99, score-0.466]
</p><p>9 A partially directed acyclic graph (PDAG) is a graph where some edges are directed and some are undirected. [sent-133, score-0.442]
</p><p>10 For every node Vi , the conditional independences are tested along the existing edges by conditioning on all subsets of the current neighbors and edges are deleted whenever dependences are found to be insigniﬁcant. [sent-145, score-0.796]
</p><p>11 The extended version of the PC algorithm uses the recovered skeleton to recover the CPDAG of the equivalence class by identifying v-structures and using an additional set of CPDAG orientation rules (Meek, 1995). [sent-146, score-0.394]
</p><p>12 In addition, a post-processing step is performed in which edges are deleted if the dependence between two nodes does not appear to be signiﬁcant from both sides simultaneously. [sent-151, score-0.423]
</p><p>13 The extended version of the algorithm, MaxMin hill climbing (MMHC), is a hybrid algorithm that uses a score-based search constrained by the recovered skeleton to recover the CPDAG of the equivalence class. [sent-152, score-0.421]
</p><p>14 Difﬁculties when Learning from Small Sample Data Using independence testing statistics to learn a BN from small sample data gives rise to a number of issues that can deteriorate the accuracy of both the recovery of the skeleton and the orientation of the edges. [sent-154, score-0.468]
</p><p>15 1 Unused Conditional Independence Testing Information In local constraint-based learning algorithms, an edge Exy is usually excluded from the skeleton if either node, x or y, ﬁnds at least one subset of their neighboring variables to induce complete conditional independence between x and y. [sent-158, score-0.441]
</p><p>16 Although this approach is sufﬁcient to recover the correct skeleton in the inﬁnite sample case, its use in learning from limited sample data ignores information about how probable we are to be correct in rejecting the conditional independence hypothesis with the lower conﬁdence. [sent-161, score-0.522]
</p><p>17 Therefore, to improve the accuracy of constraint-based learning, candidate edges should be ranked based on a joint conﬁdence criterion that combines the outcome of conditional independence tests at both sides of the edge simultaneously as complementary sources of information. [sent-168, score-0.593]
</p><p>18 Multiple Testing: In order to recover a correct edge Exy , the number of conditional independence tests that must be correctly rejected grows fast with the number of current neighbors (CPx or CPy ). [sent-174, score-0.386]
</p><p>19 First, candidate edges are ranked using a new measure that combines independence tests when conditioning on all subsets of neighboring variables of the ﬁrst and second node simultaneously. [sent-197, score-0.546]
</p><p>20 In Section A of the Appendix, ranking edges using Equation (4) is shown to be sufﬁcient for recovering the correct skeleton in the asymptotic limit. [sent-226, score-0.668]
</p><p>21 This is equivalent to ranking edges using the minimum of the two one-sided conditional posteriors in the right hand side of Equation (4) and ignoring how larger than the minimum the other one-sided conditional posterior was in comparison. [sent-230, score-0.421]
</p><p>22 In Section B of the Appendix, we elaborate on the effect of overlapping neighbor sets on ranking edges and use examples to illustrate why the overlap of neighbor sets is not likely to have a signiﬁcant negative effect on the accuracy of ranking edges. [sent-234, score-0.492]
</p><p>23 Afterward, candidate edges connecting to the identiﬁed node become candidates for relaxation of constraints. [sent-260, score-0.395]
</p><p>24 This relaxation is performed whenever a new edge Ei j is added to the skeleton by selectively updating the neighbor sets of i and j using the following rules: 1572  S UB -L OCAL L EARNING OF BN U SING A J OINT D EPENDENCE C RITERION  1. [sent-261, score-0.41]
</p><p>25 The process of occasionally making only one of a connected pair of nodes aware of a recovered edge will result in a reduction in the number of tests used to evaluate edges connecting to the other node. [sent-268, score-0.543]
</p><p>26 Due to ranking edges by the product of the one-sided CPPDs from both sides of an edge, for an edge to be incorrectly selected by the algorithm, the dependence has to appear incorrectly signiﬁcant from both sides simultaneously. [sent-283, score-0.476]
</p><p>27 Assigning information about edges to the nodes that show higher one-sided CPPD serves to assign neighbor sets of minimal size in a way that provides a maximal mutual information 1573  M AHDI AND M EZEY  with the nodes to which they are assigned. [sent-285, score-0.49]
</p><p>28 As a result, in the case of a large number of training samples, every node will become aware of all of its neighbors, and hence the algorithm still retains its correctness in recovering the true skeleton in the asymptotic limit (see Appendix for proof). [sent-290, score-0.41]
</p><p>29 1 gives an example that illustrates the behavior of the proposed relaxation of independence testing during skeleton recovery. [sent-294, score-0.397]
</p><p>30 In the forward selection, the algorithm incrementally adds new edges with the highest joint CPPD (Equation 4) until a certain number of edges are added or a threshold is reached. [sent-298, score-0.643]
</p><p>31 In the backward elimination phase, the algorithm incrementally eliminates edges with the least joint CPPD. [sent-299, score-0.388]
</p><p>32 As the forward selection phase of LMM proceeds, the joint CPPD (Equation 4) along a previously recovered edge Ei j might become less signiﬁcant as more edges are added due to the recovery of new neighbors, such as common parent variables that explain the observed dependence between i and j. [sent-302, score-0.652]
</p><p>33 However, as more edges are added that connect to i, j might become the node with the higher one-sided CPPD and hence, j should be made aware of the edge (CPj ← CPj i). [sent-307, score-0.449]
</p><p>34 In the case of applying OPT1-3, to avoid closed loops of adding and deleting the same edges or updating the same sets of neighbors, in our implementation, edges added in the last ten iterations are not considered for either of OPT1, OPT2, or OPT3 operations. [sent-330, score-0.568]
</p><p>35 Figure 2 (a) shows a network of 13 edges recovered by LMM forward selection. [sent-336, score-0.44]
</p><p>36 Figure 2 (b) shows a network of 25 edges recovered by continuing the forward selection for the same network and the same run of LMM. [sent-341, score-0.491]
</p><p>37 On the other hand, = node O is only aware of three of its child variables and that does not have any effect of increasing the chances of recovering false edges among its child variables. [sent-347, score-0.577]
</p><p>38 Also, note that all the false edges recovered in Figure 2 (b) were only selected due to forcing LMM to recover more than 20 edges and the dependence along these edges is already blocked (e. [sent-348, score-1.039]
</p><p>39 2 R ANKING  OF  S KELETON E DGES  The LMM algorithm for skeleton recovery as given in Algorithm 1 can be used to recover skeletons in any of the following three approaches: 1. [sent-356, score-0.445]
</p><p>40 By specifying the number of edges to be added in the forward selection and the number of edges to remain after backward elimination (MaxSize > MinSize > 0, and γ = 0). [sent-359, score-0.685]
</p><p>41 By allowing the algorithm to add many edges in the forward selection (large MaxSize, and γ = 0) and letting the backward elimination eliminate all of them one at a time (MinSize = 0). [sent-361, score-0.401]
</p><p>42 The order at which edges are deleted in the backward elimination is then used to rank edges where the last deleted edges become the most signiﬁcant. [sent-365, score-0.981]
</p><p>43 Nonetheless, using LMM to provide a rank of edges (Approach 3) instead of recovering a single skeleton can be more useful, in that, it relieves users from providing a threshold or guessing the number of edges ahead of time. [sent-372, score-0.863]
</p><p>44 In addition, the rank of edges has a simple interpretation, where the top edges have the highest conﬁdence of being correct while the later edges have less conﬁdence of being correct. [sent-373, score-0.9]
</p><p>45 However, we should note that, ranking skeleton edges is not new to BN learning. [sent-377, score-0.567]
</p><p>46 For example, Tsamardinos and Brown (2008) ranked edges based on their p-values to control for false discovery rate in skeleton recovery. [sent-378, score-0.562]
</p><p>47 Therefore, although the node with the maximum connectivity is connected to q edges in the true DAG, during the skeleton recovery, it will only be aware of a fraction of them, leading to a reduced number of independence tests (i. [sent-393, score-0.736]
</p><p>48 Using this randomization setting, in the case of p variables, the expected number of neighbors CPi∗ for a node i can be estimated as: E(|CPi∗ |) = s × (p − 1), while the expected number of all edges can be estimated as: E(|G|) = s × (p − 1) × p/2. [sent-497, score-0.388]
</p><p>49 For comparison, the receiver operating characteristics curve (ROC) is used to evaluate the accuracy of skeleton recovery while the true positive rate (TPR) plots (TPR against number of retrieved edges) are used to evaluate the accuracy of CPDAG inference. [sent-521, score-0.412]
</p><p>50 5 × numbero f nodes) and the order at which edges were removed in the backward elimination was used to rank all selected edges (MinSize = 0). [sent-541, score-0.641]
</p><p>51 Figure 3, shows an averaged plot of the false positive rate (FPR) and the true positive rate (TPR) of the three algorithms in recovering the skeletons of four sets of the simulated networks where the number of edges is ∼100 and ∼400 while the number of observations is 30 and 100 samples. [sent-559, score-0.477]
</p><p>52 06  FPR  Figure 3: Average ROC of skeleton recovery of LMM, LMM-1, and LMM-2 when the number of true edges is ∼100, and ∼400 and the number of observations is: A) 30 and B) 100 samples. [sent-579, score-0.596]
</p><p>53 50  A) 30 Samples  LMM −1  100  200  400  100  200  400  # of True Edges  Figure 4: Bar plots of the auROC of skeleton recovery of LMM, LMM-1, and LMM-2 when the number of true edges is ∼100, ∼200, and ∼400 and the number of observations is: A) 30 B) 100 and C) 300 Samples. [sent-591, score-0.596]
</p><p>54 X-Axis is the correct number of edges in the true networks and Y-Axis is the area under the partial ROC (auROC) where FPR is constrained to [0, 0. [sent-592, score-0.393]
</p><p>55 03  FPR  Figure 5: Average ROC of skeleton recovery of LMM, PC, and MaxMin when the number of true edges is ∼100, and ∼400 and the number of observations is: A) 30 and B) 100 samples. [sent-622, score-0.596]
</p><p>56 55  (A) 30 Samples  PCAlg  100  200  400  100E  200  400  # of True Edges  Figure 6: Bar plots of the auROC of skeleton recovery of LMM, PC, and MaxMin when the number of true edges is ∼100, ∼200, and ∼400 and the number of observations is: A) 30 B) 100 and C) 300 samples. [sent-645, score-0.596]
</p><p>57 X-Axis is the correct number of edges in the true networks and Y-Axis is the area under the partial ROC (auROC) where FPR is constrained to [0, 0. [sent-646, score-0.393]
</p><p>58 Note that, here, we deﬁne the distance between two skeleton as the number of operations of adding and deleting edges that are needed to transform one skeleton into the other. [sent-662, score-0.768]
</p><p>59 X-Axis is the number of retrieved edges normalized by the true number of edges (X = Number of edges in the correct network). [sent-736, score-1.0]
</p><p>60 be common and a lot of edges are rejected leading to a small number of recovered edges, even when every node identiﬁed a relatively large number of candidate neighbors. [sent-738, score-0.426]
</p><p>61 We again note that in addition to this low computational cost, the algorithm offers an ease of use property where users can make the algorithm retrieve a ﬁxed number of edges or a ranked list of edges (see Section 6. [sent-747, score-0.568]
</p><p>62 Afterward, a plot of the true positive rate (TPR) versus the number of retrieved edges is generated for each network. [sent-753, score-0.384]
</p><p>63 In addition, comparing two methods using the curve of TPR versus the number of retrieved edges has the same semantic as a ROC curve where higher always means better, in that, at a ﬁxed number of retrieved edges (XAxis), higher TPR also means lowers FP and FN. [sent-755, score-0.768]
</p><p>64 The ﬁrst set is simulated so that every network had ∼100 edges while the second set is simulated so that every network had ∼300 edges. [sent-760, score-0.386]
</p><p>65 The ﬁgure shows the average TPR of each algorithm at different levels of recovery (number of retrieved edges) up to 30-60% more edges than the true number of edges. [sent-763, score-0.454]
</p><p>66 In the case of using MMHC for recovering network of ∼300 edges (Figure 8:B), we were not able to get BNLearn to recover CPDAGs with more than ∼10% edges more than the true CPDAG due to the deletion of edges performed by hill climbing search to maximize the score. [sent-764, score-1.074]
</p><p>67 3X  # of Retrieved Edges  Figure 8: Average TPR when recovering CPDAGs of networks of 100 nodes and A) ∼100 or B) ∼300 true edges when the number of samples is 100. [sent-772, score-0.449]
</p><p>68 X-Axis is the number of retrieved edges (X = Number of edges in the correct network). [sent-773, score-0.716]
</p><p>69 To illustrate the variance of recovery among all recovered networks, Figure 9 shows the bar plots for the TPR when recovering networks containing as many edges as 0. [sent-777, score-0.526]
</p><p>70 X-Axis is the number of retrieved edges (X = Number of edges in the correct network). [sent-794, score-0.716]
</p><p>71 Figure 10 shows the average TPR for every method for the recovery of the CPDAGs of every set while Figure 11 shows the corresponding bar plots for the TPR when the number of recovered edges is as many as 0. [sent-797, score-0.442]
</p><p>72 When the observational data are limited in size, LMM improves the assessment of candidate skeleton edges by ranking them based on an estimate of the 1591  M AHDI AND M EZEY  Optimal  LMM_EO  MMHC  (B) 500 Nodes  PCAlg  (C) 2500 Nodes  0. [sent-812, score-0.643]
</p><p>73 5X  # of Retrieved Edges  Figure 10: Average TPR when recovering CPDAGs of networks of A) 100, B) 500, and C) 2500 nodes with ∼200, ∼1,000, and ∼5,000 true edges respectively, when the number of samples is 100 (1st row) and 300 (2nd row). [sent-820, score-0.449]
</p><p>74 X-Axis is the number of retrieved edges (X = Number of edges in the correct network). [sent-821, score-0.716]
</p><p>75 This relaxation is only performed whenever asymmetric evidence of conditional dependence is found between a pair of connected variables, where the aim is to reduce the accidental rejection of true edges connecting high degree nodes due to multiple testing. [sent-826, score-0.462]
</p><p>76 7X  1X  # of Retrieved Edges  Figure 11: TPR of CPDAGs recovery of networks of size: A) 100, B) 500, and C) 2500 nodes with ∼200, ∼1,000, and ∼5,000 true edges when number of samples is 100 (1st row) and 300 (2nd row). [sent-850, score-0.466]
</p><p>77 X-Axis is the number of retrieved edges (X = Number of edges in the correct network). [sent-851, score-0.716]
</p><p>78 In addition, we proposed a new approach to recover v-structures in a given skeleton based on the conﬁdence about the dependence induced by the addition of the common neighbor to conditioning sets. [sent-854, score-0.456]
</p><p>79 Proof of Correctness In this section, we provide proofs of correctness of the proposed methods for the recovery of both the correct skeleton and CPDAG when given a very large number of observational samples (asymptotic limit). [sent-858, score-0.459]
</p><p>80 X-Axis is the number of retrieved edges (X = Number of edges in the correct network). [sent-870, score-0.716]
</p><p>81 1 Correctness of LMM for Skeleton Recovery The proof of LMM correctness for skeleton recovery relies on the symmetric dependence between parent and child variables in the asymptotic limit. [sent-884, score-0.481]
</p><p>82 f  ˆ Lemma 15 If GS is the set of all edges in the skeleton of G, and GS is the set of all edges recovered f ˆ in the forward selection phase of LMM(D, ω, γ), then GS ⊆ GS , ∀γ < 1. [sent-892, score-0.915]
</p><p>83 S  Lemma 16 If CPi∗ is the set of all parent and child variables of node i in the correct skeleton GS , and CPi is the set of neighbors of node i recovered in the forward selection phase of LMM(D, ω, γ), then ∀ω > 0, CPi∗ ⊆ CPi (assuming γ < 1 and ω is not inﬁnitesimal). [sent-896, score-0.669]
</p><p>84 ˆ Lemma 17 If GS is the set of all edges in the skeleton of the graph G, and GS is the set of all edges ˆ recovered by LMM(D, ω, γ), then GS = GS . [sent-904, score-0.905]
</p><p>85 Proof From Lemma 15 and 16, after forward selection, all true edges were correctly recovered ˆf (GS ⊆ GS ), and the neighbor set of every node contains all its parent variables (pa(i) ⊆ CPi , ∀i). [sent-905, score-0.565]
</p><p>86 1595  M AHDI AND M EZEY  However, it is also expected that the forward selection might have recovered some false edges ˆf E f alse = GS − GS . [sent-906, score-0.425]
</p><p>87 Based on these conclusions, once the backward elimination starts, all false edges will have a joint CPPD of zero while all true edges will have a joint CPPD of one and therefore all false edges will ˆ be eliminated and thus: GS = GS . [sent-913, score-1.059]
</p><p>88 For example, if two nodes x and y found the common neighbor w, who also happen to be the only neighbor of both x and y, to induce conditional independence with the highest statistical signiﬁcance at both sides of the edge, the joint ˆ ˆ conditional posterior in (4) becomes P(ρxy|w = 0|ρxy|w )2 . [sent-926, score-0.423]
</p><p>89 Under Assumptions B1-3, we now analytically compute the expected rank of two edges (Exy , Ei j ) in different circumstances where we assume two nodes to have full overlap of neighbors and the other two nodes to have no common neighbors. [sent-949, score-0.448]
</p><p>90 On the other hand, it is evident from Case 2 that Equation (4) might introduce bias in ranking false edges among each other in the presence of overlapping neighbor sets. [sent-969, score-0.424]
</p><p>91 This bias however is not enough to rank false edges higher than correct edges and this is the most crucial part of the learning algorithm. [sent-970, score-0.652]
</p><p>92 In both plots, the X-axis is the number of retrieved edges normalized by the correct number of edges. [sent-973, score-0.432]
</p><p>93 TP counts the number of recovered edges that are correct in presence and orientation. [sent-976, score-0.393]
</p><p>94 In the TPR plot, the FN becomes (1-TP) while FP becomes the number of retrieved edges minus TP, and both of these metrics can be easily observed in the plot as shown in Figure 13:B. [sent-979, score-0.384]
</p><p>95 In both plots, the X-Axis is the number of retrieved edges normalized by the number of correct edges. [sent-1014, score-0.432]
</p><p>96 3X  # of Retrieved Edges  Figure 14: Average SHD when recovering CPDAGs of networks of 100 nodes and A) ∼100 or B) ∼300 true edges when the number of samples is 100. [sent-1024, score-0.449]
</p><p>97 X-Axis is the number of retrieved edges (X = Number of edges in the correct network). [sent-1025, score-0.716]
</p><p>98 4X  # of Retrieved Edges  Figure 15: Average SHD when recovering CPDAGs of networks of A) 100, B) 500, and C) 2500 nodes with ∼200, ∼1,000, and ∼5,000 true edges respectively, when the number of samples is 100 (1st row) and 300 (2nd row). [sent-1055, score-0.449]
</p><p>99 X-Axis is the number of retrieved edges (X = Number of edges in the correct network). [sent-1056, score-0.716]
</p><p>100 X-Axis is the number of retrieved edges (X = Number of edges in the correct network). [sent-1067, score-0.716]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lmm', 0.482), ('cpi', 0.296), ('edges', 0.284), ('skeleton', 0.242), ('maxmin', 0.209), ('tpr', 0.205), ('cpj', 0.198), ('cppd', 0.195), ('exy', 0.168), ('gs', 0.151), ('ei', 0.15), ('shd', 0.141), ('cpdag', 0.124), ('cpx', 0.123), ('retrieved', 0.1), ('pc', 0.099), ('ahdi', 0.091), ('ependence', 0.091), ('ezey', 0.091), ('riterion', 0.091), ('independence', 0.089), ('cpdags', 0.086), ('tsamardinos', 0.082), ('edge', 0.075), ('cpy', 0.073), ('mmhc', 0.073), ('skeletons', 0.073), ('recovery', 0.07), ('oint', 0.065), ('ocal', 0.065), ('auroc', 0.064), ('neighbor', 0.063), ('recovered', 0.061), ('bn', 0.061), ('conditioning', 0.061), ('recover', 0.06), ('nodes', 0.058), ('child', 0.057), ('ub', 0.057), ('parent', 0.057), ('node', 0.056), ('icting', 0.055), ('fpr', 0.055), ('sing', 0.054), ('recovering', 0.053), ('observational', 0.051), ('xy', 0.051), ('network', 0.051), ('correct', 0.048), ('neighbors', 0.048), ('dag', 0.046), ('forward', 0.044), ('ranking', 0.041), ('spirtes', 0.039), ('elimination', 0.038), ('dsepg', 0.036), ('fp', 0.036), ('false', 0.036), ('testing', 0.036), ('backward', 0.035), ('conditional', 0.035), ('pcalg', 0.035), ('graph', 0.034), ('roc', 0.034), ('aware', 0.034), ('directed', 0.032), ('maxsize', 0.032), ('minsize', 0.032), ('climbing', 0.032), ('earning', 0.031), ('dags', 0.031), ('neapolitan', 0.031), ('kalisch', 0.031), ('networks', 0.031), ('orientation', 0.031), ('joint', 0.031), ('tests', 0.031), ('dependence', 0.03), ('partial', 0.03), ('relaxation', 0.03), ('deleted', 0.028), ('eo', 0.027), ('bar', 0.027), ('faithfulness', 0.027), ('mutual', 0.027), ('acyclic', 0.026), ('causal', 0.026), ('con', 0.026), ('posterior', 0.026), ('hill', 0.026), ('correctness', 0.025), ('candidate', 0.025), ('asymmetric', 0.025), ('resolve', 0.024), ('correlation', 0.024), ('aliferis', 0.023), ('samples', 0.023), ('cppds', 0.023), ('sides', 0.023), ('afterward', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="110-tfidf-1" href="./jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion.html">110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</a></p>
<p>Author: Rami Mahdi, Jason Mezey</p><p>Abstract: Constraint-based learning of Bayesian networks (BN) from limited data can lead to multiple testing problems when recovering dense areas of the skeleton and to conﬂicting results in the orientation of edges. In this paper, we present a new constraint-based algorithm, light mutual min (LMM) for improved accuracy of BN learning from small sample data. LMM improves the assessment of candidate edges by using a ranking criterion that considers conditional independence on neighboring variables at both sides of an edge simultaneously. The algorithm also employs an adaptive relaxation of constraints that, selectively, allows some nodes not to condition on some neighbors. This relaxation aims at reducing the incorrect rejection of true edges connecting high degree nodes due to multiple testing. LMM additionally incorporates a new criterion for ranking v-structures that is used to recover the completed partially directed acyclic graph (CPDAG) and to resolve conﬂicting v-structures, a common problem in small sample constraint-based learning. Using simulated data, each of these components of LMM is shown to signiﬁcantly improve network inference compared to commonly applied methods when learning from limited data, including more accurate recovery of skeletons and CPDAGs compared to the PC, MaxMin, and MaxMin hill climbing algorithms. A proof of asymptotic correctness is also provided for LMM for recovering the correct skeleton and CPDAG. Keywords: Bayesian networks, skeleton, constraint-based learning, mutual min</p><p>2 0.13585733 <a title="110-tfidf-2" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>Author: Naftali Harris, Mathias Drton</p><p>Abstract: The PC algorithm uses conditional independence tests for model selection in graphical modeling with acyclic directed graphs. In Gaussian models, tests of conditional independence are typically based on Pearson correlations, and high-dimensional consistency results have been obtained for the PC algorithm in this setting. Analyzing the error propagation from marginal to partial correlations, we prove that high-dimensional consistency carries over to a broader class of Gaussian copula or nonparanormal models when using rank-based measures of correlation. For graph sequences with bounded degree, our consistency result is as strong as prior Gaussian results. In simulations, the ‘Rank PC’ algorithm works as well as the ‘Pearson PC’ algorithm for normal data and considerably better for non-normal data, all the while incurring a negligible increase of computation time. While our interest is in the PC algorithm, the presented analysis of error propagation could be applied to other algorithms that test the vanishing of low-order partial correlations. Keywords: Gaussian copula, graphical model, model selection, multivariate normal distribution, nonparanormal distribution</p><p>3 0.060322795 <a title="110-tfidf-3" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>Author: Pekka Parviainen, Mikko Koivisto</p><p>Abstract: We consider the problem of ﬁnding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space 2n , to within a factor polynomial in the number of nodes n. In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: ﬁrst, the user may trade space against time; second, the proposed algorithms easily and efﬁciently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order P on the nodes, an optimal DAG compatible with P can be found in time and space roughly proportional to the number of ideals of P , which can be signiﬁcantly less than 2n . Considering sufﬁciently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders. Keywords: exact algorithm, parallelization, partial order, space-time tradeoff, structure learning</p><p>4 0.051587332 <a title="110-tfidf-4" href="./jmlr-2013-Algorithms_for_Discovery_of_Multiple_Markov_Boundaries.html">11 jmlr-2013-Algorithms for Discovery of Multiple Markov Boundaries</a></p>
<p>Author: Alexander Statnikov, Nikita I. Lytkin, Jan Lemeire, Constantin F. Aliferis</p><p>Abstract: Algorithms for Markov boundary discovery from data constitute an important recent development in machine learning, primarily because they offer a principled solution to the variable/feature selection problem and give insight on local causal structure. Over the last decade many sound algorithms have been proposed to identify a single Markov boundary of the response variable. Even though faithful distributions and, more broadly, distributions that satisfy the intersection property always have a single Markov boundary, other distributions/data sets may have multiple Markov boundaries of the response variable. The latter distributions/data sets are common in practical data-analytic applications, and there are several reasons why it is important to induce multiple Markov boundaries from such data. However, there are currently no sound and efﬁcient algorithms that can accomplish this task. This paper describes a family of algorithms TIE* that can discover all Markov boundaries in a distribution. The broad applicability as well as efﬁciency of the new algorithmic family is demonstrated in an extensive benchmarking study that involved comparison with 26 state-of-the-art algorithms/variants in 15 data sets from a diversity of application domains. Keywords: Markov boundary discovery, variable/feature selection, information equivalence, violations of faithfulness</p><p>5 0.051025957 <a title="110-tfidf-5" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>Author: Nicolò Cesa-Bianchi, Claudio Gentile, Fabio Vitale, Giovanni Zappella</p><p>Abstract: We investigate the problem of sequentially predicting the binary labels on the nodes of an arbitrary weighted graph. We show that, under a suitable parametrization of the problem, the optimal number of prediction mistakes can be characterized (up to logarithmic factors) by the cutsize of a random spanning tree of the graph. The cutsize is induced by the unknown adversarial labeling of the graph nodes. In deriving our characterization, we obtain a simple randomized algorithm achieving in expectation the optimal mistake bound on any polynomially connected weighted graph. Our algorithm draws a random spanning tree of the original graph and then predicts the nodes of this tree in constant expected amortized time and linear space. Experiments on real-world data sets show that our method compares well to both global (Perceptron) and local (label propagation) methods, while being generally faster in practice. Keywords: online learning, learning on graphs, graph prediction, random spanning trees</p><p>6 0.048259091 <a title="110-tfidf-6" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>7 0.044112284 <a title="110-tfidf-7" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>8 0.043127537 <a title="110-tfidf-8" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>9 0.037607133 <a title="110-tfidf-9" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>10 0.037211221 <a title="110-tfidf-10" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>11 0.032642175 <a title="110-tfidf-11" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>12 0.031645108 <a title="110-tfidf-12" href="./jmlr-2013-Ranking_Forests.html">95 jmlr-2013-Ranking Forests</a></p>
<p>13 0.030936658 <a title="110-tfidf-13" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>14 0.030865224 <a title="110-tfidf-14" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>15 0.02932041 <a title="110-tfidf-15" href="./jmlr-2013-Optimally_Fuzzy_Temporal_Memory.html">82 jmlr-2013-Optimally Fuzzy Temporal Memory</a></p>
<p>16 0.029207354 <a title="110-tfidf-16" href="./jmlr-2013-Segregating_Event_Streams_and_Noise_with_a_Markov_Renewal_Process_Model.html">98 jmlr-2013-Segregating Event Streams and Noise with a Markov Renewal Process Model</a></p>
<p>17 0.026795339 <a title="110-tfidf-17" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>18 0.025588751 <a title="110-tfidf-18" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>19 0.024635354 <a title="110-tfidf-19" href="./jmlr-2013-Counterfactual_Reasoning_and_Learning_Systems%3A_The_Example_of_Computational_Advertising.html">30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</a></p>
<p>20 0.023278426 <a title="110-tfidf-20" href="./jmlr-2013-Pairwise_Likelihood_Ratios_for_Estimation_of_Non-Gaussian_Structural_Equation_Models.html">85 jmlr-2013-Pairwise Likelihood Ratios for Estimation of Non-Gaussian Structural Equation Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.141), (1, 0.006), (2, -0.014), (3, 0.044), (4, 0.129), (5, 0.198), (6, 0.037), (7, 0.053), (8, -0.036), (9, 0.11), (10, -0.068), (11, -0.004), (12, -0.043), (13, -0.175), (14, -0.073), (15, 0.081), (16, 0.166), (17, -0.017), (18, 0.021), (19, -0.028), (20, 0.1), (21, 0.022), (22, -0.061), (23, 0.03), (24, 0.077), (25, 0.057), (26, -0.025), (27, -0.02), (28, 0.111), (29, -0.068), (30, -0.056), (31, -0.024), (32, -0.028), (33, -0.114), (34, -0.064), (35, 0.013), (36, 0.069), (37, 0.06), (38, -0.107), (39, 0.195), (40, 0.115), (41, -0.211), (42, -0.151), (43, 0.085), (44, 0.108), (45, 0.067), (46, -0.042), (47, -0.196), (48, -0.011), (49, -0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95310581 <a title="110-lsi-1" href="./jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion.html">110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</a></p>
<p>Author: Rami Mahdi, Jason Mezey</p><p>Abstract: Constraint-based learning of Bayesian networks (BN) from limited data can lead to multiple testing problems when recovering dense areas of the skeleton and to conﬂicting results in the orientation of edges. In this paper, we present a new constraint-based algorithm, light mutual min (LMM) for improved accuracy of BN learning from small sample data. LMM improves the assessment of candidate edges by using a ranking criterion that considers conditional independence on neighboring variables at both sides of an edge simultaneously. The algorithm also employs an adaptive relaxation of constraints that, selectively, allows some nodes not to condition on some neighbors. This relaxation aims at reducing the incorrect rejection of true edges connecting high degree nodes due to multiple testing. LMM additionally incorporates a new criterion for ranking v-structures that is used to recover the completed partially directed acyclic graph (CPDAG) and to resolve conﬂicting v-structures, a common problem in small sample constraint-based learning. Using simulated data, each of these components of LMM is shown to signiﬁcantly improve network inference compared to commonly applied methods when learning from limited data, including more accurate recovery of skeletons and CPDAGs compared to the PC, MaxMin, and MaxMin hill climbing algorithms. A proof of asymptotic correctness is also provided for LMM for recovering the correct skeleton and CPDAG. Keywords: Bayesian networks, skeleton, constraint-based learning, mutual min</p><p>2 0.56649762 <a title="110-lsi-2" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>Author: Naftali Harris, Mathias Drton</p><p>Abstract: The PC algorithm uses conditional independence tests for model selection in graphical modeling with acyclic directed graphs. In Gaussian models, tests of conditional independence are typically based on Pearson correlations, and high-dimensional consistency results have been obtained for the PC algorithm in this setting. Analyzing the error propagation from marginal to partial correlations, we prove that high-dimensional consistency carries over to a broader class of Gaussian copula or nonparanormal models when using rank-based measures of correlation. For graph sequences with bounded degree, our consistency result is as strong as prior Gaussian results. In simulations, the ‘Rank PC’ algorithm works as well as the ‘Pearson PC’ algorithm for normal data and considerably better for non-normal data, all the while incurring a negligible increase of computation time. While our interest is in the PC algorithm, the presented analysis of error propagation could be applied to other algorithms that test the vanishing of low-order partial correlations. Keywords: Gaussian copula, graphical model, model selection, multivariate normal distribution, nonparanormal distribution</p><p>3 0.53157622 <a title="110-lsi-3" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>Author: Pekka Parviainen, Mikko Koivisto</p><p>Abstract: We consider the problem of ﬁnding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space 2n , to within a factor polynomial in the number of nodes n. In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: ﬁrst, the user may trade space against time; second, the proposed algorithms easily and efﬁciently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order P on the nodes, an optimal DAG compatible with P can be found in time and space roughly proportional to the number of ideals of P , which can be signiﬁcantly less than 2n . Considering sufﬁciently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders. Keywords: exact algorithm, parallelization, partial order, space-time tradeoff, structure learning</p><p>4 0.38949436 <a title="110-lsi-4" href="./jmlr-2013-Algorithms_for_Discovery_of_Multiple_Markov_Boundaries.html">11 jmlr-2013-Algorithms for Discovery of Multiple Markov Boundaries</a></p>
<p>Author: Alexander Statnikov, Nikita I. Lytkin, Jan Lemeire, Constantin F. Aliferis</p><p>Abstract: Algorithms for Markov boundary discovery from data constitute an important recent development in machine learning, primarily because they offer a principled solution to the variable/feature selection problem and give insight on local causal structure. Over the last decade many sound algorithms have been proposed to identify a single Markov boundary of the response variable. Even though faithful distributions and, more broadly, distributions that satisfy the intersection property always have a single Markov boundary, other distributions/data sets may have multiple Markov boundaries of the response variable. The latter distributions/data sets are common in practical data-analytic applications, and there are several reasons why it is important to induce multiple Markov boundaries from such data. However, there are currently no sound and efﬁcient algorithms that can accomplish this task. This paper describes a family of algorithms TIE* that can discover all Markov boundaries in a distribution. The broad applicability as well as efﬁciency of the new algorithmic family is demonstrated in an extensive benchmarking study that involved comparison with 26 state-of-the-art algorithms/variants in 15 data sets from a diversity of application domains. Keywords: Markov boundary discovery, variable/feature selection, information equivalence, violations of faithfulness</p><p>5 0.38303789 <a title="110-lsi-5" href="./jmlr-2013-Efficient_Program_Synthesis_Using_Constraint_Satisfaction_in_Inductive_Logic_Programming.html">40 jmlr-2013-Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming</a></p>
<p>Author: John Ahlgren, Shiu Yin Yuen</p><p>Abstract: We present NrSample, a framework for program synthesis in inductive logic programming. NrSample uses propositional logic constraints to exclude undesirable candidates from the search. This is achieved by representing constraints as propositional formulae and solving the associated constraint satisfaction problem. We present a variety of such constraints: pruning, input-output, functional (arithmetic), and variable splitting. NrSample is also capable of detecting search space exhaustion, leading to further speedups in clause induction and optimality. We benchmark NrSample against enumeration search (Aleph’s default) and Progol’s A∗ search in the context of program synthesis. The results show that, on large program synthesis problems, NrSample induces between 1 and 1358 times faster than enumeration (236 times faster on average), always with similar or better accuracy. Compared to Progol A∗ , NrSample is 18 times faster on average with similar or better accuracy except for two problems: one in which Progol A∗ substantially sacriﬁced accuracy to induce faster, and one in which Progol A∗ was a clear winner. Functional constraints provide a speedup of up to 53 times (21 times on average) with similar or better accuracy. We also benchmark using a few concept learning (non-program synthesis) problems. The results indicate that without strong constraints, the overhead of solving constraints is not compensated for. Keywords: inductive logic programming, program synthesis, theory induction, constraint satisfaction, Boolean satisﬁability problem</p><p>6 0.35186043 <a title="110-lsi-6" href="./jmlr-2013-Using_Symmetry_and_Evolutionary_Search_to_Minimize_Sorting_Networks.html">118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</a></p>
<p>7 0.31655994 <a title="110-lsi-7" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>8 0.28853092 <a title="110-lsi-8" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>9 0.26450348 <a title="110-lsi-9" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>10 0.25957781 <a title="110-lsi-10" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>11 0.2295111 <a title="110-lsi-11" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<p>12 0.22027199 <a title="110-lsi-12" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>13 0.21665181 <a title="110-lsi-13" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>14 0.2152871 <a title="110-lsi-14" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>15 0.20531704 <a title="110-lsi-15" href="./jmlr-2013-Ranking_Forests.html">95 jmlr-2013-Ranking Forests</a></p>
<p>16 0.19648708 <a title="110-lsi-16" href="./jmlr-2013-Variable_Selection_in_High-Dimension_with_Random_Designs_and_Orthogonal_Matching_Pursuit.html">119 jmlr-2013-Variable Selection in High-Dimension with Random Designs and Orthogonal Matching Pursuit</a></p>
<p>17 0.18862808 <a title="110-lsi-17" href="./jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications.html">18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</a></p>
<p>18 0.18738557 <a title="110-lsi-18" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>19 0.1838973 <a title="110-lsi-19" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>20 0.18188922 <a title="110-lsi-20" href="./jmlr-2013-Fast_Generalized_Subset_Scan_for_Anomalous_Pattern_Detection.html">42 jmlr-2013-Fast Generalized Subset Scan for Anomalous Pattern Detection</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.053), (5, 0.147), (6, 0.052), (10, 0.057), (20, 0.014), (23, 0.017), (68, 0.042), (70, 0.022), (71, 0.41), (75, 0.025), (85, 0.017), (87, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.77043462 <a title="110-lda-1" href="./jmlr-2013-Universal_Consistency_of_Localized_Versions_of_Regularized_Kernel_Methods.html">117 jmlr-2013-Universal Consistency of Localized Versions of Regularized Kernel Methods</a></p>
<p>Author: Robert Hable</p><p>Abstract: In supervised learning problems, global and local learning algorithms are used. In contrast to global learning algorithms, the prediction of a local learning algorithm in a testing point is only based on training data which are close to the testing point. Every global algorithm such as support vector machines (SVM) can be localized in the following way: in every testing point, the (global) learning algorithm is not applied to the whole training data but only to the k nearest neighbors (kNN) of the testing point. In case of support vector machines, the success of such mixtures of SVM and kNN (called SVM-KNN) has been shown in extensive simulation studies and also for real data sets but only little has been known on theoretical properties so far. In the present article, it is shown how a large class of regularized kernel methods (including SVM) can be localized in order to get a universally consistent learning algorithm. Keywords: machine learning, regularized kernel methods, localization, SVM, k-nearest neighbors, SVM-KNN</p><p>same-paper 2 0.72515434 <a title="110-lda-2" href="./jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion.html">110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</a></p>
<p>Author: Rami Mahdi, Jason Mezey</p><p>Abstract: Constraint-based learning of Bayesian networks (BN) from limited data can lead to multiple testing problems when recovering dense areas of the skeleton and to conﬂicting results in the orientation of edges. In this paper, we present a new constraint-based algorithm, light mutual min (LMM) for improved accuracy of BN learning from small sample data. LMM improves the assessment of candidate edges by using a ranking criterion that considers conditional independence on neighboring variables at both sides of an edge simultaneously. The algorithm also employs an adaptive relaxation of constraints that, selectively, allows some nodes not to condition on some neighbors. This relaxation aims at reducing the incorrect rejection of true edges connecting high degree nodes due to multiple testing. LMM additionally incorporates a new criterion for ranking v-structures that is used to recover the completed partially directed acyclic graph (CPDAG) and to resolve conﬂicting v-structures, a common problem in small sample constraint-based learning. Using simulated data, each of these components of LMM is shown to signiﬁcantly improve network inference compared to commonly applied methods when learning from limited data, including more accurate recovery of skeletons and CPDAGs compared to the PC, MaxMin, and MaxMin hill climbing algorithms. A proof of asymptotic correctness is also provided for LMM for recovering the correct skeleton and CPDAG. Keywords: Bayesian networks, skeleton, constraint-based learning, mutual min</p><p>3 0.70255494 <a title="110-lda-3" href="./jmlr-2013-Similarity-based_Clustering_by_Left-Stochastic_Matrix_Factorization.html">100 jmlr-2013-Similarity-based Clustering by Left-Stochastic Matrix Factorization</a></p>
<p>Author: Raman Arora, Maya R. Gupta, Amol Kapila, Maryam Fazel</p><p>Abstract: For similarity-based clustering, we propose modeling the entries of a given similarity matrix as the inner products of the unknown cluster probabilities. To estimate the cluster probabilities from the given similarity matrix, we introduce a left-stochastic non-negative matrix factorization problem. A rotation-based algorithm is proposed for the matrix factorization. Conditions for unique matrix factorizations and clusterings are given, and an error bound is provided. The algorithm is particularly efﬁcient for the case of two clusters, which motivates a hierarchical variant for cases where the number of desired clusters is large. Experiments show that the proposed left-stochastic decomposition clustering model produces relatively high within-cluster similarity on most data sets and can match given class labels, and that the efﬁcient hierarchical variant performs surprisingly well. Keywords: clustering, non-negative matrix factorization, rotation, indeﬁnite kernel, similarity, completely positive</p><p>4 0.39160416 <a title="110-lda-4" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>Author: Yuchen Zhang, John C. Duchi, Martin J. Wainwright</p><p>Abstract: We analyze two communication-efﬁcient algorithms for distributed optimization in statistical settings involving large-scale data sets. The ﬁrst algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves √ mean-squared error (MSE) that decays as O (N −1 + (N/m)−2 ). Whenever m ≤ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O (N −1 + (N/m)−3 ), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O (N −1 + (N/m)−3/2 ), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efﬁciently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 108 samples and d ≈ 740,000 covariates. Keywords: distributed learning, stochastic optimization, averaging, subsampling</p><p>5 0.38612908 <a title="110-lda-5" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>Author: Nima Noorshams, Martin J. Wainwright</p><p>Abstract: The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP ﬁxed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefﬁcients via Monte Carlo techniques and damped updates. We prove that the SOSMP iterates converge to a δ-neighborhood of the unique BP ﬁxed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefﬁcients as a function of the desired approximation accuracy δ and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical ﬂow estimation. Keywords: graphical models, sum-product for continuous state spaces, low-complexity belief propagation, stochastic approximation, Monte Carlo methods, orthogonal basis expansion</p><p>6 0.38470921 <a title="110-lda-6" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>7 0.3838605 <a title="110-lda-7" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>8 0.38199136 <a title="110-lda-8" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>9 0.38169375 <a title="110-lda-9" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<p>10 0.38127795 <a title="110-lda-10" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>11 0.38071054 <a title="110-lda-11" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>12 0.37981495 <a title="110-lda-12" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>13 0.37854731 <a title="110-lda-13" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>14 0.37818241 <a title="110-lda-14" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>15 0.37674952 <a title="110-lda-15" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>16 0.37648439 <a title="110-lda-16" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>17 0.37643585 <a title="110-lda-17" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>18 0.3761712 <a title="110-lda-18" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>19 0.37564695 <a title="110-lda-19" href="./jmlr-2013-Stochastic_Dual_Coordinate_Ascent_Methods_for_Regularized_Loss_Minimization.html">107 jmlr-2013-Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization</a></p>
<p>20 0.37527159 <a title="110-lda-20" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
