<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-23" href="#">jmlr2013-23</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</h1>
<br/><p>Source: <a title="jmlr-2013-23-pdf" href="http://jmlr.org/papers/volume14/pan13a/pan13a.pdf">pdf</a></p><p>Author: Wei Pan, Xiaotong Shen, Binghui Liu</p><p>Abstract: Clustering analysis is widely used in many ﬁelds. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classiﬁcation and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classiﬁcation and regression, such as model selection criteria to select the number of clusters, a difﬁcult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method’s promising performance. Keywords: generalized degrees of freedom, grouping, K-means clustering, Lasso, penalized regression, truncated Lasso penalty (TLP)</p><p>Reference: <a title="jmlr-2013-23-reference" href="../jmlr2013_reference/jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classiﬁcation and regression. [sent-6, score-0.13]
</p><p>2 Here we formulate clustering as penalized regression with grouping pursuit. [sent-7, score-0.22]
</p><p>3 Keywords: generalized degrees of freedom, grouping, K-means clustering, Lasso, penalized regression, truncated Lasso penalty (TLP)  1. [sent-11, score-0.135]
</p><p>4 In the absence of a class label, clustering analysis is also called unsupervised learning, as opposed to supervised learning that includes classiﬁcation and regression. [sent-14, score-0.13]
</p><p>5 Accordingly, approaches to clustering analysis are typically quite different from supervised learning. [sent-15, score-0.13]
</p><p>6 In this paper we adopt a novel framework for clustering analysis by viewing it as a regression problem (Pelckmans et al. [sent-16, score-0.149]
</p><p>7 We call our proposed method as penalized regression-based clustering (PRclust). [sent-26, score-0.17]
</p><p>8 An advantage of regarding clustering as a regression problem is its uniﬁcation with regression, which in turn provides the opportunity to apply or modify many established results and techniques, such as model selection criteria, in regression to clustering. [sent-27, score-0.185]
</p><p>9 In particular, a notoriously difﬁcult model selection problem in clustering analysis is to determine the number of clusters; already numerous methods exist with new ones constantly emerging (Tibshirani et al. [sent-28, score-0.147]
</p><p>10 In clustering analysis, due to the data-adaptive nature of model searches in ﬁnding clusters, it is unclear what is, or how to estimate df. [sent-33, score-0.13]
</p><p>11 Again by formulating clustering as regression, we can adapt the use of GDF to our current context. [sent-36, score-0.13]
</p><p>12 In spite of many advantages of formulating clustering analysis as a penalized regression problem, there are some challenges in its implementation. [sent-38, score-0.189]
</p><p>13 In particular, with a desired non-smooth and non-convex penalty function, many existing algorithms for penalized regression are not suitable. [sent-39, score-0.141]
</p><p>14 Hence, complementary to the K-means, PRclust is a potentially useful clustering tool. [sent-44, score-0.13]
</p><p>15 Albeit not the focus here, we also propose GDF-based GCV as a general model selection criterion to determine the number of clusters in the above clustering approaches; a comparison with several existing methods demonstrates the promising performance of GCV. [sent-47, score-0.34]
</p><p>16 , xip )′ , we would like to conduct a cluster analysis; that is, we would like to identify group-memberships of the observations such that the withingroup similarity and between-group dissimilarity are as strong as possible. [sent-58, score-0.119]
</p><p>17 There are different ways of deﬁning a similarity/dissimilarity between two observations, leading to various clustering approaches. [sent-59, score-0.13]
</p><p>18 The main idea is that, for the purpose of clustering, we would like to strike a balance between minimizing the distance between the observations and their centroids and reducing the number of centroids via grouping some close centroids together. [sent-74, score-0.332]
</p><p>19 As pointed out by a reviewer, the general idea with a convex Lq -norm as the fusion penalty has appeared in the literature (Pelckmans et al. [sent-75, score-0.146]
</p><p>20 , Lasso) penalty for a small α ≤ τ, but imposes no further penalty for a large α > τ. [sent-84, score-0.164]
</p><p>21 Again, to alleviate the bias of the usual convex L2 -norm (or more generally, Lq -norm for q > 1) group penalty, we propose a novel and non-convex group penalty based on TLP, called group TLP or simply gTLP, deﬁned as gTLP(µi − µ j ; τ) = TLP(||µi − µ j ||2 ; τ). [sent-88, score-0.167]
</p><p>22 Since the objective function (2) is a sum of a differentiable and ˆ convex function and a convex penalty in µ and θ (while θ(m) is a known vector), the coordinate-wise descent algorithm will converge to its minimizer (Tseng, 2001). [sent-117, score-0.144]
</p><p>23 We form ˆ i j ’s: for any two observations xi and x j , if θi j = 0, they are declared to be in ˆ clusters based on θ the same cluster. [sent-124, score-0.232]
</p><p>24 We construct a graph G based on an adjacency matrix A = (ai j ) with elements ˆ ai j = I(θi j = 0); ﬁnding clusters is equivalent to ﬁnding connected subcomponents of G. [sent-125, score-0.226]
</p><p>25 In contrast, by specifying K, the number of clusters as the only tuning parameter, the K-means starts with some K initial centroids, then assigns each observation to a cluster before updating the centroid estimates. [sent-144, score-0.367]
</p><p>26 Hence, PRclust differs from the K-means in that PRclust does not explicitly assign an observation to any cluster; clustering is implicitly done after the convergence of the algorithm. [sent-145, score-0.13]
</p><p>27 For any threshold d, we can deﬁne an adjacency matrix A = (ai j ) with ai j = I(di j < d); as in PRclust, we can deﬁne any connected subcomponent based on A as a cluster, resulting in a clustering method called HTclust, which is named as a“connected components” algorithm in Ng et al. [sent-148, score-0.163]
</p><p>28 HTclust is related to agglomerative hierarchical clustering: the latter also starts with each observation being its own cluster, then sequentially merges two nearest clusters until only one cluster left. [sent-151, score-0.303]
</p><p>29 4 Selecting Tuning Parameters or Number of Clusters A bonus with the regression approach to clustering is the potential application of many existing model selection methods for regression or supervised learning to clustering. [sent-156, score-0.185]
</p><p>30 In our notation, GCV(df) =  p ˆ RSS ∑n ∑ (xik − µik )2 = i=1 k=1 , 2 2 (np − df) (np − df)  where df is the degrees of freedom used in estimating µi ’s. [sent-161, score-0.228]
</p><p>31 For our problem, a naive treatment is to take df = K p, the number of unknown parameters in µi ’s, which however does not take into account the data-adaptive nature in estimating µi ’s in clustering analysis. [sent-162, score-0.332]
</p><p>32 In Step 3, we apply the same clustering algorithm (e. [sent-192, score-0.13]
</p><p>33 The above method can be equally applied to the K-means method to select the number of clusters: we just need to apply the K-means with a ﬁxed number of clusters, say K, in Step 3, then use the cluster centroid of observation xi as its estimated mean µi ; other steps remain the same. [sent-196, score-0.18]
</p><p>34 As a comparison, we also apply the Jump statistic to select the number of clusters for the Kmeans (Sugar and James, 2003). [sent-198, score-0.217]
</p><p>35 Wang (2010) proposed a consistent estimator for the number of clusters based on clustering stability. [sent-201, score-0.323]
</p><p>36 It is based on an intuitive idea: with the correct number of clusters, the clustering results should be most stable. [sent-202, score-0.13]
</p><p>37 Case I: two convex clusters in two dimensions (Figure 1a). [sent-211, score-0.224]
</p><p>38 We consider two somewhat overlapping clusters with the same spherical shape, which is ideal for the K-means. [sent-212, score-0.208]
</p><p>39 Case II: two non-convex clusters in two dimensions (Figure 1b). [sent-216, score-0.193]
</p><p>40 There were 2 clusters as two nested circles (distorted with some added noises), each with 100 observations (see the upper-left 2 panel in Figure 3). [sent-218, score-0.215]
</p><p>41 Case VI: three clusters in 2 dimension with two spherically shaped clusters inside 3/4 of a perturbed circle (Figure 1c). [sent-271, score-0.444]
</p><p>42 For comparison, we also applied Gaussian mixturemodel based clustering as implemented in R package mclust (Fraley and Raftery, 2006); for each data set, we ﬁtted each of the 10 models corresponding to 10 different ways of parameterizing the mixture model, for K = 1, 2, . [sent-291, score-0.251]
</p><p>43 1873  PAN , S HEN AND L IU  Due to the conceptual similarity between our proposed PRclust and spectral clustering (Sclust), we also included the spectral clustering algorithm of Ng et al. [sent-295, score-0.324]
</p><p>44 Fourth, for j=1 a speciﬁed number of clusters k, we stack the k top eigen-vectors (corresponding to the k largest eigen-values) of L column-wise to form an n × k matrix, say Zk ; normalize each row of Zk to have a unit L2 -norm. [sent-303, score-0.193]
</p><p>45 To evaluate the performance of a clustering algorithm, we used the Rand index (Rand, 1971), adjusted Rand index (Hubert and Arabie, 1985) and Jaccard index (Jaccard, 1912), all measuring the agreement between estimated cluster memberships and the truth. [sent-313, score-0.279]
</p><p>46 2 Simulation Results Case I: For the K-means, we chose the number of clusters using Jump, CV1, CV2 and GCV statistics; for comparison, we also ﬁxed the number of clusters around its true value. [sent-316, score-0.386]
</p><p>47 Figure 2 shows how GDF and NDF changed with K, the number of clusters in the K-means algorithm, for the ﬁrst simulated data set. [sent-320, score-0.216]
</p><p>48 Since the two clusters were formed by observations drawn from two Normal distributions, as expected, the model-based clustering Mclust performed best. [sent-325, score-0.36]
</p><p>49 PRclust with GCV(GDF) selecting its tuning parameters performed well too: the average number of clusters is close to the truth K0 = 2; the corresponding clustering results had high degrees of agreement with the truth, as evidenced by the high indices. [sent-337, score-0.421]
</p><p>50 Table 1 also displays the frequencies of the number of clusters selected by GCV(GDF): for the overwhelming majority (98%), either the correct number of cluster K0 = 2 was selected, or a slightly larger K = 3 or 4 with very high agreement indices was chosen. [sent-338, score-0.325]
</p><p>51 (2002), HTclust is not robust to outliers: since an “outlier” (lower left corner in Figure 1a) was farthest away from any other observations, it formed its own cluster while all others formed another cluster when the threshold d was chosen to yield two clusters. [sent-428, score-0.194]
</p><p>52 Case II: Since each cluster was not spherically shaped, and more importantly, the two true cluster centroids completely overlapped with each other, the K-means would not work: it could not distinguish the two clusters. [sent-430, score-0.316]
</p><p>53 to the cluster and its assigning a cluster membership of an observation based on its distance to the centroids; since the two clusters share the same centroid in truth, the K-means cannot distinguish the two clusters. [sent-434, score-0.429]
</p><p>54 Note that, the cluster memberships in PRclust are determined by the estimates of θi j = µi − µ j ; due to the use of ˆ the ridge penalty with a ﬁxed λ1 = 1, we might have θi j = 0 but µi = µ j . [sent-437, score-0.209]
</p><p>55 ˆ ˆ Since HTclust assigned the cluster-memberships according to the pair-wise distances among the observations, not the nearest distance of an observation to the centroids as done in the K-means, it also performed well. [sent-438, score-0.122]
</p><p>56 Interestingly, an exception happened in four (out of 100) simulations: when Sclust ˆ could not correctly distinguish the two true clusters (with low agreement statistics) with K = 2, it ˆ = 1. [sent-441, score-0.228]
</p><p>57 The results here suggest that, although Sclust had a smaller GCV(GDF) statistic than that for K may perform well for non-convex clusters with an appropriately chosen γ (as selected by the method 1876  C LUSTER A NALYSIS WITH A N ON - CONVEX P ENALTY  b) PRclust2−gTLP, large λ1  1. [sent-442, score-0.193]
</p><p>58 Case IV seems to be challenging with partially overlapping spherically shaped clusters of smaller cluster sizes: the number of clusters could be under- or over-selected by various methods. [sent-571, score-0.556]
</p><p>59 In Case V, all performed perfectly except that the GCV(GDF) over-selected the number of the clusters in the K-means and the two spectral clustering methods. [sent-574, score-0.37]
</p><p>60 Note that the K-means implicitly assumes that all clusters share the same volume and spherical shape, and GCV also implicitly favors such clusters (with smaller within-cluster sum of squares, and thus a smaller GCV statistic). [sent-577, score-0.386]
</p><p>61 On the other hand, the K-means with CV1 or CV2 and the two spectral clustering methods seemed to under-select the number of clusters, leading to lower agreement statistics. [sent-674, score-0.211]
</p><p>62 For the K-means, we tried the number of clusters K = 1, 2, . [sent-802, score-0.193]
</p><p>63 ˆ For the K-means, in agreement with simulations, the Jump selected perhaps a too large K = 27, ˆ while GCV(GDF) selected K = 9 perhaps due to the non-spherical shapes of the true clusters (Table ˆ 5). [sent-819, score-0.26]
</p><p>64 Both the K-means with CV1 (or CV2) and Mclust selected K = 2 and yielded the same clustering ˆ results. [sent-820, score-0.13]
</p><p>65 In comparison, PRclust with 1880  C LUSTER A NALYSIS WITH A N ON - CONVEX P ENALTY  ˆ GCV(GDF) yielded K = 3 clusters with higher agreement indices than those of the K-means, Mclust ˆ and Sclust. [sent-822, score-0.228]
</p><p>66 HTclust selected K = 4 clusters with the agreement indices less than but close to those ˆ of PRclust. [sent-823, score-0.228]
</p><p>67 We also applied the K-means and Sclust with a ﬁxed K = 2 or 3, and took the subset of the tuning parameter values yielding 2 or 3 clusters for HTclust and PRclust. [sent-824, score-0.228]
</p><p>68 We applied PRclust2 to the earlier examples and obtained ˆ the following results: when all the clusters were convex, PRclust2 yielded results very similar to those of PRclust; otherwise, their results were different. [sent-837, score-0.193]
</p><p>69 PRclust2 forms clusters based on the (approximate) equality of µi ’s, while ˆ PRclust clusters two observations i and j together if their µi and µ j are close to each other, say, ˆ ˆ ||ˆ i − µ j ||2 < d0,i j , where the threshold d0,i j is possibly (i, j)-speciﬁc. [sent-842, score-0.408]
</p><p>70 Hence, PRclust2 seems to be µ ˆ more rigid and greedy in forming clusters than PRclust. [sent-843, score-0.193]
</p><p>71 More ˆ i j ’s obtained generally, one can apply the spectral clustering of Ng et al. [sent-848, score-0.162]
</p><p>72 (2005) proposed using a fusion penalty based on the Lq -norm with the objective function 1 n ∑ ||xi − µi ||2 + λ ∑ ||µi − µ j ||q , 2 2 i=1 i< j and proposed an efﬁcient quadratic convex programming-based computing method for q = 1. [sent-891, score-0.146]
</p><p>73 (2011) recognized the importance of using a group penalty with q > 1, and applied the Matlab CVX package (Grant and Boyd, 2011) to solve the general convex programming problem for the group Lasso penalty with q = 2 (Yuan and Lin, 2006). [sent-893, score-0.231]
</p><p>74 More importantly, overall the solution paths of all three PRclust-Lq were similar to each other, sharing the common feature that the estimated centroids were more and more biased towards the overall mean as the penalty parameter λ increased. [sent-901, score-0.175]
</p><p>75 (2011) treated PRclust-Lq as a hierarchical clustering tool; none of the authors discussed the choice of the number of clusters. [sent-933, score-0.143]
</p><p>76 The issue of an Lq -norm penalty in yielding possibly severely biased estimates is well known in penalized regression, which partially motivated the development of non-convex penalties such as TLP (Shen et al. [sent-934, score-0.141]
</p><p>77 (2011) has recognized the issue of the biased centroid estimates in PRclust-Lq and thus proposed a second stage to re-estimate the centroids after a clustering result is obtained. [sent-937, score-0.265]
</p><p>78 When we applied the GCV(GDF) to select the number of clusters for PRclust-Lq in simulation Case I, as expected, it performed poorly. [sent-939, score-0.26]
</p><p>79 For any d0 ≥ 0, similar to hierarchical clustering, we deﬁned an adjacency matrix A = (ai j ) with ai j = I(||ˆ i − µ j ||2 ≤ d0 ); any two observations xi and x j were assigned to the µ ˆ same cluster if ai j = 1. [sent-941, score-0.198]
</p><p>80 Similarly, Mclust does not perform well for non-convex clusters (Table 2), but may have advantages with overlapping and ellipsoidal clusters as for simulation Case I (Table 1) and the iris data (Table 5). [sent-961, score-0.476]
</p><p>81 (2002) in selecting the scale parameter γ, but the clustering result also critically depended on the speciﬁed k, the number of clusters, for which the GCV(GDF) might not perform well. [sent-966, score-0.13]
</p><p>82 More generally, model selection is related to kernel learning in spectral clustering (Bach and Jordan, 2006). [sent-968, score-0.179]
</p><p>83 It is currently an open problem whether the strengths of PRclust and spectral clustering can be combined. [sent-969, score-0.162]
</p><p>84 K-median clustering is closely related to partitioning-around-centroids (PAM) of Kaufman and Rousseeuw (1990), and is more robust to outliers than is the K-means. [sent-975, score-0.13]
</p><p>85 Alternatively, we can also relax the equal cluster volume assumption and use: 1 L(xi − µi ) = (xi − µi )′ (xi − µi )/σ2 , i 2 where observation-speciﬁc variances σ2 ’s have to be estimated through grouping pursuit, as for i observation-speciﬁc means/centroids µi ’s (Xie et al. [sent-979, score-0.128]
</p><p>86 Among others, it might provide a computationally more efﬁcient algorithm than the EM algorithm commonly adopted in mixture model-based clustering (Dempster et al. [sent-982, score-0.13]
</p><p>87 This is a special and simple approach to more general graphbased clustering (Xu and Wunsch, 2005); other more sophisticated approaches may be borrowed or adapted. [sent-986, score-0.13]
</p><p>88 We implemented a speciﬁc combination of PRclust and spectral clustering along with GCV(GDF) for model selection: we ﬁrst applied PRclust, then used its output as the input to spectral clustering, but it did not show improvement over PRclust. [sent-987, score-0.194]
</p><p>89 Other options exist; for example, as suggested by a reviewer, it might be more fruitful to replace the K-means in spectral clustering with PRclust. [sent-988, score-0.162]
</p><p>90 In principle, we may add a penalty into our objective function for variable selection (Pan and Shen, 2007), which again requires a fast method to select more tuning parameters and is worth future investigation. [sent-996, score-0.158]
</p><p>91 Perhaps the most interesting idea of our proposal is the view of regarding clustering analysis as a penalized regression problem, blurring the typical line drawn to distinguish clustering (or unsupervised learning) with regression and classiﬁcation (i. [sent-997, score-0.338]
</p><p>92 We prove the equivalence between HTclust and the single-linkage hierarchical clustering (SLHclust). [sent-1024, score-0.143]
</p><p>93 Both the HTclust and SL-Hclust form clusters sequentially and in a ﬁnite number of steps; we show that, in each step, the SL-Hclust gives the same clusters as those of HTclust if an appropriate threshold is chosen in the latter. [sent-1025, score-0.386]
</p><p>94 In the next step, the SL-Hclust combines observations according to whether their distances satisfy di j ≤ d(1) to form clusters; in HTclust, if we use a threshold d0 = d(1) + ε with a tiny ε > 0, then it results in the same clusters as those of the SL-Hclust. [sent-1031, score-0.229]
</p><p>95 If the clustering results of the two methods are the same in step k − 1 > 0 and if we use d0 = d(k) + ε in HTclust, then it leads to the same clusters as the SL-Hclust in step k. [sent-1032, score-0.323]
</p><p>96 Finding the number of clusters in a data set: An information theoretic approach. [sent-1227, score-0.193]
</p><p>97 Evaluation and comparison of gene clustering methods in microarray analysis. [sent-1235, score-0.13]
</p><p>98 Estimating the number of clusters in a data set via the gap statistic. [sent-1253, score-0.193]
</p><p>99 Consistent selection of the number of clusters via crossvalidation. [sent-1264, score-0.21]
</p><p>100 Penalized model-based clustering with cluster-speciﬁc diagonal covariance matrices and grouped variables. [sent-1276, score-0.13]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gdf', 0.517), ('prclust', 0.464), ('gcv', 0.435), ('htclust', 0.255), ('df', 0.202), ('clusters', 0.193), ('clustering', 0.13), ('mclust', 0.121), ('cluster', 0.097), ('sclust', 0.094), ('centroids', 0.093), ('tlp', 0.092), ('pan', 0.086), ('fixed', 0.084), ('penalty', 0.082), ('luster', 0.081), ('enalty', 0.069), ('freq', 0.062), ('rand', 0.059), ('jaccard', 0.057), ('arand', 0.054), ('hen', 0.054), ('nalysis', 0.048), ('gtlp', 0.047), ('nnn', 0.047), ('jump', 0.046), ('iu', 0.044), ('lasso', 0.043), ('centroid', 0.042), ('penalized', 0.04), ('jasa', 0.04), ('lq', 0.036), ('tuning', 0.035), ('shen', 0.035), ('agreement', 0.035), ('bic', 0.034), ('iris', 0.034), ('hocking', 0.034), ('lindsten', 0.034), ('ndf', 0.034), ('pelckmans', 0.034), ('fusion', 0.033), ('spectral', 0.032), ('grouping', 0.031), ('convex', 0.031), ('ik', 0.029), ('xik', 0.029), ('shaped', 0.029), ('spherically', 0.029), ('ng', 0.028), ('simulation', 0.028), ('tibshirani', 0.026), ('select', 0.024), ('simulated', 0.023), ('observations', 0.022), ('reviewer', 0.021), ('sugar', 0.02), ('regression', 0.019), ('penalties', 0.019), ('ye', 0.018), ('np', 0.018), ('group', 0.018), ('memberships', 0.017), ('minneapolis', 0.017), ('minnesota', 0.017), ('tep', 0.017), ('xi', 0.017), ('adjacency', 0.017), ('selection', 0.017), ('perhaps', 0.016), ('ai', 0.016), ('rss', 0.016), ('performed', 0.015), ('overlapping', 0.015), ('cv', 0.015), ('seemed', 0.014), ('cvx', 0.014), ('shrinkage', 0.014), ('distances', 0.014), ('ridge', 0.013), ('wu', 0.013), ('appl', 0.013), ('ban', 0.013), ('binghui', 0.013), ('eik', 0.013), ('elongated', 0.013), ('fraley', 0.013), ('gdfs', 0.013), ('hik', 0.013), ('mclachlan', 0.013), ('sepal', 0.013), ('subtype', 0.013), ('thalamuthu', 0.013), ('wunsch', 0.013), ('ellipsoidal', 0.013), ('lange', 0.013), ('nocedal', 0.013), ('degrees', 0.013), ('freedom', 0.013), ('hierarchical', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="23-tfidf-1" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>Author: Wei Pan, Xiaotong Shen, Binghui Liu</p><p>Abstract: Clustering analysis is widely used in many ﬁelds. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classiﬁcation and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classiﬁcation and regression, such as model selection criteria to select the number of clusters, a difﬁcult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method’s promising performance. Keywords: generalized degrees of freedom, grouping, K-means clustering, Lasso, penalized regression, truncated Lasso penalty (TLP)</p><p>2 0.12715061 <a title="23-tfidf-2" href="./jmlr-2013-Consistent_Selection_of_Tuning_Parameters_via_Variable_Selection_Stability.html">27 jmlr-2013-Consistent Selection of Tuning Parameters via Variable Selection Stability</a></p>
<p>Author: Wei Sun, Junhui Wang, Yixin Fang</p><p>Abstract: Penalized regression models are popularly used in high-dimensional data analysis to conduct variable selection and model ﬁtting simultaneously. Whereas success has been widely reported in literature, their performances largely depend on the tuning parameters that balance the trade-off between model ﬁtting and model sparsity. Existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability, such as cross validation, AIC and BIC. This article introduces a general tuning parameter selection criterion based on variable selection stability. The key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection. The asymptotic selection consistency is established for both ﬁxed and diverging dimensions. Its effectiveness is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data. Keywords: kappa coefﬁcient, penalized regression, selection consistency, stability, tuning</p><p>3 0.069955721 <a title="23-tfidf-3" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>Author: Gang Niu, Bo Dai, Lin Shang, Masashi Sugiyama</p><p>Abstract: The large volume principle proposed by Vladimir Vapnik, which advocates that hypotheses lying in an equivalence class with a larger volume are more preferable, is a useful alternative to the large margin principle. In this paper, we introduce a new discriminative clustering model based on the large volume principle called maximum volume clustering (MVC), and then propose two approximation schemes to solve this MVC model: A soft-label MVC method using sequential quadratic programming and a hard-label MVC method using semi-deﬁnite programming, respectively. The proposed MVC is theoretically advantageous for three reasons. The optimization involved in hardlabel MVC is convex, and under mild conditions, the optimization involved in soft-label MVC is akin to a convex one in terms of the resulting clusters. Secondly, the soft-label MVC method pos∗. A preliminary and shorter version has appeared in Proceedings of 14th International Conference on Artiﬁcial Intelligence and Statistics (Niu et al., 2011). The preliminary work was done when GN was studying at Department of Computer Science and Technology, Nanjing University, and BD was studying at Institute of Automation, Chinese Academy of Sciences. A Matlab implementation of maximum volume clustering is available from http://sugiyama-www.cs.titech.ac.jp/∼gang/software.html. c 2013 Gang Niu, Bo Dai, Lin Shang and Masashi Sugiyama. N IU , DAI , S HANG AND S UGIYAMA sesses a clustering error bound. Thirdly, MVC includes the optimization problems of a spectral clustering, two relaxed k-means clustering and an information-maximization clustering as special limit cases when its regularization parameter goes to inﬁnity. Experiments on several artiﬁcial and benchmark data sets demonstrate that the proposed MVC compares favorably with state-of-the-art clustering methods. Keywords: discriminative clustering, large volume principle, sequential quadratic programming, semi-deﬁnite programming, ﬁnite sample stability, clustering error</p><p>4 0.061953545 <a title="23-tfidf-4" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<p>Author: Lauren A. Hannah, David B. Dunson</p><p>Abstract: We propose a new, nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function. Convexity constraints are common in economics, statistics, operations research, ﬁnancial engineering and optimization, but there is currently no multivariate method that is stable and computationally feasible for more than a few thousand observations. We introduce convex adaptive partitioning (CAP), which creates a globally convex regression model from locally linear estimates ﬁt on adaptively selected covariate partitions. CAP is a computationally efﬁcient, consistent method for convex regression. We demonstrate empirical performance by comparing the performance of CAP to other shape-constrained and unconstrained regression methods for predicting weekly wages and value function approximation for pricing American basket options. Keywords: adaptive partitioning, convex regression, nonparametric regression, shape constraint, treed linear model</p><p>5 0.047724407 <a title="23-tfidf-5" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>Author: Tingni Sun, Cun-Hui Zhang</p><p>Abstract: We propose a new method of learning a sparse nonnegative-deﬁnite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm ﬁrst estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other ℓ1 regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the ℓ1 and spectrum norms of the target inverse matrix diverges to inﬁnity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso. Keywords: precision matrix, concentration matrix, inverse matrix, graphical model, scaled Lasso, linear regression, spectrum norm</p><p>6 0.0441061 <a title="23-tfidf-6" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>7 0.041869726 <a title="23-tfidf-7" href="./jmlr-2013-Similarity-based_Clustering_by_Left-Stochastic_Matrix_Factorization.html">100 jmlr-2013-Similarity-based Clustering by Left-Stochastic Matrix Factorization</a></p>
<p>8 0.041734129 <a title="23-tfidf-8" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>9 0.031737801 <a title="23-tfidf-9" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>10 0.028112732 <a title="23-tfidf-10" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>11 0.026245194 <a title="23-tfidf-11" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>12 0.025699686 <a title="23-tfidf-12" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>13 0.023806721 <a title="23-tfidf-13" href="./jmlr-2013-CODA%3A_High_Dimensional_Copula_Discriminant_Analysis.html">20 jmlr-2013-CODA: High Dimensional Copula Discriminant Analysis</a></p>
<p>14 0.023164377 <a title="23-tfidf-14" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>15 0.023029281 <a title="23-tfidf-15" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>16 0.022414865 <a title="23-tfidf-16" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>17 0.022014963 <a title="23-tfidf-17" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>18 0.02188007 <a title="23-tfidf-18" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>19 0.019791588 <a title="23-tfidf-19" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>20 0.018987719 <a title="23-tfidf-20" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.108), (1, 0.024), (2, 0.013), (3, -0.017), (4, 0.098), (5, -0.015), (6, -0.019), (7, -0.023), (8, 0.179), (9, -0.066), (10, 0.0), (11, -0.164), (12, -0.092), (13, 0.084), (14, -0.043), (15, -0.115), (16, 0.087), (17, 0.253), (18, -0.149), (19, 0.135), (20, -0.06), (21, 0.244), (22, 0.021), (23, -0.326), (24, 0.019), (25, 0.033), (26, 0.059), (27, 0.044), (28, -0.041), (29, 0.071), (30, 0.073), (31, 0.183), (32, 0.001), (33, 0.042), (34, -0.042), (35, 0.047), (36, 0.054), (37, 0.04), (38, -0.003), (39, -0.03), (40, 0.023), (41, -0.048), (42, -0.012), (43, 0.058), (44, -0.093), (45, -0.008), (46, -0.006), (47, -0.07), (48, 0.096), (49, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9572292 <a title="23-lsi-1" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>Author: Wei Pan, Xiaotong Shen, Binghui Liu</p><p>Abstract: Clustering analysis is widely used in many ﬁelds. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classiﬁcation and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classiﬁcation and regression, such as model selection criteria to select the number of clusters, a difﬁcult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method’s promising performance. Keywords: generalized degrees of freedom, grouping, K-means clustering, Lasso, penalized regression, truncated Lasso penalty (TLP)</p><p>2 0.64062893 <a title="23-lsi-2" href="./jmlr-2013-Consistent_Selection_of_Tuning_Parameters_via_Variable_Selection_Stability.html">27 jmlr-2013-Consistent Selection of Tuning Parameters via Variable Selection Stability</a></p>
<p>Author: Wei Sun, Junhui Wang, Yixin Fang</p><p>Abstract: Penalized regression models are popularly used in high-dimensional data analysis to conduct variable selection and model ﬁtting simultaneously. Whereas success has been widely reported in literature, their performances largely depend on the tuning parameters that balance the trade-off between model ﬁtting and model sparsity. Existing tuning criteria mainly follow the route of minimizing the estimated prediction error or maximizing the posterior model probability, such as cross validation, AIC and BIC. This article introduces a general tuning parameter selection criterion based on variable selection stability. The key idea is to select the tuning parameters so that the resultant penalized regression model is stable in variable selection. The asymptotic selection consistency is established for both ﬁxed and diverging dimensions. Its effectiveness is also demonstrated in a variety of simulated examples as well as an application to the prostate cancer data. Keywords: kappa coefﬁcient, penalized regression, selection consistency, stability, tuning</p><p>3 0.41808915 <a title="23-lsi-3" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<p>Author: Lauren A. Hannah, David B. Dunson</p><p>Abstract: We propose a new, nonparametric method for multivariate regression subject to convexity or concavity constraints on the response function. Convexity constraints are common in economics, statistics, operations research, ﬁnancial engineering and optimization, but there is currently no multivariate method that is stable and computationally feasible for more than a few thousand observations. We introduce convex adaptive partitioning (CAP), which creates a globally convex regression model from locally linear estimates ﬁt on adaptively selected covariate partitions. CAP is a computationally efﬁcient, consistent method for convex regression. We demonstrate empirical performance by comparing the performance of CAP to other shape-constrained and unconstrained regression methods for predicting weekly wages and value function approximation for pricing American basket options. Keywords: adaptive partitioning, convex regression, nonparametric regression, shape constraint, treed linear model</p><p>4 0.40907517 <a title="23-lsi-4" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>Author: Gang Niu, Bo Dai, Lin Shang, Masashi Sugiyama</p><p>Abstract: The large volume principle proposed by Vladimir Vapnik, which advocates that hypotheses lying in an equivalence class with a larger volume are more preferable, is a useful alternative to the large margin principle. In this paper, we introduce a new discriminative clustering model based on the large volume principle called maximum volume clustering (MVC), and then propose two approximation schemes to solve this MVC model: A soft-label MVC method using sequential quadratic programming and a hard-label MVC method using semi-deﬁnite programming, respectively. The proposed MVC is theoretically advantageous for three reasons. The optimization involved in hardlabel MVC is convex, and under mild conditions, the optimization involved in soft-label MVC is akin to a convex one in terms of the resulting clusters. Secondly, the soft-label MVC method pos∗. A preliminary and shorter version has appeared in Proceedings of 14th International Conference on Artiﬁcial Intelligence and Statistics (Niu et al., 2011). The preliminary work was done when GN was studying at Department of Computer Science and Technology, Nanjing University, and BD was studying at Institute of Automation, Chinese Academy of Sciences. A Matlab implementation of maximum volume clustering is available from http://sugiyama-www.cs.titech.ac.jp/∼gang/software.html. c 2013 Gang Niu, Bo Dai, Lin Shang and Masashi Sugiyama. N IU , DAI , S HANG AND S UGIYAMA sesses a clustering error bound. Thirdly, MVC includes the optimization problems of a spectral clustering, two relaxed k-means clustering and an information-maximization clustering as special limit cases when its regularization parameter goes to inﬁnity. Experiments on several artiﬁcial and benchmark data sets demonstrate that the proposed MVC compares favorably with state-of-the-art clustering methods. Keywords: discriminative clustering, large volume principle, sequential quadratic programming, semi-deﬁnite programming, ﬁnite sample stability, clustering error</p><p>5 0.39894879 <a title="23-lsi-5" href="./jmlr-2013-Similarity-based_Clustering_by_Left-Stochastic_Matrix_Factorization.html">100 jmlr-2013-Similarity-based Clustering by Left-Stochastic Matrix Factorization</a></p>
<p>Author: Raman Arora, Maya R. Gupta, Amol Kapila, Maryam Fazel</p><p>Abstract: For similarity-based clustering, we propose modeling the entries of a given similarity matrix as the inner products of the unknown cluster probabilities. To estimate the cluster probabilities from the given similarity matrix, we introduce a left-stochastic non-negative matrix factorization problem. A rotation-based algorithm is proposed for the matrix factorization. Conditions for unique matrix factorizations and clusterings are given, and an error bound is provided. The algorithm is particularly efﬁcient for the case of two clusters, which motivates a hierarchical variant for cases where the number of desired clusters is large. Experiments show that the proposed left-stochastic decomposition clustering model produces relatively high within-cluster similarity on most data sets and can match given class labels, and that the efﬁcient hierarchical variant performs surprisingly well. Keywords: clustering, non-negative matrix factorization, rotation, indeﬁnite kernel, similarity, completely positive</p><p>6 0.24165398 <a title="23-lsi-6" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>7 0.20431094 <a title="23-lsi-7" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>8 0.17871085 <a title="23-lsi-8" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>9 0.17843179 <a title="23-lsi-9" href="./jmlr-2013-Divvy%3A_Fast_and_Intuitive_Exploratory_Data_Analysis.html">37 jmlr-2013-Divvy: Fast and Intuitive Exploratory Data Analysis</a></p>
<p>10 0.17058262 <a title="23-lsi-10" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>11 0.15545651 <a title="23-lsi-11" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>12 0.15523124 <a title="23-lsi-12" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>13 0.14924102 <a title="23-lsi-13" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>14 0.14159876 <a title="23-lsi-14" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>15 0.13379386 <a title="23-lsi-15" href="./jmlr-2013-Derivative_Estimation_with_Local_Polynomial_Fitting.html">31 jmlr-2013-Derivative Estimation with Local Polynomial Fitting</a></p>
<p>16 0.13084449 <a title="23-lsi-16" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>17 0.13068128 <a title="23-lsi-17" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>18 0.12144239 <a title="23-lsi-18" href="./jmlr-2013-The_CAM_Software_for_Nonnegative_Blind_Source_Separation_in_R-Java.html">113 jmlr-2013-The CAM Software for Nonnegative Blind Source Separation in R-Java</a></p>
<p>19 0.120918 <a title="23-lsi-19" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>20 0.10920658 <a title="23-lsi-20" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.018), (5, 0.075), (6, 0.035), (10, 0.074), (20, 0.017), (23, 0.021), (41, 0.013), (68, 0.039), (70, 0.014), (75, 0.539), (85, 0.012), (87, 0.016), (89, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98203331 <a title="23-lda-1" href="./jmlr-2013-GPstuff%3A_Bayesian_Modeling_with_Gaussian_Processes.html">45 jmlr-2013-GPstuff: Bayesian Modeling with Gaussian Processes</a></p>
<p>Author: Jarno Vanhatalo, Jaakko Riihimäki, Jouni Hartikainen, Pasi Jylänki, Ville Tolvanen, Aki Vehtari</p><p>Abstract: The GPstuff toolbox is a versatile collection of Gaussian process models and computational tools required for Bayesian inference. The tools include, among others, various inference methods, sparse approximations and model assessment methods. Keywords: Gaussian process, Bayesian hierarchical model, nonparametric Bayes</p><p>2 0.95259547 <a title="23-lda-2" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>Author: Lisha Chen, Andreas Buja</p><p>Abstract: Multidimensional scaling (MDS) is the art of reconstructing pointsets (embeddings) from pairwise distance data, and as such it is at the basis of several approaches to nonlinear dimension reduction and manifold learning. At present, MDS lacks a unifying methodology as it consists of a discrete collection of proposals that differ in their optimization criteria, called “stress functions”. To correct this situation we propose (1) to embed many of the extant stress functions in a parametric family of stress functions, and (2) to replace the ad hoc choice among discrete proposals with a principled parameter selection method. This methodology yields the following beneﬁts and problem solutions: (a) It provides guidance in tailoring stress functions to a given data situation, responding to the fact that no single stress function dominates all others across all data situations; (b) the methodology enriches the supply of available stress functions; (c) it helps our understanding of stress functions by replacing the comparison of discrete proposals with a characterization of the effect of parameters on embeddings; (d) it builds a bridge to graph drawing, which is the related but not identical art of constructing embeddings from graphs. Keywords: multidimensional scaling, force-directed layout, cluster analysis, clustering strength, unsupervised learning, Box-Cox transformations</p><p>3 0.88537169 <a title="23-lda-3" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>Author: Philémon Brakel, Dirk Stroobandt, Benjamin Schrauwen</p><p>Abstract: Imputing missing values in high dimensional time-series is a difﬁcult problem. This paper presents a strategy for training energy-based graphical models for imputation directly, bypassing difﬁculties probabilistic approaches would face. The training strategy is inspired by recent work on optimization-based learning (Domke, 2012) and allows complex neural models with convolutional and recurrent structures to be trained for imputation tasks. In this work, we use this training strategy to derive learning rules for three substantially different neural architectures. Inference in these models is done by either truncated gradient descent or variational mean-ﬁeld iterations. In our experiments, we found that the training methods outperform the Contrastive Divergence learning algorithm. Moreover, the training methods can easily handle missing values in the training data itself during learning. We demonstrate the performance of this learning scheme and the three models we introduce on one artiﬁcial and two real-world data sets. Keywords: neural networks, energy-based models, time-series, missing values, optimization</p><p>same-paper 4 0.86070895 <a title="23-lda-4" href="./jmlr-2013-Cluster_Analysis%3A_Unsupervised_Learning_via_Supervised_Learning_with_a_Non-convex_Penalty.html">23 jmlr-2013-Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty</a></p>
<p>Author: Wei Pan, Xiaotong Shen, Binghui Liu</p><p>Abstract: Clustering analysis is widely used in many ﬁelds. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classiﬁcation and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classiﬁcation and regression, such as model selection criteria to select the number of clusters, a difﬁcult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method’s promising performance. Keywords: generalized degrees of freedom, grouping, K-means clustering, Lasso, penalized regression, truncated Lasso penalty (TLP)</p><p>5 0.79792142 <a title="23-lda-5" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>Author: Ran Gilad-Bachrach, Christopher J.C. Burges</p><p>Abstract: Typically, one approaches a supervised machine learning problem by writing down an objective function and ﬁnding a hypothesis that minimizes it. This is equivalent to ﬁnding the Maximum A Posteriori (MAP) hypothesis for a Boltzmann distribution. However, MAP is not a robust statistic. We present an alternative approach by deﬁning a median of the distribution, which we show is both more robust, and has good generalization guarantees. We present algorithms to approximate this median. One contribution of this work is an efﬁcient method for approximating the Tukey median. The Tukey median, which is often used for data visualization and outlier detection, is a special case of the family of medians we deﬁne: however, computing it exactly is exponentially slow in the dimension. Our algorithm approximates such medians in polynomial time while making weaker assumptions than those required by previous work. Keywords: classiﬁcation, estimation, median, Tukey depth</p><p>6 0.63201666 <a title="23-lda-6" href="./jmlr-2013-Nested_Expectation_Propagation_for_Gaussian_Process_Classification_with_a_Multinomial_Probit_Likelihood.html">75 jmlr-2013-Nested Expectation Propagation for Gaussian Process Classification with a Multinomial Probit Likelihood</a></p>
<p>7 0.57152802 <a title="23-lda-7" href="./jmlr-2013-A_Framework_for_Evaluating_Approximation_Methods_for_Gaussian_Process_Regression.html">3 jmlr-2013-A Framework for Evaluating Approximation Methods for Gaussian Process Regression</a></p>
<p>8 0.53349507 <a title="23-lda-8" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>9 0.51656598 <a title="23-lda-9" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>10 0.4876304 <a title="23-lda-10" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>11 0.47604856 <a title="23-lda-11" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>12 0.47555679 <a title="23-lda-12" href="./jmlr-2013-Using_Symmetry_and_Evolutionary_Search_to_Minimize_Sorting_Networks.html">118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</a></p>
<p>13 0.45921317 <a title="23-lda-13" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>14 0.45121768 <a title="23-lda-14" href="./jmlr-2013-Differential_Privacy_for_Functions_and_Functional_Data.html">32 jmlr-2013-Differential Privacy for Functions and Functional Data</a></p>
<p>15 0.44530424 <a title="23-lda-15" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>16 0.44164157 <a title="23-lda-16" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>17 0.4408716 <a title="23-lda-17" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>18 0.43981922 <a title="23-lda-18" href="./jmlr-2013-Large-scale_SVD_and_Manifold_Learning.html">59 jmlr-2013-Large-scale SVD and Manifold Learning</a></p>
<p>19 0.4370091 <a title="23-lda-19" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>20 0.42981181 <a title="23-lda-20" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
