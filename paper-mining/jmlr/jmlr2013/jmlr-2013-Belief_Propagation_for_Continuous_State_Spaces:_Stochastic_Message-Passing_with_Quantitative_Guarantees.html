<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-17" href="#">jmlr2013-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</h1>
<br/><p>Source: <a title="jmlr-2013-17-pdf" href="http://jmlr.org/papers/volume14/noorshams13a/noorshams13a.pdf">pdf</a></p><p>Author: Nima Noorshams, Martin J. Wainwright</p><p>Abstract: The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP ﬁxed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefﬁcients via Monte Carlo techniques and damped updates. We prove that the SOSMP iterates converge to a δ-neighborhood of the unique BP ﬁxed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefﬁcients as a function of the desired approximation accuracy δ and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical ﬂow estimation. Keywords: graphical models, sum-product for continuous state spaces, low-complexity belief propagation, stochastic approximation, Monte Carlo methods, orthogonal basis expansion</p><p>Reference: <a title="jmlr-2013-17-reference" href="../jmlr2013_reference/jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefﬁcients via Monte Carlo techniques and damped updates. [sent-8, score-0.312]
</p><p>2 We prove that the SOSMP iterates converge to a δ-neighborhood of the unique BP ﬁxed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. [sent-9, score-0.16]
</p><p>3 In addition, we demonstrate how to choose the number of basis coefﬁcients as a function of the desired approximation accuracy δ and smoothness of the compatibility functions. [sent-10, score-0.134]
</p><p>4 Keywords: graphical models, sum-product for continuous state spaces, low-complexity belief propagation, stochastic approximation, Monte Carlo methods, orthogonal basis expansion  1. [sent-12, score-0.286]
</p><p>5 The sum-product algorithm, also known as belief propagation (BP), is a form of dynamic programming that can be used to compute exact marginals much more efﬁciently for graphical models without cycles, known as trees. [sent-17, score-0.176]
</p><p>6 These techniques are typically based on approximating the messages as weighted particles (Doucet et al. [sent-41, score-0.12]
</p><p>7 Our general theory provides quantitative upper bounds on the number of iterations required to compute a δ-accurate approximation to the BP message ﬁxed point, as we illustrate in the case of kernel-based potential functions (Theorem 4). [sent-53, score-0.275]
</p><p>8 (1)  (u,v)∈E  u∈V  Here ψu : Xu → (0, ∞) is the node potential function, whereas ψuv : Xu × Xv → (0, ∞) denotes the edge potential function. [sent-76, score-0.213]
</p><p>9 , yn ) ∝  ∏ ψu (xu ; yu ) ∏ u∈V  ψuv (xu , xv ),  (3)  (u,v)∈E  where we have introduced the convenient shorthand for the modiﬁed node-wise potential functions ψu (xu ; yu ) := p(yu | xu ) ψu (xu ). [sent-98, score-0.259]
</p><p>10 Potential functions ψu and ψv are associated with nodes u and v respectively, whereas potential function ψuv is associated with edge (u, v). [sent-102, score-0.125]
</p><p>11 For each node v ∈ V , let N (v) := {u ∈ V | (u, v) ∈ E } be its set of neighbors, and we use E (v) := {(v → u) | u ∈ N (v)} to denote the set of all directed edges emanating from v. [sent-112, score-0.155]
</p><p>12 The messages passed by the BP algorithm are density functions, taking values in the space M . [sent-115, score-0.12]
</p><p>13 More precisely, we assign one message mv→u ∈ M to every directed edge (v → u) ∈ E , and we denote the 2802  S TOCHASTIC M ESSAGE -PASSING FOR C ONTINUOUS S TATE S PACES  collection of all messages by m = {mv→u , (v → u) ∈ E }. [sent-116, score-0.408]
</p><p>14 Note that the full collection of messages m takes values in the product space M |E | . [sent-117, score-0.12]
</p><p>15 At an abstract level, the BP algorithm generates a sequence of message densities {mt } in the space M |E | , where t = 0, 1, 2 . [sent-118, score-0.14]
</p><p>16 The update of message mt to message mt+1 can be written in the form mt+1 = F (mt ), where F : M |E | → M |E | is a non-linear operator. [sent-122, score-0.475]
</p><p>17 This global operator is deﬁned by the local update operators1 Fv→u : M |E | → M , one for each directed edge of the graph, such that mt+1 = Fv→u (mt ). [sent-123, score-0.21]
</p><p>18 v→u • it transmits message mt+1 to neighbor u ∈ N (v). [sent-129, score-0.14]
</p><p>19 v→u In more detail, the message update takes the form [Fv→u (mt )](·) := κ  X  ∏  ψuv (·, xv ) ψv (xv )  mtw→v (xv ) µ(dxv ),  (4)  w∈N (v)\{u}  mt+1 (·) v→u  where κ is a normalization constant chosen to enforce the normalization condition X  mt+1 (xu ) µ(dxu ) = 1. [sent-130, score-0.239]
</p><p>20 Given a ﬁxed point m∗ , each node u ∈ V computes its marginal approximation τ∗ ∈ M by u combining the local potential function ψu with a product of all incoming messages as τ∗ (xu ) ∝ ψu (xu ) u  ∏  m∗ (xu ). [sent-134, score-0.244]
</p><p>21 It is worth mentioning, and important for the computational efﬁciency of BP, that mv→u is only a function of the messages mw→v for w ∈ N (v) \ {u}. [sent-140, score-0.12]
</p><p>22 (a) Node v transmits the message mv→u = Fv→u (m), derived from Equation (4), to its neighbor u. [sent-144, score-0.14]
</p><p>23 Although the BP algorithm is considerably more efﬁcient than the brute force approach to marginalization, the message update Equation (4) still involves computing an integral and transmitting a real-valued function (message). [sent-146, score-0.176]
</p><p>24 With certain exceptions (such as multivariate Gaussians), these continuous-valued messages do not have ﬁnite representations, so that this approach is computationally very expensive. [sent-147, score-0.12]
</p><p>25 Before doing so, we begin with some background on the main underlying ingredients: orthogonal series expansion, and stochastic message updates. [sent-151, score-0.188]
</p><p>26 1 Orthogonal Series Expansion As described in the previous section, for continuous random variables, each message is a density function in the space M ⊂ L2 (X ; µ). [sent-153, score-0.14]
</p><p>27 L2  Any function f ∈ M ⊂ L2 (X ; µ) then has an expansion of the form f = ∑∞ a j φ j , where a j = f , φ j j=1 are the basis expansion coefﬁcients. [sent-157, score-0.212]
</p><p>28 Consequently, the approximation error will depend both on • how many coefﬁcients r that we retain, and • the decay rate of the expansion coefﬁcients {a j }∞ . [sent-162, score-0.194]
</p><p>29 j=1 For future reference, it is worth noting that the local message update (4) is deﬁned in terms of an integral operator of the form f (·) →  X  ψuv (·, y) f (y) µ(dy). [sent-163, score-0.202]
</p><p>30 (7)  Consequently, whenever the edge potential function ψuv has desirable properties—such as differentiability and/or higher order smoothness—then the messages also inherit these properties. [sent-164, score-0.245]
</p><p>31 With an appropriate choice of the basis {φ j }∞ , such properties translate into decay conditions on the basis j=1 coefﬁcients {a j }∞ . [sent-165, score-0.153]
</p><p>32 We begin by observing that message update (4), following the appropriate normalization, can be cast as an expectation operation. [sent-172, score-0.176]
</p><p>33 This equivalence is essential, because it allows us to obtain unbiased approximations of the message update using stochastic techniques. [sent-173, score-0.224]
</p><p>34 In particular, for every directed edge (v → u) let us deﬁne Γuv (·, y) :=  ψuv (·, y) , ψuv (x, y) µ(dx) X  and βv→u (y) := ψv (y)  X  ψuv (x, y) µ(dx),  (8)  the normalized compatibility function and the marginal potential weight respectively. [sent-174, score-0.242]
</p><p>35 2805  N OORSHAMS AND WAINWRIGHT  Lemma 1 Given an input collection of messages m, let Y be a random variable with density proportional to  ∏  [pv→u (m)](y) ∝ βv→u (y)  mw→v (y). [sent-176, score-0.12]
</p><p>36 (9)  w∈N (v)\{u}  Then the message update Equation (4) can be written as [Fv→u (m)](·) = EY Γuv (·,Y ) . [sent-177, score-0.176]
</p><p>37 By deﬁnition (4) of  w∈N (v)\{u}  the message update, we have [Fv→u (m)](·) =  ψuv (·, y) ψv (y) M(y)µ(dy) . [sent-179, score-0.14]
</p><p>38 Based on Lemma 1, we can obtain a stochastic approximation to the message update by drawing k i. [sent-182, score-0.26]
</p><p>39 3 Precise Description of the Algorithm The SOSMP algorithm involves a combination of the orthogonal series expansion techniques and stochastic methods previously described. [sent-189, score-0.127]
</p><p>40 Prior to running the algorithm, for each directed edge (v → u), we pre-compute the inner products γv→u; j (y) :=  X  Γuv (x, y)φ j (x) µ(dx), Γuv (·, y), φ j (·)  for j = 1, . [sent-191, score-0.148]
</p><p>41 At time t = 0, initialize the message coefﬁcients a0 j = 1/r v→u;  for all j = 1, . [sent-200, score-0.14]
</p><p>42 , and for each directed edge (v → u) (a) Form the projected message approximation mtw→v (·) = ∑rj=1 atw→v; j φ j (·) w ∈ N (v) \ {u}. [sent-208, score-0.324]
</p><p>43 (d) For step size ηt = 1/(t + 1), update the r-dimensional message coefﬁcient vectors atv→u → at+1 via v→u at+1 = (1 − ηt ) atv→u + ηt bt+1 . [sent-220, score-0.176]
</p><p>44 , the algorithm maintains an r-dimensional vector of basis expansion coefﬁcients atv→u = (atv→u;1 , . [sent-225, score-0.133]
</p><p>45 , atv→u;r ) ∈ Rr ,  on directed edge (v → u) ∈ E . [sent-228, score-0.148]
</p><p>46 This vector should be understood as deﬁning the current message approximation mtv→u on edge (v → u) via the expansion r  mtv→u (·) :=  ∑ atv→u; j φ j (·). [sent-229, score-0.33]
</p><p>47 With this notation, the algorithm consists of a sequence of steps, detailed in Figure 3, that perform the update at → at+1 , and hence implicitly the update mt → mt+1 . [sent-231, score-0.231]
</p><p>48 , 2010; Isard, 2003), involves approximating each message with an ℓcomponent mixture of Gaussians. [sent-240, score-0.14]
</p><p>49 Ihler and McAllester (2009) suggested an improvement known as particle BP, in which messages are approximated by particles associated with the nodes (as opposed to the edges). [sent-242, score-0.144]
</p><p>50 For any tree-structured graph, the BP algorithm is guaranteed to have a unique message ﬁxed point m∗ = {m∗ , (v → u) ∈ E }. [sent-251, score-0.14]
</p><p>51 ∞ The SOSMP algorithm generates a random sequence {at }t=0 , which deﬁne message approxit }∞ via the expansion (11). [sent-254, score-0.219]
</p><p>52 Of interest to us are the following questions: mations {m t=0  • under what conditions do the message iterates approach a neighborhood of the BP ﬁxed point m∗ as t → +∞? [sent-255, score-0.14]
</p><p>53 j=1  For each directed edge (v → u) ∈ E , deﬁne the functional error ∆tv→u := mtv→u − Πr (m∗ ) v→u  (14)  between the message approximation at time t, and the BP ﬁxed point projected onto the ﬁrst r basis functions. [sent-263, score-0.412]
</p><p>54 (15)  Since ∆tv→u belongs to the span of the ﬁrst r basis functions, the Pythagorean theorem implies that the overall error can be decomposed as mtv→u − m∗ v→u  2 L2  ∆tv→u  =  2 L2  Estimation error  Ar v→u  +  2 L2  . [sent-265, score-0.122]
</p><p>55 Our analysis of the estimation error is based on controlling the |E |-dimensional error vector ρ2 ∆t :=  ∆tv→u  2 L2 ,  (v → u) ∈ E  ∈ R|E | ,  (16)  and in particular showing that it decreases as O (1/t) up to an error ﬂoor imposed by the approximation error. [sent-267, score-0.138]
</p><p>56 These directed edges are precisely those that pass messages relevant in updating the message from v to u, so that N tracks the propagation of message information in the graph. [sent-278, score-0.564]
</p><p>57 Rows and columns of the matrix are indexed by directed edges (v → u) ∈ E ; for the row indexed by (v → u), there can be non-zero entries only for edges in the set {(w → v), w ∈ N (v)\{u}}. [sent-281, score-0.161]
</p><p>58 ) Moreover, our results on tree-structured graphs impose one condition on the vector of approximation errors Ar , namely that inf Πr Γuv (x, y) > 0,  y∈X  and |Ar (x)| ≤ v→u  1 inf Πr Γuv (x, y) 2 y∈X  (18)  for all x ∈ X and all directed edges (v → u) ∈ E . [sent-284, score-0.195]
</p><p>59 Since supx,y∈X |Πr Γuv (x, y) − Γuv (x, y)| → 0 and supx∈X |Ar (x)| → 0 as r → +∞, assuming that the compatibility functions are v→u uniformly bounded away from zero, condition (18) will hold once the number of basis expansion coefﬁcients r is sufﬁciently large. [sent-286, score-0.177]
</p><p>60 (19)  With this set-up, we have the following guarantees: Theorem 2 (tree-structured graphs) Suppose that X is closed and bounded, the node and edge potential functions are continuous, and that condition (18) holds. [sent-288, score-0.163]
</p><p>61 Theorem 3 (general graphs) Suppose that the ordinary BP algorithm is γ-contractive (20), and ∞ consider the sequence of messages {mt }t=0 generated with step-size ηt = 1/(γ(t + 1)). [sent-309, score-0.12]
</p><p>62 where Ar = m∗ − Πr (m∗ ) is the approximation error on edge (v → u). [sent-314, score-0.145]
</p><p>63 The error offset depends on the approximation error term that decays to zero as r is increased. [sent-316, score-0.132]
</p><p>64 3 Explicit Rates for Kernel Classes Theorems 2 and 3 are generic results that apply to any choices of the edge potential functions. [sent-320, score-0.125]
</p><p>65 By a δ-uniformly accurate approximation, we mean a collection of messages m such that max E mv→u − m∗ v→u  (v→u)∈E  2 L2  ≤ δ. [sent-322, score-0.12]
</p><p>66 Based on these quantities, our goal is to specify the minimal number of basic arithmetic operations T (δ) that are sufﬁcient to compute a δ-accurate message approximation. [sent-324, score-0.14]
</p><p>67 In many applications, the edge potentials ψuv : X × X → R+ are symmetric and positive semideﬁnite (PSD) functions, frequently referred to as kernel functions. [sent-326, score-0.143]
</p><p>68 If we run the SOSMP algorithm with r∗ = r∗ (δ; {λ j }) basis coefﬁcients, and k = O (1) samples, then it sufﬁces to perform T (δ; {λ j }) = O r  ∗  r∗  ∑ λ2j  1/δ log(1/δ)  (23)  j=1  arithmetic operations per edge in order to obtain a δ-accurate message vector m. [sent-344, score-0.269]
</p><p>69 Then it sufﬁces to perform Tpoly (δ) = O  1/δ  1+α α  log(1/δ)  operations per edge in order to obtain a δ-accurate message vector m. [sent-356, score-0.215]
</p><p>70 In this experiment node and edge potentials are mixtures of three Gaussians and we implemented SOSMP using the ﬁrst r = 10 Fourier coefﬁcients with k = 5 samples. [sent-371, score-0.152]
</p><p>71 Then it sufﬁces to perform Texp (δ) = O (1/δ) log(1/δ)  1+α α  (26)  operations per edge in order to obtain a uniformly δ-accurate message vector m. [sent-373, score-0.215]
</p><p>72 1 Synthetic Data We begin by running some experiments for a simple model, in which both the node and edge potentials are mixtures of Gaussians. [sent-380, score-0.152]
</p><p>73 the number of iterations for different number of expansion coefﬁcients r ∈ {2, 3, 5, 10} when k = 5 ﬁxed; whereas Figure 6(b) depicts the expected error vs. [sent-401, score-0.162]
</p><p>74 3 E FFECT OF THE E DGE P OTENTIAL S MOOTHNESS In our next set of experiments, still on a chain with n = 100 vertices, we test the behavior of the SOSMP algorithm on graphs with edge potentials of varying degrees of smoothness. [sent-409, score-0.156]
</p><p>75 The error ﬂoor, which corresponds to the approximation error incurred by message expansion truncation, decreases as the number of coefﬁcients r is increased. [sent-415, score-0.323]
</p><p>76 we use node potentials from the Gaussian mixture ensemble (27) previously discussed, but form the edge potentials in terms of a family of kernel functions. [sent-418, score-0.22]
</p><p>77 We use this basis to form the edge potential functions 1000  ψuv (x, y) =  ∑ (1/ j)α φ j (x) φ j (y),  j=1  where α > 0 is a parameter to be speciﬁed. [sent-425, score-0.179]
</p><p>78 By construction, each edge potential is a positive semideﬁnite kernel function satisfying the α-polynomial decay condition (24). [sent-426, score-0.199]
</p><p>79 For the larger value of α shown in panel (b), the messages in the BP algorithm are smoother, so that the SOSMP estimates are more accurate with the same number of expansion coefﬁcients. [sent-429, score-0.199]
</p><p>80 Moreover, similar to what we have observed previously, the error decays with the rate of 1/t till it hits the error ﬂoor. [sent-430, score-0.126]
</p><p>81 The BP messages are smoother when α = 1, and accordingly the SOSMP estimates are more accurate with the same number of expansion coefﬁcients. [sent-433, score-0.199]
</p><p>82 Moreover, the error decays with the rate of 1/t till it hits a ﬂoor corresponding to the approximation error incurred by truncating the message expansion coefﬁcients. [sent-434, score-0.381]
</p><p>83 Moreover, as predicted by our theory, the approximation error decays faster for the smoother kernel, as shown by the plots in Figure 8, in which we plot the ﬁnal error, due purely to approximation effects, versus the number of expansion coefﬁcients r. [sent-437, score-0.213]
</p><p>84 Our goal is to estimate a 2-dimensional motion vector xu = (xu;1 , xu;2 ) that captures the local motion at each pixel u = (i, j), √ i, j = 1, 2, . [sent-444, score-0.206]
</p><p>85 At node u = (i, j), we use the change between the two image frames to specify the node potential ψu (xu;1 , xu;2 ) ∝ exp −  (I(i, j) − I ′ (i + xu;1 , j + xu;2 ))2 . [sent-451, score-0.126]
</p><p>86 As predicted by the theory, the error ﬂoor decays with a faster pace for the smoother edge potential. [sent-459, score-0.137]
</p><p>87 On each edge (u, v), we introduce the potential function ψuv (xu , xv ) ∝ exp − 2818  xu − xv 2σ2 uv  2  ,  S TOCHASTIC M ESSAGE -PASSING FOR C ONTINUOUS S TATE S PACES  which enforces a type of smoothness prior over the image. [sent-461, score-0.703]
</p><p>88 To estimate the motion of a truck, we applied the SOSMP algorithm using the 2-dimensional Fourier expansion as our orthonormal basis to two 250 × 250 frames from a truck video sequence (see Figure 9). [sent-462, score-0.178]
</p><p>89 Here {a∗ j }rj=1 are the basis expansion v→u; coefﬁcients that deﬁne the best r-approximation to the BP ﬁxed point m∗ . [sent-481, score-0.133]
</p><p>90 First, we let {btv→u; j }∞ denote j=1 the basis function expansion coefﬁcients of the Fv→u (mt )—that is, [Fv→u (mt )](·) = ∑∞ btv→u; j φ j (·). [sent-483, score-0.133]
</p><p>91 By deﬁnition, the message mτ is the L2 -projection of mτ w→v w→v onto M . [sent-512, score-0.14]
</p><p>92 , mt ) be the σ-ﬁeld generated by all messages through time t. [sent-523, score-0.279]
</p><p>93 By the Pythagorean ∗ term, we begin by representing mv→u in terms of the basis expansion ∑ j=1 a j j theorem, we have m∗ − Πr (m∗ ) v→u v→u  2 L2  ∞  =  ∑  (a∗ )2 . [sent-631, score-0.133]
</p><p>94 Indeed, since m∗ is a ﬁxed point of the message update j=1 j equation, we have m∗ (·) ∝ v→u  X  ψuv (·, y) M(y) µ(dy),  where M(·) := ψv (·) ∏w∈N (v)\{u} m∗ (·). [sent-633, score-0.176]
</p><p>95 k=1  Substituting back into our initial Equation (48), we ﬁnd that a∗ ∝ λ j j  X  φ j (y) M(y) µ(dy) = λ j a j ,  where a j are the basis expansion coefﬁcients of M. [sent-636, score-0.133]
</p><p>96 It is based on two forms of approximation: a deterministic approximation that involves projecting messages onto the span of r basis functions, and a stochastic approximation that involves approximating basis coefﬁcients via Monte Carlo techniques and damped updates. [sent-648, score-0.348]
</p><p>97 For graphs with relatively smooth potential functions, as reﬂected in the decay rate of their basis coefﬁcients, we provided a quantitative bound on the total number of basic arithmetic operations required to compute the BP ﬁxed point to within δ-accuracy. [sent-650, score-0.191]
</p><p>98 ′ For an arbitrary message mv→u ∈ Mv→u there exist 0 < α < 1 and a bounded probability density f so that mv→u (x) = α m∗ (x) + (1 − α) EY ∼ f Γuv (x,Y ) v→u  +  ,  4. [sent-684, score-0.14]
</p><p>99 The last step of the proof is to verify that m∗ ∈ M ′ , and mt ∈ M ′ for all t = 1, 2, . [sent-699, score-0.159]
</p><p>100 Loopy belief propagation: Convergence and effects of message errors. [sent-864, score-0.198]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('uv', 0.336), ('sosmp', 0.318), ('bp', 0.305), ('atv', 0.292), ('fv', 0.184), ('pv', 0.177), ('mv', 0.17), ('mtv', 0.168), ('mt', 0.159), ('ontinuous', 0.159), ('oorshams', 0.159), ('tate', 0.159), ('ar', 0.153), ('message', 0.14), ('essage', 0.136), ('tochastic', 0.129), ('paces', 0.123), ('messages', 0.12), ('xu', 0.116), ('wainwright', 0.115), ('btv', 0.115), ('tv', 0.101), ('tst', 0.097), ('mw', 0.094), ('rj', 0.088), ('expansion', 0.079), ('lv', 0.078), ('edge', 0.075), ('directed', 0.073), ('cients', 0.072), ('ey', 0.072), ('mtw', 0.071), ('noorshams', 0.071), ('ihler', 0.068), ('xv', 0.063), ('hw', 0.062), ('coef', 0.059), ('belief', 0.058), ('bt', 0.057), ('basis', 0.054), ('nilpotent', 0.053), ('isard', 0.053), ('potential', 0.05), ('iterations', 0.049), ('stochastic', 0.048), ('propagation', 0.047), ('graphical', 0.047), ('motion', 0.045), ('optical', 0.045), ('decay', 0.045), ('contractivity', 0.044), ('gateaux', 0.044), ('compatibility', 0.044), ('edges', 0.044), ('graphs', 0.042), ('oor', 0.041), ('cycles', 0.04), ('dx', 0.039), ('potentials', 0.039), ('node', 0.038), ('pythagorean', 0.038), ('approximation', 0.036), ('update', 0.036), ('frechet', 0.035), ('parseval', 0.035), ('unwrapping', 0.035), ('sudderth', 0.035), ('updates', 0.034), ('error', 0.034), ('ms', 0.034), ('vv', 0.033), ('dw', 0.032), ('loopy', 0.032), ('martingale', 0.031), ('hits', 0.03), ('shorthand', 0.03), ('kernel', 0.029), ('dv', 0.029), ('ow', 0.028), ('dy', 0.028), ('decays', 0.028), ('coughlan', 0.027), ('hue', 0.027), ('kschischang', 0.027), ('roosta', 0.027), ('saturation', 0.027), ('verses', 0.027), ('operator', 0.026), ('graph', 0.026), ('st', 0.026), ('lemma', 0.025), ('doucet', 0.025), ('fubini', 0.025), ('kernels', 0.025), ('fourier', 0.024), ('particle', 0.024), ('marginals', 0.024), ('logt', 0.023), ('arulampalam', 0.023), ('cuv', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="17-tfidf-1" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>Author: Nima Noorshams, Martin J. Wainwright</p><p>Abstract: The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP ﬁxed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefﬁcients via Monte Carlo techniques and damped updates. We prove that the SOSMP iterates converge to a δ-neighborhood of the unique BP ﬁxed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefﬁcients as a function of the desired approximation accuracy δ and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical ﬂow estimation. Keywords: graphical models, sum-product for continuous state spaces, low-complexity belief propagation, stochastic approximation, Monte Carlo methods, orthogonal basis expansion</p><p>2 0.1637482 <a title="17-tfidf-2" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>Author: Naftali Harris, Mathias Drton</p><p>Abstract: The PC algorithm uses conditional independence tests for model selection in graphical modeling with acyclic directed graphs. In Gaussian models, tests of conditional independence are typically based on Pearson correlations, and high-dimensional consistency results have been obtained for the PC algorithm in this setting. Analyzing the error propagation from marginal to partial correlations, we prove that high-dimensional consistency carries over to a broader class of Gaussian copula or nonparanormal models when using rank-based measures of correlation. For graph sequences with bounded degree, our consistency result is as strong as prior Gaussian results. In simulations, the ‘Rank PC’ algorithm works as well as the ‘Pearson PC’ algorithm for normal data and considerably better for non-normal data, all the while incurring a negligible increase of computation time. While our interest is in the PC algorithm, the presented analysis of error propagation could be applied to other algorithms that test the vanishing of low-order partial correlations. Keywords: Gaussian copula, graphical model, model selection, multivariate normal distribution, nonparanormal distribution</p><p>3 0.14168324 <a title="17-tfidf-3" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>Author: Qiang Liu, Alexander Ihler</p><p>Abstract: The marginal maximum a posteriori probability (MAP) estimation problem, which calculates the mode of the marginal posterior distribution of a subset of variables with the remaining variables marginalized, is an important inference problem in many models, such as those with hidden variables or uncertain parameters. Unfortunately, marginal MAP can be NP-hard even on trees, and has attracted less attention in the literature compared to the joint MAP (maximization) and marginalization problems. We derive a general dual representation for marginal MAP that naturally integrates the marginalization and maximization operations into a joint variational optimization problem, making it possible to easily extend most or all variational-based algorithms to marginal MAP. In particular, we derive a set of “mixed-product” message passing algorithms for marginal MAP, whose form is a hybrid of max-product, sum-product and a novel “argmax-product” message updates. We also derive a class of convergent algorithms based on proximal point methods, including one that transforms the marginal MAP problem into a sequence of standard marginalization problems. Theoretically, we provide guarantees under which our algorithms give globally or locally optimal solutions, and provide novel upper bounds on the optimal objectives. Empirically, we demonstrate that our algorithms signiﬁcantly outperform the existing approaches, including a state-of-the-art algorithm based on local search methods. Keywords: graphical models, message passing, belief propagation, variational methods, maximum a posteriori, marginal-MAP, hidden variable models</p><p>4 0.13566007 <a title="17-tfidf-4" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>Author: Nicholas Ruozzi, Sekhar Tatikonda</p><p>Abstract: Gaussian belief propagation (GaBP) is an iterative algorithm for computing the mean (and variances) of a multivariate Gaussian distribution, or equivalently, the minimum of a multivariate positive deﬁnite quadratic function. Sufﬁcient conditions, such as walk-summability, that guarantee the convergence and correctness of GaBP are known, but GaBP may fail to converge to the correct solution given an arbitrary positive deﬁnite covariance matrix. As was observed by Malioutov et al. (2006), the GaBP algorithm fails to converge if the computation trees produced by the algorithm are not positive deﬁnite. In this work, we will show that the failure modes of the GaBP algorithm can be understood via graph covers, and we prove that a parameterized generalization of the min-sum algorithm can be used to ensure that the computation trees remain positive deﬁnite whenever the input matrix is positive deﬁnite. We demonstrate that the resulting algorithm is closely related to other iterative schemes for quadratic minimization such as the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically, that there always exists a choice of parameters such that the above generalization of the GaBP algorithm converges. Keywords: belief propagation, Gaussian graphical models, graph covers</p><p>5 0.13104956 <a title="17-tfidf-5" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>Author: Pekka Parviainen, Mikko Koivisto</p><p>Abstract: We consider the problem of ﬁnding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space 2n , to within a factor polynomial in the number of nodes n. In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: ﬁrst, the user may trade space against time; second, the proposed algorithms easily and efﬁciently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order P on the nodes, an optimal DAG compatible with P can be found in time and space roughly proportional to the number of ideals of P , which can be signiﬁcantly less than 2n . Considering sufﬁciently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders. Keywords: exact algorithm, parallelization, partial order, space-time tradeoff, structure learning</p><p>6 0.12030347 <a title="17-tfidf-6" href="./jmlr-2013-Approximating_the_Permanent_with_Fractional_Belief_Propagation.html">13 jmlr-2013-Approximating the Permanent with Fractional Belief Propagation</a></p>
<p>7 0.10104726 <a title="17-tfidf-7" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>8 0.065218322 <a title="17-tfidf-8" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>9 0.051199596 <a title="17-tfidf-9" href="./jmlr-2013-Comment_on_%22Robustness_and_Regularization_of_Support_Vector_Machines%22_by_H._Xu_et_al._%28Journal_of_Machine_Learning_Research%2C_vol._10%2C_pp._1485-1510%2C_2009%29.html">24 jmlr-2013-Comment on "Robustness and Regularization of Support Vector Machines" by H. Xu et al. (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009)</a></p>
<p>10 0.047883976 <a title="17-tfidf-10" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>11 0.047451418 <a title="17-tfidf-11" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>12 0.046307128 <a title="17-tfidf-12" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>13 0.045017313 <a title="17-tfidf-13" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>14 0.044228137 <a title="17-tfidf-14" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>15 0.041015141 <a title="17-tfidf-15" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>16 0.039850689 <a title="17-tfidf-16" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>17 0.039082486 <a title="17-tfidf-17" href="./jmlr-2013-Algorithms_for_Discovery_of_Multiple_Markov_Boundaries.html">11 jmlr-2013-Algorithms for Discovery of Multiple Markov Boundaries</a></p>
<p>18 0.03800888 <a title="17-tfidf-18" href="./jmlr-2013-Improving_CUR_Matrix_Decomposition_and_the_Nystrom_Approximation_via_Adaptive_Sampling.html">53 jmlr-2013-Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling</a></p>
<p>19 0.037308134 <a title="17-tfidf-19" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>20 0.035824306 <a title="17-tfidf-20" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.228), (1, -0.013), (2, -0.004), (3, 0.072), (4, 0.157), (5, 0.278), (6, 0.106), (7, 0.049), (8, -0.042), (9, 0.19), (10, -0.093), (11, 0.244), (12, 0.219), (13, 0.027), (14, 0.012), (15, -0.245), (16, -0.041), (17, -0.178), (18, 0.031), (19, -0.009), (20, -0.125), (21, 0.14), (22, -0.002), (23, -0.019), (24, -0.061), (25, -0.013), (26, 0.112), (27, 0.056), (28, 0.03), (29, -0.008), (30, 0.002), (31, 0.139), (32, 0.069), (33, 0.039), (34, 0.019), (35, -0.107), (36, -0.015), (37, -0.034), (38, 0.041), (39, -0.082), (40, -0.004), (41, 0.049), (42, -0.011), (43, 0.044), (44, -0.06), (45, 0.061), (46, -0.026), (47, 0.053), (48, -0.04), (49, 0.116)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93574685 <a title="17-lsi-1" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>Author: Nima Noorshams, Martin J. Wainwright</p><p>Abstract: The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP ﬁxed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefﬁcients via Monte Carlo techniques and damped updates. We prove that the SOSMP iterates converge to a δ-neighborhood of the unique BP ﬁxed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefﬁcients as a function of the desired approximation accuracy δ and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical ﬂow estimation. Keywords: graphical models, sum-product for continuous state spaces, low-complexity belief propagation, stochastic approximation, Monte Carlo methods, orthogonal basis expansion</p><p>2 0.57031298 <a title="17-lsi-2" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>Author: Pekka Parviainen, Mikko Koivisto</p><p>Abstract: We consider the problem of ﬁnding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space 2n , to within a factor polynomial in the number of nodes n. In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: ﬁrst, the user may trade space against time; second, the proposed algorithms easily and efﬁciently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order P on the nodes, an optimal DAG compatible with P can be found in time and space roughly proportional to the number of ideals of P , which can be signiﬁcantly less than 2n . Considering sufﬁciently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders. Keywords: exact algorithm, parallelization, partial order, space-time tradeoff, structure learning</p><p>3 0.51839989 <a title="17-lsi-3" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>Author: Nicholas Ruozzi, Sekhar Tatikonda</p><p>Abstract: Gaussian belief propagation (GaBP) is an iterative algorithm for computing the mean (and variances) of a multivariate Gaussian distribution, or equivalently, the minimum of a multivariate positive deﬁnite quadratic function. Sufﬁcient conditions, such as walk-summability, that guarantee the convergence and correctness of GaBP are known, but GaBP may fail to converge to the correct solution given an arbitrary positive deﬁnite covariance matrix. As was observed by Malioutov et al. (2006), the GaBP algorithm fails to converge if the computation trees produced by the algorithm are not positive deﬁnite. In this work, we will show that the failure modes of the GaBP algorithm can be understood via graph covers, and we prove that a parameterized generalization of the min-sum algorithm can be used to ensure that the computation trees remain positive deﬁnite whenever the input matrix is positive deﬁnite. We demonstrate that the resulting algorithm is closely related to other iterative schemes for quadratic minimization such as the Gauss-Seidel and Jacobi algorithms. Finally, we observe, empirically, that there always exists a choice of parameters such that the above generalization of the GaBP algorithm converges. Keywords: belief propagation, Gaussian graphical models, graph covers</p><p>4 0.50142425 <a title="17-lsi-4" href="./jmlr-2013-Approximating_the_Permanent_with_Fractional_Belief_Propagation.html">13 jmlr-2013-Approximating the Permanent with Fractional Belief Propagation</a></p>
<p>Author: Michael Chertkov, Adam B. Yedidia</p><p>Abstract: We discuss schemes for exact and approximate computations of permanents, and compare them with each other. Speciﬁcally, we analyze the belief propagation (BP) approach and its fractional belief propagation (FBP) generalization for computing the permanent of a non-negative matrix. Known bounds and Conjectures are veriﬁed in experiments, and some new theoretical relations, bounds and Conjectures are proposed. The fractional free energy (FFE) function is parameterized by a scalar parameter γ ∈ [−1; 1], where γ = −1 corresponds to the BP limit and γ = 1 corresponds to the exclusion principle (but ignoring perfect matching constraints) mean-ﬁeld (MF) limit. FFE shows monotonicity and continuity with respect to γ. For every non-negative matrix, we deﬁne its special value γ∗ ∈ [−1; 0] to be the γ for which the minimum of the γ-parameterized FFE function is equal to the permanent of the matrix, where the lower and upper bounds of the γ-interval corresponds to respective bounds for the permanent. Our experimental analysis suggests that the distribution of γ∗ varies for different ensembles but γ∗ always lies within the [−1; −1/2] interval. Moreover, for all ensembles considered, the behavior of γ∗ is highly distinctive, offering an empirical practical guidance for estimating permanents of non-negative matrices via the FFE approach. Keywords: permanent, graphical models, belief propagation, exact and approximate algorithms, learning ﬂows</p><p>5 0.48267257 <a title="17-lsi-5" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>Author: Naftali Harris, Mathias Drton</p><p>Abstract: The PC algorithm uses conditional independence tests for model selection in graphical modeling with acyclic directed graphs. In Gaussian models, tests of conditional independence are typically based on Pearson correlations, and high-dimensional consistency results have been obtained for the PC algorithm in this setting. Analyzing the error propagation from marginal to partial correlations, we prove that high-dimensional consistency carries over to a broader class of Gaussian copula or nonparanormal models when using rank-based measures of correlation. For graph sequences with bounded degree, our consistency result is as strong as prior Gaussian results. In simulations, the ‘Rank PC’ algorithm works as well as the ‘Pearson PC’ algorithm for normal data and considerably better for non-normal data, all the while incurring a negligible increase of computation time. While our interest is in the PC algorithm, the presented analysis of error propagation could be applied to other algorithms that test the vanishing of low-order partial correlations. Keywords: Gaussian copula, graphical model, model selection, multivariate normal distribution, nonparanormal distribution</p><p>6 0.44484618 <a title="17-lsi-6" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>7 0.4240019 <a title="17-lsi-7" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>8 0.32297149 <a title="17-lsi-8" href="./jmlr-2013-Comment_on_%22Robustness_and_Regularization_of_Support_Vector_Machines%22_by_H._Xu_et_al._%28Journal_of_Machine_Learning_Research%2C_vol._10%2C_pp._1485-1510%2C_2009%29.html">24 jmlr-2013-Comment on "Robustness and Regularization of Support Vector Machines" by H. Xu et al. (Journal of Machine Learning Research, vol. 10, pp. 1485-1510, 2009)</a></p>
<p>9 0.28842553 <a title="17-lsi-9" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>10 0.25361344 <a title="17-lsi-10" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>11 0.19392441 <a title="17-lsi-11" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>12 0.18489814 <a title="17-lsi-12" href="./jmlr-2013-Perturbative_Corrections_for_Approximate_Inference_in_Gaussian_Latent_Variable_Models.html">88 jmlr-2013-Perturbative Corrections for Approximate Inference in Gaussian Latent Variable Models</a></p>
<p>13 0.18216558 <a title="17-lsi-13" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>14 0.17740694 <a title="17-lsi-14" href="./jmlr-2013-Sub-Local_Constraint-Based_Learning_of_Bayesian_Networks_Using_A_Joint_Dependence_Criterion.html">110 jmlr-2013-Sub-Local Constraint-Based Learning of Bayesian Networks Using A Joint Dependence Criterion</a></p>
<p>15 0.16845013 <a title="17-lsi-15" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>16 0.16613476 <a title="17-lsi-16" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>17 0.1642268 <a title="17-lsi-17" href="./jmlr-2013-Kernel_Bayes%27_Rule%3A_Bayesian_Inference_with_Positive_Definite_Kernels.html">57 jmlr-2013-Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels</a></p>
<p>18 0.162953 <a title="17-lsi-18" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>19 0.1627714 <a title="17-lsi-19" href="./jmlr-2013-Similarity-based_Clustering_by_Left-Stochastic_Matrix_Factorization.html">100 jmlr-2013-Similarity-based Clustering by Left-Stochastic Matrix Factorization</a></p>
<p>20 0.16213644 <a title="17-lsi-20" href="./jmlr-2013-Improving_CUR_Matrix_Decomposition_and_the_Nystrom_Approximation_via_Adaptive_Sampling.html">53 jmlr-2013-Improving CUR Matrix Decomposition and the Nystrom Approximation via Adaptive Sampling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.039), (5, 0.171), (6, 0.114), (10, 0.066), (19, 0.314), (20, 0.015), (23, 0.049), (68, 0.014), (70, 0.024), (75, 0.027), (85, 0.028), (87, 0.018), (89, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74586397 <a title="17-lda-1" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>Author: Nima Noorshams, Martin J. Wainwright</p><p>Abstract: The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP ﬁxed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefﬁcients via Monte Carlo techniques and damped updates. We prove that the SOSMP iterates converge to a δ-neighborhood of the unique BP ﬁxed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefﬁcients as a function of the desired approximation accuracy δ and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical ﬂow estimation. Keywords: graphical models, sum-product for continuous state spaces, low-complexity belief propagation, stochastic approximation, Monte Carlo methods, orthogonal basis expansion</p><p>2 0.73503864 <a title="17-lda-2" href="./jmlr-2013-Experiment_Selection_for_Causal_Discovery.html">41 jmlr-2013-Experiment Selection for Causal Discovery</a></p>
<p>Author: Antti Hyttinen, Frederick Eberhardt, Patrik O. Hoyer</p><p>Abstract: Randomized controlled experiments are often described as the most reliable tool available to scientists for discovering causal relationships among quantities of interest. However, it is often unclear how many and which different experiments are needed to identify the full (possibly cyclic) causal structure among some given (possibly causally insufﬁcient) set of variables. Recent results in the causal discovery literature have explored various identiﬁability criteria that depend on the assumptions one is able to make about the underlying causal process, but these criteria are not directly constructive for selecting the optimal set of experiments. Fortunately, many of the needed constructions already exist in the combinatorics literature, albeit under terminology which is unfamiliar to most of the causal discovery community. In this paper we translate the theoretical results and apply them to the concrete problem of experiment selection. For a variety of settings we give explicit constructions of the optimal set of experiments and adapt some of the general combinatorics results to answer questions relating to the problem of experiment selection. Keywords: causality, randomized experiments, experiment selection, separating systems, completely separating systems, cut-coverings</p><p>3 0.55009437 <a title="17-lda-3" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>Author: Yuejia He, Yiyuan She, Dapeng Wu</p><p>Abstract: Recently, researchers have proposed penalized maximum likelihood to identify network topology underlying a dynamical system modeled by multivariate time series. The time series of interest are assumed to be stationary, but this restriction is never taken into consideration by existing estimation methods. Moreover, practical problems of interest may have ultra-high dimensionality and obvious node collinearity. In addition, none of the available algorithms provides a probabilistic measure of the uncertainty for the obtained network topology which is informative in reliable network identiﬁcation. The main purpose of this paper is to tackle these challenging issues. We propose the S2 learning framework, which stands for stationary-sparse network learning. We propose a novel algorithm referred to as the Berhu iterative sparsity pursuit with stationarity (BISPS), where the Berhu regularization can improve the Lasso in detection and estimation. The algorithm is extremely easy to implement, efﬁcient in computation and has a theoretical guarantee to converge to a global optimum. We also incorporate a screening technique into BISPS to tackle ultra-high dimensional problems and enhance computational efﬁciency. Furthermore, a stationary bootstrap technique is applied to provide connection occurring frequency for reliable topology learning. Experiments show that our method can achieve stationary and sparse causality network learning and is scalable for high-dimensional problems. Keywords: stationarity, sparsity, Berhu, screening, bootstrap</p><p>4 0.54293597 <a title="17-lda-4" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>Author: Yuchen Zhang, John C. Duchi, Martin J. Wainwright</p><p>Abstract: We analyze two communication-efﬁcient algorithms for distributed optimization in statistical settings involving large-scale data sets. The ﬁrst algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves √ mean-squared error (MSE) that decays as O (N −1 + (N/m)−2 ). Whenever m ≤ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O (N −1 + (N/m)−3 ), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O (N −1 + (N/m)−3/2 ), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efﬁciently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 108 samples and d ≈ 740,000 covariates. Keywords: distributed learning, stochastic optimization, averaging, subsampling</p><p>5 0.54161757 <a title="17-lda-5" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>Author: Philip M. Long, Rocco A. Servedio</p><p>Abstract: We consider the problem of learning an unknown large-margin halfspace in the context of parallel computation, giving both positive and negative results. As our main positive result, we give a parallel algorithm for learning a large-margin halfspace, based on an algorithm of Nesterov’s that performs gradient descent with a momentum term. We show that this algorithm can learn an unknown γ-margin halfspace over n dimensions using ˜ n · poly(1/γ) processors and running in time O(1/γ) + O(log n). In contrast, naive parallel algorithms that learn a γ-margin halfspace in time that depends polylogarithmically on n have an inverse quadratic running time dependence on the margin parameter γ. Our negative result deals with boosting, which is a standard approach to learning large-margin halfspaces. We prove that in the original PAC framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized. More precisely, we show that, if the algorithm is allowed to call the weak learner multiple times in parallel within a single boosting stage, this ability does not reduce the overall number of successive stages of boosting needed for learning by even a single stage. Our proof is information-theoretic and does not rely on unproven assumptions. Keywords: PAC learning, parallel learning algorithms, halfspace learning, linear classiﬁers</p><p>6 0.53186274 <a title="17-lda-6" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>7 0.53165251 <a title="17-lda-7" href="./jmlr-2013-Random_Spanning_Trees_and_the_Prediction_of_Weighted_Graphs.html">92 jmlr-2013-Random Spanning Trees and the Prediction of Weighted Graphs</a></p>
<p>8 0.53137738 <a title="17-lda-8" href="./jmlr-2013-Segregating_Event_Streams_and_Noise_with_a_Markov_Renewal_Process_Model.html">98 jmlr-2013-Segregating Event Streams and Noise with a Markov Renewal Process Model</a></p>
<p>9 0.53001583 <a title="17-lda-9" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>10 0.52950627 <a title="17-lda-10" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>11 0.52915847 <a title="17-lda-11" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>12 0.52654004 <a title="17-lda-12" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>13 0.52648717 <a title="17-lda-13" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>14 0.52629596 <a title="17-lda-14" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>15 0.523103 <a title="17-lda-15" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>16 0.52260369 <a title="17-lda-16" href="./jmlr-2013-Optimally_Fuzzy_Temporal_Memory.html">82 jmlr-2013-Optimally Fuzzy Temporal Memory</a></p>
<p>17 0.52247173 <a title="17-lda-17" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>18 0.5219422 <a title="17-lda-18" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>19 0.52183354 <a title="17-lda-19" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>20 0.51768392 <a title="17-lda-20" href="./jmlr-2013-Bayesian_Canonical_Correlation_Analysis.html">15 jmlr-2013-Bayesian Canonical Correlation Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
