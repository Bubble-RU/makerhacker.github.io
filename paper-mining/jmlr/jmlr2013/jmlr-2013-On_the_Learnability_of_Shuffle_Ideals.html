<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 jmlr-2013-On the Learnability of Shuffle Ideals</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-78" href="#">jmlr2013-78</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>78 jmlr-2013-On the Learnability of Shuffle Ideals</h1>
<br/><p>Source: <a title="jmlr-2013-78-pdf" href="http://jmlr.org/papers/volume14/angluin13a/angluin13a.pdf">pdf</a></p><p>Author: Dana Angluin, James Aspnes, Sarah Eisenstat, Aryeh Kontorovich</p><p>Abstract: PAC learning of unrestricted regular languages is long known to be a difﬁcult problem. The class of shufﬂe ideals is a very restricted subclass of regular languages, where the shufﬂe ideal generated by a string u is the collection of all strings containing u as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shufﬂe ideals appears quite difﬁcult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP = NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efﬁcient algorithm for properly learning shufﬂe ideals in the statistical query (and therefore also PAC) model under the uniform distribution. Keywords: PAC learning, statistical queries, regular languages, deterministic ﬁnite automata, shufﬂe ideals, subsequences</p><p>Reference: <a title="jmlr-2013-78-reference" href="../jmlr2013_reference/jmlr-2013-On_the_Learnability_of_Shuffle_Ideals_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The class of shufﬂe ideals is a very restricted subclass of regular languages, where the shufﬂe ideal generated by a string u is the collection of all strings containing u as a subsequence. [sent-10, score-0.943]
</p><p>2 In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP = NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. [sent-13, score-0.326]
</p><p>3 In the positive direction, we give an efﬁcient algorithm for properly learning shufﬂe ideals in the statistical query (and therefore also PAC) model under the uniform distribution. [sent-14, score-0.309]
</p><p>4 Their approach was to embed a speciﬁc family of regular languages (the piecewise-testable ones) in a Hilbert space via a kernel and to identify languages with hyperplanes. [sent-44, score-0.258]
</p><p>5 (1995) consider the learnability u 1514  O N THE L EARNABILITY OF S HUFFLE I DEALS  b,c  0  a  1  a  a,b,c  a,c  b,c  2  b  3  Figure 1: The canonical DFA for recognizing the shufﬂe ideal of u = aab over Σ = {a, b, c}, which accepts precisely those strings that contain u as a subsequence. [sent-53, score-0.364]
</p><p>6 The shufﬂe ideal generated by a string u is the collection of all strings containing u as a (not necessarily contiguous) subsequence (see Figure 1 for an illustration). [sent-57, score-0.963]
</p><p>7 Despite being a particularly simple subfamily of the regular languages, shufﬂe ideals play a prominent role in formal language theory. [sent-58, score-0.325]
</p><p>8 On a more applied front, the shufﬂe ideals capture some rudimentary phenomena in human language morphology (Kontorovich et al. [sent-61, score-0.265]
</p><p>9 In Section 3 we show that shufﬂe ideals of known length are exactly learnable in the statistical query model under the uniform distribution, though not efﬁciently. [sent-63, score-0.351]
</p><p>10 On the other hand, in Section 4 we show that the shufﬂe ideals are not properly PAC learnable under general distributions unless RP=NP. [sent-65, score-0.32]
</p><p>11 In Section 5 we show that a polynomial time improper PAC learning algorithm for the class of shufﬂe ideals would imply the existence of polynomial time algorithms to break the RSA cryptosystem, factor Blum integers, and test quadratic residuosity. [sent-66, score-0.324]
</p><p>12 The elements of Σ∗ will be referred to as strings with their length denoted by |·|; the empty string is λ. [sent-71, score-0.659]
</p><p>13 The concatenation of strings u1 and u2 is denoted by u1 · u2 or u1 u2 . [sent-72, score-0.257]
</p><p>14 The string u is a preﬁx of a string v if there exists a string w such that v = uw. [sent-73, score-1.128]
</p><p>15 Similarly, u is a sufﬁx of v if there exists a string w such that v = wu. [sent-74, score-0.376]
</p><p>16 We use exponential notation for repeated concatenation of a string with itself, that is, un is the concatenation of n copies of u. [sent-75, score-0.458]
</p><p>17 If u ⊑ v then the leftmost span of u in v 1515  A NGLUIN , A SPNES , E ISENSTAT AND KONTOROVICH  is the shortest preﬁx v1 of v such that u ⊑ v1 and the rightmost span of u in v is the shortest sufﬁx v2 of v such that u ⊑ v2 . [sent-82, score-0.305]
</p><p>18 The shufﬂe ideal of string u consists of all strings v over the given alphabet such that u ⊑ v. [sent-87, score-0.724]
</p><p>19 Lemma 1 Suppose u = u1 u2 u3 and v = v1 v2 v3 are strings such that u ⊑ v and v1 is the leftmost span of u1 in v and v3 is the rightmost span of u3 in v. [sent-91, score-0.562]
</p><p>20 Proof If u = λ, then u is certainly a subsequence of x. [sent-94, score-0.296]
</p><p>21 If there is no such occurrence, then u is certainly not a subsequence of x. [sent-96, score-0.296]
</p><p>22 Otherwise, we write x = yax′ , where y contains no occurrence of a; then u is a subsequence of x if and only if u′ is a subsequence of x′ , so we continue recursively with u′ and x′ . [sent-97, score-0.636]
</p><p>23 If n is an upper bound on the length of the string u ∈ Σ∗ generating the target shufﬂe ideal, then our concept class contains exactly n  ∑ |Σ|ℓ = O(|Σ|n)  ℓ=0  1516  O N THE L EARNABILITY OF S HUFFLE I DEALS  members. [sent-110, score-0.402]
</p><p>24 m  Hence, the problem of properly PAC learning shufﬂe ideals has been reduced to ﬁnding one that is consistent with a given sample. [sent-112, score-0.26]
</p><p>25 In addition, in Section 5 we show that the existence of a polynomial time improper PAC learning algorithm for shufﬂe ideals would imply the existence of polynomial time algorithms for certain cryptographic problems. [sent-115, score-0.387]
</p><p>26 SQ Learning Under the Uniform Distribution The main result of this section is that shufﬂe ideals are efﬁciently PAC learnable under the uniform distribution. [sent-117, score-0.276]
</p><p>27 If u is not a subsequence of x′ , χu,a (x, y) = 0. [sent-126, score-0.296]
</p><p>28 Proof Fix an unknown string u of length L ≥ 1; by assumption, we have recovered in u = u1 . [sent-138, score-0.402]
</p><p>29 Let X be a random variable representing the uniformly chosen sample string x. [sent-146, score-0.376]
</p><p>30 TA is the length of the longest preﬁx of ′ that is a subsequence of X with X u Iℓ +1 excluded: TA = max t : u′ . [sent-184, score-0.322]
</p><p>31 1 Intuitively, TB is the length of the longest preﬁx of u′ with u′ excluded that is a subsequence of X ℓ+1 with XIℓ +1 excluded. [sent-194, score-0.322]
</p><p>32 s 1518  O N THE L EARNABILITY OF S HUFFLE I DEALS  If X is a positive example, then u is a subsequence of X and a leftmost embedding of u in X ¯ ¯ embeds u1 . [sent-230, score-0.474]
</p><p>33 ¯ ¯ 1519  A NGLUIN , A SPNES , E ISENSTAT AND KONTOROVICH  Theorem 4 When the length L of the target string u is known, u is exactly identiﬁable with O(Ls) ¯ ¯ 2 statistical queries at tolerance τ = 3(s−1) P(L, n, s). [sent-265, score-0.524]
</p><p>34 Theorem 5 When the length L of the target string u is known, u is approximately identiﬁable to ¯ ¯ within ε > 0 with O(Ls) statistical queries at tolerance τ = 2ε/(9(s − 1)n). [sent-270, score-0.524]
</p><p>35 Theorem 7 For any alphabet of size at least 2, given two disjoint sets of strings S, T ⊂ Σ∗ , the problem of determining whether there exists a string u such that u ⊑ x for each x ∈ S and u ⊑ x for each x ∈ T is NP-complete. [sent-294, score-0.69]
</p><p>36 Let Σ = {0, 1}, let n be a positive integer and deﬁne An to be the set of 2n binary strings described by the regular expression ((00000 + 00100)11)n . [sent-296, score-0.317]
</p><p>37 Deﬁne strings v0 = 000100, v1 = 001000, d = 11, and let Sn consist of the two strings s0 = (v0 d)n , s1 = (v1 d)n . [sent-297, score-0.514]
</p><p>38 The strings ti,0 , ti,1 and ti,2 are obtained from s0 by replacing occurrence i of v0 by y0 , y1 , and z, respectively. [sent-299, score-0.301]
</p><p>39 The string ti,3 is obtained from s0 by replacing occurrence i of d by d0 . [sent-300, score-0.42]
</p><p>40 The following lemma shows that the set of strings consistent with Sn and Tn is precisely the 2n strings in An . [sent-302, score-0.514]
</p><p>41 Lemma 8 Let Cn be the set of strings u such that u is a subsequence of both strings in Sn and not a subsequence of any string in Tn . [sent-303, score-1.482]
</p><p>42 Proof We ﬁrst observe that for any positive integer m and any string u ∈ Am , the leftmost span of u in (v0 d)m is (v0 d)m itself, and the leftmost span of u in (v1 d)m is (v1 d)m itself. [sent-305, score-0.76]
</p><p>43 Similarly, for any string u ∈ Am , the rightmost span of du in d(v0 d)m is d(v0 d)m itself, and the rightmost span of du in d(v1 d)m is d(v1 d)m itself. [sent-308, score-0.602]
</p><p>44 The leftmost span of u′ in ti,0 is (v0 d)i−1 , and the rightmost span of u′′ in ti,0 is d(v0 d)n−i , which implies that ui ⊑ y0 by Lemma 1. [sent-317, score-0.42]
</p><p>45 Similar arguments show that u is not a subsequence of ti,1 or ti,2 . [sent-320, score-0.296]
</p><p>46 We divide u into parts, u = u′ ui dui+1 u′′ , where u′ = u1 d · · · ui−1 d and ′′ = du ′ i−1 and the rightmost span of u′′ in t u i+2 · · · un d. [sent-322, score-0.261]
</p><p>47 1522  O N THE L EARNABILITY OF S HUFFLE I DEALS  That is, at least one of the strings 000001100000, 001001100000, 000001100100, 001001100100 must be a subsequence of 0001001000100, which is false, showing that u is not a subsequence of ti,3 . [sent-325, score-0.849]
</p><p>48 Thus u is not a subsequence of any string in Tn , and u ∈ Cn . [sent-326, score-0.672]
</p><p>49 Similarly, if ui is a subsequence of y0 , y1 or z, then u is a subsequence of ti,0 , ti,1 , or ti,2 , respectively, so we know that each ui is a subsequence of the string 000100, but not a subsequence of the strings 00010, 01000, or 0000. [sent-332, score-2.047]
</p><p>50 To eliminate the third possibility we use the fact that u is a subsequence of s1 . [sent-334, score-0.296]
</p><p>51 Consider any string w = w1 dw2 d · · · wn d, where wi = 000100 and each w j for j = i is either 00000 or 00100. [sent-335, score-0.376]
</p><p>52 If w ⊑ s1 , then the leftmost span of w′ in s1 is (v1 d)i−1 , and the rightmost span of w′′ in s1 is d(v1 d)n−i , which by Lemma 1 means that 000100 must be a subsequence of v1 = 001000, a contradiction. [sent-337, score-0.601]
</p><p>53 Thus no such w is a subsequence of s1 , and we must have ui equal to 00000 or 00100 for all i, that is, u must be in An . [sent-338, score-0.411]
</p><p>54 Proof To see that this decision problem is in NP, note that if S is empty, then any string of length longer than the longest string in T satisﬁes the necessary requirements, so that the answer in this case is necessarily “yes. [sent-341, score-0.778]
</p><p>55 Given a CNF formula φ over the n variables xi for 1 ≤ i ≤ n, we construct two sets of binary strings S and T such that φ is satisﬁable if and only if there exists a shufﬂe string u that is a subsequence of every string in S and of no string in T . [sent-344, score-1.736]
</p><p>56 The set T is the strings in the set Tn together with additional strings determined by the clauses of φ. [sent-346, score-0.514]
</p><p>57 By Lemma 8, the strings consistent with Sn and Tn are the 2n strings in An . [sent-347, score-0.514]
</p><p>58 We use each u = u1 du2 d · · · un d in An to represent an assignment to the n variables xi by choosing xi = 0 if ui is 00000 and xi = 1 if ui = 00100. [sent-348, score-0.403]
</p><p>59 We construct additional elements of T based on the clauses of the formula φ to exclude any strings representing assignments that do not satisfy φ. [sent-349, score-0.313]
</p><p>60 The strings in An that are subsequences of t j are exactly those that correspond to assignments that falsify clause j of φ, and adding t j to T eliminates these strings from those consistent with S and T . [sent-351, score-0.648]
</p><p>61 By adding one string t j to T for each clause j of φ, we ensure that the only strings u that are subsequences of both elements of S and not subsequences of any element of T are exactly those elements of An that correspond to assignments that do not falsify any clause of φ. [sent-352, score-0.847]
</p><p>62 Thus, there exists at least one string u that is a subsequence of both strings in S and not a subsequence of any string in T if and only if φ is satisﬁable. [sent-353, score-1.601]
</p><p>63 Note that S contains two strings of length O(n), Tn contains 4n strings of length O(n), and T additionally contains one string of length O(n) for each clause of φ, so the sizes of S and T are polynomial in the size of φ. [sent-354, score-1.067]
</p><p>64 Thus, the class of shufﬂe ideals faces the same cryptographic limitations on PAC learnability as demonstrated by Kearns and Valiant for the class of general regular languages represented by deterministic ﬁnite automata. [sent-359, score-0.511]
</p><p>65 Thus, an OR of m inputs is equivalent to a threshold function with threshold 1, and an AND of m inputs is equivalent to a threshold function with threshold m. [sent-362, score-0.354]
</p><p>66 For d > 0, the formulas of depth d consist of a threshold function with m inputs applied to a sequence of m formulas of depth d − 1. [sent-373, score-0.36]
</p><p>67 The string alphabet consists of the symbols 0 and 1 and a set of d + 1 delimiters: #0 , #1 , . [sent-380, score-0.433]
</p><p>68 In this case, the shufﬂe string is r0 ( f ) = y1 #0 y2 #0 . [sent-385, score-0.376]
</p><p>69 If the assignment a is given by a binary string a1 a2 . [sent-391, score-0.438]
</p><p>70 an , indicating that xi is assigned the value ai , then the string representing the assignment is just s0 (a) = a1 #0 a2 #0 . [sent-394, score-0.464]
</p><p>71 It is clear that r0 ( f ) is a subsequence of s0 (a) if and only if the n occurrences of #0 in each string are matched, and y j is a subsequence of a j for all j = 1, 2, . [sent-398, score-0.968]
</p><p>72 Thus, when f is a constant or a literal, r0 ( f ) is a subsequence of s0 (a) if and only if a satisﬁes f . [sent-405, score-0.296]
</p><p>73 In addition to deﬁning the shufﬂe string and the assignment strings at each level, we also deﬁne a slack string. [sent-406, score-0.735]
</p><p>74 For level 0, the slack string z0 is deﬁned as follows. [sent-407, score-0.416]
</p><p>75 z0 = (01#0 )n , That is, z0 consists of n repetitions of the string 01#0 . [sent-408, score-0.376]
</p><p>76 For level d, the slack string is designed to ensure that rd ( f ) is a subsequence of zd for any f ∈ T (n, m, d); this clearly holds at level d = 0. [sent-409, score-0.84]
</p><p>77 , fm ), where each fi is a depth d − 1 threshold formula and θ is a threshold function with threshold t. [sent-418, score-0.358]
</p><p>78 We deﬁne the shufﬂe string rd ( f ) = u1 u1 u2 u2 · · · um um (#d )2t , where for each i = 1, 2, . [sent-419, score-0.427]
</p><p>79 Given an assignment a to the variables Vn , we deﬁne a level d assignment string sd (a) = v2m , where v = sd−1 (a)#d zd−1 #d . [sent-428, score-0.61]
</p><p>80 1525  A NGLUIN , A SPNES , E ISENSTAT AND KONTOROVICH  That is, sd (a) is 2m copies of the string v consisting of the level d − 1 code for a, followed by #d , followed by the level d − 1 slack string, followed by #d . [sent-429, score-0.575]
</p><p>81 Finally, the level d slack string is deﬁned as follows. [sent-431, score-0.416]
</p><p>82 A straightforward induction shows that for any threshold formula f in T (n, m, d), rd ( f ) is a subsequence of zd , and for any assignment a to the variables, sd (a) is also a subsequence of zd . [sent-433, score-1.073]
</p><p>83 Lemma 9 For all threshold formulas f in T (n, m, d) and assignments a to the variables in Vn , a satisﬁes f if and only if rd ( f ) is a subsequence of sd (a). [sent-434, score-0.63]
</p><p>84 For d = 0, the basis construction showed that for all constants or literals f and assignments a, a satisﬁes f if and only if r0 ( f ) is a subsequence of s0 (a). [sent-436, score-0.347]
</p><p>85 , fm ), where each fi is a depth d − 1 threshold formula and θ is a threshold function with threshold t. [sent-441, score-0.358]
</p><p>86 Because rd−1 ( fi ) is a subsequence of the slack string zd−1 , ui ui is a subsequence of vv. [sent-443, score-1.284]
</p><p>87 Also, ui ui is a subsequence of v if and only if rd−1 ( fi ) is a subsequence of sd−1 (a), which holds if and only if a satisﬁes fi , by the inductive assumption. [sent-444, score-0.914]
</p><p>88 If ui ui is not a subsequence of v, then a leftmost embedding of ui ui in vv must match the ﬁrst #d in ui ui to the second #d in vv and the second #d in ui ui to the fourth #d in vv, thereby “consuming” all of vv for the embedding. [sent-445, score-1.493]
</p><p>89 By the inductive assumption, this means that rd−1 ( fi ) is a subsequence of sd−1 (a) for each i ∈ T . [sent-448, score-0.342]
</p><p>90 For i ∈ T , ui ui is a subsequence of vv but not of v. [sent-450, score-0.559]
</p><p>91 Conversely, suppose that rd ( f ) is a subsequence of sd (a), and consider a leftmost embedding. [sent-453, score-0.592]
</p><p>92 Considering the segments ui ui of rd ( f ) from left to right, we see that the leftmost embedding consumes one copy of v if a satisﬁes fi and two copies if a does not satisfy fi . [sent-454, score-0.6]
</p><p>93 Each is a subsequence of zd , and for m ≥ 2, the length of zd is bounded by (10m)d (3n). [sent-458, score-0.476]
</p><p>94 The ﬁrst result assumes a polynomial time algorithm to learn shufﬂe ideals over some ﬁxed alphabet. [sent-461, score-0.27]
</p><p>95 Theorem 10 Suppose for some positive integer d, there exists a polynomial time algorithm to PAC learn shufﬂe ideals over an alphabet of size d + 2. [sent-462, score-0.327]
</p><p>96 The second result assumes a polynomial time algorithm to learn shufﬂe ideals over an arbitrary ﬁnite alphabet, where the dependence on the alphabet size must be at most exponential. [sent-464, score-0.327]
</p><p>97 Theorem 11 Suppose there exists an algorithm to PAC learn shufﬂe ideals over arbitrary ﬁnite alphabets that runs in time polynomial in n and Cs , where n is a bound on the length of examples, s is the alphabet size and C is a ﬁxed constant. [sent-465, score-0.353]
</p><p>98 The assignment strings for the assignment a = 001 are as follows. [sent-476, score-0.381]
</p><p>99 Assignment a satisﬁes f and r2 ( f ) is a subsequence of s2 (a). [sent-478, score-0.296]
</p><p>100 Discussion We have shown that the class of shufﬂe ideals is not efﬁciently properly PAC learnable if RP = NP, and is not efﬁciently improperly PAC learnable under certain cryptographic assumptions. [sent-480, score-0.443]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('string', 0.376), ('pac', 0.352), ('subsequence', 0.296), ('shuf', 0.29), ('strings', 0.257), ('ideals', 0.216), ('kontorovich', 0.211), ('ta', 0.168), ('tb', 0.165), ('leftmost', 0.135), ('ui', 0.115), ('sd', 0.11), ('sq', 0.105), ('doi', 0.102), ('languages', 0.099), ('aryeh', 0.095), ('earnability', 0.095), ('huffle', 0.095), ('isenstat', 0.095), ('ngluin', 0.095), ('spnes', 0.095), ('dana', 0.09), ('automata', 0.088), ('issn', 0.081), ('kearns', 0.079), ('pr', 0.079), ('zd', 0.077), ('threshold', 0.075), ('learnability', 0.073), ('formulas', 0.071), ('cryptographic', 0.063), ('angluin', 0.063), ('tolerance', 0.063), ('assignment', 0.062), ('regular', 0.06), ('learnable', 0.06), ('queries', 0.059), ('depth', 0.058), ('alphabet', 0.057), ('span', 0.057), ('rightmost', 0.056), ('polynomial', 0.054), ('aspnes', 0.053), ('symbol', 0.052), ('deals', 0.051), ('rd', 0.051), ('mehryar', 0.05), ('language', 0.049), ('copies', 0.049), ('query', 0.049), ('dfa', 0.049), ('fi', 0.046), ('clause', 0.045), ('pre', 0.044), ('properly', 0.044), ('occurrence', 0.044), ('embedding', 0.043), ('delimiters', 0.042), ('pitt', 0.042), ('valiant', 0.041), ('automaton', 0.041), ('slack', 0.04), ('mohri', 0.038), ('leonid', 0.037), ('branching', 0.035), ('subsequences', 0.035), ('ideal', 0.034), ('vv', 0.033), ('rp', 0.033), ('un', 0.033), ('tn', 0.032), ('corinna', 0.032), ('delimiter', 0.032), ('sarah', 0.032), ('formula', 0.029), ('boolean', 0.029), ('isbn', 0.028), ('np', 0.028), ('assignments', 0.027), ('inputs', 0.027), ('vazirani', 0.027), ('falsify', 0.027), ('eisenstat', 0.027), ('parekh', 0.027), ('dui', 0.027), ('alt', 0.026), ('november', 0.026), ('yale', 0.026), ('xi', 0.026), ('length', 0.026), ('linguistics', 0.025), ('bshouty', 0.024), ('literal', 0.024), ('literals', 0.024), ('morphological', 0.024), ('palmer', 0.024), ('heidelberg', 0.024), ('ul', 0.022), ('cortes', 0.022), ('james', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="78-tfidf-1" href="./jmlr-2013-On_the_Learnability_of_Shuffle_Ideals.html">78 jmlr-2013-On the Learnability of Shuffle Ideals</a></p>
<p>Author: Dana Angluin, James Aspnes, Sarah Eisenstat, Aryeh Kontorovich</p><p>Abstract: PAC learning of unrestricted regular languages is long known to be a difﬁcult problem. The class of shufﬂe ideals is a very restricted subclass of regular languages, where the shufﬂe ideal generated by a string u is the collection of all strings containing u as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shufﬂe ideals appears quite difﬁcult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP = NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efﬁcient algorithm for properly learning shufﬂe ideals in the statistical query (and therefore also PAC) model under the uniform distribution. Keywords: PAC learning, statistical queries, regular languages, deterministic ﬁnite automata, shufﬂe ideals, subsequences</p><p>2 0.18735556 <a title="78-tfidf-2" href="./jmlr-2013-Learning_Trees_from_Strings%3A_A_Strong_Learning_Algorithm_for_some_Context-Free_Grammars.html">63 jmlr-2013-Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars</a></p>
<p>Author: Alexander Clark</p><p>Abstract: Standard models of language learning are concerned with weak learning: the learner, receiving as input only information about the strings in the language, must learn to generalise and to generate the correct, potentially inﬁnite, set of strings generated by some target grammar. Here we deﬁne the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees. We formalise this using a modiﬁcation of Gold’s identiﬁcation in the limit model, requiring convergence to a grammar that is isomorphic to the target grammar. We take as our starting point a simple learning algorithm for substitutable context-free languages, based on principles of distributional learning, and modify it so that it will converge to a canonical grammar for each language. We prove a corresponding strong learning result for a subclass of context-free grammars. Keywords: context-free grammars, grammatical inference, identiﬁcation in the limit, structure learning</p><p>3 0.15347983 <a title="78-tfidf-3" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<p>Author: Reza Bosagh Zadeh, Ashish Goel</p><p>Abstract: We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high-dimensional sparse vectors. All of our results are provably independent of dimension, meaning that apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension; thus the dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard similarity measures. For Jaccard similarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems with large scale experiments using data from the social networking site Twitter. At time of writing, our algorithms are live in production at twitter.com. Keywords: cosine, Jaccard, overlap, dice, similarity, MapReduce, dimension independent</p><p>4 0.076077551 <a title="78-tfidf-4" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>Author: Philip M. Long, Rocco A. Servedio</p><p>Abstract: We consider the problem of learning an unknown large-margin halfspace in the context of parallel computation, giving both positive and negative results. As our main positive result, we give a parallel algorithm for learning a large-margin halfspace, based on an algorithm of Nesterov’s that performs gradient descent with a momentum term. We show that this algorithm can learn an unknown γ-margin halfspace over n dimensions using ˜ n · poly(1/γ) processors and running in time O(1/γ) + O(log n). In contrast, naive parallel algorithms that learn a γ-margin halfspace in time that depends polylogarithmically on n have an inverse quadratic running time dependence on the margin parameter γ. Our negative result deals with boosting, which is a standard approach to learning large-margin halfspaces. We prove that in the original PAC framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized. More precisely, we show that, if the algorithm is allowed to call the weak learner multiple times in parallel within a single boosting stage, this ability does not reduce the overall number of successive stages of boosting needed for learning by even a single stage. Our proof is information-theoretic and does not rely on unproven assumptions. Keywords: PAC learning, parallel learning algorithms, halfspace learning, linear classiﬁers</p><p>5 0.071742453 <a title="78-tfidf-5" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>Author: Pekka Parviainen, Mikko Koivisto</p><p>Abstract: We consider the problem of ﬁnding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space 2n , to within a factor polynomial in the number of nodes n. In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: ﬁrst, the user may trade space against time; second, the proposed algorithms easily and efﬁciently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order P on the nodes, an optimal DAG compatible with P can be found in time and space roughly proportional to the number of ideals of P , which can be signiﬁcantly less than 2n . Considering sufﬁciently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders. Keywords: exact algorithm, parallelization, partial order, space-time tradeoff, structure learning</p><p>6 0.070350282 <a title="78-tfidf-6" href="./jmlr-2013-Query_Induction_with_Schema-Guided_Pruning_Strategies.html">91 jmlr-2013-Query Induction with Schema-Guided Pruning Strategies</a></p>
<p>7 0.063708872 <a title="78-tfidf-7" href="./jmlr-2013-Efficient_Program_Synthesis_Using_Constraint_Satisfaction_in_Inductive_Logic_Programming.html">40 jmlr-2013-Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming</a></p>
<p>8 0.043095108 <a title="78-tfidf-8" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>9 0.042821743 <a title="78-tfidf-9" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>10 0.042052258 <a title="78-tfidf-10" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>11 0.034870323 <a title="78-tfidf-11" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<p>12 0.030799085 <a title="78-tfidf-12" href="./jmlr-2013-Ranked_Bandits_in_Metric_Spaces%3A_Learning_Diverse_Rankings_over_Large_Document_Collections.html">94 jmlr-2013-Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections</a></p>
<p>13 0.029879658 <a title="78-tfidf-13" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>14 0.029740604 <a title="78-tfidf-14" href="./jmlr-2013-Tapkee%3A_An_Efficient_Dimension_Reduction_Library.html">112 jmlr-2013-Tapkee: An Efficient Dimension Reduction Library</a></p>
<p>15 0.029497094 <a title="78-tfidf-15" href="./jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning.html">35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</a></p>
<p>16 0.029103294 <a title="78-tfidf-16" href="./jmlr-2013-On_the_Convergence_of_Maximum_Variance_Unfolding.html">77 jmlr-2013-On the Convergence of Maximum Variance Unfolding</a></p>
<p>17 0.029009668 <a title="78-tfidf-17" href="./jmlr-2013-Parallel_Vector_Field_Embedding.html">86 jmlr-2013-Parallel Vector Field Embedding</a></p>
<p>18 0.028891277 <a title="78-tfidf-18" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>19 0.028717404 <a title="78-tfidf-19" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>20 0.027928445 <a title="78-tfidf-20" href="./jmlr-2013-A_Theory_of_Multiclass_Boosting.html">8 jmlr-2013-A Theory of Multiclass Boosting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.138), (1, 0.065), (2, -0.014), (3, 0.005), (4, -0.047), (5, -0.006), (6, 0.132), (7, 0.026), (8, -0.309), (9, 0.191), (10, -0.006), (11, -0.303), (12, -0.115), (13, 0.327), (14, -0.35), (15, -0.008), (16, -0.018), (17, -0.053), (18, -0.043), (19, 0.013), (20, -0.069), (21, -0.131), (22, -0.029), (23, 0.041), (24, -0.099), (25, 0.003), (26, -0.055), (27, -0.027), (28, -0.088), (29, 0.005), (30, -0.101), (31, 0.059), (32, 0.019), (33, -0.064), (34, 0.054), (35, 0.08), (36, -0.001), (37, 0.014), (38, 0.02), (39, -0.022), (40, -0.024), (41, 0.054), (42, 0.019), (43, 0.028), (44, 0.002), (45, 0.043), (46, -0.006), (47, -0.01), (48, 0.029), (49, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.969405 <a title="78-lsi-1" href="./jmlr-2013-On_the_Learnability_of_Shuffle_Ideals.html">78 jmlr-2013-On the Learnability of Shuffle Ideals</a></p>
<p>Author: Dana Angluin, James Aspnes, Sarah Eisenstat, Aryeh Kontorovich</p><p>Abstract: PAC learning of unrestricted regular languages is long known to be a difﬁcult problem. The class of shufﬂe ideals is a very restricted subclass of regular languages, where the shufﬂe ideal generated by a string u is the collection of all strings containing u as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shufﬂe ideals appears quite difﬁcult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP = NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efﬁcient algorithm for properly learning shufﬂe ideals in the statistical query (and therefore also PAC) model under the uniform distribution. Keywords: PAC learning, statistical queries, regular languages, deterministic ﬁnite automata, shufﬂe ideals, subsequences</p><p>2 0.82139647 <a title="78-lsi-2" href="./jmlr-2013-Learning_Trees_from_Strings%3A_A_Strong_Learning_Algorithm_for_some_Context-Free_Grammars.html">63 jmlr-2013-Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars</a></p>
<p>Author: Alexander Clark</p><p>Abstract: Standard models of language learning are concerned with weak learning: the learner, receiving as input only information about the strings in the language, must learn to generalise and to generate the correct, potentially inﬁnite, set of strings generated by some target grammar. Here we deﬁne the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees. We formalise this using a modiﬁcation of Gold’s identiﬁcation in the limit model, requiring convergence to a grammar that is isomorphic to the target grammar. We take as our starting point a simple learning algorithm for substitutable context-free languages, based on principles of distributional learning, and modify it so that it will converge to a canonical grammar for each language. We prove a corresponding strong learning result for a subclass of context-free grammars. Keywords: context-free grammars, grammatical inference, identiﬁcation in the limit, structure learning</p><p>3 0.61522847 <a title="78-lsi-3" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<p>Author: Reza Bosagh Zadeh, Ashish Goel</p><p>Abstract: We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high-dimensional sparse vectors. All of our results are provably independent of dimension, meaning that apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension; thus the dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard similarity measures. For Jaccard similarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems with large scale experiments using data from the social networking site Twitter. At time of writing, our algorithms are live in production at twitter.com. Keywords: cosine, Jaccard, overlap, dice, similarity, MapReduce, dimension independent</p><p>4 0.30153972 <a title="78-lsi-4" href="./jmlr-2013-Efficient_Program_Synthesis_Using_Constraint_Satisfaction_in_Inductive_Logic_Programming.html">40 jmlr-2013-Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming</a></p>
<p>Author: John Ahlgren, Shiu Yin Yuen</p><p>Abstract: We present NrSample, a framework for program synthesis in inductive logic programming. NrSample uses propositional logic constraints to exclude undesirable candidates from the search. This is achieved by representing constraints as propositional formulae and solving the associated constraint satisfaction problem. We present a variety of such constraints: pruning, input-output, functional (arithmetic), and variable splitting. NrSample is also capable of detecting search space exhaustion, leading to further speedups in clause induction and optimality. We benchmark NrSample against enumeration search (Aleph’s default) and Progol’s A∗ search in the context of program synthesis. The results show that, on large program synthesis problems, NrSample induces between 1 and 1358 times faster than enumeration (236 times faster on average), always with similar or better accuracy. Compared to Progol A∗ , NrSample is 18 times faster on average with similar or better accuracy except for two problems: one in which Progol A∗ substantially sacriﬁced accuracy to induce faster, and one in which Progol A∗ was a clear winner. Functional constraints provide a speedup of up to 53 times (21 times on average) with similar or better accuracy. We also benchmark using a few concept learning (non-program synthesis) problems. The results indicate that without strong constraints, the overhead of solving constraints is not compensated for. Keywords: inductive logic programming, program synthesis, theory induction, constraint satisfaction, Boolean satisﬁability problem</p><p>5 0.25192317 <a title="78-lsi-5" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>Author: Pekka Parviainen, Mikko Koivisto</p><p>Abstract: We consider the problem of ﬁnding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space 2n , to within a factor polynomial in the number of nodes n. In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: ﬁrst, the user may trade space against time; second, the proposed algorithms easily and efﬁciently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order P on the nodes, an optimal DAG compatible with P can be found in time and space roughly proportional to the number of ideals of P , which can be signiﬁcantly less than 2n . Considering sufﬁciently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders. Keywords: exact algorithm, parallelization, partial order, space-time tradeoff, structure learning</p><p>6 0.24473517 <a title="78-lsi-6" href="./jmlr-2013-Query_Induction_with_Schema-Guided_Pruning_Strategies.html">91 jmlr-2013-Query Induction with Schema-Guided Pruning Strategies</a></p>
<p>7 0.23814005 <a title="78-lsi-7" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>8 0.19515589 <a title="78-lsi-8" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>9 0.19003305 <a title="78-lsi-9" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>10 0.17053367 <a title="78-lsi-10" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>11 0.15619612 <a title="78-lsi-11" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>12 0.14768511 <a title="78-lsi-12" href="./jmlr-2013-Ranked_Bandits_in_Metric_Spaces%3A_Learning_Diverse_Rankings_over_Large_Document_Collections.html">94 jmlr-2013-Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections</a></p>
<p>13 0.1420045 <a title="78-lsi-13" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>14 0.13239923 <a title="78-lsi-14" href="./jmlr-2013-Distribution-Dependent_Sample_Complexity_of_Large_Margin_Learning.html">35 jmlr-2013-Distribution-Dependent Sample Complexity of Large Margin Learning</a></p>
<p>15 0.13038734 <a title="78-lsi-15" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>16 0.12872083 <a title="78-lsi-16" href="./jmlr-2013-Tapkee%3A_An_Efficient_Dimension_Reduction_Library.html">112 jmlr-2013-Tapkee: An Efficient Dimension Reduction Library</a></p>
<p>17 0.12870751 <a title="78-lsi-17" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<p>18 0.12631798 <a title="78-lsi-18" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>19 0.12437458 <a title="78-lsi-19" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>20 0.12422371 <a title="78-lsi-20" href="./jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications.html">18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.012), (5, 0.081), (6, 0.03), (9, 0.538), (10, 0.04), (20, 0.017), (23, 0.02), (41, 0.024), (68, 0.015), (70, 0.029), (75, 0.033), (85, 0.035), (87, 0.018), (89, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.83307159 <a title="78-lda-1" href="./jmlr-2013-On_the_Learnability_of_Shuffle_Ideals.html">78 jmlr-2013-On the Learnability of Shuffle Ideals</a></p>
<p>Author: Dana Angluin, James Aspnes, Sarah Eisenstat, Aryeh Kontorovich</p><p>Abstract: PAC learning of unrestricted regular languages is long known to be a difﬁcult problem. The class of shufﬂe ideals is a very restricted subclass of regular languages, where the shufﬂe ideal generated by a string u is the collection of all strings containing u as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shufﬂe ideals appears quite difﬁcult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP = NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efﬁcient algorithm for properly learning shufﬂe ideals in the statistical query (and therefore also PAC) model under the uniform distribution. Keywords: PAC learning, statistical queries, regular languages, deterministic ﬁnite automata, shufﬂe ideals, subsequences</p><p>2 0.82376492 <a title="78-lda-2" href="./jmlr-2013-Efficient_Program_Synthesis_Using_Constraint_Satisfaction_in_Inductive_Logic_Programming.html">40 jmlr-2013-Efficient Program Synthesis Using Constraint Satisfaction in Inductive Logic Programming</a></p>
<p>Author: John Ahlgren, Shiu Yin Yuen</p><p>Abstract: We present NrSample, a framework for program synthesis in inductive logic programming. NrSample uses propositional logic constraints to exclude undesirable candidates from the search. This is achieved by representing constraints as propositional formulae and solving the associated constraint satisfaction problem. We present a variety of such constraints: pruning, input-output, functional (arithmetic), and variable splitting. NrSample is also capable of detecting search space exhaustion, leading to further speedups in clause induction and optimality. We benchmark NrSample against enumeration search (Aleph’s default) and Progol’s A∗ search in the context of program synthesis. The results show that, on large program synthesis problems, NrSample induces between 1 and 1358 times faster than enumeration (236 times faster on average), always with similar or better accuracy. Compared to Progol A∗ , NrSample is 18 times faster on average with similar or better accuracy except for two problems: one in which Progol A∗ substantially sacriﬁced accuracy to induce faster, and one in which Progol A∗ was a clear winner. Functional constraints provide a speedup of up to 53 times (21 times on average) with similar or better accuracy. We also benchmark using a few concept learning (non-program synthesis) problems. The results indicate that without strong constraints, the overhead of solving constraints is not compensated for. Keywords: inductive logic programming, program synthesis, theory induction, constraint satisfaction, Boolean satisﬁability problem</p><p>3 0.25782928 <a title="78-lda-3" href="./jmlr-2013-Learning_Trees_from_Strings%3A_A_Strong_Learning_Algorithm_for_some_Context-Free_Grammars.html">63 jmlr-2013-Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars</a></p>
<p>Author: Alexander Clark</p><p>Abstract: Standard models of language learning are concerned with weak learning: the learner, receiving as input only information about the strings in the language, must learn to generalise and to generate the correct, potentially inﬁnite, set of strings generated by some target grammar. Here we deﬁne the corresponding notion of strong learning: the learner, again only receiving strings as input, must learn a grammar that generates the correct set of structures or parse trees. We formalise this using a modiﬁcation of Gold’s identiﬁcation in the limit model, requiring convergence to a grammar that is isomorphic to the target grammar. We take as our starting point a simple learning algorithm for substitutable context-free languages, based on principles of distributional learning, and modify it so that it will converge to a canonical grammar for each language. We prove a corresponding strong learning result for a subclass of context-free grammars. Keywords: context-free grammars, grammatical inference, identiﬁcation in the limit, structure learning</p><p>4 0.23434523 <a title="78-lda-4" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>Author: Kamalika Chaudhuri, Anand D. Sarwate, Kaushik Sinha</p><p>Abstract: The principal components analysis (PCA) algorithm is a standard tool for identifying good lowdimensional approximations to high-dimensional data. Many data sets of interest contain private or sensitive information about individuals. Algorithms which operate on such data should be sensitive to the privacy risks in publishing their outputs. Differential privacy is a framework for developing tradeoffs between privacy and the utility of these outputs. In this paper we investigate the theory and empirical performance of differentially private approximations to PCA and propose a new method which explicitly optimizes the utility of the output. We show that the sample complexity of the proposed method differs from the existing procedure in the scaling with the data dimension, and that our method is nearly optimal in terms of this scaling. We furthermore illustrate our results, showing that on real data there is a large performance gap between the existing method and our method. Keywords: differential privacy, principal components analysis, dimension reduction</p><p>5 0.22208388 <a title="78-lda-5" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>Author: Philip M. Long, Rocco A. Servedio</p><p>Abstract: We consider the problem of learning an unknown large-margin halfspace in the context of parallel computation, giving both positive and negative results. As our main positive result, we give a parallel algorithm for learning a large-margin halfspace, based on an algorithm of Nesterov’s that performs gradient descent with a momentum term. We show that this algorithm can learn an unknown γ-margin halfspace over n dimensions using ˜ n · poly(1/γ) processors and running in time O(1/γ) + O(log n). In contrast, naive parallel algorithms that learn a γ-margin halfspace in time that depends polylogarithmically on n have an inverse quadratic running time dependence on the margin parameter γ. Our negative result deals with boosting, which is a standard approach to learning large-margin halfspaces. We prove that in the original PAC framework, in which a weak learning algorithm is provided as an oracle that is called by the booster, boosting cannot be parallelized. More precisely, we show that, if the algorithm is allowed to call the weak learner multiple times in parallel within a single boosting stage, this ability does not reduce the overall number of successive stages of boosting needed for learning by even a single stage. Our proof is information-theoretic and does not rely on unproven assumptions. Keywords: PAC learning, parallel learning algorithms, halfspace learning, linear classiﬁers</p><p>6 0.21559785 <a title="78-lda-6" href="./jmlr-2013-Divvy%3A_Fast_and_Intuitive_Exploratory_Data_Analysis.html">37 jmlr-2013-Divvy: Fast and Intuitive Exploratory Data Analysis</a></p>
<p>7 0.21100372 <a title="78-lda-7" href="./jmlr-2013-Using_Symmetry_and_Evolutionary_Search_to_Minimize_Sorting_Networks.html">118 jmlr-2013-Using Symmetry and Evolutionary Search to Minimize Sorting Networks</a></p>
<p>8 0.20928594 <a title="78-lda-8" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>9 0.2005112 <a title="78-lda-9" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>10 0.19944064 <a title="78-lda-10" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>11 0.19886956 <a title="78-lda-11" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>12 0.19695307 <a title="78-lda-12" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>13 0.19585708 <a title="78-lda-13" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>14 0.19581044 <a title="78-lda-14" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>15 0.19559555 <a title="78-lda-15" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>16 0.19529793 <a title="78-lda-16" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>17 0.1949584 <a title="78-lda-17" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>18 0.19379885 <a title="78-lda-18" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>19 0.19368289 <a title="78-lda-19" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>20 0.19300313 <a title="78-lda-20" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
