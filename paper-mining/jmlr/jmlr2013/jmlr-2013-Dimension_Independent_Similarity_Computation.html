<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>33 jmlr-2013-Dimension Independent Similarity Computation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-33" href="#">jmlr2013-33</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>33 jmlr-2013-Dimension Independent Similarity Computation</h1>
<br/><p>Source: <a title="jmlr-2013-33-pdf" href="http://jmlr.org/papers/volume14/bosagh-zadeh13a/bosagh-zadeh13a.pdf">pdf</a></p><p>Author: Reza Bosagh Zadeh, Ashish Goel</p><p>Abstract: We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high-dimensional sparse vectors. All of our results are provably independent of dimension, meaning that apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension; thus the dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard similarity measures. For Jaccard similarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems with large scale experiments using data from the social networking site Twitter. At time of writing, our algorithms are live in production at twitter.com. Keywords: cosine, Jaccard, overlap, dice, similarity, MapReduce, dimension independent</p><p>Reference: <a title="jmlr-2013-33-reference" href="../jmlr2013_reference/jmlr-2013-Dimension_Independent_Similarity_Computation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We study Cosine, Dice, Overlap, and the Jaccard similarity measures. [sent-5, score-0.225]
</p><p>2 For Jaccard similarity we include an improved version of MinHash. [sent-6, score-0.225]
</p><p>3 Introduction Computing similarity between all pairs of vectors in large-scale data sets is a challenge. [sent-12, score-0.326]
</p><p>4 Consider the problem of ﬁnding all pairs of similarities between D indicator (0/1 entries) vectors, each of dimension N. [sent-18, score-0.211]
</p><p>5 In particular we focus on cosine similarities between all pairs of D vectors in RN . [sent-19, score-0.302]
</p><p>6 For example, typical values to compute similarities between all pairs of a subset of Twitter users can be: N = 109 (the universe of Twitter users), D = 107 (a subset of Twitter users), L = 1000. [sent-21, score-0.226]
</p><p>7 In particular, in addition to correctness, we prove that to estimate similarities above ε, the shufﬂe size of the DISCO scheme is only O(DL log(D)/ε), with no dependence on the dimension N, hence the name. [sent-34, score-0.186]
</p><p>8 This means as long as there are enough mappers to read the data, one can use the DISCO sampling scheme to make the shufﬂe size tractable. [sent-35, score-0.178]
</p><p>9 We have also used the scheme to ﬁnd highly similar pairs of words by taking each dimension to be the indicator vector that signals in which tweets the word appears. [sent-38, score-0.335]
</p><p>10 Our sampling scheme can be used to implement many other similarity measures. [sent-40, score-0.262]
</p><p>11 We focus on four similarity measures: Cosine, Dice, Overlap, and the Jaccard similarity measures. [sent-41, score-0.45]
</p><p>12 For Jaccard similarity we present an improved version of the well known MinHash scheme (Broder, 1997). [sent-42, score-0.262]
</p><p>13 Third, we prove results on highly similar pairs, since common applications require thresholding the similarity score with a high threshold value. [sent-49, score-0.247]
</p><p>14 For such applications of similarity, DISCO is particularly helpful since higher similarity pairs are estimated with provably better accuracy. [sent-51, score-0.326]
</p><p>15 We are interested in similarity scores between pairs of words in a dictionary containing D words {w1 , . [sent-81, score-0.454]
</p><p>16 The number of documents in which two words wi and w j co-occur is denoted #(wi , w j ). [sent-85, score-0.226]
</p><p>17 The number of documents in which a single word wi occurs is denoted #(wi ). [sent-86, score-0.254]
</p><p>18 To each word in the dictionary, an N-dimensional indicator vector is assigned, indicating in which documents the word occurs. [sent-87, score-0.206]
</p><p>19 We focus on 5 different similarity measures, including cosine similarity, which is very popular and produces high quality results across different domains (Chien and Immorlica, 2005; Chuang and Chien, 2005; Sahami and Heilman, 2006; Spertus et al. [sent-91, score-0.342]
</p><p>20 Cosine similarity is simply the vector √ normalized dot product: √ #(x,y) where #(x) = ∑N x[i] and #(x, y) = ∑N x[i]y[i]. [sent-93, score-0.278]
</p><p>21 In addition to i=1 i=1 #(x)  #(y)  cosine similarity, we consider many variations of similarity scores that use the dot product. [sent-94, score-0.419]
</p><p>22 We focus on shufﬂe size because after one trivially reads in the data via mappers, the shufﬂe phase is the bottleneck since our mappers and reducers are all linear in their input size. [sent-100, score-0.23]
</p><p>23 We assume that the dictionary can ﬁt into memory but that the number of dimensions (documents) is so large that many machines will be needed to even hold the documents on disk. [sent-113, score-0.182]
</p><p>24 We are interested in several similarity measures outlined in Table 1. [sent-120, score-0.225]
</p><p>25 Our goal is to compute similarity scores between all pairs of words, and prove accuracy results for pairs which have similarity above ε. [sent-122, score-0.676]
</p><p>26 The naive approach is to ﬁrst compute the dot product between all pairs of words in a Map-Reduce (Dean and Ghemawat, 2008) style framework. [sent-123, score-0.259]
</p><p>27 Mappers act on each document, and emit key-value pairs of the form (wi , w j ) → 1. [sent-124, score-0.242]
</p><p>28 The difﬁculty of computing the similarity scores lies almost entirely in computing #(wi , w j ). [sent-127, score-0.249]
</p><p>29 Instead of emitting every pair, only some pairs are output, and instead of computing intermediary dot products, we directly compute the similarity score. [sent-133, score-0.401]
</p><p>30 Related Work Previously the all-pairs similarity search problem has been studied (Broder et al. [sent-143, score-0.225]
</p><p>31 There are many papers discussing the all pairs similarity computation problem. [sent-150, score-0.326]
</p><p>32 The all-pairs similarity search problem has also been addressed in the database community, where it is known as the similarity join problem (Arasu et al. [sent-154, score-0.45]
</p><p>33 (2010), the authors consider the problem of ﬁnding highly similar pairs of documents in MapReduce. [sent-160, score-0.213]
</p><p>34 In particular, there are two problems with pairwise similarity computation, large computation time with respect to dimensions, which we address, and large computation time with respect to the number of points. [sent-168, score-0.225]
</p><p>35 , 2009) proposed a highly scalable term similarity algorithm, implemented in the MapReduce framework, and deployed an over 200 billion word crawl of the Web to compute pairwise similarities between terms. [sent-170, score-0.389]
</p><p>36 Algorithms and Methods The Naive algorithm for computing similarities ﬁrst computes dot products, then simply divides by whatever is necessary to obtain a similarity score. [sent-179, score-0.362]
</p><p>37 Reduce using the NaiveReducer (Algorithm 2) 1610  D IMENSION I NDEPENDENT S IMILARITY C OMPUTATION  Algorithm 1 NaiveMapper(t) for all pairs (w1 , w2 ) in t do emit ((w1 , w2 ) → 1) end for  Algorithm 2 NaiveReducer((w1 , w2 ), r1 , . [sent-184, score-0.242]
</p><p>38 , rR ) a = ∑R ri i=1 output √  a #(w1 )#(w2 )  The above steps will compute all dot products, which will then be scaled by appropriate factors for each of the similarity scores. [sent-187, score-0.278]
</p><p>39 Instead of using the Naive algorithm, we modify the mapper of the naive algorithm and replace it with Algorithm 3, and replace the reducer with Algorithm 4 to directly compute the actual similarity score, rather than the dot products. [sent-188, score-0.397]
</p><p>40 The following sections detail how to obtain dimensionality independence for each of the similarity scores. [sent-189, score-0.225]
</p><p>41 Algorithm 3 DISCOMapper(t) for all pairs (w1 , w2 ) in t do emit using custom Emit function end for  Algorithm 4 DISCOReducer((w1 , w2 ), r1 , . [sent-190, score-0.242]
</p><p>42 Algorithm 5 CosineSampleEmit(w1 , w2 ) - p/ε is the oversampling parameter With probability p 1 ε #(w1 ) #(w2 ) emit ((w1 , w2 ) → 1) Note that the more popular a word is, the less likely it is to be output. [sent-195, score-0.225]
</p><p>43 Since the CosineSampleEmit algorithm is only guaranteed to produce the correct similarity score in expectation, we must show that the expected value is highly likely to be obtained. [sent-199, score-0.247]
</p><p>44 In Theorem 2 we prove that any algorithm that purports to accurately calculate highly similar pairs must at least output them, and sometimes there are at least DL such pairs, and so any algorithm that is accurate on highly similar pairs must have at least DL shufﬂe size. [sent-215, score-0.246]
</p><p>45 In each group, it is trivial to check that all pairs of words of have similarity exactly 1. [sent-226, score-0.356]
</p><p>46 There are L pairs for 2 each group and there are D/L groups, making for a total of (D/L) L = Ω(DL) pairs with similarity 2 1, and thus also at least ε. [sent-227, score-0.427]
</p><p>47 Similar corrections have to be made for the other similarity scores (Dice and Overlap, but not MinHash), so we do not repeat this point. [sent-232, score-0.249]
</p><p>48 2 Jaccard Similarity Traditionally MinHash (Broder, 1997) is used to compute Jaccard similarity scores between all pairs in a dictionary. [sent-239, score-0.35]
</p><p>49 Let h(t) be a hash function that maps documents to distinct numbers in [0, 1], and for any word w deﬁne g(w) (called the MinHash of w) to be the minimum value of h(t) over all t that contain w. [sent-241, score-0.314]
</p><p>50 Then g(w1 ) = g(w2 ) exactly when the minimum hash value of the union with size #(w1 ) + #(w2 ) − #(w1 , w2 ) lies in the intersection with size #(w1 , w2 ). [sent-242, score-0.244]
</p><p>51 Thus Pr[g(w1 ) = g(w2 )] =  #(w1 , w2 ) = Jac(w1 , w2 ) #(w1 ) + #(w2 ) − #(w1 , w2 )  Therefore the indicator random variable that is 1 when g(w1 ) = g(w2 ) has expectation equal to the Jaccard similarity between the two words. [sent-243, score-0.225]
</p><p>52 The idea of the MinHash scheme is to reduce the variance by averaging together k of these variables constructed in the same way with k different hash functions. [sent-245, score-0.203]
</p><p>53 a similarity score that is above ε has a relative error of no more than δ. [sent-253, score-0.225]
</p><p>54 Now that minhash values for all hash functions are available, for each pair we can simply compute the number of hash collisions and divide by the total number of hash functions Algorithm 7 MinHashSampleMap(t, w1 , . [sent-272, score-0.746]
</p><p>55 , wL ) for i = 1 to L do for j = 1 to k do if h j (t) ≤ c log(Dk) then #(wi ) emit ((wi , j) → h j (t)) end if end for end for Recall that a document has at most L words. [sent-275, score-0.219]
</p><p>56 After the map phase, for each of the k hash functions, the standard MinReducer is used, which will compute g j (w). [sent-277, score-0.187]
</p><p>57 We now prove that MinHashSampleMap will with high probability Emit the minimum hash value for a given word w and hash function h, thus ensuring the steps following MinHashSampleMap will be unaffected. [sent-281, score-0.39]
</p><p>58 , hk map documents to [0, 1] uniform randomly, and c = 1 3, then with probability at least 1 − (Dk)2 , for all words w and hash functions h ∈ {h1 , . [sent-285, score-0.307]
</p><p>59 , hk }, MinHashSampleMap will emit the document that realizes g(w). [sent-288, score-0.241]
</p><p>60 Proof Fix a word w and hash function h and let z = mint|w∈t h(t). [sent-289, score-0.224]
</p><p>61 Now the probability that MinHashSampleMap will not emit the document that realizes g(w) is Pr z >  c log(Dk) c log(Dk) = 1− #(w) #(w) 1615  #(w)  ≤ e−c log(Dk) =  1 (Dk)c  B OSAGH Z ADEH AND G OEL  Thus for a single w and h we have shown MinHashSampleMap will w. [sent-290, score-0.241]
</p><p>62 To show the same result for all hash functions and words in the dictionary, set 1 c = 3 and use union bound to get a ( Dk )2 bound on the probability of error for any w1 , . [sent-294, score-0.196]
</p><p>63 For the other similarity measures, combining the Naive mappers can only bring down the shufﬂe size to O(mD2 ), which DISCO improves upon asymptotically by obtaining a bound of O(DL/ε log(D)) without even combining, so combining will help even more. [sent-315, score-0.366]
</p><p>64 In this setting, data is streamed dimension-by-dimension through a single machine that can at any time answer queries of the form “what is the similarity between points x and y considering all the input so far? [sent-319, score-0.249]
</p><p>65 Our algorithm will be very similar to the mapreduce setup, but in place of emitting pairs to be shufﬂed for a reduce phase, we instead insert them into a hash map H, keyed by pairs of words, with each entry holding a bag of emissions. [sent-323, score-0.72]
</p><p>66 There are two operations to be described: the update that occurs when a new dimension (document) arrives, and the constant time algorithm used to answer similarity queries. [sent-328, score-0.251]
</p><p>67 Let the query be for the similarity between words x and y. [sent-338, score-0.281]
</p><p>68 We show that the memory used by H when our algorithm is run on C′ with the all-knowing counters dominates (in expectation) the size of H for the original data set in the streaming model, thus achieving the claimed bound in the current theorem statement. [sent-367, score-0.179]
</p><p>69 Using the same analysis used in Theorem 2, the shufﬂe size for C′ is at most O(DL lg(N) log(D)/ε) and therefore so is the size of H for the all-knowing algorithm run on C′ , and, by the analysis above, so is the hash map size for the original data set C. [sent-374, score-0.304]
</p><p>70 Correctness and Shufﬂe Size Proofs for other Similarity Measures We now give consideration to similarity scores other than cosine similarity, along proofs of their shufﬂe size. [sent-376, score-0.366]
</p><p>71 1 Overlap Similarity Overlap similarity follows the same pattern we used for cosine similarity; thus we only explain the parts that are different. [sent-378, score-0.342]
</p><p>72 Algorithm 8 OverlapSampleEmit(w1 , w2 ) - p/ε is the oversampling parameter With probability 1 p ε min(#(w1 ), #(w2 )) emit ((w1 , w2 ) → 1) The correctness proof is nearly identical to that of cosine similarity so we do not restate it. [sent-380, score-0.509]
</p><p>73 2 Dice Similarity Dice similarity follows the same pattern as we used for cosine similarity, thus we only explain the parts that are different. [sent-387, score-0.342]
</p><p>74 Algorithm 9 DiceSampleEmit(w1 , w2 ) - p/ε is the oversampling parameter With probability p 2 ε #(w1 ) + #(w2 ) emit ((w1 , w2 ) → 1) The correctness proof is nearly identical to cosine similarity so we do not restate it. [sent-389, score-0.509]
</p><p>75 1 Similar Users Consider the problem of ﬁnding all pairs of similarities between a subset of D = 107 twitter users. [sent-410, score-0.348]
</p><p>76 The number of dimensions N = 198, 134, 530 in our data is equal to the number of tweets and each tweet is a document with size at most 140 characters, providing a small upper bound for L. [sent-424, score-0.204]
</p><p>77 We ran experiments with D = 106 , but cannot report true error since ﬁnding the true cosine similarities is too computationally intensive. [sent-432, score-0.201]
</p><p>78 5  similarity threshold  Figure 1: Average error for all pairs with similarity ≥ ε. [sent-454, score-0.551]
</p><p>79 4 Error vs Similarity Magnitude All of our theorems report better accuracy for pairs that have higher similarity than otherwise. [sent-459, score-0.326]
</p><p>80 To see this empirically, we plot the average error of all pairs that have true similarity above ε. [sent-460, score-0.326]
</p><p>81 Note that the reason for large portions of the error being constant in these plots is that there are very few pairs with very high similarities, and therefore the error remains constant while ε is between the difference of two such very high similarity pairs. [sent-462, score-0.326]
</p><p>82 Although we use the MapReduce (Dean and Ghemawat, 2008) and Streaming computation models to discuss shufﬂe size and memory, the sampling strategy we use can be generalized to 1621  B OSAGH Z ADEH AND G OEL  DISCO Cosine shuffle size vs accuracy tradeoff 1  2. [sent-468, score-0.222]
</p><p>83 When the true similarity is zero, DISCO always also returns zero, so we always get those pairs right. [sent-478, score-0.326]
</p><p>84 It remains to estimate those pairs with similarity > 0, and that is the average relative error for those pairs that we report here. [sent-479, score-0.427]
</p><p>85 5  similarity threshold  Figure 3: Average error for all pairs with similarity ≥ ε. [sent-493, score-0.551]
</p><p>86 1622  D IMENSION I NDEPENDENT S IMILARITY C OMPUTATION  DISCO Dice shuffle size vs accuracy tradeoff 1  2  0. [sent-502, score-0.183]
</p><p>87 5  0  1  0  2  4  6  8  10  12  14  16  avg relative err  DISCO Shuffle / Naive Shuffle  DISCO / Naive avg relative err  0 18  log(p / ε)  Figure 4: As p/ε increases, shufﬂe size increases and error decreases. [sent-503, score-0.379]
</p><p>88 5  similarity threshold  Figure 5: Average error for all pairs with similarity ≥ ε. [sent-512, score-0.551]
</p><p>89 1623  B OSAGH Z ADEH AND G OEL  DISCO Overlap shuffle size vs accuracy tradeoff 2. [sent-521, score-0.183]
</p><p>90 5  similarity threshold  Figure 7: Average error for all pairs with similarity ≥ ε. [sent-543, score-0.551]
</p><p>91 MinHash Jaccard similarity error decreases for more similar pairs. [sent-544, score-0.225]
</p><p>92 Keyword generation for search engine advertising using semantic similarity between terms. [sent-549, score-0.246]
</p><p>93 5  similarity threshold  Figure 8: Average error for all pairs with similarity ≥ ε. [sent-579, score-0.551]
</p><p>94 DISCO MinHash Jaccard similarity error decreases for more similar pairs. [sent-580, score-0.225]
</p><p>95 Finding associations and computing similarity via biased pair sampling. [sent-611, score-0.249]
</p><p>96 A primitive operator for similarity joins in data cleaning. [sent-623, score-0.225]
</p><p>97 Semantic similarity between search engine queries using temporal correlation. [sent-629, score-0.27]
</p><p>98 Brute force and indexed approaches to pairwise document similarity comparisons with mapreduce. [sent-705, score-0.303]
</p><p>99 A web-based kernel function for measuring the similarity of short text snippets. [sent-737, score-0.225]
</p><p>100 Evaluating similarity measures: a large-scale study in the orkut social network. [sent-750, score-0.252]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('shuf', 0.472), ('disco', 0.367), ('mapreduce', 0.285), ('similarity', 0.225), ('minhash', 0.224), ('hash', 0.166), ('twitter', 0.163), ('emit', 0.141), ('dice', 0.122), ('cosine', 0.117), ('dl', 0.115), ('adeh', 0.112), ('oel', 0.112), ('osagh', 0.112), ('shuffle', 0.112), ('wi', 0.106), ('jac', 0.102), ('mappers', 0.102), ('ndependent', 0.102), ('pairs', 0.101), ('jaccard', 0.096), ('avg', 0.096), ('lg', 0.096), ('cosinesampleemit', 0.092), ('documents', 0.09), ('imension', 0.087), ('imilarity', 0.087), ('streaming', 0.087), ('similarities', 0.084), ('document', 0.078), ('naive', 0.075), ('err', 0.074), ('omputation', 0.072), ('minhashsamplemap', 0.071), ('goel', 0.061), ('hadoop', 0.061), ('reducers', 0.061), ('tweets', 0.061), ('pr', 0.058), ('dk', 0.058), ('word', 0.058), ('broder', 0.055), ('dot', 0.053), ('dean', 0.052), ('ghemawat', 0.051), ('chernoff', 0.05), ('cos', 0.047), ('overlap', 0.046), ('dictionary', 0.044), ('reducer', 0.044), ('users', 0.041), ('minhashmap', 0.041), ('sahami', 0.041), ('zadeh', 0.041), ('emissions', 0.039), ('size', 0.039), ('scheme', 0.037), ('emission', 0.036), ('production', 0.036), ('chien', 0.035), ('stanford', 0.035), ('tradeoff', 0.032), ('log', 0.032), ('counters', 0.031), ('dicesampleemit', 0.031), ('munagala', 0.031), ('olston', 0.031), ('overlapsampleemit', 0.031), ('pig', 0.031), ('spertus', 0.031), ('occurrences', 0.03), ('words', 0.03), ('vldb', 0.029), ('coin', 0.029), ('keyword', 0.029), ('phase', 0.028), ('social', 0.027), ('acm', 0.027), ('dimensions', 0.026), ('indyk', 0.026), ('oversampling', 0.026), ('dimension', 0.026), ('query', 0.026), ('magnitudes', 0.026), ('web', 0.025), ('bag', 0.024), ('scores', 0.024), ('pair', 0.024), ('queries', 0.024), ('memory', 0.022), ('xy', 0.022), ('sigmod', 0.022), ('realizes', 0.022), ('emitting', 0.022), ('highly', 0.022), ('engine', 0.021), ('map', 0.021), ('google', 0.02), ('arasu', 0.02), ('baraglia', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="33-tfidf-1" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<p>Author: Reza Bosagh Zadeh, Ashish Goel</p><p>Abstract: We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high-dimensional sparse vectors. All of our results are provably independent of dimension, meaning that apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension; thus the dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard similarity measures. For Jaccard similarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems with large scale experiments using data from the social networking site Twitter. At time of writing, our algorithms are live in production at twitter.com. Keywords: cosine, Jaccard, overlap, dice, similarity, MapReduce, dimension independent</p><p>2 0.15347983 <a title="33-tfidf-2" href="./jmlr-2013-On_the_Learnability_of_Shuffle_Ideals.html">78 jmlr-2013-On the Learnability of Shuffle Ideals</a></p>
<p>Author: Dana Angluin, James Aspnes, Sarah Eisenstat, Aryeh Kontorovich</p><p>Abstract: PAC learning of unrestricted regular languages is long known to be a difﬁcult problem. The class of shufﬂe ideals is a very restricted subclass of regular languages, where the shufﬂe ideal generated by a string u is the collection of all strings containing u as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shufﬂe ideals appears quite difﬁcult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP = NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efﬁcient algorithm for properly learning shufﬂe ideals in the statistical query (and therefore also PAC) model under the uniform distribution. Keywords: PAC learning, statistical queries, regular languages, deterministic ﬁnite automata, shufﬂe ideals, subsequences</p><p>3 0.064688526 <a title="33-tfidf-3" href="./jmlr-2013-Ranked_Bandits_in_Metric_Spaces%3A_Learning_Diverse_Rankings_over_Large_Document_Collections.html">94 jmlr-2013-Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections</a></p>
<p>Author: Aleksandrs Slivkins, Filip Radlinski, Sreenivas Gollapudi</p><p>Abstract: Most learning to rank research has assumed that the utility of different documents is independent, which results in learned ranking functions that return redundant results. The few approaches that avoid this have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisﬁed users, with several scalable algorithms that explicitly takes document similarity and ranking context into account. Our formulation is a non-trivial common generalization of two multi-armed bandit models from the literature: ranked bandits (Radlinski et al., 2008) and Lipschitz bandits (Kleinberg et al., 2008b). We present theoretical justiﬁcations for this approach, as well as a near-optimal algorithm. Our evaluation adds optimizations that improve empirical performance, and shows that our algorithms learn orders of magnitude more quickly than previous approaches. Keywords: online learning, clickthrough data, diversity, multi-armed bandits, contextual bandits, regret, metric spaces</p><p>4 0.036313608 <a title="33-tfidf-4" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>Author: David Picard, Nicolas Thome, Matthieu Cord</p><p>Abstract: JKernelMachines is a Java library for learning with kernels. It is primarily designed to deal with custom kernels that are not easily found in standard libraries, such as kernels on structured data. These types of kernels are often used in computer vision or bioinformatics applications. We provide several kernels leading to state of the art classiﬁcation performances in computer vision, as well as various kernels on sets. The main focus of the library is to be easily extended with new kernels. Standard SVM optimization algorithms are available, but also more sophisticated learning-based kernel combination methods such as Multiple Kernel Learning (MKL), and a recently published algorithm to learn powered products of similarities (Product Kernel Learning). Keywords: classiﬁcation, support vector machines, kernel, computer vision</p><p>5 0.032873806 <a title="33-tfidf-5" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>Author: Gang Niu, Bo Dai, Lin Shang, Masashi Sugiyama</p><p>Abstract: The large volume principle proposed by Vladimir Vapnik, which advocates that hypotheses lying in an equivalence class with a larger volume are more preferable, is a useful alternative to the large margin principle. In this paper, we introduce a new discriminative clustering model based on the large volume principle called maximum volume clustering (MVC), and then propose two approximation schemes to solve this MVC model: A soft-label MVC method using sequential quadratic programming and a hard-label MVC method using semi-deﬁnite programming, respectively. The proposed MVC is theoretically advantageous for three reasons. The optimization involved in hardlabel MVC is convex, and under mild conditions, the optimization involved in soft-label MVC is akin to a convex one in terms of the resulting clusters. Secondly, the soft-label MVC method pos∗. A preliminary and shorter version has appeared in Proceedings of 14th International Conference on Artiﬁcial Intelligence and Statistics (Niu et al., 2011). The preliminary work was done when GN was studying at Department of Computer Science and Technology, Nanjing University, and BD was studying at Institute of Automation, Chinese Academy of Sciences. A Matlab implementation of maximum volume clustering is available from http://sugiyama-www.cs.titech.ac.jp/∼gang/software.html. c 2013 Gang Niu, Bo Dai, Lin Shang and Masashi Sugiyama. N IU , DAI , S HANG AND S UGIYAMA sesses a clustering error bound. Thirdly, MVC includes the optimization problems of a spectral clustering, two relaxed k-means clustering and an information-maximization clustering as special limit cases when its regularization parameter goes to inﬁnity. Experiments on several artiﬁcial and benchmark data sets demonstrate that the proposed MVC compares favorably with state-of-the-art clustering methods. Keywords: discriminative clustering, large volume principle, sequential quadratic programming, semi-deﬁnite programming, ﬁnite sample stability, clustering error</p><p>6 0.030848062 <a title="33-tfidf-6" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>7 0.029315701 <a title="33-tfidf-7" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>8 0.02809526 <a title="33-tfidf-8" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>9 0.027767105 <a title="33-tfidf-9" href="./jmlr-2013-Similarity-based_Clustering_by_Left-Stochastic_Matrix_Factorization.html">100 jmlr-2013-Similarity-based Clustering by Left-Stochastic Matrix Factorization</a></p>
<p>10 0.025413176 <a title="33-tfidf-10" href="./jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications.html">18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</a></p>
<p>11 0.025384204 <a title="33-tfidf-11" href="./jmlr-2013-Learning_Bilinear_Model_for_Matching_Queries_and_Documents.html">60 jmlr-2013-Learning Bilinear Model for Matching Queries and Documents</a></p>
<p>12 0.025306942 <a title="33-tfidf-12" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>13 0.024144558 <a title="33-tfidf-13" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>14 0.023855893 <a title="33-tfidf-14" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>15 0.023336748 <a title="33-tfidf-15" href="./jmlr-2013-Query_Induction_with_Schema-Guided_Pruning_Strategies.html">91 jmlr-2013-Query Induction with Schema-Guided Pruning Strategies</a></p>
<p>16 0.022954751 <a title="33-tfidf-16" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>17 0.022460477 <a title="33-tfidf-17" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>18 0.022184387 <a title="33-tfidf-18" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>19 0.021785302 <a title="33-tfidf-19" href="./jmlr-2013-Counterfactual_Reasoning_and_Learning_Systems%3A_The_Example_of_Computational_Advertising.html">30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</a></p>
<p>20 0.021692254 <a title="33-tfidf-20" href="./jmlr-2013-Distance_Preserving_Embeddings_for_General_n-Dimensional_Manifolds.html">34 jmlr-2013-Distance Preserving Embeddings for General n-Dimensional Manifolds</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.118), (1, 0.017), (2, -0.026), (3, 0.012), (4, -0.034), (5, 0.03), (6, 0.046), (7, 0.016), (8, -0.125), (9, 0.108), (10, 0.012), (11, -0.285), (12, -0.036), (13, 0.242), (14, -0.171), (15, 0.055), (16, -0.057), (17, -0.101), (18, -0.063), (19, 0.002), (20, -0.059), (21, 0.008), (22, 0.013), (23, 0.026), (24, -0.107), (25, 0.084), (26, 0.086), (27, 0.105), (28, 0.015), (29, 0.051), (30, 0.052), (31, -0.062), (32, 0.017), (33, -0.025), (34, -0.118), (35, 0.104), (36, -0.019), (37, 0.226), (38, 0.164), (39, 0.026), (40, -0.136), (41, 0.142), (42, 0.067), (43, 0.14), (44, 0.099), (45, 0.034), (46, -0.09), (47, 0.012), (48, 0.069), (49, -0.094)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95313001 <a title="33-lsi-1" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<p>Author: Reza Bosagh Zadeh, Ashish Goel</p><p>Abstract: We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high-dimensional sparse vectors. All of our results are provably independent of dimension, meaning that apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension; thus the dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard similarity measures. For Jaccard similarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems with large scale experiments using data from the social networking site Twitter. At time of writing, our algorithms are live in production at twitter.com. Keywords: cosine, Jaccard, overlap, dice, similarity, MapReduce, dimension independent</p><p>2 0.62925398 <a title="33-lsi-2" href="./jmlr-2013-On_the_Learnability_of_Shuffle_Ideals.html">78 jmlr-2013-On the Learnability of Shuffle Ideals</a></p>
<p>Author: Dana Angluin, James Aspnes, Sarah Eisenstat, Aryeh Kontorovich</p><p>Abstract: PAC learning of unrestricted regular languages is long known to be a difﬁcult problem. The class of shufﬂe ideals is a very restricted subclass of regular languages, where the shufﬂe ideal generated by a string u is the collection of all strings containing u as a subsequence. This fundamental language family is of theoretical interest in its own right and provides the building blocks for other important language families. Despite its apparent simplicity, the class of shufﬂe ideals appears quite difﬁcult to learn. In particular, just as for unrestricted regular languages, the class is not properly PAC learnable in polynomial time if RP = NP, and PAC learning the class improperly in polynomial time would imply polynomial time algorithms for certain fundamental problems in cryptography. In the positive direction, we give an efﬁcient algorithm for properly learning shufﬂe ideals in the statistical query (and therefore also PAC) model under the uniform distribution. Keywords: PAC learning, statistical queries, regular languages, deterministic ﬁnite automata, shufﬂe ideals, subsequences</p><p>3 0.40664271 <a title="33-lsi-3" href="./jmlr-2013-Ranked_Bandits_in_Metric_Spaces%3A_Learning_Diverse_Rankings_over_Large_Document_Collections.html">94 jmlr-2013-Ranked Bandits in Metric Spaces: Learning Diverse Rankings over Large Document Collections</a></p>
<p>Author: Aleksandrs Slivkins, Filip Radlinski, Sreenivas Gollapudi</p><p>Abstract: Most learning to rank research has assumed that the utility of different documents is independent, which results in learned ranking functions that return redundant results. The few approaches that avoid this have rather unsatisfyingly lacked theoretical foundations, or do not scale. We present a learning-to-rank formulation that optimizes the fraction of satisﬁed users, with several scalable algorithms that explicitly takes document similarity and ranking context into account. Our formulation is a non-trivial common generalization of two multi-armed bandit models from the literature: ranked bandits (Radlinski et al., 2008) and Lipschitz bandits (Kleinberg et al., 2008b). We present theoretical justiﬁcations for this approach, as well as a near-optimal algorithm. Our evaluation adds optimizations that improve empirical performance, and shows that our algorithms learn orders of magnitude more quickly than previous approaches. Keywords: online learning, clickthrough data, diversity, multi-armed bandits, contextual bandits, regret, metric spaces</p><p>4 0.29095247 <a title="33-lsi-4" href="./jmlr-2013-Similarity-based_Clustering_by_Left-Stochastic_Matrix_Factorization.html">100 jmlr-2013-Similarity-based Clustering by Left-Stochastic Matrix Factorization</a></p>
<p>Author: Raman Arora, Maya R. Gupta, Amol Kapila, Maryam Fazel</p><p>Abstract: For similarity-based clustering, we propose modeling the entries of a given similarity matrix as the inner products of the unknown cluster probabilities. To estimate the cluster probabilities from the given similarity matrix, we introduce a left-stochastic non-negative matrix factorization problem. A rotation-based algorithm is proposed for the matrix factorization. Conditions for unique matrix factorizations and clusterings are given, and an error bound is provided. The algorithm is particularly efﬁcient for the case of two clusters, which motivates a hierarchical variant for cases where the number of desired clusters is large. Experiments show that the proposed left-stochastic decomposition clustering model produces relatively high within-cluster similarity on most data sets and can match given class labels, and that the efﬁcient hierarchical variant performs surprisingly well. Keywords: clustering, non-negative matrix factorization, rotation, indeﬁnite kernel, similarity, completely positive</p><p>5 0.28995037 <a title="33-lsi-5" href="./jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications.html">18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</a></p>
<p>Author: Ming-Jie Zhao, Narayanan Edakunni, Adam Pocock, Gavin Brown</p><p>Abstract: Fano’s inequality lower bounds the probability of transmission error through a communication channel. Applied to classiﬁcation problems, it provides a lower bound on the Bayes error rate and motivates the widely used Infomax principle. In modern machine learning, we are often interested in more than just the error rate. In medical diagnosis, different errors incur different cost; hence, the overall risk is cost-sensitive. Two other popular criteria are balanced error rate (BER) and F-score. In this work, we focus on the two-class problem and use a general deﬁnition of conditional entropy (including Shannon’s as a special case) to derive upper/lower bounds on the optimal F-score, BER and cost-sensitive risk, extending Fano’s result. As a consequence, we show that Infomax is not suitable for optimizing F-score or cost-sensitive risk, in that it can potentially lead to low F-score and high risk. For cost-sensitive risk, we propose a new conditional entropy formulation which avoids this inconsistency. In addition, we consider the common practice of using a threshold on the posterior probability to tune performance of a classiﬁer. As is widely known, a threshold of 0.5, where the posteriors cross, minimizes error rate—we derive similar optimal thresholds for F-score and BER. Keywords: balanced error rate, F-score (Fβ -measure), cost-sensitive risk, conditional entropy, lower/upper bound</p><p>6 0.25589946 <a title="33-lsi-6" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>7 0.24867234 <a title="33-lsi-7" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>8 0.2460828 <a title="33-lsi-8" href="./jmlr-2013-Learning_Trees_from_Strings%3A_A_Strong_Learning_Algorithm_for_some_Context-Free_Grammars.html">63 jmlr-2013-Learning Trees from Strings: A Strong Learning Algorithm for some Context-Free Grammars</a></p>
<p>9 0.21470508 <a title="33-lsi-9" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>10 0.20658676 <a title="33-lsi-10" href="./jmlr-2013-Learning_Bilinear_Model_for_Matching_Queries_and_Documents.html">60 jmlr-2013-Learning Bilinear Model for Matching Queries and Documents</a></p>
<p>11 0.18823783 <a title="33-lsi-11" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>12 0.18612225 <a title="33-lsi-12" href="./jmlr-2013-Maximum_Volume_Clustering%3A_A_New_Discriminative_Clustering_Approach.html">70 jmlr-2013-Maximum Volume Clustering: A New Discriminative Clustering Approach</a></p>
<p>13 0.18194391 <a title="33-lsi-13" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>14 0.16878781 <a title="33-lsi-14" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>15 0.16809152 <a title="33-lsi-15" href="./jmlr-2013-Joint_Harmonic_Functions_and_Their_Supervised_Connections.html">55 jmlr-2013-Joint Harmonic Functions and Their Supervised Connections</a></p>
<p>16 0.15983617 <a title="33-lsi-16" href="./jmlr-2013-Counterfactual_Reasoning_and_Learning_Systems%3A_The_Example_of_Computational_Advertising.html">30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</a></p>
<p>17 0.15942138 <a title="33-lsi-17" href="./jmlr-2013-Stress_Functions_for_Nonlinear_Dimension_Reduction%2C_Proximity_Analysis%2C_and_Graph_Drawing.html">109 jmlr-2013-Stress Functions for Nonlinear Dimension Reduction, Proximity Analysis, and Graph Drawing</a></p>
<p>18 0.15701333 <a title="33-lsi-18" href="./jmlr-2013-On_the_Mutual_Nearest_Neighbors_Estimate_in_Regression.html">79 jmlr-2013-On the Mutual Nearest Neighbors Estimate in Regression</a></p>
<p>19 0.1470414 <a title="33-lsi-19" href="./jmlr-2013-Tapkee%3A_An_Efficient_Dimension_Reduction_Library.html">112 jmlr-2013-Tapkee: An Efficient Dimension Reduction Library</a></p>
<p>20 0.14039326 <a title="33-lsi-20" href="./jmlr-2013-GURLS%3A_A_Least_Squares_Library_for_Supervised_Learning.html">46 jmlr-2013-GURLS: A Least Squares Library for Supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.03), (5, 0.069), (6, 0.037), (10, 0.052), (20, 0.019), (23, 0.033), (53, 0.025), (68, 0.027), (70, 0.032), (75, 0.032), (78, 0.494), (85, 0.017), (87, 0.019), (89, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.6834802 <a title="33-lda-1" href="./jmlr-2013-Dimension_Independent_Similarity_Computation.html">33 jmlr-2013-Dimension Independent Similarity Computation</a></p>
<p>Author: Reza Bosagh Zadeh, Ashish Goel</p><p>Abstract: We present a suite of algorithms for Dimension Independent Similarity Computation (DISCO) to compute all pairwise similarities between very high-dimensional sparse vectors. All of our results are provably independent of dimension, meaning that apart from the initial cost of trivially reading in the data, all subsequent operations are independent of the dimension; thus the dimension can be very large. We study Cosine, Dice, Overlap, and the Jaccard similarity measures. For Jaccard similarity we include an improved version of MinHash. Our results are geared toward the MapReduce framework. We empirically validate our theorems with large scale experiments using data from the social networking site Twitter. At time of writing, our algorithms are live in production at twitter.com. Keywords: cosine, Jaccard, overlap, dice, similarity, MapReduce, dimension independent</p><p>2 0.61144662 <a title="33-lda-2" href="./jmlr-2013-Learning_Theory_Analysis_for_Association_Rules_and_Sequential_Event_Prediction.html">61 jmlr-2013-Learning Theory Analysis for Association Rules and Sequential Event Prediction</a></p>
<p>Author: Cynthia Rudin, Benjamin Letham, David Madigan</p><p>Abstract: We present a theoretical analysis for prediction algorithms based on association rules. As part of this analysis, we introduce a problem for which rules are particularly natural, called “sequential event prediction.” In sequential event prediction, events in a sequence are revealed one by one, and the goal is to determine which event will next be revealed. The training set is a collection of past sequences of events. An example application is to predict which item will next be placed into a customer’s online shopping cart, given his/her past purchases. In the context of this problem, algorithms based on association rules have distinct advantages over classical statistical and machine learning methods: they look at correlations based on subsets of co-occurring past events (items a and b imply item c), they can be applied to the sequential event prediction problem in a natural way, they can potentially handle the “cold start” problem where the training set is small, and they yield interpretable predictions. In this work, we present two algorithms that incorporate association rules. These algorithms can be used both for sequential event prediction and for supervised classiﬁcation, and they are simple enough that they can possibly be understood by users, customers, patients, managers, etc. We provide generalization guarantees on these algorithms based on algorithmic stability analysis from statistical learning theory. We include a discussion of the strict minimum support threshold often used in association rule mining, and introduce an “adjusted conﬁdence” measure that provides a weaker minimum support condition that has advantages over the strict minimum support. The paper brings together ideas from statistical learning theory, association rule mining and Bayesian analysis. Keywords: statistical learning theory, algorithmic stability, association rules, sequence prediction, associative classiﬁcation c 2013 Cynthia Rudin, Benjamin Letham and David Madigan. RUDIN , L E</p><p>3 0.21502106 <a title="33-lda-3" href="./jmlr-2013-Manifold_Regularization_and_Semi-supervised_Learning%3A_Some_Theoretical_Analyses.html">69 jmlr-2013-Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses</a></p>
<p>Author: Partha Niyogi</p><p>Abstract: Manifold regularization (Belkin et al., 2006) is a geometrically motivated framework for machine learning within which several semi-supervised algorithms have been constructed. Here we try to provide some theoretical understanding of this approach. Our main result is to expose the natural structure of a class of problems on which manifold regularization methods are helpful. We show that for such problems, no supervised learner can learn effectively. On the other hand, a manifold based learner (that knows the manifold or “learns” it from unlabeled examples) can learn with relatively few labeled examples. Our analysis follows a minimax style with an emphasis on ﬁnite sample results (in terms of n: the number of labeled examples). These results allow us to properly interpret manifold regularization and related spectral and geometric algorithms in terms of their potential use in semi-supervised learning. Keywords: semi-supervised learning, manifold regularization, graph Laplacian, minimax rates</p><p>4 0.21392466 <a title="33-lda-4" href="./jmlr-2013-Stochastic_Variational_Inference.html">108 jmlr-2013-Stochastic Variational Inference</a></p>
<p>Author: Matthew D. Hoffman, David M. Blei, Chong Wang, John Paisley</p><p>Abstract: We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets. Keywords: Bayesian inference, variational inference, stochastic optimization, topic models, Bayesian nonparametrics</p><p>5 0.21336022 <a title="33-lda-5" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>Author: Yuchen Zhang, John C. Duchi, Martin J. Wainwright</p><p>Abstract: We analyze two communication-efﬁcient algorithms for distributed optimization in statistical settings involving large-scale data sets. The ﬁrst algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves √ mean-squared error (MSE) that decays as O (N −1 + (N/m)−2 ). Whenever m ≤ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O (N −1 + (N/m)−3 ), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O (N −1 + (N/m)−3/2 ), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efﬁciently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 108 samples and d ≈ 740,000 covariates. Keywords: distributed learning, stochastic optimization, averaging, subsampling</p><p>6 0.21207412 <a title="33-lda-6" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>7 0.21196614 <a title="33-lda-7" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>8 0.21136752 <a title="33-lda-8" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>9 0.21108073 <a title="33-lda-9" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>10 0.20718093 <a title="33-lda-10" href="./jmlr-2013-Variational_Algorithms_for_Marginal_MAP.html">120 jmlr-2013-Variational Algorithms for Marginal MAP</a></p>
<p>11 0.2069733 <a title="33-lda-11" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>12 0.20667487 <a title="33-lda-12" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>13 0.20627818 <a title="33-lda-13" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>14 0.20622124 <a title="33-lda-14" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>15 0.20616895 <a title="33-lda-15" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>16 0.20607005 <a title="33-lda-16" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>17 0.20580381 <a title="33-lda-17" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<p>18 0.20549071 <a title="33-lda-18" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>19 0.20548378 <a title="33-lda-19" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<p>20 0.20537403 <a title="33-lda-20" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
