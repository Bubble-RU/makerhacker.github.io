<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-14" href="#">jmlr2013-14</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</h1>
<br/><p>Source: <a title="jmlr-2013-14-pdf" href="http://jmlr.org/papers/volume14/neuvial13a/neuvial13a.pdf">pdf</a></p><p>Author: Pierre Neuvial</p><p>Abstract: The False Discovery Rate (FDR) is a commonly used type I error rate in multiple testing problems. It is deﬁned as the expected False Discovery Proportion (FDP), that is, the expected fraction of false positives among rejected hypotheses. When the hypotheses are independent, the BenjaminiHochberg procedure achieves FDR control at any pre-speciﬁed level. By construction, FDR control offers no guarantee in terms of power, or type II error. A number of alternative procedures have been developed, including plug-in procedures that aim at gaining power by incorporating an estimate of the proportion of true null hypotheses. In this paper, we study the asymptotic behavior of a class of plug-in procedures based on kernel estimators of the density of the p-values, as the number m of tested hypotheses grows to inﬁnity. In a setting where the hypotheses tested are independent, we prove that these procedures are asymptotically more powerful in two respects: (i) a tighter asymptotic FDR control for any target FDR level and (ii) a broader range of target levels yielding positive asymptotic power. We also show that this increased asymptotic power comes at the price of slower, non-parametric convergence rates for the FDP. These rates are of the form m−k/(2k+1) , where k is determined by the regularity of the density of the p-value distribution, or, equivalently, of the test statistics distribution. These results are applied to one- and two-sided tests statistics for Gaussian and Laplace location models, and for the Student model. Keywords: multiple testing, false discovery rate, Benjamini Hochberg’s procedure, power, criticality, plug-in procedures, adaptive control, test statistics distribution, convergence rates, kernel estimators</p><p>Reference: <a title="jmlr-2013-14-reference" href="../jmlr2013_reference/jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A number of alternative procedures have been developed, including plug-in procedures that aim at gaining power by incorporating an estimate of the proportion of true null hypotheses. [sent-8, score-0.346]
</p><p>2 In this paper, we study the asymptotic behavior of a class of plug-in procedures based on kernel estimators of the density of the p-values, as the number m of tested hypotheses grows to inﬁnity. [sent-9, score-0.425]
</p><p>3 In a setting where the hypotheses tested are independent, we prove that these procedures are asymptotically more powerful in two respects: (i) a tighter asymptotic FDR control for any target FDR level and (ii) a broader range of target levels yielding positive asymptotic power. [sent-10, score-0.46]
</p><p>4 Keywords: multiple testing, false discovery rate, Benjamini Hochberg’s procedure, power, criticality, plug-in procedures, adaptive control, test statistics distribution, convergence rates, kernel estimators  1. [sent-14, score-0.313]
</p><p>5 A simple procedure called the Benjamini-Hochberg (BH) procedure provides FDR control when the tested hypotheses are independent (Benjamini and Hochberg, 1995) or follow speciﬁc types of positive dependence (Benjamini and Yekutieli, 2001). [sent-23, score-0.206]
</p><p>6 When the hypotheses tested are independent, applying the BH procedure at level α in fact yields FDR = π0 α, where π0 is the unknown fraction of true null hypotheses (Benjamini and Yekutieli, 2001). [sent-24, score-0.268]
</p><p>7 This paper studies the inﬂuence of the plug-in step on the asymptotic properties of the corresponding procedure for a particular class of estimators of π0 , which may be written as kernel estimators of the density of the p-value distribution at 1. [sent-31, score-0.418]
</p><p>8 Speciﬁcally, the power of a multiple testing procedure at level α is generally deﬁned as the (random) proportion of correct rejections (true positives) among true alternative hypotheses (see, for example, Chi, 2007a): Πm =  Rm −Vm . [sent-87, score-0.344]
</p><p>9 When all true null hypotheses are independent, the BH procedure at level α yields strong FDR control, that is, it entails FDR ≤ α regardless of the number of true null hypotheses (Benjamini and Hochberg, 1995). [sent-101, score-0.375]
</p><p>10 4 Plug-in Procedures In our setting where all of the hypotheses tested are independent, the BH procedure at target level α (henceforth denoted by BH(α) for short) in fact yields FDR control at level π0 α exactly (Benjamini 1427  N EUVIAL  and Hochberg, 1995; Benjamini and Yekutieli, 2001). [sent-113, score-0.204]
</p><p>11 Therefore, as the threshold of the BH(α) procedure is a nondecreasing function of α and by Remark 6, the BH(α/π0 ) procedure is optimal in our setting, in the sense that it yields maximum power among procedures of the form BH(α′ ) that control the FDR at level α. [sent-115, score-0.338]
</p><p>12 We note that the Storey-λ estimator πSto (λ) can be viewed as a kernel estimator of the density g at 1. [sent-133, score-0.208]
</p><p>13 1428  A SYMPTOTICS OF K ERNEL -BASED A DAPTIVE FDR C ONTROLLING P ROCEDURES  ˆ 0,m By Deﬁnition 8, πSto (λ) is a kernel estimator of the density g at 1 with kernel K Sto (t) = 1[−1,0] (t) and bandwidth h = 1 − λ. [sent-147, score-0.221]
</p><p>14 They are tightly connected, with the important difference that the former only depends on the multiple testing problem, while the latter depends on both the multiple testing problem and a speciﬁc multiple testing procedure. [sent-152, score-0.243]
</p><p>15 Deﬁnition 9 (Critical value of a multiple testing problem) The critical value of the multiple testing problem parametrized by π0 and G is deﬁned by π0t . [sent-153, score-0.226]
</p><p>16 The following Deﬁnition generalizes BH BH the notion of critical value of to a generic multiple testing procedure: Deﬁnition 11 (Critical value of a multiple testing procedure) Let P denote a multiple testing procedure. [sent-177, score-0.307]
</p><p>17 P Remark 12 (Criticality of a multiple testing problem versus criticality of a procedure) Whether Condition (5) hols or not only depends on the behavior of the test statistics distribution. [sent-187, score-0.218]
</p><p>18 Therefore, as the Oracle BH procedure at level α is the most powerful procedure among thresholding-based procedures that control FDR at level α, α⋆ is a lower bound on the critical values of these procedures. [sent-190, score-0.355]
</p><p>19 This paper extends the asymptotic results of Chi (2007a) and Neuvial (2008) to the case of plugˆ ˆ in procedures of the form BH(α/π0,m ), where π0,m is a kernel estimator of the p-value distribution g at 1. [sent-192, score-0.335]
</p><p>20 In Section 3, we prove that this class of estimators of π0 achieves non-parametric convergence rates of the form m−k/(2k+1) /ηm , where ηm goes to 0 slowly enough as m → +∞, and k controls the regularity of g at 1. [sent-194, score-0.205]
</p><p>21 Therefore, we have G1 (λ) < 1 for any λ ∈ (0, 1), and the bias π0 (λ) − π0 is positive: the Storey-λ estimator achieves a parametric convergence rate, but it is not a consistent estimator of π0 . [sent-204, score-0.214]
</p><p>22 We consider plug-in procedures where π0 is estimated by πSto (1 − hm ), with hm → 0 as m → +∞. [sent-207, score-0.528]
</p><p>23 The asymptotic bias and variance of πSto (1 − hm ) are characterized by Proposition 13: ˆ 0,m Proposition 13 (Asymptotic bias and variance of πSto (1 − hm )) Let (hm )m∈N be a positive sequence such that hm → 0. [sent-210, score-0.778]
</p><p>24 If mhm → +∞ as m → +∞, then ˆ 0,m ˆ 0,m mhm πSto (1 − hm ) − E πSto (1 − hm )  N (0, g(1)) . [sent-212, score-0.796]
</p><p>25 Then ˆ 0,m E πSto (1 − hm ) − g(1)  =  m→+∞  (−1)k g(k) (1) k hm + o hk . [sent-215, score-0.416]
</p><p>26 Only the bias term in Proposition 13 depends on the regularity k of the distribution near 1: the ˆ 0,m asymptotic bias is of order hk , while the asymptotic variance of πSto (1 − hm ) is of order (mhm )−1 , m regardless of the regularity of the distribution. [sent-217, score-0.56]
</p><p>27 The bandwidth hm in Proposition 13 realizes a tradeˆ 0,m off between the asymptotic bias and variance of πSto (1−hm ). [sent-218, score-0.363]
</p><p>28 When the regularity of the distribution is known, a natural way to resolve this bias/variance trade-off is to calibrate hm such that the Mean Squared Error (MSE) of the corresponding estimator is asymptotically minimum. [sent-219, score-0.374]
</p><p>29 This gives rise to an optimal choice of the bandwidth, which is characterized by the following proposition: 1431  N EUVIAL  ˆ 0,m Proposition 14 (Asymptotic properties of πSto (1 − hm )) Assume that g is k times differentiable at (l) (1) = 0 for 1 ≤ l < k. [sent-220, score-0.268]
</p><p>30 If g(k) (1) = 0, then the asymptotically optimal bandwidth for πSto (1 − hm ) in terms of MSE is −1/(2k+1) , and the corresponding MSE is of order m−2k/(2k+1) . [sent-222, score-0.272]
</p><p>31 Then, letting hm (k) = m−1/(2k+1) η2 , we have, as m → +∞: m ˆ 0,m mk/(2k+1) ηm πSto (1 − hm (k)) − g(1)  N (0, g(1)) . [sent-225, score-0.416]
</p><p>32 The convergence rate in (9) is a typical convergence rate for non-parametric estimators of a density at a point. [sent-227, score-0.273]
</p><p>33 Let gk (1) be a kernel estimator of g(1) with bandwidth hm , associated with a kth order kernel. [sent-232, score-0.379]
</p><p>34 Then letting hm (k) = m−1/(2k+1) η2 , we have, as m → +∞: m mk/(2k+1) ηm gk (1) − g(1) ˆm  N (0, g(1)) . [sent-236, score-0.208]
</p><p>35 Propositions 14 and 15 show that the convergence rate of kernel estimators of g(1) with asymptotically optimal bandwidth directly depends on the regularity k of g at 1. [sent-237, score-0.324]
</p><p>36 The only difference between the two propositions is that the assumption that the ﬁrst k − 1 derivatives of g are null at 1 ˆ for π0,m (1 − hm ) is not needed for kth order kernel estimators. [sent-238, score-0.363]
</p><p>37 We now brieﬂy review asymptotic properties of these estimators in the context of multiple testing, as stated in Genovese and Wasserman (2004), and show that their convergence rates can essentially be recovered by Propositions 14 and 15. [sent-241, score-0.276]
</p><p>38 As both estimators are estimators of g(1), the differences in their asymptotic properties are driven by the differences in the regularity assumptions made for g (or g1 ) near 1, rather than by their speciﬁc form. [sent-250, score-0.322]
</p><p>39 For completeness, the asymptotic properties of the BH procedure and the plug-in procedure based on the Storey-λ estimator are derived in Appendix C. [sent-258, score-0.279]
</p><p>40 The problem considered in this section is more challenging, as the kernel estimators introduced in Section 3 depend on m not only through Gm , but also through ˆ 0,m the bandwidth of the kernel (for example, hm for πSto (1 − hm )). [sent-259, score-0.628]
</p><p>41 ∞ We have τ0 (α) = τ∞ (α/π0,∞ ), that is, the asymptotic threshold of the BH procedure deﬁned in ∞ Equation (6) at level α/π0,∞ . [sent-267, score-0.204]
</p><p>42 Further assume that the asymptotic distribution of π0,m is given by  N (0, s2 ) 0  ˆ mhm (π0,m − π0,∞ )  for some s0 , with hm = o (1/ ln ln m) and mhm → +∞ as m → +∞. [sent-272, score-0.765]
</p><p>43 ˆ Theorem 16 states that for α > α⋆ , for any estimator π0,m that converges in distribution at a rate 0 ˆ rm slower than the parametric rate m−1/2 , the plug-in procedure BH(α/π0,m ) converges at rate rm as well. [sent-274, score-0.376]
</p><p>44 We now state the main result of the paper (Corollary 17), that is, the asymptotic properties of plug-in procedures associated with the estimators of π0 studied in Section 3, for which s2 = g(1). [sent-276, score-0.292]
</p><p>45 Deﬁne ˆ 0,m hm (k) = m−1/(2k+1) η2 , where ηm → 0 and mk/(2k+1) ηm → +∞ as m → +∞. [sent-279, score-0.208]
</p><p>46 Denote by πk one of m the following two estimators of π0 : ˆ 0,m • Storey’s estimator πSto (1 − hm (k)); in this case, it is further assumed that g(l) (1) = 0 for 1 ≤ l < k; • A kernel estimator of g(1) associated with a kth order kernel with bandwidth hm (k). [sent-280, score-0.786]
</p><p>47 Then the asymptotic threshold of the BH(α/π0,m ) procedure is τ∞ (α/π0 ), that is, the asymptotic threshold of the Oracle procedure BH(α/π0 ). [sent-287, score-0.37]
</p><p>48 In particular, the asymptotic FDP achieved by the estimators in Corollary 17 is then exactly α (and its asymptotic variance is α2 /π0 ), whereas the asymptotic FDP of the original BH procedure is π0 α. [sent-288, score-0.421]
</p><p>49 In models where Condition (5) is not satisﬁed, all the critical values in (4) are null, implying that all the corresponding procedures have positive power for any target FDR level. [sent-290, score-0.247]
</p><p>50 In models where Condition (5) is satisﬁed, all the critical values in (4) are positive, and (4) implies that the range of target FDR values α that yield asymptotically positive power is larger for the plug-in procedures studied in this paper than for the BH procedure or the Storey-λ procedure. [sent-291, score-0.327]
</p><p>51 of FDP)/ FDR (π0 τ∞ (α))−1 − 1 (τ∞ (α/π0 ))−1 − 1 (π0 τ0,λ (α))−1 + (1 − G(λ))−1 ∞ g(1)−1  Table 1: Summary of the asymptotic properties of the FDR controlling procedures considered in this paper, for a target FDR level α greater than the (procedure-speciﬁc) critical value. [sent-296, score-0.337]
</p><p>52 Note that “Storey-λ” denotes the original procedure with a ﬁxed λ, while our extension with λ = 1 − hm (k) is categorized in the table as a particular case of kernel estimator (last row). [sent-297, score-0.377]
</p><p>53 ∞ These results characterize the increase in asymptotic power achieved by plug-in procedures based on kernel estimators of π0 . [sent-299, score-0.363]
</p><p>54 Then, we derive convergence rates for plug-in procedures based on the kernel estimators of π0 considered in Sections 3 and 4, both for two-sided tests (Section 5. [sent-309, score-0.366]
</p><p>55 1 L OCATION M ODELS In location models the distribution of the test statistic under H1 is a shift from that of the test statistic under H0 : F1 = F0 (· − θ) for some location parameter θ > 0. [sent-316, score-0.285]
</p><p>56 The most widely studied location models are the Gaussian and Laplace (double exponential) location models. [sent-317, score-0.219]
</p><p>57 2 Criticality As the asymptotic behavior of plug-in procedures crucially depends on whether the target FDR level is above or below the critical value α⋆ characterized by Theorem 16, it is of primary importance to 0 study criticality in the models we are interested in. [sent-341, score-0.462]
</p><p>58 Thus |x|γ 1 − |1 − θ/x|γ ∼ γθxγ−1 , γ γ and the behavior of the likelihood ratio f1 / f0 is driven by the value of γ, as illustrated by Figure 2 for the Gaussian and Laplace location models with location parameter θ ∈ {1, 2}. [sent-348, score-0.279]
</p><p>59 0  p−value  Figure 2: Distribution functions G for one-sided p-values (black) and two-sided p-values (gray), in Gaussian location models (left: Condition (5) is not satisﬁed), and Laplace location models (right: Condition (5) is satisﬁed) for π0 =0, 0. [sent-431, score-0.24]
</p><p>60 In such a situation, for any target FDR level α, the asymptotic fraction of rejections by the BH(α) ˆ ˆ procedure or by a plug-in procedure of the form BH(α/π0,m ), where π0,m → π0,∞ in probability as m → +∞, is positive by Lemma 27. [sent-439, score-0.305]
</p><p>61 BH 0 ˆ Let us now consider the modiﬁcation of the Storey-λ estimator introduced in Section 3: π0,m = ˆ 0,m πSto (1 − hm ), with hm → 0 as m → +∞. [sent-477, score-0.486]
</p><p>62 −∞ f 1 +∞ f 0 For one-sided tests in symmetric location models, Lemma 20 implies the following result: 1441  N EUVIAL  Proposition 21 (Purity and criticality) Let g1 be the density of one-sided p-values under the alternative hypothesis, and α⋆ the critical value of the multiple testing problem. [sent-587, score-0.334]
</p><p>63 Proposition 21 implies that contrary to two-sided location models, in which we always have g1 (1) > 0, consistency may be achieved in one-sided location models using kernel estimators such as those considered here, depending on model parameters. [sent-591, score-0.348]
</p><p>64 In particular, there is no criticality in the onesided Gaussian model, implying that Condition (8) is satisﬁed in that model: we have g(1) = π0 , and π0 can be consistently estimated using the kernel estimators of g(1) introduced in Section 3. [sent-592, score-0.244]
</p><p>65 Therefore, for any ﬁxed λ ≥ 1/2, the Storey-λ estimator is an unbiased estimator of g(1), which converges to g(1) at rate m−1/2 . [sent-601, score-0.221]
</p><p>66 2  Therefore, g1 is not differentiable at 1, and the convergence rates of the kernel estimators of π0 studied in Section 3 are slower than m−1/3 in our setting. [sent-606, score-0.231]
</p><p>67 The difference between one- and two-sided tests in the Gaussian location model is illustrated by Figure 4 for θ = 1, that is when testing N (0, 1) against N (1, 1). [sent-608, score-0.209]
</p><p>68 Concluding Remarks This paper studies asymptotic properties of a family of plug-in procedures based on the BH procedure. [sent-615, score-0.203]
</p><p>69 These improvements come at the price of a reduction in the convergence rate from the parametric rate m−1/2 to a non-parametric rate m−k/(2k+1) , where k is connected to the order of differentiability of the test statistics distribution. [sent-617, score-0.207]
</p><p>70 As the results obtained for the proposed ˆ 0,m modiﬁcation of the Storey-λ estimator πSto (1 − hm ) require stronger conditions (null derivatives of g1 ) than for kernel estimators with a kernel of order k, we conclude that it is generally better to use the latter class of estimators. [sent-618, score-0.465]
</p><p>71 • When the regularity of the test statistics distribution is poor (such as in the one-sided Gaussian model), the convergence rate of the FDP achieved by the plug-in procedures studied in this paper is slower. [sent-621, score-0.287]
</p><p>72 The plug-in procedures studied are still asymptotically more powerful than the BH procedure or the Storey-λ procedure, but the FDP actually achieved by the plug-in procedures studied for a given m may be far from the target FDR level. [sent-622, score-0.323]
</p><p>73 Second, we have shown that the asymptotic properties of FDR controlling procedures are driven by the shape and regularity of the test statistics distribution. [sent-639, score-0.31]
</p><p>74 In the spirit of the results of Chi (2007b) on the inﬂuence of sample size on criticality, it would be interesting to study the convergence rates of plug-in procedures when both the sample size and the number of hypotheses tested grow to inﬁnity. [sent-642, score-0.24]
</p><p>75 2 Alternative Strategies to Estimate π0 The estimators of π0 considered in this paper are kernel estimators of the density g at 1. [sent-644, score-0.246]
</p><p>76 This estimator was shown to be consistent for the estimation of π0 when the Z-scores follow a Gaussian location mixture, but no convergence rates were established. [sent-651, score-0.232]
</p><p>77 Asymptotics of a Modiﬁcation of the Storey-λ Estimator ˆ 0,m This section gathers the proofs of the asymptotic properties of the estimator πSto (1 − hm ) stated in Section 3. [sent-710, score-0.369]
</p><p>78 Zim follows a Bernoulli distribution with parameter 1 − G(1 − hm ). [sent-714, score-0.23]
</p><p>79 Letting Yim =  Zim − E [Zim ] √ , mhm  √ ˆ 0,m ˆ 0,m we have ∑m Yim = mhm πSto (1 − hm ) − E πSto (1 − hm ) . [sent-715, score-0.796]
</p><p>80 1449  N EUVIAL  As Zim ∈ {0, 1} and E [Zim ] ∈ [0, 1], we have (Yim )2 ≤ 1/(mhm ), and m  ∑E  i=1  1 E 1|Y1m |>ε hm 1 P(|Y1m | > ε) = hm 1 VarY1m ≤ hm ε2  (Yim )2 1|Yim |>ε ≤  by Chebycheff’s inequality. [sent-719, score-0.624]
</p><p>81 As mhm → +∞ and VarY1m ∼ g(1)/m as m → +∞, the above sum therefore goes to 0 as mhm → +∞. [sent-720, score-0.38]
</p><p>82 Therefore, if hm → 0 as m → +∞, we have (k)  ˆ 0,m E πSto (1 − hm ) − g(1) = (1 − π0 )  (−1)k g1 (1) k hm + o hk , m (k + 1)! [sent-728, score-0.624]
</p><p>83 The optimal bandwidth is obtained for hm proporm tional to m−1/(2k+1) , because this choice balances variance and squared bias. [sent-731, score-0.251]
</p><p>84 To prove (2), we note that ˆ mhm (π0,m − g(1)) =  ˆ ˆ mhm (π0,m − E [π0,m ]) +  ˆ mhm (E [π0,m ] − g(1)) ,  ˆ ˆ where π0,m denotes π0,m (1 − hm ) to alleviate notation. [sent-734, score-0.778]
</p><p>85 The ﬁrst term (variance) converges in distri√ bution to N (0, g(1)) by Proposition 13 (1) as soon as mhm → +∞. [sent-735, score-0.227]
</p><p>86 The second term (bias) is of √ 2k+1 the order of mhm hk = mhm by Proposition 13 (2). [sent-736, score-0.38]
</p><p>87 Taking hm (k) = h⋆ (k)η2 , where ηm → 0, m m m 2k+1 → 0, which ensures that the bias term converges in probability to 0. [sent-737, score-0.266]
</p><p>88 Therefore, the asymptotic properties of the BH procedure and Storey’s procedure (that is, ˆ 0,m BH(·/πSto (λ)) in the unconditional setting can be obtained by adapting the proof of the corresponding theorems (Theorems 4. [sent-765, score-0.278]
</p><p>89 Corollary 30 (Storey-λ procedure, unconditional model) For any λ ∈ [0, 1), and α ∈ [0, 1], let τ0,λ (α) = T Sto(λ) (Gm ) be the empirical threshold τ0,λ (α) of Storey’s procedure at level α, and m m τ0,λ (α) = T Sto(λ) (G) be the corresponding asymptotic threshold. [sent-771, score-0.254]
</p><p>90 1 Proof of Theorem 16 We denote by ρ0 (α) the proportion of rejections, and by ν0 (α) the proportion of incorrect rejections m m ˆ by the plug-in procedure BH(α/π0,m ) (among all m hypotheses tested). [sent-779, score-0.244]
</p><p>91 Further assume that mhm (π0,m − π0,∞ ) converges in distribution for some hm such that hm = o (1/ ln√ m) and mhm → +∞ as m → +∞. [sent-789, score-0.855]
</p><p>92 Then (τ0 , ν0 , ρ0 ) converges at in distribution at ln m m m rate 1/ mhm , with    0  0 1 τm τ∞ 0 /α τ∞  π0  (π0,∞ − π0,m )(1 + oP (1)) ,  ν0  −  ν0  = ˆ m ∞ π0,∞ /α − g(τ0 ) 0 0 ∞ g(τ0 ) ρm ρ∞ ∞ where ν0 = π0 τ0 and ρ0 = G(τ0 ) = π0,∞ τ0 /α. [sent-790, score-0.325]
</p><p>93 g(τ0 ) − π0,∞ /α g(τ0 ) − π0,∞ /α ∞ ∞ 1454  A SYMPTOTICS OF K ERNEL -BASED A DAPTIVE FDR C ONTROLLING P ROCEDURES  ¯ Finally, we note that as Gm ∞ ∼ c ln ln m/m (by the Law of the Iterated Logarithm) and hm = √ √ ¯ ˆ o (1/ ln ln m), we have Gm (τ0 ) = oP 1/ mhm . [sent-812, score-0.526]
</p><p>94 Moreover, mhm (π0,m − π0,∞ ) converges in distrim 0 /α dominates the right-hand side. [sent-813, score-0.227]
</p><p>95 On the adaptive control of the false discovery rate in multiple testing with independent statistics. [sent-856, score-0.243]
</p><p>96 The control of the false discovery rate in multiple testing under dependency. [sent-861, score-0.243]
</p><p>97 Adaptive linear step-up procedures that control the false discovery rate. [sent-868, score-0.23]
</p><p>98 Asymptotic properties of false discovery rate controlling procedures under independence. [sent-952, score-0.283]
</p><p>99 Corrigendum to “Asymptotic properties of false discovery rate controlling procedures under independence”. [sent-956, score-0.283]
</p><p>100 On efﬁcient estimators of the proportion of true null hypotheses in a multiple testing setup. [sent-962, score-0.326]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bh', 0.511), ('fdr', 0.462), ('sto', 0.285), ('hm', 0.208), ('mhm', 0.19), ('student', 0.177), ('neuvial', 0.136), ('euvial', 0.122), ('rocedures', 0.122), ('storey', 0.122), ('symptotics', 0.122), ('criticality', 0.115), ('procedures', 0.112), ('benjamini', 0.11), ('gm', 0.107), ('daptive', 0.105), ('ontrolling', 0.105), ('location', 0.099), ('fdp', 0.095), ('asymptotic', 0.091), ('estimators', 0.089), ('ernel', 0.087), ('laplace', 0.086), ('fdpm', 0.081), ('estimator', 0.07), ('hypotheses', 0.065), ('critical', 0.064), ('chi', 0.063), ('proposition', 0.063), ('tests', 0.062), ('null', 0.06), ('procedure', 0.059), ('hochberg', 0.058), ('yim', 0.058), ('rejections', 0.058), ('false', 0.056), ('genovese', 0.054), ('regularity', 0.053), ('unconditional', 0.05), ('testing', 0.048), ('roquain', 0.048), ('entails', 0.047), ('rate', 0.044), ('bandwidth', 0.043), ('kernel', 0.04), ('differentiable', 0.039), ('discovery', 0.039), ('converges', 0.037), ('threshold', 0.035), ('zim', 0.035), ('convergence', 0.034), ('multiple', 0.033), ('controlling', 0.032), ('likelihood', 0.032), ('ln', 0.032), ('power', 0.031), ('proportion', 0.031), ('op', 0.03), ('limt', 0.029), ('rates', 0.029), ('wasserman', 0.028), ('ratio', 0.028), ('density', 0.028), ('delattre', 0.027), ('donsker', 0.027), ('subbotin', 0.027), ('yekutieli', 0.027), ('uctuations', 0.024), ('sgn', 0.024), ('oracle', 0.024), ('hengartner', 0.023), ('zoom', 0.023), ('control', 0.023), ('lemma', 0.023), ('panels', 0.023), ('crossing', 0.023), ('test', 0.022), ('distribution', 0.022), ('surely', 0.022), ('vm', 0.021), ('asymptotically', 0.021), ('inserted', 0.021), ('models', 0.021), ('bias', 0.021), ('characterized', 0.021), ('gaussian', 0.021), ('matias', 0.02), ('swanepoel', 0.02), ('level', 0.019), ('proof', 0.019), ('hh', 0.019), ('purity', 0.019), ('parametric', 0.019), ('propositions', 0.019), ('mse', 0.019), ('target', 0.019), ('hi', 0.018), ('derivatives', 0.018), ('condition', 0.018), ('kth', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="14-tfidf-1" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>Author: Pierre Neuvial</p><p>Abstract: The False Discovery Rate (FDR) is a commonly used type I error rate in multiple testing problems. It is deﬁned as the expected False Discovery Proportion (FDP), that is, the expected fraction of false positives among rejected hypotheses. When the hypotheses are independent, the BenjaminiHochberg procedure achieves FDR control at any pre-speciﬁed level. By construction, FDR control offers no guarantee in terms of power, or type II error. A number of alternative procedures have been developed, including plug-in procedures that aim at gaining power by incorporating an estimate of the proportion of true null hypotheses. In this paper, we study the asymptotic behavior of a class of plug-in procedures based on kernel estimators of the density of the p-values, as the number m of tested hypotheses grows to inﬁnity. In a setting where the hypotheses tested are independent, we prove that these procedures are asymptotically more powerful in two respects: (i) a tighter asymptotic FDR control for any target FDR level and (ii) a broader range of target levels yielding positive asymptotic power. We also show that this increased asymptotic power comes at the price of slower, non-parametric convergence rates for the FDP. These rates are of the form m−k/(2k+1) , where k is determined by the regularity of the density of the p-value distribution, or, equivalently, of the test statistics distribution. These results are applied to one- and two-sided tests statistics for Gaussian and Laplace location models, and for the Student model. Keywords: multiple testing, false discovery rate, Benjamini Hochberg’s procedure, power, criticality, plug-in procedures, adaptive control, test statistics distribution, convergence rates, kernel estimators</p><p>2 0.067797601 <a title="14-tfidf-2" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>3 0.067586891 <a title="14-tfidf-3" href="./jmlr-2013-Global_Analytic_Solution_of_Fully-observed_Variational_Bayesian_Matrix_Factorization.html">49 jmlr-2013-Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama, S. Derin Babacan, Ryota Tomioka</p><p>Abstract: The variational Bayesian (VB) approximation is known to be a promising approach to Bayesian estimation, when the rigorous calculation of the Bayes posterior is intractable. The VB approximation has been successfully applied to matrix factorization (MF), offering automatic dimensionality selection for principal component analysis. Generally, ﬁnding the VB solution is a non-convex problem, and most methods rely on a local search algorithm derived through a standard procedure for the VB approximation. In this paper, we show that a better option is available for fully-observed VBMF—the global solution can be analytically computed. More speciﬁcally, the global solution is a reweighted SVD of the observed matrix, and each weight can be obtained by solving a quartic equation with its coefﬁcients being functions of the observed singular value. We further show that the global optimal solution of empirical VBMF (where hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments in multi-variate analysis. Keywords: variational Bayes, matrix factorization, empirical Bayes, model-induced regularization, probabilistic PCA</p><p>4 0.05966996 <a title="14-tfidf-4" href="./jmlr-2013-Manifold_Regularization_and_Semi-supervised_Learning%3A_Some_Theoretical_Analyses.html">69 jmlr-2013-Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses</a></p>
<p>Author: Partha Niyogi</p><p>Abstract: Manifold regularization (Belkin et al., 2006) is a geometrically motivated framework for machine learning within which several semi-supervised algorithms have been constructed. Here we try to provide some theoretical understanding of this approach. Our main result is to expose the natural structure of a class of problems on which manifold regularization methods are helpful. We show that for such problems, no supervised learner can learn effectively. On the other hand, a manifold based learner (that knows the manifold or “learns” it from unlabeled examples) can learn with relatively few labeled examples. Our analysis follows a minimax style with an emphasis on ﬁnite sample results (in terms of n: the number of labeled examples). These results allow us to properly interpret manifold regularization and related spectral and geometric algorithms in terms of their potential use in semi-supervised learning. Keywords: semi-supervised learning, manifold regularization, graph Laplacian, minimax rates</p><p>5 0.044910733 <a title="14-tfidf-5" href="./jmlr-2013-Universal_Consistency_of_Localized_Versions_of_Regularized_Kernel_Methods.html">117 jmlr-2013-Universal Consistency of Localized Versions of Regularized Kernel Methods</a></p>
<p>Author: Robert Hable</p><p>Abstract: In supervised learning problems, global and local learning algorithms are used. In contrast to global learning algorithms, the prediction of a local learning algorithm in a testing point is only based on training data which are close to the testing point. Every global algorithm such as support vector machines (SVM) can be localized in the following way: in every testing point, the (global) learning algorithm is not applied to the whole training data but only to the k nearest neighbors (kNN) of the testing point. In case of support vector machines, the success of such mixtures of SVM and kNN (called SVM-KNN) has been shown in extensive simulation studies and also for real data sets but only little has been known on theoretical properties so far. In the present article, it is shown how a large class of regularized kernel methods (including SVM) can be localized in order to get a universally consistent learning algorithm. Keywords: machine learning, regularized kernel methods, localization, SVM, k-nearest neighbors, SVM-KNN</p><p>6 0.043368828 <a title="14-tfidf-6" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>7 0.038917352 <a title="14-tfidf-7" href="./jmlr-2013-Derivative_Estimation_with_Local_Polynomial_Fitting.html">31 jmlr-2013-Derivative Estimation with Local Polynomial Fitting</a></p>
<p>8 0.038719784 <a title="14-tfidf-8" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>9 0.03672317 <a title="14-tfidf-9" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>10 0.036147404 <a title="14-tfidf-10" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>11 0.036128242 <a title="14-tfidf-11" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>12 0.036116801 <a title="14-tfidf-12" href="./jmlr-2013-Variational_Inference_in_Nonconjugate_Models.html">121 jmlr-2013-Variational Inference in Nonconjugate Models</a></p>
<p>13 0.029511517 <a title="14-tfidf-13" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>14 0.029308282 <a title="14-tfidf-14" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>15 0.029171407 <a title="14-tfidf-15" href="./jmlr-2013-Regularization-Free_Principal_Curve_Estimation.html">96 jmlr-2013-Regularization-Free Principal Curve Estimation</a></p>
<p>16 0.027875122 <a title="14-tfidf-16" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>17 0.027466582 <a title="14-tfidf-17" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>18 0.027194776 <a title="14-tfidf-18" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<p>19 0.026372246 <a title="14-tfidf-19" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>20 0.025160894 <a title="14-tfidf-20" href="./jmlr-2013-On_the_Convergence_of_Maximum_Variance_Unfolding.html">77 jmlr-2013-On the Convergence of Maximum Variance Unfolding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.134), (1, -0.01), (2, 0.049), (3, -0.008), (4, -0.01), (5, -0.069), (6, 0.04), (7, 0.026), (8, 0.042), (9, -0.056), (10, -0.044), (11, 0.016), (12, 0.044), (13, -0.079), (14, -0.086), (15, 0.062), (16, -0.007), (17, -0.029), (18, 0.041), (19, 0.049), (20, 0.006), (21, -0.172), (22, 0.088), (23, -0.104), (24, -0.152), (25, -0.298), (26, 0.122), (27, -0.2), (28, 0.219), (29, -0.146), (30, 0.032), (31, 0.119), (32, -0.121), (33, -0.114), (34, -0.003), (35, 0.063), (36, 0.095), (37, -0.013), (38, 0.105), (39, -0.003), (40, -0.136), (41, -0.155), (42, 0.062), (43, 0.105), (44, 0.088), (45, -0.065), (46, 0.117), (47, 0.009), (48, 0.039), (49, 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93926114 <a title="14-lsi-1" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>Author: Pierre Neuvial</p><p>Abstract: The False Discovery Rate (FDR) is a commonly used type I error rate in multiple testing problems. It is deﬁned as the expected False Discovery Proportion (FDP), that is, the expected fraction of false positives among rejected hypotheses. When the hypotheses are independent, the BenjaminiHochberg procedure achieves FDR control at any pre-speciﬁed level. By construction, FDR control offers no guarantee in terms of power, or type II error. A number of alternative procedures have been developed, including plug-in procedures that aim at gaining power by incorporating an estimate of the proportion of true null hypotheses. In this paper, we study the asymptotic behavior of a class of plug-in procedures based on kernel estimators of the density of the p-values, as the number m of tested hypotheses grows to inﬁnity. In a setting where the hypotheses tested are independent, we prove that these procedures are asymptotically more powerful in two respects: (i) a tighter asymptotic FDR control for any target FDR level and (ii) a broader range of target levels yielding positive asymptotic power. We also show that this increased asymptotic power comes at the price of slower, non-parametric convergence rates for the FDP. These rates are of the form m−k/(2k+1) , where k is determined by the regularity of the density of the p-value distribution, or, equivalently, of the test statistics distribution. These results are applied to one- and two-sided tests statistics for Gaussian and Laplace location models, and for the Student model. Keywords: multiple testing, false discovery rate, Benjamini Hochberg’s procedure, power, criticality, plug-in procedures, adaptive control, test statistics distribution, convergence rates, kernel estimators</p><p>2 0.66933715 <a title="14-lsi-2" href="./jmlr-2013-Global_Analytic_Solution_of_Fully-observed_Variational_Bayesian_Matrix_Factorization.html">49 jmlr-2013-Global Analytic Solution of Fully-observed Variational Bayesian Matrix Factorization</a></p>
<p>Author: Shinichi Nakajima, Masashi Sugiyama, S. Derin Babacan, Ryota Tomioka</p><p>Abstract: The variational Bayesian (VB) approximation is known to be a promising approach to Bayesian estimation, when the rigorous calculation of the Bayes posterior is intractable. The VB approximation has been successfully applied to matrix factorization (MF), offering automatic dimensionality selection for principal component analysis. Generally, ﬁnding the VB solution is a non-convex problem, and most methods rely on a local search algorithm derived through a standard procedure for the VB approximation. In this paper, we show that a better option is available for fully-observed VBMF—the global solution can be analytically computed. More speciﬁcally, the global solution is a reweighted SVD of the observed matrix, and each weight can be obtained by solving a quartic equation with its coefﬁcients being functions of the observed singular value. We further show that the global optimal solution of empirical VBMF (where hyperparameters are also learned from data) can also be analytically computed. We illustrate the usefulness of our results through experiments in multi-variate analysis. Keywords: variational Bayes, matrix factorization, empirical Bayes, model-induced regularization, probabilistic PCA</p><p>3 0.33693871 <a title="14-lsi-3" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>Author: Edward Challis, David Barber</p><p>Abstract: We investigate Gaussian Kullback-Leibler (G-KL) variational approximate inference techniques for Bayesian generalised linear models and various extensions. In particular we make the following novel contributions: sufﬁcient conditions for which the G-KL objective is differentiable and convex are described; constrained parameterisations of Gaussian covariance that make G-KL methods fast and scalable are provided; the lower bound to the normalisation constant provided by G-KL methods is proven to dominate those provided by local lower bounding methods; complexity and model applicability issues of G-KL versus other Gaussian approximate inference methods are discussed. Numerical results comparing G-KL and other deterministic Gaussian approximate inference methods are presented for: robust Gaussian process regression models with either Student-t or Laplace likelihoods, large scale Bayesian binary logistic regression models, and Bayesian sparse linear models for sequential experimental design. Keywords: generalised linear models, latent linear models, variational approximate inference, large scale inference, sparse learning, experimental design, active learning, Gaussian processes</p><p>4 0.23487428 <a title="14-lsi-4" href="./jmlr-2013-Manifold_Regularization_and_Semi-supervised_Learning%3A_Some_Theoretical_Analyses.html">69 jmlr-2013-Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses</a></p>
<p>Author: Partha Niyogi</p><p>Abstract: Manifold regularization (Belkin et al., 2006) is a geometrically motivated framework for machine learning within which several semi-supervised algorithms have been constructed. Here we try to provide some theoretical understanding of this approach. Our main result is to expose the natural structure of a class of problems on which manifold regularization methods are helpful. We show that for such problems, no supervised learner can learn effectively. On the other hand, a manifold based learner (that knows the manifold or “learns” it from unlabeled examples) can learn with relatively few labeled examples. Our analysis follows a minimax style with an emphasis on ﬁnite sample results (in terms of n: the number of labeled examples). These results allow us to properly interpret manifold regularization and related spectral and geometric algorithms in terms of their potential use in semi-supervised learning. Keywords: semi-supervised learning, manifold regularization, graph Laplacian, minimax rates</p><p>5 0.23392208 <a title="14-lsi-5" href="./jmlr-2013-Derivative_Estimation_with_Local_Polynomial_Fitting.html">31 jmlr-2013-Derivative Estimation with Local Polynomial Fitting</a></p>
<p>Author: Kris De Brabanter, Jos De Brabanter, Bart De Moor, Irène Gijbels</p><p>Abstract: We present a fully automated framework to estimate derivatives nonparametrically without estimating the regression function. Derivative estimation plays an important role in the exploration of structures in curves (jump detection and discontinuities), comparison of regression curves, analysis of human growth data, etc. Hence, the study of estimating derivatives is equally important as regression estimation itself. Via empirical derivatives we approximate the qth order derivative and create a new data set which can be smoothed by any nonparametric regression estimator. We derive L1 and L2 rates and establish consistency of the estimator. The new data sets created by this technique are no longer independent and identically distributed (i.i.d.) random variables anymore. As a consequence, automated model selection criteria (data-driven procedures) break down. Therefore, we propose a simple factor method, based on bimodal kernels, to effectively deal with correlated data in the local polynomial regression framework. Keywords: nonparametric derivative estimation, model selection, empirical derivative, factor rule</p><p>6 0.2330994 <a title="14-lsi-6" href="./jmlr-2013-Kernel_Bayes%27_Rule%3A_Bayesian_Inference_with_Positive_Definite_Kernels.html">57 jmlr-2013-Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels</a></p>
<p>7 0.22385369 <a title="14-lsi-7" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>8 0.21604495 <a title="14-lsi-8" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>9 0.2078405 <a title="14-lsi-9" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>10 0.20673537 <a title="14-lsi-10" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>11 0.19573475 <a title="14-lsi-11" href="./jmlr-2013-Regularization-Free_Principal_Curve_Estimation.html">96 jmlr-2013-Regularization-Free Principal Curve Estimation</a></p>
<p>12 0.19001471 <a title="14-lsi-12" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>13 0.18806811 <a title="14-lsi-13" href="./jmlr-2013-Distributions_of_Angles_in_Random_Packing_on_Spheres.html">36 jmlr-2013-Distributions of Angles in Random Packing on Spheres</a></p>
<p>14 0.18260856 <a title="14-lsi-14" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>15 0.18051557 <a title="14-lsi-15" href="./jmlr-2013-Random_Walk_Kernels_and_Learning_Curves_for_Gaussian_Process_Regression_on_Random_Graphs.html">93 jmlr-2013-Random Walk Kernels and Learning Curves for Gaussian Process Regression on Random Graphs</a></p>
<p>16 0.17615585 <a title="14-lsi-16" href="./jmlr-2013-Optimal_Discovery_with_Probabilistic_Expert_Advice%3A_Finite_Time_Analysis_and_Macroscopic_Optimality.html">81 jmlr-2013-Optimal Discovery with Probabilistic Expert Advice: Finite Time Analysis and Macroscopic Optimality</a></p>
<p>17 0.17378713 <a title="14-lsi-17" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>18 0.17184474 <a title="14-lsi-18" href="./jmlr-2013-Joint_Harmonic_Functions_and_Their_Supervised_Connections.html">55 jmlr-2013-Joint Harmonic Functions and Their Supervised Connections</a></p>
<p>19 0.16040583 <a title="14-lsi-19" href="./jmlr-2013-Universal_Consistency_of_Localized_Versions_of_Regularized_Kernel_Methods.html">117 jmlr-2013-Universal Consistency of Localized Versions of Regularized Kernel Methods</a></p>
<p>20 0.15928718 <a title="14-lsi-20" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.017), (5, 0.1), (6, 0.037), (10, 0.07), (16, 0.406), (20, 0.01), (23, 0.042), (41, 0.013), (68, 0.043), (70, 0.041), (75, 0.026), (85, 0.055), (87, 0.021), (89, 0.011), (93, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.66728252 <a title="14-lda-1" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>Author: Pierre Neuvial</p><p>Abstract: The False Discovery Rate (FDR) is a commonly used type I error rate in multiple testing problems. It is deﬁned as the expected False Discovery Proportion (FDP), that is, the expected fraction of false positives among rejected hypotheses. When the hypotheses are independent, the BenjaminiHochberg procedure achieves FDR control at any pre-speciﬁed level. By construction, FDR control offers no guarantee in terms of power, or type II error. A number of alternative procedures have been developed, including plug-in procedures that aim at gaining power by incorporating an estimate of the proportion of true null hypotheses. In this paper, we study the asymptotic behavior of a class of plug-in procedures based on kernel estimators of the density of the p-values, as the number m of tested hypotheses grows to inﬁnity. In a setting where the hypotheses tested are independent, we prove that these procedures are asymptotically more powerful in two respects: (i) a tighter asymptotic FDR control for any target FDR level and (ii) a broader range of target levels yielding positive asymptotic power. We also show that this increased asymptotic power comes at the price of slower, non-parametric convergence rates for the FDP. These rates are of the form m−k/(2k+1) , where k is determined by the regularity of the density of the p-value distribution, or, equivalently, of the test statistics distribution. These results are applied to one- and two-sided tests statistics for Gaussian and Laplace location models, and for the Student model. Keywords: multiple testing, false discovery rate, Benjamini Hochberg’s procedure, power, criticality, plug-in procedures, adaptive control, test statistics distribution, convergence rates, kernel estimators</p><p>2 0.34241444 <a title="14-lda-2" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>Author: Sébastien Gerchinovitz</p><p>Abstract: We consider the problem of online linear regression on arbitrary deterministic sequences when the ambient dimension d can be much larger than the number of time rounds T . We introduce the notion of sparsity regret bound, which is a deterministic online counterpart of recent risk bounds derived in the stochastic setting under a sparsity scenario. We prove such regret bounds for an online-learning algorithm called SeqSEW and based on exponential weighting and data-driven truncation. In a second part we apply a parameter-free version of this algorithm to the stochastic setting (regression model with random design). This yields risk bounds of the same ﬂavor as in Dalalyan and Tsybakov (2012a) but which solve two questions left open therein. In particular our risk bounds are adaptive (up to a logarithmic factor) to the unknown variance of the noise if the latter is Gaussian. We also address the regression model with ﬁxed design. Keywords: sparsity, online linear regression, individual sequences, adaptive regret bounds</p><p>3 0.34207949 <a title="14-lda-3" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>Author: Yuchen Zhang, John C. Duchi, Martin J. Wainwright</p><p>Abstract: We analyze two communication-efﬁcient algorithms for distributed optimization in statistical settings involving large-scale data sets. The ﬁrst algorithm is a standard averaging method that distributes the N data samples evenly to m machines, performs separate minimization on each subset, and then averages the estimates. We provide a sharp analysis of this average mixture algorithm, showing that under a reasonable set of conditions, the combined parameter achieves √ mean-squared error (MSE) that decays as O (N −1 + (N/m)−2 ). Whenever m ≤ N, this guarantee matches the best possible rate achievable by a centralized algorithm having access to all N samples. The second algorithm is a novel method, based on an appropriate form of bootstrap subsampling. Requiring only a single round of communication, it has mean-squared error that decays as O (N −1 + (N/m)−3 ), and so is more robust to the amount of parallelization. In addition, we show that a stochastic gradient-based method attains mean-squared error decaying as O (N −1 + (N/m)−3/2 ), easing computation at the expense of a potentially slower MSE rate. We also provide an experimental evaluation of our methods, investigating their performance both on simulated data and on a large-scale regression problem from the internet search domain. In particular, we show that our methods can be used to efﬁciently solve an advertisement prediction problem from the Chinese SoSo Search Engine, which involves logistic regression with N ≈ 2.4 × 108 samples and d ≈ 740,000 covariates. Keywords: distributed learning, stochastic optimization, averaging, subsampling</p><p>4 0.3366085 <a title="14-lda-4" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>Author: Takafumi Kanamori, Akiko Takeda, Taiji Suzuki</p><p>Abstract: There are two main approaches to binary classiÄ?Ĺš cation problems: the loss function approach and the uncertainty set approach. The loss function approach is widely used in real-world data analysis. Statistical decision theory has been used to elucidate its properties such as statistical consistency. Conditional probabilities can also be estimated by using the minimum solution of the loss function. In the uncertainty set approach, an uncertainty set is deÄ?Ĺš ned for each binary label from training samples. The best separating hyperplane between the two uncertainty sets is used as the decision function. Although the uncertainty set approach provides an intuitive understanding of learning algorithms, its statistical properties have not been sufÄ?Ĺš ciently studied. In this paper, we show that the uncertainty set is deeply connected with the convex conjugate of a loss function. On the basis of the conjugate relation, we propose a way of revising the uncertainty set approach so that it will have good statistical properties such as statistical consistency. We also introduce statistical models corresponding to uncertainty sets in order to estimate conditional probabilities. Finally, we present numerical experiments, verifying that the learning with revised uncertainty sets improves the prediction accuracy. Keywords: loss function, uncertainty set, convex conjugate, consistency</p><p>5 0.33570427 <a title="14-lda-5" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>Author: Tingni Sun, Cun-Hui Zhang</p><p>Abstract: We propose a new method of learning a sparse nonnegative-deﬁnite target matrix. Our primary example of the target matrix is the inverse of a population covariance or correlation matrix. The algorithm ﬁrst estimates each column of the target matrix by the scaled Lasso and then adjusts the matrix estimator to be symmetric. The penalty level of the scaled Lasso for each column is completely determined by data via convex minimization, without using cross-validation. We prove that this scaled Lasso method guarantees the fastest proven rate of convergence in the spectrum norm under conditions of weaker form than those in the existing analyses of other ℓ1 regularized algorithms, and has faster guaranteed rate of convergence when the ratio of the ℓ1 and spectrum norms of the target inverse matrix diverges to inﬁnity. A simulation study demonstrates the computational feasibility and superb performance of the proposed method. Our analysis also provides new performance bounds for the Lasso and scaled Lasso to guarantee higher concentration of the error at a smaller threshold level than previous analyses, and to allow the use of the union bound in column-by-column applications of the scaled Lasso without an adjustment of the penalty level. In addition, the least squares estimation after the scaled Lasso selection is considered and proven to guarantee performance bounds similar to that of the scaled Lasso. Keywords: precision matrix, concentration matrix, inverse matrix, graphical model, scaled Lasso, linear regression, spectrum norm</p><p>6 0.33564851 <a title="14-lda-6" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>7 0.3353734 <a title="14-lda-7" href="./jmlr-2013-On_the_Convergence_of_Maximum_Variance_Unfolding.html">77 jmlr-2013-On the Convergence of Maximum Variance Unfolding</a></p>
<p>8 0.33271122 <a title="14-lda-8" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>9 0.33237302 <a title="14-lda-9" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>10 0.33141351 <a title="14-lda-10" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>11 0.33117491 <a title="14-lda-11" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>12 0.32976285 <a title="14-lda-12" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>13 0.327447 <a title="14-lda-13" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>14 0.32708633 <a title="14-lda-14" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>15 0.32602084 <a title="14-lda-15" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>16 0.32557145 <a title="14-lda-16" href="./jmlr-2013-A_Near-Optimal_Algorithm_for_Differentially-Private_Principal_Components.html">5 jmlr-2013-A Near-Optimal Algorithm for Differentially-Private Principal Components</a></p>
<p>17 0.32428357 <a title="14-lda-17" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>18 0.32415205 <a title="14-lda-18" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>19 0.32398942 <a title="14-lda-19" href="./jmlr-2013-Beyond_Fano%27s_Inequality%3A_Bounds_on_the_Optimal_F-Score%2C_BER%2C_and_Cost-Sensitive_Risk_and_Their_Implications.html">18 jmlr-2013-Beyond Fano's Inequality: Bounds on the Optimal F-Score, BER, and Cost-Sensitive Risk and Their Implications</a></p>
<p>20 0.32312101 <a title="14-lda-20" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
