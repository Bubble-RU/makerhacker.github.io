<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-56" href="#">jmlr2013-56</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</h1>
<br/><p>Source: <a title="jmlr-2013-56-pdf" href="http://jmlr.org/papers/volume14/fanello13a/fanello13a.pdf">pdf</a></p><p>Author: Sean Ryan Fanello, Ilaria Gori, Giorgio Metta, Francesca Odone</p><p>Abstract: Sparsity has been showed to be one of the most important properties for visual recognition purposes. In this paper we show that sparse representation plays a fundamental role in achieving one-shot learning and real-time recognition of actions. We start off from RGBD images, combine motion and appearance cues and extract state-of-the-art features in a computationally efﬁcient way. The proposed method relies on descriptors based on 3D Histograms of Scene Flow (3DHOFs) and Global Histograms of Oriented Gradient (GHOGs); adaptive sparse coding is applied to capture high-level patterns from data. We then propose a simultaneous on-line video segmentation and recognition of actions using linear SVMs. The main contribution of the paper is an effective realtime system for one-shot action modeling and recognition; the paper highlights the effectiveness of sparse coding techniques to represent 3D actions. We obtain very good results on three different data sets: a benchmark data set for one-shot action learning (the ChaLearn Gesture Data Set), an in-house data set acquired by a Kinect sensor including complex actions and gestures differing by small details, and a data set created for human-robot interaction purposes. Finally we demonstrate that our system is effective also in a human-robot interaction setting and propose a memory game, “All Gestures You Can”, to be played against a humanoid robot. Keywords: real-time action recognition, sparse representation, one-shot action learning, human robot interaction</p><p>Reference: <a title="jmlr-2013-56-reference" href="../jmlr2013_reference/jmlr-2013-Keep_It_Simple_And_Sparse%3A_Real-Time_Action_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We start off from RGBD images, combine motion and appearance cues and extract state-of-the-art features in a computationally efﬁcient way. [sent-11, score-0.337]
</p><p>2 The proposed method relies on descriptors based on 3D Histograms of Scene Flow (3DHOFs) and Global Histograms of Oriented Gradient (GHOGs); adaptive sparse coding is applied to capture high-level patterns from data. [sent-12, score-0.367]
</p><p>3 We then propose a simultaneous on-line video segmentation and recognition of actions using linear SVMs. [sent-13, score-0.646]
</p><p>4 The main contribution of the paper is an effective realtime system for one-shot action modeling and recognition; the paper highlights the effectiveness of sparse coding techniques to represent 3D actions. [sent-14, score-0.571]
</p><p>5 Keywords: real-time action recognition, sparse representation, one-shot action learning, human robot interaction  1. [sent-17, score-0.953]
</p><p>6 Introduction Action recognition as a general problem is a very fertile research theme due to its strong applicability in several real world domains, ranging from video-surveillance to content-based video retrieval and video classiﬁcation. [sent-18, score-0.499]
</p><p>7 This paper refers speciﬁcally to action recognition in the context of HumanMachine Interaction (HMI), and therefore it focuses on whole-body actions performed by a human who is standing at a short distance from the sensor. [sent-19, score-0.805]
</p><p>8 , 2011) by Microsoft); these depth-based sensors are drastically changing the ﬁeld of action recognition, enabling the achievement of high performance using fast algorithms. [sent-31, score-0.321]
</p><p>9 Following this recent trend we propose a complete system based on RGBD video sequences, which models actions from one example only. [sent-32, score-0.414]
</p><p>10 Subsequently, we summarize the action within adjacent frames by building feature vectors that describe the feature evolution over time. [sent-36, score-0.371]
</p><p>11 Finally, we train a Support Vector Machine (SVM) for each action class. [sent-37, score-0.321]
</p><p>12 Furthermore, thanks to the simultaneous appearance and motion description complemented by the sparse coding stage, the method provides a one-shot learning procedure. [sent-39, score-0.497]
</p><p>13 Our objective in designing this interaction game is to stress the effectiveness of our gesture recognition system in complex and uncontrolled settings. [sent-43, score-0.615]
</p><p>14 In particular, some approaches are based on machine learning techniques, where each action is described as a complex structure; in this class we ﬁnd methods based on Hidden Markov Models (Malgireddy et al. [sent-61, score-0.321]
</p><p>15 , 2012), Coupled Hidden Semi-Markov models (Natarajan and Nevatia, 2007), action graphs (Li et al. [sent-62, score-0.321]
</p><p>16 Other methods are based on matching: the recognition of actions is carried out through a similarity match with all the available data, and the most similar datum dictates the estimated class (Seo and Milanfar, 2012; Mahbub et al. [sent-65, score-0.4]
</p><p>17 Each action is then modeled as a multi channel Hidden Markov Model (mcHMM). [sent-72, score-0.321]
</p><p>18 Another recent method following the trend of matching-based action recognition algorithms is Mahbub et al. [sent-75, score-0.51]
</p><p>19 2619  FANELLO , G ORI , M ETTA AND O DONE  An alternative to classifying gesture recognition algorithms is based on the data representation of gesture models. [sent-81, score-0.699]
</p><p>20 Differently from these works, our approach aims speciﬁcally to obtain an accurate real-time recognition from one video example only. [sent-103, score-0.344]
</p><p>21 We conclude the section with a reference to some works focusing on continuous action or activity recognition (Ali and Aggarwal, 2001; Green and Guan, 2004; Liao et al. [sent-104, score-0.541]
</p><p>22 Our work deals with continuous action recognition as well, indeed the proposed framework comprehends a novel and robust temporal segmentation algorithm. [sent-108, score-0.63]
</p><p>23 In this work we rely on sparse coding to obtain a compact descriptor with a good discriminative power even if it is derived from very small data sets. [sent-112, score-0.393]
</p><p>24 We refer to adaptive sparse coding when the coding is driven by data. [sent-114, score-0.351]
</p><p>25 The motivations behind the use of image coding arise from biology: there is evidence that similar signal coding happens in the neurons of the primary visual cortex (V1), which produces sparse and overcomplete activations (Olshausen and Fieldt, 1997). [sent-118, score-0.433]
</p><p>26 The action of coding is followed by a pooling stage, whose purpose is to aggregate multiple local descriptors in a single and global one. [sent-145, score-0.717]
</p><p>27 The ﬁnal descriptor of the image is the concatenation of the descriptors ps among all the regions. [sent-157, score-0.361]
</p><p>28 Action Recognition System In this section we describe the versatile real-time action recognition system we propose. [sent-161, score-0.558]
</p><p>29 The resultant 3DHOF+GHOG descriptor is processed via a sparse coding step to compute a compact and meaningful representation of the performed action. [sent-165, score-0.384]
</p><p>30 A novel on-line video segmentation algorithm is proposed which allows isolating different actions while recognizing the action sequence. [sent-167, score-0.778]
</p><p>31 1 Region Of Interest Segmentation The ﬁrst step of each action recognition system is to identify correctly where in the image the action is occurring. [sent-169, score-0.931]
</p><p>32 Many suggests that motion alone can be used to recognize actions (Bisio et al. [sent-187, score-0.414]
</p><p>33 In artiﬁcial systems this developmental-scale experience is typically not available, although actions can still be represented from two main cues: motion and appearance (Giese and Poggio, 2003). [sent-189, score-0.506]
</p><p>34 Although many variants of complex features describing human actions have been proposed, many of them imply computationally expensive routines. [sent-190, score-0.337]
</p><p>35 2 G LOBAL H ISTOGRAM  OF  O RIENTED G RADIENT  In speciﬁc contexts, motion information is not sufﬁcient to discriminate actions, and information on the pose or appearance becomes crucial. [sent-226, score-0.334]
</p><p>36 Thus we extend the motion descriptor with a shape feature computed on the depth map. [sent-229, score-0.434]
</p><p>37 This appearance descriptor produces an overall description of the appearance of the ROI without splitting the image in cells. [sent-235, score-0.438]
</p><p>38 3 S PARSE C ODING At this stage, each frame Ft is represented by two global descriptors: z(t) ∈ Rn1 for the motion component and h(t) ∈ Rn2 for the appearance component. [sent-241, score-0.408]
</p><p>39 , z(K)], where K is the number of all the frames in the training data, our goal is to learn one motion dictionary DM (a n1 × d1 matrix, with d1 the dictionary size and n1 the motion vector size) and the codes UM (a d1 × K matrix) that minimize the Equation 1, so that z(t) ∼ DM uM (t). [sent-246, score-0.5]
</p><p>40 The local minima of the standard deviation function are break points that deﬁne the end of an action and the beginning of another one. [sent-249, score-0.321]
</p><p>41 Therefore, after the Sparse Coding stage, we can describe a frame as a code u(i), which is the concatenation of the motion and appearance codes: u(i) = [uM (i), uG (i)]. [sent-257, score-0.408]
</p><p>42 3 Learning and Recognition The goal of this phase is to learn a model of a given action from data. [sent-260, score-0.321]
</p><p>43 Since we are implementing a one-shot action recognition system, the available training data amounts to one training sequence for each action of interest. [sent-261, score-0.831]
</p><p>44 In order to model the temporal extent of an action we extract sets of sub-sequences from a sequence, each one containing T adjacent frames. [sent-262, score-0.35]
</p><p>45 The remainder of the section describes in details the two phases of action learning and action recognition. [sent-277, score-0.642]
</p><p>46 Blue dots are the break points computed by the video segmentation algorithm that indicate the end of an action and the beginning of a new one. [sent-281, score-0.567]
</p><p>47 1 ACTION L EARNING Given a video Vs of ts frames, containing only one action As , we compute a set of descriptors [u(1), . [sent-284, score-0.641]
</p><p>48 Then, action learning is carried out on a set of data that are descriptions of a frame buffer BT (t), where T is its length: BT (t) = (u(t − T ), . [sent-289, score-0.516]
</p><p>49 , BT (ts )] computed from the single video Vs of the class As are used as positive examples for the action As . [sent-300, score-0.476]
</p><p>50 Although we use only one example for each class, we beneﬁt from the chosen representation: indeed, descriptors are computed per frame, therefore one single video of length ts provides a number of examples equal to ts − T where T is the buffer size. [sent-302, score-0.402]
</p><p>51 As long as the scores evolve we need to predict (on-line) when an action ends and another one begins; this is achieved computing the standard deviation σ(H) for a ﬁxed t over all the scores Hit (Figure 4, right chart). [sent-317, score-0.385]
</p><p>52 When an action ends we can expect all the SVM output scores to be similar, because no model should be predominant with respect to idle states; this brings to a local minimum in the function σ(H). [sent-318, score-0.353]
</p><p>53 Therefore, each local minimum corresponds to the end of an action and the beginning of a new one. [sent-319, score-0.321]
</p><p>54 When the standard deviation trend is below the mean, all the SVMs scores predict similar values, hence it is likely that an action has just ended. [sent-322, score-0.353]
</p><p>55 On the right the overall Levenshtein Distance computed in 20 batches with respect to the buffer size parameter is depicted for both 3DHOF+GHOG features and descriptors processed with sparse coding. [sent-340, score-0.408]
</p><p>56 We empirically choose a quantization parameter for the 3DHOF, n1 equal to 5, n2 = 64 bins for the GHOG descriptor, and dictionary sizes d1 and d2 equal to 256 for both motion and appearance components. [sent-341, score-0.346]
</p><p>57 This led to a frame descriptor of size 189 for simple descriptors, which increases to 512 after the sparse coding processing. [sent-342, score-0.459]
</p><p>58 The data set is organized in batches, where each batch includes 100 recorded gestures grouped in sequences of 1 to 5 gestures arbitrarily performed at different speeds. [sent-348, score-0.426]
</p><p>59 The gestures are drawn from a small vocabulary of 8 to 15 unique gestures called lexicon, which is deﬁned within a batch. [sent-349, score-0.426]
</p><p>60 11% for features processed with sparse coding, whereas simple 3DHOF+GHOG descriptors without sparse coding lead to a performance of 43. [sent-356, score-0.462]
</p><p>61 In the recognition phase we classify each slice of the video comparing it with all the templates. [sent-385, score-0.344]
</p><p>62 (2012) has a training computational complexity of O (n × k2 ) for each action class, where k is the number of HMM states and n the number of examples, while the testing computational complexity for a video frame is O (k2 ). [sent-398, score-0.589]
</p><p>63 Furthermore our on-line video segmentation algorithm shows excellent results with respect to the temporal segmentation used in the compared frameworks; in fact it is worth noting that the proposed algorithm leads to an action detection error rate TeLen = FP+FN equal to 5. [sent-404, score-0.687]
</p><p>64 In general we notice that the combination of both motion and appearance descriptors leads to the best results when the lexicon is composed of actions where both motion and appearance are equally important. [sent-410, score-1.048]
</p><p>65 The error obtained using only the 3DHOF descriptors was expected, due to the nature of the lexicons chosen: indeed in most gestures the motion component has little signiﬁcance. [sent-416, score-0.552]
</p><p>66 Considering instead batch devel 01, where motion is an important component in the gesture vocabulary, we have that 3DHOF descriptors lead to a Levenshtein Distance equal to 29. [sent-417, score-0.618]
</p><p>67 2 L INEAR VS N ON -L INEAR C LASSIFIERS In this section we compare the performances of linear and non linear SVM for the action recognition task. [sent-424, score-0.51]
</p><p>68 For this experiment we used coded features where both motion and appearance are employed. [sent-429, score-0.337]
</p><p>69 1, we noted that the resolution of the proposed appearance descriptor is quite low and may not be ideal when actions differ by small details, especially on the hands, therefore a localization of the interesting parts to model would be effective. [sent-436, score-0.476]
</p><p>70 The simplest way to build in this speciﬁc information is to resort to a body part tracker; indeed, if a body tracker were available it would have been easy to extract descriptors from different limbs and then concatenate all the features to obtain the ﬁnal frame representation. [sent-437, score-0.519]
</p><p>71 2; these gestures are difﬁcult to model without a proper body tracker, indeed the most contribution for the GHOG comes from the body shape rather than the hand. [sent-443, score-0.387]
</p><p>72 Then, we slightly modify the approach, computing 3DHOF and GHOG descriptors on three different body parts (left/right hand and whole body shape); the ﬁnal frame representation becomes the concatenation of all the part descriptors. [sent-449, score-0.46]
</p><p>73 In Figure 7 on the left the overall accuracy is shown; using sparse coding descriptors computed only on the body shape we obtain a Levenshtein Distance around 30%. [sent-451, score-0.469]
</p><p>74 By concatenating descriptors extracted from the hands the system achieves 10% for features enhanced with sparse coding and 20% for normal descriptors. [sent-452, score-0.457]
</p><p>75 3 Human-Robot Interaction The action recognition system has been implemented and tested on the iCub, a 53 degrees of freedom humanoid robot developed by the RobotCub Consortium (Metta et al. [sent-458, score-0.788]
</p><p>76 In this setting the action recognition system can be used for more general purposes such as Human-Robot-Interaction (HRI) or learning by imitation tasks. [sent-462, score-0.558]
</p><p>77 Each action is modeled using only the motion component (3DHOF), since we want the descriptor to be independent on the particular object shape used. [sent-468, score-0.669]
</p><p>78 Our game takes inspiration from the classic “Simon” game; nevertheless, since the original version has been often deﬁned as “visually boring”, we developed a revisited version, based on gesture recognition, which involves a “less boring” opponent: the iCub (Metta et al. [sent-475, score-0.338]
</p><p>79 Both the human and the robot have to take turns and perform the longest possible sequence of gestures by adding one gesture at each turn: one player starts performing a gesture, the opponent has to recognize the gesture, imitate it and add another gesture to the sequence. [sent-477, score-0.932]
</p><p>80 The game is carried on until one of the two players loses: the human player can lose because of limited memory skills, whereas the robot can lose because the gesture recognition system fails. [sent-478, score-0.871]
</p><p>81 The typical game setting is shown in Figure 10: the player stays in front of the robot while performing gestures that are recognized with Kinect. [sent-481, score-0.449]
</p><p>82 Importantly, hand gestures cannot be learned exploiting the Skeleton Data of Kinect: the body tracker detects the position of the hand and it is not enough to discriminate more complicate actions,—for example, see gesture classes 1 and 5 or 2 and 6 in Figure 9. [sent-482, score-0.576]
</p><p>83 The vision system has been trained using 8 different actors performing each gesture class for 3 times. [sent-484, score-0.392]
</p><p>84 There are three main modules that take care of recognizing the action sequence, deﬁning the game rules and making the robot gestures. [sent-486, score-0.557]
</p><p>85 This result indicates that the recognition system is robust also to different players performing variable gestures at various speeds. [sent-495, score-0.488]
</p><p>86 Discussion This paper presented the design and implementation of a complete action recognition system to be used in real world applications such as HMI. [sent-500, score-0.558]
</p><p>87 Left: the human player performs the ﬁrst gesture of the sequence. [sent-513, score-0.32]
</p><p>88 One-Shot Learning: one example is sufﬁcient to teach an new action to the system; this is mainly due to the effective per-frame representation. [sent-521, score-0.353]
</p><p>89 Sparse Frame Representation: starting from a simple and computationally inexpensive description that combines global motion (3DHOF) and appearance (GHOG) information over a ROI, subsequently ﬁltered through sparse coding, we obtained a sparse representation at each frame. [sent-523, score-0.439]
</p><p>90 We showed that these global descriptors are appropriate to model actions of the upper body of a person. [sent-524, score-0.448]
</p><p>91 On-line Video Segmentation: we propose a new, effective, reliable and on-line video segmentation algorithm that achieved a 5% error rate on action detection on a set of 2000 actions grouped in sequences of 1 to 5 gestures. [sent-526, score-0.778]
</p><p>92 This segmentation procedure works concurrently with the recognition process, thus a sequence of actions is simultaneously segmented and recognized. [sent-527, score-0.491]
</p><p>93 For testing purposes, we proposed a memory game, called “All Gestures You Can”, where a person can challenge the iCub robot on action recognition and sequencing. [sent-533, score-0.684]
</p><p>94 The approach is competitive against many of the state-of-the-art methods for action recognition. [sent-541, score-0.321]
</p><p>95 We are currently working on a more precise appearance description at frame level still under the severe constraint of real-time performance; this would enable the use of more complex actions even when the body tracker is not available. [sent-542, score-0.572]
</p><p>96 A uniﬁed framework for gesture recognition and spatiotemporal gesture segmentation. [sent-564, score-0.661]
</p><p>97 All gestures you can: a memory game against a humanoid robot. [sent-693, score-0.451]
</p><p>98 Continuous human action segmentation and recognition using a spatio-temporal probabilistic framework. [sent-774, score-0.685]
</p><p>99 Single view human action recognition using key pose matching and viterbi path searching. [sent-784, score-0.633]
</p><p>100 Real-time human pose recognition in parts from a single depth image. [sent-895, score-0.398]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('action', 0.321), ('gesture', 0.236), ('gestures', 0.213), ('actions', 0.211), ('fanello', 0.203), ('recognition', 0.189), ('motion', 0.174), ('descriptors', 0.165), ('levenshtein', 0.156), ('video', 0.155), ('coding', 0.149), ('descriptor', 0.144), ('ghog', 0.139), ('robot', 0.134), ('etta', 0.128), ('icub', 0.128), ('chalearn', 0.124), ('appearance', 0.121), ('eep', 0.118), ('imple', 0.118), ('frame', 0.113), ('roi', 0.11), ('ori', 0.11), ('vision', 0.108), ('kinect', 0.107), ('game', 0.102), ('eal', 0.101), ('ecognition', 0.099), ('gori', 0.096), ('humanoid', 0.096), ('metta', 0.096), ('segmentation', 0.091), ('depth', 0.086), ('human', 0.084), ('ime', 0.084), ('lexicon', 0.082), ('buffer', 0.082), ('pooling', 0.082), ('histograms', 0.079), ('malgireddy', 0.074), ('body', 0.072), ('parse', 0.07), ('batches', 0.066), ('scene', 0.064), ('tracker', 0.055), ('mahbub', 0.053), ('sparse', 0.053), ('image', 0.052), ('dictionary', 0.051), ('frames', 0.05), ('system', 0.048), ('histogram', 0.048), ('flow', 0.047), ('discriminative', 0.047), ('oriented', 0.046), ('svm', 0.044), ('svms', 0.044), ('devel', 0.043), ('ghogs', 0.043), ('mhi', 0.043), ('odone', 0.043), ('rgbd', 0.043), ('features', 0.042), ('stage', 0.042), ('invariance', 0.041), ('laptev', 0.041), ('workshops', 0.041), ('ow', 0.041), ('memory', 0.04), ('interaction', 0.04), ('pose', 0.039), ('representation', 0.038), ('players', 0.038), ('pattern', 0.038), ('wu', 0.037), ('ngers', 0.037), ('pipeline', 0.036), ('videos', 0.036), ('bt', 0.034), ('ft', 0.033), ('scores', 0.032), ('adhd', 0.032), ('bobick', 0.032), ('cit', 0.032), ('comoldi', 0.032), ('disorder', 0.032), ('francesca', 0.032), ('giorgio', 0.032), ('hmi', 0.032), ('hri', 0.032), ('ilaria', 0.032), ('shotton', 0.032), ('teach', 0.032), ('telen', 0.032), ('telev', 0.032), ('activity', 0.031), ('shape', 0.03), ('visual', 0.03), ('recognize', 0.029), ('temporal', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000014 <a title="56-tfidf-1" href="./jmlr-2013-Keep_It_Simple_And_Sparse%3A_Real-Time_Action_Recognition.html">56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</a></p>
<p>Author: Sean Ryan Fanello, Ilaria Gori, Giorgio Metta, Francesca Odone</p><p>Abstract: Sparsity has been showed to be one of the most important properties for visual recognition purposes. In this paper we show that sparse representation plays a fundamental role in achieving one-shot learning and real-time recognition of actions. We start off from RGBD images, combine motion and appearance cues and extract state-of-the-art features in a computationally efﬁcient way. The proposed method relies on descriptors based on 3D Histograms of Scene Flow (3DHOFs) and Global Histograms of Oriented Gradient (GHOGs); adaptive sparse coding is applied to capture high-level patterns from data. We then propose a simultaneous on-line video segmentation and recognition of actions using linear SVMs. The main contribution of the paper is an effective realtime system for one-shot action modeling and recognition; the paper highlights the effectiveness of sparse coding techniques to represent 3D actions. We obtain very good results on three different data sets: a benchmark data set for one-shot action learning (the ChaLearn Gesture Data Set), an in-house data set acquired by a Kinect sensor including complex actions and gestures differing by small details, and a data set created for human-robot interaction purposes. Finally we demonstrate that our system is effective also in a human-robot interaction setting and propose a memory game, “All Gestures You Can”, to be played against a humanoid robot. Keywords: real-time action recognition, sparse representation, one-shot action learning, human robot interaction</p><p>2 0.35482365 <a title="56-tfidf-2" href="./jmlr-2013-Language-Motivated_Approaches_to_Action_Recognition.html">58 jmlr-2013-Language-Motivated Approaches to Action Recognition</a></p>
<p>Author: Manavender R. Malgireddy, Ifeoma Nwogu, Venu Govindaraju</p><p>Abstract: We present language-motivated approaches to detecting, localizing and classifying activities and gestures in videos. In order to obtain statistical insight into the underlying patterns of motions in activities, we develop a dynamic, hierarchical Bayesian model which connects low-level visual features in videos with poses, motion patterns and classes of activities. This process is somewhat analogous to the method of detecting topics or categories from documents based on the word content of the documents, except that our documents are dynamic. The proposed generative model harnesses both the temporal ordering power of dynamic Bayesian networks such as hidden Markov models (HMMs) and the automatic clustering power of hierarchical Bayesian models such as the latent Dirichlet allocation (LDA) model. We also introduce a probabilistic framework for detecting and localizing pre-speciﬁed activities (or gestures) in a video sequence, analogous to the use of ﬁller models for keyword detection in speech processing. We demonstrate the robustness of our classiﬁcation model and our spotting framework by recognizing activities in unconstrained real-life video sequences and by spotting gestures via a one-shot-learning approach. Keywords: dynamic hierarchical Bayesian networks, topic models, activity recognition, gesture spotting, generative models</p><p>3 0.32999665 <a title="56-tfidf-3" href="./jmlr-2013-One-shot_Learning_Gesture_Recognition_from_RGB-D_Data_Using_Bag_of_Features.html">80 jmlr-2013-One-shot Learning Gesture Recognition from RGB-D Data Using Bag of Features</a></p>
<p>Author: Jun Wan, Qiuqi Ruan, Wei Li, Shuang Deng</p><p>Abstract: For one-shot learning gesture recognition, two important challenges are: how to extract distinctive features and how to learn a discriminative model from only one training sample per gesture class. For feature extraction, a new spatio-temporal feature representation called 3D enhanced motion scale-invariant feature transform (3D EMoSIFT) is proposed, which fuses RGB-D data. Compared with other features, the new feature set is invariant to scale and rotation, and has more compact and richer visual representations. For learning a discriminative model, all features extracted from training samples are clustered with the k-means algorithm to learn a visual codebook. Then, unlike the traditional bag of feature (BoF) models using vector quantization (VQ) to map each feature into a certain visual codeword, a sparse coding method named simulation orthogonal matching pursuit (SOMP) is applied and thus each feature can be represented by some linear combination of a small number of codewords. Compared with VQ, SOMP leads to a much lower reconstruction error and achieves better performance. The proposed approach has been evaluated on ChaLearn gesture database and the result has been ranked amongst the top best performing techniques on ChaLearn gesture challenge (round 2). Keywords: gesture recognition, bag of features (BoF) model, one-shot learning, 3D enhanced motion scale invariant feature transform (3D EMoSIFT), Simulation Orthogonal Matching Pursuit (SOMP)</p><p>4 0.20689127 <a title="56-tfidf-4" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>Author: Daniel Kyu Hwa Kohlsdorf, Thad E. Starner</p><p>Abstract: Gestures for interfaces should be short, pleasing, intuitive, and easily recognized by a computer. However, it is a challenge for interface designers to create gestures easily distinguishable from users’ normal movements. Our tool MAGIC Summoning addresses this problem. Given a speciﬁc platform and task, we gather a large database of unlabeled sensor data captured in the environments in which the system will be used (an “Everyday Gesture Library” or EGL). The EGL is quantized and indexed via multi-dimensional Symbolic Aggregate approXimation (SAX) to enable quick searching. MAGIC exploits the SAX representation of the EGL to suggest gestures with a low likelihood of false triggering. Suggested gestures are ordered according to brevity and simplicity, freeing the interface designer to focus on the user experience. Once a gesture is selected, MAGIC can output synthetic examples of the gesture to train a chosen classiﬁer (for example, with a hidden Markov model). If the interface designer suggests his own gesture and provides several examples, MAGIC estimates how accurately that gesture can be recognized and estimates its false positive rate by comparing it against the natural movements in the EGL. We demonstrate MAGIC’s effectiveness in gesture selection and helpfulness in creating accurate gesture recognizers. Keywords: gesture recognition, gesture spotting, false positives, continuous recognition</p><p>5 0.10592437 <a title="56-tfidf-5" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>Author: Anastasios Roussos, Stavros Theodorakis, Vassilis Pitsikalis, Petros Maragos</p><p>Abstract: We propose the novel approach of dynamic afﬁne-invariant shape-appearance model (Aff-SAM) and employ it for handshape classiﬁcation and sign recognition in sign language (SL) videos. AffSAM offers a compact and descriptive representation of hand conﬁgurations as well as regularized model-ﬁtting, assisting hand tracking and extracting handshape features. We construct SA images representing the hand’s shape and appearance without landmark points. We model the variation of the images by linear combinations of eigenimages followed by afﬁne transformations, accounting for 3D hand pose changes and improving model’s compactness. We also incorporate static and dynamic handshape priors, offering robustness in occlusions, which occur often in signing. The approach includes an afﬁne signer adaptation component at the visual level, without requiring training from scratch a new singer-speciﬁc model. We rather employ a short development data set to adapt the models for a new signer. Experiments on the Boston-University-400 continuous SL corpus demonstrate improvements on handshape classiﬁcation when compared to other feature extraction approaches. Supplementary evaluations of sign recognition experiments, are conducted on a multi-signer, 100-sign data set, from the Greek sign language lemmas corpus. These explore the fusion with movement cues as well as signer adaptation of Aff-SAM to multiple signers providing promising results. Keywords: afﬁne-invariant shape-appearance model, landmarks-free shape representation, static and dynamic priors, feature extraction, handshape classiﬁcation</p><p>6 0.079764113 <a title="56-tfidf-6" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>7 0.069386013 <a title="56-tfidf-7" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>8 0.067847893 <a title="56-tfidf-8" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>9 0.063755132 <a title="56-tfidf-9" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>10 0.060289029 <a title="56-tfidf-10" href="./jmlr-2013-Optimally_Fuzzy_Temporal_Memory.html">82 jmlr-2013-Optimally Fuzzy Temporal Memory</a></p>
<p>11 0.057279252 <a title="56-tfidf-11" href="./jmlr-2013-AC%2B%2BTemplate-Based_Reinforcement_Learning_Library%3A_Fitting_the_Code_to_the_Mathematics.html">1 jmlr-2013-AC++Template-Based Reinforcement Learning Library: Fitting the Code to the Mathematics</a></p>
<p>12 0.045578156 <a title="56-tfidf-12" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>13 0.045539644 <a title="56-tfidf-13" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>14 0.042903107 <a title="56-tfidf-14" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>15 0.038689554 <a title="56-tfidf-15" href="./jmlr-2013-Convex_and_Scalable_Weakly_Labeled_SVMs.html">29 jmlr-2013-Convex and Scalable Weakly Labeled SVMs</a></p>
<p>16 0.035995461 <a title="56-tfidf-16" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>17 0.035899702 <a title="56-tfidf-17" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>18 0.032642119 <a title="56-tfidf-18" href="./jmlr-2013-A_Theory_of_Multiclass_Boosting.html">8 jmlr-2013-A Theory of Multiclass Boosting</a></p>
<p>19 0.030742113 <a title="56-tfidf-19" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>20 0.030032564 <a title="56-tfidf-20" href="./jmlr-2013-Counterfactual_Reasoning_and_Learning_Systems%3A_The_Example_of_Computational_Advertising.html">30 jmlr-2013-Counterfactual Reasoning and Learning Systems: The Example of Computational Advertising</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.209), (1, -0.035), (2, -0.677), (3, -0.039), (4, 0.01), (5, -0.178), (6, 0.033), (7, -0.027), (8, 0.015), (9, 0.048), (10, -0.069), (11, 0.084), (12, 0.056), (13, 0.017), (14, 0.031), (15, 0.018), (16, 0.019), (17, 0.039), (18, -0.006), (19, -0.032), (20, 0.018), (21, -0.02), (22, 0.04), (23, -0.021), (24, -0.018), (25, 0.003), (26, 0.007), (27, 0.019), (28, -0.008), (29, 0.005), (30, -0.012), (31, 0.014), (32, 0.042), (33, -0.007), (34, -0.004), (35, 0.02), (36, -0.027), (37, -0.007), (38, -0.002), (39, 0.056), (40, 0.007), (41, -0.007), (42, -0.0), (43, 0.004), (44, -0.02), (45, -0.021), (46, -0.006), (47, 0.01), (48, 0.03), (49, -0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96900177 <a title="56-lsi-1" href="./jmlr-2013-Keep_It_Simple_And_Sparse%3A_Real-Time_Action_Recognition.html">56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</a></p>
<p>Author: Sean Ryan Fanello, Ilaria Gori, Giorgio Metta, Francesca Odone</p><p>Abstract: Sparsity has been showed to be one of the most important properties for visual recognition purposes. In this paper we show that sparse representation plays a fundamental role in achieving one-shot learning and real-time recognition of actions. We start off from RGBD images, combine motion and appearance cues and extract state-of-the-art features in a computationally efﬁcient way. The proposed method relies on descriptors based on 3D Histograms of Scene Flow (3DHOFs) and Global Histograms of Oriented Gradient (GHOGs); adaptive sparse coding is applied to capture high-level patterns from data. We then propose a simultaneous on-line video segmentation and recognition of actions using linear SVMs. The main contribution of the paper is an effective realtime system for one-shot action modeling and recognition; the paper highlights the effectiveness of sparse coding techniques to represent 3D actions. We obtain very good results on three different data sets: a benchmark data set for one-shot action learning (the ChaLearn Gesture Data Set), an in-house data set acquired by a Kinect sensor including complex actions and gestures differing by small details, and a data set created for human-robot interaction purposes. Finally we demonstrate that our system is effective also in a human-robot interaction setting and propose a memory game, “All Gestures You Can”, to be played against a humanoid robot. Keywords: real-time action recognition, sparse representation, one-shot action learning, human robot interaction</p><p>2 0.94886106 <a title="56-lsi-2" href="./jmlr-2013-One-shot_Learning_Gesture_Recognition_from_RGB-D_Data_Using_Bag_of_Features.html">80 jmlr-2013-One-shot Learning Gesture Recognition from RGB-D Data Using Bag of Features</a></p>
<p>Author: Jun Wan, Qiuqi Ruan, Wei Li, Shuang Deng</p><p>Abstract: For one-shot learning gesture recognition, two important challenges are: how to extract distinctive features and how to learn a discriminative model from only one training sample per gesture class. For feature extraction, a new spatio-temporal feature representation called 3D enhanced motion scale-invariant feature transform (3D EMoSIFT) is proposed, which fuses RGB-D data. Compared with other features, the new feature set is invariant to scale and rotation, and has more compact and richer visual representations. For learning a discriminative model, all features extracted from training samples are clustered with the k-means algorithm to learn a visual codebook. Then, unlike the traditional bag of feature (BoF) models using vector quantization (VQ) to map each feature into a certain visual codeword, a sparse coding method named simulation orthogonal matching pursuit (SOMP) is applied and thus each feature can be represented by some linear combination of a small number of codewords. Compared with VQ, SOMP leads to a much lower reconstruction error and achieves better performance. The proposed approach has been evaluated on ChaLearn gesture database and the result has been ranked amongst the top best performing techniques on ChaLearn gesture challenge (round 2). Keywords: gesture recognition, bag of features (BoF) model, one-shot learning, 3D enhanced motion scale invariant feature transform (3D EMoSIFT), Simulation Orthogonal Matching Pursuit (SOMP)</p><p>3 0.90048963 <a title="56-lsi-3" href="./jmlr-2013-Language-Motivated_Approaches_to_Action_Recognition.html">58 jmlr-2013-Language-Motivated Approaches to Action Recognition</a></p>
<p>Author: Manavender R. Malgireddy, Ifeoma Nwogu, Venu Govindaraju</p><p>Abstract: We present language-motivated approaches to detecting, localizing and classifying activities and gestures in videos. In order to obtain statistical insight into the underlying patterns of motions in activities, we develop a dynamic, hierarchical Bayesian model which connects low-level visual features in videos with poses, motion patterns and classes of activities. This process is somewhat analogous to the method of detecting topics or categories from documents based on the word content of the documents, except that our documents are dynamic. The proposed generative model harnesses both the temporal ordering power of dynamic Bayesian networks such as hidden Markov models (HMMs) and the automatic clustering power of hierarchical Bayesian models such as the latent Dirichlet allocation (LDA) model. We also introduce a probabilistic framework for detecting and localizing pre-speciﬁed activities (or gestures) in a video sequence, analogous to the use of ﬁller models for keyword detection in speech processing. We demonstrate the robustness of our classiﬁcation model and our spotting framework by recognizing activities in unconstrained real-life video sequences and by spotting gestures via a one-shot-learning approach. Keywords: dynamic hierarchical Bayesian networks, topic models, activity recognition, gesture spotting, generative models</p><p>4 0.84724581 <a title="56-lsi-4" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>Author: Daniel Kyu Hwa Kohlsdorf, Thad E. Starner</p><p>Abstract: Gestures for interfaces should be short, pleasing, intuitive, and easily recognized by a computer. However, it is a challenge for interface designers to create gestures easily distinguishable from users’ normal movements. Our tool MAGIC Summoning addresses this problem. Given a speciﬁc platform and task, we gather a large database of unlabeled sensor data captured in the environments in which the system will be used (an “Everyday Gesture Library” or EGL). The EGL is quantized and indexed via multi-dimensional Symbolic Aggregate approXimation (SAX) to enable quick searching. MAGIC exploits the SAX representation of the EGL to suggest gestures with a low likelihood of false triggering. Suggested gestures are ordered according to brevity and simplicity, freeing the interface designer to focus on the user experience. Once a gesture is selected, MAGIC can output synthetic examples of the gesture to train a chosen classiﬁer (for example, with a hidden Markov model). If the interface designer suggests his own gesture and provides several examples, MAGIC estimates how accurately that gesture can be recognized and estimates its false positive rate by comparing it against the natural movements in the EGL. We demonstrate MAGIC’s effectiveness in gesture selection and helpfulness in creating accurate gesture recognizers. Keywords: gesture recognition, gesture spotting, false positives, continuous recognition</p><p>5 0.49332511 <a title="56-lsi-5" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>Author: Anastasios Roussos, Stavros Theodorakis, Vassilis Pitsikalis, Petros Maragos</p><p>Abstract: We propose the novel approach of dynamic afﬁne-invariant shape-appearance model (Aff-SAM) and employ it for handshape classiﬁcation and sign recognition in sign language (SL) videos. AffSAM offers a compact and descriptive representation of hand conﬁgurations as well as regularized model-ﬁtting, assisting hand tracking and extracting handshape features. We construct SA images representing the hand’s shape and appearance without landmark points. We model the variation of the images by linear combinations of eigenimages followed by afﬁne transformations, accounting for 3D hand pose changes and improving model’s compactness. We also incorporate static and dynamic handshape priors, offering robustness in occlusions, which occur often in signing. The approach includes an afﬁne signer adaptation component at the visual level, without requiring training from scratch a new singer-speciﬁc model. We rather employ a short development data set to adapt the models for a new signer. Experiments on the Boston-University-400 continuous SL corpus demonstrate improvements on handshape classiﬁcation when compared to other feature extraction approaches. Supplementary evaluations of sign recognition experiments, are conducted on a multi-signer, 100-sign data set, from the Greek sign language lemmas corpus. These explore the fusion with movement cues as well as signer adaptation of Aff-SAM to multiple signers providing promising results. Keywords: afﬁne-invariant shape-appearance model, landmarks-free shape representation, static and dynamic priors, feature extraction, handshape classiﬁcation</p><p>6 0.26283893 <a title="56-lsi-6" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>7 0.2382426 <a title="56-lsi-7" href="./jmlr-2013-Training_Energy-Based_Models_for_Time-Series_Imputation.html">115 jmlr-2013-Training Energy-Based Models for Time-Series Imputation</a></p>
<p>8 0.21585499 <a title="56-lsi-8" href="./jmlr-2013-AC%2B%2BTemplate-Based_Reinforcement_Learning_Library%3A_Fitting_the_Code_to_the_Mathematics.html">1 jmlr-2013-AC++Template-Based Reinforcement Learning Library: Fitting the Code to the Mathematics</a></p>
<p>9 0.20414743 <a title="56-lsi-9" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>10 0.19198489 <a title="56-lsi-10" href="./jmlr-2013-Optimally_Fuzzy_Temporal_Memory.html">82 jmlr-2013-Optimally Fuzzy Temporal Memory</a></p>
<p>11 0.18673031 <a title="56-lsi-11" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>12 0.18591908 <a title="56-lsi-12" href="./jmlr-2013-Classifying_With_Confidence_From_Incomplete_Information.html">22 jmlr-2013-Classifying With Confidence From Incomplete Information</a></p>
<p>13 0.18442437 <a title="56-lsi-13" href="./jmlr-2013-Sparse_Activity_and_Sparse_Connectivity_in_Supervised_Learning.html">101 jmlr-2013-Sparse Activity and Sparse Connectivity in Supervised Learning</a></p>
<p>14 0.18396834 <a title="56-lsi-14" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>15 0.16548935 <a title="56-lsi-15" href="./jmlr-2013-Classifier_Selection_using_the_Predicate_Depth.html">21 jmlr-2013-Classifier Selection using the Predicate Depth</a></p>
<p>16 0.1647277 <a title="56-lsi-16" href="./jmlr-2013-BudgetedSVM%3A_A_Toolbox_for_Scalable_SVM_Approximations.html">19 jmlr-2013-BudgetedSVM: A Toolbox for Scalable SVM Approximations</a></p>
<p>17 0.15418932 <a title="56-lsi-17" href="./jmlr-2013-Convex_and_Scalable_Weakly_Labeled_SVMs.html">29 jmlr-2013-Convex and Scalable Weakly Labeled SVMs</a></p>
<p>18 0.141139 <a title="56-lsi-18" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>19 0.13573952 <a title="56-lsi-19" href="./jmlr-2013-Truncated_Power_Method_for_Sparse_Eigenvalue_Problems.html">116 jmlr-2013-Truncated Power Method for Sparse Eigenvalue Problems</a></p>
<p>20 0.13512503 <a title="56-lsi-20" href="./jmlr-2013-Segregating_Event_Streams_and_Noise_with_a_Markov_Renewal_Process_Model.html">98 jmlr-2013-Segregating Event Streams and Noise with a Markov Renewal Process Model</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.018), (5, 0.082), (6, 0.026), (10, 0.049), (20, 0.045), (23, 0.583), (68, 0.02), (70, 0.013), (75, 0.033), (87, 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90437704 <a title="56-lda-1" href="./jmlr-2013-Keep_It_Simple_And_Sparse%3A_Real-Time_Action_Recognition.html">56 jmlr-2013-Keep It Simple And Sparse: Real-Time Action Recognition</a></p>
<p>Author: Sean Ryan Fanello, Ilaria Gori, Giorgio Metta, Francesca Odone</p><p>Abstract: Sparsity has been showed to be one of the most important properties for visual recognition purposes. In this paper we show that sparse representation plays a fundamental role in achieving one-shot learning and real-time recognition of actions. We start off from RGBD images, combine motion and appearance cues and extract state-of-the-art features in a computationally efﬁcient way. The proposed method relies on descriptors based on 3D Histograms of Scene Flow (3DHOFs) and Global Histograms of Oriented Gradient (GHOGs); adaptive sparse coding is applied to capture high-level patterns from data. We then propose a simultaneous on-line video segmentation and recognition of actions using linear SVMs. The main contribution of the paper is an effective realtime system for one-shot action modeling and recognition; the paper highlights the effectiveness of sparse coding techniques to represent 3D actions. We obtain very good results on three different data sets: a benchmark data set for one-shot action learning (the ChaLearn Gesture Data Set), an in-house data set acquired by a Kinect sensor including complex actions and gestures differing by small details, and a data set created for human-robot interaction purposes. Finally we demonstrate that our system is effective also in a human-robot interaction setting and propose a memory game, “All Gestures You Can”, to be played against a humanoid robot. Keywords: real-time action recognition, sparse representation, one-shot action learning, human robot interaction</p><p>2 0.83687598 <a title="56-lda-2" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>Author: Pierre Alquier, Gérard Biau</p><p>Abstract: Let (X,Y ) be a random pair taking values in R p × R. In the so-called single-index model, one has Y = f ⋆ (θ⋆T X) +W , where f ⋆ is an unknown univariate measurable function, θ⋆ is an unknown vector in Rd , and W denotes a random noise satisfying E[W |X] = 0. The single-index model is known to offer a ﬂexible way to model a variety of high-dimensional real-world phenomena. However, despite its relative simplicity, this dimension reduction scheme is faced with severe complications as soon as the underlying dimension becomes larger than the number of observations (“p larger than n” paradigm). To circumvent this difﬁculty, we consider the single-index model estimation problem from a sparsity perspective using a PAC-Bayesian approach. On the theoretical side, we offer a sharp oracle inequality, which is more powerful than the best known oracle inequalities for other common procedures of single-index recovery. The proposed method is implemented by means of the reversible jump Markov chain Monte Carlo technique and its performance is compared with that of standard procedures. Keywords: single-index model, sparsity, regression estimation, PAC-Bayesian, oracle inequality, reversible jump Markov chain Monte Carlo method</p><p>3 0.59242469 <a title="56-lda-3" href="./jmlr-2013-One-shot_Learning_Gesture_Recognition_from_RGB-D_Data_Using_Bag_of_Features.html">80 jmlr-2013-One-shot Learning Gesture Recognition from RGB-D Data Using Bag of Features</a></p>
<p>Author: Jun Wan, Qiuqi Ruan, Wei Li, Shuang Deng</p><p>Abstract: For one-shot learning gesture recognition, two important challenges are: how to extract distinctive features and how to learn a discriminative model from only one training sample per gesture class. For feature extraction, a new spatio-temporal feature representation called 3D enhanced motion scale-invariant feature transform (3D EMoSIFT) is proposed, which fuses RGB-D data. Compared with other features, the new feature set is invariant to scale and rotation, and has more compact and richer visual representations. For learning a discriminative model, all features extracted from training samples are clustered with the k-means algorithm to learn a visual codebook. Then, unlike the traditional bag of feature (BoF) models using vector quantization (VQ) to map each feature into a certain visual codeword, a sparse coding method named simulation orthogonal matching pursuit (SOMP) is applied and thus each feature can be represented by some linear combination of a small number of codewords. Compared with VQ, SOMP leads to a much lower reconstruction error and achieves better performance. The proposed approach has been evaluated on ChaLearn gesture database and the result has been ranked amongst the top best performing techniques on ChaLearn gesture challenge (round 2). Keywords: gesture recognition, bag of features (BoF) model, one-shot learning, 3D enhanced motion scale invariant feature transform (3D EMoSIFT), Simulation Orthogonal Matching Pursuit (SOMP)</p><p>4 0.43767667 <a title="56-lda-4" href="./jmlr-2013-Language-Motivated_Approaches_to_Action_Recognition.html">58 jmlr-2013-Language-Motivated Approaches to Action Recognition</a></p>
<p>Author: Manavender R. Malgireddy, Ifeoma Nwogu, Venu Govindaraju</p><p>Abstract: We present language-motivated approaches to detecting, localizing and classifying activities and gestures in videos. In order to obtain statistical insight into the underlying patterns of motions in activities, we develop a dynamic, hierarchical Bayesian model which connects low-level visual features in videos with poses, motion patterns and classes of activities. This process is somewhat analogous to the method of detecting topics or categories from documents based on the word content of the documents, except that our documents are dynamic. The proposed generative model harnesses both the temporal ordering power of dynamic Bayesian networks such as hidden Markov models (HMMs) and the automatic clustering power of hierarchical Bayesian models such as the latent Dirichlet allocation (LDA) model. We also introduce a probabilistic framework for detecting and localizing pre-speciﬁed activities (or gestures) in a video sequence, analogous to the use of ﬁller models for keyword detection in speech processing. We demonstrate the robustness of our classiﬁcation model and our spotting framework by recognizing activities in unconstrained real-life video sequences and by spotting gestures via a one-shot-learning approach. Keywords: dynamic hierarchical Bayesian networks, topic models, activity recognition, gesture spotting, generative models</p><p>5 0.43063483 <a title="56-lda-5" href="./jmlr-2013-MAGIC_Summoning%3A__Towards_Automatic_Suggesting_and_Testing_of_Gestures_With_Low_Probability_of_False_Positives_During_Use.html">66 jmlr-2013-MAGIC Summoning:  Towards Automatic Suggesting and Testing of Gestures With Low Probability of False Positives During Use</a></p>
<p>Author: Daniel Kyu Hwa Kohlsdorf, Thad E. Starner</p><p>Abstract: Gestures for interfaces should be short, pleasing, intuitive, and easily recognized by a computer. However, it is a challenge for interface designers to create gestures easily distinguishable from users’ normal movements. Our tool MAGIC Summoning addresses this problem. Given a speciﬁc platform and task, we gather a large database of unlabeled sensor data captured in the environments in which the system will be used (an “Everyday Gesture Library” or EGL). The EGL is quantized and indexed via multi-dimensional Symbolic Aggregate approXimation (SAX) to enable quick searching. MAGIC exploits the SAX representation of the EGL to suggest gestures with a low likelihood of false triggering. Suggested gestures are ordered according to brevity and simplicity, freeing the interface designer to focus on the user experience. Once a gesture is selected, MAGIC can output synthetic examples of the gesture to train a chosen classiﬁer (for example, with a hidden Markov model). If the interface designer suggests his own gesture and provides several examples, MAGIC estimates how accurately that gesture can be recognized and estimates its false positive rate by comparing it against the natural movements in the EGL. We demonstrate MAGIC’s effectiveness in gesture selection and helpfulness in creating accurate gesture recognizers. Keywords: gesture recognition, gesture spotting, false positives, continuous recognition</p><p>6 0.39340988 <a title="56-lda-6" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>7 0.38562778 <a title="56-lda-7" href="./jmlr-2013-Dynamic_Affine-Invariant_Shape-Appearance_Handshape_Features_and_Classification_in_Sign_Language_Videos.html">38 jmlr-2013-Dynamic Affine-Invariant Shape-Appearance Handshape Features and Classification in Sign Language Videos</a></p>
<p>8 0.37780204 <a title="56-lda-8" href="./jmlr-2013-JKernelMachines%3A_A_Simple_Framework_for_Kernel_Machines.html">54 jmlr-2013-JKernelMachines: A Simple Framework for Kernel Machines</a></p>
<p>9 0.3596822 <a title="56-lda-9" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>10 0.3554292 <a title="56-lda-10" href="./jmlr-2013-Optimal_Discovery_with_Probabilistic_Expert_Advice%3A_Finite_Time_Analysis_and_Macroscopic_Optimality.html">81 jmlr-2013-Optimal Discovery with Probabilistic Expert Advice: Finite Time Analysis and Macroscopic Optimality</a></p>
<p>11 0.34137979 <a title="56-lda-11" href="./jmlr-2013-Asymptotic_Results_on_Adaptive_False_Discovery_Rate_Controlling_Procedures_Based_on_Kernel_Estimators.html">14 jmlr-2013-Asymptotic Results on Adaptive False Discovery Rate Controlling Procedures Based on Kernel Estimators</a></p>
<p>12 0.33839869 <a title="56-lda-12" href="./jmlr-2013-A_Widely_Applicable_Bayesian_Information_Criterion.html">9 jmlr-2013-A Widely Applicable Bayesian Information Criterion</a></p>
<p>13 0.33829942 <a title="56-lda-13" href="./jmlr-2013-Greedy_Feature_Selection_for_Subspace_Clustering.html">50 jmlr-2013-Greedy Feature Selection for Subspace Clustering</a></p>
<p>14 0.33661416 <a title="56-lda-14" href="./jmlr-2013-GURLS%3A_A_Least_Squares_Library_for_Supervised_Learning.html">46 jmlr-2013-GURLS: A Least Squares Library for Supervised Learning</a></p>
<p>15 0.3295399 <a title="56-lda-15" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>16 0.3289862 <a title="56-lda-16" href="./jmlr-2013-Learning_Bilinear_Model_for_Matching_Queries_and_Documents.html">60 jmlr-2013-Learning Bilinear Model for Matching Queries and Documents</a></p>
<p>17 0.3276335 <a title="56-lda-17" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>18 0.32701388 <a title="56-lda-18" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>19 0.32603842 <a title="56-lda-19" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>20 0.32414481 <a title="56-lda-20" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
