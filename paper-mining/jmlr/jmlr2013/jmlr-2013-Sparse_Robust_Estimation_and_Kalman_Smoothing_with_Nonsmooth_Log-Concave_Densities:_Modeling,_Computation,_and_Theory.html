<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2013" href="../home/jmlr2013_home.html">jmlr2013</a> <a title="jmlr-2013-103" href="#">jmlr2013-103</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</h1>
<br/><p>Source: <a title="jmlr-2013-103-pdf" href="http://jmlr.org/papers/volume14/aravkin13a/aravkin13a.pdf">pdf</a></p><p>Author: Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto</p><p>Abstract: We introduce a new class of quadratic support (QS) functions, many of which already play a crucial role in a variety of applications, including machine learning, robust statistical inference, sparsity promotion, and inverse problems such as Kalman smoothing. Well known examples of QS penalties include the ℓ2 , Huber, ℓ1 and Vapnik losses. We build on a dual representation for QS functions, using it to characterize conditions necessary to interpret these functions as negative logs of true probability densities. This interpretation establishes the foundation for statistical modeling with both known and new QS loss functions, and enables construction of non-smooth multivariate distributions with speciﬁed means and variances from simple scalar building blocks. The main contribution of this paper is a ﬂexible statistical modeling framework for a variety of learning applications, together with a toolbox of efﬁcient numerical methods for estimation. In particular, a broad subclass of QS loss functions known as piecewise linear quadratic (PLQ) penalties has a dual representation that can be exploited to design interior point (IP) methods. IP methods solve nonsmooth optimization problems by working directly with smooth systems of equations characterizing their optimality. We provide several numerical examples, along with a code that can be used to solve general PLQ problems. The efﬁciency of the IP approach depends on the structure of particular applications. We consider the class of dynamic inverse problems using Kalman smoothing. This class comprises a wide variety of applications, where the aim is to reconstruct the state of a dynamical system with known process and measurement models starting from noisy output samples. In the classical case, Gaus∗. The authors would like to thank Bradley M. Bell for insightful discussions and helpful suggestions. The research leading to these results has received funding from the European Union Seventh Framework Programme [FP7/2007-2013</p><p>Reference: <a title="jmlr-2013-103-reference" href="../jmlr2013_reference/jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Well known examples of QS penalties include the ℓ2 , Huber, ℓ1 and Vapnik losses. [sent-11, score-0.127]
</p><p>2 In particular, a broad subclass of QS loss functions known as piecewise linear quadratic (PLQ) penalties has a dual representation that can be exploited to design interior point (IP) methods. [sent-15, score-0.216]
</p><p>3 This class comprises a wide variety of applications, where the aim is to reconstruct the state of a dynamical system with known process and measurement models starting from noisy output samples. [sent-20, score-0.068]
</p><p>4 A RAVKIN , B URKE AND P ILLONETTO  sian errors are assumed both in the process and measurement models for such problems. [sent-28, score-0.068]
</p><p>5 We show that the extended framework allows arbitrary PLQ densities to be used, and that the proposed IP approach solves the generalized Kalman smoothing problem while maintaining the linear complexity in the size of the time series, just as in the Gaussian case. [sent-29, score-0.132]
</p><p>6 This extends the computational efﬁciency of the Mayne-Fraser and Rauch-Tung-Striebel algorithms to a much broader nonsmooth setting, and includes many recently proposed robust and sparse smoothers as special cases. [sent-30, score-0.117]
</p><p>7 Keywords: statistical modeling, convex analysis, nonsmooth optimization, robust inference, sparsity optimization, Kalman smoothing, interior point methods  1. [sent-31, score-0.114]
</p><p>8 Let z denote a linear transformation of x contaminated with additive zero mean measurement noise v with covariance R, z = Hx + v ,  (2)  where H ∈ Rℓ×n is a known matrix, while v and w are independent. [sent-33, score-0.068]
</p><p>9 In addition, sparsity-promoting regularization is often used in order to extract a small subset from a large measurement or parameter vector which has greatest impact on the predictive capability of the estimate for future data. [sent-44, score-0.068]
</p><p>10 2690  (4)  N ONSMOOTH D ENSITIES : M ODELING , C OMPUTATION AND T HEORY  where the loss V may be the ℓ2 -norm, the Huber penalty (Huber, 1981), Vapnik’s ε-insensitive loss, used in support vector regression (Vapnik, 1998; Hastie et al. [sent-49, score-0.072]
</p><p>11 o These robust and sparse approaches can often be interpreted as placing non-Gaussian priors on w (or directly on x) and on the measurement noise v. [sent-56, score-0.092]
</p><p>12 Non-Gaussian model errors and priors leading to a great variety of loss and penalty functions are also reviewed by Palmer et al. [sent-59, score-0.072]
</p><p>13 This class of functions generalizes the notion of piecewise linear quadratic (PLQ) penalties (Rockafellar and Wets, 1998). [sent-62, score-0.162]
</p><p>14 The dual representation is the key to identifying which QS loss functions can be associated with a density, which in turn allows us to interpret the solution to the problem (4) as a MAP estimator when the loss functions V and W come from this subclass of QS penalties. [sent-63, score-0.06]
</p><p>15 This is essentially a smoothing approach to many (non-smooth) robust and sparse problems of interest to practitioners. [sent-68, score-0.101]
</p><p>16 In particular, we design new Kalman smoothers tailored for systems subject to noises coming from PLQ densities. [sent-79, score-0.06]
</p><p>17 We show that the IP iterations for all PLQ Kalman smoothing problems can be computed with a number of operations that scales linearly in N, as in the quadratic case. [sent-83, score-0.112]
</p><p>18 In Section 3 we show how to construct QS penalties and densities having a desired structure from basic components, and in particular how multivariate densities can be endowed with prescribed means and variances using scalar building blocks. [sent-91, score-0.237]
</p><p>19 In Section 6, we present the Kalman smoothing dynamic model, formulate Kalman smoothing with PLQ penalties, present the KKT system for the dynamic case, and show that IP iterations for PLQ smoothing preserve the classical computational efﬁciency known for the Gaussian case. [sent-95, score-0.231]
</p><p>20 Quadratic Support Functions And Densities In this section, we introduce the class of Quadratic Support (QS) functions, characterize some of their properties, and show that many commonly used penalties fall into this class. [sent-99, score-0.127]
</p><p>21 • (Cone) For any set C ⊂ Rn , denote by cone C the set {tr|r ∈ C,t ∈ R+ }. [sent-105, score-0.088]
</p><p>22 2692  N ONSMOOTH D ENSITIES : M ODELING , C OMPUTATION AND T HEORY  • (Polars of convex sets) For any convex set C ⊂ Rm , the polar of C is deﬁned to be C◦ := {r| r, d ≤ 1 ∀ d ∈ C}, and if C is a convex cone, this representation is equivalent to C◦ := {r| r, d ≤ 0 ∀ d ∈ C}. [sent-107, score-0.099]
</p><p>23 The horizon cone C∞ is the convex cone of ‘unbounded directions’ for C, that is, d ∈ C∞ if C + d ⊂ C. [sent-110, score-0.209]
</p><p>24 The barrier cone of a convex set C is denoted by bar(C): bar(C) := {x∗ |for some β ∈ R, x, x∗ ≤ β ∀x ∈ C} . [sent-112, score-0.148]
</p><p>25 Remark 2 When U is polyhedral, 0 ∈ U, b = 0 and B = I, we recover the basic piecewise linearquadratic penalties characterized in Rockafellar and Wets (1998, Example 11. [sent-120, score-0.127]
</p><p>26 Then B−1 [bar(U) + Ran (M) − b] ⊂ dom[ρ(U, M, B, b; ·)] ⊂ B−1 [K ◦ − b] , with equality throughout when bar(U) + Ran (M) is closed, where bar(U) = dom (δ∗ (· |U )) is the barrier cone of U. [sent-123, score-0.162]
</p><p>27 We now show that many commonly used penalties are special cases of QS (and indeed, of the PLQ) class. [sent-125, score-0.127]
</p><p>28 u∈R  1 The function inside the sup is maximized at u = y, hence ρ(y) = 2 y2 , see top left panel of Figure 1. [sent-130, score-0.066]
</p><p>29 u∈[−1,1]  The function inside the sup is maximized by taking u = sign(y), hence ρ(y) = |y|, see top right panel of Figure 1. [sent-134, score-0.066]
</p><p>30 The Vapnik penalty is shown in the middle right panel of Figure 1. [sent-152, score-0.08]
</p><p>31 Using example 7, we can create a symmetric soft insensitive loss function (which one might term the Hubnik) by adding together to soft hinge loss functions: ρ(y) = sup {(y − ε)u} − 1 u2 + sup {(−y − ε)u} − 1 u2 2 2 u∈[0,κ]  = sup u∈[0,κ]2  u∈[0,κ]  y−ε ,u −y − ε 2695  − 1 uT 2  1 0 u. [sent-162, score-0.249]
</p><p>32 Remark 5 Let ρ1 (y) and ρ2 (y) be two QS penalties speciﬁed by Ui , Mi , bi , Bi , for i = 1, 2. [sent-167, score-0.127]
</p><p>33 However, with some practice the design of QS penalties is not as daunting a task as it ﬁrst appears. [sent-170, score-0.127]
</p><p>34 The formulas (9)-(16) show how one can build PLQ penalties having a wide range of desirable properties. [sent-182, score-0.127]
</p><p>35 Remark 8 (General examples) In this remark we show how the representations in Lemma 7 can be used to build QS penalties with speciﬁc structure. [sent-184, score-0.16]
</p><p>36 Let · be a norm with closed unit ball B, let K be a non-empty closed convex cone in Rn , and let v ∈ Rn . [sent-207, score-0.121]
</p><p>37 By taking · = · 1 , K = Rn so (−K)◦ = K, and v = ε1, + where 1 is the vector of all ones, we recover the multivariate hinge loss function in Remark 4. [sent-211, score-0.066]
</p><p>38 For example, in the setting of (20) with Uv = {u | Av u ≤ av }  and Uw = {u | Aw u ≤ bw }  (24)  this condition reduces to null(Mv ) ∩ null(AT ) = {0} and v  null(Mw ) ∩ null(AT ) = {0}. [sent-240, score-0.259]
</p><p>39 w  (25)  Corollary 15 The densities corresponding to ℓ1 , ℓ2 , Huber, and Vapnik penalties all satisfy hypothesis (25). [sent-241, score-0.182]
</p><p>40 In this case  Uv = [−1m , 1m ], Mv = 0m×m , bv = 0m , Bv = Im×m , Uw = Rn , Mw = In×n , bw = 0n , Bw = In×n , and R ∈ Rm×m and Q ∈ Rn×n are symmetric positive deﬁnite covariance matrices. [sent-250, score-0.326]
</p><p>41 For problems lacking these features, such as general formulations built from (nonsmooth) PLQ penalties and possibly ill-conditioned linear operators, IP can dominate ADMM, reaching the true solution while ADMM struggles. [sent-283, score-0.127]
</p><p>42 The ADMM method now comprises the following iterative updates: η 1 xk+1 = argmin Ax − b 2 + x + yk − zk 2 , 2 2 2 2 x η zk+1 = argmin λ z 1 + z − xk+1 + yk 2 , 2 2 z yk+1 = yk + (zk+1 − xk+1 ) . [sent-298, score-0.438]
</p><p>43 Turning our attention to the x-update, note that the gradient is given by AT (Ax − b) + η(x + yk − zk ) = (AT A + I)x − AT b + η(yk − zk ) . [sent-299, score-0.222]
</p><p>44 3 Robust Lasso For the examples in this section, we take ρ(·) to be a robust convex loss, either the 1-norm or the Huber function, and consider the robust Lasso problem min ρ(Ax − b) + λ x x  1  . [sent-362, score-0.081]
</p><p>45 2 2 The ADMM updates for this formulation are η xk+1 = argmin λ x 1 + Ax − yk − zk 2 , 2 2 x η zk+1 = argmin ρ(z) + z + yk − Axk+1 + b 2 , 2 2 z yk+1 = yk + (zk+1 − Axk+1 + b) . [sent-366, score-0.438]
</p><p>46 2 2 The ADMM updates are: η xk+1 = argmin ρ(Ax − b) + x − zk + yk 2 , 2 2 x η zk+1 = argmin λ z 1 + z + (xk+1 + yk ) 2 , 2 2 z yk+1 = yk + (zk+1 − xk+1 ) . [sent-371, score-0.438]
</p><p>47 Kalman Smoothing With PLQ Penalties Consider now a dynamic scenario, where the system state xk evolves according to the following stochastic discrete-time linear model x1 = x0 + w1 , xk = Gk xk−1 + wk ,  k = 2, 3, . [sent-381, score-0.123]
</p><p>48 The problem (3) becomes the classical Kalman smoothing problem with quadratic penalties. [sent-401, score-0.112]
</p><p>49 Note also that once the linear system above is formed, it takes only O(n3 N) operations to solve due to special block tridiagonal structure (for a generic system, it would take O(n3 N 3 ) time). [sent-404, score-0.081]
</p><p>50 In this section, we will show that IP methods can preserve this complexity for much more general penalties on the measurement and process residuals. [sent-405, score-0.195]
</p><p>51 2710  N ONSMOOTH D ENSITIES : M ODELING , C OMPUTATION AND T HEORY  Remark 16 Suppose we decide to move to an outlier robust formulation, where the 1-norm or Huber penalties are used, but the measurement variance is known to be R. [sent-407, score-0.219]
</p><p>52 Analogously, the statistically correct objective when measurement error is the Huber penalty with parameter κ is 1 Gx − µ 2 −1 + c2 ρ(R−1/2 (Hx − z)) , Q 2 where √ 2 4 exp −κ2 /2 1+κ + 2π[2Φ(κ) − 1] κ3 √ c2 = . [sent-409, score-0.11]
</p><p>53 1, and ensures that the weighting between process and measurement terms is still consistent with the situation regardless of which shapes are used for the process and measurement penalties. [sent-411, score-0.136]
</p><p>54 Next, we show that when the penalties used on the process residual Gx − w and measurement residual Hx − z arise from general PLQ densities, the general Kalman smoothing problem takes the form (20), studied in the previous section. [sent-413, score-0.272]
</p><p>55 Then, for suitable Uw , Mw , bw , Bw and Uv , Mv , bv , Bv and corresponding ρw and ρv we have p(w) ∝ exp −ρ Uw , Mw , bw , Bw ; Q−1/2 w  ,  p(v) ∝ exp −ρ(Uv , Mv , bv , Bv ; R−1/2 v) while the MAP estimator of x in the model (29) is    ρ Uw , Mw , bw , Bw ; Q−1/2 (Gx − µ)  . [sent-417, score-0.843]
</p><p>56 The proof is presented in the Appendix and shows that IP methods for solving (30) preserve the key block tridiagonal structure of the standard smoother. [sent-426, score-0.081]
</p><p>57 If the number of IP iterations is ﬁxed (10 − 20 are typically used in practice), general smoothing estimates can thus be computed in O[N(n3 + m3 + l)] time. [sent-427, score-0.077]
</p><p>58 Numerical Example In this section, we illustrate the modeling capabilities and computational efﬁciency of PLQ Kalman smoothers on simulated and real data. [sent-430, score-0.06]
</p><p>59 The measurement noise vk was generated using a mixture of two Gaussian densities, with p = 0. [sent-436, score-0.091]
</p><p>60 This model captures the Bayesian interpretation of cubic smoothing splines (Wahba, 1990), and admits a two-dimensional state space representation where the ﬁrst component of x(t), which models f (·) − 1, corresponds to the integral of the second state component, modelled as Brownian motion. [sent-443, score-0.077]
</p><p>61 The ﬁrst (classical) estimator uses a quadratic loss function to describe the negative log of the measurement noise density and contains only λ2 as unknown parameter. [sent-455, score-0.133]
</p><p>62 Efﬁciency is 2713  A RAVKIN , B URKE AND P ILLONETTO  Figure 3: Estimation of the smoothing ﬁlter parameters using the Vapnik loss. [sent-460, score-0.077]
</p><p>63 In contrast to the Vapnik penalty, the quadratic loss does not induce any sparsity, so that, in this case, the number of support vectors equals the size of the training set. [sent-470, score-0.065]
</p><p>64 It is clear that the estimate obtained using the quadratic penalty is heavily affected by the outliers. [sent-472, score-0.077]
</p><p>65 Our aim is to compare the prediction performance of two smoothers that rely on ℓ2 and ℓ1 measure2714  N ONSMOOTH D ENSITIES : M ODELING , C OMPUTATION AND T HEORY  ment loss functions. [sent-486, score-0.09]
</p><p>66 1 and 1000 (30 point grid), the smoothers are trained on the training set, and the γ chosen corresponds to the smoother that achieves the best prediction on the validation set. [sent-492, score-0.104]
</p><p>67 Then, the prediction capability of the smoothers is evaluated by computing the 236 relative percentage errors (ratio of residual and observation times 100) in the reconstruction of the test set. [sent-497, score-0.06]
</p><p>68 The QS class captures a variety of existing penalties, including all PLQ penalties, and we have shown how to construct natural generalizations based on general norm and cone geometries, and explored the structure of these functions using Euclidean projections. [sent-507, score-0.088]
</p><p>69 Many penalties in the QS class can be interpreted as negative logs of true probability densities. [sent-508, score-0.15]
</p><p>70 Coercivity (characterized in Theorem 10) is the key necessary condition for this interpretation, as well as a fundamental prerequisite in sparse and robust estimation, since it precludes directions for which the sum of the loss and the regularizer are insensitive to large parameter changes. [sent-509, score-0.079]
</p><p>71 The key condition for successful execution of IP iterations is for PLQ penalties to be ﬁnite valued, which implies non-degeneracy of the corresponding statistical distribution (the support cannot be contained in a lower-dimensional subspace). [sent-516, score-0.127]
</p><p>72 We then applied both the statistical framework and the computational approach to the class of state estimation problems in discrete-time dynamic systems, extending the classical formulations to allow dynamics and measurement noise to come from any PLQ densities. [sent-518, score-0.068]
</p><p>73 We showed that clas2715  A RAVKIN , B URKE AND P ILLONETTO  Figure 4: Left panels: data set for variable 14 (top) and relative percentage errors in the reconstruction of the test set obtained by Kalman smoothers based on the ℓ2 and the ℓ1 loss (bottom). [sent-519, score-0.09]
</p><p>74 Right panels: data set for variable 36 (top) and relative percentage errors in the reconstruction of the test set obtained by Kalman smoothers based on the ℓ2 and the ℓ1 loss (bottom). [sent-520, score-0.09]
</p><p>75 sical computational efﬁciency results are preserved when the general IP approach is used for state estimation; speciﬁcally, the computational cost of PLQ Kalman smoothing scales linearly with the length of the time series, as in the quadratic case. [sent-521, score-0.112]
</p><p>76 (2011a), which considers non-convex Kalman smoothing problems with nonlinear process and measurement models and solves by using the standard methodology of convex composite optimization (Burke, 1985). [sent-525, score-0.178]
</p><p>77 At each outer iteration the process and measurement models are linearized around the current iterate, and the descent direction is found by solving a particular subproblem of type (4) using IP methods. [sent-526, score-0.068]
</p><p>78 This strategy could help in designing a good proposal density for posterior simulation using, for example, particle smoothing ﬁlters (Ristic et al. [sent-530, score-0.077]
</p><p>79 Then dom (ρ(U, M, B, b; ·)) = B−1 (dom (ρ) − b), hence the theorem follows if it can be shown that bar(U) + Ran (M) ⊂ dom (ρ) ⊂ [U ∞ ∩ null(M)]◦ with equality when bar(U) + Ran (M) is closed. [sent-537, score-0.094]
</p><p>80 In the polyhedral case, bar(U) is a polyhedral convex set, and the sum of such sets is also a polyhedral convex set (Rockafellar, 1970, Corollary 19. [sent-549, score-0.189]
</p><p>81 2 Proof of Theorem 7 1 To see the ﬁrst equation in (8) write ρ(y) = supu y, u − 2 LT u 2 + δ (u |U ) , and then apply 2 the calculus of convex conjugate functions (Rockafellar, 1970, Section 16) to ﬁnd that 1 2  LT ·  ∗ 2 2 + δ (· |U ) (y)  = inf  s∈Rk  1 2  s  2 ∗ 2 + δ (y − Ls |U )  . [sent-553, score-0.089]
</p><p>82 3 Proof of Theorem 9 / First we will show that if ρ is convex coercive, then for any x ∈ argmin f = 0, there exist constants ¯ R and K > 0 such that ρ(x) ≥ ρ(x) + K x − x ¯ ¯ ∀ x ∈ RB . [sent-569, score-0.064]
</p><p>83 Then we can ﬁnd a sequence {yk } with yk > k and a constant P so that ρ(yk ) ≤ P for all k > 0. [sent-585, score-0.106]
</p><p>84 Without loss of generality, we may assume that yk ¯ yk → y. [sent-586, score-0.242]
</p><p>85 Then by deﬁnition of ρ, we have for all u ∈ U 1 b + Byk , u − 2 uT Mu ≤ P, b + Byk , u ≤ P + 1 uT Mu, 2 b+Byk P , u ≤ yk + 2 1k uT Mu. [sent-587, score-0.106]
</p><p>86 Hence, we can further reduce this matrix to the block upper triangular form  I 0 AT 0  0 D(s) −D(q)CT 0 . [sent-594, score-0.07]
</p><p>87 Hence this ﬁnal block upper triangular is invertible proving Part (i). [sent-596, score-0.092]
</p><p>88 6 Details for Remark 17 The Lagrangian for (30) for feasible (x, uw , uv ) is L(x, uw , uv ) =  ˜ bw uw ˜ , uv bv  −  1 uw 2 uv  T  Mw 0 0 Mv  −Bw Q−1/2 G uw , x uv Bv R−1/2 H  uw − uv  ˜ ˜ where bw = bw − Bw Q−1/2 x0 and bv = bv − Bv R−1/2 z. [sent-652, score-3.018]
</p><p>89 For a polyhedral set U ⊂ Rm and any point u ∈ U, the normal cone NU (u) is ¯ ¯ ¯ ¯ polyhedral. [sent-655, score-0.129]
</p><p>90 ¯  Using (35), Then we may rewrite the optimality conditions (34) more explicitly as GT Q−T/2 BT uw − H T R−T/2 BT uv = 0, w¯ v ¯ −1/2 ¯ ˜ bw − Mw uw + Bw Q ¯ Gd = Aw qw , ˜ bv − Mv uv − Bv R−1/2 H d¯ = Av qv , ¯ {qv ≥ 0|qv(i) = 0 for i ∈ I(uv )}, ¯  {qw ≥ 0|qw(i) = 0 for i ∈ I(uw )} . [sent-657, score-1.357]
</p><p>91 ¯ 2721  (35)  A RAVKIN , B URKE AND P ILLONETTO  where qv(i) and qw(i) denote the ith elements of qv and qw . [sent-658, score-0.351]
</p><p>92 Deﬁne slack variables sw ≥ 0 and sv ≥ 0 as follows: sw = aw − AT uw , w sv = av − AT uv . [sent-659, score-1.084]
</p><p>93 7 Proof of Theorem 18 IP methods apply a damped Newton iteration to ﬁnd the solution of the relaxed KKT system Fγ = 0, where    AT uw + sw − aw sw w   sv   AT uv + sv − av v      qw   D(qw )D(sw )1 − γ1     . [sent-665, score-1.084]
</p><p>94 In the above expressions, Tw := Mw + Aw D(sw )−1 D(qw )AT , w Tv := Mv + Av D(sv )−1 D(qv )AT , v  (38)  where D(sw )−1 D(qw ) and D(sv )−1 D(qv ) are always full-rank diagonal matrices, since the vectors sw , qw , sv , qv . [sent-670, score-0.639]
</p><p>95 Matrices Tw and Tv are invertible as long as the PLQ densities for w and v satisfy (25). [sent-671, score-0.077]
</p><p>96 Moreover, Tρ is block diagonal, with ith diagonal block given by Mi + Ai Di AT . [sent-677, score-0.094]
</p><p>97 Q−T/2 BT Tw Bw Q−1/2 is block diagonal, and G is block bidiagonal, hence ΩG is block tridiw agonal. [sent-684, score-0.141]
</p><p>98 The matrices Tv and Tw are block diagonal, with sizes Nn and Nm, assuming m measurements at each time point. [sent-686, score-0.07]
</p><p>99 Since Ω is block tridiagonal, symmetric, and positive deﬁnite, Ω∆x = ρ can be solved in O(Nn3 ) time using the block tridiagonal algorithm in Bell (2000). [sent-688, score-0.128]
</p><p>100 Doubly robust smoothing of dynamical processes via outlier sparsity constraints. [sent-831, score-0.101]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('qs', 0.327), ('plq', 0.309), ('admm', 0.238), ('ip', 0.208), ('uw', 0.194), ('bw', 0.191), ('qw', 0.185), ('sw', 0.177), ('kalman', 0.171), ('qv', 0.166), ('illonetto', 0.159), ('ravkin', 0.159), ('urke', 0.159), ('uv', 0.146), ('ensities', 0.141), ('onsmooth', 0.141), ('mu', 0.136), ('bv', 0.135), ('huber', 0.135), ('penalties', 0.127), ('odeling', 0.121), ('sv', 0.111), ('aravkin', 0.106), ('yk', 0.106), ('mw', 0.1), ('heory', 0.1), ('omputation', 0.1), ('aw', 0.1), ('cone', 0.088), ('bar', 0.088), ('null', 0.084), ('mv', 0.082), ('smoothing', 0.077), ('vapnik', 0.071), ('gx', 0.07), ('av', 0.068), ('measurement', 0.068), ('bt', 0.066), ('rockafellar', 0.065), ('hx', 0.065), ('pm', 0.062), ('smoothers', 0.06), ('tw', 0.059), ('zk', 0.058), ('densities', 0.055), ('ax', 0.053), ('cond', 0.053), ('burke', 0.053), ('tv', 0.05), ('xk', 0.05), ('dom', 0.047), ('block', 0.047), ('smoother', 0.044), ('penalty', 0.042), ('polyhedral', 0.041), ('rn', 0.041), ('gt', 0.04), ('lasso', 0.04), ('panel', 0.038), ('im', 0.038), ('hinge', 0.036), ('quadratic', 0.035), ('tridiagonal', 0.034), ('nonsmooth', 0.033), ('remark', 0.033), ('convex', 0.033), ('kkt', 0.033), ('argmin', 0.031), ('vec', 0.03), ('supu', 0.03), ('elastic', 0.03), ('loss', 0.03), ('rm', 0.029), ('sup', 0.028), ('barrier', 0.027), ('aq', 0.027), ('byk', 0.026), ('dinuzzo', 0.026), ('ferris', 0.026), ('lucidi', 0.026), ('pillonetto', 0.026), ('uy', 0.026), ('bell', 0.026), ('inf', 0.026), ('insensitive', 0.025), ('interior', 0.024), ('gi', 0.024), ('robust', 0.024), ('panels', 0.023), ('triangular', 0.023), ('measurements', 0.023), ('vk', 0.023), ('wk', 0.023), ('ls', 0.023), ('outliers', 0.023), ('ohlsson', 0.023), ('farahmand', 0.023), ('logs', 0.023), ('soft', 0.022), ('invertible', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999994 <a title="103-tfidf-1" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>Author: Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto</p><p>Abstract: We introduce a new class of quadratic support (QS) functions, many of which already play a crucial role in a variety of applications, including machine learning, robust statistical inference, sparsity promotion, and inverse problems such as Kalman smoothing. Well known examples of QS penalties include the ℓ2 , Huber, ℓ1 and Vapnik losses. We build on a dual representation for QS functions, using it to characterize conditions necessary to interpret these functions as negative logs of true probability densities. This interpretation establishes the foundation for statistical modeling with both known and new QS loss functions, and enables construction of non-smooth multivariate distributions with speciﬁed means and variances from simple scalar building blocks. The main contribution of this paper is a ﬂexible statistical modeling framework for a variety of learning applications, together with a toolbox of efﬁcient numerical methods for estimation. In particular, a broad subclass of QS loss functions known as piecewise linear quadratic (PLQ) penalties has a dual representation that can be exploited to design interior point (IP) methods. IP methods solve nonsmooth optimization problems by working directly with smooth systems of equations characterizing their optimality. We provide several numerical examples, along with a code that can be used to solve general PLQ problems. The efﬁciency of the IP approach depends on the structure of particular applications. We consider the class of dynamic inverse problems using Kalman smoothing. This class comprises a wide variety of applications, where the aim is to reconstruct the state of a dynamical system with known process and measurement models starting from noisy output samples. In the classical case, Gaus∗. The authors would like to thank Bradley M. Bell for insightful discussions and helpful suggestions. The research leading to these results has received funding from the European Union Seventh Framework Programme [FP7/2007-2013</p><p>2 0.11019552 <a title="103-tfidf-2" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>Author: Xin Tong</p><p>Abstract: The Neyman-Pearson (NP) paradigm in binary classiﬁcation treats type I and type II errors with different priorities. It seeks classiﬁers that minimize type II error, subject to a type I error constraint under a user speciﬁed level α. In this paper, plug-in classiﬁers are developed under the NP paradigm. Based on the fundamental Neyman-Pearson Lemma, we propose two related plug-in classiﬁers which amount to thresholding respectively the class conditional density ratio and the regression function. These two classiﬁers handle different sampling schemes. This work focuses on theoretical properties of the proposed classiﬁers; in particular, we derive oracle inequalities that can be viewed as ﬁnite sample versions of risk bounds. NP classiﬁcation can be used to address anomaly detection problems, where asymmetry in errors is an intrinsic property. As opposed to a common practice in anomaly detection that consists of thresholding normal class density, our approach does not assume a speciﬁc form for anomaly distributions. Such consideration is particularly necessary when the anomaly class density is far from uniformly distributed. Keywords: plug-in approach, Neyman-Pearson paradigm, nonparametric statistics, oracle inequality, anomaly detection</p><p>3 0.10104726 <a title="103-tfidf-3" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>Author: Nima Noorshams, Martin J. Wainwright</p><p>Abstract: The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP ﬁxed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefﬁcients via Monte Carlo techniques and damped updates. We prove that the SOSMP iterates converge to a δ-neighborhood of the unique BP ﬁxed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefﬁcients as a function of the desired approximation accuracy δ and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical ﬂow estimation. Keywords: graphical models, sum-product for continuous state spaces, low-complexity belief propagation, stochastic approximation, Monte Carlo methods, orthogonal basis expansion</p><p>4 0.0795094 <a title="103-tfidf-4" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>Author: Julien Mairal, Bin Yu</p><p>Abstract: We consider supervised learning problems where the features are embedded in a graph, such as gene expressions in a gene network. In this context, it is of much interest to automatically select a subgraph with few connected components; by exploiting prior knowledge, one can indeed improve the prediction performance or obtain results that are easier to interpret. Regularization or penalty functions for selecting features in graphs have recently been proposed, but they raise new algorithmic challenges. For example, they typically require solving a combinatorially hard selection problem among all connected subgraphs. In this paper, we propose computationally feasible strategies to select a sparse and well-connected subset of features sitting on a directed acyclic graph (DAG). We introduce structured sparsity penalties over paths on a DAG called “path coding” penalties. Unlike existing regularization functions that model long-range interactions between features in a graph, path coding penalties are tractable. The penalties and their proximal operators involve path selection problems, which we efﬁciently solve by leveraging network ﬂow optimization. We experimentally show on synthetic, image, and genomic data that our approach is scalable and leads to more connected subgraphs than other regularization functions for graphs. Keywords: convex and non-convex optimization, network ﬂow optimization, graph sparsity</p><p>5 0.065674432 <a title="103-tfidf-5" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>Author: Naftali Harris, Mathias Drton</p><p>Abstract: The PC algorithm uses conditional independence tests for model selection in graphical modeling with acyclic directed graphs. In Gaussian models, tests of conditional independence are typically based on Pearson correlations, and high-dimensional consistency results have been obtained for the PC algorithm in this setting. Analyzing the error propagation from marginal to partial correlations, we prove that high-dimensional consistency carries over to a broader class of Gaussian copula or nonparanormal models when using rank-based measures of correlation. For graph sequences with bounded degree, our consistency result is as strong as prior Gaussian results. In simulations, the ‘Rank PC’ algorithm works as well as the ‘Pearson PC’ algorithm for normal data and considerably better for non-normal data, all the while incurring a negligible increase of computation time. While our interest is in the PC algorithm, the presented analysis of error propagation could be applied to other algorithms that test the vanishing of low-order partial correlations. Keywords: Gaussian copula, graphical model, model selection, multivariate normal distribution, nonparanormal distribution</p><p>6 0.059612632 <a title="103-tfidf-6" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>7 0.048027951 <a title="103-tfidf-7" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>8 0.043675072 <a title="103-tfidf-8" href="./jmlr-2013-Kernel_Bayes%27_Rule%3A_Bayesian_Inference_with_Positive_Definite_Kernels.html">57 jmlr-2013-Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels</a></p>
<p>9 0.042205378 <a title="103-tfidf-9" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>10 0.041962754 <a title="103-tfidf-10" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>11 0.038796913 <a title="103-tfidf-11" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>12 0.038254675 <a title="103-tfidf-12" href="./jmlr-2013-Gaussian_Kullback-Leibler_Approximate_Inference.html">47 jmlr-2013-Gaussian Kullback-Leibler Approximate Inference</a></p>
<p>13 0.038211811 <a title="103-tfidf-13" href="./jmlr-2013-Manifold_Regularization_and_Semi-supervised_Learning%3A_Some_Theoretical_Analyses.html">69 jmlr-2013-Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses</a></p>
<p>14 0.037872899 <a title="103-tfidf-14" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>15 0.037338413 <a title="103-tfidf-15" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>16 0.036928967 <a title="103-tfidf-16" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>17 0.03681381 <a title="103-tfidf-17" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>18 0.034482725 <a title="103-tfidf-18" href="./jmlr-2013-A_Theory_of_Multiclass_Boosting.html">8 jmlr-2013-A Theory of Multiclass Boosting</a></p>
<p>19 0.034449309 <a title="103-tfidf-19" href="./jmlr-2013-The_Rate_of_Convergence_of_AdaBoost.html">114 jmlr-2013-The Rate of Convergence of AdaBoost</a></p>
<p>20 0.033837166 <a title="103-tfidf-20" href="./jmlr-2013-Quasi-Newton_Method%3A_A_New_Direction.html">90 jmlr-2013-Quasi-Newton Method: A New Direction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2013_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.19), (1, 0.029), (2, 0.034), (3, 0.06), (4, 0.085), (5, 0.028), (6, 0.061), (7, -0.002), (8, 0.081), (9, 0.016), (10, -0.017), (11, 0.05), (12, 0.028), (13, -0.002), (14, -0.013), (15, -0.192), (16, 0.062), (17, -0.054), (18, -0.0), (19, -0.036), (20, -0.312), (21, -0.126), (22, -0.015), (23, 0.016), (24, -0.204), (25, -0.124), (26, 0.116), (27, 0.232), (28, -0.036), (29, -0.146), (30, 0.232), (31, -0.081), (32, 0.124), (33, 0.123), (34, -0.012), (35, -0.09), (36, 0.091), (37, -0.028), (38, -0.106), (39, -0.057), (40, 0.009), (41, -0.03), (42, 0.073), (43, 0.021), (44, -0.048), (45, -0.065), (46, -0.091), (47, -0.009), (48, -0.075), (49, -0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92488205 <a title="103-lsi-1" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>Author: Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto</p><p>Abstract: We introduce a new class of quadratic support (QS) functions, many of which already play a crucial role in a variety of applications, including machine learning, robust statistical inference, sparsity promotion, and inverse problems such as Kalman smoothing. Well known examples of QS penalties include the ℓ2 , Huber, ℓ1 and Vapnik losses. We build on a dual representation for QS functions, using it to characterize conditions necessary to interpret these functions as negative logs of true probability densities. This interpretation establishes the foundation for statistical modeling with both known and new QS loss functions, and enables construction of non-smooth multivariate distributions with speciﬁed means and variances from simple scalar building blocks. The main contribution of this paper is a ﬂexible statistical modeling framework for a variety of learning applications, together with a toolbox of efﬁcient numerical methods for estimation. In particular, a broad subclass of QS loss functions known as piecewise linear quadratic (PLQ) penalties has a dual representation that can be exploited to design interior point (IP) methods. IP methods solve nonsmooth optimization problems by working directly with smooth systems of equations characterizing their optimality. We provide several numerical examples, along with a code that can be used to solve general PLQ problems. The efﬁciency of the IP approach depends on the structure of particular applications. We consider the class of dynamic inverse problems using Kalman smoothing. This class comprises a wide variety of applications, where the aim is to reconstruct the state of a dynamical system with known process and measurement models starting from noisy output samples. In the classical case, Gaus∗. The authors would like to thank Bradley M. Bell for insightful discussions and helpful suggestions. The research leading to these results has received funding from the European Union Seventh Framework Programme [FP7/2007-2013</p><p>2 0.64205748 <a title="103-lsi-2" href="./jmlr-2013-A_Plug-in_Approach_to_Neyman-Pearson_Classification.html">6 jmlr-2013-A Plug-in Approach to Neyman-Pearson Classification</a></p>
<p>Author: Xin Tong</p><p>Abstract: The Neyman-Pearson (NP) paradigm in binary classiﬁcation treats type I and type II errors with different priorities. It seeks classiﬁers that minimize type II error, subject to a type I error constraint under a user speciﬁed level α. In this paper, plug-in classiﬁers are developed under the NP paradigm. Based on the fundamental Neyman-Pearson Lemma, we propose two related plug-in classiﬁers which amount to thresholding respectively the class conditional density ratio and the regression function. These two classiﬁers handle different sampling schemes. This work focuses on theoretical properties of the proposed classiﬁers; in particular, we derive oracle inequalities that can be viewed as ﬁnite sample versions of risk bounds. NP classiﬁcation can be used to address anomaly detection problems, where asymmetry in errors is an intrinsic property. As opposed to a common practice in anomaly detection that consists of thresholding normal class density, our approach does not assume a speciﬁc form for anomaly distributions. Such consideration is particularly necessary when the anomaly class density is far from uniformly distributed. Keywords: plug-in approach, Neyman-Pearson paradigm, nonparametric statistics, oracle inequality, anomaly detection</p><p>3 0.41859522 <a title="103-lsi-3" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>Author: Nima Noorshams, Martin J. Wainwright</p><p>Abstract: The sum-product or belief propagation (BP) algorithm is a widely used message-passing technique for computing approximate marginals in graphical models. We introduce a new technique, called stochastic orthogonal series message-passing (SOSMP), for computing the BP ﬁxed point in models with continuous random variables. It is based on a deterministic approximation of the messages via orthogonal series basis expansion, and a stochastic estimation of the basis coefﬁcients via Monte Carlo techniques and damped updates. We prove that the SOSMP iterates converge to a δ-neighborhood of the unique BP ﬁxed point for any tree-structured graph, and for any graphs with cycles in which the BP updates satisfy a contractivity condition. In addition, we demonstrate how to choose the number of basis coefﬁcients as a function of the desired approximation accuracy δ and smoothness of the compatibility functions. We illustrate our theory with both simulated examples and in application to optical ﬂow estimation. Keywords: graphical models, sum-product for continuous state spaces, low-complexity belief propagation, stochastic approximation, Monte Carlo methods, orthogonal basis expansion</p><p>4 0.31359023 <a title="103-lsi-4" href="./jmlr-2013-Kernel_Bayes%27_Rule%3A_Bayesian_Inference_with_Positive_Definite_Kernels.html">57 jmlr-2013-Kernel Bayes' Rule: Bayesian Inference with Positive Definite Kernels</a></p>
<p>Author: Kenji Fukumizu, Le Song, Arthur Gretton</p><p>Abstract: A kernel method for realizing Bayes’ rule is proposed, based on representations of probabilities in reproducing kernel Hilbert spaces. Probabilities are uniquely characterized by the mean of the canonical map to the RKHS. The prior and conditional probabilities are expressed in terms of RKHS functions of an empirical sample: no explicit parametric model is needed for these quantities. The posterior is likewise an RKHS mean of a weighted sample. The estimator for the expectation of a function of the posterior is derived, and rates of consistency are shown. Some representative applications of the kernel Bayes’ rule are presented, including Bayesian computation without likelihood and ﬁltering with a nonparametric state-space model. Keywords: kernel method, Bayes’ rule, reproducing kernel Hilbert space</p><p>5 0.29717001 <a title="103-lsi-5" href="./jmlr-2013-Stationary-Sparse_Causality_Network_Learning.html">106 jmlr-2013-Stationary-Sparse Causality Network Learning</a></p>
<p>Author: Yuejia He, Yiyuan She, Dapeng Wu</p><p>Abstract: Recently, researchers have proposed penalized maximum likelihood to identify network topology underlying a dynamical system modeled by multivariate time series. The time series of interest are assumed to be stationary, but this restriction is never taken into consideration by existing estimation methods. Moreover, practical problems of interest may have ultra-high dimensionality and obvious node collinearity. In addition, none of the available algorithms provides a probabilistic measure of the uncertainty for the obtained network topology which is informative in reliable network identiﬁcation. The main purpose of this paper is to tackle these challenging issues. We propose the S2 learning framework, which stands for stationary-sparse network learning. We propose a novel algorithm referred to as the Berhu iterative sparsity pursuit with stationarity (BISPS), where the Berhu regularization can improve the Lasso in detection and estimation. The algorithm is extremely easy to implement, efﬁcient in computation and has a theoretical guarantee to converge to a global optimum. We also incorporate a screening technique into BISPS to tackle ultra-high dimensional problems and enhance computational efﬁciency. Furthermore, a stationary bootstrap technique is applied to provide connection occurring frequency for reliable topology learning. Experiments show that our method can achieve stationary and sparse causality network learning and is scalable for high-dimensional problems. Keywords: stationarity, sparsity, Berhu, screening, bootstrap</p><p>6 0.29431173 <a title="103-lsi-6" href="./jmlr-2013-Supervised_Feature_Selection_in_Graphs_with_Path_Coding_Penalties_and_Network_Flows.html">111 jmlr-2013-Supervised Feature Selection in Graphs with Path Coding Penalties and Network Flows</a></p>
<p>7 0.24625102 <a title="103-lsi-7" href="./jmlr-2013-Multivariate_Convex_Regression_with_Adaptive_Partitioning.html">74 jmlr-2013-Multivariate Convex Regression with Adaptive Partitioning</a></p>
<p>8 0.2448675 <a title="103-lsi-8" href="./jmlr-2013-Message-Passing_Algorithms_for_Quadratic_Minimization.html">71 jmlr-2013-Message-Passing Algorithms for Quadratic Minimization</a></p>
<p>9 0.24042742 <a title="103-lsi-9" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>10 0.23525801 <a title="103-lsi-10" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>11 0.23198439 <a title="103-lsi-11" href="./jmlr-2013-PC_Algorithm_for_Nonparanormal_Graphical_Models.html">84 jmlr-2013-PC Algorithm for Nonparanormal Graphical Models</a></p>
<p>12 0.23092423 <a title="103-lsi-12" href="./jmlr-2013-Finding_Optimal_Bayesian_Networks_Using_Precedence_Constraints.html">44 jmlr-2013-Finding Optimal Bayesian Networks Using Precedence Constraints</a></p>
<p>13 0.21791336 <a title="103-lsi-13" href="./jmlr-2013-Sparse_Single-Index_Model.html">104 jmlr-2013-Sparse Single-Index Model</a></p>
<p>14 0.21252002 <a title="103-lsi-14" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>15 0.21053535 <a title="103-lsi-15" href="./jmlr-2013-Manifold_Regularization_and_Semi-supervised_Learning%3A_Some_Theoretical_Analyses.html">69 jmlr-2013-Manifold Regularization and Semi-supervised Learning: Some Theoretical Analyses</a></p>
<p>16 0.20438966 <a title="103-lsi-16" href="./jmlr-2013-Greedy_Sparsity-Constrained_Optimization.html">51 jmlr-2013-Greedy Sparsity-Constrained Optimization</a></p>
<p>17 0.1910681 <a title="103-lsi-17" href="./jmlr-2013-Machine_Learning_with_Operational_Costs.html">68 jmlr-2013-Machine Learning with Operational Costs</a></p>
<p>18 0.18806383 <a title="103-lsi-18" href="./jmlr-2013-Lovasz_theta_function%2C_SVMs_and_Finding_Dense_Subgraphs.html">64 jmlr-2013-Lovasz theta function, SVMs and Finding Dense Subgraphs</a></p>
<p>19 0.17695341 <a title="103-lsi-19" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>20 0.17490144 <a title="103-lsi-20" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2013_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.027), (5, 0.123), (6, 0.04), (10, 0.063), (14, 0.011), (20, 0.016), (23, 0.022), (41, 0.428), (53, 0.013), (68, 0.032), (70, 0.018), (75, 0.035), (85, 0.044), (87, 0.025), (89, 0.012), (93, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.9484846 <a title="103-lda-1" href="./jmlr-2013-Orange%3A_Data_Mining_Toolbox_in_Python.html">83 jmlr-2013-Orange: Data Mining Toolbox in Python</a></p>
<p>Author: Janez Demšar, Tomaž Curk, Aleš Erjavec, Črt Gorup, Tomaž Hočevar, Mitar Milutinovič, Martin Možina, Matija Polajnar, Marko Toplak, Anže Starič, Miha Štajdohar, Lan Umek, Lan Žagar, Jure Žbontar, Marinka Žitnik, Blaž Zupan</p><p>Abstract: Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the ﬂexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationallyintensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining. Keywords: Python, data mining, machine learning, toolbox, scripting</p><p>2 0.90924191 <a title="103-lda-2" href="./jmlr-2013-Query_Induction_with_Schema-Guided_Pruning_Strategies.html">91 jmlr-2013-Query Induction with Schema-Guided Pruning Strategies</a></p>
<p>Author: Joachim Niehren, Jérôme Champavère, Aurélien Lemay, Rémi Gilleron</p><p>Abstract: Inference algorithms for tree automata that deﬁne node selecting queries in unranked trees rely on tree pruning strategies. These impose additional assumptions on node selection that are needed to compensate for small numbers of annotated examples. Pruning-based heuristics in query learning algorithms for Web information extraction often boost the learning quality and speed up the learning process. We will distinguish the class of regular queries that are stable under a given schemaguided pruning strategy, and show that this class is learnable with polynomial time and data. Our learning algorithm is obtained by adding pruning heuristics to the traditional learning algorithm for tree automata from positive and negative examples. While justiﬁed by a formal learning model, our learning algorithm for stable queries also performs very well in practice of XML information extraction. Keywords: XML information extraction, XML schemas, interactive learning, tree automata, grammatical inference</p><p>same-paper 3 0.74508822 <a title="103-lda-3" href="./jmlr-2013-Sparse_Robust_Estimation_and_Kalman_Smoothing_with_Nonsmooth_Log-Concave_Densities%3A_Modeling%2C_Computation%2C_and_Theory.html">103 jmlr-2013-Sparse Robust Estimation and Kalman Smoothing with Nonsmooth Log-Concave Densities: Modeling, Computation, and Theory</a></p>
<p>Author: Aleksandr Y. Aravkin, James V. Burke, Gianluigi Pillonetto</p><p>Abstract: We introduce a new class of quadratic support (QS) functions, many of which already play a crucial role in a variety of applications, including machine learning, robust statistical inference, sparsity promotion, and inverse problems such as Kalman smoothing. Well known examples of QS penalties include the ℓ2 , Huber, ℓ1 and Vapnik losses. We build on a dual representation for QS functions, using it to characterize conditions necessary to interpret these functions as negative logs of true probability densities. This interpretation establishes the foundation for statistical modeling with both known and new QS loss functions, and enables construction of non-smooth multivariate distributions with speciﬁed means and variances from simple scalar building blocks. The main contribution of this paper is a ﬂexible statistical modeling framework for a variety of learning applications, together with a toolbox of efﬁcient numerical methods for estimation. In particular, a broad subclass of QS loss functions known as piecewise linear quadratic (PLQ) penalties has a dual representation that can be exploited to design interior point (IP) methods. IP methods solve nonsmooth optimization problems by working directly with smooth systems of equations characterizing their optimality. We provide several numerical examples, along with a code that can be used to solve general PLQ problems. The efﬁciency of the IP approach depends on the structure of particular applications. We consider the class of dynamic inverse problems using Kalman smoothing. This class comprises a wide variety of applications, where the aim is to reconstruct the state of a dynamical system with known process and measurement models starting from noisy output samples. In the classical case, Gaus∗. The authors would like to thank Bradley M. Bell for insightful discussions and helpful suggestions. The research leading to these results has received funding from the European Union Seventh Framework Programme [FP7/2007-2013</p><p>4 0.36554754 <a title="103-lda-4" href="./jmlr-2013-Efficient_Active_Learning_of_Halfspaces%3A_An_Aggressive_Approach.html">39 jmlr-2013-Efficient Active Learning of Halfspaces: An Aggressive Approach</a></p>
<p>Author: Alon Gonen, Sivan Sabato, Shai Shalev-Shwartz</p><p>Abstract: We study pool-based active learning of half-spaces. We revisit the aggressive approach for active learning in the realizable case, and show that it can be made efﬁcient and practical, while also having theoretical guarantees under reasonable assumptions. We further show, both theoretically and experimentally, that it can be preferable to mellow approaches. Our efﬁcient aggressive active learner of half-spaces has formal approximation guarantees that hold when the pool is separable with a margin. While our analysis is focused on the realizable setting, we show that a simple heuristic allows using the same algorithm successfully for pools with low error as well. We further compare the aggressive approach to the mellow approach, and prove that there are cases in which the aggressive approach results in signiﬁcantly better label complexity compared to the mellow approach. We demonstrate experimentally that substantial improvements in label complexity can be achieved using the aggressive approach, for both realizable and low-error settings. Keywords: active learning, linear classiﬁers, margin, adaptive sub-modularity</p><p>5 0.35773623 <a title="103-lda-5" href="./jmlr-2013-Multicategory_Large-Margin_Unified_Machines.html">73 jmlr-2013-Multicategory Large-Margin Unified Machines</a></p>
<p>Author: Chong Zhang, Yufeng Liu</p><p>Abstract: Hard and soft classiﬁers are two important groups of techniques for classiﬁcation problems. Logistic regression and Support Vector Machines are typical examples of soft and hard classiﬁers respectively. The essential difference between these two groups is whether one needs to estimate the class conditional probability for the classiﬁcation task or not. In particular, soft classiﬁers predict the label based on the obtained class conditional probabilities, while hard classiﬁers bypass the estimation of probabilities and focus on the decision boundary. In practice, for the goal of accurate classiﬁcation, it is unclear which one to use in a given situation. To tackle this problem, the Largemargin Uniﬁed Machine (LUM) was recently proposed as a uniﬁed family to embrace both groups. The LUM family enables one to study the behavior change from soft to hard binary classiﬁers. For multicategory cases, however, the concept of soft and hard classiﬁcation becomes less clear. In that case, class probability estimation becomes more involved as it requires estimation of a probability vector. In this paper, we propose a new Multicategory LUM (MLUM) framework to investigate the behavior of soft versus hard classiﬁcation under multicategory settings. Our theoretical and numerical results help to shed some light on the nature of multicategory classiﬁcation and its transition behavior from soft to hard classiﬁers. The numerical results suggest that the proposed tuned MLUM yields very competitive performance. Keywords: hard classiﬁcation, large-margin, soft classiﬁcation, support vector machine</p><p>6 0.35227963 <a title="103-lda-6" href="./jmlr-2013-Learning_Bilinear_Model_for_Matching_Queries_and_Documents.html">60 jmlr-2013-Learning Bilinear Model for Matching Queries and Documents</a></p>
<p>7 0.34715122 <a title="103-lda-7" href="./jmlr-2013-Communication-Efficient_Algorithms_for_Statistical_Optimization.html">25 jmlr-2013-Communication-Efficient Algorithms for Statistical Optimization</a></p>
<p>8 0.3412891 <a title="103-lda-8" href="./jmlr-2013-A_Max-Norm_Constrained_Minimization_Approach_to_1-Bit_Matrix_Completion.html">4 jmlr-2013-A Max-Norm Constrained Minimization Approach to 1-Bit Matrix Completion</a></p>
<p>9 0.34059861 <a title="103-lda-9" href="./jmlr-2013-Sparsity_Regret_Bounds_for_Individual_Sequences_in_Online_Linear_Regression.html">105 jmlr-2013-Sparsity Regret Bounds for Individual Sequences in Online Linear Regression</a></p>
<p>10 0.33955368 <a title="103-lda-10" href="./jmlr-2013-Conjugate_Relation_between_Loss_Functions_and_Uncertainty_Sets_in_Classification_Problems.html">26 jmlr-2013-Conjugate Relation between Loss Functions and Uncertainty Sets in Classification Problems</a></p>
<p>11 0.33940649 <a title="103-lda-11" href="./jmlr-2013-Nonparametric_Sparsity_and_Regularization.html">76 jmlr-2013-Nonparametric Sparsity and Regularization</a></p>
<p>12 0.33875707 <a title="103-lda-12" href="./jmlr-2013-Multi-Stage_Multi-Task_Feature_Learning.html">72 jmlr-2013-Multi-Stage Multi-Task Feature Learning</a></p>
<p>13 0.33804017 <a title="103-lda-13" href="./jmlr-2013-A_Binary-Classification-Based_Metric_between_Time-Series_Distributions_and_Its_Use_in_Statistical_and_Learning_Problems.html">2 jmlr-2013-A Binary-Classification-Based Metric between Time-Series Distributions and Its Use in Statistical and Learning Problems</a></p>
<p>14 0.33779794 <a title="103-lda-14" href="./jmlr-2013-How_to_Solve_Classification_and_Regression_Problems_on_High-Dimensional_Data_with_a_Supervised_Extension_of_Slow_Feature_Analysis.html">52 jmlr-2013-How to Solve Classification and Regression Problems on High-Dimensional Data with a Supervised Extension of Slow Feature Analysis</a></p>
<p>15 0.33706105 <a title="103-lda-15" href="./jmlr-2013-Sparse_Matrix_Inversion_with_Scaled_Lasso.html">102 jmlr-2013-Sparse Matrix Inversion with Scaled Lasso</a></p>
<p>16 0.3362155 <a title="103-lda-16" href="./jmlr-2013-Construction_of_Approximation_Spaces_for_Reinforcement_Learning.html">28 jmlr-2013-Construction of Approximation Spaces for Reinforcement Learning</a></p>
<p>17 0.3361229 <a title="103-lda-17" href="./jmlr-2013-Belief_Propagation_for_Continuous_State_Spaces%3A_Stochastic_Message-Passing_with_Quantitative_Guarantees.html">17 jmlr-2013-Belief Propagation for Continuous State Spaces: Stochastic Message-Passing with Quantitative Guarantees</a></p>
<p>18 0.33539692 <a title="103-lda-18" href="./jmlr-2013-Generalized_Spike-and-Slab_Priors_for_Bayesian_Group_Feature_Selection_Using_Expectation_Propagation.html">48 jmlr-2013-Generalized Spike-and-Slab Priors for Bayesian Group Feature Selection Using Expectation Propagation</a></p>
<p>19 0.33438775 <a title="103-lda-19" href="./jmlr-2013-Algorithms_and_Hardness_Results_for_Parallel_Large_Margin_Learning.html">10 jmlr-2013-Algorithms and Hardness Results for Parallel Large Margin Learning</a></p>
<p>20 0.33286184 <a title="103-lda-20" href="./jmlr-2013-Semi-Supervised_Learning_Using_Greedy_Max-Cut.html">99 jmlr-2013-Semi-Supervised Learning Using Greedy Max-Cut</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
