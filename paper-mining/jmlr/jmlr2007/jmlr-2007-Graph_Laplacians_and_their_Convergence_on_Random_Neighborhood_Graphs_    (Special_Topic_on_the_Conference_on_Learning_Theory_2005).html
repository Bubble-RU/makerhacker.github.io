<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-38" href="#">jmlr2007-38</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</h1>
<br/><p>Source: <a title="jmlr-2007-38-pdf" href="http://jmlr.org/papers/volume8/hein07a/hein07a.pdf">pdf</a></p><p>Author: Matthias Hein, Jean-Yves Audibert, Ulrike von Luxburg</p><p>Abstract: Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. Keywords: graphs, graph Laplacians, semi-supervised learning, spectral clustering, dimensionality reduction</p><p>Reference: <a title="jmlr-2007-38-reference" href="../jmlr2007_reference/jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Sch¨ lkopf o Spemannstraße 38 T¨ bingen 72076, Germany u  Editor: Sanjoy Dasgupta  Abstract Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. [sent-9, score-0.441]
</p><p>2 The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. [sent-10, score-0.306]
</p><p>3 In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. [sent-11, score-0.326]
</p><p>4 We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. [sent-12, score-0.4]
</p><p>5 However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. [sent-13, score-0.46]
</p><p>6 In particular, if the data has support on a low-dimensional submanifold the neighborhood graph is a discrete approximation of the submanifold. [sent-21, score-0.441]
</p><p>7 Another more involved example is the work of Varopoulos (1984), where for a graph generated by an ε-packing of a manifold, the equivalence of certain properties of random walks on the graph and Brownian motion on the manifold have been established. [sent-29, score-0.446]
</p><p>8 The connection between random walks and the graph Laplacian becomes obvious by noting that the graph Laplacian as well as the Laplace-Beltrami operator are the generators of the diffusion process on the graph and the manifold, respectively. [sent-30, score-0.628]
</p><p>9 In Belkin and Niyogi (2005) the convergence was shown for the so called unnormalized graph Laplacian in the case of a uniform probability measure on a compact manifold without boundary and using the Gaussian kernel for the weights, whereas in Hein et al. [sent-51, score-0.568]
</p><p>10 (2005) the pointwise convergence was shown for the random walk graph Laplacian in the case of general probability measures on non-compact manifolds with boundary using general isotropic data-dependent weights. [sent-52, score-0.504]
</p><p>11 More recently Gin´ and Koltchinskii (2006) have extended the e pointwise convergence for the unnormalized graph Laplacian shown by Belkin and Niyogi (2005) to uniform convergence on compact submanifolds without boundary giving explicit rates. [sent-53, score-0.59]
</p><p>12 In particular, we deﬁne the three graph Laplacians used in machine learning so far, which we call the normalized, the unnormalized and the random walk Laplacian. [sent-59, score-0.338]
</p><p>13 We introduce a framework for studying noncompact manifolds with boundary and provide the necessary assumptions on the submanifold M, the data generating measure P and the kernel k used for deﬁning the weights of the edges. [sent-65, score-0.476]
</p><p>14 Abstract Deﬁnition of the Graph Structure In this section we deﬁne the structure on a graph which is required in order to deﬁne the graph Laplacian. [sent-69, score-0.306]
</p><p>15 We derive also the limit operator of the graph Laplacian induced by the difference operator of Equation (1) introduced by Zhou et al. [sent-115, score-0.418]
</p><p>16 in the machine learning literature and usually denoted as the normalized graph Laplacian in spectral graph theory (Chung, 1997). [sent-116, score-0.335]
</p><p>17 Unfortunately there exist no unique names for the three graph Laplacians we introduce here, most of the time all of them are just called graph Laplacians. [sent-140, score-0.306]
</p><p>18 Since the ﬁrst one is closely related to a random walk on the graph we call it random walk graph Laplacian and the other one normalized graph Laplacian. [sent-143, score-0.641]
</p><p>19 n χ(dl ) dl dl n i=1 i=1  The choice χ(dl ) = 1 and γ2 (wil )φ(wil ) = wil leads then to the graph Laplacian proposed in Chung and Langlands (1996); Zhou et al. [sent-153, score-0.375]
</p><p>20 (2004), (∆(n) f )(l) =  1 wil f (l) 1 n 1 n f (i) 1 √ √ dl − ∑ √ wli = , f (l) − ∑ f (i) √ n i=1 di n n i=1 n dl dl dl di  or equivalently 1  1  1  1  ∆(n) f = D− 2 (D −W )D− 2 f = ( − D− 2 W D− 2 ) f . [sent-154, score-0.383]
</p><p>21 Maybe not surprisingly, in general the LaplaceBeltrami operator will not be the limit operator of the graph Laplacian. [sent-158, score-0.418]
</p><p>22 Instead it will converge to the weighted Laplace-Beltrami operator which is the natural generalization of the Laplace-Beltrami operator for a Riemannian manifold equipped with a non-uniform probability measure. [sent-159, score-0.374]
</p><p>23 From now on we consider these graph Laplacians for the random neighborhood graph, ˜ that is the weights of the graph wi j have the form wi j = w(Xi , X j ) = kλ,h (Xi , X j ). [sent-181, score-0.474]
</p><p>24 Using the kernel function we can easily extend the graph Laplacians to the whole submanifold M. [sent-182, score-0.407]
</p><p>25 We would like to note that in the case of the random walk and and the normalized graph Laplacian the normalization with 1/hm in the weights cancels out, whereas it does not cancel for the unnormalized graph Laplacian except in the case λ = 1/2. [sent-194, score-0.491]
</p><p>26 However, in this work we assume for simplicity that for the unnormalized graph Laplacian the intrinsic dimension m of the submanifold is known. [sent-200, score-0.495]
</p><p>27 Deﬁnition 9 (Weighted Laplacian) Let (M, gab ) be a Riemannian manifold with measure P where P √ has a differentiable and positive density p with respect to the natural volume element dV = det g dx, and let ∆M be the Laplace-Beltrami operator on M. [sent-223, score-0.396]
</p><p>28 p In the next section it will turn out that through a data-dependent change of the weights of the graph we can get the just deﬁned weighted Laplacians as the limit operators of the graph Laplacian. [sent-229, score-0.337]
</p><p>29 • The eigenfunctions of the Laplacian ∆s can be seen as the limit partioning of spectral clustering for the normalized graph Laplacian (however, a rigorous mathematical proof has not been given yet, see von Luxburg et al. [sent-248, score-0.346]
</p><p>30 In this case all limits disagree and only the random walk graph Laplacian converges towards the weighted Laplace-Beltrami operator which is the natural generalization of the Laplace-Beltrami operator when the manifold is equipped with a non-uniform probability measure. [sent-268, score-0.618]
</p><p>31 If one uses this graph Laplacian as the diffusion operator to propagate the labeled data, it means that the diffusion for λ < 1/2 is faster in regions where the density is high. [sent-271, score-0.402]
</p><p>32 We note that up to a multiplication with the inverse of the density the elimination of density effects is also possible for the unnormalized graph Laplacian, but not for the normalized graph Laplacian. [sent-284, score-0.456]
</p><p>33 Therefore, in our opinion there is no universal best choice between the random walk and the unnormalized graph Laplacian from a machine learning point of view. [sent-290, score-0.338]
</p><p>34 Note that ∆ f = 0 so that for the random walk and the unnormalized graph Laplacian only the anisotropic part of the limit operator, 1 ∇p∇ f is non-zero. [sent-306, score-0.369]
</p><p>35 1339  H EIN , AUDIBERT AND VON L UXBURG  random walk graph Laplacian together with the result of the weighted Laplace-Beltrami operator and an error plot for λ = 0, 1, 2 resulting in s = −2, 0, 2 for the function f (φ, θ) = cos(θ). [sent-323, score-0.361]
</p><p>36 In particular, it turns out that the so called manifolds with boundary of bounded geometry are the natural framework where one can still deal with non-compact manifolds in a setting comparable to the compact case. [sent-335, score-0.426]
</p><p>37 After a proper statement of the assumptions under which we prove the convergence results of the graph Laplacian and a preliminary result about convolutions on submanifolds which is of interest on its own, we then start with the ﬁnal proofs. [sent-336, score-0.315]
</p><p>38 Therefore we have to restrict the class of submanifolds since manifolds with unbounded curvature do not allow reasonable function spaces. [sent-341, score-0.32]
</p><p>39 Nevertheless we think that it is worth this effort since the class of manifolds with boundary of bounded geometry includes almost any kind of submanifold one could have in mind. [sent-354, score-0.483]
</p><p>40 6  1341  H EIN , AUDIBERT AND VON L UXBURG  Note that the boundary ∂M is an isometric submanifold of M of dimension m − 1. [sent-386, score-0.336]
</p><p>41 Deﬁnition 11 (Manifold with boundary of bounded geometry) Let M be a manifold with boundary ∂M (possibly empty). [sent-390, score-0.314]
</p><p>42 The lower bound on the injectivity radius of M and the bound on the curvature are standard to deﬁne manifolds of bounded geometry without boundary. [sent-400, score-0.371]
</p><p>43 Therefore one replaces next to the boundary standard normal coordinates with normal collar coordinates. [sent-402, score-0.304]
</p><p>44 g−1 ,  Lemma 14 (Schick 2001) Let (M, g) be a Riemannian manifold with boundary of bounded geometry of dimension m. [sent-414, score-0.304]
</p><p>45 In that way it is possible to establish a well-deﬁned notion of Sobolev spaces on manifolds with boundary of bounded geometry in the sense that any norm deﬁned with respect to a different covering of M by normal coordinate charts is equivalent. [sent-425, score-0.369]
</p><p>46 Let (Ui , φi )i∈I be a countable covering of the submanifold M with normal coordinate charts of M, that is M ⊂ ∪ i∈I Ui , then: f  Ck (M)  = max sup sup  m≤k i∈I x∈φi (Ui )  Dm ( f ◦ φ−1 )(x) . [sent-426, score-0.318]
</p><p>47 The next proposition proven by Smolyanov, von Weizs acker, and Wittich (2000) provides an asymptotic expression of geometric quantities of the submanifold M in 1343  H EIN , AUDIBERT AND VON L UXBURG  the neighborhood of a point x ∈ M. [sent-433, score-0.467]
</p><p>48 The volume form dV = det gi j (y) dy of M satisﬁes in normal coordinates, 1 dV = 1 + Riuvi yu yv + O( y 6  3 Rm )  dy,  In particular, 1 (∆ det gi j )(0) = − R, 3 where R is the scalar curvature (i. [sent-438, score-0.412]
</p><p>49 We would like to note that in Smolyanov, von Weizs¨ cker, and Wittich (2007) this proposition a was formulated for general ambient spaces X, that is arbitrary Riemannian manifolds X. [sent-441, score-0.32]
</p><p>50 Then the segment c along the boundary will be a geodesic of the submanifold ∂M, see Alexander and Alexander (1981), that 1344  L IMIT OF G RAPH L APLACIANS ON R ANDOM N EIGHBORHOOD G RAPHS  ˙ is Dt c = ∇c c = 0 where ∇ is the connection of ∂M induced by M. [sent-455, score-0.345]
</p><p>51 However, we will indicate how the technical assumptions simplify if one has a compact submanifold with boundary or even a compact manifold without boundary. [sent-483, score-0.587]
</p><p>52 The use of the abstract manifold M as a starting point emphasizes that there exists an m-dimensional smooth manifold M or roughly equivalent an m-dimensional smooth parameter space underlying the data. [sent-489, score-0.344]
</p><p>53 1, manifolds of bounded geometry are in general non-compact, complete Riemannian manifolds with boundary where one has uniform control over all intrinsic curvatures. [sent-498, score-0.402]
</p><p>54 Moreover, note that for submanifolds with boundary one has inj(x) → 0 as x approaches the boundary 2 ∂M. [sent-513, score-0.304]
</p><p>55 Manifold M √ gi j , det g natural volume element ∆s  open set in Rd δi j , 1 Lebesgue measure ∂p ∂ ∂2 s ∆s = ∑d ∂(zi )2 + p ∑d ∂zi ∂zi i=1 i=1  The kernel functions which are used to deﬁne the weights of the graph are always functions of the squared norm in Rd . [sent-521, score-0.302]
</p><p>56 This is the reason why one replaces normal coordinates in the neighborhood of the boundary with normal collar coordinates. [sent-526, score-0.376]
</p><p>57 Also let us introduce the helpful notation, kh (t) = h1 k ht2 where we call h the bandwidth of the kernel. [sent-532, score-0.329]
</p><p>58 Let x ∈ M\∂M and Vi := kh ( i(x) − i(Xi ) 2 ) f (Xi ). [sent-560, score-0.329]
</p><p>59 n i=1  Let Wi = kh ( i(x) − i(Xi ) 2 )( f (x) − f (Xi )). [sent-562, score-0.329]
</p><p>60 1349  H EIN , AUDIBERT AND VON L UXBURG  Let Wi := kh ( i(x) − i(Xi ) 2 ) f (Xi ). [sent-566, score-0.329]
</p><p>61 Then we get, 2 VarW ≤ EZ kh ( i(x) − i(Z) 2 ) f 2 (Z) ≤  k ∞ f hm  2 ∞  k ∞ f hm  ph (x) ≤ D2  2 ∞,  where we have used Lemma 46 in the last step. [sent-570, score-0.504]
</p><p>62 R √ Note that EZ kh ( i(x) − i(Z) 2 ) f (Z) = kh ( i(x) − i(y) 2 ) f (y)p(y) det g dy. [sent-578, score-0.769]
</p><p>63 Approximations of the Laplace-Beltrami operator based on averaging with the Gaussian kernel in the case of a uniform probability measure have been studied for compact submanifolds without boundary by Smolyanov, von Weizs¨ cker, and Wittich (2000, 2007) and Belkin (2003). [sent-592, score-0.561]
</p><p>64 The proof given in Lafon (2004) applies only to compact hypersurfaces4 in Rd , a proof for the general case of compact submanifolds with boundary using boundary conditions has been presented in Coifman and Lafon (2006). [sent-594, score-0.416]
</p><p>65 In this section we will prove the pointwise convergence of the continuous approximation for general submanifolds M with boundary of bounded geometry with the additional Assumptions 19. [sent-595, score-0.364]
</p><p>66 The rigorous treatment of the approximation of the boundary respectively the boundary conditions of a function on a randomly sampled graph remains as an open problem. [sent-604, score-0.327]
</p><p>67 Using f (y) = 1 we get, d˜λ,h (x) =  Z  kh  BRd (x,hRk )∩M  i(x) − i(y) pλ (x) h  2  C1 p(y) − λ/2C2 h2 (p(y)S + ∆p) λ+1 C1 p(y)λ  + O(h3 )  det g dy,  as an estimate for d˜λ,h (x). [sent-622, score-0.44]
</p><p>68 This is due to the fact that the Laplaceλ,h Beltrami operator on manifolds is usually deﬁned as a negative deﬁnite operator (in analogy to the Laplace operator in Rd ), whereas the graph Laplacian is positive deﬁnite. [sent-625, score-0.607]
</p><p>69 2C1  Using the lower bound of ph (x) = EZ kh ( i(x) − i(Z) 2 ) derived in Lemma 46 we can for hRk ≤ κ/2 directly apply Lemma 23. [sent-653, score-0.504]
</p><p>70 The weak pointwise consistency of the unnormalized graph Laplacian for compact submanifolds with the uniform probability measure using the Gaussian kernel for the weights and λ = 0 was proven by Belkin and Niyogi (2005). [sent-659, score-0.541]
</p><p>71 We prove here the limits of all three graph Laplacians for general submanifolds with boundary of bounded geometry, general probability measures P, and general kernel functions k as stated in our standard assumptions. [sent-662, score-0.408]
</p><p>72 , n}, dh,n (X j ) − ph (X j ) ≤ ε,   d (x) − p (x) ≤ ε, h,n h 2 √  1 n kh ( i(x)−i(X j ) )|g(X j )| R p(y)  − kh ( i(x) − i(y) 2 )|g(y)| [p (x) p (y)]λ det g dy ≤ h ε. [sent-680, score-0.997]
</p><p>73 For the second assertion deﬁning E , we use Lemma 23 P( |dh,n (x) − ph (x)| > ε ) ≤ 2 exp − 1354  nhm ε2 , 2b2 + 2b1 ε/3  L IMIT OF G RAPH L APLACIANS ON R ANDOM N EIGHBORHOOD G RAPHS  where b1 and b2 are constants depending on the kernel k and p. [sent-682, score-0.372]
</p><p>74 We get for nhm < ε/2 and 1 ≤ j ≤ n, 1 n  P  (n−1)h ε ∑n kh ( i(X j ) − i(Xi ) ) − ph (x) > ε X j ≤ 2 exp − 8b2 +4b1 ε/3 . [sent-684, score-0.663]
</p><p>75 i=1 m 2  2  This follows by n 1 n 1 2 2 kh ( i(X j ) − i(Xi ) ) − ph (X j ) ≤ ∑ ∑ kh ( i(X j ) − i(Xi ) ) n i=1 n(n − 1) i=1 1 2 + ∑ kh ( i(X j ) − i(Xi ) ) − ph (X j ) , n − 1 i= j k  ∞ where the ﬁrst term is upper bounded by nhm . [sent-685, score-1.496]
</p><p>76 Let us deﬁne  √ 2 −λ M kh ( i(x) − i(y) )( f (x) − f (y))[ph (x) ph (y)] p(y) det gdy, 2 −λ 1 n , n ∑ j=1 kh ( i(x) − i(X j ) )( f (x) − f (X j )) dh,n (x) dh,n (X j )  B := ˆ B :=  R  ˆ ˜ ˜ then (Aλ,h,n g)(x) = B and (Aλ,h g)(x) = B . [sent-692, score-0.944]
</p><p>77 So we can write for ε < D1 /2, 1 dh,n (x) dh,n (X j )  λ  −  1 ph (x) ph (X j )  λ  ≤ λ(D1 − ε)−2λ−2 |dh,n (x)dh,n (X j ) − ph (x)ph (X j )| ≤ 2 λ(D1 − ε)−2λ−2 (D2 + ε)ε := C ε. [sent-696, score-0.525]
</p><p>78 Noting that for hRk ≤ κ/2 by Lemma 18, dM (x, y) ≤ 2hRk , ∀y ∈ BRd (x, hRk ) ∩ M, ˆ B −B  ≤  1 n  2  ∑n kh ( i(x) − i(X j ) )| f (x) − f (X j )|C ε j=1 2  ∑n kh ( i(x) − i(X j ) )( f (x) − f (X j ))[ph (x) ph (X j )]−λ − B j=1 ≤ 2C k ∞ Rk supy∈M ∇ f Ty M h ε + h ε. [sent-697, score-0.833]
</p><p>79 1355  and  2 k ∞ nhm  < ε < 1/C,  H EIN , AUDIBERT AND VON L UXBURG  with probability at least 1 −Cne−  nhm ε2 C  . [sent-700, score-0.318]
</p><p>80 This leads us to our ﬁrst main result for the random walk and the unnormalized graph Laplacian. [sent-701, score-0.338]
</p><p>81 A smooth manifold with boundary is a manifold with boundary with a C∞ -atlas. [sent-756, score-0.486]
</p><p>82 Deﬁnition 33 A subset M of a d-dimensional manifold X is a m-dimensional submanifold M with boundary if every point x ∈ M is in the domain of a chart (U, φ) of X such that φ : U ∩ M → Hm × a,  φ(x) = (x1 , . [sent-761, score-0.443]
</p><p>83 Deﬁnition 34 A Riemannian manifold (M, g) is a smooth manifold M together with a tensor 6 of type (0, 2), called the metric tensor g, at each p ∈ M, such that g deﬁnes an inner product on the tangent space Tp M which varies smoothly over M. [sent-777, score-0.408]
</p><p>84 A submanifold M of a Riemannian manifold (X, g) has a natural Riemannian metric h induced from X in the following way. [sent-790, score-0.356]
</p><p>85 Intuitively, normal coordinates around a point p of an m-dimensional Riemannian manifold M are coordinates chosen such that M looks around p like Rm in the best possible way. [sent-799, score-0.334]
</p><p>86 3 The Second Fundamental Form In this section we assume that M is an isometrically embedded submanifold of a manifold X. [sent-835, score-0.356]
</p><p>87 The following Lemma shows that the second fundamental form Π of an isometrically embedded submanifold M of Rd is in normal coordinates just the Hessian of i. [sent-850, score-0.338]
</p><p>88 Assume now z 2 ≥ z 2 + 1 Vi jkl zi z j zk zl + β(z) z 5 ≥ 4 z 2 on B(0, rmin ) ⊂ Rm , where β(z) is continuous and β(z) ∼ O(1) as z → 0. [sent-870, score-0.398]
</p><p>89 Proof As a ﬁrst step we do a Taylor expansion of the kernel around z kh  z  2  +η  h2  = kh  z 2 h2  +  ∂kh ∂x  z 2 h2  ≤ Ch3 ,  η ∂2 kh (x) + h2 ∂x2  2  /h2 :  z 2 (1−θ)+θ η h2  η2 , h4  where in the last term 0 ≤ θ(z) ≤ 1. [sent-872, score-1.025]
</p><p>90 For the argument of the kernel in α1 and α2 we have by our assumptions on B(0, rmin ): z z 2 ≥ 2 h  2  +Vi jkl zi z j zk zl + β(z) z h2  5  ≥  z 2 . [sent-879, score-0.468]
</p><p>91 |α1 | ≤  Z  B(0,rmin )  ≤h4 f  C3  Z  ∂2 k h ∂x2  r B(0, min ) h  z ∂2 k ∂x2  2  (1 − θ) + θη h2 u  2  Vi jkl zi z j zk zl + β(z) z h4  (1 − θ) + θη 1363  m2 max |Vi jkl | u i, j,k,l  5 2  f (z)dz 4  +h β  ∞  u  5  2  du. [sent-883, score-0.39]
</p><p>92 h h √ √ √ Now suppose rmin ≥ 2 A and decompose B(0, rmin ) as B(0, rmin ) = B(0, 2 A)∪B(0, rmin )\B(0, 2 A). [sent-885, score-0.492]
</p><p>93 Now one has10 : e  1 α 2 rmin , so that for h < min{ 3  − ξ2 h  ≤ hs /ξs for  rmin α √ 2 rmin , A } =  h0 all error terms are smaller than a constant times h3 where the constant depends on k, rmin , Vi jkl and f C3 . [sent-890, score-0.607]
</p><p>94 The integral over M\B(x, ε) can be upper bounded by using the deﬁnition of δ(x) (see Assumption 19) and the fact that k is non-increasing: Z M  kh  i(x) − i(y)  2 Rd  f (y)p(y)  Z  det g dy =  kh  i(x) − i(y)  kh  i(x) − i(y)  2 Rd  f (y)p(y)  det g dy  f (y)p(y)  det g dy. [sent-894, score-1.426]
</p><p>95 B(x,ε)  +  Z  2 Rd  M\B(x,ε)  Since k is non-increasing, we have the following inequality for the integral over M\B(x, ε): Z  M\B(x,ε)  kh  i(x) − i(y)  2 Rd  f (y)p(y)  10. [sent-895, score-0.329]
</p><p>96 That implies k ph (y) ≤ m∞ pmax h  Z  BM (y,2hRk )  det g dz ≤ k  1365  ∞  pmax S2 2m Rm , k  H EIN , AUDIBERT AND VON L UXBURG  where the last inequality follows from Lemma 14. [sent-914, score-0.36]
</p><p>97 We get ph (y) ≥  k ∞ 2hm  Z  p(z)  BRd (x,h rk )∩M  det g dz ≥  k ∞ k ∞ m pmin volM (BM (x, h rk )) ≥ pmin S1 rk . [sent-917, score-0.504]
</p><p>98 Then ph (y) ≤ ph (y) ≥ ≥  Z  M  kh (dM (y, z))p(z)  det g dz ≥  k ∞ hm  Z  ≤ k  BM (y,h rk )  ∞  Rk m . [sent-919, score-0.912]
</p><p>99 s  For the lower bound we get  kh (dM (y, z))p(z)  det g dz  k k ∞ rk P BM (y, h rk ) ≥ m ∞ P BM (y, s ) . [sent-920, score-0.61]
</p><p>100 From graphs to manifolds - weak and strong pointwise consistency of graph Laplacians. [sent-1080, score-0.326]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('laplacian', 0.332), ('kh', 0.329), ('submanifold', 0.216), ('laplacians', 0.181), ('ph', 0.175), ('nhm', 0.159), ('graph', 0.153), ('rw', 0.153), ('imit', 0.143), ('tp', 0.143), ('uxburg', 0.143), ('manifold', 0.14), ('aplacians', 0.136), ('eighborhood', 0.136), ('von', 0.133), ('wil', 0.132), ('submanifolds', 0.13), ('rmin', 0.123), ('audibert', 0.121), ('operator', 0.117), ('hrk', 0.117), ('jkl', 0.115), ('raphs', 0.115), ('det', 0.111), ('ein', 0.108), ('manifolds', 0.103), ('raph', 0.103), ('rd', 0.101), ('inj', 0.097), ('unnormalized', 0.094), ('walk', 0.091), ('brd', 0.091), ('dm', 0.09), ('andom', 0.088), ('boundary', 0.087), ('curvature', 0.087), ('riemannian', 0.082), ('geometry', 0.077), ('lafon', 0.077), ('dz', 0.074), ('za', 0.074), ('neighborhood', 0.072), ('coordinates', 0.072), ('wli', 0.071), ('pointwise', 0.07), ('dv', 0.07), ('zb', 0.067), ('zi', 0.067), ('bm', 0.066), ('injectivity', 0.065), ('extrinsic', 0.06), ('tx', 0.058), ('rm', 0.056), ('compact', 0.056), ('dy', 0.053), ('charts', 0.052), ('smolyanov', 0.052), ('weizs', 0.052), ('diffusion', 0.052), ('normal', 0.05), ('hein', 0.05), ('zk', 0.05), ('wi', 0.048), ('rk', 0.048), ('proposition', 0.046), ('cker', 0.045), ('collar', 0.045), ('wittich', 0.045), ('dl', 0.045), ('acceleration', 0.045), ('ps', 0.045), ('vi', 0.043), ('zl', 0.043), ('geodesic', 0.042), ('ez', 0.041), ('coifman', 0.039), ('hv', 0.039), ('radius', 0.039), ('ambient', 0.038), ('belkin', 0.038), ('kernel', 0.038), ('lemma', 0.037), ('tensor', 0.033), ('covariant', 0.033), ('isometric', 0.033), ('intrinsic', 0.032), ('div', 0.032), ('geodesics', 0.032), ('differential', 0.032), ('assumptions', 0.032), ('vertex', 0.032), ('dt', 0.032), ('euclidean', 0.032), ('smooth', 0.032), ('limit', 0.031), ('tangent', 0.03), ('luxburg', 0.029), ('spectral', 0.029), ('undirected', 0.029), ('density', 0.028)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="38-tfidf-1" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Matthias Hein, Jean-Yves Audibert, Ulrike von Luxburg</p><p>Abstract: Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. Keywords: graphs, graph Laplacians, semi-supervised learning, spectral clustering, dimensionality reduction</p><p>2 0.18580393 <a title="38-tfidf-2" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A speciﬁc instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a ﬁnal parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A speciﬁc instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystr¨ m extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker o sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are brieﬂy summarized at the end. Keywords: Markov decision processes, reinforcement learning, value function approximation, manifold learning, spectral graph theory</p><p>3 0.17467725 <a title="38-tfidf-3" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>4 0.065363295 <a title="38-tfidf-4" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>5 0.062349014 <a title="38-tfidf-5" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>Author: Yiming Ying, Ding-Xuan Zhou</p><p>Abstract: Gaussian kernels with ﬂexible variances provide a rich family of Mercer kernels for learning algorithms. We show that the union of the unit balls of reproducing kernel Hilbert spaces generated by Gaussian kernels with ﬂexible variances is a uniform Glivenko-Cantelli (uGC) class. This result conﬁrms a conjecture concerning learnability of Gaussian kernels and veriﬁes the uniform convergence of many learning algorithms involving Gaussians with changing variances. Rademacher averages and empirical covering numbers are used to estimate sample errors of multi-kernel regularization schemes associated with general loss functions. It is then shown that the regularization error associated with the least square loss and the Gaussian kernels can be greatly improved when ﬂexible variances are allowed. Finally, for regularization schemes generated by Gaussian kernels with ﬂexible variances we present explicit learning rates for regression with least square loss and classiﬁcation with hinge loss. Keywords: Gaussian kernel, ﬂexible variances, learning theory, Glivenko-Cantelli class, regularization scheme, empirical covering number</p><p>6 0.054747779 <a title="38-tfidf-6" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>7 0.053727228 <a title="38-tfidf-7" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>8 0.05112756 <a title="38-tfidf-8" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>9 0.045500968 <a title="38-tfidf-9" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>10 0.043133087 <a title="38-tfidf-10" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>11 0.039152205 <a title="38-tfidf-11" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>12 0.038967248 <a title="38-tfidf-12" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>13 0.038104456 <a title="38-tfidf-13" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>14 0.03764696 <a title="38-tfidf-14" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>15 0.037594631 <a title="38-tfidf-15" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>16 0.036617696 <a title="38-tfidf-16" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>17 0.036589194 <a title="38-tfidf-17" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>18 0.036320604 <a title="38-tfidf-18" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>19 0.036187965 <a title="38-tfidf-19" href="./jmlr-2007-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">80 jmlr-2007-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>20 0.034969129 <a title="38-tfidf-20" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.23), (1, -0.092), (2, 0.125), (3, 0.046), (4, -0.285), (5, -0.005), (6, 0.209), (7, 0.107), (8, 0.265), (9, 0.123), (10, 0.173), (11, 0.016), (12, -0.061), (13, -0.18), (14, 0.182), (15, -0.078), (16, -0.142), (17, 0.024), (18, 0.101), (19, 0.009), (20, -0.088), (21, -0.026), (22, -0.169), (23, 0.128), (24, 0.067), (25, -0.008), (26, 0.146), (27, 0.021), (28, 0.058), (29, 0.084), (30, 0.039), (31, 0.009), (32, 0.103), (33, -0.01), (34, 0.036), (35, -0.026), (36, 0.024), (37, 0.05), (38, -0.027), (39, -0.037), (40, 0.007), (41, -0.039), (42, -0.037), (43, 0.0), (44, 0.031), (45, 0.042), (46, 0.06), (47, -0.052), (48, -0.003), (49, -0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96536481 <a title="38-lsi-1" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Matthias Hein, Jean-Yves Audibert, Ulrike von Luxburg</p><p>Abstract: Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. Keywords: graphs, graph Laplacians, semi-supervised learning, spectral clustering, dimensionality reduction</p><p>2 0.80413091 <a title="38-lsi-2" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>3 0.72659826 <a title="38-lsi-3" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A speciﬁc instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a ﬁnal parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A speciﬁc instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystr¨ m extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker o sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are brieﬂy summarized at the end. Keywords: Markov decision processes, reinforcement learning, value function approximation, manifold learning, spectral graph theory</p><p>4 0.2113865 <a title="38-lsi-4" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>5 0.20963354 <a title="38-lsi-5" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>Author: Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee, Robert L. Wolpert</p><p>Abstract: Kernel methods have been very popular in the machine learning literature in the last ten years, mainly in the context of Tikhonov regularization algorithms. In this paper we study a coherent Bayesian kernel model based on an integral operator deﬁned as the convolution of a kernel with a signed measure. Priors on the random signed measures correspond to prior distributions on the functions mapped by the integral operator. We study several classes of signed measures and their image mapped by the integral operator. In particular, we identify a general class of measures whose image is dense in the reproducing kernel Hilbert space (RKHS) induced by the kernel. A consequence of this result is a function theoretic foundation for using non-parametric prior speciﬁcations in Bayesian modeling, such as Gaussian process and Dirichlet process prior distributions. We discuss the construction of priors on spaces of signed measures using Gaussian and L´ vy processes, e with the Dirichlet processes being a special case the latter. Computational issues involved with sampling from the posterior distribution are outlined for a univariate regression and a high dimensional classiﬁcation problem. Keywords: reproducing kernel Hilbert space, non-parametric Bayesian methods, L´ vy processes, e Dirichlet processes, integral operator, Gaussian processes c 2007 Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee and Robert L. Wolpert. P ILLAI , W U , L IANG , M UKHERJEE AND W OLPERT</p><p>6 0.19946699 <a title="38-lsi-6" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>7 0.19378436 <a title="38-lsi-7" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>8 0.18763196 <a title="38-lsi-8" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>9 0.18491141 <a title="38-lsi-9" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>10 0.17639636 <a title="38-lsi-10" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>11 0.17004982 <a title="38-lsi-11" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>12 0.16639689 <a title="38-lsi-12" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>13 0.15681285 <a title="38-lsi-13" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>14 0.1561882 <a title="38-lsi-14" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>15 0.15143284 <a title="38-lsi-15" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>16 0.14419056 <a title="38-lsi-16" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>17 0.14171945 <a title="38-lsi-17" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>18 0.14153941 <a title="38-lsi-18" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>19 0.14151374 <a title="38-lsi-19" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>20 0.14096376 <a title="38-lsi-20" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.028), (10, 0.018), (12, 0.592), (28, 0.047), (40, 0.042), (48, 0.024), (60, 0.03), (85, 0.028), (98, 0.072)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89396161 <a title="38-lda-1" href="./jmlr-2007-Structure_and_Majority_Classes_in_Decision_Tree_Learning.html">79 jmlr-2007-Structure and Majority Classes in Decision Tree Learning</a></p>
<p>Author: Ray J. Hickey</p><p>Abstract: To provide good classification accuracy on unseen examples, a decision tree, learned by an algorithm such as ID3, must have sufficient structure and also identify the correct majority class in each of its leaves. If there are inadequacies in respect of either of these, the tree will have a percentage classification rate below that of the maximum possible for the domain, namely (100 Bayes error rate). An error decomposition is introduced which enables the relative contributions of deficiencies in structure and in incorrect determination of majority class to be isolated and quantified. A sub-decomposition of majority class error permits separation of the sampling error at the leaves from the possible bias introduced by the attribute selection method of the induction algorithm. It is shown that sampling error can extend to 25% when there are more than two classes. Decompositions are obtained from experiments on several data sets. For ID3, the effect of selection bias is shown to vary from being statistically non-significant to being quite substantial, with the latter appearing to be associated with a simple underlying model. Keywords: decision tree learning, error decomposition, majority classes, sampling error, attribute selection bias 1</p><p>same-paper 2 0.88494831 <a title="38-lda-2" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Matthias Hein, Jean-Yves Audibert, Ulrike von Luxburg</p><p>Abstract: Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. Keywords: graphs, graph Laplacians, semi-supervised learning, spectral clustering, dimensionality reduction</p><p>3 0.83654237 <a title="38-lda-3" href="./jmlr-2007-Dynamics_and_Generalization_Ability_of_LVQ_Algorithms.html">30 jmlr-2007-Dynamics and Generalization Ability of LVQ Algorithms</a></p>
<p>Author: Michael Biehl, Anarta Ghosh, Barbara Hammer</p><p>Abstract: Learning vector quantization (LVQ) schemes constitute intuitive, powerful classiﬁcation heuristics with numerous successful applications but, so far, limited theoretical background. We study LVQ rigorously within a simplifying model situation: two competing prototypes are trained from a sequence of examples drawn from a mixture of Gaussians. Concepts from statistical physics and the theory of on-line learning allow for an exact description of the training dynamics in highdimensional feature space. The analysis yields typical learning curves, convergence properties, and achievable generalization abilities. This is also possible for heuristic training schemes which do not relate to a cost function. We compare the performance of several algorithms, including Kohonen’s LVQ1 and LVQ+/-, a limiting case of LVQ2.1. The former shows close to optimal performance, while LVQ+/- displays divergent behavior. We investigate how early stopping can overcome this difﬁculty. Furthermore, we study a crisp version of robust soft LVQ, which was recently derived from a statistical formulation. Surprisingly, it exhibits relatively poor generalization. Performance improves if a window for the selection of data is introduced; the resulting algorithm corresponds to cost function based LVQ2. The dependence of these results on the model parameters, for example, prior class probabilities, is investigated systematically, simulations conﬁrm our analytical ﬁndings. Keywords: prototype based classiﬁcation, learning vector quantization, Winner-Takes-All algorithms, on-line learning, competitive learning</p><p>4 0.42674387 <a title="38-lda-4" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>5 0.4016265 <a title="38-lda-5" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>Author: Dima Kuzmin, Manfred K. Warmuth</p><p>Abstract: n Maximum concept classes of VC dimension d over n domain points have size ≤d , and this is an upper bound on the size of any concept class of VC dimension d over n points. We give a compression scheme for any maximum class that represents each concept by a subset of up to d unlabeled domain points and has the property that for any sample of a concept in the class, the representative of exactly one of the concepts consistent with the sample is a subset of the domain of the sample. This allows us to compress any sample of a concept in the class to a subset of up to d unlabeled sample points such that this subset represents a concept consistent with the entire original sample. Unlike the previously known compression scheme for maximum classes (Floyd and Warmuth, 1995) which compresses to labeled subsets of the sample of size equal d, our new scheme is tight in the sense that the number of possible unlabeled compression sets of size at most d equals the number of concepts in the class. Keywords: compression schemes, VC dimension, maximum classes, one-inclusion graph</p><p>6 0.38865632 <a title="38-lda-6" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>7 0.38570914 <a title="38-lda-7" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>8 0.3438639 <a title="38-lda-8" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>9 0.32289267 <a title="38-lda-9" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>10 0.32160664 <a title="38-lda-10" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>11 0.31786457 <a title="38-lda-11" href="./jmlr-2007-Loop_Corrections_for_Approximate_Inference_on_Factor_Graphs.html">51 jmlr-2007-Loop Corrections for Approximate Inference on Factor Graphs</a></p>
<p>12 0.31167781 <a title="38-lda-12" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>13 0.30976421 <a title="38-lda-13" href="./jmlr-2007-Bayesian_Quadratic_Discriminant_Analysis.html">13 jmlr-2007-Bayesian Quadratic Discriminant Analysis</a></p>
<p>14 0.30888823 <a title="38-lda-14" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>15 0.29936397 <a title="38-lda-15" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>16 0.2961677 <a title="38-lda-16" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>17 0.29260576 <a title="38-lda-17" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>18 0.2884953 <a title="38-lda-18" href="./jmlr-2007-Truncating_the_Loop_Series_Expansion_for_Belief_Propagation.html">86 jmlr-2007-Truncating the Loop Series Expansion for Belief Propagation</a></p>
<p>19 0.28750041 <a title="38-lda-19" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>20 0.28718364 <a title="38-lda-20" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
