<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>47 jmlr-2007-Learning Horn Expressions with LOGAN-H</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-47" href="#">jmlr2007-47</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>47 jmlr-2007-Learning Horn Expressions with LOGAN-H</h1>
<br/><p>Source: <a title="jmlr-2007-47-pdf" href="http://jmlr.org/papers/volume8/arias07a/arias07a.pdf">pdf</a></p><p>Author: Marta Arias, Roni Khardon, Jérôme Maloberti</p><p>Abstract: The paper introduces L OG A N -H —a system for learning ﬁrst-order function-free Horn expressions from interpretations. The system is based on an algorithm that learns by asking questions and that was proved correct in previous work. The current paper shows how the algorithm can be implemented in a practical system, and introduces a new algorithm based on it that avoids interaction and learns from examples only. The L OG A N -H system implements these algorithms and adds several facilities and optimizations that allow efﬁcient applications in a wide range of problems. As one of the important ingredients, the system includes several fast procedures for solving the subsumption problem, an NP-complete problem that needs to be solved many times during the learning process. We describe qualitative and quantitative experiments in several domains. The experiments demonstrate that the system can deal with varied problems, large amounts of data, and that it achieves good classiﬁcation accuracy. Keywords: inductive logic programming, subsumption, bottom-up learning, learning with queries</p><p>Reference: <a title="jmlr-2007-47-reference" href="../jmlr2007_reference/jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 As one of the important ingredients, the system includes several fast procedures for solving the subsumption problem, an NP-complete problem that needs to be solved many times during the learning process. [sent-10, score-0.532]
</p><p>2 The hypothesis of the system may include recursive clauses where the same predicate appears both in the condition of the rule and in the conclusion (obviously with different arguments). [sent-91, score-0.391]
</p><p>3 One of the new methods modiﬁes D JANGO to use a different representation of the subsumption problem and requires slightly different constraint satisfaction algorithms. [sent-107, score-0.529]
</p><p>4 The experiments also demonstrate that the different subsumption algorithms are effective on a range of problems but no one dominates the others with L OG A N -H. [sent-123, score-0.458]
</p><p>5 Further experiments evaluating the subsumption methods on their own show that our modiﬁed D JANGO method is a promising approach when hard subsumption problems need to be solved. [sent-124, score-0.916]
</p><p>6 The paper also makes a contribution to the study of efﬁcient subsumption algorithms. [sent-129, score-0.458]
</p><p>7 We develop some new subsumption algorithms and evaluate them both in the context of machine learning and independently. [sent-130, score-0.458]
</p><p>8 One of the new methods, called D JANGO P RIMAL below, seems particularly promising when hard subsumption problems need to be solved. [sent-131, score-0.458]
</p><p>9 Section 4 describes three subsumption engines that our system uses. [sent-135, score-0.532]
</p><p>10 Section 5 describes extensive experiments that demonstrate the validity of our method and of the subsumption procedures. [sent-136, score-0.458]
</p><p>11 A Horn clause (sometimes called a Horn rule or just a rule) is an expression C = (∧n∈N n) → P, where N is a set of atoms and P is an atom. [sent-145, score-0.398]
</p><p>12 The clause set [s, c] (where c = 0) repV resents the conjunction of clauses b∈c (s → b). [sent-162, score-0.485]
</p><p>13 The ﬁrst step results in a set of clauses [s, c] such that s (the antecedent) is the conjunction of all atoms true in I and c (the conclusions) is the set of all atoms (over the domain of I) which are false in I. [sent-187, score-0.425]
</p><p>14 This clause set is such that all arguments to the predicates are domain elements of I. [sent-188, score-0.423]
</p><p>15 While domain elements are not constants in the language and we do not allow constants in the rules we slightly abuse normal terminology and call this intermediate form a ground clause set. [sent-190, score-0.382]
</p><p>16 For example, the clause set rel-cands(e3 ) includes among others the clauses [p(x1 , x2 ) ∧ p(x2 , x2 ) ∧ q(x2 , x1 ) → p(x2 , x1 )], and [p(x1 , x2 ) ∧ p(x2 , x2 ) ∧ q(x2 , x1 ) → q(x1 , x1 )], where all variables are universally quantiﬁed. [sent-192, score-0.485]
</p><p>17 When removing an object from a clause set [s, c] we remove all atoms referring to it from s and c. [sent-198, score-0.398]
</p><p>18 Pairing: The pairing operation combines two clause sets [sa , ca ] and [sb , cb ] to create a new clause set [s p , c p ]. [sent-213, score-0.713]
</p><p>19 The clause set [s p , c p ] obtained by the pairing can be more general than the original clause sets [sa , ca ] and [sb , cb ] since s p is contained in both sa and sb (under the injective mapping) and it thus subsumes both s a and sb . [sent-225, score-0.713]
</p><p>20 Hence, the pairing operation can be intuitively viewed as a generalization of both participating clause sets. [sent-226, score-0.414]
</p><p>21 Now when we pair this clause set with another one the antecedent will be a subset of s and surely p will still be wrong. [sent-233, score-0.409]
</p><p>22 The algorithm then tries to ﬁnd a “useful” pairing of [s, c] with one of the clause sets [s i , ci ] in S. [sent-249, score-0.414]
</p><p>23 In case no such pairing is found for any of the [s i , ci ], the minimized clause set [s, c] is added to S as the last element. [sent-254, score-0.414]
</p><p>24 Then the interactive algorithm will stop and produce a hypothesis equivalent to T with O(mpk a kk ) clauses after O(mpka kk ) equivalence queries and O((n + m2 )pka k3k ) membership queries. [sent-288, score-0.46]
</p><p>25 The one-pass procedure: Given a clause set [s, c] the procedure one-pass tests clauses in [s, c] against all positive examples in E. [sent-312, score-0.485]
</p><p>26 First, once we match the antecedent we can test all the consequents simultaneously so it is better to keep clause sets together rather than split them into individual clauses. [sent-323, score-0.513]
</p><p>27 Minimization: The minimization procedure acting on a clause set [s, c] assumes the input clause set has already been validated by one-pass. [sent-327, score-0.598]
</p><p>28 If we can guarantee in addition that all membership queries implicit in one-pass are answered correctly then the batch algorithm can be seen as performing some run of the interactive algorithm and bounds on queries and hypothesis size translate as well. [sent-340, score-0.435]
</p><p>29 As a result any clause C produced by the interactive algorithm has at most k variables which in turn guarantees that we can test whether I |= C in time O(n k ) where I has n domain elements. [sent-380, score-0.428]
</p><p>30 It is therefore crucial for our system to have efﬁcient procedures to test subsumption and enumerate matching. [sent-385, score-0.532]
</p><p>31 In addition, sorting the examples by size helps reduce run time since the rules generated from the small examples have less variables, a property that generally implies faster subsumption tests. [sent-393, score-0.526]
</p><p>32 Aside from these, the most crucial run time issue is the subsumption test which we discuss next. [sent-408, score-0.484]
</p><p>33 Efﬁcient Subsumption Tests The one-pass procedure must enumerate all substitutions that embed a clause in an example. [sent-411, score-0.384]
</p><p>34 To compute all substitutions between an example and a clause the system repeatedly performs joins of these tables (in the database sense) to get a table of all substitutions. [sent-419, score-0.581]
</p><p>35 Then for each predicate in the clause we pull the appropriate table from the example, and perform a join which matches the variables already instantiated in our intermediate table. [sent-421, score-0.443]
</p><p>36 Thus if the predicate in the clause does not introduce new variables the table size cannot grow. [sent-422, score-0.414]
</p><p>37 It may be worth clarifying here that this is the standard subsumption problem and we do not require different variables to be matched to different objects. [sent-439, score-0.458]
</p><p>38 2 Randomized Table Based Subsumption If lookahead is still not sufﬁcient or too slow we can resort to randomized subsumption tests. [sent-451, score-0.521]
</p><p>39 3 Subsumption Based on Constraint Satisfaction Algorithms The idea of using constraint satisfaction algorithms to solve subsumption problems has been investigated in Maloberti and Sebag (2004), where a very effective system D JANGO is developed. [sent-461, score-0.603]
</p><p>40 The D JANGO system was originally designed to ﬁnd a single solution for the subsumption problem but this can be easily extended to give all solutions through backtracking. [sent-462, score-0.532]
</p><p>41 To illustrate how constraint satisfaction can be used consider the following subsumption problem where the clause is Cl and the example is Ex: Cl : p(X0 , X1 ), q(X0 , X2 , X3 ), r(X0 ), Ex : p(a0 , a1 ), p(a1 , a2 ), q(a0 , a2 , a3 ), q(a0 , a1 , a3 ), r(a0 ). [sent-473, score-0.828]
</p><p>42 It does however incur an overhead before starting to solve the subsumption problem and can represent an important part of the execution time in case of easy instances of subsumption problems. [sent-499, score-0.916]
</p><p>43 Despite the different representations, both versions use a similar method to solve the subsumption problem, using arc-consistency and following with depth ﬁrst search with dynamic variable ordering. [sent-503, score-0.458]
</p><p>44 Within L OG A N -H the backtracking approach can beneﬁt when we have many substitutions of which only a few are needed to remove all consequents from a clause set. [sent-512, score-0.488]
</p><p>45 (2003) have previously introduced a table based method for subsumption tests (although our system was developed independently). [sent-514, score-0.56]
</p><p>46 The table-based subsumption method incorporates aspects from both the primal and dual representation of D JANGO. [sent-520, score-0.519]
</p><p>47 Finally, calculating the dual form representation of the subsumption problem incurs some overhead that can be noticeable if the subsumption problems themselves are quickly solved. [sent-524, score-0.944]
</p><p>48 Since in L OG A N -H we have many subsumption tests with the same example or the same clause these can be avoided with additional caching. [sent-525, score-0.757]
</p><p>49 Other systems have developed different methods to reduce the computational cost of subsumption tests. [sent-527, score-0.458]
</p><p>50 The ﬁrst one, initially reported in Khardon (2000), implements the interactive and batch algorithms in the Prolog language but does not include discretization, pruning or special subsumption engines. [sent-541, score-0.676]
</p><p>51 In this section we describe several experiments using the system and its subsumption engines. [sent-543, score-0.532]
</p><p>52 Fourth, the experiments evaluate the performance of different subsumption engines both within the learning system and independently. [sent-548, score-0.532]
</p><p>53 They also demonstrate that different subsumption engines may lead to faster execution in different problems. [sent-555, score-0.458]
</p><p>54 We also compare the subsumption methods in an experiment based on the Phase Transition phenomenon similar to the experiments performed by Maloberti and Sebag (2004). [sent-556, score-0.458]
</p><p>55 These experiments show that D JANGO P RIMAL is very effective and may perform better than the other two approaches if hard subsumption problems around the phase transition region need to be solved. [sent-557, score-0.526]
</p><p>56 The Prolog system required 35 equivalence queries, 455 membership queries and about 8 minutes (running Sicstus Prolog on a Linux platform using a Pentium 2/366MHz processor) to recover the set of clauses exactly. [sent-572, score-0.402]
</p><p>57 The ﬁrst 3 targets have 2 clauses each but vary in the number of atoms in the antecedent and the fourth one has 10 clauses of the larger kind making the problem more challenging. [sent-663, score-0.581]
</p><p>58 Notice that for target I the task is not too hard since it is not unlikely that we get a random example matching the antecedent of a rule exactly (so that discovering the clause is easy) but for the larger targets this is not the case. [sent-672, score-0.409]
</p><p>59 10% 16000 Table 7: Runtime comparison for subsumption tests on KRK-illegal data set from averaging among 10 runs over an independent test set of 10000 examples. [sent-778, score-0.458]
</p><p>60 This domain is also a good case to illustrate the various subsumption tests in our system. [sent-785, score-0.499]
</p><p>61 Note that since we put the position predicate in the antecedent, the consequent is nullary so iterative subsumption tests are likely to be faster. [sent-786, score-0.628]
</p><p>62 80 GHz) for various subsumption settings averaged over 10 independent runs. [sent-789, score-0.458]
</p><p>63 93% Table 9: Runtime comparison for subsumption tests on Mutagenesis data set. [sent-861, score-0.458]
</p><p>64 We have also run experiments comparing run time with the different subsumption engines. [sent-862, score-0.51]
</p><p>65 For this domain, deterministic table-based subsumption was not possible, not even with lookahead and arc-consistency since the table size grew beyond memory capacity of our computer. [sent-863, score-0.518]
</p><p>66 2 Subsumption and Phase Transition The previous experiments have demonstrated that different subsumption engines may lead to faster performance in different domains. [sent-871, score-0.458]
</p><p>67 In this section we further compare the different algorithms but purely on subsumption problems, that is, not in the context of learning. [sent-872, score-0.458]
</p><p>68 Previous work (Giordana and Saitta, 2000) has shown that one can parameterize subsumption problems so that there is a sharp transition between regions where most problems have a solution and regions where most problems do not have a solution. [sent-873, score-0.496]
</p><p>69 This is known as the phase transition phenomenon, and it has been used to evaluate subsumption and learning algorithms (Giordana and Saitta, 2000; Giordana et al. [sent-874, score-0.526]
</p><p>70 All m literals in a generated clause C are built on distinct binary predicate symbols and clause C is connected, that is, all n variables are linked. [sent-878, score-0.794]
</p><p>71 The latter requirement prevents the subsumption problem from being decomposable into simpler problems. [sent-879, score-0.458]
</p><p>72 Each literal in Ex is built on a predicate symbol occurring in C (other literals are irrelevant to the subsumption problem). [sent-881, score-0.695]
</p><p>73 For each pair of values of m, L , 50 clauses and 50 examples are generated, each clause is tested against all examples. [sent-883, score-0.485]
</p><p>74 This region is called Phase Transition, and is particularly important for the subsumption problem, since computationally hard problems are located in this region. [sent-886, score-0.458]
</p><p>75 This is important in the context of a system that dynamically builds clauses and tests subsumption for them. [sent-892, score-0.718]
</p><p>76 Finally, in Maloberti and Sebag (2004), 100 clauses and 100 examples were generated, each clause was tested against one example. [sent-900, score-0.485]
</p><p>77 (2003) and Maloberti and Suzuki (2004) for simple subsumption as well as for enumerating all solutions. [sent-916, score-0.458]
</p><p>78 To summarize, the subsumption experiments suggest that in general the D JANGO methods are more robust than the table based methods and that D JANGO P RIMAL is a promising alternative. [sent-920, score-0.486]
</p><p>79 However, these results must be interpreted with caution, since in these experiments the tables meth576  L EARNING H ORN E XPRESSIONS WITH L OG A N -H  (A)  (B)  Figure 4: (A) Percentage of wrong subsumption tests for randomized tables method on 50×50 pairs (C , Ex), with TH= 1 and R= 1. [sent-921, score-0.638]
</p><p>80 Notice that the table based method with such low valued parameters is not able to discover any subsumption substitution, and hence the error rate corresponds to the satisﬁability rate of the subsumption problem suite. [sent-923, score-0.944]
</p><p>81 The paper also introduced new algorithms for solving the subsumption problem and evaluated their performance. [sent-939, score-0.458]
</p><p>82 The table based methods give competitive performance within L OG A N -H and D JANGO P RIMAL is a promising new approach where hard subsumption problems in the phase transition region are solved. [sent-940, score-0.554]
</p><p>83 As discussed above, bottom up search suffers from two aspects: subsumption tests are more costly than in top down approaches, and overﬁtting may occur in small data sets with large examples. [sent-948, score-0.488]
</p><p>84 This clause is then used as a seed for a small step reﬁnement search that evaluates clauses as usual. [sent-954, score-0.485]
</p><p>85 A related problem occurs when we use randomized subsumption tests. [sent-976, score-0.489]
</p><p>86 Here since the subsumption test is incomplete we may not notice that a rule in the hypothesis is violated by a negative example. [sent-977, score-0.502]
</p><p>87 2 Caching The algorithms described above may produce repeated calls to one-pass with the same antecedent since pairings of one clause set with several others may result in the same clause set. [sent-983, score-0.75]
</p><p>88 If we try to cache a universally quantiﬁed expression then matching it requires a subsumption test which is expensive. [sent-986, score-0.499]
</p><p>89 For both algorithms the system caches interpretations rather than clauses or clause sets (the s part of [s, c]). [sent-988, score-0.608]
</p><p>90 / In fact, for the batch algorithm we only need to cache positive interpretations—if a clause set [s, 0] 579  A RIAS , K HARDON AND M ALOBERTI  was returned by one-pass then s does not imply any of the possible consequents and therefore it is a positive interpretation. [sent-989, score-0.535]
</p><p>91 This is matched with the fact that pairing keeps object names of existing clause sets in the hypothesis. [sent-992, score-0.414]
</p><p>92 Caching can reduce or increase run time of the system, depending on the data set, the cost for subsumption for examples in the data set, and the rate of cache hits. [sent-994, score-0.525]
</p><p>93 Recall that the system starts with an example and essentially turns objects into variables in the maximally speciﬁc clause set. [sent-1036, score-0.416]
</p><p>94 For example, if we discretized the logp attribute from above and variabilize we get logp(X) logp val>00(X) logp val>01(X) logp val<02(X) logp val<03(X). [sent-1040, score-0.737]
</p><p>95 Notice that if the system ever considers the clause BK(b) → b as a candidate, one-pass will ﬁnd the posi/ tive interpretation I and will drop b, as desired. [sent-1108, score-0.403]
</p><p>96 In fact, one-pass will return [BK(b), 0] for any input clause set [BK(b), c] since it will drop all consequents in c that are not included in BK(b) (including false), thus precluding clause BK(b) → b from ever being included in any hypothesis. [sent-1109, score-0.702]
</p><p>97 As an example, suppose that in the normal ILP setting, the clause p(a, b) ∧ p(b, c) → q() is labeled positive and the clause p(a, b) → q() is labeled negative. [sent-1115, score-0.598]
</p><p>98 In the case of zero arity consequents, the check whether a given clause C is satisﬁed by some interpretation I can be considerably simpliﬁed. [sent-1118, score-0.386]
</p><p>99 As a result subsumption procedures that enumerate solutions one by one can quit early, after the ﬁrst substitution, and are likely to be faster in this case. [sent-1120, score-0.458]
</p><p>100 A signature captures the idea that when we map a literal in a clause onto a literal in the example all the neighborhood of the ﬁrst literal must exist in the example as well. [sent-1149, score-0.422]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('subsumption', 0.458), ('jango', 0.336), ('clause', 0.299), ('og', 0.279), ('clauses', 0.186), ('logp', 0.14), ('ilp', 0.129), ('aloberti', 0.116), ('orn', 0.116), ('rias', 0.116), ('xpressions', 0.116), ('pairing', 0.115), ('antecedent', 0.11), ('literals', 0.109), ('consequents', 0.104), ('rimal', 0.104), ('atoms', 0.099), ('hardon', 0.098), ('khardon', 0.098), ('horn', 0.096), ('muggleton', 0.093), ('batch', 0.091), ('interactive', 0.088), ('predicate', 0.087), ('substitutions', 0.085), ('blockeel', 0.083), ('predicates', 0.083), ('consequent', 0.083), ('logic', 0.08), ('csp', 0.078), ('raedt', 0.076), ('queries', 0.074), ('system', 0.074), ('laer', 0.073), ('maloberti', 0.073), ('sebag', 0.067), ('grammar', 0.067), ('ual', 0.067), ('icl', 0.067), ('mode', 0.063), ('giordana', 0.061), ('tables', 0.059), ('arity', 0.057), ('prolog', 0.055), ('interpretations', 0.049), ('discretization', 0.047), ('lnai', 0.047), ('atom', 0.047), ('mutagenesis', 0.047), ('srinivasan', 0.046), ('val', 0.046), ('bk', 0.046), ('background', 0.045), ('inductive', 0.044), ('hypothesis', 0.044), ('np', 0.043), ('objects', 0.043), ('rules', 0.042), ('pairings', 0.042), ('dropping', 0.042), ('satisfaction', 0.041), ('cache', 0.041), ('literal', 0.041), ('domain', 0.041), ('earning', 0.04), ('pruning', 0.039), ('th', 0.039), ('lt', 0.038), ('transition', 0.038), ('membership', 0.038), ('adj', 0.037), ('variabilize', 0.037), ('joins', 0.036), ('relational', 0.034), ('primal', 0.033), ('lookahead', 0.032), ('nement', 0.031), ('vp', 0.031), ('golem', 0.031), ('arias', 0.031), ('si', 0.031), ('wrong', 0.031), ('randomized', 0.031), ('bongard', 0.031), ('programming', 0.03), ('interpretation', 0.03), ('constraint', 0.03), ('phase', 0.03), ('equivalence', 0.03), ('bottom', 0.03), ('join', 0.029), ('king', 0.029), ('substitution', 0.028), ('dual', 0.028), ('table', 0.028), ('bins', 0.028), ('logical', 0.028), ('sentences', 0.027), ('run', 0.026), ('ex', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="47-tfidf-1" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>Author: Marta Arias, Roni Khardon, Jérôme Maloberti</p><p>Abstract: The paper introduces L OG A N -H —a system for learning ﬁrst-order function-free Horn expressions from interpretations. The system is based on an algorithm that learns by asking questions and that was proved correct in previous work. The current paper shows how the algorithm can be implemented in a practical system, and introduces a new algorithm based on it that avoids interaction and learns from examples only. The L OG A N -H system implements these algorithms and adds several facilities and optimizations that allow efﬁcient applications in a wide range of problems. As one of the important ingredients, the system includes several fast procedures for solving the subsumption problem, an NP-complete problem that needs to be solved many times during the learning process. We describe qualitative and quantitative experiments in several domains. The experiments demonstrate that the system can deal with varied problems, large amounts of data, and that it achieves good classiﬁcation accuracy. Keywords: inductive logic programming, subsumption, bottom-up learning, learning with queries</p><p>2 0.16446698 <a title="47-tfidf-2" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>Author: Niels Landwehr, Kristian Kersting, Luc De Raedt</p><p>Abstract: A novel relational learning approach that tightly integrates the na¨ve Bayes learning scheme with ı the inductive logic programming rule-learner FOIL is presented. In contrast to previous combinations that have employed na¨ve Bayes only for post-processing the rule sets, the presented approach ı employs the na¨ve Bayes criterion to guide its search directly. The proposed technique is impleı mented in the N FOIL and T FOIL systems, which employ standard na¨ve Bayes and tree augmented ı na¨ve Bayes models respectively. We show that these integrated approaches to probabilistic model ı and rule learning outperform post-processing approaches. They also yield signiﬁcantly more accurate models than simple rule learning and are competitive with more sophisticated ILP systems. Keywords: rule learning, na¨ve Bayes, statistical relational learning, inductive logic programming ı</p><p>3 0.061987944 <a title="47-tfidf-3" href="./jmlr-2007-Polynomial_Identification_in_the_Limit_of_Substitutable_Context-free_Languages.html">67 jmlr-2007-Polynomial Identification in the Limit of Substitutable Context-free Languages</a></p>
<p>Author: Alexander Clark, Rémi Eyraud</p><p>Abstract: This paper formalises the idea of substitutability introduced by Zellig Harris in the 1950s and makes it the basis for a learning algorithm from positive data only for a subclass of context-free languages. We show that there is a polynomial characteristic set, and thus prove polynomial identiﬁcation in the limit of this class. We discuss the relationship of this class of languages to other common classes discussed in grammatical inference. It transpires that it is not necessary to identify constituents in order to learn a context-free language—it is sufﬁcient to identify the syntactic congruence, and the operations of the syntactic monoid can be converted into a context-free grammar. We also discuss modiﬁcations to the algorithm that produces a reduction system rather than a context-free grammar, that will be much more compact. We discuss the relationship to Angluin’s notion of reversibility for regular languages. We also demonstrate that an implementation of this algorithm is capable of learning a classic example of structure dependent syntax in English: this constitutes a refutation of an argument that has been used in support of nativist theories of language. Keywords: grammatical inference, context-free languages, positive data only, reduction system, natural languages</p><p>4 0.040248182 <a title="47-tfidf-4" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Vitaly Feldman</p><p>Abstract: We consider the problems of attribute-efﬁcient PAC learning of two well-studied concept classes: parity functions and DNF expressions over {0, 1}n . We show that attribute-efﬁcient learning of parities with respect to the uniform distribution is equivalent to decoding high-rate random linear codes from low number of errors, a long-standing open problem in coding theory. This is the ﬁrst evidence that attribute-efﬁcient learning of a natural PAC learnable concept class can be computationally hard. An algorithm is said to use membership queries (MQs) non-adaptively if the points at which the algorithm asks MQs do not depend on the target concept. Using a simple non-adaptive parity learning algorithm and a modiﬁcation of Levin’s algorithm for locating a weakly-correlated parity due to Bshouty et al. (1999), we give the ﬁrst non-adaptive and attribute-efﬁcient algorithm for ˜ learning DNF with respect to the uniform distribution. Our algorithm runs in time O(ns4 /ε) and ˜ uses O(s4 · log2 n/ε) non-adaptive MQs, where s is the number of terms in the shortest DNF representation of the target concept. The algorithm improves on the best previous algorithm for learning DNF (of Bshouty et al., 1999) and can also be easily modiﬁed to tolerate random persistent classiﬁcation noise in MQs. Keywords: attribute-efﬁcient, non-adaptive, membership query, DNF, parity function, random linear code</p><p>5 0.038550209 <a title="47-tfidf-5" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>Author: Sofus A. Macskassy, Foster Provost</p><p>Abstract: paper1 This is about classifying entities that are interlinked with entities for which the class is known. After surveying prior work, we present NetKit, a modular toolkit for classiﬁcation in networked data, and a case-study of its application to networked data used in prior machine learning research. NetKit is based on a node-centric framework in which classiﬁers comprise a local classiﬁer, a relational classiﬁer, and a collective inference procedure. Various existing node-centric relational learning algorithms can be instantiated with appropriate choices for these components, and new combinations of components realize new algorithms. The case study focuses on univariate network classiﬁcation, for which the only information used is the structure of class linkage in the network (i.e., only links and some class labels). To our knowledge, no work previously has evaluated systematically the power of class-linkage alone for classiﬁcation in machine learning benchmark data sets. The results demonstrate that very simple network-classiﬁcation models perform quite well—well enough that they should be used regularly as baseline classiﬁers for studies of learning with networked data. The simplest method (which performs remarkably well) highlights the close correspondence between several existing methods introduced for different purposes—that is, Gaussian-ﬁeld classiﬁers, Hopﬁeld networks, and relational-neighbor classiﬁers. The case study also shows that there are two sets of techniques that are preferable in different situations, namely when few versus many labels are known initially. We also demonstrate that link selection plays an important role similar to traditional feature selection. Keywords: relational learning, network learning, collective inference, collective classiﬁcation, networked data, probabilistic relational models, network analysis, network data</p><p>6 0.037065282 <a title="47-tfidf-6" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>7 0.036677092 <a title="47-tfidf-7" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>8 0.036356095 <a title="47-tfidf-8" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>9 0.034595061 <a title="47-tfidf-9" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>10 0.027278729 <a title="47-tfidf-10" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>11 0.026710266 <a title="47-tfidf-11" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>12 0.026594412 <a title="47-tfidf-12" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>13 0.025265444 <a title="47-tfidf-13" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>14 0.024995206 <a title="47-tfidf-14" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>15 0.024430458 <a title="47-tfidf-15" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>16 0.024403879 <a title="47-tfidf-16" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>17 0.024175938 <a title="47-tfidf-17" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>18 0.023699293 <a title="47-tfidf-18" href="./jmlr-2007-Distances_between_Data_Sets_Based_on_Summary_Statistics.html">27 jmlr-2007-Distances between Data Sets Based on Summary Statistics</a></p>
<p>19 0.022527907 <a title="47-tfidf-19" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>20 0.022188798 <a title="47-tfidf-20" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.149), (1, 0.108), (2, -0.054), (3, 0.013), (4, -0.017), (5, 0.119), (6, 0.036), (7, -0.02), (8, 0.005), (9, -0.217), (10, 0.038), (11, 0.241), (12, 0.096), (13, 0.136), (14, 0.03), (15, 0.25), (16, -0.064), (17, -0.464), (18, 0.217), (19, 0.1), (20, 0.073), (21, 0.055), (22, -0.083), (23, 0.013), (24, 0.04), (25, 0.006), (26, -0.005), (27, -0.085), (28, 0.096), (29, -0.02), (30, -0.032), (31, -0.006), (32, -0.019), (33, -0.022), (34, 0.023), (35, -0.032), (36, -0.057), (37, -0.018), (38, -0.041), (39, 0.047), (40, -0.016), (41, -0.103), (42, 0.037), (43, -0.041), (44, -0.03), (45, 0.033), (46, 0.014), (47, -0.029), (48, 0.009), (49, 0.073)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95114696 <a title="47-lsi-1" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>Author: Marta Arias, Roni Khardon, Jérôme Maloberti</p><p>Abstract: The paper introduces L OG A N -H —a system for learning ﬁrst-order function-free Horn expressions from interpretations. The system is based on an algorithm that learns by asking questions and that was proved correct in previous work. The current paper shows how the algorithm can be implemented in a practical system, and introduces a new algorithm based on it that avoids interaction and learns from examples only. The L OG A N -H system implements these algorithms and adds several facilities and optimizations that allow efﬁcient applications in a wide range of problems. As one of the important ingredients, the system includes several fast procedures for solving the subsumption problem, an NP-complete problem that needs to be solved many times during the learning process. We describe qualitative and quantitative experiments in several domains. The experiments demonstrate that the system can deal with varied problems, large amounts of data, and that it achieves good classiﬁcation accuracy. Keywords: inductive logic programming, subsumption, bottom-up learning, learning with queries</p><p>2 0.86173308 <a title="47-lsi-2" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>Author: Niels Landwehr, Kristian Kersting, Luc De Raedt</p><p>Abstract: A novel relational learning approach that tightly integrates the na¨ve Bayes learning scheme with ı the inductive logic programming rule-learner FOIL is presented. In contrast to previous combinations that have employed na¨ve Bayes only for post-processing the rule sets, the presented approach ı employs the na¨ve Bayes criterion to guide its search directly. The proposed technique is impleı mented in the N FOIL and T FOIL systems, which employ standard na¨ve Bayes and tree augmented ı na¨ve Bayes models respectively. We show that these integrated approaches to probabilistic model ı and rule learning outperform post-processing approaches. They also yield signiﬁcantly more accurate models than simple rule learning and are competitive with more sophisticated ILP systems. Keywords: rule learning, na¨ve Bayes, statistical relational learning, inductive logic programming ı</p><p>3 0.27496275 <a title="47-lsi-3" href="./jmlr-2007-Polynomial_Identification_in_the_Limit_of_Substitutable_Context-free_Languages.html">67 jmlr-2007-Polynomial Identification in the Limit of Substitutable Context-free Languages</a></p>
<p>Author: Alexander Clark, Rémi Eyraud</p><p>Abstract: This paper formalises the idea of substitutability introduced by Zellig Harris in the 1950s and makes it the basis for a learning algorithm from positive data only for a subclass of context-free languages. We show that there is a polynomial characteristic set, and thus prove polynomial identiﬁcation in the limit of this class. We discuss the relationship of this class of languages to other common classes discussed in grammatical inference. It transpires that it is not necessary to identify constituents in order to learn a context-free language—it is sufﬁcient to identify the syntactic congruence, and the operations of the syntactic monoid can be converted into a context-free grammar. We also discuss modiﬁcations to the algorithm that produces a reduction system rather than a context-free grammar, that will be much more compact. We discuss the relationship to Angluin’s notion of reversibility for regular languages. We also demonstrate that an implementation of this algorithm is capable of learning a classic example of structure dependent syntax in English: this constitutes a refutation of an argument that has been used in support of nativist theories of language. Keywords: grammatical inference, context-free languages, positive data only, reduction system, natural languages</p><p>4 0.17784274 <a title="47-lsi-4" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>Author: Roni Khardon, Gabriel Wachman</p><p>Abstract: A large number of variants of the Perceptron algorithm have been proposed and partially evaluated in recent work. One type of algorithm aims for noise tolerance by replacing the last hypothesis of the perceptron with another hypothesis or a vote among hypotheses. Another type simply adds a margin term to the perceptron in order to increase robustness and accuracy, as done in support vector machines. A third type borrows further from support vector machines and constrains the update function of the perceptron in ways that mimic soft-margin techniques. The performance of these algorithms, and the potential for combining different techniques, has not been studied in depth. This paper provides such an experimental study and reveals some interesting facts about the algorithms. In particular the perceptron with margin is an effective method for tolerating noise and stabilizing the algorithm. This is surprising since the margin in itself is not designed or used for noise tolerance, and there are no known guarantees for such performance. In most cases, similar performance is obtained by the voted-perceptron which has the advantage that it does not require parameter selection. Techniques using soft margin ideas are run-time intensive and do not give additional performance beneﬁts. The results also highlight the difﬁculty with automatic parameter selection which is required with some of these variants. Keywords: perceptron algorithm, on-line learning, noise tolerance, kernel methods</p><p>5 0.1575751 <a title="47-lsi-5" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>Author: Manu Chhabra, Robert A. Jacobs, Daniel Štefankovič</p><p>Abstract: In a search problem, an agent uses the membership oracle of a target concept to ﬁnd a positive example of the concept. In a shaped search problem the agent is aided by a sequence of increasingly restrictive concepts leading to the target concept (analogous to behavioral shaping). The concepts are given by membership oracles, and the agent has to ﬁnd a positive example of the target concept while minimizing the total number of oracle queries. We show that for the concept class of intervals on the real line an agent using a bounded number of queries per oracle exists. In contrast, for the concept class of unions of two intervals on the real line no agent with a bounded number of queries per oracle exists. We then investigate the (amortized) number of queries per oracle needed for the shaped search problem over other concept classes. We explore the following methods to obtain efﬁcient agents. For axis-parallel rectangles we use a bootstrapping technique to obtain gradually better approximations of the target concept. We show that given rectangles R ⊆ A ⊆ R d one can obtain a rectangle A ⊇ R with vol(A )/vol(R) ≤ 2, using only O(d · vol(A)/vol(R)) random samples from A. For ellipsoids of bounded eccentricity in Rd we analyze a deterministic ray-shooting process which uses a sequence of rays to get close to the centroid. Finally, we use algorithms for generating random points in convex bodies (Dyer et al., 1991; Kannan et al., 1997) to give a randomized agent for the concept class of convex bodies. In the ﬁnal section, we explore connections between our bootstrapping method and active learning. Speciﬁcally, we use the bootstrapping technique for axis-parallel rectangles to active learn axis-parallel rectangles under the uniform distribution in O(d ln(1/ε)) labeled samples. Keywords: computational learning theory, behavioral shaping, active learning</p><p>6 0.15335201 <a title="47-lsi-6" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>7 0.15054959 <a title="47-lsi-7" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>8 0.12736735 <a title="47-lsi-8" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>9 0.11763563 <a title="47-lsi-9" href="./jmlr-2007-Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning.html">85 jmlr-2007-Transfer Learning via Inter-Task Mappings for Temporal Difference Learning</a></p>
<p>10 0.11618993 <a title="47-lsi-10" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>11 0.11549243 <a title="47-lsi-11" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>12 0.11534671 <a title="47-lsi-12" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>13 0.11192155 <a title="47-lsi-13" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>14 0.11123752 <a title="47-lsi-14" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>15 0.10678813 <a title="47-lsi-15" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>16 0.10561039 <a title="47-lsi-16" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>17 0.10371687 <a title="47-lsi-17" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>18 0.10260554 <a title="47-lsi-18" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>19 0.102062 <a title="47-lsi-19" href="./jmlr-2007-Dynamics_and_Generalization_Ability_of_LVQ_Algorithms.html">30 jmlr-2007-Dynamics and Generalization Ability of LVQ Algorithms</a></p>
<p>20 0.098985158 <a title="47-lsi-20" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.019), (12, 0.02), (15, 0.02), (22, 0.013), (28, 0.519), (40, 0.029), (45, 0.019), (48, 0.027), (58, 0.013), (60, 0.069), (77, 0.011), (80, 0.031), (85, 0.032), (98, 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.94944745 <a title="47-lda-1" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>Author: David Mease, Abraham J. Wyner, Andreas Buja</p><p>Abstract: The standard by which binary classiﬁers are usually judged, misclassiﬁcation error, assumes equal costs of misclassifying the two classes or, equivalently, classifying at the 1/2 quantile of the conditional class probability function P[y = 1|x]. Boosted classiﬁcation trees are known to perform quite well for such problems. In this article we consider the use of standard, off-the-shelf boosting for two more general problems: 1) classiﬁcation with unequal costs or, equivalently, classiﬁcation at quantiles other than 1/2, and 2) estimation of the conditional class probability function P[y = 1|x]. We ﬁrst examine whether the latter problem, estimation of P[y = 1|x], can be solved with LogitBoost, and with AdaBoost when combined with a natural link function. The answer is negative: both approaches are often ineffective because they overﬁt P[y = 1|x] even though they perform well as classiﬁers. A major negative point of the present article is the disconnect between class probability estimation and classiﬁcation. Next we consider the practice of over/under-sampling of the two classes. We present an algorithm that uses AdaBoost in conjunction with Over/Under-Sampling and Jittering of the data (“JOUS-Boost”). This algorithm is simple, yet successful, and it preserves the advantage of relative protection against overﬁtting, but for arbitrary misclassiﬁcation costs and, equivalently, arbitrary quantile boundaries. We then use collections of classiﬁers obtained from a grid of quantiles to form estimators of class probabilities. The estimates of the class probabilities compare favorably to those obtained by a variety of methods across both simulated and real data sets. Keywords: boosting algorithms, LogitBoost, AdaBoost, class probability estimation, over-sampling, under-sampling, stratiﬁcation, data jittering</p><p>2 0.93001717 <a title="47-lda-2" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>Author: Yiming Ying, Ding-Xuan Zhou</p><p>Abstract: Gaussian kernels with ﬂexible variances provide a rich family of Mercer kernels for learning algorithms. We show that the union of the unit balls of reproducing kernel Hilbert spaces generated by Gaussian kernels with ﬂexible variances is a uniform Glivenko-Cantelli (uGC) class. This result conﬁrms a conjecture concerning learnability of Gaussian kernels and veriﬁes the uniform convergence of many learning algorithms involving Gaussians with changing variances. Rademacher averages and empirical covering numbers are used to estimate sample errors of multi-kernel regularization schemes associated with general loss functions. It is then shown that the regularization error associated with the least square loss and the Gaussian kernels can be greatly improved when ﬂexible variances are allowed. Finally, for regularization schemes generated by Gaussian kernels with ﬂexible variances we present explicit learning rates for regression with least square loss and classiﬁcation with hinge loss. Keywords: Gaussian kernel, ﬂexible variances, learning theory, Glivenko-Cantelli class, regularization scheme, empirical covering number</p><p>same-paper 3 0.87814134 <a title="47-lda-3" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>Author: Marta Arias, Roni Khardon, Jérôme Maloberti</p><p>Abstract: The paper introduces L OG A N -H —a system for learning ﬁrst-order function-free Horn expressions from interpretations. The system is based on an algorithm that learns by asking questions and that was proved correct in previous work. The current paper shows how the algorithm can be implemented in a practical system, and introduces a new algorithm based on it that avoids interaction and learns from examples only. The L OG A N -H system implements these algorithms and adds several facilities and optimizations that allow efﬁcient applications in a wide range of problems. As one of the important ingredients, the system includes several fast procedures for solving the subsumption problem, an NP-complete problem that needs to be solved many times during the learning process. We describe qualitative and quantitative experiments in several domains. The experiments demonstrate that the system can deal with varied problems, large amounts of data, and that it achieves good classiﬁcation accuracy. Keywords: inductive logic programming, subsumption, bottom-up learning, learning with queries</p><p>4 0.61565912 <a title="47-lda-4" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: Motivated by mathematical learning from training data, we introduce the notion of reﬁnable kernels. Various characterizations of reﬁnable kernels are presented. The concept of reﬁnable kernels leads to the introduction of wavelet-like reproducing kernels. We also investigate a reﬁnable kernel that forms a Riesz basis. In particular, we characterize reﬁnable translation invariant kernels, and reﬁnable kernels deﬁned by reﬁnable functions. This study leads to multiresolution analysis of reproducing kernel Hilbert spaces. Keywords: reﬁnable kernels, reﬁnable feature maps, wavelet-like reproducing kernels, dual kernels, learning with kernels, reproducing kernel Hilbert spaces, Riesz bases</p><p>5 0.60854203 <a title="47-lda-5" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>Author: Francesco Dinuzzo, Marta Neve, Giuseppe De Nicolao, Ugo Pietro Gianazza</p><p>Abstract: Support Vector Regression (SVR) for discrete data is considered. An alternative formulation of the representer theorem is derived. This result is based on the newly introduced notion of pseudoresidual and the use of subdifferential calculus. The representer theorem is exploited to analyze the sensitivity properties of ε-insensitive SVR and introduce the notion of approximate degrees of freedom. The degrees of freedom are shown to play a key role in the evaluation of the optimism, that is the difference between the expected in-sample error and the expected empirical risk. In this way, it is possible to deﬁne a C p -like statistic that can be used for tuning the parameters of SVR. The proposed tuning procedure is tested on a simulated benchmark problem and on a real world problem (Boston Housing data set). Keywords: statistical learning, reproducing kernel Hilbert spaces, support vector machines, representer theorem, regularization theory</p><p>6 0.57677615 <a title="47-lda-6" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>7 0.52840358 <a title="47-lda-7" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>8 0.52704567 <a title="47-lda-8" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>9 0.52535403 <a title="47-lda-9" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>10 0.51979387 <a title="47-lda-10" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>11 0.50668061 <a title="47-lda-11" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>12 0.50214756 <a title="47-lda-12" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>13 0.49701059 <a title="47-lda-13" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>14 0.49517611 <a title="47-lda-14" href="./jmlr-2007-Large_Margin_Semi-supervised_Learning.html">44 jmlr-2007-Large Margin Semi-supervised Learning</a></p>
<p>15 0.49218726 <a title="47-lda-15" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>16 0.49212801 <a title="47-lda-16" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>17 0.4863686 <a title="47-lda-17" href="./jmlr-2007-Stagewise_Lasso.html">77 jmlr-2007-Stagewise Lasso</a></p>
<p>18 0.48513013 <a title="47-lda-18" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>19 0.48160979 <a title="47-lda-19" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>20 0.47464055 <a title="47-lda-20" href="./jmlr-2007-Bayesian_Quadratic_Discriminant_Analysis.html">13 jmlr-2007-Bayesian Quadratic Discriminant Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
