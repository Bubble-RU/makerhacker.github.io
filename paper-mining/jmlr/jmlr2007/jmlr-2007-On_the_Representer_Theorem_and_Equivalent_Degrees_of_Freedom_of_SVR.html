<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-63" href="#">jmlr2007-63</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</h1>
<br/><p>Source: <a title="jmlr-2007-63-pdf" href="http://jmlr.org/papers/volume8/dinuzzo07a/dinuzzo07a.pdf">pdf</a></p><p>Author: Francesco Dinuzzo, Marta Neve, Giuseppe De Nicolao, Ugo Pietro Gianazza</p><p>Abstract: Support Vector Regression (SVR) for discrete data is considered. An alternative formulation of the representer theorem is derived. This result is based on the newly introduced notion of pseudoresidual and the use of subdifferential calculus. The representer theorem is exploited to analyze the sensitivity properties of ε-insensitive SVR and introduce the notion of approximate degrees of freedom. The degrees of freedom are shown to play a key role in the evaluation of the optimism, that is the difference between the expected in-sample error and the expected empirical risk. In this way, it is possible to deﬁne a C p -like statistic that can be used for tuning the parameters of SVR. The proposed tuning procedure is tested on a simulated benchmark problem and on a real world problem (Boston Housing data set). Keywords: statistical learning, reproducing kernel Hilbert spaces, support vector machines, representer theorem, regularization theory</p><p>Reference: <a title="jmlr-2007-63-reference" href="../jmlr2007_reference/jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 An alternative formulation of the representer theorem is derived. [sent-10, score-0.14]
</p><p>2 This result is based on the newly introduced notion of pseudoresidual and the use of subdifferential calculus. [sent-11, score-0.141]
</p><p>3 The representer theorem is exploited to analyze the sensitivity properties of ε-insensitive SVR and introduce the notion of approximate degrees of freedom. [sent-12, score-0.258]
</p><p>4 In this way, it is possible to deﬁne a C p -like statistic that can be used for tuning the parameters of SVR. [sent-14, score-0.207]
</p><p>5 The proposed tuning procedure is tested on a simulated benchmark problem and on a real world problem (Boston Housing data set). [sent-15, score-0.153]
</p><p>6 Keywords: statistical learning, reproducing kernel Hilbert spaces, support vector machines, representer theorem, regularization theory  1. [sent-16, score-0.168]
</p><p>7 D INUZZO , N EVE , G IANAZZA AND D E N ICOLAO  The representer theorem was further generalized to differentiable loss functions (Cox and O’ ¨ Sullivan, 1990; Poggio and Girosi, 1992) and even arbitrary monotonic ones (Sch olkopf et al. [sent-25, score-0.191]
</p><p>8 More precisely, Steinwart (2003) has proven a quantitative representer theorem that, without using the dual problem, characterizes the coefﬁcients by means of inclusions, when convex loss functions are considered. [sent-33, score-0.19]
</p><p>9 In particular, besides providing an alternative simpler proof of the quantitative representer theorem, De Vito and coworkers allow for the offset space and cover both regression and classiﬁcation. [sent-36, score-0.16]
</p><p>10 Then, these results are specialized to SVR in order to study its sensitivity to data and develop a tuning method for its parameters. [sent-39, score-0.177]
</p><p>11 Concerning the quantitative representation of the coefﬁcients a i , the paper provides a simple derivation of the quantitative representer theorem based on Fourier arguments (see Appendix A). [sent-40, score-0.24]
</p><p>12 Another result is a new formulation of the quantitative representer theorem that replaces inclusions with equations by using the newly introduced notion of pseudoresidual (Theorem 1). [sent-41, score-0.31]
</p><p>13 As a byproduct of the sensitivity analysis, the degrees of freedom of SVR, deﬁned as the trace of the sensitivity matrix, are found to be equal to the number of marginal support vectors. [sent-46, score-0.267]
</p><p>14 The interpretation of SVR as a Bayesian estimator provides a conceptually elegant framework for reformulating parameter tuning as a statistical estimation problem (Gao et al. [sent-49, score-0.175]
</p><p>15 As a matter of fact, the great majority of tuning approaches aims at the minimization of the prediction error. [sent-52, score-0.143]
</p><p>16 Herein, it is shown how the optimism, that is the difference between the insample prediction error and the expected empirical risk, depends on the sensitivity of the estimator (Theorem 3). [sent-64, score-0.144]
</p><p>17 After some preliminaries (Section 2), the major results regarding the representation of the coefﬁcients ai and the sensitivity analysis of SVR are derived in Section 3, which ends with the deﬁnition of the degrees of freedom. [sent-69, score-0.227]
</p><p>18 The issue of parameter tuning is treated in Section 4, where the estimation of the in-sample prediction error by means of a suitable C p statistic is addressed. [sent-70, score-0.233]
</p><p>19 Finally, the proposed parameter tuning procedure is illustrated in Section 5 by means of both a simulated problem and a real-world one. [sent-71, score-0.153]
</p><p>20 Among the H possible convex loss functions V (quadratic, Laplace, etc) particular attention will be given to the ε-insensitive one: V (yi , f (xi )) = Vε (yi − f (xi )) =  0, | f (xi ) − yi | ≤ ε | f (xi ) − yi | − ε, | f (xi ) − yi | > ε. [sent-80, score-0.387]
</p><p>21 If the kernel is positive deﬁnite, the representer theorem states that the solution fˆ can be written as 2469  D INUZZO , N EVE , G IANAZZA AND D E N ICOLAO  fˆ(x) = ∑ ai K(xi , x),  (3)  i=1  where ai are suitable coefﬁcients. [sent-84, score-0.38]
</p><p>22 If V is everywhere differentiable with respect to its second argument, it can be shown that ai = −C∂2V (yi , fˆ(xi )),  where ∂2 denotes the partial derivative with respect to the second argument. [sent-85, score-0.16]
</p><p>23 (2004) (Theorem 2) have shown that ai ∈ −C∂2V (yi , fˆ(xi )),  (4)  where, now, ∂2 is the subdifferential with respect to the second argument. [sent-90, score-0.178]
</p><p>24 This result goes under the name of quantitative representer theorem. [sent-91, score-0.16]
</p><p>25 In Appendix A, we provide an alternative concise proof of the quantitative representer theorem based on Fourier arguments. [sent-95, score-0.19]
</p><p>26 Quantitative Representation and Sensitivity Analysis Hereafter, it is assumed that the loss function is of the type V (yi , f (xi )) = V ( f (xi ) − yi ),  where V (·) is a convex function and is twice differentiable everywhere except in a ﬁnite number of points γ j , j = 1, . [sent-99, score-0.18]
</p><p>27 , }, deﬁne the pseudoresiduals as D+ (γ) = lim+  ηi := yi − ∑ a j K(xi , x j ). [sent-106, score-0.153]
</p><p>28 , , that characterize the solution of problem (1), satisfy a system of algebraic equations ai = Si (ηi ), where Si (ηi ) are monotone nondecreasing Lipschitz continuous functions. [sent-110, score-0.173]
</p><p>29 Moreover, when ηi ∈ − γ j +CK(xi , xi )D+ (γ j ) , − γ j +CK(xi , xi )D− (γ j ) ,  j = 1, . [sent-111, score-0.24]
</p><p>30 By the deﬁnition of pseudoresidual, fˆ(xi ) − yi = ai K(xi , xi ) − ηi . [sent-116, score-0.358]
</p><p>31 K(xi , xi )  Now, there are two cases depending on whether V (·) is twice differentiable at γ := fˆ(xi ) − yi or not. [sent-118, score-0.3]
</p><p>32 , N, V (γ) is twice differentiable and its subdifferential is single-valued so that (4) yields ai = −CV ( fˆ(xi ) − yi ) = −CV (ai K(xi , xi ) − ηi ). [sent-122, score-0.478]
</p><p>33 ∂ηi 1 +CK(xi , xi )V (ai K(xi , xi ) − ηi )  The denominator is always different from zero because, by convexity, V ≥ 0 whenever it exists. [sent-125, score-0.24]
</p><p>34 Therefore, locally, ai is a differentiable function of ηi : ai = S(ηi ). [sent-126, score-0.269]
</p><p>35 Then, from (6), ai = Si (ηi ) = so that ai is an afﬁne function of ηi in the interval 2471  γ j + ηi , K(xi , xi )  (9)  D INUZZO , N EVE , G IANAZZA AND D E N ICOLAO  I j := [ηL , ηR ], j j where  ηL := − γ j +CK(xi , xi )D+ (γ j ) , j  ηR := − γ j +CK(xi , xi )D− (γ j ) . [sent-130, score-0.578]
</p><p>36 j  On the other hand, recalling the properties of the subdifferential of a convex function, ai ∈ −CD+ (γ j ), −CD− (γ j ) . [sent-131, score-0.178]
</p><p>37 Finally, since ∂ai 1 = > 0, ∂ηi K(xi , xi ) the functions Si (ηi ) are locally monotone nondecreasing also in the second case. [sent-133, score-0.184]
</p><p>38 K(xi , xi )  Now, observe that, if ai tends to −CD+ (γ j ) from below, then ηi tends to ηL from the left. [sent-141, score-0.229]
</p><p>39 Indeed, j taking the limit in (7) for ai → −CD+ (γ j ) from below, we obtain that fˆ(xi ) − yi → γ j from the right (recall that V (·) is nondecreasing). [sent-142, score-0.238]
</p><p>40 In turn,  lim  −  ai →(−CD+ (γ j ))  ηi =  lim  −  ai →(−CD+ (γ j ))  yi − fˆ(xi ) + ai K(xi , xi )  = −γ j −CD+ (γ j )K(xi , xi ) = ηL j  from the left. [sent-143, score-0.758]
</p><p>41 For the subsequent derivation it is useful to deﬁne the following sets: Iin = {i ∈ I : | fˆ(xi ) − yi | < ε}, I + = {i ∈ I : fˆ(xi ) − yi > ε}, C  − IC = {i ∈ I : fˆ(xi ) − yi < −ε}, I + = {i ∈ I : fˆ(xi ) − yi = ε}, M  − IM = {i ∈ I : fˆ(xi ) − yi = −ε},  Iout  − + = IC ∪ IC  − + IM = IM ∪ IM . [sent-148, score-0.645]
</p><p>42 Corollary 1 For the ε-insensitive loss function,   −C  η +ε ηi ≤ −(ε +CK(xi , xi )),   i  K(x ,x ) −(ε +CK(xi , xi )) < ηi < −ε,  i i 0 −ε ≤ ηi ≤ ε, ai = Si (ηi ) =  ηi −ε    K(xi ,xi ) ε < ηi < (ε +CK(xi , xi )),   C ηi ≥ (ε +CK(xi , xi )). [sent-153, score-0.589]
</p><p>43 For linear-in-parameter regression it is usual to deﬁne the degrees of freedom of the estimator as the trace of the so-called “hat matrix,” that maps the vector of output data into the corresponding predictions. [sent-189, score-0.176]
</p><p>44 , y )T , where yi = fˆ(xi ) denotes the SVR prediction at xi . [sent-195, score-0.275]
</p><p>45 More precisely, the i-th observation yi is a marginal vector if ε ≤ |ηi | ≤ ε +CK(xi , xi ). [sent-203, score-0.278]
</p><p>46 For such “transition data” yi , the right and left derivatives ∂yi /∂yi are different, ˆ so that the degrees of freedom would not be uniquely deﬁned. [sent-206, score-0.247]
</p><p>47 In order to proceed, it is assumed that the training data are given by yi = f 0 (xi ) + vi ,  (15)  where f 0 (x) is the “true function” to be estimated and the measurement error vector v = [v 1 . [sent-214, score-0.334]
</p><p>48 Note that f 0 (xi ) can be seen as the conditional 1 expectation of yi given xi ( f 0 (xi ) = E[yi |xi ]) and f 0 (x) is also known as regression function. [sent-221, score-0.249]
</p><p>49 A ﬁrst type of error is the empirical risk err :=  1  y − y 2. [sent-223, score-0.151]
</p><p>50 As an alternative, one can look for probabilistic upper bounds on the expected risk which may be, in some cases, too loose for an optimal tuning of the SVR parameters. [sent-227, score-0.145]
</p><p>51 (2001), is deﬁned as Errin := E  1  ynew − y ˆ  2  ,  where the xi are ﬁxed and the expected value is taken with respect to both the distribution of y i and ynew . [sent-230, score-0.216]
</p><p>52 The index Errin can be approximated by Errin = err + op, where op is an estimate of op. [sent-245, score-0.246]
</p><p>53 If y is a linear function of y, and σ2 = σ2 , ∀i, then the optimism can ˆ i be expressed as op =  2qσ2  (16)  (note that in the linear case the degrees of freedom q do not depend on the training data y). [sent-246, score-0.445]
</p><p>54 In this linear case, Errin is better known as C p statistic 2477  D INUZZO , N EVE , G IANAZZA AND D E N ICOLAO  C p = err +  2qσ2  . [sent-247, score-0.213]
</p><p>55 The following theorem highlights the relationship between the optimism and the sensitivities ∂h i /∂yi of a generic estimator y = h(y). [sent-249, score-0.292]
</p><p>56 (15) holds, (ii) the errors vi are independent of each other, (iii) the variances σ2 = Var[vi ] are ﬁnite, i (iv) the estimator h(y) is such that for i = 1, . [sent-255, score-0.263]
</p><p>57 |yi |  Then, op =  2  ∑ σ2 i  i=1  Z  R  ∂hi (y) ∏ p j (v j )φi (vi )dv, ∂yi j=i  where φi (vi ) =  1 σ2 i  Z +∞ vi  spi (s)ds,  and pi (vi ) denotes the probability density function of vi . [sent-259, score-0.628]
</p><p>58 Moreover, if the errors vi are Gaussian, op =  2  ∂hi (y) . [sent-260, score-0.328]
</p><p>59 By deﬁnition,  op =  2  E[yT v] = ˆ  2  ∑ E[hi (y)vi ]  i=1  =  2  ∑  Z  i=1 R  −1  ∏ p j (v j ) j=i  Z  R  hi (y0 + v)vi pi (vi )dvi dv[−i] ,  where dv[−i] = ∏ dv j . [sent-262, score-0.314]
</p><p>60 j=i  2478  (18)  R EPRESENTER T HEOREM AND D EGREES OF F REEDOM OF SVR  Integration by parts of the inner integral yields Z  σ2 i  0  R  hi (y + v)vi pi (vi )dvi =  ∂hi 0 (y + v)φi (vi )dvi − [hi (y0 + v)φi (vi )]|+∞ . [sent-263, score-0.148]
</p><p>61 In fact, for positive v i we have Z +∞  1 φi (vi ) = 2 σi  vi  1 spi (s)ds = 2 σi  Z +∞ 2 s pi (s)  s  vi  R +∞  1 ds ≤ 2 σi v i  Z +∞ vi  s2 pi (s)ds =  1 . [sent-265, score-0.79]
</p><p>62 vi  R vi  For negative vi , observing that vi spi (s)ds = − −∞ spi (s)ds (recall that E[vi ] = 0), a similar argu1 1 ment yields |φi (vi )| ≤ |vi | . [sent-266, score-0.94]
</p><p>63 R ∂yi  Z  2  R  hi (y0 + v)vi pi (vi )dvi = σ2 i  ∂hi 0 (y + v)φi (vi )dvi dv[−i] R ∂yi  Then,  op =  σ2 i  Z  ∑ σ2 i  Z  ∑  i=1  2  =  i=1  R  R  −1  ∏ p j (v j ) j=i  Z  ∂hi (y) ∏ p j (v j )φi (vi )dv. [sent-270, score-0.271]
</p><p>64 ∂yi j=i  Finally, if the errors vi are Gaussian, φi (vi ) =  1 σ2 i  Z +∞ vi  2  2  − s2 − s2 s 1 √ e 2σi ds = − √ e 2σi 2πσi 2πσi  +∞  = pi (vi ). [sent-271, score-0.49]
</p><p>65 vi  Therefore, op =  2  ∑ σ2 i  i=1  Z  R  ∂hi 2 ∂hi (y) ∏ p j (v j )φi (vi )dv = ∑ σ2 E (y) i ∂yi ∂yi i=1 j=i  thus proving the thesis. [sent-272, score-0.328]
</p><p>66 In fact, recalling that for a linear estimator the degrees of freedom do not depend on y, expression (16) is eventually recovered. [sent-274, score-0.176]
</p><p>67 The next theorem derives a simple expression for the optimism of the SVR estimator in a somehow ideal case (see assumption (v) below). [sent-276, score-0.292]
</p><p>68 (15) holds, (ii) the errors vi are independent of each other, (iii) the variances σ2 = Var[vi ] are ﬁnite, i (iv) C < +∞, (v) the set IM of the marginal vectors does not depend on v. [sent-278, score-0.234]
</p><p>69 Then, the optimism of the SVR is opSV R =  2  ∑ σ2 . [sent-279, score-0.204]
</p><p>70 R EPRESENTER T HEOREM AND D EGREES OF F REEDOM OF SVR  This approximated optimism can be used to assess the in-sample error: CSV R = err + opSV R . [sent-292, score-0.327]
</p><p>71 ∂yi  is an unbiased estimate of the true optimism opSV R . [sent-296, score-0.204]
</p><p>72 Numerical Examples In this section the use of the C p statistic for tuning the SVR parameters (ε,C) is illustrated by means of two numerical examples. [sent-298, score-0.207]
</p><p>73 Finally, a simulated experiment is used to assess the precision of the optimism estimate opSV R as a function of the noise variance. [sent-299, score-0.24]
</p><p>74 , , are generated as yi = y 0 + v i , i y0 = f 0 (xi ), i where the errors vi ∼ N(0, σ2 ) , σ2 = 0. [sent-306, score-0.334]
</p><p>75 A cubic B-spline kernel was adopted: xi =  K(x, x ) = B3 (x − x ). [sent-309, score-0.142]
</p><p>76 Then, the optimism op was estimated as the 2482  R EPRESENTER T HEOREM AND D EGREES OF F REEDOM OF SVR  0. [sent-368, score-0.327]
</p><p>77 The results have been obtained by SVR with C p tuning (Panel A) and SVR with best possible tuning (Panel B). [sent-402, score-0.234]
</p><p>78 The SVR estimate corresponding to the best possible tuning ( ε(1) , C(1) ) is plotted for 2484  R EPRESENTER T HEOREM AND D EGREES OF F REEDOM OF SVR  A  B  25  25  20  20  15  15  10  10  5  5  0  0  0. [sent-429, score-0.143]
</p><p>79 25  Figure 5: Distribution of the RMSE over the 100 data sets using C p tuning (Panel A) and the best possible tuning (Panel B). [sent-439, score-0.234]
</p><p>80 9  1  Figure 7: Data set #1: True function (continuous), data (crosses), SVR estimate with C p tuning (thick continuous) and SVR with best possible tuning (dash-dot). [sent-498, score-0.234]
</p><p>81 err = opSV R =  1  ∑  i=1 ˆ 2 σ2 m  2  yi − fˆ(xi ) , ,  CSV R = err + opSV R , p GCV SV R =  2 err  ( − m)2  . [sent-499, score-0.498]
</p><p>82 2487  D INUZZO , N EVE , G IANAZZA AND D E N ICOLAO  op SV R  err 0. [sent-512, score-0.246]
</p><p>83 2  Figure 8: Boston Housing data: empirical risk (err), optimism estimate (opSV R ), C p statistic (CSV R ), p 5-fold cross-validation score (5-CV ), Generalized Cross Validation score (GCV SV R ), and mean square error on test data (Test Error). [sent-545, score-0.368]
</p><p>84 2488  R EPRESENTER T HEOREM AND D EGREES OF F REEDOM OF SVR  op SV R  err  4  4 3 log10 C  log10 C  3 2 1 0  2 1  0  0. [sent-546, score-0.246]
</p><p>85 3  Figure 9: Boston Housing data: contour plots of empirical risk (err), optimism estimate (opSV R ), C p statistic (CSV R ), 5-fold cross-validation score (5-CV ), Generalized Cross Validation p score (GCV SV R ), and mean square error on test data (Test Error). [sent-564, score-0.39]
</p><p>86 0  Figure 10: Simulated experiment: boxplots of the estimated optimism opSV R against different values of the noise standard deviation. [sent-582, score-0.204]
</p><p>87 40 Next, for each σ j , 100 independent data sets were generated according to the model σj =  yi = sinc(3xi ) + vi , vi ∼ N(0, σ2 ), j 2i − 101 xi = , i = 1, . [sent-589, score-0.659]
</p><p>88 Since, under Gaussian noise, opSV R is an unbiased estimator, the true optimism opSV R , which is not reported in the plot, coincides with the expected value of opSV R . [sent-599, score-0.204]
</p><p>89 Concluding Remarks In this paper, a novel formulation of the quantitative representer theorem is derived for convex loss functions. [sent-603, score-0.19]
</p><p>90 In view of the sensitivity analysis, the degrees of freedom of SVR are deﬁned as the number of marginal support vectors. [sent-606, score-0.207]
</p><p>91 Such a deﬁnition is further justiﬁed by the role that the degrees of freedom play in the assessment of the optimism, that is the difference between the in-sample prediction error and the expected empirical risk. [sent-607, score-0.144]
</p><p>92 A C p statistic for SVR is deﬁned and proposed as a criterion for tuning both the parameters ε and C. [sent-608, score-0.207]
</p><p>93 (2000) who prove the representer theorem for differentiable loss functions. [sent-615, score-0.191]
</p><p>94 In view of this, solving (1) is equivalent to minimizing the following functional with respect to the coefﬁcient sequence: +∞  F[{cn }] = C ∑ V  yi , ∑ cn φn (xi ) +  i=1  n=1  1 +∞ c2 ∑ n. [sent-621, score-0.224]
</p><p>95 Exploiting the linearity of the subdifferential with respect to sums of convex functions and the fact that the second term is Gˆ teaux-differentiable, we obtain: a ∂F = C ∑ ∂V i=1  +∞  yi , ∑ cn φn (xi ) + n=1  cn λn  . [sent-624, score-0.388]
</p><p>96 Introducing the linear operators Ji :  2  →R +∞  Ji ({cn }) =  ∑ cn φn (xi ),  n=1  2492  R EPRESENTER T HEOREM AND D EGREES OF F REEDOM OF SVR  we can write +∞  yi , ∑ cn φn (xi )  ∂V  n=1  Notice that the adjoint Ji∗ : R → {cn }, Ji∗ (t)  2 (R)  = ∂V (yi , Ji ({cn })) . [sent-630, score-0.319]
</p><p>97 In fact,  +∞ 2  +∞  n=1  =  n=1  ∑ cn (Ji∗ (t))n = t ∑ cn φn (xi ) =  Ji ({cn }),t  R. [sent-632, score-0.19]
</p><p>98 In view of Proposition 4, ∂F = C ∑ φn (xi )∂2V i=1  +∞  yi , ∑ cn φn (xi ) + n=1  cn λn  so that the condition 0 ∈ ∂F implies that the optimal sequence {c n } must satisfy ˆ cn ∈ − ∑ C∂2V yi , fˆ(xi ) λn φn (xi ). [sent-633, score-0.543]
</p><p>99 ˆ i=1  It is then possible to write cn = ∑ ai λn φn (xi ), ˆ i=1  where ai ∈ −C∂2V (yi , fˆ(xi )). [sent-634, score-0.313]
</p><p>100 Finally, exploiting the bilinear formula for the reproducing kernel, we obtain fˆ(x) =  +∞  +∞  ˆ ∑ cn φn (x) = ∑ ai ∑ λn φn (xi )φn (x) = ∑ ai K(xi , x). [sent-635, score-0.349]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('svr', 0.44), ('opsv', 0.264), ('errin', 0.252), ('vi', 0.205), ('optimism', 0.204), ('egrees', 0.168), ('epresenter', 0.168), ('eve', 0.168), ('ianazza', 0.168), ('icolao', 0.168), ('inuzzo', 0.168), ('reedom', 0.168), ('gcv', 0.132), ('rmse', 0.132), ('yi', 0.129), ('heorem', 0.127), ('op', 0.123), ('err', 0.123), ('xi', 0.12), ('im', 0.119), ('tuning', 0.117), ('hi', 0.113), ('representer', 0.11), ('ai', 0.109), ('sv', 0.108), ('cn', 0.095), ('dvi', 0.091), ('kmm', 0.091), ('statistic', 0.09), ('housing', 0.072), ('iout', 0.072), ('pseudoresidual', 0.072), ('ck', 0.072), ('vito', 0.071), ('subdifferential', 0.069), ('freedom', 0.06), ('si', 0.06), ('nicolao', 0.06), ('spi', 0.06), ('sensitivity', 0.06), ('estimator', 0.058), ('degrees', 0.058), ('yk', 0.052), ('differentiable', 0.051), ('gunter', 0.051), ('quantitative', 0.05), ('cd', 0.049), ('csv', 0.048), ('iin', 0.048), ('inclusions', 0.048), ('pavia', 0.048), ('ynew', 0.048), ('hastie', 0.046), ('ds', 0.045), ('dv', 0.043), ('ji', 0.04), ('panel', 0.039), ('nondecreasing', 0.039), ('bands', 0.036), ('cherkassky', 0.036), ('dinuzzo', 0.036), ('errv', 0.036), ('francesco', 0.036), ('gianazza', 0.036), ('giuseppe', 0.036), ('neve', 0.036), ('simulated', 0.036), ('reproducing', 0.036), ('pi', 0.035), ('proposition', 0.034), ('cp', 0.033), ('coef', 0.033), ('lim', 0.031), ('marta', 0.03), ('theorem', 0.03), ('marginal', 0.029), ('risk', 0.028), ('cients', 0.028), ('panels', 0.027), ('hereafter', 0.027), ('boston', 0.027), ('plotted', 0.026), ('prediction', 0.026), ('monotone', 0.025), ('fourier', 0.025), ('ik', 0.025), ('dipartimento', 0.024), ('ekeland', 0.024), ('hat', 0.024), ('jv', 0.024), ('pseudoresiduals', 0.024), ('ssrl', 0.024), ('ugo', 0.024), ('score', 0.023), ('yt', 0.022), ('steinwart', 0.022), ('kernel', 0.022), ('achievable', 0.022), ('contour', 0.022), ('tikhonov', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="63-tfidf-1" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>Author: Francesco Dinuzzo, Marta Neve, Giuseppe De Nicolao, Ugo Pietro Gianazza</p><p>Abstract: Support Vector Regression (SVR) for discrete data is considered. An alternative formulation of the representer theorem is derived. This result is based on the newly introduced notion of pseudoresidual and the use of subdifferential calculus. The representer theorem is exploited to analyze the sensitivity properties of ε-insensitive SVR and introduce the notion of approximate degrees of freedom. The degrees of freedom are shown to play a key role in the evaluation of the optimism, that is the difference between the expected in-sample error and the expected empirical risk. In this way, it is possible to deﬁne a C p -like statistic that can be used for tuning the parameters of SVR. The proposed tuning procedure is tested on a simulated benchmark problem and on a real world problem (Boston Housing data set). Keywords: statistical learning, reproducing kernel Hilbert spaces, support vector machines, representer theorem, regularization theory</p><p>2 0.076906353 <a title="63-tfidf-2" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>Author: Ryan M. Rifkin, Ross A. Lippert</p><p>Abstract: Regularization is an approach to function learning that balances ﬁt and smoothness. In practice, we search for a function f with a ﬁnite representation f = ∑i ci φi (·). In most treatments, the ci are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning. Keywords: kernel machines, duality, optimization, convex analysis, kernel learning</p><p>3 0.062882818 <a title="63-tfidf-3" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>Author: Yiming Ying, Ding-Xuan Zhou</p><p>Abstract: Gaussian kernels with ﬂexible variances provide a rich family of Mercer kernels for learning algorithms. We show that the union of the unit balls of reproducing kernel Hilbert spaces generated by Gaussian kernels with ﬂexible variances is a uniform Glivenko-Cantelli (uGC) class. This result conﬁrms a conjecture concerning learnability of Gaussian kernels and veriﬁes the uniform convergence of many learning algorithms involving Gaussians with changing variances. Rademacher averages and empirical covering numbers are used to estimate sample errors of multi-kernel regularization schemes associated with general loss functions. It is then shown that the regularization error associated with the least square loss and the Gaussian kernels can be greatly improved when ﬂexible variances are allowed. Finally, for regularization schemes generated by Gaussian kernels with ﬂexible variances we present explicit learning rates for regression with least square loss and classiﬁcation with hinge loss. Keywords: Gaussian kernel, ﬂexible variances, learning theory, Glivenko-Cantelli class, regularization scheme, empirical covering number</p><p>4 0.050900016 <a title="63-tfidf-4" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>Author: Yann Guermeur</p><p>Abstract: In the context of discriminant analysis, Vapnik’s statistical learning theory has mainly been developed in three directions: the computation of dichotomies with binary-valued functions, the computation of dichotomies with real-valued functions, and the computation of polytomies with functions taking their values in ﬁnite sets, typically the set of categories itself. The case of classes of vectorvalued functions used to compute polytomies has seldom been considered independently, which is unsatisfactory, for three main reasons. First, this case encompasses the other ones. Second, it cannot be treated appropriately through a na¨ve extension of the results devoted to the computation of ı dichotomies. Third, most of the classiﬁcation problems met in practice involve multiple categories. In this paper, a VC theory of large margin multi-category classiﬁers is introduced. Central in this theory are generalized VC dimensions called the γ-Ψ-dimensions. First, a uniform convergence bound on the risk of the classiﬁers of interest is derived. The capacity measure involved in this bound is a covering number. This covering number can be upper bounded in terms of the γ-Ψdimensions thanks to generalizations of Sauer’s lemma, as is illustrated in the speciﬁc case of the scale-sensitive Natarajan dimension. A bound on this latter dimension is then computed for the class of functions on which multi-class SVMs are based. This makes it possible to apply the structural risk minimization inductive principle to those machines. Keywords: multi-class discriminant analysis, large margin classiﬁers, uniform strong laws of large numbers, generalized VC dimensions, multi-class SVMs, structural risk minimization inductive principle, model selection</p><p>5 0.043462679 <a title="63-tfidf-5" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>Author: François Laviolette, Mario Marchand</p><p>Abstract: We propose a PAC-Bayes theorem for the sample-compression setting where each classiﬁer is described by a compression subset of the training data and a message string of additional information. This setting, which is the appropriate one to describe many learning algorithms, strictly generalizes the usual data-independent setting where classiﬁers are represented only by data-independent message strings (or parameters taken from a continuous set). The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classiﬁer vanishes. For posteriors having all their weights on a single sample-compressed classiﬁer, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. (2005). Finally, we extend our results to the case where each sample-compressed classiﬁer of a data-dependent ensemble may abstain of predicting a class label. Keywords: PAC-Bayes, risk bounds, sample-compression, set covering machines, decision list machines</p><p>6 0.043427773 <a title="63-tfidf-6" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>7 0.042189062 <a title="63-tfidf-7" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>8 0.04179981 <a title="63-tfidf-8" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>9 0.041626152 <a title="63-tfidf-9" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>10 0.040487442 <a title="63-tfidf-10" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>11 0.039943647 <a title="63-tfidf-11" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>12 0.039323084 <a title="63-tfidf-12" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>13 0.037594631 <a title="63-tfidf-13" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>14 0.033959664 <a title="63-tfidf-14" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>15 0.033958092 <a title="63-tfidf-15" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>16 0.033446707 <a title="63-tfidf-16" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>17 0.032899607 <a title="63-tfidf-17" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>18 0.031839054 <a title="63-tfidf-18" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>19 0.031566512 <a title="63-tfidf-19" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>20 0.031455766 <a title="63-tfidf-20" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.19), (1, -0.074), (2, 0.044), (3, 0.02), (4, 0.009), (5, -0.024), (6, -0.037), (7, -0.046), (8, -0.024), (9, -0.021), (10, 0.092), (11, 0.041), (12, -0.146), (13, -0.0), (14, 0.105), (15, 0.041), (16, -0.124), (17, 0.02), (18, -0.007), (19, -0.148), (20, 0.067), (21, 0.001), (22, 0.199), (23, 0.169), (24, 0.148), (25, -0.096), (26, -0.254), (27, 0.027), (28, -0.176), (29, -0.188), (30, -0.085), (31, 0.076), (32, -0.133), (33, -0.072), (34, -0.077), (35, 0.134), (36, 0.124), (37, 0.007), (38, 0.074), (39, -0.002), (40, -0.017), (41, 0.099), (42, 0.165), (43, -0.08), (44, -0.042), (45, 0.066), (46, 0.066), (47, -0.267), (48, -0.159), (49, -0.29)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95143312 <a title="63-lsi-1" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>Author: Francesco Dinuzzo, Marta Neve, Giuseppe De Nicolao, Ugo Pietro Gianazza</p><p>Abstract: Support Vector Regression (SVR) for discrete data is considered. An alternative formulation of the representer theorem is derived. This result is based on the newly introduced notion of pseudoresidual and the use of subdifferential calculus. The representer theorem is exploited to analyze the sensitivity properties of ε-insensitive SVR and introduce the notion of approximate degrees of freedom. The degrees of freedom are shown to play a key role in the evaluation of the optimism, that is the difference between the expected in-sample error and the expected empirical risk. In this way, it is possible to deﬁne a C p -like statistic that can be used for tuning the parameters of SVR. The proposed tuning procedure is tested on a simulated benchmark problem and on a real world problem (Boston Housing data set). Keywords: statistical learning, reproducing kernel Hilbert spaces, support vector machines, representer theorem, regularization theory</p><p>2 0.38348484 <a title="63-lsi-2" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>Author: Ryan M. Rifkin, Ross A. Lippert</p><p>Abstract: Regularization is an approach to function learning that balances ﬁt and smoothness. In practice, we search for a function f with a ﬁnite representation f = ∑i ci φi (·). In most treatments, the ci are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning. Keywords: kernel machines, duality, optimization, convex analysis, kernel learning</p><p>3 0.32450402 <a title="63-lsi-3" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>Author: Masashi Sugiyama</p><p>Abstract: Reducing the dimensionality of data without losing intrinsic information is an important preprocessing step in high-dimensional data analysis. Fisher discriminant analysis (FDA) is a traditional technique for supervised dimensionality reduction, but it tends to give undesired results if samples in a class are multimodal. An unsupervised dimensionality reduction method called localitypreserving projection (LPP) can work well with multimodal data due to its locality preserving property. However, since LPP does not take the label information into account, it is not necessarily useful in supervised learning scenarios. In this paper, we propose a new linear supervised dimensionality reduction method called local Fisher discriminant analysis (LFDA), which effectively combines the ideas of FDA and LPP. LFDA has an analytic form of the embedding transformation and the solution can be easily computed just by solving a generalized eigenvalue problem. We demonstrate the practical usefulness and high scalability of the LFDA method in data visualization and classiﬁcation tasks through extensive simulation studies. We also show that LFDA can be extended to non-linear dimensionality reduction scenarios by applying the kernel trick. Keywords: dimensionality reduction, supervised learning, Fisher discriminant analysis, locality preserving projection, afﬁnity matrix</p><p>4 0.32123882 <a title="63-lsi-4" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: In numerous domains it is useful to represent a single example by the set of the local features or parts that comprise it. However, this representation poses a challenge to many conventional machine learning techniques, since sets may vary in cardinality and elements lack a meaningful ordering. Kernel methods can learn complex functions, but a kernel over unordered set inputs must somehow solve for correspondences—generally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function called the pyramid match that measures partial match similarity in time linear in the number of features. The pyramid match maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in order to ﬁnd implicit correspondences based on the ﬁnest resolution histogram cell where a matched pair ﬁrst appears. We show the pyramid match yields a Mercer kernel, and we prove bounds on its error relative to the optimal partial matching cost. We demonstrate our algorithm on both classiﬁcation and regression tasks, including object recognition, 3-D human pose inference, and time of publication estimation for documents, and we show that the proposed method is accurate and signiﬁcantly more efﬁcient than current approaches. Keywords: kernel, sets of features, histogram intersection, multi-resolution histogram pyramid, approximate matching, object recognition</p><p>5 0.3103908 <a title="63-lsi-5" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>Author: Yiming Ying, Ding-Xuan Zhou</p><p>Abstract: Gaussian kernels with ﬂexible variances provide a rich family of Mercer kernels for learning algorithms. We show that the union of the unit balls of reproducing kernel Hilbert spaces generated by Gaussian kernels with ﬂexible variances is a uniform Glivenko-Cantelli (uGC) class. This result conﬁrms a conjecture concerning learnability of Gaussian kernels and veriﬁes the uniform convergence of many learning algorithms involving Gaussians with changing variances. Rademacher averages and empirical covering numbers are used to estimate sample errors of multi-kernel regularization schemes associated with general loss functions. It is then shown that the regularization error associated with the least square loss and the Gaussian kernels can be greatly improved when ﬂexible variances are allowed. Finally, for regularization schemes generated by Gaussian kernels with ﬂexible variances we present explicit learning rates for regression with least square loss and classiﬁcation with hinge loss. Keywords: Gaussian kernel, ﬂexible variances, learning theory, Glivenko-Cantelli class, regularization scheme, empirical covering number</p><p>6 0.29674929 <a title="63-lsi-6" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>7 0.25569522 <a title="63-lsi-7" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>8 0.24805416 <a title="63-lsi-8" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>9 0.24421559 <a title="63-lsi-9" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>10 0.23473021 <a title="63-lsi-10" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>11 0.2084544 <a title="63-lsi-11" href="./jmlr-2007-Covariate_Shift_Adaptation_by_Importance_Weighted_Cross_Validation.html">25 jmlr-2007-Covariate Shift Adaptation by Importance Weighted Cross Validation</a></p>
<p>12 0.19534419 <a title="63-lsi-12" href="./jmlr-2007-Large_Margin_Semi-supervised_Learning.html">44 jmlr-2007-Large Margin Semi-supervised Learning</a></p>
<p>13 0.19328967 <a title="63-lsi-13" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>14 0.18965454 <a title="63-lsi-14" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>15 0.18250798 <a title="63-lsi-15" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>16 0.17277028 <a title="63-lsi-16" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>17 0.16532326 <a title="63-lsi-17" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>18 0.16408806 <a title="63-lsi-18" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>19 0.16222827 <a title="63-lsi-19" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>20 0.16140583 <a title="63-lsi-20" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.016), (8, 0.025), (10, 0.027), (12, 0.042), (28, 0.092), (40, 0.053), (45, 0.013), (48, 0.053), (60, 0.048), (85, 0.035), (92, 0.382), (98, 0.111)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70530623 <a title="63-lda-1" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>Author: Francesco Dinuzzo, Marta Neve, Giuseppe De Nicolao, Ugo Pietro Gianazza</p><p>Abstract: Support Vector Regression (SVR) for discrete data is considered. An alternative formulation of the representer theorem is derived. This result is based on the newly introduced notion of pseudoresidual and the use of subdifferential calculus. The representer theorem is exploited to analyze the sensitivity properties of ε-insensitive SVR and introduce the notion of approximate degrees of freedom. The degrees of freedom are shown to play a key role in the evaluation of the optimism, that is the difference between the expected in-sample error and the expected empirical risk. In this way, it is possible to deﬁne a C p -like statistic that can be used for tuning the parameters of SVR. The proposed tuning procedure is tested on a simulated benchmark problem and on a real world problem (Boston Housing data set). Keywords: statistical learning, reproducing kernel Hilbert spaces, support vector machines, representer theorem, regularization theory</p><p>2 0.38669777 <a title="63-lda-2" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>Author: Marta Arias, Roni Khardon, Jérôme Maloberti</p><p>Abstract: The paper introduces L OG A N -H —a system for learning ﬁrst-order function-free Horn expressions from interpretations. The system is based on an algorithm that learns by asking questions and that was proved correct in previous work. The current paper shows how the algorithm can be implemented in a practical system, and introduces a new algorithm based on it that avoids interaction and learns from examples only. The L OG A N -H system implements these algorithms and adds several facilities and optimizations that allow efﬁcient applications in a wide range of problems. As one of the important ingredients, the system includes several fast procedures for solving the subsumption problem, an NP-complete problem that needs to be solved many times during the learning process. We describe qualitative and quantitative experiments in several domains. The experiments demonstrate that the system can deal with varied problems, large amounts of data, and that it achieves good classiﬁcation accuracy. Keywords: inductive logic programming, subsumption, bottom-up learning, learning with queries</p><p>3 0.38323027 <a title="63-lda-3" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>4 0.38295311 <a title="63-lda-4" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>Author: Onur C. Hamsici, Aleix M. Martinez</p><p>Abstract: Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere S p−1 . Such representations should then be modeled using spherical distributions. However, the difﬁculty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead—as if the data were represented in R p rather than S p−1 . This opens the question to whether the classiﬁcation results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases (which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to deﬁne optimal classiﬁers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classiﬁers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classiﬁcation of images of objects, gene expression sequences, and text data. Keywords: directional data, spherical distributions, normal distributions, norm normalization, linear and non-linear classiﬁers, computer vision</p><p>5 0.38120329 <a title="63-lda-5" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>Author: Jean-Yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, ﬁrst, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester (1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand (see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds. Keywords: statistical learning theory, PAC-Bayes theorems, generalization error bounds</p><p>6 0.3811658 <a title="63-lda-6" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>7 0.38046837 <a title="63-lda-7" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>8 0.38045514 <a title="63-lda-8" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>9 0.37889636 <a title="63-lda-9" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>10 0.37861764 <a title="63-lda-10" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>11 0.37732211 <a title="63-lda-11" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>12 0.37608793 <a title="63-lda-12" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>13 0.37547815 <a title="63-lda-13" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>14 0.37517959 <a title="63-lda-14" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>15 0.36944568 <a title="63-lda-15" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>16 0.36916366 <a title="63-lda-16" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>17 0.36905015 <a title="63-lda-17" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>18 0.36859193 <a title="63-lda-18" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>19 0.36857241 <a title="63-lda-19" href="./jmlr-2007-Stagewise_Lasso.html">77 jmlr-2007-Stagewise Lasso</a></p>
<p>20 0.3664223 <a title="63-lda-20" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
