<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-68" href="#">jmlr2007-68</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2007-68-pdf" href="http://jmlr.org/papers/volume8/cawley07a/cawley07a.pdf">pdf</a></p><p>Author: Gavin C. Cawley, Nicola L. C. Talbot</p><p>Abstract: While the model parameters of a kernel machine are typically given by the solution of a convex optimisation problem, with a single global optimum, the selection of good values for the regularisation and kernel parameters is much less straightforward. Fortunately the leave-one-out cross-validation procedure can be performed or a least approximated very efﬁciently in closed form for a wide variety of kernel learning methods, providing a convenient means for model selection. Leave-one-out cross-validation based estimates of performance, however, generally exhibit a relatively high variance and are therefore prone to over-ﬁtting. In this paper, we investigate the novel use of Bayesian regularisation at the second level of inference, adding a regularisation term to the model selection criterion corresponding to a prior over the hyper-parameter values, where the additional regularisation parameters are integrated out analytically. Results obtained on a suite of thirteen real-world and synthetic benchmark data sets clearly demonstrate the beneﬁt of this approach. Keywords: model selection, kernel methods, Bayesian regularisation</p><p>Reference: <a title="jmlr-2007-68-reference" href="../jmlr2007_reference/jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Fortunately the leave-one-out cross-validation procedure can be performed or a least approximated very efﬁciently in closed form for a wide variety of kernel learning methods, providing a convenient means for model selection. [sent-11, score-0.144]
</p><p>2 Results obtained on a suite of thirteen real-world and synthetic benchmark data sets clearly demonstrate the beneﬁt of this approach. [sent-14, score-0.144]
</p><p>3 Keywords: model selection, kernel methods, Bayesian regularisation  1. [sent-15, score-0.906]
</p><p>4 These methods have proved highly successful for kernel machines having only a small number of hyper-parameters to optimise, as demonstrated by the set of models achieving the best average score in the WCCI-2006 performance prediction challenge1 (Cawley, 2006; Guyon et al. [sent-22, score-0.106]
</p><p>5 A kernel with many hyper-parameters, for instance those used in Automatic Relevance Determination (ARD) (e. [sent-27, score-0.106]
</p><p>6 C AWLEY AND TALBOT  degrees of freedom to over-ﬁt leave-one-out cross-validation based model selection criteria, resulting in performance inferior to that obtained using a less ﬂexible kernel function. [sent-39, score-0.227]
</p><p>7 In this paper, we investigate the novel use of regularisation (Tikhonov and Arsenin, 1977) of the hyper-parameters in model selection in order to ameliorate the effects of the high variance of leave-one-out crossvalidation based selection criteria, and so improve predictive performance. [sent-40, score-0.966]
</p><p>8 The regularisation term corresponds to a zero-mean Gaussian prior over the values of the kernel parameters, representing a preference for smooth kernel functions, and hence a relatively simple classiﬁer. [sent-41, score-1.013]
</p><p>9 The regularisation parameters introduced in this step are integrated out analytically in the style of Buntine and Weigend (1991), to provide a Bayesian model selection criterion that can be optimised in a straightforward manner via, for example, scaled conjugate gradient descent (Williams, 1991). [sent-42, score-1.047]
</p><p>10 The paper is structured as follows: The remainder of this section provides a brief overview of the least-squares support vector machine, including the use of leave-one-out cross-validation based model selection procedures, given in sufﬁcient detail to ensure the reproducibility of the results. [sent-43, score-0.121]
</p><p>11 Section 2 describes the use of Bayesian regularisation to prevent over-ﬁtting at the second level of inference, that is, model selection. [sent-44, score-0.8]
</p><p>12 Section 3 presents results obtained over a suite of thirteen benchmark data sets, which demonstrate the utility of this approach. [sent-45, score-0.144]
</p><p>13 Rather than specify the feature space directly, it is instead induced by a kernel function, K : X × X → R, which evaluates the inner-product between the projections of the data onto the feature space, F , that is, K (x, x ) = φ(x) · φ(x ). [sent-53, score-0.106]
</p><p>14 The interpretation of an inner-product in a ﬁxed feature space is valid for any Mercer kernel (Mercer, 1909), for which the Gram matrix, K = [k i j = K (xi , x j )]i, j=1 is positive semi-deﬁnite, that is, aT Ka ≥ 0,  ∀a∈R ,  a = 0. [sent-54, score-0.106]
</p><p>15 The Radial Basis Function (RBF) kernel,  K (x, x ) = exp −η x − x  2  is commonly encountered in practical applications of kernel learning methods, here η is a kernel parameter, controlling the sensitivity of the kernel function. [sent-61, score-0.318]
</p><p>16 The feature space for the radial basis function kernel consists of the positive orthant of an inﬁnite-dimensional unit hyper-sphere (e. [sent-62, score-0.158]
</p><p>17 The Gram matrix for the radial basis function kernel is thus of full rank (Micchelli, 1986), and so the kernel model is able to form an arbitrary shattering of the data. [sent-65, score-0.327]
</p><p>18 1 A D UAL T RAINING A LGORITHM The basic training algorithm for the least-squares support vector machine (Suykens and Vandewalle, 1999) views the regularised loss function (1) as a constrained minimisation problem: 1 w min w,b,εi 2  2  +  1 ∑ ε2 2µ i=1 i  subject to  εi = yi − w · φ(xi ) − b. [sent-68, score-0.149]
</p><p>19 The primal Lagrangian for this constrained optimisation problem gives the unconstrained minimisation problem deﬁned by the following regularised loss function,  L=  1 w 2  2  +  1 ∑ ε2 − ∑ αi {w · φ(xi ) + b + εi − yi } , 2µ i=1 i i=1  where α = (α1 , α2 , . [sent-69, score-0.208]
</p><p>20 2 Leave-One-Out Cross-Validation Cross-validation (Stone, 1974) is commonly used to obtain a reliable estimate of the test error for performance estimation or for use as a model selection criterion. [sent-103, score-0.121]
</p><p>21 3 Model Selection The virtual leave-one-out cross-validation procedure described in the previous section provides the basis for a simple automated model selection strategy for the least-squares support vector machine. [sent-163, score-0.121]
</p><p>22 Perhaps the most basic model selection criterion is provided by the Predicted REsidual Sum of Squares (PRESS) criterion (Allen, 1974), which is simply the leave-one-out estimate of the sum-ofsquares error, 1 (−i) 2 ˆ . [sent-164, score-0.249]
</p><p>23 Q(θ) = ∑ yi − yi 2 i=1 A minimum of the model selection criterion is often found via a simple grid-search procedure in the majority of practical applications of kernel learning methods. [sent-165, score-0.349]
</p><p>24 ∂C−1 ii , 2 ∂θ j  C AWLEY AND TALBOT  T  We begin by deriving the partial derivatives of the model parameters, αT b , with respect to the hyper-parameter θ j . [sent-184, score-0.121]
</p><p>25 Note the computational complexity of evaluating the partial derivatives of the model parameters is O ( 2 ), as only two successive matrix-vector products are required. [sent-187, score-0.121]
</p><p>26 The partial derivatives of the diagonal elements of C −1 can be found using the inverse matrix derivative identity (13). [sent-188, score-0.108]
</p><p>27 If, on the other hand, we consider the regularisation parameter, µ, we have that ∂C = ∂µ  I 0T  0 0  ,  and so the computation of the partial derivatives of the model parameters, with respect to the regularisation parameter, is slightly simpliﬁed, ∂ αT b ∂µ  T  = −C−1 αT b  T  . [sent-190, score-1.645]
</p><p>28 This suggests that it may be more efﬁcient to adopt different strategies for optimising the regularisation parameter, µ, and the vector of kernel parameters, η, (cf. [sent-192, score-0.895]
</p><p>29 For a kernel parameter, η j , the partial derivatives of C with respect to η j are given by the partial derivatives of the kernel matrix, that is, ∂C = ∂η j  ∂K/∂η j 0 0T 0  . [sent-195, score-0.378]
</p><p>30 For the spherical radial basis function kernel, used in this study, the partial derivative with respect to the kernel parameter is given by ∂K (x, x ) = −K (x, x ) x − x ∂η  2  . [sent-196, score-0.239]
</p><p>31 , 2006), aims to identify informative input features as a natural consequence of optimising the model selection criterion. [sent-204, score-0.148]
</p><p>32 This can be most easily achieved using an elliptical radial basis function kernel, d  K (x, x ) = exp − ∑ ηi [xi − xi ]2 , i=1  that incorporates individual scaling factors for each input dimension. [sent-205, score-0.177]
</p><p>33 The partial derivatives with respect to the kernel parameters are then given by, ∂K (x, x ) = −K (x, x )[xi − xi ]2 . [sent-206, score-0.189]
</p><p>34 It is therefore hoped that minimising the model selection criterion will lead to very small values for the scaling factors associated with redundant input features, allowing them to be identiﬁed and pruned from the model. [sent-208, score-0.185]
</p><p>35 2 i=1 i  In this study we have left the regularisation parameter, µ, unregularised. [sent-212, score-0.762]
</p><p>36 However, we have now introduced two further regularisation parameters ξ and ζ for which good values must also be found. [sent-213, score-0.762]
</p><p>37 This problem may be solved by taking a Bayesian approach and adopting an ignorance prior and integrating out the additional regularisation parameters analytically in the style of Buntine and Weigend (1991). [sent-214, score-0.843]
</p><p>38 Adapting the approach taken by Williams (1995), the regularised model selection criterion (14) can be interpreted as the posterior density in the space of the hyper-parameters, P(θ|D ) ∝ P(D |θ)P(θ), by taking the negative logarithm and neglecting additive constants. [sent-215, score-0.305]
</p><p>39 In this study, however we opt to integrate out the hyper-parameters analytically, extending the work by Buntine and Weigend (1991) and Williams (1995) to consider Bayesian regularisation at the second level of inference, namely the selection of good values for the hyper-parameters. [sent-231, score-0.845]
</p><p>40 The regularisation parameter ξ may then be integrated out analytically using a suitable prior, P(ξ), P(θ) =  Z  P(θ|ξ)P(ξ)dξ. [sent-233, score-0.804]
</p><p>41 d/2 Ωd/2 2 (2π)  Finally, adopting a similar procedure to eliminate ζ, we obtain a revised model selection criterion with Bayesian regularisation, d L = log Q(θ) + log Ω(θ). [sent-237, score-0.185]
</p><p>42 (15) 2 2 in which the regularisation parameters have been eliminated. [sent-238, score-0.762]
</p><p>43 As before, this criterion can be optimised via standard methods, such as the Nelder-Mead simplex algorithm (Nelder and Mead, 1965) or scaled conjugate gradient descent (Williams, 1991). [sent-239, score-0.122]
</p><p>44 The partial derivatives of the proposed Bayesian model selection criterion are given by ∂L d ∂Ω(θ) ∂Q(θ) = + ∂θi 2Q(θ) ∂θi 2Ω(θ) ∂θi  and  ∂Ω(θ) = ηi . [sent-240, score-0.268]
</p><p>45 Per iteration of the model selection process, the cost of the Bayesian regularisation is therefore minimal. [sent-242, score-0.883]
</p><p>46 There seems little reason to suppose that the regularisation will have an adverse effect on convergence, and this seems to be the case in practice. [sent-243, score-0.762]
</p><p>47 2 Relationship with the Evidence Framework Under the evidence framework of MacKay (1992a,b,c) the regularisation parameters, ξ and ζ, are selected so as to maximise the marginal likelihood, also known as the Bayesian evidence, for the model. [sent-245, score-0.793]
</p><p>48 The log-evidence is given by d 1 log P(D ) = −ξΩ(θ) − ζQ(θ) − log |A| + log ξ + log ζ − log {2π} , 2 2 2 2 where A is the Hessian of the regularised model selection criterion (14) with respect to the hyperparameters, θ. [sent-246, score-0.305]
</p><p>49 In the case of the L2 regularisation term, corresponding to a Gaussian prior, the number of well determined hyper-parameters is given by γ=  n  λj  ∑ λj +ξ  j=1  where, λi , . [sent-248, score-0.762]
</p><p>50 , λn represent the eigenvalues of the Hessian of the unregularised model selection criterion, Q(θ) with respect to the kernel parameters. [sent-251, score-0.227]
</p><p>51 Since γ ≤ d, it seems self evident that the proposed Bayesian regularisation scheme will be prone to a degree of under-ﬁtting, especially in the case of a feature scaling kernel with many redundant features. [sent-254, score-0.868]
</p><p>52 However, the integrate-out approach does not require the evaluation of the Hessian matrix of the original selection criterion, Q(θ), which is likely to prove computationally prohibitive. [sent-256, score-0.108]
</p><p>53 Results In this section, we present experimental results demonstrating the beneﬁts of the proposed model selection strategy incorporating Bayesian regularisation to overcome the inherent high variance of leave-one-out cross-validation based selection criteria. [sent-258, score-0.966]
</p><p>54 Table 2 shows a comparison of the error rates of least-squares support vector machines, using model selection procedures with, and without, Bayesian regularisation, (LS-SVM and LS-SVM-BR respectively) over the suite of thirteen public domain benchmark data sets used in the study by Mika et al. [sent-259, score-0.265]
</p><p>55 In each case, model selection is performed independently for each realisation of the data set, such that the standard errors reﬂect the variability of both the training algorithm and the model selection procedure with changes in the sampling of the data. [sent-263, score-0.242]
</p><p>56 Both conventional spherical and elliptical radial basis kernels are used for all kernel learning methods, so that the effectiveness of each algorithm for automatic relevance determination can be assessed. [sent-264, score-0.445]
</p><p>57 LS-SVM models with and without Bayesian regularisation are very similar, with neither model proving signiﬁcantly better than the other on any of the data sets. [sent-273, score-0.8]
</p><p>58 This seems reasonable given that only two hyper-parameters are optimised during model selection and so there is little scope for over-ﬁtting the PRESS model selection criterion and the regularisation term will have little effect. [sent-274, score-1.101]
</p><p>59 The LS-SVM model with Bayesian regularisation is signiﬁcantly out-performed by the Gaussian Process classiﬁer on one benchmark banana, but performs signiﬁcantly better on a further four (ringnorm, splice, twonorm, waveform). [sent-275, score-0.826]
</p><p>60 2 Performance of Models Based on the Elliptical RBF Kernel The performances of LS-SVM, LS-SVM-BR and EP-GPC models based on the elliptical Gaussian kernel, which includes a separate scale parameter for each input feature, are shown in the last three columns of Table 2. [sent-279, score-0.125]
</p><p>61 Before evaluating the effects of Bayesian regularisation in model selection it is worth noting that the use of elliptical RBF kernels does not generally improve performance. [sent-280, score-1.008]
</p><p>62 This seems likely to be a result of the additional degrees of freedom involved in the model selection process, allowing over-ﬁtting of the PRESS model selection criterion as a result of its inherently high variance. [sent-284, score-0.306]
</p><p>63 Note that fully Bayesian approaches, such as the Gaussian Process Classiﬁer, are also unable to reliably select kernel parameters for the elliptical RBF kernel. [sent-285, score-0.231]
</p><p>64 The elliptical kernel is signiﬁcantly better on only three benchmarks (flare solar, 853  Radial Basis Function  Data Set  Automatic Relevance Determination EP-GPC  LSSVM  LSSVM-BR  EP-GPC  10. [sent-286, score-0.268]
</p><p>65 195  Splice  Table 2: Error rates of least-squares support vector machine, with and without Bayesian regularisation of the model selection criterion, in ¨ this case the PRESS statistic (Allen, 1974), and Gaussian process classiﬁers over thirteen benchmark data sets (R atsch et al. [sent-442, score-1.041]
</p><p>66 , 2001), using both standard radial basis function and automatic relevance determination kernels. [sent-443, score-0.169]
</p><p>67 This demonstrates that over-ﬁtting in model selection, due to the larger number of kernel parameters, is likely to be the signiﬁcant factor causing the relatively poor performance of models with the elliptical RBF kernel. [sent-449, score-0.269]
</p><p>68 Again, the Gaussian Process classiﬁer is signiﬁcantly better than the LS-SVM with Bayesian regularisation on the banana and twonorm data sets, but is signiﬁcantly worse on four of the remaining eleven (diabetis, heart, ringnorm and splice). [sent-450, score-0.981]
</p><p>69 However the magnitude of the difference in performance between LSSVM-BR and EP-GPC approaches tends to be greatest when the LSSVM-BR out-performs EP-GPC, most notably on the heart, splice and ringnorm data sets. [sent-452, score-0.187]
</p><p>70 This provides some support for the observation of Wahba (1990) that cross-validation based model selection procedures should be more robust against model mis-speciﬁcation (see also Rasmussen and Williams, 2006). [sent-453, score-0.159]
</p><p>71 Discussion The experimental evaluation presented in the previous section demonstrates that over-ﬁtting can occur in model selection, due to the variance of the model selection criterion. [sent-455, score-0.159]
</p><p>72 In many cases the minimum of the selection criterion using the elliptical RBF kernel is lower than that achievable using the spherical RBF kernel, however this results in a degradation in generalisation performance. [sent-456, score-0.474]
</p><p>73 Using the standard LSSVM, the elliptical RBF kernel only out-performs the spherical RBF kernel on two of the thirteen data sets, image and splice, which also happen to be the two largest data sets in terms of the number of training patterns. [sent-458, score-0.472]
</p><p>74 A large number of input features introduces a many additional degrees of freedom to optimise the model selection criterion, and so will generally tend to encourage over-ﬁtting. [sent-461, score-0.148]
</p><p>75 In this case the advantage of suppressing the noisy inputs is so great that it overcomes the predisposition towards over-ﬁtting, and so results in improved generalisation (as observed in the case of the image and splice benchmarks). [sent-463, score-0.151]
</p><p>76 Whether the use of an elliptical RBF kernel will improve or degrade generalisation largely depends on such characteristics of the data that are not known a-priori, and so it seems prudent to consider a range of kernel functions and select the best via cross-validation. [sent-464, score-0.388]
</p><p>77 The experimental results indicate that Bayesian regularisation of the hyper-parameters is generally beneﬁcial, without at this stage providing a complete solution to the problem of over-ﬁtting the model selection criterion. [sent-465, score-0.883]
</p><p>78 The effectiveness of the Bayesian regularisation scheme is to a large extent dependent on the appropriateness of the prior imposed on the hyper-parameters. [sent-466, score-0.801]
</p><p>79 The LSSVM with Bayesian regularisation of the hyper-parameters does not signiﬁcantly outperform the expectation propagation based Gaussian process classiﬁer over the suite of thirteen benchmark data sets considered. [sent-471, score-0.906]
</p><p>80 The EP-GPC uses the marginal likelihood as the model selection criterion, which gives the probability of the data, given the assumptions of the model (Rasmussen and Williams, 2006). [sent-473, score-0.159]
</p><p>81 Cross-validation based approaches, on the other hand, provide an estimate of generalisation performance that does not depend on the model assumptions, and so may be more robust against model mis-speciﬁcation (Wahba, 1990). [sent-474, score-0.127]
</p><p>82 The problem of over-ﬁtting in model selection has also been addressed by Qi et al. [sent-482, score-0.121]
</p><p>83 2 Directions for Further Research In this paper, the regularisation term corresponds to a simple spherical Gaussian prior over the kernel parameters. [sent-492, score-0.952]
</p><p>84 One direction of research would be to investigate alternative regularisation terms. [sent-493, score-0.762]
</p><p>85 The 856  BAYESIAN R EGULARISATION IN M ODEL S ELECTION  ﬁrst possibility would be to use a regularisation term corresponding to a separable Laplace prior, Ω(θ) =  1 d ∑ |ηi | 2 i=1  d ξ p(θ) = ∏ exp {−ξ|ηi |} . [sent-494, score-0.762]
</p><p>86 In this way explicit feature selection may be obtained as a consequence of (regularised) model selection. [sent-496, score-0.121]
</p><p>87 The model selection criterion with Bayesian regularisation then becomes L=  2  log Q(θ) + N log Ω(θ)  where N is the number of input features with non-zero scale factors. [sent-497, score-0.947]
</p><p>88 Alternatively, deﬁning a prior over the function of a model seems more in accordance with Bayesian ideals than choosing a prior over the parameters of the model. [sent-500, score-0.138]
</p><p>89 In this case, the regularisation term might take the form, 1 Ω(θ) = 2  d  ∑∑  i=1 j=1  ∂2 y i ˆ ∂xi2j  2  ,  directly penalising models with excess curvature. [sent-502, score-0.762]
</p><p>90 This regularisation term corresponds to curvature driven smoothing in multi-layer perceptron networks (Bishop, 1993), except that the model output yi is viewed as a function of the hyper-parameters, rather than of the weights. [sent-503, score-0.829]
</p><p>91 Conclusion Leave-one-out cross-validation has proved to be an effective means of model selection for a variety of kernel learning methods, provided the number of hyper-parameters to be tuned is relatively small. [sent-507, score-0.227]
</p><p>92 The use of kernel functions with large numbers of parameters often provides sufﬁcient degrees of freedom to over-ﬁt the model selection criterion, leading to poor generalisation. [sent-508, score-0.227]
</p><p>93 In this paper, we have proposed the use of regularisation at the second level of inference, that is, model selection. [sent-509, score-0.8]
</p><p>94 The use of Bayesian regularisation is shown to be effective in reducing over-ﬁtting, by ensuring the values of the kernel parameters remain small, giving a smoother kernel and hence a less complex classiﬁer. [sent-510, score-0.974]
</p><p>95 This is achieved with only a minimal computational expense as the additional regularisation parameters are integrated out analytically using a reference prior. [sent-511, score-0.828]
</p><p>96 While a fully Bayesian 857  C AWLEY AND TALBOT  model selection strategy is conceptually more elegant, it may also be less robust to model misspeciﬁcation. [sent-512, score-0.159]
</p><p>97 The use of leave-one-out cross-validation in model selection and Bayesian methods at the next level seems to be a pragmatic compromise. [sent-513, score-0.121]
</p><p>98 The effectiveness of this approach is clearly demonstrated in the experimental evaluation where, on average, the LS-SVM with Bayesian regularisation out-performs the expectation-propagation based Gaussian process classiﬁer, using both spherical and elliptical RBF kernels. [sent-514, score-0.932]
</p><p>99 Feature scaling for kernel Fisher discriminant analysis using leaveone-out cross-validation. [sent-550, score-0.142]
</p><p>100 Leave-one-out cross-validation based model selection criteria for weighted LSSVMs. [sent-562, score-0.121]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('regularisation', 0.762), ('talbot', 0.22), ('awley', 0.129), ('egularisation', 0.129), ('elliptical', 0.125), ('williams', 0.125), ('cawley', 0.12), ('cholesky', 0.12), ('regularised', 0.12), ('bayesian', 0.106), ('kernel', 0.106), ('splice', 0.1), ('odel', 0.098), ('thirteen', 0.09), ('ringnorm', 0.087), ('selection', 0.083), ('election', 0.081), ('twonorm', 0.077), ('ct', 0.073), ('rasmussen', 0.07), ('heart', 0.07), ('rbf', 0.067), ('diabetis', 0.065), ('criterion', 0.064), ('optimisation', 0.059), ('suykens', 0.059), ('banana', 0.055), ('radial', 0.052), ('lssvm', 0.052), ('determination', 0.051), ('generalisation', 0.051), ('waveform', 0.05), ('derivatives', 0.047), ('tting', 0.046), ('spherical', 0.045), ('factorisation', 0.044), ('luntz', 0.044), ('statistic', 0.042), ('analytically', 0.042), ('gram', 0.042), ('prior', 0.039), ('buntine', 0.039), ('flare', 0.039), ('bo', 0.039), ('cii', 0.039), ('lachenbruch', 0.039), ('saadi', 0.039), ('sundararajan', 0.039), ('weigend', 0.039), ('zq', 0.039), ('model', 0.038), ('gaussian', 0.037), ('benchmarks', 0.037), ('partial', 0.036), ('relevance', 0.036), ('discriminant', 0.036), ('solar', 0.036), ('laplace', 0.035), ('equations', 0.034), ('mika', 0.033), ('optimised', 0.033), ('brailovsky', 0.033), ('mackay', 0.031), ('tikhonov', 0.031), ('evidence', 0.031), ('breast', 0.03), ('automatic', 0.03), ('ard', 0.029), ('vandewalle', 0.029), ('arsenin', 0.029), ('yi', 0.029), ('keerthi', 0.028), ('suite', 0.028), ('nelder', 0.027), ('hyperparameters', 0.027), ('optimise', 0.027), ('optimising', 0.027), ('allen', 0.027), ('cancer', 0.027), ('benchmark', 0.026), ('eff', 0.026), ('holesky', 0.026), ('mead', 0.026), ('mickey', 0.026), ('mplementation', 0.026), ('nicola', 0.026), ('sii', 0.026), ('syminv', 0.026), ('uea', 0.026), ('inversion', 0.025), ('sm', 0.025), ('matrix', 0.025), ('conjugate', 0.025), ('cance', 0.024), ('german', 0.024), ('expense', 0.024), ('biases', 0.024), ('chapelle', 0.023), ('accordance', 0.022), ('tsch', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="68-tfidf-1" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>Author: Gavin C. Cawley, Nicola L. C. Talbot</p><p>Abstract: While the model parameters of a kernel machine are typically given by the solution of a convex optimisation problem, with a single global optimum, the selection of good values for the regularisation and kernel parameters is much less straightforward. Fortunately the leave-one-out cross-validation procedure can be performed or a least approximated very efﬁciently in closed form for a wide variety of kernel learning methods, providing a convenient means for model selection. Leave-one-out cross-validation based estimates of performance, however, generally exhibit a relatively high variance and are therefore prone to over-ﬁtting. In this paper, we investigate the novel use of Bayesian regularisation at the second level of inference, adding a regularisation term to the model selection criterion corresponding to a prior over the hyper-parameter values, where the additional regularisation parameters are integrated out analytically. Results obtained on a suite of thirteen real-world and synthetic benchmark data sets clearly demonstrate the beneﬁt of this approach. Keywords: model selection, kernel methods, Bayesian regularisation</p><p>2 0.046693802 <a title="68-tfidf-2" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>Author: Sébastien Gadat, Laurent Younes</p><p>Abstract: We introduce a new model addressing feature selection from a large dictionary of variables that can be computed from a signal or an image. Features are extracted according to an efﬁciency criterion, on the basis of speciﬁed classiﬁcation or recognition tasks. This is done by estimating a probability distribution P on the complete dictionary, which distributes its mass over the more efﬁcient, or informative, components. We implement a stochastic gradient descent algorithm, using the probability as a state variable and optimizing a multi-task goodness of ﬁt criterion for classiﬁers based on variable randomly chosen according to P. We then generate classiﬁers from the optimal distribution of weights learned on the training set. The method is ﬁrst tested on several pattern recognition problems including face detection, handwritten digit recognition, spam classiﬁcation and micro-array analysis. We then compare our approach with other step-wise algorithms like random forests or recursive feature elimination. Keywords: stochastic learning algorithms, Robbins-Monro application, pattern recognition, classiﬁcation algorithm, feature selection</p><p>3 0.046095669 <a title="68-tfidf-3" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>Author: Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee, Robert L. Wolpert</p><p>Abstract: Kernel methods have been very popular in the machine learning literature in the last ten years, mainly in the context of Tikhonov regularization algorithms. In this paper we study a coherent Bayesian kernel model based on an integral operator deﬁned as the convolution of a kernel with a signed measure. Priors on the random signed measures correspond to prior distributions on the functions mapped by the integral operator. We study several classes of signed measures and their image mapped by the integral operator. In particular, we identify a general class of measures whose image is dense in the reproducing kernel Hilbert space (RKHS) induced by the kernel. A consequence of this result is a function theoretic foundation for using non-parametric prior speciﬁcations in Bayesian modeling, such as Gaussian process and Dirichlet process prior distributions. We discuss the construction of priors on spaces of signed measures using Gaussian and L´ vy processes, e with the Dirichlet processes being a special case the latter. Computational issues involved with sampling from the posterior distribution are outlined for a univariate regression and a high dimensional classiﬁcation problem. Keywords: reproducing kernel Hilbert space, non-parametric Bayesian methods, L´ vy processes, e Dirichlet processes, integral operator, Gaussian processes c 2007 Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee and Robert L. Wolpert. P ILLAI , W U , L IANG , M UKHERJEE AND W OLPERT</p><p>4 0.044460744 <a title="68-tfidf-4" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>5 0.040539857 <a title="68-tfidf-5" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>6 0.035367437 <a title="68-tfidf-6" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>7 0.032381091 <a title="68-tfidf-7" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>8 0.031112768 <a title="68-tfidf-8" href="./jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</a></p>
<p>9 0.030951273 <a title="68-tfidf-9" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>10 0.030624636 <a title="68-tfidf-10" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>11 0.030555885 <a title="68-tfidf-11" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>12 0.030448712 <a title="68-tfidf-12" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>13 0.028659036 <a title="68-tfidf-13" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>14 0.027611565 <a title="68-tfidf-14" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>15 0.027150955 <a title="68-tfidf-15" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>16 0.026616389 <a title="68-tfidf-16" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>17 0.025753083 <a title="68-tfidf-17" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>18 0.023697613 <a title="68-tfidf-18" href="./jmlr-2007-A_Complete_Characterization_of_a_Family_of_Solutions_to_a_Generalized_Fisher_Criterion.html">2 jmlr-2007-A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion</a></p>
<p>19 0.023395441 <a title="68-tfidf-19" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>20 0.023185693 <a title="68-tfidf-20" href="./jmlr-2007-Bayesian_Quadratic_Discriminant_Analysis.html">13 jmlr-2007-Bayesian Quadratic Discriminant Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.152), (1, 0.036), (2, 0.047), (3, 0.073), (4, -0.028), (5, -0.016), (6, -0.044), (7, -0.1), (8, -0.013), (9, 0.038), (10, -0.077), (11, -0.026), (12, 0.01), (13, -0.052), (14, -0.036), (15, 0.075), (16, -0.06), (17, -0.037), (18, -0.052), (19, -0.122), (20, 0.158), (21, -0.13), (22, 0.026), (23, 0.051), (24, 0.019), (25, -0.07), (26, -0.168), (27, 0.152), (28, -0.106), (29, 0.016), (30, -0.191), (31, 0.05), (32, 0.189), (33, 0.328), (34, -0.122), (35, -0.045), (36, -0.149), (37, 0.023), (38, -0.061), (39, -0.032), (40, 0.077), (41, 0.092), (42, 0.121), (43, 0.298), (44, 0.261), (45, 0.328), (46, -0.166), (47, -0.028), (48, 0.073), (49, 0.281)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93292701 <a title="68-lsi-1" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>Author: Gavin C. Cawley, Nicola L. C. Talbot</p><p>Abstract: While the model parameters of a kernel machine are typically given by the solution of a convex optimisation problem, with a single global optimum, the selection of good values for the regularisation and kernel parameters is much less straightforward. Fortunately the leave-one-out cross-validation procedure can be performed or a least approximated very efﬁciently in closed form for a wide variety of kernel learning methods, providing a convenient means for model selection. Leave-one-out cross-validation based estimates of performance, however, generally exhibit a relatively high variance and are therefore prone to over-ﬁtting. In this paper, we investigate the novel use of Bayesian regularisation at the second level of inference, adding a regularisation term to the model selection criterion corresponding to a prior over the hyper-parameter values, where the additional regularisation parameters are integrated out analytically. Results obtained on a suite of thirteen real-world and synthetic benchmark data sets clearly demonstrate the beneﬁt of this approach. Keywords: model selection, kernel methods, Bayesian regularisation</p><p>2 0.34630349 <a title="68-lsi-2" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>3 0.28467572 <a title="68-lsi-3" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>Author: Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee, Robert L. Wolpert</p><p>Abstract: Kernel methods have been very popular in the machine learning literature in the last ten years, mainly in the context of Tikhonov regularization algorithms. In this paper we study a coherent Bayesian kernel model based on an integral operator deﬁned as the convolution of a kernel with a signed measure. Priors on the random signed measures correspond to prior distributions on the functions mapped by the integral operator. We study several classes of signed measures and their image mapped by the integral operator. In particular, we identify a general class of measures whose image is dense in the reproducing kernel Hilbert space (RKHS) induced by the kernel. A consequence of this result is a function theoretic foundation for using non-parametric prior speciﬁcations in Bayesian modeling, such as Gaussian process and Dirichlet process prior distributions. We discuss the construction of priors on spaces of signed measures using Gaussian and L´ vy processes, e with the Dirichlet processes being a special case the latter. Computational issues involved with sampling from the posterior distribution are outlined for a univariate regression and a high dimensional classiﬁcation problem. Keywords: reproducing kernel Hilbert space, non-parametric Bayesian methods, L´ vy processes, e Dirichlet processes, integral operator, Gaussian processes c 2007 Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee and Robert L. Wolpert. P ILLAI , W U , L IANG , M UKHERJEE AND W OLPERT</p><p>4 0.24966586 <a title="68-lsi-4" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>Author: Sébastien Gadat, Laurent Younes</p><p>Abstract: We introduce a new model addressing feature selection from a large dictionary of variables that can be computed from a signal or an image. Features are extracted according to an efﬁciency criterion, on the basis of speciﬁed classiﬁcation or recognition tasks. This is done by estimating a probability distribution P on the complete dictionary, which distributes its mass over the more efﬁcient, or informative, components. We implement a stochastic gradient descent algorithm, using the probability as a state variable and optimizing a multi-task goodness of ﬁt criterion for classiﬁers based on variable randomly chosen according to P. We then generate classiﬁers from the optimal distribution of weights learned on the training set. The method is ﬁrst tested on several pattern recognition problems including face detection, handwritten digit recognition, spam classiﬁcation and micro-array analysis. We then compare our approach with other step-wise algorithms like random forests or recursive feature elimination. Keywords: stochastic learning algorithms, Robbins-Monro application, pattern recognition, classiﬁcation algorithm, feature selection</p><p>5 0.19314085 <a title="68-lsi-5" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>6 0.18505792 <a title="68-lsi-6" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>7 0.18077773 <a title="68-lsi-7" href="./jmlr-2007-Nonlinear_Boosting_Projections_for_Ensemble_Construction.html">59 jmlr-2007-Nonlinear Boosting Projections for Ensemble Construction</a></p>
<p>8 0.17434889 <a title="68-lsi-8" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>9 0.16972402 <a title="68-lsi-9" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>10 0.16138247 <a title="68-lsi-10" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>11 0.15867153 <a title="68-lsi-11" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>12 0.15740824 <a title="68-lsi-12" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>13 0.15432483 <a title="68-lsi-13" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>14 0.14967595 <a title="68-lsi-14" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>15 0.14242738 <a title="68-lsi-15" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>16 0.14095159 <a title="68-lsi-16" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>17 0.13945436 <a title="68-lsi-17" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>18 0.13449991 <a title="68-lsi-18" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>19 0.13310345 <a title="68-lsi-19" href="./jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</a></p>
<p>20 0.12781905 <a title="68-lsi-20" href="./jmlr-2007-Covariate_Shift_Adaptation_by_Importance_Weighted_Cross_Validation.html">25 jmlr-2007-Covariate Shift Adaptation by Importance Weighted Cross Validation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.025), (8, 0.035), (10, 0.014), (12, 0.029), (15, 0.023), (17, 0.402), (28, 0.053), (40, 0.039), (45, 0.03), (48, 0.032), (60, 0.034), (80, 0.024), (85, 0.057), (98, 0.105)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.66781628 <a title="68-lda-1" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>Author: Gavin C. Cawley, Nicola L. C. Talbot</p><p>Abstract: While the model parameters of a kernel machine are typically given by the solution of a convex optimisation problem, with a single global optimum, the selection of good values for the regularisation and kernel parameters is much less straightforward. Fortunately the leave-one-out cross-validation procedure can be performed or a least approximated very efﬁciently in closed form for a wide variety of kernel learning methods, providing a convenient means for model selection. Leave-one-out cross-validation based estimates of performance, however, generally exhibit a relatively high variance and are therefore prone to over-ﬁtting. In this paper, we investigate the novel use of Bayesian regularisation at the second level of inference, adding a regularisation term to the model selection criterion corresponding to a prior over the hyper-parameter values, where the additional regularisation parameters are integrated out analytically. Results obtained on a suite of thirteen real-world and synthetic benchmark data sets clearly demonstrate the beneﬁt of this approach. Keywords: model selection, kernel methods, Bayesian regularisation</p><p>2 0.34197795 <a title="68-lda-2" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>Author: Guy Lebanon, Yi Mao, Joshua Dillon</p><p>Abstract: The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efﬁcient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks. Keywords: text processing, local smoothing</p><p>3 0.34126323 <a title="68-lda-3" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>4 0.34025103 <a title="68-lda-4" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis. Keywords: embedding algorithms, manifold learning, exponential families, multidimensional scaling, matrix factorization, semideﬁnite programming</p><p>5 0.34007263 <a title="68-lda-5" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>Author: Wei Pan, Xiaotong Shen</p><p>Abstract: Variable selection in clustering analysis is both challenging and important. In the context of modelbased clustering analysis with a common diagonal covariance matrix, which is especially suitable for “high dimension, low sample size” settings, we propose a penalized likelihood approach with an L1 penalty function, automatically realizing variable selection via thresholding and delivering a sparse solution. We derive an EM algorithm to ﬁt our proposed model, and propose a modiﬁed BIC as a model selection criterion to choose the number of components and the penalization parameter. A simulation study and an application to gene function prediction with gene expression proﬁles demonstrate the utility of our method. Keywords: BIC, EM, mixture model, penalized likelihood, soft-thresholding, shrinkage</p><p>6 0.33936524 <a title="68-lda-6" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>7 0.33637494 <a title="68-lda-7" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>8 0.33636987 <a title="68-lda-8" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>9 0.33560801 <a title="68-lda-9" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>10 0.3323442 <a title="68-lda-10" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>11 0.33198464 <a title="68-lda-11" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>12 0.33145344 <a title="68-lda-12" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>13 0.33053255 <a title="68-lda-13" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>14 0.3299191 <a title="68-lda-14" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>15 0.32943922 <a title="68-lda-15" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>16 0.32940176 <a title="68-lda-16" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>17 0.32801437 <a title="68-lda-17" href="./jmlr-2007-Stagewise_Lasso.html">77 jmlr-2007-Stagewise Lasso</a></p>
<p>18 0.32792914 <a title="68-lda-18" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>19 0.32754007 <a title="68-lda-19" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>20 0.32710057 <a title="68-lda-20" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
