<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-53" href="#">jmlr2007-53</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</h1>
<br/><p>Source: <a title="jmlr-2007-53-pdf" href="http://jmlr.org/papers/volume8/dudik07a/dudik07a.pdf">pdf</a></p><p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We present a uniﬁed and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including 1 , 2 , 2 , and 1 + 2 style regularization. We propose an algorithm solving a large and 2 2 general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and uniﬁes techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models. Keywords: maximum entropy, density estimation, regularization, iterative scaling, species distribution modeling</p><p>Reference: <a title="jmlr-2007-53-reference" href="../jmlr2007_reference/jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. [sent-14, score-0.294]
</p><p>2 ı  D UD´K , P HILLIPS AND S CHAPIRE I  Figure 1: Left to right: Yellow-throated Vireo training localities from the ﬁrst random partition, an example environmental variable (annual average temperature, higher values in red), maxent prediction using linear, quadratic and product features. [sent-25, score-0.856]
</p><p>3 ) The work in this paper was motivated by a new application of maxent to the problem of modeling the distribution of a plant or animal species, a critical problem in conservation biology. [sent-31, score-0.823]
</p><p>4 Input data for species distribution modeling consists of occurrence locations of a particular species in a region and of environmental variables for that region. [sent-32, score-0.521]
</p><p>5 It should not be surprising that maxent can severely overﬁt training data when the constraints on the output distribution are based on empirical averages, as described above, especially if there is a very large number of features. [sent-37, score-0.747]
</p><p>6 Thus, there are many ways of modifying maxent to control overﬁtting calling for a general treatment. [sent-46, score-0.747]
</p><p>7 Although mentioned by other authors as fuzzy maxent (Lau, 1994; Chen and Rosenfeld, 2000; Lebanon and Lafferty, 2001), we give the ﬁrst complete theoretical treatment of this very general framework, including fully general and uniﬁed performance guarantees, algorithms, and convergence proofs. [sent-48, score-0.747]
</p><p>8 More speciﬁcally, we 2 2 derive guarantees on the performance of maxent solutions compared to the “best” Gibbs distribution q deﬁned by a weight vector λ . [sent-51, score-0.789]
</p><p>9 For an inﬁnite set of binary features with VC-dimension d, the difference between the 1 -regularized maxent solution and q is at most O( λ 1 d ln(m2 /d)/m). [sent-54, score-0.797]
</p><p>10 For maxent with 2 and 2 -style regularization, it is possible to 2 obtain bounds which are independent of the number of features, provided that the feature vector can be bounded in the 2 norm. [sent-56, score-0.769]
</p><p>11 In the second part, we propose algorithms solving a large and general subclass of generalized maxent problems. [sent-57, score-0.777]
</p><p>12 The main novel ingredient is a modiﬁed deﬁnition of an auxiliary function, a customary measure of progress, which we view as a surrogate for the difference between the primal and dual objective rather than a bound on the change in the dual objective. [sent-62, score-0.291]
</p><p>13 Standard maxent algorithms such as iterative scaling (Darroch and Ratcliff, 1972; Della Pietra et al. [sent-63, score-0.747]
</p><p>14 1219  D UD´K , P HILLIPS AND S CHAPIRE I  For cases when the number of features is relatively small, yet we want to use beneﬁts of regularization to prevent overﬁtting on small sample sets, it might be more efﬁcient to solve generalized maxent by parallel updates. [sent-74, score-0.916]
</p><p>15 In particular, we apply 1 -regularized maxent to estimate distributions of bird species in North America. [sent-77, score-0.96]
</p><p>16 1 Previous Work There have been many studies of maxent and logistic regression, which is a conditional version of maxent, with 1 -style regularization (Khudanpur, 1995; Williams, 1995; Kazama and Tsujii, 2003; Ng, 2004; Goodman, 2004; Krishnapuram et al. [sent-83, score-0.836]
</p><p>17 In a recent work, Altun and Smola (2006) derive duality and performance guarantees for settings in which the entropy is replaced by an arbitrary Bregman or Csisz´ r divergence and regularization takes the form of a norm a raised to a power greater than one. [sent-86, score-0.295]
</p><p>18 The latter work evaluates 1 -regularized maxent as one of a group of twelve methods in the task of modeling species distributions. [sent-100, score-0.971]
</p><p>19 1220  M AXIMUM E NTROPY D ENSITY E STIMATION  Among these, however, maxent is the only method designed for presence-only data. [sent-105, score-0.747]
</p><p>20 Here, as usual, the entropy of a distribution p on X is deﬁned to be H(p) = − ∑x∈X p(x) ln p(x). [sent-126, score-0.283]
</p><p>21 The conjugate of relative entropy is the log partition function ψ∗ (r) = ln ∑x∈X q0 (x)er(x) where r ∈ RX and its components are denoted by r(x). [sent-163, score-0.386]
</p><p>22 1222  M AXIMUM E NTROPY D ENSITY E STIMATION  The second example is the unnormalized relative entropy D(p q0 ) = ∑x∈X p(x) ln  p(x) − p(x) + q0 (x) q0 (x)  . [sent-164, score-0.311]
</p><p>23 Generalized Maximum Entropy In this paper we study a generalized maxent problem  P:  min D(p q0 ) + U(p[f]) p∈∆  where U : Rn → (−∞, ∞] is an arbitrary closed proper convex function. [sent-190, score-0.881]
</p><p>24 It is viewed as a potential for the maxent problem. [sent-191, score-0.819]
</p><p>25 The deﬁnition of generalized maxent captures many cases of interest including basic maxent, 1 regularized maxent and 2 -regularized maxent. [sent-194, score-1.524]
</p><p>26 Basic maxent is obtained by using a point indicator 2 potential U(0) (u) = I(u = π[f]). [sent-195, score-0.819]
</p><p>27 Finally, as pointed out by Chen and Rosenfeld (2000) and Lebanon and Lafferty (2001), 2 -regularized maxent is obtained using the potential U(2) (u) = π[f] − u 2 /(2α) which ˜ 2 2 2 -style penalty for deviating from empirical averages. [sent-199, score-0.819]
</p><p>28 incurs an 2 The primal objective of generalized maxent will be referred to as P: P(p) = D(p q0 ) + U(p[f]) . [sent-200, score-0.872]
</p><p>29 To derive the dual of P , deﬁne the matrix F jx = f j (x) and use Fenchel’s duality: min [D(p q0 ) + U(p[f])] = min [D(p q0 ) + U(Fp)] p∈∆  p∈∆  = sup − ln ∑x∈X q0 (x) exp (F λ)x λ∈Rn  = sup [− ln Zλ − U∗ (−λ)] . [sent-203, score-0.568]
</p><p>30 The dual objective will be referred to as Q: Q(λ) = − ln Zλ − U∗ (−λ) . [sent-207, score-0.308]
</p><p>31 There are two formal differences between generalized maxent and basic maxent. [sent-208, score-0.777]
</p><p>32 The potentials U(0) , U(1) , U(2) of basic maxent, maxent with (0) box constraints, and maxent with 2 penalty could thus have been speciﬁed by deﬁning U π (u) = ˜ 2 (1) (2) I(u = 0), Uπ (u) = I(|u j | ≤ β j for all j) and Uπ (u) = u 2 /(2α). [sent-227, score-1.587]
</p><p>33 This will highlight how the dual of the generalized maxent extends the dual of the basic maxent. [sent-232, score-0.913]
</p><p>34 Using Equation (7), we rewrite Q(λ) as follows: Q(λ) = − ln Zλ − U∗ (−λ) = − ln Zλ − U∗ (λ) + λ · r[f] r = −r[ln q0 ] + r[ln q0 + λ · f − ln Zλ ] − U∗ (λ) r  = Lr (0) − Lr (λ) − U∗ (λ) . [sent-233, score-0.639]
</p><p>35 To capture the more general case, we formulate the generalized maxent using the absolute potential. [sent-251, score-0.777]
</p><p>36 3 Maxent Duality We know from Equation (6) that the generalized maxent primal and dual have equal values. [sent-253, score-0.913]
</p><p>37 Speciﬁcally, we show that the maxent primal P is solved by the Gibbs distribution whose parameter vector λ solves the dual 1226  M AXIMUM E NTROPY D ENSITY E STIMATION  (possibly in a limit). [sent-255, score-0.883]
</p><p>38 This parallels the result of Della Pietra, Della Pietra, and Lafferty (1997) for the basic maxent and gives additional motivation for the view of the dual objective as the regularized log loss. [sent-256, score-0.864]
</p><p>39 such that lim Q(λt ) = sup Q(λ)  t→∞  λ∈Rn  the sequence of qt = qλt has a limit and P lim qt = min P(p) . [sent-263, score-0.381]
</p><p>40 Since the qt ’s come from the compact set ∆, we obtain qt → p. [sent-281, score-0.298]
</p><p>41 Bounding the Loss on the Target Distribution In this section, we derive bounds on the performance of generalized maxent relative to the true ˆ distribution π. [sent-285, score-0.805]
</p><p>42 π The ﬁnal result for 1 -regularized maxent is motivated by the Central Limit Theorem approx√ imation |˜ [ f j ] − π[ f j ]| = O(σ[ f j ]/ m), where σ[ f j ] is the standard deviation of f j under π. [sent-347, score-0.747]
</p><p>43 Let D∞ = supx,x ∈X f(x) − f(x ) ∞ be the ˆ let λ minimize Lπ (λ) + αβ∑ j ln cosh(λ j /α) with ˜ α=  εL1 , n ln 2  β = D∞  ∞  diameter of f(X ). [sent-389, score-0.474]
</p><p>44 Thus, maxent with smoothed 1 regularization performs almost as well as 1 -regularized maxent, provided that we specify an upper bound on the 1 norm of λ in advance. [sent-392, score-0.888]
</p><p>45 An analogous bound can also be obtained for 1 -regularized maxent in terms of the ∞ diameter of the feature space (relaxing the requirement of Corollary 5 that features be bounded in [0, 1]): ˆ Lπ (λ) ≤ Lπ (λ ) +  λ  D √1 ∞ m  2 ln(2n/δ) . [sent-412, score-0.893]
</p><p>46 The guarantee for 2 -regularized maxent then grows as Ω( n) while the guarantee for 1 √ regularized maxent grows only as Ω( ln n). [sent-417, score-1.707]
</p><p>47 Note, however, that in practice the distribution returned by 2 -regularized maxent may perform better than indicated by this guarantee. [sent-418, score-0.747]
</p><p>48 ˜ 2 In the case of 2 -regularized maxent it is possible to derive guarantees on the expected per2 formance in addition to probabilistic guarantees. [sent-424, score-0.789]
</p><p>49 5 Maxent with  2  Regularization versus  2 2  Regularization  In the previous two sections we have seen that performance guarantees for maxent with 2 and 2 2 regularization differ whenever we require that β and α be ﬁxed before running the algorithm. [sent-451, score-0.878]
</p><p>50 We now show that if all possible values of β and α are considered then the sets of models generated by the two maxent versions are the same. [sent-452, score-0.747]
</p><p>51 √ Let Λ( 2),β and Λ(2),α denote the respective solution sets for maxent with 2 and 2 regularization: 2 Λ(  √ 2),β  = arg min [Lπ (λ) + β λ 2 ] ˜ n  (20)  λ∈R  Λ(2),α = arg min Lπ (λ) + α λ 2 /2 ˜ 2 n λ∈R  . [sent-453, score-0.747]
</p><p>52 This set will be empty if the basic maxent solutions are attained only in a limit. [sent-457, score-0.747]
</p><p>53 1 The main implication of Theorem 14 is for maxent density estimation with model selection, for example, by minimization of held-out or cross-validated empirical error. [sent-470, score-0.747]
</p><p>54 In those cases, maxent versions with 2 , 2 (and an 2 -ball indicator) regularization yield the same solution. [sent-471, score-0.836]
</p><p>55 This will typically be 2 -regularized maxent 2 whose potential and regularization are smooth. [sent-473, score-0.908]
</p><p>56 Setting δ = s/m, we bound the difference in performance between the maxent distribution and √ √ any Gibbs distribution of a bounded weight vector by O D∞ λ 1 ln(2mn/s) + D2 L2 s / m . [sent-496, score-0.795]
</p><p>57 For instance, for an arbitrary norm · B ∗ , the regularization function β λ B ∗ corresponds to the potential IB (u) where B = { u B ≤ β} and · B is the dual norm of · B ∗ . [sent-502, score-0.302]
</p><p>58 Now we turn our attention to algorithms for solving generalized maxent problems. [sent-508, score-0.777]
</p><p>59 In the present and the following section, we propose two algorithms for generalized maxent with complete proofs of convergence. [sent-509, score-0.777]
</p><p>60 The 2 -ball potential U(˜ 2) does not fall in this class, but we show that the corresponding π maxent problem can be reduced and our algorithms can still be applied. [sent-511, score-0.819]
</p><p>61 There are a number of algorithms for ﬁnding the basic maxent distribution, especially iterative scaling and its variants (Darroch and Ratcliff, 1972; Della Pietra et al. [sent-512, score-0.747]
</p><p>62 This approach is particularly useful in the context of 1 -regularized maxent which often yields sparse solutions. [sent-523, score-0.747]
</p><p>63 A potential U : Rn → (−∞, ∞] is called decomposable if it can be written as a sum of coordinate potentials U(u) = ∑ j U j (u j ), each of which is a closed proper convex function bounded from below. [sent-529, score-0.325]
</p><p>64 In Appendix G we show that a generalized maxent problem with a decomposable potential can always be reduced to the non-degenerate form. [sent-542, score-0.902]
</p><p>65 For maxent with box constraints (which subsumes the basic maxent), the optimizing δ can be derived explicitly. [sent-555, score-0.772]
</p><p>66 First note that Fj(1) (λ, δ) = − ln 1 + (eδ − 1)qλ [ f j ] − U(1)∗ (−λ j − δ) + U(1)∗ (−λ j ) j j = − ln 1 + (eδ − 1)qλ [ f j ] + δ˜ [ f j ] − β j (|λ j + δ| − |λ j |) π 1237  D UD´K , P HILLIPS AND S CHAPIRE I  Input: ﬁnite domain X , default estimate q0 examples x1 , . [sent-556, score-0.426]
</p><p>67 In order to ˜ reduce 2 -regularized maxent to maxent with a decomposable potential, we replace the constraint π[f] − p[f] 2 ≤ β by π[f] − p[f] 2 ≤ β2 which yields an equivalent primal: ˜ ˜ 2  P :  min D(p q0 ) subject to π[f] − p[f] ˜ p∈∆  2 2  ≤ β2 . [sent-578, score-1.547]
</p><p>68 One measure of progress is the difference between the primal evaluated at q λ and the dual evaluated at λ: P(qλ ) − Q(λ) = [D(qλ q0 ) + U(qλ [f])] − [− ln Zλ − U∗ (−λ)]  = qλ [λ · f − ln Zλ ] + U(qλ [f]) + qλ [ln Zλ ] + U∗ (−λ)  = U(qλ [f]) + U∗ (−λ) + λ · qλ [f] . [sent-584, score-0.562]
</p><p>69 Moreover, if A(λ, a) = 0 then qλ [f] = a and A(λ, a) = P(qλ ) − Q(λ) = 0, that is, by maxent duality, qλ solves the primal and λ solves the dual. [sent-601, score-0.815]
</p><p>70 Finally, combining Equations (33– ˆ 35), we obtain by maxent duality that q minimizes the primal and λ τ maximizes the dual as τ → ∞. [sent-656, score-0.93]
</p><p>71 The value of relative entropy D((a , a) (1 − qt [ f j ], qt [ f j ])) is inﬁnite whenever (a , a) is not a probability distribution, so it sufﬁces to consider pairs where 0 ≤ a ≤ 1 and a = 1 − a. [sent-688, score-0.396]
</p><p>72 In Equation (38), we use D(a qt [ f j ]) as a shorthand for D((1 − a, a) (1 − qt [ f j ], qt [ f j ])). [sent-689, score-0.447]
</p><p>73 In this section, we describe a variant of generalized iterative scaling (Darroch and Ratcliff, 1972) applicable to generalized maxent with an arbitrary decomposable potential and prove its convergence. [sent-700, score-0.953]
</p><p>74 ) Thus, ln qλ exp ∑ j δ j f j  ≤ ln qλ 1 + ∑ j f j (eδ j − 1)  = ln 1 + ∑ j qλ [ f j ](eδ j − 1)  ≤ ∑ j qλ [ f j ](eδ j − 1)  since ln(1 + x) ≤ x for all x > −1. [sent-724, score-0.639]
</p><p>75 We can rewrite Ft using Fenchel’s duality: Ft = sup ∑ −qt [ f j ](eδ j − 1) − U j∗ (−δ j ) + U∗ (−λt ) δ  (41)  j  = inf ∑ D(a j a≥0 j  qt [ f j ]) + U j (a j ) + U∗ (−λt )  = inf D(a qt [f]) + U(a) + λt · a + U∗ (−λt ) . [sent-740, score-0.335]
</p><p>76 Species Distribution Modeling Experiments In this section we study how generalized maxent can be applied to the problem of modeling geographic distributions of species. [sent-749, score-0.843]
</p><p>77 A model of the ecological niche can further be used to predict the set of locations with sufﬁcient conditions for the species to persist, that is, the potential distribution of the species (Anderson and Mart´nez-Meyer, 2004; Phillips et al. [sent-751, score-0.525]
</p><p>78 To explore the utility of generalized maxent and effects of regularization, we used 1 -regularized maxent to model distributions of bird species, based on occurrence records in the North American Breeding Bird Survey (Sauer et al. [sent-768, score-1.58]
</p><p>79 The impact of sample selection bias on maxent models, and various ways of coping with it are explored by Dud´k, Schapire, and Phillips (2005). [sent-773, score-0.747]
</p><p>80 ı A comprehensive comparison of maxent and other species distribution modeling techniques was carried out by Elith et al. [sent-775, score-0.971]
</p><p>81 In that comparison, maxent is in the group of the best-performing methods. [sent-777, score-0.747]
</p><p>82 all environmental variables, we also used raw environmental variables (linear features), squares of environmental variables (quadratic features), and products of pairs of environmental variables (product features). [sent-819, score-0.328]
</p><p>83 On each training set, we ran maxent with four different subsets of the feature types: linear (L); linear and quadratic (LQ); linear, quadratic and product (LQP); and threshold (T). [sent-832, score-0.774]
</p><p>84 First, we ran maxent on increasing subsets of the training data and evaluated log loss on the test data. [sent-834, score-0.769]
</p><p>85 In addition to these curves, we show how Gibbs distributions returned by maxent can be interpreted in terms of contribution of individual environmental variables to the exponent. [sent-839, score-0.829]
</p><p>86 We give examples of feature proﬁles returned by maxent with and without regularization. [sent-841, score-0.747]
</p><p>87 For a ﬁxed value of β0 , maxent ﬁnds better solutions (with smaller log loss) as the number of examples grows. [sent-898, score-0.769]
</p><p>88 In the absence of regularization, maxent would exactly ﬁt the training data with delta functions around sample values of the environmental variables which would result in severe overﬁtting even when the number of training examples is large. [sent-907, score-0.829]
</p><p>89 As the learning curves show, regularized maxent does not exhibit this behavior. [sent-908, score-0.747]
</p><p>90 Figure 7 shows the sensitivity of maxent to the regularization value β 0 for LQP and T versions of maxent. [sent-912, score-0.836]
</p><p>91 To derive feature proﬁles, recall that maxent with a uniform default distribution returns the Gibbs distribution qλ (x) = eλ·f(x) /Zλ minimizing the regularized log loss. [sent-929, score-0.769]
</p><p>92 Such interpretations should be made with caution as the objective of maxent is based solely on the predictive performance. [sent-944, score-0.774]
</p><p>93 In this work, we have provided a uniﬁed and complete account of maxent with generalized regularization. [sent-951, score-0.777]
</p><p>94 We have explored one direction of generalizing maxent: replacing equality constraints by an arbitrary convex potential in the primal or, equivalently, adding a convex regularization term to the maximum likelihood estimation in the dual. [sent-960, score-0.326]
</p><p>95 Finally, we have demonstrated the utility of generalized maxent in a novel application to species distribution modeling. [sent-965, score-0.963]
</p><p>96 Even though maxent ﬁts the problem of species distribution modeling cleanly and effectively, there are many other techniques that could be used such as Markov random ﬁelds or mixture models. [sent-967, score-0.971]
</p><p>97 Note that the supremum in Equation (44) takes form of a dual objective in a basic maxent over a two-sample space, say X = {0, 1}, with a single ˜ feature f (0) = β j , f (1) = −β j , and the empirical expectation π[ f ] = u j . [sent-1007, score-0.864]
</p><p>98 Thus, by maxent duality, the value of the supremum equals D(p (1/2, 1/2)), where p comes from a closure of the set of Gibbs distribution and p[ f ] = u j . [sent-1008, score-0.769]
</p><p>99 By the convexity of g(λ) and α λ 2 /2, the gradients 2 of α λ 2 α 2 = ln Zλ − λ · π[f] + 2 α λ 2 α 2 Lπ (λ) + = ln Zλ − λ · π[f] + ˜ ˜ 2 at their respective minima must equal zero: Lπ (λ) +  λ 2 λ 2  2 2 2 2  ∇g(λ ) − π[f] + αλ = 0 ˆ ˆ ∇g(λ) − π[f] + αλ = 0 . [sent-1052, score-0.426]
</p><p>100 Rewrite F j (λ, δ) as follows: Fj (λ, δ) = − ln 1 + (eδ − 1)qλ [ f j ] − U∗ (−λ j − δ) + U∗ (−λ j ) j j = − ln eδ e−δ (1 − qλ [ f j ]) + qλ [ f j ]  + δr[ f j ] − U∗ j (λ j + δ) + U∗ j (λ j ) r, r,  = − ln e−δ (1 − qλ [ f j ]) + qλ [ f j ] − δ(1 − r[ f j ]) − U∗ j (λ j + δ) + U∗ j (λ j ). [sent-1117, score-0.639]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('maxent', 0.747), ('ln', 0.213), ('species', 0.186), ('qt', 0.149), ('chapire', 0.138), ('hillips', 0.138), ('ud', 0.138), ('ur', 0.133), ('ensity', 0.132), ('ntropy', 0.112), ('fenchel', 0.106), ('aximum', 0.1), ('stimation', 0.091), ('regularization', 0.089), ('fj', 0.085), ('gibbs', 0.083), ('environmental', 0.082), ('summet', 0.075), ('potential', 0.072), ('entropy', 0.07), ('della', 0.069), ('dual', 0.068), ('potentials', 0.068), ('primal', 0.068), ('dud', 0.064), ('pietra', 0.061), ('ft', 0.057), ('vireo', 0.056), ('conjugate', 0.053), ('lq', 0.053), ('decomposable', 0.053), ('features', 0.05), ('ecological', 0.05), ('plummet', 0.05), ('dom', 0.049), ('diameter', 0.048), ('duality', 0.047), ('lqp', 0.044), ('lemma', 0.043), ('uq', 0.042), ('guarantees', 0.042), ('ces', 0.041), ('phillips', 0.04), ('rn', 0.039), ('modeling', 0.038), ('conservation', 0.038), ('darroch', 0.038), ('elith', 0.038), ('kazama', 0.038), ('convex', 0.038), ('sup', 0.037), ('bregman', 0.036), ('inequality', 0.035), ('north', 0.034), ('auxiliary', 0.034), ('pro', 0.033), ('biodiversity', 0.031), ('climate', 0.031), ('niche', 0.031), ('ratcliff', 0.031), ('tsujii', 0.031), ('rosenfeld', 0.03), ('les', 0.03), ('generalized', 0.03), ('occurrence', 0.029), ('schapire', 0.029), ('mcdiarmid', 0.028), ('altun', 0.028), ('geographic', 0.028), ('relative', 0.028), ('equation', 0.027), ('threshold', 0.027), ('objective', 0.027), ('coordinate', 0.027), ('lr', 0.027), ('localities', 0.027), ('bird', 0.027), ('elevation', 0.027), ('norm', 0.026), ('bound', 0.026), ('hutton', 0.025), ('leathwick', 0.025), ('box', 0.025), ('lebanon', 0.024), ('krishnapuram', 0.024), ('lau', 0.024), ('lim', 0.023), ('closed', 0.023), ('lafferty', 0.023), ('exponent', 0.023), ('degenerate', 0.023), ('proper', 0.022), ('log', 0.022), ('bounded', 0.022), ('divergences', 0.022), ('supremum', 0.022), ('decomposability', 0.021), ('peterson', 0.021), ('corollary', 0.021), ('arbitrary', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="53-tfidf-1" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We present a uniﬁed and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including 1 , 2 , 2 , and 1 + 2 style regularization. We propose an algorithm solving a large and 2 2 general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and uniﬁes techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models. Keywords: maximum entropy, density estimation, regularization, iterative scaling, species distribution modeling</p><p>2 0.12452777 <a title="53-tfidf-2" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>Author: András György, Tamás Linder, Gábor Lugosi, György Ottucsák</p><p>Abstract: The on-line shortest path problem is considered under various models of partial monitoring. Given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (deﬁned as the sum of the weights of its composing edges) be as small as possible. In a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. For this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is √ proportional to 1/ n and depends only polynomially on the number of edges of the graph. The algorithm can be implemented with complexity that is linear in the number of rounds n (i.e., the average complexity per round is constant) and in the number of edges. An extension to the so-called label efﬁcient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m n time instances. Another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. A version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. Applications to routing in packet switched networks along with simulation results are also presented. Keywords: on-line learning, shortest path problem, multi-armed bandit problem c 2007 Andr´ s Gy¨ rgy, Tam´ s Linder, G´ bor Lugosi and Gy¨ rgy Ottucs´ k. a o a a o a ¨ ´ G Y ORGY, L INDER , L UGOSI AND OTTUCS AK</p><p>3 0.11188533 <a title="53-tfidf-3" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>Author: Ryan M. Rifkin, Ross A. Lippert</p><p>Abstract: Regularization is an approach to function learning that balances ﬁt and smoothness. In practice, we search for a function f with a ﬁnite representation f = ∑i ci φi (·). In most treatments, the ci are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning. Keywords: kernel machines, duality, optimization, convex analysis, kernel learning</p><p>4 0.10091249 <a title="53-tfidf-4" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>Author: Peter L. Bartlett, Mikhail Traskin</p><p>Abstract: The risk, or probability of error, of the classiﬁer produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after n1−ε iterations—for sample size n and ε ∈ (0, 1)—the sequence of risks of the classiﬁers it produces approaches the Bayes risk. Keywords: boosting, adaboost, consistency</p><p>5 0.063462056 <a title="53-tfidf-5" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>Author: Mads Dyrholm, Christoforos Christoforou, Lucas C. Parra</p><p>Abstract: Factor analysis and discriminant analysis are often used as complementary approaches to identify linear components in two dimensional data arrays. For three dimensional arrays, which may organize data in dimensions such as space, time, and trials, the opportunity arises to combine these two approaches. A new method, Bilinear Discriminant Component Analysis (BDCA), is derived and demonstrated in the context of functional brain imaging data for which it seems ideally suited. The work suggests to identify a subspace projection which optimally separates classes while ensuring that each dimension in this space captures an independent contribution to the discrimination. Keywords: bilinear, decomposition, component, classiﬁcation, regularization</p><p>6 0.059017312 <a title="53-tfidf-6" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>7 0.053149424 <a title="53-tfidf-7" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>8 0.048303511 <a title="53-tfidf-8" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>9 0.045863584 <a title="53-tfidf-9" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>10 0.045501985 <a title="53-tfidf-10" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>11 0.043795209 <a title="53-tfidf-11" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>12 0.041327573 <a title="53-tfidf-12" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>13 0.040387139 <a title="53-tfidf-13" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>14 0.040206343 <a title="53-tfidf-14" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>15 0.039566617 <a title="53-tfidf-15" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>16 0.037294362 <a title="53-tfidf-16" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>17 0.035708357 <a title="53-tfidf-17" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>18 0.035529967 <a title="53-tfidf-18" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>19 0.032504257 <a title="53-tfidf-19" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>20 0.029492537 <a title="53-tfidf-20" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, -0.143), (2, -0.101), (3, -0.122), (4, 0.024), (5, -0.003), (6, -0.155), (7, 0.099), (8, -0.015), (9, -0.09), (10, 0.159), (11, 0.004), (12, -0.157), (13, -0.065), (14, -0.178), (15, 0.017), (16, 0.071), (17, -0.157), (18, -0.238), (19, -0.103), (20, -0.078), (21, -0.087), (22, -0.131), (23, -0.155), (24, -0.043), (25, -0.104), (26, 0.074), (27, -0.027), (28, -0.03), (29, -0.015), (30, 0.085), (31, 0.007), (32, 0.142), (33, 0.01), (34, -0.03), (35, -0.027), (36, -0.044), (37, 0.096), (38, 0.129), (39, 0.071), (40, 0.051), (41, 0.009), (42, 0.008), (43, 0.072), (44, 0.008), (45, -0.003), (46, -0.098), (47, -0.002), (48, 0.204), (49, -0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93134928 <a title="53-lsi-1" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We present a uniﬁed and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including 1 , 2 , 2 , and 1 + 2 style regularization. We propose an algorithm solving a large and 2 2 general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and uniﬁes techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models. Keywords: maximum entropy, density estimation, regularization, iterative scaling, species distribution modeling</p><p>2 0.51404691 <a title="53-lsi-2" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>Author: Ryan M. Rifkin, Ross A. Lippert</p><p>Abstract: Regularization is an approach to function learning that balances ﬁt and smoothness. In practice, we search for a function f with a ﬁnite representation f = ∑i ci φi (·). In most treatments, the ci are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning. Keywords: kernel machines, duality, optimization, convex analysis, kernel learning</p><p>3 0.51263994 <a title="53-lsi-3" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>Author: András György, Tamás Linder, Gábor Lugosi, György Ottucsák</p><p>Abstract: The on-line shortest path problem is considered under various models of partial monitoring. Given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (deﬁned as the sum of the weights of its composing edges) be as small as possible. In a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. For this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is √ proportional to 1/ n and depends only polynomially on the number of edges of the graph. The algorithm can be implemented with complexity that is linear in the number of rounds n (i.e., the average complexity per round is constant) and in the number of edges. An extension to the so-called label efﬁcient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m n time instances. Another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. A version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. Applications to routing in packet switched networks along with simulation results are also presented. Keywords: on-line learning, shortest path problem, multi-armed bandit problem c 2007 Andr´ s Gy¨ rgy, Tam´ s Linder, G´ bor Lugosi and Gy¨ rgy Ottucs´ k. a o a a o a ¨ ´ G Y ORGY, L INDER , L UGOSI AND OTTUCS AK</p><p>4 0.44871801 <a title="53-lsi-4" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>Author: Mads Dyrholm, Christoforos Christoforou, Lucas C. Parra</p><p>Abstract: Factor analysis and discriminant analysis are often used as complementary approaches to identify linear components in two dimensional data arrays. For three dimensional arrays, which may organize data in dimensions such as space, time, and trials, the opportunity arises to combine these two approaches. A new method, Bilinear Discriminant Component Analysis (BDCA), is derived and demonstrated in the context of functional brain imaging data for which it seems ideally suited. The work suggests to identify a subspace projection which optimally separates classes while ensuring that each dimension in this space captures an independent contribution to the discrimination. Keywords: bilinear, decomposition, component, classiﬁcation, regularization</p><p>5 0.39792636 <a title="53-lsi-5" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Kwangmoo Koh, Seung-Jean Kim, Stephen Boyd</p><p>Abstract: Logistic regression with 1 regularization has been proposed as a promising method for feature selection in classiﬁcation problems. In this paper we describe an efﬁcient interior-point method for solving large-scale 1 -regularized logistic regression problems. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC; medium sized problems, with tens of thousands of features and examples, can be solved in tens of seconds (assuming some sparsity in the data). A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve very large problems, with a million features and examples (e.g., the 20 Newsgroups data set), in a few minutes, on a PC. Using warm-start techniques, a good approximation of the entire regularization path can be computed much more efﬁciently than by solving a family of problems independently. Keywords: logistic regression, feature selection, 1 regularization, regularization path, interiorpoint methods.</p><p>6 0.3969239 <a title="53-lsi-6" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>7 0.27124456 <a title="53-lsi-7" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>8 0.22268946 <a title="53-lsi-8" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>9 0.21676351 <a title="53-lsi-9" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>10 0.20938012 <a title="53-lsi-10" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>11 0.20458256 <a title="53-lsi-11" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>12 0.19063036 <a title="53-lsi-12" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>13 0.18667896 <a title="53-lsi-13" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>14 0.18236338 <a title="53-lsi-14" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>15 0.17688072 <a title="53-lsi-15" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>16 0.16654129 <a title="53-lsi-16" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>17 0.16463695 <a title="53-lsi-17" href="./jmlr-2007-General_Polynomial_Time_Decomposition_Algorithms_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">35 jmlr-2007-General Polynomial Time Decomposition Algorithms     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>18 0.1570359 <a title="53-lsi-18" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>19 0.15492037 <a title="53-lsi-19" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>20 0.15325764 <a title="53-lsi-20" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.017), (8, 0.019), (10, 0.033), (12, 0.03), (15, 0.021), (28, 0.065), (40, 0.097), (45, 0.025), (46, 0.343), (48, 0.025), (60, 0.065), (80, 0.013), (85, 0.048), (98, 0.089), (99, 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69434386 <a title="53-lda-1" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We present a uniﬁed and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including 1 , 2 , 2 , and 1 + 2 style regularization. We propose an algorithm solving a large and 2 2 general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and uniﬁes techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models. Keywords: maximum entropy, density estimation, regularization, iterative scaling, species distribution modeling</p><p>2 0.41630995 <a title="53-lda-2" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>Author: Ryan M. Rifkin, Ross A. Lippert</p><p>Abstract: Regularization is an approach to function learning that balances ﬁt and smoothness. In practice, we search for a function f with a ﬁnite representation f = ∑i ci φi (·). In most treatments, the ci are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning. Keywords: kernel machines, duality, optimization, convex analysis, kernel learning</p><p>3 0.40645739 <a title="53-lda-3" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Kwangmoo Koh, Seung-Jean Kim, Stephen Boyd</p><p>Abstract: Logistic regression with 1 regularization has been proposed as a promising method for feature selection in classiﬁcation problems. In this paper we describe an efﬁcient interior-point method for solving large-scale 1 -regularized logistic regression problems. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC; medium sized problems, with tens of thousands of features and examples, can be solved in tens of seconds (assuming some sparsity in the data). A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve very large problems, with a million features and examples (e.g., the 20 Newsgroups data set), in a few minutes, on a PC. Using warm-start techniques, a good approximation of the entire regularization path can be computed much more efﬁciently than by solving a family of problems independently. Keywords: logistic regression, feature selection, 1 regularization, regularization path, interiorpoint methods.</p><p>4 0.40643802 <a title="53-lda-4" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>5 0.39276418 <a title="53-lda-5" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: Motivated by mathematical learning from training data, we introduce the notion of reﬁnable kernels. Various characterizations of reﬁnable kernels are presented. The concept of reﬁnable kernels leads to the introduction of wavelet-like reproducing kernels. We also investigate a reﬁnable kernel that forms a Riesz basis. In particular, we characterize reﬁnable translation invariant kernels, and reﬁnable kernels deﬁned by reﬁnable functions. This study leads to multiresolution analysis of reproducing kernel Hilbert spaces. Keywords: reﬁnable kernels, reﬁnable feature maps, wavelet-like reproducing kernels, dual kernels, learning with kernels, reproducing kernel Hilbert spaces, Riesz bases</p><p>6 0.3917307 <a title="53-lda-6" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>7 0.39100742 <a title="53-lda-7" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>8 0.3900044 <a title="53-lda-8" href="./jmlr-2007-Stagewise_Lasso.html">77 jmlr-2007-Stagewise Lasso</a></p>
<p>9 0.38958424 <a title="53-lda-9" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>10 0.38790107 <a title="53-lda-10" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>11 0.38527289 <a title="53-lda-11" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>12 0.38518524 <a title="53-lda-12" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>13 0.3841942 <a title="53-lda-13" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>14 0.38401744 <a title="53-lda-14" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>15 0.383715 <a title="53-lda-15" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>16 0.38334602 <a title="53-lda-16" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>17 0.38300934 <a title="53-lda-17" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>18 0.38168371 <a title="53-lda-18" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>19 0.37988812 <a title="53-lda-19" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>20 0.37789267 <a title="53-lda-20" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
