<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-31" href="#">jmlr2007-31</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</h1>
<br/><p>Source: <a title="jmlr-2007-31-pdf" href="http://jmlr.org/papers/volume8/kalisch07a/kalisch07a.pdf">pdf</a></p><p>Author: Markus Kalisch, Peter Bühlmann</p><p>Abstract: We consider the PC-algorithm (Spirtes et al., 2000) for estimating the skeleton and equivalence class of a very high-dimensional directed acyclic graph (DAG) with corresponding Gaussian distribution. The PC-algorithm is computationally feasible and often very fast for sparse problems with many nodes (variables), and it has the attractive property to automatically achieve high computational efﬁciency as a function of sparseness of the true underlying DAG. We prove uniform consistency of the algorithm for very high-dimensional, sparse DAGs where the number of nodes is allowed to quickly grow with sample size n, as fast as O(na ) for any 0 < a < ∞. The sparseness assumption is rather minimal requiring only that the neighborhoods in the DAG are of lower order than sample size n. We also demonstrate the PC-algorithm for simulated data. Keywords: asymptotic consistency, DAG, graphical model, PC-algorithm, skeleton</p><p>Reference: <a title="jmlr-2007-31-reference" href="../jmlr2007_reference/jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 , 2000) for estimating the skeleton and equivalence class of a very high-dimensional directed acyclic graph (DAG) with corresponding Gaussian distribution. [sent-8, score-0.695]
</p><p>2 The PC-algorithm is computationally feasible and often very fast for sparse problems with many nodes (variables), and it has the attractive property to automatically achieve high computational efﬁciency as a function of sparseness of the true underlying DAG. [sent-9, score-0.164]
</p><p>3 We prove uniform consistency of the algorithm for very high-dimensional, sparse DAGs where the number of nodes is allowed to quickly grow with sample size n, as fast as O(na ) for any 0 < a < ∞. [sent-10, score-0.175]
</p><p>4 Keywords: asymptotic consistency, DAG, graphical model, PC-algorithm, skeleton  1. [sent-13, score-0.421]
</p><p>5 Of particular current interest are directed acyclic graphs (DAGs), containing directed rather than undirected edges, which restrict in a sense the conditional dependence relations. [sent-17, score-0.389]
</p><p>6 These graphs can be interpreted by applying the directed Markov property (see Lauritzen, 1996). [sent-18, score-0.123]
</p><p>7 When ignoring the directions of a DAG, we get the skeleton of a DAG. [sent-19, score-0.398]
</p><p>8 In general, it is different from the conditional independence graph (CIG), see Section 2. [sent-20, score-0.138]
</p><p>9 (Thus, estimation methods for directed graphs cannot be easily borrowed from approaches for undirected CIGs. [sent-22, score-0.226]
</p><p>10 1, the skeleton can be interpreted easily and thus yields interesting insights into the dependence structure of the data. [sent-24, score-0.398]
</p><p>11 The greedy DAG search can be improved by exploiting probabilistic equivalence relations, and the search space can be reduced from individual DAGs to equivalence classes, as proposed in GES (Greedy Equivalent Search, see Chickering, 2002a). [sent-30, score-0.232]
</p><p>12 Although this method seems quite promising when having few or a moderate number of nodes, it is limited by the fact that the space of equivalence classes is conjectured to grow super-exponentially in the nodes as well (Gillispie and Perlman, 2001). [sent-31, score-0.203]
</p><p>13 It starts from a complete, undirected graph and deletes recursively edges based on conditional independence decisions. [sent-37, score-0.301]
</p><p>14 This yields an undirected graph which can then be partially directed and further extended to represent the underlying DAG (see later). [sent-38, score-0.24]
</p><p>15 We focus in this paper on estimating the equivalence class and the skeleton of DAGs (corresponding to multivariate Gaussian distributions) in the high-dimensional context, that is, the number of nodes p may be much larger than sample size n. [sent-44, score-0.65]
</p><p>16 We prove that the PC-algorithm consistently estimates the equivalence class and the skeleton of an underlying sparse DAG, as sample size n → ∞, even if p = pn = O(na ) (0 ≤ a < ∞) is allowed to grow very quickly as a function of n. [sent-45, score-0.603]
</p><p>17 They show that, assuming only faithfulness (explained in Section 2), uniform consistency cannot be achieved, but pointwise consistency can. [sent-53, score-0.16]
</p><p>18 More importantly, we show that consistency holds even as the number of nodes and neighbors increases and the size of the smallest non-zero partial correlations decrease as a function of the sample size. [sent-55, score-0.319]
</p><p>19 Stricter assumptions than the faithfulness condition that render uniform consistency possible have been also proposed in Zhang and Spirtes (2003). [sent-56, score-0.122]
</p><p>20 The problem of ﬁnding the equivalence class of a DAG has a substantial overlap with the problem of feature selection: If the equivalence class is found, the Markov Blanket of any variable (node) can be read of easily. [sent-59, score-0.232]
</p><p>21 Given a set of nodes V and suppose that M is the Markov Blanket of node X, then X is conditionally independent of V \M given M. [sent-60, score-0.16]
</p><p>22 1 Deﬁnitions and Preliminaries A graph G = (V, E) consists of a set of nodes or vertices V = {1, . [sent-68, score-0.138]
</p><p>23 , p} and a set of edges E ⊆ V ×V , that is, the edge set is a subset of ordered pairs of distinct nodes. [sent-71, score-0.164]
</p><p>24 An edge (i, j) ∈ E is called directed if (i, j) ∈ E but ( j, i) ∈ E; we then use the notation i → j. [sent-73, score-0.152]
</p><p>25 A directed acyclic graph (DAG) is a graph G where all edges are directed and not containing any cycle. [sent-75, score-0.378]
</p><p>26 If there is a directed edge i → j, node i is said to be a parent of node j. [sent-76, score-0.228]
</p><p>27 The adjacency set of a node j in graph G, denoted by ad j(G, j), are all nodes i which are directly connected to j by an edge (directed or undirected). [sent-78, score-0.363]
</p><p>28 The elements of ad j(G, j) are also called neighbors of or adjacent to j. [sent-79, score-0.188]
</p><p>29 A probability distribution P on R p is said to be faithful with respect to a graph G if conditional independencies of the distribution can be inferred from so-called d-separation in the graph G and vice-versa. [sent-80, score-0.246]
</p><p>30 The skeleton of a DAG G is the undirected graph obtained from G by substituting undirected edges for directed edges. [sent-92, score-0.801]
</p><p>31 A v-structure in a DAG G is an ordered triple of nodes (i, j, k) such that G contains the directed edges i → j and k → j, and i and k are not adjacent in G. [sent-93, score-0.322]
</p><p>32 Using a result from Verma and Pearl (1990), we can characterize equivalent classes more precisely: Two DAGs are equivalent if and only if they have the same skeleton and the same v-structures. [sent-97, score-0.398]
</p><p>33 A common tool for visualizing equivalence classes of DAGs are completed partially directed acyclic graphs (CPDAG). [sent-98, score-0.283]
</p><p>34 A partially directed acyclic graph (PDAG) is a graph where some edges are directed and some are undirected and one cannot trace a cycle by following the direction of directed edges and any direction for undirected edges. [sent-99, score-0.73]
</p><p>35 A PDAG is completed, if (1) every directed edge exists also in every DAG belonging to the equivalence class of the DAG and (2) for every undirected edge i − j there exists a DAG with i → j and a DAG with i ← j in the equivalence class. [sent-101, score-0.553]
</p><p>36 CPDAGs encode all independence informations contained in the corresponding equivalence class. [sent-102, score-0.17]
</p><p>37 It was shown in Chickering (2002b) that two CPDAGs are identical if and only if they represent the same equivalence class, that is, they represent a equivalence class uniquely. [sent-103, score-0.232]
</p><p>38 Although the main goal is to identify the CPDAG, the skeleton itself already contains interesting information. [sent-104, score-0.398]
</p><p>39 In particular, if P is faithful with respect to a DAG G, there is an edge between nodes i and j in the skeleton of DAG G ⇔ for all s ⊆ V \ {i, j}, X(i) and X( j) are conditionally dependent  given {X(r) ; r ∈ s},  (1)  (Spirtes et al. [sent-105, score-0.655]
</p><p>40 This implies that if P is faithful with respect to a DAG G, the skeleton of the DAG G is a subset (or equal) to the conditional independence graph (CIG) corresponding to P. [sent-109, score-0.605]
</p><p>41 More importantly, every edge in the skeleton indicates some strong dependence which cannot be explained away by accounting for other variables. [sent-111, score-0.464]
</p><p>42 As we will see later in more detail, estimating the CPDAG consists of two main parts (which will naturally structure our analysis): (1) Estimation of the skeleton and (2) partial orientation of edges. [sent-113, score-0.442]
</p><p>43 The skeleton itself oftentimes already provides interesting insights, and in a high-dimensional setting it might be interesting to use the undirected skeleton as an alternative target to the CPDAG when ﬁnding a useful approximation of the CPDAG seems hopeless. [sent-122, score-0.899]
</p><p>44 2 The PC-algorithm for Finding the Skeleton A naive strategy for ﬁnding the skeleton would be to check conditional independencies given all subsets s ⊆ V \ {i, j} (see Formula 1), that is, all partial correlations in the case of multivariate normal distributions as ﬁrst suggested by Verma and J. [sent-128, score-0.594]
</p><p>45 More precisely, we apply the part of the PC-algorithm that identiﬁes the undirected edges of the DAG. [sent-132, score-0.163]
</p><p>46 1 P OPULATION V ERSION FOR THE S KELETON In the population version of the PC-algorithm, we assume that perfect knowledge about all necessary conditional independence relations is available. [sent-135, score-0.118]
</p><p>47 Algorithm 1 The PC pop -algorithm 1: INPUT: Vertex Set V , Conditional Independence Information 2: OUTPUT: Estimated skeleton C, separation sets S (only needed when directing the skeleton afterwards) ˜ 3: Form the complete undirected graph C on the vertex set V. [sent-138, score-1.049]
</p><p>48 ˜ 4: = −1; C = C 5: repeat 6: = +1 7: repeat 8: Select a (new) ordered pair of nodes i, j that are adjacent in C such that |ad j(C, i) \ { j}| ≥ 9: repeat 10: Choose (new) k ⊆ ad j(C, i) \ { j} with |k| = . [sent-139, score-0.267]
</p><p>49 The maximal value of rithm 1 is denoted by mreach = maximal reached value of . [sent-142, score-0.151]
</p><p>50 A proof that this algorithm produces the correct skeleton can be easily deduced from Theorem 5. [sent-144, score-0.398]
</p><p>51 Then, the PC pop -algorithm constructs the true skeleton of the DAG. [sent-150, score-0.497]
</p><p>52 Furthermore, we assume faithful models, which means that the conditional independence relations correspond to d-separations (and so can be read off the graph) and vice versa; see Section 2. [sent-157, score-0.156]
</p><p>53 In the Gaussian case, conditional independencies can be inferred from partial correlations. [sent-159, score-0.119]
</p><p>54 We can thus estimate partial correlations to obtain estimates of conditional independencies. [sent-173, score-0.13]
</p><p>55 3 Extending the Skeleton to the Equivalence Class While ﬁnding the skeleton as in Algorithm 1, we recorded the separation sets that made edges drop out in the variable denoted by S. [sent-190, score-0.458]
</p><p>56 This was not necessary for ﬁnding the skeleton itself, but will be essential for extending the skeleton to the equivalence class. [sent-191, score-0.912]
</p><p>57 50f) to extend the skeleton to a CPDAG belonging to the equivalence class of the underlying DAG. [sent-193, score-0.514]
</p><p>58 2 is asymptotically consistent for the skeleton of a DAG, even if p is much larger than n but the DAG is sparse. [sent-205, score-0.398]
</p><p>59 (A3) The maximal number of neighbors in the DAG Gn is denoted by qn = max1≤ j≤pn |ad j(G, j)|, with qn = O(n1−b ) for some 0 < b ≤ 1. [sent-217, score-0.21]
</p><p>60 Recently, for undirected graphs the Lasso has been proposed as a computationally efﬁcient algorithm for estimating high-dimensional conditional independence graphs where ¨ the growth in dimensionality is as in (A2) (see Meinshausen and B uhlmann, 2006). [sent-227, score-0.264]
</p><p>61 2 and by Gskel,n the true skeleton from the DAG Gn . [sent-232, score-0.398]
</p><p>62 A choice for the value of the signiﬁcance level is α n = 2(1 − Φ(n1/2 cn /2)) which depends on the unknown lower bound of partial correlations in (A4). [sent-235, score-0.148]
</p><p>63 If this part is completed perfectly, that is, if there was no error while testing conditional independencies (it is not enough to assume that the skeleton was estimated correctly), the second part will never fail (see Meek, 1995b). [sent-238, score-0.473]
</p><p>64 Numerical Examples We analyze the PC-algorithm for ﬁnding the skeleton and the CPDAG using various simulated data sets. [sent-251, score-0.398]
</p><p>65 1 Simulating Data In this section, we analyze the PC-algorithm for the skeleton using simulated data. [sent-256, score-0.398]
</p><p>66 The corresponding DAG draws a directed edge from node i to node j if i < j and A ji = 0. [sent-269, score-0.228]
</p><p>67 3 Performance for Different Parameter Settings In this section, we give an overview over the performance in terms of the true positive rate (TPR) and false positive rate (FPR) for the skeleton and the SHD for the CPDAG. [sent-313, score-0.398]
</p><p>68 As expected, the ﬁt for a dense graph (triangles; E[N] = 5) is worse than the ﬁt for a sparse graph (circles; E[N] = 2). [sent-331, score-0.151]
</p><p>69 We should note, that while the number of neighbors to a given variable may be growing almost as fast as n, so that the number of neighbors is increasing with sample size, the percentage of true among all possible edges is going down with n. [sent-347, score-0.177]
</p><p>70 00002)  Table 1: The number of variables p increases exponentially, the sample size n increases linearly and the expected neighborhood size E[N] increases sub-linearly. [sent-460, score-0.141]
</p><p>71 06  TPR  p=9  p=27  p=81  p=243  p=729  p=2187  p=9  p=27  p=81  p=243  p=729  p=2187  Figure 3: While the number of variables p increases exponentially, the sample size n increases linearly and the expected neighborhood size E[N] increases sub-linearly, the TPR increases and the FPR decreases. [sent-474, score-0.167]
</p><p>72 The average processor time together with its standard deviation for estimating both the skeleton and the CPDAG is given in Table 2. [sent-492, score-0.438]
</p><p>73 Graphs of p = 1000 nodes and 8 neighbors on average could be estimated in about 25 minutes, while networks with up to p = 100 nodes could be estimated in about a second. [sent-493, score-0.22]
</p><p>74 The additional time spent for ﬁnding the CPDAG from the skeleton is comparable for both neighborhood sizes and varies between a couple to almost 100 percent of the time needed to estimate the skeleton. [sent-494, score-0.436]
</p><p>75 While the line for the dense graph is very straight, the line for the sparse graphs has a positive curvature. [sent-498, score-0.137]
</p><p>76 6 GHz, 4 GB) for estimating the skeleton ( Gskel ) ˆ CPDAG ) for different DAGs in seconds, with standard errors in brackets. [sent-544, score-0.398]
</p><p>77 R-Package pcalg The R-package pcalg can be used to estimate from data the underlying skeleton or equivalence class of a DAG. [sent-548, score-0.612]
</p><p>78 In the following, we show an example of how to generate a random DAG, draw samples and infer from data the skeleton and the equivalence class of the original DAG using the R-package pcalg. [sent-563, score-0.514]
</p><p>79 The line width of the edges in the resulting skeleton and CPDAG can be adjusted to correspond to the reliability of the estimated dependencies. [sent-564, score-0.458]
</p><p>80 seed(42) g <- randomDAG(p,s) # generate a random DAG d <- rmvDAG(n,g) # generate random samples Then we estimate the underlying skeleton by using the function pcAlgo and extend the skeleton to the CPDAG by using the function udag2cpdag. [sent-579, score-0.796]
</p><p>81 Then, for any 0 < γ ≤ 2, sup I ρn;i, j − ρn;i, j | > γ] ≤ C1 (n − 2) exp (n − 4) log( P[| ˆ  m i, j,k∈Ki, jn  4 − γ2 ) , 4 + γ2  for some constant 0 < C1 < ∞ depending on M only. [sent-582, score-0.225]
</p><p>82 Consider sup −1<ρ<1;ρ+γ≤x≤1  gρ (x) =  sup −1<ρ≤1−γ  1 − γ4  2  =  1 − ρ2 1 − (ρ + γ)2 1 − ρ(ρ + γ)  1 − γ4  2  γ 1 − ( −γ )( 2 ) 2  630  =  4 − γ2 < 1 for all 0 < γ ≤ 2. [sent-594, score-0.124]
</p><p>83 Then, for any γ > 0, P[| ˆ sup I ρn;i, j|k − ρn;i, j|k | > γ]  m i, j,k∈Ki, jn  ≤ C1 (n − 2 − mn ) exp (n − 4 − mn ) log(  4 − γ2 ) , 4 + γ2  for some constant 0 < C1 < ∞ depending on M from (A4) only. [sent-605, score-0.577]
</p><p>84 Then, for any γ > 0, P[|Z sup I n;i, j|k − zn;i, j|k | > γ]  m i, j,k∈Ki, jn  ≤ O(n − mn ) exp((n − 4 − mn ) log(  4 − (γ/L)2 )) + exp(−C2 (n − mn )) 4 + (γ/L)2  for some constant 0 < C2 < ∞ and L = 1/(1 − (1 + M)2 /4). [sent-610, score-0.727]
</p><p>85 By applying Corollary 1 with γ = κ = (1 − M)/2 we have sup I ρn;i, j|k − ρn;i, j|k | ≤ κ] P[| ˜  m i, j,k∈Ki, jn  > 1 −C1 (n − 2 − mn ) exp(−C2 (n − mn )). [sent-614, score-0.551]
</p><p>86 Therefore, since κ = (1 − M)/2 yielding 1/(1 − (M + κ)2 ) = L, and using Formula (11), we get ˜ sup I P[|g (ρn;i, j|k )| ≤ L]  m i, j,k∈Ki, jn  ≥ 1 −C1 (n − 2 − mn ) exp(−C2 (n − mn )). [sent-616, score-0.551]
</p><p>87 (12)  Since |g (ρ)| ≥ 1 for all ρ, we obtain with Formula (10): sup I n;i, j|k − zn;i, j|k | > γ] P[|Z  (13)  m i, j,k∈Ki, jn  ≤  ˜ sup I P[|g (ρn;i, j|k )| > L] +  m i, j,k∈Ki, jn  sup I ρn;i, j|k − ρn;i, j|k | > γ/L]. [sent-617, score-0.46]
</p><p>88 2 is then same as the PC(mreach )-algorithm, where mreach is the sample version of Formula (2). [sent-628, score-0.124]
</p><p>89 ˆ ˆ The population version PC pop (mn )-algorithm when stopped at level mn = mreach,n constructs the true skeleton according to Proposition 1. [sent-629, score-0.704]
</p><p>90 ˜ = −1; C = C repeat = +1 repeat Select a (new) ordered pair of nodes i, j that are adjacent in C such that |ad j(C, i) \ { j}| ≥ repeat Choose (new) k ⊆ ad j(C, i) \ { j} with |k| = . [sent-633, score-0.267]
</p><p>91 if i and j are conditionally independent given k then Delete edge i, j Denote this new graph by C. [sent-634, score-0.152]
</p><p>92 Denote by ˆ Gskel,n (αn , mn ) the estimate from the PC(mn )-algorithm in Section 2. [sent-637, score-0.176]
</p><p>93 2 and by Gskel,n the true skeleton from the DAG Gn . [sent-639, score-0.398]
</p><p>94 Then, for mn ≥ mreach,n , mn = O(n1−b ) (n → ∞), there exists αn → 0 (n → ∞) such that I Gskel,n (αn , mn ) = Gskel,n ] P[ ˆ  = 1 − O(exp(−Cn1−2d )) → 1 (n → ∞) for some 0 < C < ∞. [sent-641, score-0.528]
</p><p>95 Then, sup I i, j|k ] = P[E I  m i, j,k∈Ki, jn  sup I i, j|k − zi, j|k | > (n/(n − |k| − 3))1/2 cn /2] P[|Z  m i, j,k∈Ki, jn  ≤ O(n − mn ) exp(−C3 (n − mn )c2 ), n  (16)  4−δ2  for some 0 < C3 < ∞ using Lemma 3 and the fact that log( 4+δ2 ) ∼ −δ2 /2 as δ → 0. [sent-646, score-0.801]
</p><p>96 By invoking Lemma 3 we then obtain:  sup I i, j|k ] ≤ O(n − mn ) exp(−C4 (n − mn )c2 ) P[E II n  (17)  m i, j,k∈Ki, jn  for some 0 < C4 < ∞. [sent-648, score-0.551]
</p><p>97 Proof: Consider the population algorithm PC pop (m): the reached stopping level satisﬁes mreach ∈ {qn − 1, qn }, see Proposition 1. [sent-655, score-0.321]
</p><p>98 The sample PC(mn )-algorithm with stopping level in the range of mreach ≤ mn = O(n1−b ), coincides with the population version on a set A having probability P[A] = 1 − O(exp(−Cn1−2d )), see the last formula in the proof of Lemma 4. [sent-656, score-0.382]
</p><p>99 Hence, on the set A, mreach,n = mreach ∈ {qn − 1, qn }. [sent-657, score-0.168]
</p><p>100 3, due to the result of Meek (1995b), it is sufﬁcient to estimate the correct skeleton and separation sets. [sent-664, score-0.398]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dag', 0.496), ('skeleton', 0.398), ('cpdag', 0.272), ('shd', 0.235), ('dags', 0.178), ('mn', 0.176), ('fpr', 0.173), ('tpr', 0.173), ('jn', 0.137), ('alisch', 0.136), ('uhlmann', 0.126), ('imensional', 0.124), ('pc', 0.12), ('equivalence', 0.116), ('igh', 0.105), ('undirected', 0.103), ('mreach', 0.099), ('pop', 0.099), ('spirtes', 0.099), ('ad', 0.091), ('nodes', 0.087), ('directed', 0.086), ('faithfulness', 0.084), ('ki', 0.078), ('qn', 0.069), ('faithful', 0.069), ('edge', 0.066), ('gcpdag', 0.062), ('gskel', 0.062), ('sup', 0.062), ('edges', 0.06), ('independence', 0.054), ('correlations', 0.053), ('orient', 0.052), ('sparseness', 0.052), ('causal', 0.051), ('cn', 0.051), ('graph', 0.051), ('adjacent', 0.051), ('cance', 0.051), ('pcalg', 0.049), ('verma', 0.049), ('neighbors', 0.046), ('acyclic', 0.044), ('partial', 0.044), ('replicates', 0.042), ('independencies', 0.042), ('inspecting', 0.042), ('processor', 0.04), ('lauritzen', 0.04), ('pn', 0.039), ('neighborhood', 0.038), ('ordered', 0.038), ('node', 0.038), ('consistency', 0.038), ('tsamardinos', 0.037), ('cig', 0.037), ('kimn', 0.037), ('pdag', 0.037), ('pmn', 0.037), ('graphs', 0.037), ('exponentially', 0.036), ('conditionally', 0.035), ('conditional', 0.033), ('ei', 0.033), ('zn', 0.032), ('kalisch', 0.031), ('hotelling', 0.031), ('triangles', 0.031), ('lemma', 0.031), ('population', 0.031), ('chickering', 0.03), ('meek', 0.03), ('adjacency', 0.03), ('gn', 0.029), ('settings', 0.028), ('formula', 0.028), ('zi', 0.027), ('maximal', 0.026), ('exp', 0.026), ('increases', 0.026), ('afterwards', 0.026), ('sample', 0.025), ('cpdags', 0.025), ('ersion', 0.025), ('ethz', 0.025), ('gillispie', 0.025), ('goldenberg', 0.025), ('keleton', 0.025), ('pdags', 0.025), ('robins', 0.025), ('skeletons', 0.025), ('zuk', 0.025), ('sparse', 0.025), ('correlation', 0.024), ('dense', 0.024), ('multivariate', 0.024), ('na', 0.023), ('stopping', 0.023), ('graphical', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="31-tfidf-1" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>Author: Markus Kalisch, Peter Bühlmann</p><p>Abstract: We consider the PC-algorithm (Spirtes et al., 2000) for estimating the skeleton and equivalence class of a very high-dimensional directed acyclic graph (DAG) with corresponding Gaussian distribution. The PC-algorithm is computationally feasible and often very fast for sparse problems with many nodes (variables), and it has the attractive property to automatically achieve high computational efﬁciency as a function of sparseness of the true underlying DAG. We prove uniform consistency of the algorithm for very high-dimensional, sparse DAGs where the number of nodes is allowed to quickly grow with sample size n, as fast as O(na ) for any 0 < a < ∞. The sparseness assumption is rather minimal requiring only that the neighborhoods in the DAG are of lower order than sample size n. We also demonstrate the PC-algorithm for simulated data. Keywords: asymptotic consistency, DAG, graphical model, PC-algorithm, skeleton</p><p>2 0.060130626 <a title="31-tfidf-2" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>Author: Jean-Yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, ﬁrst, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester (1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand (see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds. Keywords: statistical learning theory, PAC-Bayes theorems, generalization error bounds</p><p>3 0.059208237 <a title="31-tfidf-3" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>4 0.053328995 <a title="31-tfidf-4" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>Author: Zakria Hussain, François Laviolette, Mario Marchand, John Shawe-Taylor, Spencer Charles Brubaker, Matthew D. Mullin</p><p>Abstract: Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classiﬁer achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classiﬁer achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassiﬁcations. Keywords: set covering machines, sample-compression, loss bounds</p><p>5 0.053199615 <a title="31-tfidf-5" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>6 0.040807363 <a title="31-tfidf-6" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>7 0.036105596 <a title="31-tfidf-7" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>8 0.034969129 <a title="31-tfidf-8" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>9 0.033600006 <a title="31-tfidf-9" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>10 0.032381259 <a title="31-tfidf-10" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>11 0.032112055 <a title="31-tfidf-11" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>12 0.031001167 <a title="31-tfidf-12" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>13 0.02867325 <a title="31-tfidf-13" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>14 0.027263813 <a title="31-tfidf-14" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>15 0.025313178 <a title="31-tfidf-15" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>16 0.024731815 <a title="31-tfidf-16" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>17 0.023928637 <a title="31-tfidf-17" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>18 0.023656435 <a title="31-tfidf-18" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>19 0.023057578 <a title="31-tfidf-19" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>20 0.022968072 <a title="31-tfidf-20" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.154), (1, 0.003), (2, 0.002), (3, -0.047), (4, 0.004), (5, 0.052), (6, 0.07), (7, -0.051), (8, 0.161), (9, 0.017), (10, 0.082), (11, 0.113), (12, 0.067), (13, -0.101), (14, 0.024), (15, 0.004), (16, 0.018), (17, 0.023), (18, -0.059), (19, 0.065), (20, 0.04), (21, -0.086), (22, -0.043), (23, -0.052), (24, 0.039), (25, 0.151), (26, 0.083), (27, 0.181), (28, -0.037), (29, 0.034), (30, -0.079), (31, 0.191), (32, -0.358), (33, 0.078), (34, -0.1), (35, 0.101), (36, -0.125), (37, -0.165), (38, -0.216), (39, 0.202), (40, 0.014), (41, 0.211), (42, 0.144), (43, 0.073), (44, -0.043), (45, -0.459), (46, -0.064), (47, 0.153), (48, 0.08), (49, 0.127)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9678703 <a title="31-lsi-1" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>Author: Markus Kalisch, Peter Bühlmann</p><p>Abstract: We consider the PC-algorithm (Spirtes et al., 2000) for estimating the skeleton and equivalence class of a very high-dimensional directed acyclic graph (DAG) with corresponding Gaussian distribution. The PC-algorithm is computationally feasible and often very fast for sparse problems with many nodes (variables), and it has the attractive property to automatically achieve high computational efﬁciency as a function of sparseness of the true underlying DAG. We prove uniform consistency of the algorithm for very high-dimensional, sparse DAGs where the number of nodes is allowed to quickly grow with sample size n, as fast as O(na ) for any 0 < a < ∞. The sparseness assumption is rather minimal requiring only that the neighborhoods in the DAG are of lower order than sample size n. We also demonstrate the PC-algorithm for simulated data. Keywords: asymptotic consistency, DAG, graphical model, PC-algorithm, skeleton</p><p>2 0.24904458 <a title="31-lsi-2" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>3 0.2482609 <a title="31-lsi-3" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>4 0.2223796 <a title="31-lsi-4" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>5 0.21596865 <a title="31-lsi-5" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>Author: Zakria Hussain, François Laviolette, Mario Marchand, John Shawe-Taylor, Spencer Charles Brubaker, Matthew D. Mullin</p><p>Abstract: Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classiﬁer achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classiﬁer achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassiﬁcations. Keywords: set covering machines, sample-compression, loss bounds</p><p>6 0.19763343 <a title="31-lsi-6" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>7 0.15783274 <a title="31-lsi-7" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>8 0.13191064 <a title="31-lsi-8" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>9 0.12782498 <a title="31-lsi-9" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>10 0.12372884 <a title="31-lsi-10" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>11 0.11135779 <a title="31-lsi-11" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>12 0.10948122 <a title="31-lsi-12" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>13 0.10539904 <a title="31-lsi-13" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>14 0.1034266 <a title="31-lsi-14" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>15 0.10234568 <a title="31-lsi-15" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>16 0.10067939 <a title="31-lsi-16" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>17 0.10059455 <a title="31-lsi-17" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>18 0.10017926 <a title="31-lsi-18" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>19 0.099656321 <a title="31-lsi-19" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>20 0.09766914 <a title="31-lsi-20" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.011), (8, 0.018), (10, 0.017), (12, 0.035), (14, 0.477), (28, 0.056), (40, 0.038), (45, 0.022), (48, 0.028), (60, 0.056), (80, 0.012), (85, 0.054), (98, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75225407 <a title="31-lda-1" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>Author: Markus Kalisch, Peter Bühlmann</p><p>Abstract: We consider the PC-algorithm (Spirtes et al., 2000) for estimating the skeleton and equivalence class of a very high-dimensional directed acyclic graph (DAG) with corresponding Gaussian distribution. The PC-algorithm is computationally feasible and often very fast for sparse problems with many nodes (variables), and it has the attractive property to automatically achieve high computational efﬁciency as a function of sparseness of the true underlying DAG. We prove uniform consistency of the algorithm for very high-dimensional, sparse DAGs where the number of nodes is allowed to quickly grow with sample size n, as fast as O(na ) for any 0 < a < ∞. The sparseness assumption is rather minimal requiring only that the neighborhoods in the DAG are of lower order than sample size n. We also demonstrate the PC-algorithm for simulated data. Keywords: asymptotic consistency, DAG, graphical model, PC-algorithm, skeleton</p><p>2 0.35141525 <a title="31-lda-2" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>3 0.27903074 <a title="31-lda-3" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>Author: Jennifer Neville, David Jensen</p><p>Abstract: Recent work on graphical models for relational data has demonstrated signiﬁcant improvements in classiﬁcation and inference when models represent the dependencies among instances. Despite its use in conventional statistical models, the assumption of instance independence is contradicted by most relational data sets. For example, in citation data there are dependencies among the topics of a paper’s references, and in genomic data there are dependencies among the functions of interacting proteins. In this paper, we present relational dependency networks (RDNs), graphical models that are capable of expressing and reasoning with such dependencies in a relational setting. We discuss RDNs in the context of relational Bayes networks and relational Markov networks and outline the relative strengths of RDNs—namely, the ability to represent cyclic dependencies, simple methods for parameter estimation, and efﬁcient structure learning techniques. The strengths of RDNs are due to the use of pseudolikelihood learning techniques, which estimate an efﬁcient approximation of the full joint distribution. We present learned RDNs for a number of real-world data sets and evaluate the models in a prediction context, showing that RDNs identify and exploit cyclic relational dependencies to achieve signiﬁcant performance gains over conventional conditional models. In addition, we use synthetic data to explore model performance under various relational data characteristics, showing that RDN learning and inference techniques are accurate over a wide range of conditions. Keywords: relational learning, probabilistic relational models, knowledge discovery, graphical models, dependency networks, pseudolikelihood estimation</p><p>4 0.26208854 <a title="31-lda-4" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We present a uniﬁed and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including 1 , 2 , 2 , and 1 + 2 style regularization. We propose an algorithm solving a large and 2 2 general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and uniﬁes techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models. Keywords: maximum entropy, density estimation, regularization, iterative scaling, species distribution modeling</p><p>5 0.26107234 <a title="31-lda-5" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>6 0.26043987 <a title="31-lda-6" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>7 0.25894341 <a title="31-lda-7" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>8 0.25798553 <a title="31-lda-8" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>9 0.25704175 <a title="31-lda-9" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>10 0.25524703 <a title="31-lda-10" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>11 0.25470793 <a title="31-lda-11" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>12 0.25437626 <a title="31-lda-12" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>13 0.25279367 <a title="31-lda-13" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>14 0.25277704 <a title="31-lda-14" href="./jmlr-2007-Stagewise_Lasso.html">77 jmlr-2007-Stagewise Lasso</a></p>
<p>15 0.25259075 <a title="31-lda-15" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>16 0.25243387 <a title="31-lda-16" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>17 0.25180432 <a title="31-lda-17" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>18 0.25146881 <a title="31-lda-18" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>19 0.25143251 <a title="31-lda-19" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>20 0.25049376 <a title="31-lda-20" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
