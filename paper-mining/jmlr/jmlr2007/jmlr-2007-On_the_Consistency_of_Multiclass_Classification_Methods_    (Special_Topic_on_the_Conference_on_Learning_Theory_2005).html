<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-61" href="#">jmlr2007-61</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</h1>
<br/><p>Source: <a title="jmlr-2007-61-pdf" href="http://jmlr.org/papers/volume8/tewari07a/tewari07a.pdf">pdf</a></p><p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: Binary classiﬁcation is a well studied special case of the classiﬁcation problem. Statistical properties of binary classiﬁers, such as consistency, have been investigated in a variety of settings. Binary classiﬁcation methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classiﬁcation method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufﬁcient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature. Keywords: multiclass classiﬁcation, consistency, Bayes risk</p><p>Reference: <a title="jmlr-2007-61-reference" href="../jmlr2007_reference/jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('py', 0.512), ('zy', 0.421), ('fy', 0.291), ('calibr', 0.267), ('nk', 0.22), ('argmin', 0.216), ('ppred', 0.209), ('rk', 0.145), ('ulticlass', 0.145), ('fn', 0.139), ('ewar', 0.122), ('multiclass', 0.118), ('argmax', 0.108), ('admiss', 0.095), ('onsist', 0.093), ('maxy', 0.093), ('inf', 0.087), ('gn', 0.083), ('hy', 0.083), ('convex', 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="61-tfidf-1" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: Binary classiﬁcation is a well studied special case of the classiﬁcation problem. Statistical properties of binary classiﬁers, such as consistency, have been investigated in a variety of settings. Binary classiﬁcation methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classiﬁcation method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufﬁcient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature. Keywords: multiclass classiﬁcation, consistency, Bayes risk</p><p>2 0.10613068 <a title="61-tfidf-2" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>Author: Kenji Fukumizu, Francis R. Bach, Arthur Gretton</p><p>Abstract: While kernel canonical correlation analysis (CCA) has been applied in many contexts, the convergence of ﬁnite sample estimates of the associated functions to their population counterparts has not yet been established. This paper gives a mathematical proof of the statistical convergence of kernel CCA, providing a theoretical justiﬁcation for the method. The proof uses covariance operators deﬁned on reproducing kernel Hilbert spaces, and analyzes the convergence of their empirical estimates of ﬁnite rank to their population counterparts, which can have inﬁnite rank. The result also gives a sufﬁcient condition for convergence on the regularization coefﬁcient involved in kernel CCA: this should decrease as n−1/3 , where n is the number of data. Keywords: canonical correlation analysis, kernel, consistency, regularization, Hilbert space</p><p>3 0.091488272 <a title="61-tfidf-3" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>Author: Peter L. Bartlett, Mikhail Traskin</p><p>Abstract: The risk, or probability of error, of the classiﬁer produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after n1−ε iterations—for sample size n and ε ∈ (0, 1)—the sequence of risks of the classiﬁers it produces approaches the Bayes risk. Keywords: boosting, adaboost, consistency</p><p>4 0.08958894 <a title="61-tfidf-4" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: One of the nice properties of kernel classiﬁers such as SVMs is that they often produce sparse solutions. However, the decision functions of these classiﬁers cannot always be used to estimate the conditional probability of the class label. We investigate the relationship between these two properties and show that these are intimately related: sparseness does not occur when the conditional probabilities can be unambiguously estimated. We consider a family of convex loss functions and derive sharp asymptotic results for the fraction of data that becomes support vectors. This enables us to characterize the exact trade-off between sparseness and the ability to estimate conditional probabilities for these loss functions. Keywords: kernel methods, support vector machines, sparseness, estimating conditional probabilities</p><p>5 0.072723992 <a title="61-tfidf-5" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>Author: Jean-Yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, ﬁrst, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester (1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand (see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds. Keywords: statistical learning theory, PAC-Bayes theorems, generalization error bounds</p><p>6 0.063453823 <a title="61-tfidf-6" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>7 0.060397483 <a title="61-tfidf-7" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>8 0.058596596 <a title="61-tfidf-8" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>9 0.050485551 <a title="61-tfidf-9" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>10 0.049358092 <a title="61-tfidf-10" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>11 0.039764076 <a title="61-tfidf-11" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>12 0.037031244 <a title="61-tfidf-12" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>13 0.032789353 <a title="61-tfidf-13" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>14 0.029714482 <a title="61-tfidf-14" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>15 0.02937741 <a title="61-tfidf-15" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>16 0.027684592 <a title="61-tfidf-16" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>17 0.027541723 <a title="61-tfidf-17" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>18 0.026526583 <a title="61-tfidf-18" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>19 0.025129089 <a title="61-tfidf-19" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>20 0.024853755 <a title="61-tfidf-20" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.169), (1, -0.167), (2, 0.022), (3, -0.023), (4, -0.049), (5, -0.101), (6, -0.092), (7, 0.005), (8, -0.068), (9, 0.14), (10, 0.072), (11, -0.107), (12, 0.064), (13, 0.155), (14, -0.023), (15, -0.095), (16, -0.193), (17, -0.101), (18, -0.042), (19, 0.187), (20, 0.186), (21, -0.302), (22, 0.035), (23, 0.014), (24, -0.043), (25, 0.102), (26, 0.2), (27, -0.013), (28, -0.187), (29, -0.132), (30, 0.057), (31, -0.087), (32, 0.087), (33, 0.018), (34, 0.014), (35, -0.107), (36, -0.045), (37, -0.119), (38, -0.013), (39, -0.045), (40, -0.028), (41, 0.074), (42, 0.013), (43, -0.006), (44, -0.043), (45, 0.026), (46, -0.002), (47, 0.035), (48, -0.209), (49, 0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94509768 <a title="61-lsi-1" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: Binary classiﬁcation is a well studied special case of the classiﬁcation problem. Statistical properties of binary classiﬁers, such as consistency, have been investigated in a variety of settings. Binary classiﬁcation methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classiﬁcation method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufﬁcient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature. Keywords: multiclass classiﬁcation, consistency, Bayes risk</p><p>2 0.58563161 <a title="61-lsi-2" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>Author: Kenji Fukumizu, Francis R. Bach, Arthur Gretton</p><p>Abstract: While kernel canonical correlation analysis (CCA) has been applied in many contexts, the convergence of ﬁnite sample estimates of the associated functions to their population counterparts has not yet been established. This paper gives a mathematical proof of the statistical convergence of kernel CCA, providing a theoretical justiﬁcation for the method. The proof uses covariance operators deﬁned on reproducing kernel Hilbert spaces, and analyzes the convergence of their empirical estimates of ﬁnite rank to their population counterparts, which can have inﬁnite rank. The result also gives a sufﬁcient condition for convergence on the regularization coefﬁcient involved in kernel CCA: this should decrease as n−1/3 , where n is the number of data. Keywords: canonical correlation analysis, kernel, consistency, regularization, Hilbert space</p><p>3 0.54426664 <a title="61-lsi-3" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: One of the nice properties of kernel classiﬁers such as SVMs is that they often produce sparse solutions. However, the decision functions of these classiﬁers cannot always be used to estimate the conditional probability of the class label. We investigate the relationship between these two properties and show that these are intimately related: sparseness does not occur when the conditional probabilities can be unambiguously estimated. We consider a family of convex loss functions and derive sharp asymptotic results for the fraction of data that becomes support vectors. This enables us to characterize the exact trade-off between sparseness and the ability to estimate conditional probabilities for these loss functions. Keywords: kernel methods, support vector machines, sparseness, estimating conditional probabilities</p><p>4 0.3147783 <a title="61-lsi-4" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>Author: Peter L. Bartlett, Mikhail Traskin</p><p>Abstract: The risk, or probability of error, of the classiﬁer produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after n1−ε iterations—for sample size n and ε ∈ (0, 1)—the sequence of risks of the classiﬁers it produces approaches the Bayes risk. Keywords: boosting, adaboost, consistency</p><p>5 0.27002198 <a title="61-lsi-5" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>Author: Ofer Dekel, Philip M. Long, Yoram Singer</p><p>Abstract: We study the problem of learning multiple tasks in parallel within the online learning framework. On each online round, the algorithm receives an instance for each of the parallel tasks and responds by predicting the label of each instance. We consider the case where the predictions made on each round all contribute toward a common goal. The relationship between the various tasks is deﬁned by a global loss function, which evaluates the overall quality of the multiple predictions made on each round. Speciﬁcally, each individual prediction is associated with its own loss value, and then these multiple loss values are combined into a single number using the global loss function. We focus on the case where the global loss function belongs to the family of absolute norms, and present several online learning algorithms for the induced problem. We prove worst-case relative loss bounds for all of our algorithms, and demonstrate the effectiveness of our approach on a largescale multiclass-multilabel text categorization problem. Keywords: online learning, multitask learning, multiclass multilabel classiifcation, perceptron</p><p>6 0.26227981 <a title="61-lsi-6" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>7 0.2553429 <a title="61-lsi-7" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>8 0.19738397 <a title="61-lsi-8" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>9 0.19482671 <a title="61-lsi-9" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>10 0.1807992 <a title="61-lsi-10" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>11 0.16620456 <a title="61-lsi-11" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>12 0.15936269 <a title="61-lsi-12" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>13 0.150397 <a title="61-lsi-13" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>14 0.1475922 <a title="61-lsi-14" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>15 0.13678518 <a title="61-lsi-15" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>16 0.13350494 <a title="61-lsi-16" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>17 0.13254187 <a title="61-lsi-17" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>18 0.1306435 <a title="61-lsi-18" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>19 0.12713404 <a title="61-lsi-19" href="./jmlr-2007-From_External_to_Internal_Regret_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">34 jmlr-2007-From External to Internal Regret     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>20 0.12606147 <a title="61-lsi-20" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(19, 0.028), (23, 0.038), (31, 0.015), (32, 0.045), (69, 0.036), (70, 0.024), (94, 0.64), (95, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.97024059 <a title="61-lda-1" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>same-paper 2 0.9412002 <a title="61-lda-2" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: Binary classiﬁcation is a well studied special case of the classiﬁcation problem. Statistical properties of binary classiﬁers, such as consistency, have been investigated in a variety of settings. Binary classiﬁcation methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classiﬁcation method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufﬁcient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature. Keywords: multiclass classiﬁcation, consistency, Bayes risk</p><p>3 0.92044419 <a title="61-lda-3" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis. Keywords: embedding algorithms, manifold learning, exponential families, multidimensional scaling, matrix factorization, semideﬁnite programming</p><p>4 0.71803069 <a title="61-lda-4" href="./jmlr-2007-Distances_between_Data_Sets_Based_on_Summary_Statistics.html">27 jmlr-2007-Distances between Data Sets Based on Summary Statistics</a></p>
<p>Author: Nikolaj Tatti</p><p>Abstract: The concepts of similarity and distance are crucial in data mining. We consider the problem of deﬁning the distance between two data sets by comparing summary statistics computed from the data sets. The initial deﬁnition of our distance is based on geometrical notions of certain sets of distributions. We show that this distance can be computed in cubic time and that it has several intuitive properties. We also show that this distance is the unique Mahalanobis distance satisfying certain assumptions. We also demonstrate that if we are dealing with binary data sets, then the distance can be represented naturally by certain parity functions, and that it can be evaluated in linear time. Our empirical tests with real world data show that the distance works well. Keywords: data mining theory, complex data, binary data, itemsets</p><p>5 0.71041334 <a title="61-lda-5" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: Motivated by mathematical learning from training data, we introduce the notion of reﬁnable kernels. Various characterizations of reﬁnable kernels are presented. The concept of reﬁnable kernels leads to the introduction of wavelet-like reproducing kernels. We also investigate a reﬁnable kernel that forms a Riesz basis. In particular, we characterize reﬁnable translation invariant kernels, and reﬁnable kernels deﬁned by reﬁnable functions. This study leads to multiresolution analysis of reproducing kernel Hilbert spaces. Keywords: reﬁnable kernels, reﬁnable feature maps, wavelet-like reproducing kernels, dual kernels, learning with kernels, reproducing kernel Hilbert spaces, Riesz bases</p><p>6 0.67202979 <a title="61-lda-6" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>7 0.66026813 <a title="61-lda-7" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>8 0.64010501 <a title="61-lda-8" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>9 0.62706012 <a title="61-lda-9" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>10 0.62698358 <a title="61-lda-10" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>11 0.62218064 <a title="61-lda-11" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>12 0.62039328 <a title="61-lda-12" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>13 0.61858588 <a title="61-lda-13" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>14 0.60462791 <a title="61-lda-14" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>15 0.60237378 <a title="61-lda-15" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>16 0.59332442 <a title="61-lda-16" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>17 0.59128726 <a title="61-lda-17" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>18 0.5721845 <a title="61-lda-18" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>19 0.56310314 <a title="61-lda-19" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>20 0.55902654 <a title="61-lda-20" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
