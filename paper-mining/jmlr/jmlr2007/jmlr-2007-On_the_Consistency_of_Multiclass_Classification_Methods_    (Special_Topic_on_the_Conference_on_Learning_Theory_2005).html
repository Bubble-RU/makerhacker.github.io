<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-61" href="#">jmlr2007-61</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</h1>
<br/><p>Source: <a title="jmlr-2007-61-pdf" href="http://jmlr.org/papers/volume8/tewari07a/tewari07a.pdf">pdf</a></p><p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: Binary classiﬁcation is a well studied special case of the classiﬁcation problem. Statistical properties of binary classiﬁers, such as consistency, have been investigated in a variety of settings. Binary classiﬁcation methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classiﬁcation method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufﬁcient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature. Keywords: multiclass classiﬁcation, consistency, Bayes risk</p><p>Reference: <a title="jmlr-2007-61-reference" href="../jmlr2007_reference/jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we investigate the consistency of multiclass (|Y | ≥ 2) methods which try to generalize (1) by replacing f with a vector function f. [sent-27, score-0.19]
</p><p>2 The authors also gave a simple characterization of classiﬁcation calibration for convex loss functions. [sent-39, score-0.357]
</p><p>3 1 below, we provide a different point of view for looking at classiﬁcation calibration for binary classiﬁcation in order to motivate our geometric approach to multiclass classiﬁcation. [sent-41, score-0.338]
</p><p>4 Section 2 deﬁnes classiﬁcation calibration in the setting of multiclass classiﬁcation and provides a justiﬁcation, in the form of Theorem 2, for studying it: classiﬁcation calibration is equivalent to Bayes consistency. [sent-43, score-0.518]
</p><p>5 The main result in Section 3 is Theorem 7 which characterizes classiﬁcation calibration in terms of geometric properties of some sets associated with the loss function Ψ. [sent-44, score-0.244]
</p><p>6 Section 5 applies the results obtained in the paper to examine the consistency of a few multiclass methods. [sent-47, score-0.19]
</p><p>7 1 Consistency of Binary Classiﬁcation Methods If we have a convex loss function φ : R → [0, ∞) which is differentiable at 0 and φ (0) < 0, then it is known (Bartlett et al. [sent-51, score-0.169]
</p><p>8 We predict the label of a new example x to be pred(Ψ(f(x))) for some function pred : RK → {1, . [sent-93, score-0.175]
</p><p>9 The Ψ-risk of a function f is RΨ (f) = EX Y [ Ψy (f(x)) ] , and we denote the least possible Ψ-risk by R∗ = inf RΨ (f) . [sent-97, score-0.223]
</p><p>10 The least possible risk is R∗ = EX [ 1 − max py (x) ] , y  where py (x) = P(Y = y | X = x). [sent-99, score-0.976]
</p><p>11 Let us ﬁx an arbitrary x for now, so we can write f instead of f(x), p y instead of py (x), etc. [sent-105, score-0.471]
</p><p>12 The minimum might not be achieved and so we consider the inﬁmum 1 inff∈C ∑y py Ψy (f) of the conditional expectation above. [sent-106, score-0.471]
</p><p>13 We then have inf  f∈C  inf ∑ py Ψy (f) = f∈C  p, Ψ(f)  y  = inf p, z z∈R  (7)  = inf p, z , z∈S  1. [sent-114, score-1.363]
</p><p>14 Since py and Ψy (f) are both non-negative, the objective function is bounded below by 0 and hence the existence of an inﬁmum is guaranteed. [sent-115, score-0.505]
</p><p>15 Deﬁnition 1 A set S ⊆ RK is classiﬁcation calibrated if there exists a predictor function pred : + RK → {1, . [sent-127, score-0.328]
</p><p>16 , K} such that ∀p ∈ ∆K ,  inf  z∈S : ppred(z)   inf p, z , z∈S  (8)  where ∆K is the probability simplex in RK . [sent-130, score-0.446]
</p><p>17 The following theorem tells us that classiﬁcation calibration is indeed the right property to study, as it is both necessary and sufﬁcient for convergence of Ψ-risk to the optimal Ψ-risk to imply convergence of risk to the Bayes risk. [sent-131, score-0.261]
</p><p>18 The deﬁnition of classiﬁcation calibration as stated above is not concrete enough to be of any use in checking the consistency of a multiclass method corresponding to a given loss function. [sent-138, score-0.479]
</p><p>19 We now study the property of classiﬁcation calibration with the aim of arriving at a characterization expressed in terms of concretely veriﬁable properties of the loss function. [sent-139, score-0.284]
</p><p>20 Lemma 3 S ⊆ RK is classiﬁcation calibrated iff there exists a predictor function pred : R K → + {1, . [sent-142, score-0.355]
</p><p>21 , K} such that the following holds: ∀p ∈ ∆K and all sequences {z(n) } in S such that p, z(n) → inf p, z , z∈S  2. [sent-145, score-0.223]
</p><p>22 1011  (9)  T EWARI AND BARTLETT  we have ppred(z(n) ) = max py  (10)  y  ultimately. [sent-147, score-0.471]
</p><p>23 3 This makes it easier to see that if S is classiﬁcation calibrated then we can ﬁnd a predictor function such that any sequence achieving the inﬁmum in (7) ultimately predicts the right label (the one having maximum probability). [sent-148, score-0.232]
</p><p>24 If there exists a predictor function pred satisfying con+ dition (8) in the deﬁnition of classiﬁcation calibration (Deﬁnition 1), then any predictor function pred satisfying ∀z ∈ S , zpred (z) = min zy (11) y  also satisﬁes (8). [sent-151, score-1.025]
</p><p>25 In order to derive a contradiction, assume that p pred (z(n) ) < maxy py inﬁnitely often. [sent-154, score-0.732]
</p><p>26 Since there are ﬁnitely many labels, this implies that there is a subsequence {z (nk ) } and labels M and m such that the following hold, pred(z(nk ) ) = M ∈ {y : py = max py } , y  pred (z  (nk )  ) = m ∈ {y : py < max py } , y  p, z (n )  (nk )  → inf p, z . [sent-155, score-2.389]
</p><p>27 There are two cases depending on whether the inequality in (n )  (n )  lim inf zM k − zm k k  ≥0  is strict or not. [sent-159, score-0.347]
</p><p>28 (n ) (n ) If it is strict, denote the value of the lim inf by ε > 0. [sent-160, score-0.286]
</p><p>29 Then zM k − zm k > ε/2 ultimately and hence we have (n )  (n )  ˜ p, z(nk ) − p, z(nk ) = (pM − pm )(zM k − zm k ) > (pM − pm )ε/2 ˜ for k large enough. [sent-161, score-0.296]
</p><p>30 Multiplying this with Otherwise, choose a subsequence m M (pM − pm ), we have ˜ ˜ ˜ lim p, z(nk ) − p, z(nk ) = 0 . [sent-164, score-0.211]
</p><p>31 1012  C ONSISTENCY OF M ULTICLASS C LASSIFICATION  We also have ˜ ˜ ˜ ˜ ˜ lim p, z(nk ) = lim p, z(nk ) = inf p, z = inf p, z = inf p, z ,  k→∞  k→∞  z∈S  z∈S  z∈S  where the last equality follows because of symmetry. [sent-169, score-0.795]
</p><p>32 This means ˜ ˜ p, z(nk ) → inf p, z z∈S  as k → ∞ and therefore ppred(z(nk ) ) = pM ˜ ultimately. [sent-170, score-0.223]
</p><p>33 ˜ ˜ Having identiﬁed the class of potentially useful predictor functions, we will henceforth assume that pred is deﬁned as in (11). [sent-172, score-0.216]
</p><p>34 A Characterization of Classiﬁcation Calibration We give a characterization of classiﬁcation calibration in terms of normals to the convex set S and its projections onto lower dimensions. [sent-174, score-0.447]
</p><p>35 Deﬁnition 5 A convex set S ⊆ RK is admissible if ∀z ∈ ∂S , ∀p ∈ N (z), we have + argmin(z) ⊆ argmax(p)  (12)  where argmin(z) = {y : zy = miny zy } and argmax(p) = {y : py = maxy py }. [sent-177, score-1.978]
</p><p>36 Lemma 6 If S ⊆ RK is admissible then for all p ∈ ∆K and all bounded sequences {z(n) } such that + p, z(n) → infz∈S p, z , we have ppred(z(n) ) = maxy py ultimately. [sent-178, score-0.647]
</p><p>37 Taking the limit of a convergent subsequence of the given bounded sequence gives us a point in ∂S which achieves the inﬁmum of the inner product with p. [sent-180, score-0.203]
</p><p>38 For a point z and a set Z, deﬁne dist(z, Z) = inf z − z , z ∈Z  to be the distance of z from Z. [sent-183, score-0.223]
</p><p>39 For if we assume the contrary, boundedness implies that we can ﬁnd a convergent subsequence {z (nk ) } such that ∀k, dist(z(nk ) , Z(p)) ≥ ε. [sent-185, score-0.2]
</p><p>40 The next theorem provides a characterization of classiﬁcation calibration in terms of normals to S . [sent-201, score-0.359]
</p><p>41 Therefore, if we set pred(z(n) ) = y , we have ppred(z(n) ) < maxy py ultimately. [sent-217, score-0.557]
</p><p>42 We claim that there exists an M < ∞ such that ∀y ≤ j, zy ≤ M (n) ultimately. [sent-228, score-0.425]
</p><p>43 Consider the subsequence consisting of those z (n) for which zy ≤ M for (n) y ∈ {1, . [sent-234, score-0.468]
</p><p>44 We have a bounded sequence with ˜ ˜ p, z(n) →  inf  ˜ z∈S ( j+|T |)  ˜ ˜ p, z . [sent-253, score-0.298]
</p><p>45 Thus, by Lemma 6, we have ppred(˜ (n) ) = maxy py = maxy py ultimately. [sent-254, score-1.114]
</p><p>46 Thus we have ppred(z(n) ) = maxy py ultimately and the theorem is proved. [sent-256, score-0.617]
</p><p>47 Sufﬁcient Conditions for Classiﬁcation Calibration In this section, we prove some propositions that reduce the work involved in checking classiﬁcation calibration of sets. [sent-258, score-0.286]
</p><p>48 Therefore z − z, p ≥ 0 which implies, since z, z agree on all but the y, y coordinates, (zy − zy )py + (zy − zy )py ≥ 0 . [sent-272, score-0.776]
</p><p>49 Proof For any y, y , Lemma 8 gives us (zy −zy )(py − py ) ≥ 0. [sent-275, score-0.471]
</p><p>50 Therefore, if | argmin(z)| = 1 and zy = miny zy then py ≥ py for all y , and so argmin(z) ⊆ argmax(p). [sent-277, score-1.763]
</p><p>51 By Lemma 8 we have (zy − zy )(py − py ) ≥ 0 which implies zy ≤ zy since py − py < 0. [sent-284, score-2.604]
</p><p>52 But we already know that zy ≤ zy ˜ ˜ and so zy = zy . [sent-285, score-1.552]
</p><p>53 Theorem 7 provides a characterization of classiﬁcation calibration in terms of admissibility of the projections S (k) . [sent-288, score-0.341]
</p><p>54 Proof We only need to prove that R (k) closed and S admissible implies S (k) is admissible since the last claim of the theorem follows from this and Theorem 7. [sent-305, score-0.225]
</p><p>55 The mapping Ψ(k) is boundedly invertible iff for all M > 0, there is an M > 0 such that z ≤ M, z ∈ R (k) implies there is g ∈ C with g ≤ M and Ψ(k) (g) = z. [sent-340, score-0.197]
</p><p>56 Ψ(k) boundedly invertible roughly means that it has an inverse carrying bounded points in its range to bounded points in the domain. [sent-341, score-0.211]
</p><p>57 Examples We apply the results of the previous section to examine the consistency of several multiclass methods. [sent-360, score-0.19]
</p><p>58 In all these examples, the functions Ψy (f) are obtained from a single real valued function ψ : RK → R as follows + Ψy (f) = ψ( fy , f1 , . [sent-361, score-0.268]
</p><p>59 1 Example 1 The method of Crammer and Singer (2001) corresponds to Ψy (f) = max φ( fy − fy ), C = RK y =y  1017  T EWARI AND BARTLETT  with φ(t) = (1−t)+ . [sent-372, score-0.536]
</p><p>60 Thus, there is no y such that py = maxy py for all p ∈ N (z). [sent-375, score-1.028]
</p><p>61 Even if we choose an everywhere differentiable convex φ with φ (0) < 0, the three normals mentioned above are still there in N (z) for z = (φ(0), φ(0), φ(0)). [sent-377, score-0.222]
</p><p>62 2 Example 2 The method of Weston and Watkins (1998) corresponds to Ψy (f) =  ∑ φ( fy − fy ), C = RK  (16)  y =y  with φ(t) = (1 − t)+ . [sent-380, score-0.536]
</p><p>63 Now assume that φ is a positive convex classiﬁcation calibrated loss function (i. [sent-385, score-0.224]
</p><p>64 The following proposition then guarantees that Ψ(k) is boundedly invertible for k > 1 and we can ignore projections. [sent-390, score-0.185]
</p><p>65 In particular, if we choose φ differentiable so that S possesses a unique normal everywhere on its boundary then, by Proposition 9, S is admissible and hence classiﬁcation calibrated. [sent-391, score-0.178]
</p><p>66 , Ψk (f)), k > 1 implies boundedness of all pairwise differences f y − fy . [sent-401, score-0.34]
</p><p>67 , Ψk (f)) (k > 1), only implies that differences of the form f y − fy are bounded from below for y ∈ {1, . [sent-407, score-0.329]
</p><p>68 This implies that f y − fy is bounded for y, y ≤ k. [sent-414, score-0.329]
</p><p>69 Set hy = fy − m∗ (this doesn’t change the value of Ψ as it depends only on differences). [sent-416, score-0.344]
</p><p>70 Now set gy = hy , y ≤ k and gy = max{−t0 , hy } for y > k. [sent-423, score-0.208]
</p><p>71 To see that we don’t change anything by this update, note that when h y < −t0 , both hy − hy and hy + t0 are greater than t0 (above which φ is constant in the case we’re considering) for y ≤ k. [sent-424, score-0.228]
</p><p>72 1018  C ONSISTENCY OF M ULTICLASS C LASSIFICATION  (a)  (b)  Figure 3: (a) Lee, Lin and Wahba (b) Loss of consistency in multiclass setting 5. [sent-426, score-0.19]
</p><p>73 (2004) corresponds to Ψy (f) =  ∑ φ(− fy ), C = {f : ∑ fy = 0}  (17)  y  y =y  with φ(t) = (1 − t)+ . [sent-428, score-0.536]
</p><p>74 The question which naturally arises is: for which convex loss functions φ does (17) lead to a consistent multiclass classiﬁcation method? [sent-436, score-0.251]
</p><p>75 Convex loss functions which are classiﬁcation calibrated for the two class case, that is, differentiable at 0 with φ (0) < 0, can lead to inconsistent classiﬁers of this kind in the multiclass setting. [sent-437, score-0.343]
</p><p>76 A nice thing about this example is that if we assume φ is a positive classiﬁcation calibrated binary loss function then Ψ(k) is boundedly invertible for k > 1. [sent-442, score-0.319]
</p><p>77 1019  T EWARI AND BARTLETT  Proof We have  R (k) =  ∑ φ(− fy ), . [sent-447, score-0.268]
</p><p>78 Since − f y = ∑y =y fy , − fy is also bounded from above. [sent-455, score-0.57]
</p><p>79 Let Ψy (f) = φ( fy ), C = {f : ∑ fy = 0} y  with φ(t) = exp(−βt) for some β > 0. [sent-459, score-0.536]
</p><p>80 Suppose, φ is a positive convex, classiﬁcation calibrated binary loss function and K ≥ 3. [sent-463, score-0.176]
</p><p>81 The last column gives a condition on φ which is sufﬁcient to ensure consistency in case of a convex, positive, classiﬁcation calibrated φ. [sent-471, score-0.194]
</p><p>82 Note that, for all the examples we considered, mere classiﬁcation calibration and positivity of φ do not sufﬁce to guarantee consistency of the derived multiclass method. [sent-472, score-0.395]
</p><p>83 Conclusion We considered multiclass generalizations of classiﬁcation methods based on convex risk minimization and gave a necessary and sufﬁcient condition for their Bayes consistency. [sent-475, score-0.215]
</p><p>84 Some examples showed that quite often straightforward generalizations of consistent binary classiﬁcation methods lead to inconsistent multiclass classiﬁers. [sent-476, score-0.191]
</p><p>85 Example 4 shows that even differentiable loss functions do not guarantee multiclass consistency. [sent-478, score-0.204]
</p><p>86 The question of consistency then reduces to checking properties of a single convex set. [sent-480, score-0.2]
</p><p>87 If B is a bounded subset of R K , then p(n) , z → p, z uniformly over z ∈ B and therefore inf p(n) , z → inf p, z . [sent-487, score-0.48]
</p><p>88 Then we have inf p(n) , z ≤ inf p(n) , z → inf p, z . [sent-489, score-0.669]
</p><p>89 z∈S  S ∩Br  S ∩Br  Therefore lim sup inf p(n) , z ≤ inf n  z∈S  z∈S ∩Br  p, z . [sent-490, score-0.534]
</p><p>90 1021  T EWARI AND BARTLETT  Letting r → ∞, we get lim sup inf p(n) , z ≤ inf p, z . [sent-493, score-0.534]
</p><p>91 So, for a sufﬁciently large ball BM ⊆ R j  z∈S  j  ∑ py zy = z∈Sinf∩B ∑ py zy ,  inf p, z = inf  z∈S ( j) y=1  ( j)  j  inf p(n) , z ≥ inf  z∈S  z∈S ( j)  ∑  M  y=1  j  (n)  p y zy =  y=1  inf  z∈S ( j) ∩B  M  ∑ py  (n)  zy . [sent-505, score-4.08]
</p><p>92 n  z∈S  z∈S  (19)  Combining (18) and (19), we get inf p(n) , z → inf p, z . [sent-507, score-0.446]
</p><p>93 f Proof (‘only if’) Suppose we could prove that ∀ε > 0, ∃δ > 0 such that ∀p ∈ ∆ K , max py − ppred(z) ≥ ε ⇒ p, z − inf p, z ≥ δ . [sent-513, score-0.694]
</p><p>94 z∈S  y  (20)  Using this it immediately follows that ∀ε, H(ε) > 0 where H(ε) =  inf  p∈∆K ,z∈S  { p, z − inf p, z : max py − ppred(z) ≥ ε} . [sent-514, score-0.917]
</p><p>95 Suppose S is classiﬁcation calibrated but there exists ε > 0 and a sequence (z(n) , p(n) ) such that (n) (n) (21) ppred(z(n) ) ≤ max py − ε y  and p(n) , z(n) − inf p(n) , z z∈S  →0. [sent-518, score-0.847]
</p><p>96 Hence j  lim sup p, z(n) = lim sup ∑ py zy ≤ lim p(n) , z(n) = inf p, z . [sent-523, score-1.321]
</p><p>97 (‘if’) If S is not classiﬁcation calibrated then by Theorem 7 and Propositions 9 and 10, we have a point in the boundary of some S (i) where there are at least two normals and which does not have a unique minimum coordinate. [sent-525, score-0.244]
</p><p>98 Therefore, we must have a sequence z (n) in R such that δn = p, z(n) − inf p, z → 0  (22)  ppred(z(n) ) < max py . [sent-527, score-0.735]
</p><p>99 Deﬁne Fn as  Fn = {gn } ∪ (F ∩ {f : ∀x, p, Ψ(f(x) − inf p, z > 4δn } z∈S  ∩ {f : ∀x, ∀ j, |Ψ j (f(x)| < Mn }) where Mn ↑ ∞ is a sequence which we will ﬁx later. [sent-532, score-0.264]
</p><p>100 Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. [sent-591, score-0.189]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('py', 0.471), ('zy', 0.388), ('fy', 0.268), ('inf', 0.223), ('calibration', 0.205), ('nk', 0.203), ('argmin', 0.199), ('ppred', 0.192), ('pred', 0.175), ('rk', 0.134), ('ulticlass', 0.133), ('fn', 0.128), ('ewari', 0.113), ('onsistency', 0.113), ('calibrated', 0.112), ('multiclass', 0.108), ('argmax', 0.104), ('boundedly', 0.103), ('normals', 0.092), ('dist', 0.089), ('maxy', 0.086), ('consistency', 0.082), ('subsequence', 0.08), ('gn', 0.076), ('hy', 0.076), ('bartlett', 0.074), ('convex', 0.073), ('lassification', 0.072), ('pm', 0.068), ('lim', 0.063), ('conv', 0.063), ('zm', 0.061), ('admissibility', 0.059), ('proj', 0.059), ('differentiable', 0.057), ('infz', 0.056), ('admissible', 0.056), ('convergent', 0.048), ('coordinates', 0.046), ('checking', 0.045), ('classi', 0.045), ('miny', 0.045), ('inclined', 0.045), ('boundedness', 0.045), ('interchanging', 0.044), ('proposition', 0.042), ('predictor', 0.041), ('br', 0.041), ('sequence', 0.041), ('boundary', 0.04), ('characterization', 0.04), ('mum', 0.04), ('invertible', 0.04), ('loss', 0.039), ('contradiction', 0.038), ('ultimately', 0.038), ('ambuj', 0.038), ('projections', 0.037), ('claim', 0.037), ('ex', 0.036), ('propositions', 0.036), ('risk', 0.034), ('lemma', 0.034), ('bounded', 0.034), ('suppose', 0.032), ('consistent', 0.031), ('bisector', 0.03), ('bredensteiner', 0.03), ('quadrant', 0.03), ('hinge', 0.029), ('symmetry', 0.028), ('gy', 0.028), ('iff', 0.027), ('closed', 0.027), ('inconsistent', 0.027), ('pn', 0.027), ('implies', 0.027), ('mn', 0.026), ('vertices', 0.026), ('bayes', 0.025), ('tong', 0.025), ('watkins', 0.025), ('binary', 0.025), ('possesses', 0.025), ('padded', 0.025), ('symmetric', 0.025), ('sup', 0.025), ('supporting', 0.024), ('coordinate', 0.024), ('differentiability', 0.023), ('tewari', 0.022), ('theorem', 0.022), ('weston', 0.022), ('crammer', 0.022), ('sign', 0.021), ('pk', 0.021), ('multicategory', 0.02), ('cation', 0.02), ('ey', 0.019), ('converging', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="61-tfidf-1" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: Binary classiﬁcation is a well studied special case of the classiﬁcation problem. Statistical properties of binary classiﬁers, such as consistency, have been investigated in a variety of settings. Binary classiﬁcation methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classiﬁcation method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufﬁcient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature. Keywords: multiclass classiﬁcation, consistency, Bayes risk</p><p>2 0.1217258 <a title="61-tfidf-2" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>Author: Peter L. Bartlett, Mikhail Traskin</p><p>Abstract: The risk, or probability of error, of the classiﬁer produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after n1−ε iterations—for sample size n and ε ∈ (0, 1)—the sequence of risks of the classiﬁers it produces approaches the Bayes risk. Keywords: boosting, adaboost, consistency</p><p>3 0.10472459 <a title="61-tfidf-3" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>Author: Kenji Fukumizu, Francis R. Bach, Arthur Gretton</p><p>Abstract: While kernel canonical correlation analysis (CCA) has been applied in many contexts, the convergence of ﬁnite sample estimates of the associated functions to their population counterparts has not yet been established. This paper gives a mathematical proof of the statistical convergence of kernel CCA, providing a theoretical justiﬁcation for the method. The proof uses covariance operators deﬁned on reproducing kernel Hilbert spaces, and analyzes the convergence of their empirical estimates of ﬁnite rank to their population counterparts, which can have inﬁnite rank. The result also gives a sufﬁcient condition for convergence on the regularization coefﬁcient involved in kernel CCA: this should decrease as n−1/3 , where n is the number of data. Keywords: canonical correlation analysis, kernel, consistency, regularization, Hilbert space</p><p>4 0.088706017 <a title="61-tfidf-4" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: One of the nice properties of kernel classiﬁers such as SVMs is that they often produce sparse solutions. However, the decision functions of these classiﬁers cannot always be used to estimate the conditional probability of the class label. We investigate the relationship between these two properties and show that these are intimately related: sparseness does not occur when the conditional probabilities can be unambiguously estimated. We consider a family of convex loss functions and derive sharp asymptotic results for the fraction of data that becomes support vectors. This enables us to characterize the exact trade-off between sparseness and the ability to estimate conditional probabilities for these loss functions. Keywords: kernel methods, support vector machines, sparseness, estimating conditional probabilities</p><p>5 0.085612774 <a title="61-tfidf-5" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>Author: Ryan M. Rifkin, Ross A. Lippert</p><p>Abstract: Regularization is an approach to function learning that balances ﬁt and smoothness. In practice, we search for a function f with a ﬁnite representation f = ∑i ci φi (·). In most treatments, the ci are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning. Keywords: kernel machines, duality, optimization, convex analysis, kernel learning</p><p>6 0.06560351 <a title="61-tfidf-6" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>7 0.057851799 <a title="61-tfidf-7" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>8 0.057095967 <a title="61-tfidf-8" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>9 0.053116206 <a title="61-tfidf-9" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>10 0.045383979 <a title="61-tfidf-10" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>11 0.039001022 <a title="61-tfidf-11" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>12 0.038104456 <a title="61-tfidf-12" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>13 0.037527449 <a title="61-tfidf-13" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>14 0.035752892 <a title="61-tfidf-14" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>15 0.031719252 <a title="61-tfidf-15" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>16 0.030468332 <a title="61-tfidf-16" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>17 0.030338258 <a title="61-tfidf-17" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>18 0.027448162 <a title="61-tfidf-18" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>19 0.02671002 <a title="61-tfidf-19" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>20 0.026237063 <a title="61-tfidf-20" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.197), (1, -0.198), (2, 0.027), (3, -0.018), (4, 0.103), (5, 0.053), (6, -0.144), (7, 0.114), (8, 0.075), (9, 0.037), (10, 0.149), (11, 0.026), (12, -0.016), (13, 0.049), (14, -0.015), (15, -0.005), (16, 0.026), (17, 0.127), (18, 0.136), (19, 0.299), (20, 0.175), (21, 0.014), (22, 0.268), (23, -0.029), (24, -0.137), (25, -0.008), (26, 0.181), (27, -0.057), (28, 0.086), (29, 0.11), (30, -0.052), (31, -0.122), (32, -0.023), (33, 0.041), (34, -0.102), (35, -0.082), (36, 0.043), (37, 0.044), (38, 0.04), (39, 0.044), (40, 0.053), (41, -0.01), (42, 0.068), (43, 0.057), (44, 0.022), (45, -0.021), (46, 0.06), (47, -0.065), (48, 0.026), (49, 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95263433 <a title="61-lsi-1" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: Binary classiﬁcation is a well studied special case of the classiﬁcation problem. Statistical properties of binary classiﬁers, such as consistency, have been investigated in a variety of settings. Binary classiﬁcation methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classiﬁcation method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufﬁcient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature. Keywords: multiclass classiﬁcation, consistency, Bayes risk</p><p>2 0.63480985 <a title="61-lsi-2" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>Author: Kenji Fukumizu, Francis R. Bach, Arthur Gretton</p><p>Abstract: While kernel canonical correlation analysis (CCA) has been applied in many contexts, the convergence of ﬁnite sample estimates of the associated functions to their population counterparts has not yet been established. This paper gives a mathematical proof of the statistical convergence of kernel CCA, providing a theoretical justiﬁcation for the method. The proof uses covariance operators deﬁned on reproducing kernel Hilbert spaces, and analyzes the convergence of their empirical estimates of ﬁnite rank to their population counterparts, which can have inﬁnite rank. The result also gives a sufﬁcient condition for convergence on the regularization coefﬁcient involved in kernel CCA: this should decrease as n−1/3 , where n is the number of data. Keywords: canonical correlation analysis, kernel, consistency, regularization, Hilbert space</p><p>3 0.61567408 <a title="61-lsi-3" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: One of the nice properties of kernel classiﬁers such as SVMs is that they often produce sparse solutions. However, the decision functions of these classiﬁers cannot always be used to estimate the conditional probability of the class label. We investigate the relationship between these two properties and show that these are intimately related: sparseness does not occur when the conditional probabilities can be unambiguously estimated. We consider a family of convex loss functions and derive sharp asymptotic results for the fraction of data that becomes support vectors. This enables us to characterize the exact trade-off between sparseness and the ability to estimate conditional probabilities for these loss functions. Keywords: kernel methods, support vector machines, sparseness, estimating conditional probabilities</p><p>4 0.42485958 <a title="61-lsi-4" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>Author: Peter L. Bartlett, Mikhail Traskin</p><p>Abstract: The risk, or probability of error, of the classiﬁer produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after n1−ε iterations—for sample size n and ε ∈ (0, 1)—the sequence of risks of the classiﬁers it produces approaches the Bayes risk. Keywords: boosting, adaboost, consistency</p><p>5 0.36009577 <a title="61-lsi-5" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>Author: Ryan M. Rifkin, Ross A. Lippert</p><p>Abstract: Regularization is an approach to function learning that balances ﬁt and smoothness. In practice, we search for a function f with a ﬁnite representation f = ∑i ci φi (·). In most treatments, the ci are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning. Keywords: kernel machines, duality, optimization, convex analysis, kernel learning</p><p>6 0.25900972 <a title="61-lsi-6" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>7 0.257709 <a title="61-lsi-7" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>8 0.25055012 <a title="61-lsi-8" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>9 0.22792253 <a title="61-lsi-9" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>10 0.19323239 <a title="61-lsi-10" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>11 0.17335719 <a title="61-lsi-11" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>12 0.17047948 <a title="61-lsi-12" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>13 0.16529919 <a title="61-lsi-13" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>14 0.16436952 <a title="61-lsi-14" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>15 0.15777922 <a title="61-lsi-15" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>16 0.15058343 <a title="61-lsi-16" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>17 0.14488271 <a title="61-lsi-17" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>18 0.13613313 <a title="61-lsi-18" href="./jmlr-2007-Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning.html">85 jmlr-2007-Transfer Learning via Inter-Task Mappings for Temporal Difference Learning</a></p>
<p>19 0.13564081 <a title="61-lsi-19" href="./jmlr-2007-Multi-class_Protein_Classification_Using_Adaptive_Codes.html">57 jmlr-2007-Multi-class Protein Classification Using Adaptive Codes</a></p>
<p>20 0.13506256 <a title="61-lsi-20" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.015), (8, 0.015), (10, 0.012), (12, 0.051), (15, 0.017), (28, 0.062), (40, 0.051), (48, 0.02), (60, 0.1), (79, 0.367), (80, 0.012), (85, 0.034), (98, 0.152)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73601657 <a title="61-lda-1" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: Binary classiﬁcation is a well studied special case of the classiﬁcation problem. Statistical properties of binary classiﬁers, such as consistency, have been investigated in a variety of settings. Binary classiﬁcation methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classiﬁcation method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufﬁcient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature. Keywords: multiclass classiﬁcation, consistency, Bayes risk</p><p>2 0.46945935 <a title="61-lda-2" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>Author: Jean-Yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, ﬁrst, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester (1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand (see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds. Keywords: statistical learning theory, PAC-Bayes theorems, generalization error bounds</p><p>3 0.46768665 <a title="61-lda-3" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>4 0.45420846 <a title="61-lda-4" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>Author: Niels Landwehr, Kristian Kersting, Luc De Raedt</p><p>Abstract: A novel relational learning approach that tightly integrates the na¨ve Bayes learning scheme with ı the inductive logic programming rule-learner FOIL is presented. In contrast to previous combinations that have employed na¨ve Bayes only for post-processing the rule sets, the presented approach ı employs the na¨ve Bayes criterion to guide its search directly. The proposed technique is impleı mented in the N FOIL and T FOIL systems, which employ standard na¨ve Bayes and tree augmented ı na¨ve Bayes models respectively. We show that these integrated approaches to probabilistic model ı and rule learning outperform post-processing approaches. They also yield signiﬁcantly more accurate models than simple rule learning and are competitive with more sophisticated ILP systems. Keywords: rule learning, na¨ve Bayes, statistical relational learning, inductive logic programming ı</p><p>5 0.45399857 <a title="61-lda-5" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We present a uniﬁed and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including 1 , 2 , 2 , and 1 + 2 style regularization. We propose an algorithm solving a large and 2 2 general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and uniﬁes techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models. Keywords: maximum entropy, density estimation, regularization, iterative scaling, species distribution modeling</p><p>6 0.45374253 <a title="61-lda-6" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>7 0.44996268 <a title="61-lda-7" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>8 0.44995457 <a title="61-lda-8" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>9 0.44964296 <a title="61-lda-9" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>10 0.44852087 <a title="61-lda-10" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>11 0.44824243 <a title="61-lda-11" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>12 0.44022959 <a title="61-lda-12" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>13 0.4402011 <a title="61-lda-13" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>14 0.43968499 <a title="61-lda-14" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>15 0.43756366 <a title="61-lda-15" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>16 0.4346638 <a title="61-lda-16" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>17 0.43112251 <a title="61-lda-17" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>18 0.43091726 <a title="61-lda-18" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>19 0.43052197 <a title="61-lda-19" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>20 0.42925006 <a title="61-lda-20" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
