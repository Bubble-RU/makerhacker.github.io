<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-4" href="#">jmlr2007-4</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2007-4-pdf" href="http://jmlr.org/papers/volume8/hue07a/hue07a.pdf">pdf</a></p><p>Author: Carine Hue, Marc Boullé</p><p>Abstract: In this paper, we consider the supervised learning task which consists in predicting the normalized rank of a numerical variable. We introduce a novel probabilistic approach to estimate the posterior distribution of the target rank conditionally to the predictors. We turn this learning task into a model selection problem. For that, we deﬁne a 2D partitioning family obtained by discretizing numerical variables and grouping categorical ones and we derive an analytical criterion to select the partition with the highest posterior probability. We show how these partitions can be used to build univariate predictors and multivariate ones under a naive Bayes assumption. We also propose a new evaluation criterion for probabilistic rank estimators. Based on the logarithmic score, we show that such criterion presents the advantage to be minored, which is not the case of the logarithmic score computed for probabilistic value estimator. A ﬁrst set of experimentations on synthetic data shows the good properties of the proposed criterion and of our partitioning approach. A second set of experimentations on real data shows competitive performance of the univariate and selective naive Bayes rank estimators projected on the value range compared to methods submitted to a recent challenge on probabilistic metric regression tasks. Our approach is applicable for all regression problems with categorical or numerical predictors. It is particularly interesting for those with a high number of predictors as it automatically detects the variables which contain predictive information. It builds pertinent predictors of the normalized rank of the numerical target from one or several predictors. As the criteria selection is regularized by the presence of a prior and a posterior term, it does not suffer from overﬁtting. Keywords: rank regression, probabilistic approach, 2D partitioning, non parametric estimation, Bayesian model selection</p><p>Reference: <a title="jmlr-2007-4-reference" href="../jmlr2007_reference/jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 COM  France Telecom R&D; 2, avenue Pierre Marzin 22307 Lannion cedex, France  Editors: Isabelle Guyon and Amir Saffari  Abstract In this paper, we consider the supervised learning task which consists in predicting the normalized rank of a numerical variable. [sent-5, score-0.301]
</p><p>2 We introduce a novel probabilistic approach to estimate the posterior distribution of the target rank conditionally to the predictors. [sent-6, score-0.421]
</p><p>3 For that, we deﬁne a 2D partitioning family obtained by discretizing numerical variables and grouping categorical ones and we derive an analytical criterion to select the partition with the highest posterior probability. [sent-8, score-0.483]
</p><p>4 We show how these partitions can be used to build univariate predictors and multivariate ones under a naive Bayes assumption. [sent-9, score-0.312]
</p><p>5 We also propose a new evaluation criterion for probabilistic rank estimators. [sent-10, score-0.338]
</p><p>6 A second set of experimentations on real data shows competitive performance of the univariate and selective naive Bayes rank estimators projected on the value range compared to methods submitted to a recent challenge on probabilistic metric regression tasks. [sent-13, score-0.601]
</p><p>7 It builds pertinent predictors of the normalized rank of the numerical target from one or several predictors. [sent-16, score-0.573]
</p><p>8 Keywords: rank regression, probabilistic approach, 2D partitioning, non parametric estimation, Bayesian model selection  1. [sent-18, score-0.31]
</p><p>9 Finally, we outline our contribution which aims at selecting a probabilistic predictive model for the rank of a numerical target with a nonparametric Bayesian approach. [sent-21, score-0.604]
</p><p>10 A less known task is the case where the target variable is ordinal, usually called ordinal regression (see Chu and Ghahramani, 2005, for a state of the art). [sent-25, score-0.373]
</p><p>11 However, this predeﬁned target discretization can separate values which form a pertinent prediction interval. [sent-33, score-0.392]
</p><p>12 More precisely, for a given numerical target variable, we aim at computing an estimator of its rank. [sent-35, score-0.328]
</p><p>13 In linear regression for example, an estimator based on the centered ranks in the minimization of the least squared equation is proposed in the approach of Hettmansperger and McKean (1998). [sent-39, score-0.344]
</p><p>14 Secondly in some applications, predicting the rank of a target variable is more interesting than predicting its intrinsic value. [sent-40, score-0.411]
</p><p>15 2 Deterministic and Probabilistic Regression Whatever the learning task, the simpler approach is determinist in so far as its outputs is deterministic: the majority class in classiﬁcation, the mean rank in ordinal regression and the conditional mean in metric regression. [sent-43, score-0.387]
</p><p>16 These punctual predictors turn out to be inefﬁcient as soon as conﬁdence intervals or prediction of extreme values are needed. [sent-44, score-0.298]
</p><p>17 In this context, quantile regression or density estimation aims at estimating the predictive density more accurately. [sent-45, score-0.394]
</p><p>18 Conditional density estimation aims at giving for any couple (x, y) an estimator of the predictive density p(y|x). [sent-58, score-0.314]
</p><p>19 The parametric approaches assume that the predictive density belongs to a ﬁxed parametric family and then reduce the density estimation problem to the estimation of a parameter vector. [sent-59, score-0.306]
</p><p>20 Instead of looking for an estimator of the value of the target variable, we aim at building an estimator of the normalized rank (between 0 and 1) of this variable. [sent-69, score-0.621]
</p><p>21 Our second objective is to propose an evaluation criterion for these rank probabilistic estimators. [sent-70, score-0.338]
</p><p>22 First, we propose a non parametric Bayesian method to build a probabilistic estimator speciﬁed by a set of quantiles of the rank cumulative distribution function. [sent-71, score-0.512]
</p><p>23 Moreover, in the case of several predictor variables, the multivariate estimator is obtained as a combination of univariate estimators under a naive Bayes assumption. [sent-73, score-0.752]
</p><p>24 Each univariate estimator is obtained from a 2D partition of the space (predictor, target) assuming that the rank density is constant on each cell of the partition. [sent-74, score-0.661]
</p><p>25 Secondly, we propose and evaluate a new criterion to evaluate probabilistic regression methods by comparing the rank predictive density and the true insertion rank of all test instances. [sent-76, score-0.727]
</p><p>26 Compared to our previous work introduced in a conference paper (see Boull´ and Hue, 2006), the approach is much more detailed, the selection criteria is completely e explicited and we propose an estimator of the rank predictive density. [sent-78, score-0.443]
</p><p>27 In the third section we ﬁrst expose how we can build a univariate estimator of the rank predictive density from each 2D partition. [sent-80, score-0.652]
</p><p>28 2729  ´ H UE AND B OULL E  The fourth section focuses on the important topic of the evaluation of such rank regression models. [sent-82, score-0.301]
</p><p>29 We then propose to use one of them, the logarithmic score, for rank probabilistic estimators without the shortcomings noted for probabilistic estimators based on values. [sent-84, score-0.395]
</p><p>30 e e For regression tasks, we present in the sequel two 2D partitioning methods depending on whether the considered predictor is numerical or categorical. [sent-92, score-0.672]
</p><p>31 For numerical predictors, the 2D partition is the grid resulting from the discretization of both target and predictor. [sent-93, score-0.621]
</p><p>32 For categorical predictors, the 2D partition is the grid resulting from the discretization of the numerical target and from the grouping of the categorical predictor. [sent-94, score-0.793]
</p><p>33 1 The 2D Discretization for Probabilistic Regression with Numerical Predictor  1  2  3 4 5 Petal Length  6  7  Figure 1: Scatter-plot of the iris data set of Fisher (1936) considered for a regression problem with the petal length variable as predictor and the sepal length variable as target. [sent-100, score-1.106]
</p><p>34 In order to illustrate the regression problem with numerical predictor, we present in Figure 1 the scatter-plot of the iris data set considered for a regression problem with the petal length variable as predictor and the sepal length variable as target. [sent-101, score-1.234]
</p><p>35 The ﬁgure shows that iris plants with petal length below 2 cm always have a sepal length below 6 cm. [sent-102, score-0.561]
</p><p>36 We propose to exhibit the predictive information of the petal length variable by discretizing both the predictor and the target variables. [sent-103, score-1.042]
</p><p>37 5  P ROBABILISTIC R ANK R EGRESSION WITH BAYESIAN PARTITIONING  1  2  3  4  5  6  7  1  2  Petal Length  3  4  5  6  7  Petal Length  Figure 2: Two discretization grids with 6 or 96 cells, describing the correlation between the petal length and sepal length variables of the Iris data set. [sent-112, score-0.701]
</p><p>38 The issue is to describe the predictive distribution of the rank of the target value given the rank of the predictor value. [sent-116, score-1.104]
</p><p>39 Deﬁnition 1 A regression 2D discretization model is deﬁned by: 1. [sent-118, score-0.294]
</p><p>40 a number of intervals for the target and predictor variables; 2. [sent-119, score-0.799]
</p><p>41 a partition of the predictor variable speciﬁed on the ranks of the predictor values; 3. [sent-120, score-1.194]
</p><p>42 for each predictor interval, the repartition of the instances among the target intervals speciﬁed by the instance counts locally to each predictor interval. [sent-121, score-1.356]
</p><p>43 Notations N : the number of training instances I : the number of predictor intervals J : the number of target intervals Ni. [sent-122, score-1.089]
</p><p>44 : the number of instances in the predictor interval i N. [sent-123, score-0.654]
</p><p>45 j : the number of instances in the target interval j 2731  ´ H UE AND B OULL E  Ni j : the number of instances in the grid cell associated to predictor interval i and the target interval j. [sent-124, score-1.442]
</p><p>46 j can be deduced by adding the Ni j for each predictor interval, according to N. [sent-128, score-0.464]
</p><p>47 Moreover, mathematically speaking, it enables to write the last factors in the precedent equation as products over the predictor intervals. [sent-137, score-0.47]
</p><p>48 Deﬁnition 2 The prior for the parameters of a regression 2D discretization model and the likelihood function of the data given a model are chosen hierarchically and uniformly at each level: 1. [sent-144, score-0.321]
</p><p>49 for a given number of predictor intervals I, every set of intervals is equiprobable, 3. [sent-146, score-0.825]
</p><p>50 for a given predictor interval, every distribution of the instances on the target intervals is equiprobable, 4. [sent-147, score-0.892]
</p><p>51 the distributions of the target intervals on each predictor interval are independent from each other, 5. [sent-148, score-0.929]
</p><p>52 for a given target interval, every distribution of the rank of the target values is equiprobable. [sent-149, score-0.542]
</p><p>53 The second hypothesis is that all the divisions into I intervals are equiprobable for a given I. [sent-163, score-0.313]
</p><p>54 Computing the probability of one set of intervals turns into the combinatorial evaluation of the number of possible interval sets. [sent-164, score-0.355]
</p><p>55 Dividing the predictor values into I intervals is equivalent to decompose the number N as the sum of the Ni. [sent-165, score-0.628]
</p><p>56 , every distribution of the instances on the J target intervals are equiprobable. [sent-171, score-0.461]
</p><p>57 The prior terms being explicited, it remains to evaluate the likelihood on each predictor interval, that is the probability to observe the data restricted to each interval knowing the multinomial distribution model on each interval. [sent-177, score-0.561]
</p><p>58 To ﬁnish, according to the last hypothesis, for a given i2 target interval, every distribution of the ranks of the target values are equiprobable which leads to the last terms. [sent-188, score-0.627]
</p><p>59 This means that a discretization with one cell is always more likely that a 2D discretization with N 2 elementary cells. [sent-192, score-0.526]
</p><p>60 We start with an initial random model and alternate the optimization on the predictor and target variables. [sent-194, score-0.602]
</p><p>61 j , we optimize the discretization of the predictor variable to determine the values of I, Ni and Ni j . [sent-196, score-0.692]
</p><p>62 Then, for this predictor discretization, we optimize the discretization of the target variable to determine new values of J, N. [sent-197, score-0.863]
</p><p>63 The univariate discretization optimizations are performed using the MODL discretization algorithm. [sent-200, score-0.594]
</p><p>64 In other words, for a given target discretization, the criterion to minimize is not identical to the criterion to minimize given a predictor discretization. [sent-210, score-0.722]
</p><p>65 The evaluation criterion c(M) given in Formula (1) is related to the probability that a regression 2D discretization model M explains the target variable. [sent-211, score-0.553]
</p><p>66 In previous work (see Boull´ and Hue, e 2006), we propose to use it to build a relevance criterion for the predictor variables in a regression problem. [sent-212, score-0.591]
</p><p>67 The predictor variables can be sorted by decreasing probability of explaining the target variable. [sent-213, score-0.602]
</p><p>68 In order to provide a normalized indicator, we consider the following transformation of c: g (M) = 1 −  c (M) , c (M0 ) /  (2)  where M0 is the null model with only one interval for the predictor and target variables. [sent-214, score-0.778]
</p><p>69 It has value 0 for the null model and is maximal when the best possible explanation of the target ranks conditionally to the predictor ranks is achieved. [sent-217, score-0.94]
</p><p>70 The drawbacks of such an algorithm are the unsupervised way the predictor pre-discretizations are led and the tuning of the merging parameter needed in the postprocessing step. [sent-224, score-0.47]
</p><p>71 5  90  Figure 4: Histograms of the age variable for three groups of the workclass factor  1. [sent-251, score-0.295]
</p><p>72 a number of intervals for the target variable and a number of groups for the predictor variable; 2. [sent-252, score-0.897]
</p><p>73 a partition of the predictor variable in a ﬁnite number of groups; 3. [sent-253, score-0.594]
</p><p>74 for each predictor group, the repartition of the instances among the target intervals speciﬁed by the instance counts locally to each predictor group. [sent-254, score-1.356]
</p><p>75 Notations N : the number of training instances V : the number of predictor values I : the number of predictor groups J : the number of target intervals ι(ν) : the group index the ν value belongs to Ni. [sent-255, score-1.381]
</p><p>76 : the number of instances in the predictor group i N. [sent-256, score-0.524]
</p><p>77 j : the number of instances in the target interval j Ni j : the number of instances in the grid cell associated to predictor group i and the target interval j. [sent-257, score-1.312]
</p><p>78 j can be deduced by adding the Ni j for each predictor group. [sent-260, score-0.464]
</p><p>79 for a given number of groups I, every partition of the predictor values into I groups is equiprobable, 4. [sent-264, score-0.67]
</p><p>80 for a given predictor group, every distribution of the instances on the target intervals is equiprobable, 5. [sent-265, score-0.892]
</p><p>81 the distributions of the target intervals on each predictor group are independent from each other, 6. [sent-266, score-0.799]
</p><p>82 for a given target interval, every distribution of the rank of the target values is equiprobable. [sent-267, score-0.542]
</p><p>83 The deﬁnition of the regression discretization/grouping model space and its prior distribution leads to the evaluation criterion given in Formula (3) for a discretization/grouping model M: I  c(M) = log(V ) + log (N) + log B(V, I) + ∑ log i=1  Ni. [sent-268, score-0.296]
</p><p>84 i=1  This criterion can be deduced from the grouping criterion in classiﬁcation (see Boull´ , 2005) and e the 2D discretization criterion in regression presented in the previous section. [sent-282, score-0.533]
</p><p>85 From 2D Partitioning to Rank Predictive Cumulative Distribution Estimate In this section we expose how we can build a univariate estimator of the rank predictive density from each 2D partition and how to obtain multivariate predictors under the naive Bayes assumption. [sent-284, score-0.908]
</p><p>86 1 From Values to Normalized Training Ranks and Vice-Versa As seen in the precedent section, the MODL partitions are deﬁned only with the ranks of the training instances and not with their values. [sent-286, score-0.301]
</p><p>87 yT ) +yT (β (β  where β j = ∑l=1 Nl and the J value partition intervals are ]−∞, b1 [, [b1 , b2 [, . [sent-301, score-0.32]
</p><p>88 For a numerical value, its rank interval index is equal to its value interval index. [sent-305, score-0.515]
</p><p>89 As presented in the introduction, ordinal regression aims at predicting an ordinal variable which takes a ﬁnite number of ordered values, most of the time already known in advance. [sent-307, score-0.317]
</p><p>90 In order to manipulate normalized values in [0, 1], we consider the NT elementary 1 equal-width rank intervals of [0, 1] denoted by Ten for n = 1, . [sent-309, score-0.485]
</p><p>91 These intervals are centered on the normalized ranks R DT (yT ) = (n) T j  j+1 )  j  2  1 2NT  n + NT of the training instances obtained by projection on [0, 1] of the rank of y T among DT . [sent-316, score-0.705]
</p><p>92 For a new value y unseen during training, we deﬁne its rank R DT (y) as the average of the normalized ranks of yT 1 ) and yT 2 ) such that yT 1 ) ≤ y < yT 2 ) . [sent-321, score-0.415]
</p><p>93 2 Univariate Case We illustrate the construction of the univariate estimator from the 2D partitioning on a synthetic data set proposed during the recent predictive uncertainty in environmental modelling competition (see Cawley et al. [sent-326, score-0.536]
</p><p>94 The optimal MODL rank intervals are denoted Pi for i = 1, . [sent-330, score-0.397]
</p><p>95 The ﬁrst and the last rank intervals are of the form [0, k f /NT [ and [kl /NT , 1] respectively and the other intervals are of the form [k1 /NT , k2 /NT [. [sent-337, score-0.594]
</p><p>96 For the predictor component x of a new instance whose rank range is Pi (x), the number of Y 3 2 1  T5 T4 T3 T2 T1 Total  y 4 y 3  0 y 2 y 1  -1 -2  x 1  -3 0  0. [sent-346, score-0.631]
</p><p>97 examples in each grid cell give us an estimator of the probability that the target standardized rank belongs to a given range T j : PModl (RDT (y) ∈ T j | RDT (x) ∈ Pi (x)) =  Ni j . [sent-351, score-0.591]
</p><p>98 Assuming that the conditional rank density is constant over each rank interval, the probabilities of the elementary intervals Ten for n = 1, . [sent-353, score-0.706]
</p><p>99 The MODL estimators are plotted for each of the seven rank predictor ranges on Figure 8 for the synthetic data set. [sent-363, score-0.703]
</p><p>100 The marks on the x−axis correspond to the normalized target ranks of the bound75 aries training instances exhibited by the target partition: 384 ≈ 0. [sent-364, score-0.65]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('predictor', 0.431), ('nt', 0.296), ('discretization', 0.221), ('rank', 0.2), ('petal', 0.199), ('intervals', 0.197), ('target', 0.171), ('ranks', 0.169), ('modl', 0.166), ('univariate', 0.152), ('sepal', 0.149), ('boull', 0.133), ('interval', 0.13), ('ni', 0.127), ('partition', 0.123), ('age', 0.119), ('yt', 0.119), ('ank', 0.118), ('equiprobable', 0.116), ('oull', 0.116), ('partitioning', 0.113), ('quantile', 0.109), ('ue', 0.104), ('predictive', 0.102), ('estimator', 0.102), ('predictors', 0.101), ('robabilistic', 0.1), ('hue', 0.098), ('rdt', 0.098), ('instances', 0.093), ('ordinal', 0.089), ('gov', 0.078), ('workclass', 0.078), ('egression', 0.076), ('categorical', 0.073), ('regression', 0.073), ('chu', 0.066), ('length', 0.066), ('parametric', 0.06), ('criterion', 0.06), ('carine', 0.059), ('experimentations', 0.059), ('rg', 0.059), ('groups', 0.058), ('numerical', 0.055), ('grid', 0.051), ('probabilistic', 0.05), ('dt', 0.048), ('normalized', 0.046), ('log', 0.045), ('cell', 0.042), ('elementary', 0.042), ('histograms', 0.042), ('iris', 0.042), ('density', 0.042), ('bj', 0.041), ('marc', 0.041), ('variable', 0.04), ('chaudhuri', 0.039), ('explicited', 0.039), ('fig', 0.039), ('grain', 0.039), ('pmodl', 0.039), ('postprocessing', 0.039), ('precedent', 0.039), ('cm', 0.039), ('quantiles', 0.038), ('bayesian', 0.037), ('synthetic', 0.037), ('frequency', 0.035), ('cumulative', 0.035), ('estimators', 0.035), ('deduced', 0.033), ('freeze', 0.033), ('cawley', 0.033), ('discretizing', 0.033), ('repartition', 0.033), ('frequencies', 0.033), ('naive', 0.032), ('modelling', 0.03), ('emp', 0.03), ('neighbourhood', 0.03), ('private', 0.03), ('evaluation', 0.028), ('cells', 0.028), ('build', 0.027), ('obtains', 0.027), ('expose', 0.027), ('hierarchically', 0.027), ('self', 0.027), ('splines', 0.027), ('whatever', 0.027), ('formalize', 0.026), ('grouping', 0.026), ('aims', 0.026), ('pi', 0.026), ('logarithmic', 0.025), ('combinatorics', 0.025), ('standardized', 0.025), ('conditional', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="4-tfidf-1" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>Author: Carine Hue, Marc Boullé</p><p>Abstract: In this paper, we consider the supervised learning task which consists in predicting the normalized rank of a numerical variable. We introduce a novel probabilistic approach to estimate the posterior distribution of the target rank conditionally to the predictors. We turn this learning task into a model selection problem. For that, we deﬁne a 2D partitioning family obtained by discretizing numerical variables and grouping categorical ones and we derive an analytical criterion to select the partition with the highest posterior probability. We show how these partitions can be used to build univariate predictors and multivariate ones under a naive Bayes assumption. We also propose a new evaluation criterion for probabilistic rank estimators. Based on the logarithmic score, we show that such criterion presents the advantage to be minored, which is not the case of the logarithmic score computed for probabilistic value estimator. A ﬁrst set of experimentations on synthetic data shows the good properties of the proposed criterion and of our partitioning approach. A second set of experimentations on real data shows competitive performance of the univariate and selective naive Bayes rank estimators projected on the value range compared to methods submitted to a recent challenge on probabilistic metric regression tasks. Our approach is applicable for all regression problems with categorical or numerical predictors. It is particularly interesting for those with a high number of predictors as it automatically detects the variables which contain predictive information. It builds pertinent predictors of the normalized rank of the numerical target from one or several predictors. As the criteria selection is regularized by the presence of a prior and a posterior term, it does not suffer from overﬁtting. Keywords: rank regression, probabilistic approach, 2D partitioning, non parametric estimation, Bayesian model selection</p><p>2 0.096477605 <a title="4-tfidf-2" href="./jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</a></p>
<p>Author: Marc Boullé</p><p>Abstract: The naive Bayes classiﬁer has proved to be very effective on many real data applications. Its performance usually beneﬁts from an accurate estimation of univariate conditional probabilities and from variable selection. However, although variable selection is a desirable feature, it is prone to overﬁtting. In this paper, we introduce a Bayesian regularization technique to select the most probable subset of variables compliant with the naive Bayes assumption. We also study the limits of Bayesian model averaging in the case of the naive Bayes assumption and introduce a new weighting scheme based on the ability of the models to conditionally compress the class labels. The weighting scheme on the models reduces to a weighting scheme on the variables, and ﬁnally results in a naive Bayes classiﬁer with “soft variable selection”. Extensive experiments show that the compressionbased averaged classiﬁer outperforms the Bayesian model averaging scheme. Keywords: naive Bayes, Bayesian, model selection, model averaging</p><p>3 0.080982424 <a title="4-tfidf-3" href="./jmlr-2007-Concave_Learners_for_Rankboost.html">23 jmlr-2007-Concave Learners for Rankboost</a></p>
<p>Author: Ofer Melnik, Yehuda Vardi, Cun-Hui Zhang</p><p>Abstract: Rankboost has been shown to be an effective algorithm for combining ranks. However, its ability to generalize well and not overﬁt is directly related to the choice of weak learner, in the sense that regularization of the rank function is due to the regularization properties of its weak learners. We present a regularization property called consistency in preference and conﬁdence that mathematically translates into monotonic concavity, and describe a new weak ranking learner (MWGR) that generates ranking functions with this property. In experiments combining ranks from multiple face recognition algorithms and an experiment combining text information retrieval systems, rank functions using MWGR proved superior to binary weak learners. Keywords: rankboost, ranking, convex/concave, regularization 1. Ranking Problems A ranking problem is a learning problem that involves ranks as the inputs, the outputs or both. An example where ranks are used as inputs is a collaborative ﬁltering application where people are asked to rank movies according to their preferences. In such an application the ranks assigned by different people are combined to generate recommendations. Another type of problem in which ranks are used as inputs are meta-search problems, where the ranks of multiple search engines are combined (Dwork et al., 2001). However, the inputs to a ranking problem are not always ranks. An object recognition ranking problem (Kittler and Roli, 2000) may receive as inputs a graphical representation and output a ranking of the possible objects, sorted by likelihood. The outputs of a ranking problem may also be ranks. For example, in combining multiple search engines the output are ranks which are a synthesis of the ranks from the individual search engines. A similar meta-recognition problem is the combination of the outputs of multiple face- recognition systems to improve the accuracy of detecting the correct face. While the inputs and outputs of this problem are ranks, the outputs can be simpliﬁed to only return the most likely candidate. Another example where the outputs do not need to be complete ranks could be an information retrieval combination task. In such a task the inputs might be ranks of sets of documents with respect to c 2007 Ofer Melnik, Yehuda Vardi and Cun-Hui Zhang. M ELNIK , VARDI AND Z HANG a particular query by different experts. Again, the outputs could be the complete ranks, or more simply a designation for each document of whether it is relevant or not to the particular query. 2. Rankboost The rankboost algorithm (Freund et al., 2003) tries to address this variety in ranking problems by maintaining generality in how it regards its inputs and how it applies different loss functions to outputs. The rankboost algorithm works on instances which are the discrete items (e.g., movies or faces) that either are ranked as input or are to be ranked as output. To allow a general input mechanism, the inputs to rankboost are called the ranking features of an instance, speciﬁed as f j (x) which is the j-th ranking feature of instance x. While this generality in how inputs are handled is potentially powerful, in the original rankboost paper as well as in this paper, only the case where the inputs are different rankings of the instances is considered. Thus, in this paper, the inputs to rankboost are constituent ranks, denoted as f 1 . . . fn , where f j (x ) < f j (x ) implies that instance x has a better rank than instance x under ranking f j . For example, in some of the experiments we present, each f j is the ranking result of a particular face recognition algorithm. The output of rankboost is a new ranking function, H(x), which deﬁnes a linear ordering on the instances, that is, H (x ) < H (x ) iff x has a better rank than x . In rankboost T H(x) = ∑ wt ht (x), (1) t=1 a weighted sum of weak ranking learners, where the ht (x)’s are relatively simple learned functions of the constituent rankings. To address the variety in possible loss functions of the outputs, in rankboost the desirable properties for the output loss function are speciﬁed with a favor function, Φ : X × X → R, where X is the space of instances (note that this function has been renamed from “preference function” to avoid confusion with the use of preference in this paper in Section 4). Here Φ (x , x ) > 0 means that x should be better ranked than x for a given query. It is an anti-symmetric function, that is, Φ (x , x ) = −Φ (x , x ) and Φ (x, x) = 0, which avoids loops where two instances should both be ranked better than the other. Also Φ (x , x ) = 0 when there is no favor in how two instances should be relatively ranked. For example, as described in Section 6.1, for the face recognition combination problem described above the favor function can be used to specify that the correct identity should be given a better rank than all other identities, while zeroing all other entries in the favor function, giving no favor in how incorrect identities are ranked between them. In a similar fashion for the information retrieval combination task mentioned above, the favor function can be speciﬁed such that all relevant documents should be better ranked than irrelevant documents, without specifying favor for the ordering between relevant documents and the ordering between irrelevant documents (Section 7.1). Rankboost is shown in Algorithm 1 as described in Freund et al. (2003). It uses the favor function to specify an initial weight function over instance pair orderings: D x ,x = c · max 0, Φ x , x −1 , (2) where c = ∑x ,x max (0, Φ (x , x )) . The algorithm itself is very similar to adaboost (Freund and Schapire, 1996). At each iteration the algorithm selects the weak learner that best maximizes a 792 C ONCAVE L EARNERS FOR R ANKBOOST Algorithm 1 rankboost algorithm for generating a ranking function. Input: constituent ranks, a favor function, and a class of weak learners with outputs between 0 and 1 and an appropriate training algorithm. Output: ranking function H(x) (Eq. 1) Initialize D(1) = D (Eq. 2) for t = 1 . . . T do Find weak learner, ht , that maximizes r(h) = ∑x ,x D(t) (x , x )(h(x ) − h(x )) (Eq. 4). Choose wt = 0.5 ln ((1 + r(ht )) / (1 − r(ht ))) . (Eq. 5) Update: D(t+1) (x , x ) = D(t) (x , x ) exp (wt (ht (x ) − ht (x ))) Zt where Zt is chosen to make ∑x ,x D(t+1) (x , x ) = 1 end for rank function’s utility, r(h), (Eq. 4). It then assigns it a weight in proportion to its performance and adds the weighted weak learner to the ranking function, H(x). After which the weight function D is adjusted to reﬂect the impact of the weak learner. Freund et al. (2003) prove that the rankboost algorithm iteratively minimizes the ranking loss function, a measure of the quantity of misranked pairs: ∑ D x ,x I H x ≤ H x x ,x where I is the indicator function (1 if true, 0 otherwise) and H(x) is a ranking function output by rankboost. The rankboost paper (Freund et al., 2003) uses binary weak learners of the following functional form:  i f f j (x) > θ,  1 0 i f f j (x) ≤ θ, (3) h(x) =  qde f i f f j (x) unde f ined. Each binary weak learner, h, operates on a single f j (ranking feature), giving a binary output of 0 or 1 depending on whether the instance’s rank is smaller than a learned parameter, θ. Note that function h(x) in (3), which is dependent on only one f j (x) with a ﬁxed j, is monotonic increasing but not convex or concave. If there is no rank for the instance then it returns a prespeciﬁed q de f value. 3. Rankboost, the Weak Learner and Regularization While rankboost inherits the accuracy of adaboost and has been shown to be very successful in practice, in two important ways it is very different from adaboost and similar classiﬁer leveraging algorithms (Meir and Ratsch, 2003). The ﬁrst is the choice of the weak learner, h. In adaboost the weak learner is expected to minimize the weighted empirical classiﬁcation error: N ∑ d (t) (i)I [yi = h (zi )] , i=1 where yi is the class label, I is the indicator function and d (t) is a weighting over training samples. This is a standard function to minimize in classiﬁcation with many possible types of algorithms to 793 M ELNIK , VARDI AND Z HANG choose from as possible weak learners. In contrast the weak ranking learner for rankboost (with outputs in [0, 1]) needs to maximize the following rank utility function: r = r(h) = ∑ D(t) (x , x )(h(x ) − h(x )), (4) x ,x where D(t) is the weight function over pairs of instances. As Eq. 4 contains the term h(x ) − h(x ), short of linear learners this equation can not be concave in h or easily approximated by a concave function. Therefore the weak learner needs to be optimized either by heuristics or by brute force, which limits the choice of h. It is not surprising that the original rankboost paper only included one type of weak learner, a binary threshold function (Eq. 3) that was tuned using brute force. The second difference between rankboost and adaboost also concerns the weak ranking learner. One feature of boosting that has sparked a great deal of interesting research is the algorithm’s ability to avoid overﬁtting for low noise classiﬁcation problems (with modiﬁcations to higher noise problems), see Meir and Ratsch (2003) for a survey. In contrast for rankboost it is only by limiting the type of underlying weak learners that overﬁtting is avoided. In their paper, Freund et al. (2003) show that not using weak ranking learners with cumulative positive coefﬁcients leads to overﬁtting and poor performance quite quickly. Therefore, choosing a weak learner that regularizes the ranking function, the output of rankboost, is very important for achieving accuracy and avoiding overﬁtting. It is clear from the above discussion that the choice of a weak ranking learner for rankboost is important and non trivial. First, the learner must be efﬁciently tunable with respect to Eq. 4, typically limiting its complexity. Second, the learner itself must demonstrate regularization properties that are appropriate for ranking functions. In this paper we present a new weak ranking learner that enforces consistency in preference and conﬁdence for the ranking function by being monotonic and concave. We start with a discussion of these regularization properties, theoretically justify them, and show what they mean in terms of the ﬁnal ranking function. Then we present a new learner, Minimum Weighted Group Ranks (MWGR), that satisﬁes these properties and can be readily learned. This learner is tested and compared with the binary learner of rankboost on combining multiple face recognition systems from the FERET study (Phillips et al., 2000) and on an information retrieval combination task from TREC (Voorhees and Harman, 2001). 4. Regularizing Ranking Functions with Consistency in Preference and Conﬁdence In this paper, as Freund et al. (2003), we consider ranking functions H(x) which depend on x only through the values of the ranking features, y j = f j (x), for that instance, so that H(x) = G ( f1 (x), . . . , fn (x)), for certain functions G (y1 , . . . , yn ) = G(y). Here, we assume that the f j (x) is an actual rank assigned to an instance, x, by the j-th ranker. Note that if the original data are numerical scores then they can easily be converted to rankings. Freund et al. (2003) make a strong case for conversion of raw measures to relative orderings (rankings) over combining measures directly, arguing that it is more general and avoids the semantics of particular measures. As the y j ’s are ranks instead of points in a general space, care should be taken as to the functional form of G. A great deal of literature in social choice theory (Arrow, 1951) revolves around the properties of various rank combination strategies that try to achieve fair rankings. In this machine learning case our motivations are different. Fairness is not the goal; the goal is to improve the 794 C ONCAVE L EARNERS FOR R ANKBOOST accuracy or performance of the ranking function. Thus, regularization, by functionally constraining G, is used to confer information on how to interpret ranks in order to ultimately improve accuracy. Freund et al. (2003) imposed the regularization principle of monotonicity on the ranking function. Monotonicity encompasses a belief that for individual features a smaller rank is always considered better than a bigger rank. It means that for every two rank vectors, a and b, if a j ≤ b j , j = 1, . . . , n then G(a) ≤ G(b). Monotonicity was enforced by requiring that the coefﬁcients, wt in Eq. 1, combining the binary weak learners (Eq. 3) were cumulatively positive. As shown by Freund et al. (2003), without enforcing monotonicity the rankboost algorithm quickly overﬁts. In this section we present another regularization principle, consistency in preference and conﬁdence (which includes monotonicity). A ranking function with this regularization property should be consistent in its preference for the individual rankers and also in how it captures their conﬁdence. The following example illustrates these two concepts. 4.1 Grocer Example A grocer needs to make 2 decisions, to decide between stocking oat bran vs. granola and to decide between stocking turnips vs. radishes. The grocer asks her consultants to express their opinion about stocking each item, and based on their responses makes her 2 decisions. First of all, in either problem, the grocer will adopt the opinion of her consultants if they all agree with each other, that is, they all prefer granola over oat bran in the ﬁrst problem. Lets assume the grocer considered the ﬁrst problem and chose granola over oat bran. What this implies is that the grocer adopted the opinions of the consultants that preferred granola over oat bran. Now consider the turnips vs. radishes decision. Lets say that the same consultants that liked granola more also liked radishes more (and the same ones that like oat bran more like turnips more). Also, for this decision the radish lovers actually feel more conﬁdent in their choice than they did for granola, while the turnip lovers are more unsure than they were for oat bran. Then for this second choice, if the grocer is consistent in her method of combining the opinions of her consultants, we would expect her to pick radishes over the turnips. In addition, we would expect her to be more conﬁdent in this second decision as a reﬂection of the consultants relative conﬁdence. The above properties of preference and conﬁdence imply that preference should be applied consistently across different problems and conﬁdence should reﬂect the relative conﬁdences of the individual consultants. 4.2 The General Principle of Consistency in Preference and Conﬁdence To generalize the above grocer’s problem consider the problem of combining the opinions of a panel of consultants on several multiple-choice decision problems. Suppose for each decision problem each consultant provides his preference as one of the possible choices and a qualitative measure of the conﬁdence level for his preference. The consistency principle in preference and conﬁdence holds if the combiner always agrees with at least one of the consultants in each decision problem and that the following condition holds for any pair of decision problems. Deﬁnition 1. Consistency principle for a pair of decision problems: Suppose that for the ﬁrst decision problem, the combiner adopts the preference of a subset A of consultants in the sense that A is the (non empty) set of all consultants whose preference is identical to that of the combiner. 795 M ELNIK , VARDI AND Z HANG Suppose that for the second decision problem, the preference of the consultants in A is again identical. Suppose further that compared with the conﬁdence level for the ﬁrst decision problem, each consultant in A has higher or the same conﬁdence level for his preference in the second problem, while each consultant with a different preference than A for the second problem has lower or the same conﬁdence level. Then, the preference of the combiner for the second problem is identical to that of the consultants in A, and the conﬁdence level of the combiner for the second problem is at least as high as his conﬁdence level for the ﬁrst problem. Let B be the set of consultants whose preferences are different from that of the combiner in the ﬁrst decision problem, that is, B = Ac . If some members of B switch sides or have no preference in the second problem, the combiner would be even more conﬁdent about the adoption of the preference of A in the second problem regardless of the conﬁdence of those who switch sides. Thus, Deﬁnition 1 requires only those against the preference of A in the second problem (necessarily a subset of B since members of A act in unison in both problems) to have lower or equal conﬁdence in the second problem, instead of all members of B. This is taken into consideration in Theorem 1 below. Note that while preference for individual consultants is speciﬁed within each decision problem, two decision problems are needed to compare the qualitative measure of conﬁdence. However, comparison of conﬁdence is not always possible (it is a partial ordering). In particular, the level of conﬁdence between different experts may not be comparable, and the levels of conﬁdence of a given expert (or the combiner) for different decision problems are not always comparable. 4.3 Conﬁdence for Ranks and Ranking Functions In order to apply consistency to rank combination we need to specify what we mean by more or less conﬁdence. For ranks we make the assumption that a constant change of ranks requires more conﬁdence for low ranks than high ranks. For example, we would expect the difference between ranks of 1 and 3 to represent a more signiﬁcant distinction on the part of a ranker than would for example the difference between ranks 34 and 36 which may be almost arbitrarily assigned. Speciﬁcally we make the following deﬁnition: Deﬁnition 2. Preference and conﬁdence for univariate rank values For any pair of rank values {r, r } ⊂ R with r < r , by monotonicity r is preferred. For any two pairs of rank values {r, r } and {r , r } with r < r and r ≤ r , the conﬁdence level for {r, r }is higher than that of {r , r } if r ≤ r , r −r ≤ r −r with at least one inequality. The pair{r, r }provides no preference if r = r . Likewise, if either r − r = r − r = 0 or r − r = r − r = 0, the two pairs {r, r } and {r , r }are deﬁned to have the same level of conﬁdence. In this deﬁnition conﬁdence between pairs of ranks can only be compared if the pair with the lowest rank has a gap at least as large as the other pair. Thus, we are actually comparing two numbers, that is, we are doing a vector comparison. As such, this comparison does not cover all pairs of ranks and applies only a partial ordering on rank pairs. As a regularization principle, a partial ordering is desirable since it does not overly constrain the ranking function and allows for ﬂexibility. 796 C ONCAVE L EARNERS FOR R ANKBOOST As the rank function, G, is a univariate function, we apply the same deﬁnitions of preference and conﬁdence to it. That is, for a ranking function G and for any ﬁxed rank vectors, y and y , the preferred rank vector is the one with smaller G, that is, y is preferred iff G(y) < G(y ). For conﬁdence again we need to consider pairs of rank vectors, {y, y } and {y , y }, with G(y) ≤ G(y ) and G(y ) ≤ G(y ). If G(y) ≤ G(y ) and G(y ) − G(y) ≥ G(y ) − G(y ) then we say that the ﬁrst decision, between yand y , shows equal or more conﬁdence than the second decision between y and y . This numerical method of capturing conﬁdence and preference in ranks, y, and ranking functions, G, allows us to apply Deﬁnition 1. Speciﬁcally, for a pair of binary decisions the conﬁdence of a consistent ranking function increases for the second decision if in the second decision the conﬁdence of the agreeing rank values are comparable and increase and the conﬁdence of the disagreeing rank values are comparable and decrease, with the exception of those that switched sides. 4.4 Three-Point Consistency in Preference and Conﬁdence for Combining Ranks As described, the consistency property is over two binary decision problems. In this section we consider ranking functions, G, that have consistency in preference and conﬁdence for all pairs of binary decision problems involving three rank vectors y, y and y . We show that such functions have speciﬁc mathematical properties under this three-point consistency principle in preference and conﬁdence for ranks.. Theorem 1: For a ranking function G whose domain is convex, three-point consistency in preference and conﬁdence holds for G iff G(y) is nondecreasing in individual components of y and is jointly concave in y. Proof of Necessity: Assume that the ranking function G exhibits three-point consistency in preference and conﬁdence for any three rank vectors. If y ≤ y component wise, then the individual components of y all agree in their preference to y, so that G(y) ≤ G(y ) by the consistency of G in preference. This implies the monotonicity of G in y. We pick three points y, y − a and y + a, such that all points are rank vectors. Consider the pair of binary comparison problems, y vs. y + a and y − a vs. y. Assume that G(y) < G(y + a). These three points are comparable by Deﬁnition 2 (r − r = r − r for all components in the rank vectors). Since the agreeing rankers, A = j y j < y j + a j , in the y vs. y + a comparison are greater than in the y − a vs. y comparison, and the disagreeing ranks, outside A, are smaller, then by the consistency of the combiner in preference and conﬁdence (deﬁnition 1), G(y − a) ≤ G(y) and G(y) − G(y − a) ≥ G(y + a) − G(y). A function G is concave in a convex domain iff 2G(y) ≥ G(y − a) + G(y + a), for every y and a with y ± a in its domain. To verify this inequality for a particular y and a, we must have G(y + a) > G(y), G(y − a) > G(y) or the third case of G(y) ≥ max{G(y + a), G(y − a)}. We have already proven that the consistency properties of preference and conﬁdence imply G(y) − G(y − a) ≥ G(y + a) − G(y) in the ﬁrst case. By symmetry this requirement also must hold for −a, so that G(y) − G(y + a) ≥ G(y − a) − G(y) in the second case. This completes the necessity part of the proof since 2G(y) ≥ G(y − a) + G(y + a) automatically holds in the third case. Proof of Sufﬁciency: Assume the ranking function G is nondecreasing in individual components of y and jointly concave in y. We need to prove consistency in preference and conﬁdence for a pair 797 M ELNIK , VARDI AND Z HANG 1) G(y) < G(y’) y ≤ y’’ A A y’’ A 4 A A 10 y’ 8 y’ 6 4) G(y) > G(y’) y ≥ y’’ A 10 8 A 2) G(y) < G(y’) y ≥ y’’ A 10 8 6 y A y 6 4 4 2 2 2 0 0 y y’ y’’ y’’ 0 2 4 6 8 10 0 2 4 B 6 8 10 0 0 B 2 4 6 8 10 B Figure 1: Consider two decision problems in two dimensions involving three points, y, y and y , where the ﬁrst decision problem is to choose between y and y and the second problem is to choose between y and y . The cases where we expect consistency in preference and conﬁdence (Deﬁnitions 1 and 2) can be enumerated by the result of the ﬁrst decision problem and relative location of y . The darker gray box is the location where B has lesser or equal conﬁdence in the second problem, and the lighter gray box is where B switches sides. of decision problems involving three rank vectors y, y and y . Without loss of generality, suppose that the ﬁrst problem is to choose between y and y and the second problem is to choose between y and y . We break the proof up by the results of the comparison between y vs. y and the relative location of the third ranking vector y that satisﬁes the preference and conﬁdence requirements. If G(y) = G(y ) = G(y ), the combiner has equal conﬁdence in the two decision problems by deﬁnition 2, so that the consistency principle holds automatically. Thus, we only need to consider the cases where G(y) = G(y ). Figure 1 illustrates three of these cases two dimensionally, where there is a single agreeing component A, the y-axis, and a single disagreeing component B, the x-axis. Let A be the set of agreeing indices, A = j : sgn(y j − y j ) = sgn (G(y ) − G(y)) = 0 , and B = c the set of disagreeing indices. We use the notation y = (y , j ∈ A) to describe corresponding A j A subvectors. Also, when used, vector inequalities are component wise. Case 1: G(y) < G(y ) and yA ≤ yA In the ﬁrst decision problem, as G agrees with A and disagrees with B yA < y A , yB ≥ y B . / We also know that A = 0 since otherwise y ≥ y and therefore by monotonicity G(y) ≥ G(y ), which is a contradiction. For the second decision problem we consider the values of y which are consistent with the conﬁdence assumption (Deﬁnitions 1 and 2), that is, agreeing with more conﬁdence along the A indices and disagreeing with less conﬁdence or switching sides along the B indices. As y A ≤ yA , either by the conﬁdence relationship or by switching sides (see Figure 1) these y values satisfy yA − y A ≥ y A − y A , 798 yB − y B ≤ y B − y B C ONCAVE L EARNERS FOR R ANKBOOST which implies y ≤ y , and therefore G(y ) ≤ G(y ) by monotonicity. Thus, G(y) < G(y ) ≤ G(y ), which implies that in the second decision problem of choosing between y and y , the preference of G is the same as that of A (i.e., y since yA ≤ yA ) and the conﬁdence of G is at least as high as the ﬁrst decision problem. Case 2: G(y) < G(y ) and yA ≥ yA Since in this case yA ≥ yA , then either by the conﬁdence relationship or by switching sides y satisﬁes yA − y A ≥ y A − y A , yB − y B ≤ y B − y B . This implies that y + y ≤ 2y, which means that G(y ) + G(y ) ≤ 2G y +y /2 ≤ 2G(y) by the concavity and monotonicity of G. Thus, G(y) − G(y ) ≥ G(y ) − G(y) and since G(y ) > G(y), we have that G(y) − G(y ) > 0 and thus G(y) > G(y ). Therefore we see that the preference in G for the two decision problems is the same as A (with y in the ﬁrst problem and with y in the second problem) and the conﬁdence is no smaller for the second comparison. Case 3: G(y) > G(y ) and yA ≤ yA Since y is preferred in the ﬁrst decision problem we have yA > y A , yB ≤ y B . For the conﬁdence assumption to hold, that is, greater conﬁdence for A in the second problem (Deﬁnition 2), the smaller ranks in the second problem have to be smaller than the smaller ranks in the ﬁrst problem. But with yA ≥ yA and yA ≤ yA that can only be if yA = yA , which leads to y ≤ y , and by monotonicity implies G(y) ≤ G(y ). However that is a contradiction to G(y) > G(y ), and therefore this is not a viable case for comparing conﬁdence. Case 4: G(y) > G(y ) and yA ≥ yA Since in this case yA ≥ yA , then either by the conﬁdence relationship or by switching sides y satisﬁes yA − y A ≥ y A − y A , yB − y B ≤ y B − y B , which means that y ≤ y . Thus, by the monotonicity of G, G(y ) ≤ G(y ). Since G(y ) < G(y) the preference in the second decision problem is also with A (i.e., y ) and the conﬁdence is at least as high as the ﬁrst decision problem. 4.5 Applying Regularization to Rankboost It follows from the above theorem that to have consistency in preference and conﬁdence we desire ranking functions that are monotonic and concave. In rankboost H(x) = ∑ wt ht (x) (Eq. 1). To make H an increasing and concave function of constituent rankings y j = f j (x) we need to constrain the weak ranking learners. If the learners themselves are monotonically increasing and concave functions of y j , then linearly combining them with positive weights will give an H that is also an increasing and concave function of y j . 799 M ELNIK , VARDI AND Z HANG In this paper, we apply the “third method” (Freund et al., 2003) to setting a wt weight value. That is, weak learners are selected on their ability to maximize r from Eq. 4 and then wt = 0.5 ln ((1 + rmax ) / (1 − rmax )) . (5) Therefore, using monotonic and concave weak learners we select only ones that rankboost gives a positive r value to, which renders a positive wt weight. If no r values are positive the rankboost algorithm stops. We mention that a ranking function can be construed as the negative of a score function, that is, G(y) = −S(y). For score functions these regularization properties become monotonically decreasing, and convex (Melnik et al., 2004). 5. Minimum Weighted Group Ranks The functional structure of the new learner we propose is h(y) = min {γ1 y1 , . . . , γn yn , 1} , (6) where y = (y1 , . . . , yn ) = ( f1 (x), . . . , fn (x)) the vector of ranking features, and the γ j are learned positive coefﬁcients. Note that the function’s range is in [0, 1] due to the 1 term in the min function. Using rankings as our features, the learner function (Eq. 6) is monotonically increasing. It is also a concave function in y. Thus if these learners are linearly combined with positive weights, the resulting ranking function, H, will have three-point consistency in conﬁdence and preference. To gain some intuition, the functional form of the learner is related to the Highest Rank combination method (Ho, 1992) that assigns combined ranks as the best rank each class receives from any ranker. This is a conﬁdence based approach, as Highest Rank bets on the classiﬁer that is most conﬁdent, the one giving the best rank. As such, a single learner can potentially be error prone. But as we combine many of these learners during boosting, it becomes more powerful, allowing the conﬁdence of different classiﬁers to be weighed with preference for their potential accuracy. 5.1 Learning At each boosting iteration, rather than selecting from all possible weak learners of form (Eq. 6), we limit our choices by building new weak learners out of the ones that have been previously trained. Let F = { f1 (x), . . . , fn (x)} be the set of ranking features. Recall that in rankboost H(x) = (t) (t) (t) ∑t wt ht (x), where ht (x) = min γ1 f1 (x), . . . , γn fn (x), 1 and γ j are learned coefﬁcients. We set (s) (s) Ht = h(x) h(x) = min γ1 f1 (x), . . . , γn fn (x) , s ≤ t and select ht+1 (x) from weak learners of the form hnew (x) = min αh(x), β f (x), 1 (7) (t+1) with h(x) ∈ Ht and f (x) ∈ F . This learner can be rewritten in the form of Eq. 6 with the γ j derived from the learned α, β, h(x) and f (x). Thus, at each iteration we look at combinations of 800 C ONCAVE L EARNERS FOR R ANKBOOST the features and existing learners. As discussed in the next section, we can either consider all such combinations, or use a heuristic selection strategy. We propose to optimize α and β separately, in stages. That is, given a value for one of the variables we optimize the other. As α and β are symmetric we show how to optimize β given α. We need to ﬁnd a value of β that maximizes r in Eq. 4. Freund et al. (2003) pointed out that this equation can be rewritten as: r = ∑ D(t) (x , x )(h(x ) − h(x )) x ,x = ∑ π(x)h(x) x where π(x) = ∑x D(t) (x , x) − D(t) (x, x ) . Given the form of Eq. 7 we can write r as a function of β, ∑ (β f (x) − 1) π(x) + ∑ r (β) = αh(x) − 1 π(x). β f j (x)≤min(αh(x),1) (8) αh(x) < 1 , then O(βl+1 ) = O(βl ) − π(xl ), P(βl+1 ) = P(βl ) − f (xl )π(xl ), Q(βl+1 ) = Q(βl ) +W π(xl ), R(βl+1 ) = R(βl ) +W αh(xl )π(xl ). Combining these formulas gives algorithm 2 for optimizing β. Algorithm 2 Algorithm for optimizing β Given α, h(x) ∈ Ht , f (x) ∈ F and the training instances. For all x’s generate and sort the set of candidate βs, B, such that β 1 ≤ β2 ≤ · · · ≤ β|B| and βl = min(αh(xl ),1) . f (xl ) O = ∑xl π(xl ) P = ∑xl f (xl )π(xl ) Q=0 R=0 rbest = 0 for j = 1 . . . |B| do r = βl P − O + R − Q if r > rbest then rbest = r end if O = O − π(xl ), P = P − f (xl )π(xl ) if αh(xl ) < 1 then Q = Q + π(xl ), R = R + αh(xl )π(xl ) end if end for 5.2 Heuristics for Learner Selection If at each boosting iteration we select from all combinations of h(x) and f (x) we end up with an O(T 2 ) algorithm, where T is the number of iterations. However, we can sacriﬁce accuracy for speed by only evaluating and selecting from a ﬁxed sized pool of previously trained learners h(x) and features f (x), where at each iteration the pool is randomly chosen from the full Ht and F . To improve performance, instead of using a uniform sampling distribution we can apply a heuristic to focus the distribution on combinations with better potential. As Eq. 8 is composed of two sums, for r to be large the terms f (x)π(x) and h(x)π(x) need to be large. We can consider s f = ∑x f (x)π(x) and sh = ∑x h(x)π(x) as indicators of how well these components work with π(x). Thus, we might expect larger r values to occur when these two score values are larger. Of course, we are discounting interactions, which is the reason for the combination. Using these score values, we can order all h(x) and f (x) separately, and sample such that learners and features with better scores are more likely to be selected. We opted for a polynomial weighting 802 C ONCAVE L EARNERS FOR R ANKBOOST Training error on Dup II 9 P=1 P=1/2 P=1/4 Avg rank of correct class 8.8 8.6 8.4 8.2 8 7.8 7.6 7.4 7.2 1 2 3 4 5 6 7 8 9 Iteration number Figure 2: These plots show convergence of training error for 3 values of the heuristic selection pressure value p. These plots are averages over 10 runs and are typical of the other training data sets as well. As can be seen the heuristic improves the convergence rate of the training error. method. Thus, for example, all f ∈ F are sorted by their score and are assigned a number based on the rank of these scores,(maxrank − rank)/maxrank, that gives each f an equally sized bin in the range 0 and 1. Given a random number ξ ∼ U(0, 1), we calculate ξ p and select the f that corresponds to the bin this number falls into. Here p < 1 can be construed as a selection pressure, where bins corresponding to higher scores are more likely to be selected. Figure 2 demonstrates the effect of different values of the p parameter in one of our experiments. 6. Face Recognition Experiments We present experiments on the combination of face recognizers, comparing the binary learner with the MWGR learner. Given an image of a face, a face recognizer assigns a similarity score to each of the faces it is trained to recognize. These scores give a linear order or ranking to the gallery of faces. Different face recognition algorithms have different performance characteristics. Thus, we explore how combining the outputs of multiple face recognizers can improve recognition accuracy. 6.1 Algorithm Methods We consider a data set I of face images (queries) to train on. For each query image, i in I, we need to rank all u ∈ U faces in the gallery. In rankboost the favor function, Φ, that speciﬁes output loss, is a function of the query and the item to be ranked. Therefore, the notational convention is to combine the query, i, and the item to be ranked, u, as an instance, x ≡ (i, u). As such, f j (x) = f j ((i, u)) is the 803 M ELNIK , VARDI AND Z HANG Error convergence 9 Train error on dup II Test error on dup I Avg rank of correct class 8.5 8 7.5 7 6.5 6 0 10 20 30 40 50 60 70 80 90 100 Iteration number Figure 3: This plot is typical of the convergence behavior of rankboost with MWGR on the FERET data. Both training and test errors tended to converge within 10-30 iterations of boosting with no signiﬁcant post-convergence divergence. rank assigned to identity u for query image i by recognition algorithm j. As there is only one correct identity, we only care about the ranking of the one correct identity for each query. We set the favor function as stated by Freund et al. (2003) for this type of output loss. Let u ∗ be the the correct identity for training image i, then Φ ((i, u) , (i, u∗ )) = +1 and Φ ((i, u∗ ) , (i, u)) = −1 for all u = u∗ , setting all remaining elements of Φ (x , x ) = 0. That is, the correct identity of a query image is given positive favor compared to all other identities for that image, while all other rankings, including interactions between training images, are given zero favor. Note that since there is no favor interaction between queries (different i’s); Φ (x , x ) is effectively a function of 3 variables, (i, u , u ). Both weak learners were trained for 100 iterations, giving them ample time to converge. See Figure 3 for an illustration of convergence times. The binary learner was trained as speciﬁed by Freund et al. (2003). At each iteration the MWGR learner was selected from a pool of candidate combinations of f (x) and h(x), with a selection pressure of p = 0.5. For each candidate, ﬁrst β was optimized with α = 1, then α was optimized using the optimized β. The candidate with the most positive r value was always selected. This is summarized in algorithm 3. 6.2 Experimental Setup FERET (Phillips et al., 2000) was a government sponsored program for the evaluation of face recognition algorithms. In this program commercial and academic algorithms were evaluated on their ability to differentiate between 1,196 individuals. The test consisted of different data sets of varying difﬁculty, for a total of 3,816 different images. The data sets, in order of perceived difﬁculty, are: the fafb data set of 1,195 images which consists of pictures taken the same day with different facial 804 C ONCAVE L EARNERS FOR R ANKBOOST Algorithm 3 Algorithm for selecting learner from pool. Given poolsize and selection pressure. Calculate s f j and sh for all rank features and existing learners. for p = 1 . . . poolsize do Generate d f ∼ U(0, 1) and dh ∼ U(0, 1) Select which h = min αh(x), β f (x), 1 to try, using s f j , sh , u f , uh . Setting α = 1, optimize β (algorithm 2) Keeping the optimized β, optimize α (algorithm 2) Get r of learner with this α, β if r > 0 and r > rbest then rbest = r, hbest = h, hbest = min αh(x), β f (x) end if end for ht = hbest, Ht+1 = Ht ∪ hbest expressions; the fafc data set of 194 images that contains pictures taken with different cameras and lighting conditions; the dup I data set of 488 images that has duplicate pictures taken within a year of the initial photo; and the most difﬁcult, the dup II data set of 234 images which contains duplicate pictures taken more than a year later. Note that in our experiments we separate the images of dup II from the dup I data set, unlike the FERET study where dup II was also a subset of dup I. The FERET study evaluated 10 baseline and proprietary face recognition algorithms. The baseline algorithms consisted of a correlation-based method and a number of eigenfaces (principle components) methods that differ in the internal metric they use. Of the proprietary algorithms, most were from different academic institutions and one was commercial. Of the 10 algorithms we selected three dominant algorithms. From the baseline algorithms we chose to use the ANM algorithm which uses a Mahalanobis distance variation on angular distances for eigenfaces (Moon and Phillips, 2001). While this algorithm’s performance is not distinctive, within the class of baseline algorithms it was strong. Moreover, in accuracy with respect to average rank of the correct class on the dup I data set it demonstrated superior performance to all other algorithms. The other two algorithms we used were the University of Maryland’s 1997 test submission (UMD) and the University of Southern California’s 1997 test submission (USC). These algorithms clearly outperformed the other algorithms. UMD is based on a discriminant analysis of eigenfaces (Zhao et al., 1998), and USC is an elastic bunch graph matching approach (Wiskott et al., 1997). The outputs of the 10 face recognizers on the four FERET data sets, fafb, fafc, dup I and dup II were the data for the experiments. Thus, we never had access to the actual classiﬁers, only to data on how they ranked the different faces in these data sets. We conducted experiments based on homogeneous and heterogeneous data sets, testing the efﬁciency and robustness (adaptivity) of the MWGR procedure. For the homogeneous case we took all 4 FERET data sets and randomly shufﬂed them together. We call this the homogeneous data set as both the training and testing data are selected from the same combined pool. On this combined data set we did 4-fold cross validation. For each fold 75% of the data was used for training and the rest for testing. We combined the results of all four runs together for evaluation purposes. 805 M ELNIK , VARDI AND Z HANG For the heterogeneous case, in each experiment one of the FERET data sets was selected as a training set and another data set was selected for testing. This gave 12 experiments (not including training and testing on the same data set) per group of face recognizers, where we get combinations of training on easy data sets and testing on hard data sets, training on hard and testing on easy data sets, and training and testing on hard data sets. To reduce noise in our experiments the training ranks were truncated at 150, and outliers were removed. In face recognition and other classiﬁcation applications usually only the top ranks are important. Thus, in evaluating the results we focused on the top 30 ranks. All ranks returned by a boosted combiner for the correct class above 30 were truncated to 30. In evaluating the performance of the combiners not all the test data are equally useful. We consider the following two cases as non informative. When the two best face recognizers, UMD and USC both give the correct class a rank of 1 there is very little reason for the combined rank to be different. Also when both the binary-learner-based combiner and the MWGR-based combiner give the correct class a rank greater than the truncation value (30) it makes little sense to compare between the combiners. The testing data was ﬁltered to remove these cases, and the results are presented without them. Before presenting the results, it should be said that rankboost with both learner types gives ranking functions that signiﬁcantly outperform all the individual face recognition algorithms. In addition, in our tests both learners also clearly outperformed other standard rank combination methods, such as the Borda count, highest rank and logistic regression (Ho et al., 1992). We present two sets of experiments—the combination of the 3 selected classiﬁers (ANM, UMD and USC) and the combination of all 10 classiﬁers. These are qualitatively different tasks. In combining 3, we seek to capitalize on the unique strengths of each classiﬁer. In combining 10, we are introducing classiﬁers which may be correlated and classiﬁers which are comparably much noisier. The size of the pool was 6 when we combined 3 classiﬁers and 20 when combining all 10 classiﬁers. For all experiments we measure the average rank of the correct class for both learners: A= 1 N ∑ min {Rank (xi∗ ) , 30} , N i=1 where Rank (xi∗ ) is the rank of the correct class for query i, and the sum is over all useful test queries, as described above. The average rank difference between the learners is calculated to show the improvement in performance. To evaluate the signiﬁcance of the improvement we ran paired one-sided t-tests and evaluated the signiﬁcance of the p-value (a value less than 0.05 is signiﬁcant). In addition we show the standard deviation of the rank difference. 6.3 Results In the experiments with the homogeneous data sets, combining all classiﬁers gives an improvement in the average rank of the correct class of 0.296 for MWGR, with a standard deviation of 3.9, a paired t-test statistic of 1.76 and p-value of 0.039, where combining the ANM, UMD and USC classiﬁers gives an average rank improvement of 0.1 for MWGR, with standard deviation of 3.6, a paired t-test statistic of 0.68 and p-value of 0.24. Table 2 contains the results for combining the 3 classiﬁers in the experiments with heterogeneous data sets (compare the combiner results in columns bin mean and mwgr mean with the aver806 C ONCAVE L EARNERS FOR R ANKBOOST ANM ARL EFAVG EFML1 EFML2 EXCA MSU RUT UMD USC dup i 9.52 16.85 15.02 18.23 15.85 11.9 17.64 17.87 13.21 12.44 dup ii 18.14 16.96 20.61 22.21 20.33 16.28 23.44 16.48 13.74 6.85 fafb 10.88 8.94 14.43 16.23 12.21 11.67 6.81 12.93 4.57 5.84 fafc 19.71 28.02 26.17 17.39 19.78 20.30 14.27 22.79 8.96 5.17 Table 1: The best average rank of the correct class on the different data sets for all constituent face recognition systems. age rank of the correct class for all constituent classiﬁers in Table 1). The diff mean column contains the improvement of MWGR over the binary learner in terms of average rank of the correct class. Of the 12 experiments, we see an improvement in 10 cases. Six of those 10 have signiﬁcant p-values. The two no improvement experiments do not have signiﬁcant p-values. Table 3 contains the results for combining all 10 classiﬁers. Of the 12 experiments, we see an improvement for MWGR in 11 cases. Eight or nine of those 11 have signiﬁcant p-values. The one no improvement experiment does not have a signiﬁcant p-value. It is interesting to note that we do not seem to see overﬁtting when increasing the number of constituents. In some case we see improvement, in others we see slight degradation, but all in all the combiner seems resilient to the noise of adding less informative constituents. All sets of experiments, homogeneous data, heterogeneous data sets, combining 3 select recognizers and combining all 10 recognizers at once yielded signiﬁcant improvements in accuracy, as is visible in the change in the average rank of the correct class and the signiﬁcance of the statistical tests. 7. Information Retrieval Experiments The annual Text REtrieval Conference (TREC) generates high-quality retrieval results of different systems on different retrieval tasks (Voorhees and Harman, 2001). We use the result data sets of the TREC-2001 web ad hoc task that uses actual web queries taken from web logs. This task has been used in other rank fusion experiments (Renda and Straccia, 2003). As did Renda and Straccia (2003) we combine the results of the following top 12 systems: iit01m, ok10wtnd1, csiro0mwal, ﬂabxtd, UniNEn7d, fub01be2, hum01tdlx, JuruFull, kuadhoc, ricMM, jscbtawtl4, apl10wd. Similar to other TREC information retrieval tasks, the TREC-2001 ad hoc task consists of 50 queries. For each query a list of relevant documents is supplied. Each system returns an ordered list of 1,000 documents for each query. The fusion goal is to combine these 12 individual lists into one list of 1,000 documents, which hopefully has greater precision than the individual systems. For the 807 M ELNIK , VARDI AND Z HANG test set dup i dup i dup i dup ii dup ii dup ii fafb fafb fafb fafc fafc fafc train set dup ii fafb fafc dup i fafb fafc dup i dup ii fafc dup i dup ii fafb bin mean 7.19 7.36 8.31 5.25 5.15 5.19 2.62 3.38 2.60 2.85 2.40 2.56 mwgr mean 5.96 6.21 6.27 5.38 4.4 4.95 2.22 2.60 2.28 2.78 2.43 2.03 diff mean 1.22 1.14 2.04 -.12 0.74 0.23 0.39 0.78 0.32 0.06 -.03 0.52 diff std 5.22 5.15 5.51 4.0 4.61 5.1 3.09 3.79 2.81 3.33 2.27 2.57 pval .7e-4 .001 .1e-6 .658 .018 .275 .115 .029 .142 .424 .537 .028 Table 2: Results of combining the ANM, UMD, and USC classiﬁers using individual FERET data sets. test set dup i dup i dup i dup ii dup ii dup ii fafb fafb fafb fafc fafc fafc train set dup ii fafb fafc dup i fafb fafc dup i dup ii fafc dup i dup ii fafb bin mean 6.73 8.06 6.68 5.75 6.24 5.86 2.67 3.56 2.68 3.36 3.17 2.22 mwgr mean 5.89 7.09 4.87 5.67 5.47 5.31 2.07 2.45 2.17 2.51 2.95 2.23 diff mean 0.84 0.96 1.81 0.08 0.76 0.55 0.59 1.11 0.51 0.85 0.22 -0.01 diff std 4.79 6.01 5.24 4.61 4.22 4.87 2.97 4.51 2.03 3.23 3.95 2.08 pval .009 .014 .5e-6 .408 .009 .074 .033 .012 .01 .007 .3 .52 Table 3: Results of combining all 10 classiﬁers using individual FERET data sets. 50 queries of the TREC-2001 ad hoc task, the number of relevant documents that intersect with the union of system results range between 662 and 2664. 7.1 Methods In the information retrieval task the favor function needs to show favor for relevant documents while disregarding other documents. Thus, we can set the favor function similarly to the way it was set in 808 C ONCAVE L EARNERS FOR R ANKBOOST JuruFull 0.759 hum01tdlx 0.760 UniNEn7d 0.763 iit01m 0.762 apl10wd 0.735 jscbtawt14 0.761 csiro0mwa1 0.721 kuadhoc2001 0.727 ﬂabxtd 0.741 ok10wtnd1 0.745 fub01be2 0.734 ricMM 0.765 Table 4: Normalized mean average precision for each constituent. the face recognition classiﬁcation task. Consider a data set with I queries to train on. For each query, i in I, we need to rank all u ∈ U documents. An instance in this case is a pair, x ≡ (i, u). For each u∗ a relevant document and u an irrelevant document for query i, we set Φ ((i, u) , (i, u ∗ )) = +1 and Φ ((i, u∗ ) , (i, u)) = −1, setting all remaining elements of Φ (x0 , x1 ) = 0. Thus, all relevant documents are given a positive favor with respect to irrelevant documents, while all other rankings, including interactions between relevant documents and interactions between irrelevant documents are given zero favor. As the task has only 50 queries, rather than separating the data into train and test, we opted to do a cross validation performance evaluation. We use 5-fold cross validation on the normalized mean average precision measure (Salton and McGill, 1983), a standard IR measure which accounts for the precision (quantity of correct documents retrieved) and their rank position in the list. It is described by the following equation: AveP = ∑N (Prec(r) × rel(r)) r=1 number o f relevant documents where r is the rank, N is the number of documents retrieved, rel() is a binary function of the relevance of the document at rank r,and Prec is the precision ( [number of relevant documents retrieved] / [number of documents retrieved]) at a given cut-off rank. It is clear that higher values of AveP are better. Note that for purposes of normalization we assigned unretrieved relevant documents a constant rank. As the binary and MWGR weak learners had signiﬁcantly different convergence properties on this task (see Figure 4) MWGR was trained for 100 iterations and the binary learner for 300 iterations. As in the FERET experiments, MWGR was optimized using algorithm 3, with a selection pressure of p = 0.5. 7.2 Results As seen in Figure 4, the MWGR learner converges in signiﬁcantly less iterations than the binary learner. This could possibly be attributed to the fact that the MWGR is a more complex function that can incorporate the rankings of multiple classiﬁers in each learner. Also the MWGR function is not tuned to particular rank cutoffs, whereas the binary learner is, so the MWGR can better accommodate the variety in the 1000 ranks being considered. The normalized mean average precision for the MWGR after 100 iterations was 0.8537 and it was 0.8508 for the binary learner after 300 iterations. Compare these results with the precision of the constituents in Table 4. Both weak learners had a performance rate of change of approximately 3 ∗ 10−5 on their ﬁnal iteration (better for MWGR). A paired t-test on the cross validation results of the two learners gives a statistically signiﬁcant p-value of 0.007 in favor of MWGR. 809 M ELNIK , VARDI AND Z HANG Cross Validation Performance 0.88 MWGR Binary 0.87 0.86 0.85 0.84 Mean Avg Precision 0.83 0.82 0.81 0.8 0.79 0.78 0.77 0.76 0.75 0.74 0.73 0.72 0.71 0.7 0 50 100 150 200 Iteration Number 250 300 Figure 4: The cross validation mean average precision score of the two weak learners, MWGR and binary, as a function of boosting iteration number. 8. Discussion The question of how to combine ordinal data has become an active focus of research in machine learning, as applications in pattern recognition, information retrieval and other domains have come to the forefront. A particular question of importance is how can the structure of ranks be correctly exploited to maximize performance. The semi parametric nature of rankboost offers the possibility to generate arbitrarily ﬂexible ranking functions. But as observed this ﬂexibility comes at a cost of signiﬁcant overﬁtting without further regularization. Freund et al. (2003) demonstrate that successful generalization only occurs when the resulting ranking functions are constrained to be monotonic. This constraint can be thought of as a regularization that incorporates prior knowledge on the interpretation of ranks and as such, how they can be combined. We present a regularization framework based on the concept of consistency in conﬁdence and preference. Ranking functions with this property show a consistency in how they treat the preference and relative conﬁdence exhibited by constituents. We prove that under a natural interpretation of preference and conﬁdence for ranks, this consistency property of the combiner is equivalent to monotonicity and concavity of its ranking function. We enhance rankboost by designing a weak ranking learner that exhibits consistency in preference and conﬁdence. A computational advantage of this weak learner, called minimum weighted group ranks (MWGR) is that its parameters can be individually optimized readily with respect to the rankboost criteria, allowing it to be tested on real-world data. In our ﬁrst experiments we compare the original rankboost binary weak learner with MWGR on a task of combining the output of multiple face recognition algorithms from the FERET study. We conducted experiments on homogeneous data, testing the intrinsic efﬁciency of the MWGR proce810 C ONCAVE L EARNERS FOR R ANKBOOST dure. We also conducted experiments on heterogeneous data, testing the robustness or adaptivity of the procedure. In almost all cases we see that MWGR shows improved performance compared with the binary weak learner, whether combining three or all of the face recognizers, conﬁrming the utility of this monotonic and concave learner. Our second experiment was on an Information Retrieval task taken from the TREC conference. In this task we see MWGR converges in signiﬁcantly less iterations and generates statistically signiﬁcant improved performance. Final Words Ofer Melnik and Cun-Hui Zhang are very saddened that our colleague and friend Yehuda Vardi passed away before he could give this paper his ﬁnal stamp of approval. He was very enthusiastic about this research and would have been very pleased to see it come to fruition. Acknowledgments This research is partially supported by NSF Grants DMS 04-05202, DMS 05-04387 and, DMS 0604571, ONR Grant N00014-02-1-056 and NSA Grant H98230-04-1-0041. Ofer Melnik would also like to thank DIMACS for their support in this research. The authors are also grateful to the editor and reviewers for their constructive suggestions which improved the presentation of the results. Appendix A. In this paper we showed how three-point consistency in preference and conﬁdence implies concave and monotonic ranking functions. For two decision problems involving two pairs of rank vectors, the four-point consistency property implies the following constraints for a ranking function   G(y) < G(y )   yB − y B ≤ 0 < y A − y A  zA ≤ yA , yA − yA ≤ zA − zA ⇒ G(z ) − G(z) > G(y ) − G(y)   yB ≤ z B , zB − z B ≤ y B − y B (11) where y and y are the rank vectors from the ﬁrst decision problem and z and z are the rank vectors from the second decision problem. Unlike the three-point case, the four-point consistency property does not imply a clearly recognizable functional form for the ranking function. What we can say about it though is that as the constraints are linear, in the same way that concavity and monotonicity in the weak learner conferred the same properties to the ranking function, a weak learner that satisﬁes Eq. 11 will also confer those properties to the ranking function that uses it with positive weights. References K. Arrow. Social Choice and Individual Values. Wiley, 1951. C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. Rank aggregation methods for the web. In Proc. 10th Intl. World Wide Web Conf., pages 613–622, 2001. 811 M ELNIK , VARDI AND Z HANG Y. Freund, R. Iyer, R.E. Schapire, and Y. Singer. An efﬁcient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933–969, 2003. Y. Freund and R.E. Schapire. Experiments with a new boosting algorithm. In Proceedings of the Thirteenth International Conference on Machine Learning, pages 148–156, 1996. T. K. Ho. A Theory of Multiple Classiﬁer Systems and Its Application to Visual Word Recognition. PhD thesis, State University of New York at Buffalo, May 1992. T. K. Ho, J. J. Hull, and S. N. Srihari. Combination of decisions by multiple classiﬁers. In H. S. Baird, H. Bunke, and K. Yamamoto (Eds.), editors, Structured Document Image Analysis, pages 188–202. Springer-Verlag, Heidelberg, 1992. J. Kittler and F. Roli, editors. Multiple Classiﬁer Systems, Lecture Notes in Computer Science 1857, 2000. Springer. R. Meir and G. Ratsch. Advanced Lectures in Machine Learning, Lecture Notes in Computer Science 2600, chapter An introduction to boosting and leveraging, pages 119–184. Springer, 2003. O. Melnik, Y. Vardi, and C-H. Zhang. Mixed group ranks: Preference and conﬁdence in classiﬁer combination. IEEE Pattern Analysis and Machine Intelligence, 26(8):973–981, 2004. H. Moon and P.J. Phillips. Computational and performance aspects of PCA-based face-recognition algorithms. Perception, 30:303–321, 2001. P.J. Phillips, H. Moon, S.A. Rizvi, and P.J. Rauss. The FERET evaluation methodology for facerecognition algorithms. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22:1090– 1104, 2000. M.E. Renda and U. Straccia. Web metasearch: Rank vs. score based rank aggregation methods. In 18th Annual ACM Symposium on Applied Computing (SAC-03), pages 841–846, Melbourne, Florida, USA, 2003. ACM. G. Salton and J.M. McGill. Introduction to Modern Information Retrieval. Addison Wesley Publ. Co., 1983. E.M. Voorhees and D.K. Harman, editors. NIST Special Publication 500-250: The Tenth Text REtrieval Conference (TREC 2001), number SN003-003-03750-8, 2001. Department of Commerce, National Institute of Standards and Technology, Government Printing Ofﬁce. URL http://trec.nist.gov. L. Wiskott, J.-M. Fellous, N. Kruger, and C. von der Malsburg. Face recognition by elastic bunch graph matching. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(7):775– 779, 1997. W. Zhao, A. Krishnaswamy, R. Chellappa, D. Swets, and J. Weng. Face Recognition: From Theory to Applications, chapter Discriminant Analysis of Principal Components, pages 73–86. SpringerVerlag, Berlin, 1998. 812</p><p>4 0.080679722 <a title="4-tfidf-4" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>Author: Art B. Owen</p><p>Abstract: In binary classiﬁcation problems it is common for the two classes to be imbalanced: one case is very rare compared to the other. In this paper we consider the inﬁnitely imbalanced case where one class has a ﬁnite sample size and the other class’s sample size grows without bound. For logistic regression, the inﬁnitely imbalanced case often has a useful solution. Under mild conditions, the intercept diverges as expected, but the rest of the coefﬁcient vector approaches a non trivial and useful limit. That limit can be expressed in terms of exponential tilting and is the minimum of a convex objective function. The limiting form of logistic regression suggests a computational shortcut for fraud detection problems. Keywords: classiﬁcation, drug discovery, fraud detection, rare events, unbalanced data</p><p>5 0.069768578 <a title="4-tfidf-5" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>Author: Jaime S. Cardoso, Joaquim F. Pinto da Costa</p><p>Abstract: Classiﬁcation of ordinal data is one of the most important tasks of relation learning. This paper introduces a new machine learning paradigm speciﬁcally intended for classiﬁcation problems where the classes have a natural order. The technique reduces the problem of classifying ordered classes to the standard two-class problem. The introduced method is then mapped into support vector machines and neural networks. Generalization bounds of the proposed ordinal classiﬁer are also provided. An experimental study with artiﬁcial and real data sets, including an application to gene expression analysis, veriﬁes the usefulness of the proposed approach. Keywords: classiﬁcation, ordinal data, support vector machines, neural networks</p><p>6 0.065221585 <a title="4-tfidf-6" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>7 0.054716375 <a title="4-tfidf-7" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>8 0.052117657 <a title="4-tfidf-8" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>9 0.043897118 <a title="4-tfidf-9" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>10 0.042216681 <a title="4-tfidf-10" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>11 0.040912431 <a title="4-tfidf-11" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>12 0.039631203 <a title="4-tfidf-12" href="./jmlr-2007-Multi-Task_Learning_for_Classification_with_Dirichlet_Process_Priors.html">56 jmlr-2007-Multi-Task Learning for Classification with Dirichlet Process Priors</a></p>
<p>13 0.037097793 <a title="4-tfidf-13" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>14 0.033759117 <a title="4-tfidf-14" href="./jmlr-2007-Distances_between_Data_Sets_Based_on_Summary_Statistics.html">27 jmlr-2007-Distances between Data Sets Based on Summary Statistics</a></p>
<p>15 0.032580525 <a title="4-tfidf-15" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>16 0.031341933 <a title="4-tfidf-16" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>17 0.030624636 <a title="4-tfidf-17" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>18 0.029809721 <a title="4-tfidf-18" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>19 0.028906874 <a title="4-tfidf-19" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>20 0.028797971 <a title="4-tfidf-20" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.2), (1, 0.059), (2, 0.002), (3, 0.024), (4, 0.055), (5, 0.073), (6, -0.138), (7, -0.092), (8, 0.077), (9, -0.044), (10, -0.187), (11, -0.129), (12, 0.051), (13, -0.015), (14, 0.055), (15, 0.292), (16, -0.137), (17, 0.183), (18, 0.139), (19, -0.076), (20, -0.204), (21, -0.027), (22, 0.123), (23, -0.13), (24, 0.059), (25, -0.185), (26, 0.045), (27, -0.089), (28, 0.076), (29, -0.036), (30, 0.001), (31, 0.152), (32, 0.01), (33, 0.022), (34, 0.077), (35, -0.037), (36, -0.155), (37, -0.135), (38, 0.017), (39, -0.053), (40, 0.001), (41, 0.041), (42, 0.021), (43, 0.062), (44, -0.051), (45, -0.029), (46, 0.15), (47, -0.103), (48, 0.08), (49, 0.108)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98276502 <a title="4-lsi-1" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>Author: Carine Hue, Marc Boullé</p><p>Abstract: In this paper, we consider the supervised learning task which consists in predicting the normalized rank of a numerical variable. We introduce a novel probabilistic approach to estimate the posterior distribution of the target rank conditionally to the predictors. We turn this learning task into a model selection problem. For that, we deﬁne a 2D partitioning family obtained by discretizing numerical variables and grouping categorical ones and we derive an analytical criterion to select the partition with the highest posterior probability. We show how these partitions can be used to build univariate predictors and multivariate ones under a naive Bayes assumption. We also propose a new evaluation criterion for probabilistic rank estimators. Based on the logarithmic score, we show that such criterion presents the advantage to be minored, which is not the case of the logarithmic score computed for probabilistic value estimator. A ﬁrst set of experimentations on synthetic data shows the good properties of the proposed criterion and of our partitioning approach. A second set of experimentations on real data shows competitive performance of the univariate and selective naive Bayes rank estimators projected on the value range compared to methods submitted to a recent challenge on probabilistic metric regression tasks. Our approach is applicable for all regression problems with categorical or numerical predictors. It is particularly interesting for those with a high number of predictors as it automatically detects the variables which contain predictive information. It builds pertinent predictors of the normalized rank of the numerical target from one or several predictors. As the criteria selection is regularized by the presence of a prior and a posterior term, it does not suffer from overﬁtting. Keywords: rank regression, probabilistic approach, 2D partitioning, non parametric estimation, Bayesian model selection</p><p>2 0.591088 <a title="4-lsi-2" href="./jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</a></p>
<p>Author: Marc Boullé</p><p>Abstract: The naive Bayes classiﬁer has proved to be very effective on many real data applications. Its performance usually beneﬁts from an accurate estimation of univariate conditional probabilities and from variable selection. However, although variable selection is a desirable feature, it is prone to overﬁtting. In this paper, we introduce a Bayesian regularization technique to select the most probable subset of variables compliant with the naive Bayes assumption. We also study the limits of Bayesian model averaging in the case of the naive Bayes assumption and introduce a new weighting scheme based on the ability of the models to conditionally compress the class labels. The weighting scheme on the models reduces to a weighting scheme on the variables, and ﬁnally results in a naive Bayes classiﬁer with “soft variable selection”. Extensive experiments show that the compressionbased averaged classiﬁer outperforms the Bayesian model averaging scheme. Keywords: naive Bayes, Bayesian, model selection, model averaging</p><p>3 0.45532313 <a title="4-lsi-3" href="./jmlr-2007-Concave_Learners_for_Rankboost.html">23 jmlr-2007-Concave Learners for Rankboost</a></p>
<p>Author: Ofer Melnik, Yehuda Vardi, Cun-Hui Zhang</p><p>Abstract: Rankboost has been shown to be an effective algorithm for combining ranks. However, its ability to generalize well and not overﬁt is directly related to the choice of weak learner, in the sense that regularization of the rank function is due to the regularization properties of its weak learners. We present a regularization property called consistency in preference and conﬁdence that mathematically translates into monotonic concavity, and describe a new weak ranking learner (MWGR) that generates ranking functions with this property. In experiments combining ranks from multiple face recognition algorithms and an experiment combining text information retrieval systems, rank functions using MWGR proved superior to binary weak learners. Keywords: rankboost, ranking, convex/concave, regularization 1. Ranking Problems A ranking problem is a learning problem that involves ranks as the inputs, the outputs or both. An example where ranks are used as inputs is a collaborative ﬁltering application where people are asked to rank movies according to their preferences. In such an application the ranks assigned by different people are combined to generate recommendations. Another type of problem in which ranks are used as inputs are meta-search problems, where the ranks of multiple search engines are combined (Dwork et al., 2001). However, the inputs to a ranking problem are not always ranks. An object recognition ranking problem (Kittler and Roli, 2000) may receive as inputs a graphical representation and output a ranking of the possible objects, sorted by likelihood. The outputs of a ranking problem may also be ranks. For example, in combining multiple search engines the output are ranks which are a synthesis of the ranks from the individual search engines. A similar meta-recognition problem is the combination of the outputs of multiple face- recognition systems to improve the accuracy of detecting the correct face. While the inputs and outputs of this problem are ranks, the outputs can be simpliﬁed to only return the most likely candidate. Another example where the outputs do not need to be complete ranks could be an information retrieval combination task. In such a task the inputs might be ranks of sets of documents with respect to c 2007 Ofer Melnik, Yehuda Vardi and Cun-Hui Zhang. M ELNIK , VARDI AND Z HANG a particular query by different experts. Again, the outputs could be the complete ranks, or more simply a designation for each document of whether it is relevant or not to the particular query. 2. Rankboost The rankboost algorithm (Freund et al., 2003) tries to address this variety in ranking problems by maintaining generality in how it regards its inputs and how it applies different loss functions to outputs. The rankboost algorithm works on instances which are the discrete items (e.g., movies or faces) that either are ranked as input or are to be ranked as output. To allow a general input mechanism, the inputs to rankboost are called the ranking features of an instance, speciﬁed as f j (x) which is the j-th ranking feature of instance x. While this generality in how inputs are handled is potentially powerful, in the original rankboost paper as well as in this paper, only the case where the inputs are different rankings of the instances is considered. Thus, in this paper, the inputs to rankboost are constituent ranks, denoted as f 1 . . . fn , where f j (x ) < f j (x ) implies that instance x has a better rank than instance x under ranking f j . For example, in some of the experiments we present, each f j is the ranking result of a particular face recognition algorithm. The output of rankboost is a new ranking function, H(x), which deﬁnes a linear ordering on the instances, that is, H (x ) < H (x ) iff x has a better rank than x . In rankboost T H(x) = ∑ wt ht (x), (1) t=1 a weighted sum of weak ranking learners, where the ht (x)’s are relatively simple learned functions of the constituent rankings. To address the variety in possible loss functions of the outputs, in rankboost the desirable properties for the output loss function are speciﬁed with a favor function, Φ : X × X → R, where X is the space of instances (note that this function has been renamed from “preference function” to avoid confusion with the use of preference in this paper in Section 4). Here Φ (x , x ) > 0 means that x should be better ranked than x for a given query. It is an anti-symmetric function, that is, Φ (x , x ) = −Φ (x , x ) and Φ (x, x) = 0, which avoids loops where two instances should both be ranked better than the other. Also Φ (x , x ) = 0 when there is no favor in how two instances should be relatively ranked. For example, as described in Section 6.1, for the face recognition combination problem described above the favor function can be used to specify that the correct identity should be given a better rank than all other identities, while zeroing all other entries in the favor function, giving no favor in how incorrect identities are ranked between them. In a similar fashion for the information retrieval combination task mentioned above, the favor function can be speciﬁed such that all relevant documents should be better ranked than irrelevant documents, without specifying favor for the ordering between relevant documents and the ordering between irrelevant documents (Section 7.1). Rankboost is shown in Algorithm 1 as described in Freund et al. (2003). It uses the favor function to specify an initial weight function over instance pair orderings: D x ,x = c · max 0, Φ x , x −1 , (2) where c = ∑x ,x max (0, Φ (x , x )) . The algorithm itself is very similar to adaboost (Freund and Schapire, 1996). At each iteration the algorithm selects the weak learner that best maximizes a 792 C ONCAVE L EARNERS FOR R ANKBOOST Algorithm 1 rankboost algorithm for generating a ranking function. Input: constituent ranks, a favor function, and a class of weak learners with outputs between 0 and 1 and an appropriate training algorithm. Output: ranking function H(x) (Eq. 1) Initialize D(1) = D (Eq. 2) for t = 1 . . . T do Find weak learner, ht , that maximizes r(h) = ∑x ,x D(t) (x , x )(h(x ) − h(x )) (Eq. 4). Choose wt = 0.5 ln ((1 + r(ht )) / (1 − r(ht ))) . (Eq. 5) Update: D(t+1) (x , x ) = D(t) (x , x ) exp (wt (ht (x ) − ht (x ))) Zt where Zt is chosen to make ∑x ,x D(t+1) (x , x ) = 1 end for rank function’s utility, r(h), (Eq. 4). It then assigns it a weight in proportion to its performance and adds the weighted weak learner to the ranking function, H(x). After which the weight function D is adjusted to reﬂect the impact of the weak learner. Freund et al. (2003) prove that the rankboost algorithm iteratively minimizes the ranking loss function, a measure of the quantity of misranked pairs: ∑ D x ,x I H x ≤ H x x ,x where I is the indicator function (1 if true, 0 otherwise) and H(x) is a ranking function output by rankboost. The rankboost paper (Freund et al., 2003) uses binary weak learners of the following functional form:  i f f j (x) > θ,  1 0 i f f j (x) ≤ θ, (3) h(x) =  qde f i f f j (x) unde f ined. Each binary weak learner, h, operates on a single f j (ranking feature), giving a binary output of 0 or 1 depending on whether the instance’s rank is smaller than a learned parameter, θ. Note that function h(x) in (3), which is dependent on only one f j (x) with a ﬁxed j, is monotonic increasing but not convex or concave. If there is no rank for the instance then it returns a prespeciﬁed q de f value. 3. Rankboost, the Weak Learner and Regularization While rankboost inherits the accuracy of adaboost and has been shown to be very successful in practice, in two important ways it is very different from adaboost and similar classiﬁer leveraging algorithms (Meir and Ratsch, 2003). The ﬁrst is the choice of the weak learner, h. In adaboost the weak learner is expected to minimize the weighted empirical classiﬁcation error: N ∑ d (t) (i)I [yi = h (zi )] , i=1 where yi is the class label, I is the indicator function and d (t) is a weighting over training samples. This is a standard function to minimize in classiﬁcation with many possible types of algorithms to 793 M ELNIK , VARDI AND Z HANG choose from as possible weak learners. In contrast the weak ranking learner for rankboost (with outputs in [0, 1]) needs to maximize the following rank utility function: r = r(h) = ∑ D(t) (x , x )(h(x ) − h(x )), (4) x ,x where D(t) is the weight function over pairs of instances. As Eq. 4 contains the term h(x ) − h(x ), short of linear learners this equation can not be concave in h or easily approximated by a concave function. Therefore the weak learner needs to be optimized either by heuristics or by brute force, which limits the choice of h. It is not surprising that the original rankboost paper only included one type of weak learner, a binary threshold function (Eq. 3) that was tuned using brute force. The second difference between rankboost and adaboost also concerns the weak ranking learner. One feature of boosting that has sparked a great deal of interesting research is the algorithm’s ability to avoid overﬁtting for low noise classiﬁcation problems (with modiﬁcations to higher noise problems), see Meir and Ratsch (2003) for a survey. In contrast for rankboost it is only by limiting the type of underlying weak learners that overﬁtting is avoided. In their paper, Freund et al. (2003) show that not using weak ranking learners with cumulative positive coefﬁcients leads to overﬁtting and poor performance quite quickly. Therefore, choosing a weak learner that regularizes the ranking function, the output of rankboost, is very important for achieving accuracy and avoiding overﬁtting. It is clear from the above discussion that the choice of a weak ranking learner for rankboost is important and non trivial. First, the learner must be efﬁciently tunable with respect to Eq. 4, typically limiting its complexity. Second, the learner itself must demonstrate regularization properties that are appropriate for ranking functions. In this paper we present a new weak ranking learner that enforces consistency in preference and conﬁdence for the ranking function by being monotonic and concave. We start with a discussion of these regularization properties, theoretically justify them, and show what they mean in terms of the ﬁnal ranking function. Then we present a new learner, Minimum Weighted Group Ranks (MWGR), that satisﬁes these properties and can be readily learned. This learner is tested and compared with the binary learner of rankboost on combining multiple face recognition systems from the FERET study (Phillips et al., 2000) and on an information retrieval combination task from TREC (Voorhees and Harman, 2001). 4. Regularizing Ranking Functions with Consistency in Preference and Conﬁdence In this paper, as Freund et al. (2003), we consider ranking functions H(x) which depend on x only through the values of the ranking features, y j = f j (x), for that instance, so that H(x) = G ( f1 (x), . . . , fn (x)), for certain functions G (y1 , . . . , yn ) = G(y). Here, we assume that the f j (x) is an actual rank assigned to an instance, x, by the j-th ranker. Note that if the original data are numerical scores then they can easily be converted to rankings. Freund et al. (2003) make a strong case for conversion of raw measures to relative orderings (rankings) over combining measures directly, arguing that it is more general and avoids the semantics of particular measures. As the y j ’s are ranks instead of points in a general space, care should be taken as to the functional form of G. A great deal of literature in social choice theory (Arrow, 1951) revolves around the properties of various rank combination strategies that try to achieve fair rankings. In this machine learning case our motivations are different. Fairness is not the goal; the goal is to improve the 794 C ONCAVE L EARNERS FOR R ANKBOOST accuracy or performance of the ranking function. Thus, regularization, by functionally constraining G, is used to confer information on how to interpret ranks in order to ultimately improve accuracy. Freund et al. (2003) imposed the regularization principle of monotonicity on the ranking function. Monotonicity encompasses a belief that for individual features a smaller rank is always considered better than a bigger rank. It means that for every two rank vectors, a and b, if a j ≤ b j , j = 1, . . . , n then G(a) ≤ G(b). Monotonicity was enforced by requiring that the coefﬁcients, wt in Eq. 1, combining the binary weak learners (Eq. 3) were cumulatively positive. As shown by Freund et al. (2003), without enforcing monotonicity the rankboost algorithm quickly overﬁts. In this section we present another regularization principle, consistency in preference and conﬁdence (which includes monotonicity). A ranking function with this regularization property should be consistent in its preference for the individual rankers and also in how it captures their conﬁdence. The following example illustrates these two concepts. 4.1 Grocer Example A grocer needs to make 2 decisions, to decide between stocking oat bran vs. granola and to decide between stocking turnips vs. radishes. The grocer asks her consultants to express their opinion about stocking each item, and based on their responses makes her 2 decisions. First of all, in either problem, the grocer will adopt the opinion of her consultants if they all agree with each other, that is, they all prefer granola over oat bran in the ﬁrst problem. Lets assume the grocer considered the ﬁrst problem and chose granola over oat bran. What this implies is that the grocer adopted the opinions of the consultants that preferred granola over oat bran. Now consider the turnips vs. radishes decision. Lets say that the same consultants that liked granola more also liked radishes more (and the same ones that like oat bran more like turnips more). Also, for this decision the radish lovers actually feel more conﬁdent in their choice than they did for granola, while the turnip lovers are more unsure than they were for oat bran. Then for this second choice, if the grocer is consistent in her method of combining the opinions of her consultants, we would expect her to pick radishes over the turnips. In addition, we would expect her to be more conﬁdent in this second decision as a reﬂection of the consultants relative conﬁdence. The above properties of preference and conﬁdence imply that preference should be applied consistently across different problems and conﬁdence should reﬂect the relative conﬁdences of the individual consultants. 4.2 The General Principle of Consistency in Preference and Conﬁdence To generalize the above grocer’s problem consider the problem of combining the opinions of a panel of consultants on several multiple-choice decision problems. Suppose for each decision problem each consultant provides his preference as one of the possible choices and a qualitative measure of the conﬁdence level for his preference. The consistency principle in preference and conﬁdence holds if the combiner always agrees with at least one of the consultants in each decision problem and that the following condition holds for any pair of decision problems. Deﬁnition 1. Consistency principle for a pair of decision problems: Suppose that for the ﬁrst decision problem, the combiner adopts the preference of a subset A of consultants in the sense that A is the (non empty) set of all consultants whose preference is identical to that of the combiner. 795 M ELNIK , VARDI AND Z HANG Suppose that for the second decision problem, the preference of the consultants in A is again identical. Suppose further that compared with the conﬁdence level for the ﬁrst decision problem, each consultant in A has higher or the same conﬁdence level for his preference in the second problem, while each consultant with a different preference than A for the second problem has lower or the same conﬁdence level. Then, the preference of the combiner for the second problem is identical to that of the consultants in A, and the conﬁdence level of the combiner for the second problem is at least as high as his conﬁdence level for the ﬁrst problem. Let B be the set of consultants whose preferences are different from that of the combiner in the ﬁrst decision problem, that is, B = Ac . If some members of B switch sides or have no preference in the second problem, the combiner would be even more conﬁdent about the adoption of the preference of A in the second problem regardless of the conﬁdence of those who switch sides. Thus, Deﬁnition 1 requires only those against the preference of A in the second problem (necessarily a subset of B since members of A act in unison in both problems) to have lower or equal conﬁdence in the second problem, instead of all members of B. This is taken into consideration in Theorem 1 below. Note that while preference for individual consultants is speciﬁed within each decision problem, two decision problems are needed to compare the qualitative measure of conﬁdence. However, comparison of conﬁdence is not always possible (it is a partial ordering). In particular, the level of conﬁdence between different experts may not be comparable, and the levels of conﬁdence of a given expert (or the combiner) for different decision problems are not always comparable. 4.3 Conﬁdence for Ranks and Ranking Functions In order to apply consistency to rank combination we need to specify what we mean by more or less conﬁdence. For ranks we make the assumption that a constant change of ranks requires more conﬁdence for low ranks than high ranks. For example, we would expect the difference between ranks of 1 and 3 to represent a more signiﬁcant distinction on the part of a ranker than would for example the difference between ranks 34 and 36 which may be almost arbitrarily assigned. Speciﬁcally we make the following deﬁnition: Deﬁnition 2. Preference and conﬁdence for univariate rank values For any pair of rank values {r, r } ⊂ R with r < r , by monotonicity r is preferred. For any two pairs of rank values {r, r } and {r , r } with r < r and r ≤ r , the conﬁdence level for {r, r }is higher than that of {r , r } if r ≤ r , r −r ≤ r −r with at least one inequality. The pair{r, r }provides no preference if r = r . Likewise, if either r − r = r − r = 0 or r − r = r − r = 0, the two pairs {r, r } and {r , r }are deﬁned to have the same level of conﬁdence. In this deﬁnition conﬁdence between pairs of ranks can only be compared if the pair with the lowest rank has a gap at least as large as the other pair. Thus, we are actually comparing two numbers, that is, we are doing a vector comparison. As such, this comparison does not cover all pairs of ranks and applies only a partial ordering on rank pairs. As a regularization principle, a partial ordering is desirable since it does not overly constrain the ranking function and allows for ﬂexibility. 796 C ONCAVE L EARNERS FOR R ANKBOOST As the rank function, G, is a univariate function, we apply the same deﬁnitions of preference and conﬁdence to it. That is, for a ranking function G and for any ﬁxed rank vectors, y and y , the preferred rank vector is the one with smaller G, that is, y is preferred iff G(y) < G(y ). For conﬁdence again we need to consider pairs of rank vectors, {y, y } and {y , y }, with G(y) ≤ G(y ) and G(y ) ≤ G(y ). If G(y) ≤ G(y ) and G(y ) − G(y) ≥ G(y ) − G(y ) then we say that the ﬁrst decision, between yand y , shows equal or more conﬁdence than the second decision between y and y . This numerical method of capturing conﬁdence and preference in ranks, y, and ranking functions, G, allows us to apply Deﬁnition 1. Speciﬁcally, for a pair of binary decisions the conﬁdence of a consistent ranking function increases for the second decision if in the second decision the conﬁdence of the agreeing rank values are comparable and increase and the conﬁdence of the disagreeing rank values are comparable and decrease, with the exception of those that switched sides. 4.4 Three-Point Consistency in Preference and Conﬁdence for Combining Ranks As described, the consistency property is over two binary decision problems. In this section we consider ranking functions, G, that have consistency in preference and conﬁdence for all pairs of binary decision problems involving three rank vectors y, y and y . We show that such functions have speciﬁc mathematical properties under this three-point consistency principle in preference and conﬁdence for ranks.. Theorem 1: For a ranking function G whose domain is convex, three-point consistency in preference and conﬁdence holds for G iff G(y) is nondecreasing in individual components of y and is jointly concave in y. Proof of Necessity: Assume that the ranking function G exhibits three-point consistency in preference and conﬁdence for any three rank vectors. If y ≤ y component wise, then the individual components of y all agree in their preference to y, so that G(y) ≤ G(y ) by the consistency of G in preference. This implies the monotonicity of G in y. We pick three points y, y − a and y + a, such that all points are rank vectors. Consider the pair of binary comparison problems, y vs. y + a and y − a vs. y. Assume that G(y) < G(y + a). These three points are comparable by Deﬁnition 2 (r − r = r − r for all components in the rank vectors). Since the agreeing rankers, A = j y j < y j + a j , in the y vs. y + a comparison are greater than in the y − a vs. y comparison, and the disagreeing ranks, outside A, are smaller, then by the consistency of the combiner in preference and conﬁdence (deﬁnition 1), G(y − a) ≤ G(y) and G(y) − G(y − a) ≥ G(y + a) − G(y). A function G is concave in a convex domain iff 2G(y) ≥ G(y − a) + G(y + a), for every y and a with y ± a in its domain. To verify this inequality for a particular y and a, we must have G(y + a) > G(y), G(y − a) > G(y) or the third case of G(y) ≥ max{G(y + a), G(y − a)}. We have already proven that the consistency properties of preference and conﬁdence imply G(y) − G(y − a) ≥ G(y + a) − G(y) in the ﬁrst case. By symmetry this requirement also must hold for −a, so that G(y) − G(y + a) ≥ G(y − a) − G(y) in the second case. This completes the necessity part of the proof since 2G(y) ≥ G(y − a) + G(y + a) automatically holds in the third case. Proof of Sufﬁciency: Assume the ranking function G is nondecreasing in individual components of y and jointly concave in y. We need to prove consistency in preference and conﬁdence for a pair 797 M ELNIK , VARDI AND Z HANG 1) G(y) < G(y’) y ≤ y’’ A A y’’ A 4 A A 10 y’ 8 y’ 6 4) G(y) > G(y’) y ≥ y’’ A 10 8 A 2) G(y) < G(y’) y ≥ y’’ A 10 8 6 y A y 6 4 4 2 2 2 0 0 y y’ y’’ y’’ 0 2 4 6 8 10 0 2 4 B 6 8 10 0 0 B 2 4 6 8 10 B Figure 1: Consider two decision problems in two dimensions involving three points, y, y and y , where the ﬁrst decision problem is to choose between y and y and the second problem is to choose between y and y . The cases where we expect consistency in preference and conﬁdence (Deﬁnitions 1 and 2) can be enumerated by the result of the ﬁrst decision problem and relative location of y . The darker gray box is the location where B has lesser or equal conﬁdence in the second problem, and the lighter gray box is where B switches sides. of decision problems involving three rank vectors y, y and y . Without loss of generality, suppose that the ﬁrst problem is to choose between y and y and the second problem is to choose between y and y . We break the proof up by the results of the comparison between y vs. y and the relative location of the third ranking vector y that satisﬁes the preference and conﬁdence requirements. If G(y) = G(y ) = G(y ), the combiner has equal conﬁdence in the two decision problems by deﬁnition 2, so that the consistency principle holds automatically. Thus, we only need to consider the cases where G(y) = G(y ). Figure 1 illustrates three of these cases two dimensionally, where there is a single agreeing component A, the y-axis, and a single disagreeing component B, the x-axis. Let A be the set of agreeing indices, A = j : sgn(y j − y j ) = sgn (G(y ) − G(y)) = 0 , and B = c the set of disagreeing indices. We use the notation y = (y , j ∈ A) to describe corresponding A j A subvectors. Also, when used, vector inequalities are component wise. Case 1: G(y) < G(y ) and yA ≤ yA In the ﬁrst decision problem, as G agrees with A and disagrees with B yA < y A , yB ≥ y B . / We also know that A = 0 since otherwise y ≥ y and therefore by monotonicity G(y) ≥ G(y ), which is a contradiction. For the second decision problem we consider the values of y which are consistent with the conﬁdence assumption (Deﬁnitions 1 and 2), that is, agreeing with more conﬁdence along the A indices and disagreeing with less conﬁdence or switching sides along the B indices. As y A ≤ yA , either by the conﬁdence relationship or by switching sides (see Figure 1) these y values satisfy yA − y A ≥ y A − y A , 798 yB − y B ≤ y B − y B C ONCAVE L EARNERS FOR R ANKBOOST which implies y ≤ y , and therefore G(y ) ≤ G(y ) by monotonicity. Thus, G(y) < G(y ) ≤ G(y ), which implies that in the second decision problem of choosing between y and y , the preference of G is the same as that of A (i.e., y since yA ≤ yA ) and the conﬁdence of G is at least as high as the ﬁrst decision problem. Case 2: G(y) < G(y ) and yA ≥ yA Since in this case yA ≥ yA , then either by the conﬁdence relationship or by switching sides y satisﬁes yA − y A ≥ y A − y A , yB − y B ≤ y B − y B . This implies that y + y ≤ 2y, which means that G(y ) + G(y ) ≤ 2G y +y /2 ≤ 2G(y) by the concavity and monotonicity of G. Thus, G(y) − G(y ) ≥ G(y ) − G(y) and since G(y ) > G(y), we have that G(y) − G(y ) > 0 and thus G(y) > G(y ). Therefore we see that the preference in G for the two decision problems is the same as A (with y in the ﬁrst problem and with y in the second problem) and the conﬁdence is no smaller for the second comparison. Case 3: G(y) > G(y ) and yA ≤ yA Since y is preferred in the ﬁrst decision problem we have yA > y A , yB ≤ y B . For the conﬁdence assumption to hold, that is, greater conﬁdence for A in the second problem (Deﬁnition 2), the smaller ranks in the second problem have to be smaller than the smaller ranks in the ﬁrst problem. But with yA ≥ yA and yA ≤ yA that can only be if yA = yA , which leads to y ≤ y , and by monotonicity implies G(y) ≤ G(y ). However that is a contradiction to G(y) > G(y ), and therefore this is not a viable case for comparing conﬁdence. Case 4: G(y) > G(y ) and yA ≥ yA Since in this case yA ≥ yA , then either by the conﬁdence relationship or by switching sides y satisﬁes yA − y A ≥ y A − y A , yB − y B ≤ y B − y B , which means that y ≤ y . Thus, by the monotonicity of G, G(y ) ≤ G(y ). Since G(y ) < G(y) the preference in the second decision problem is also with A (i.e., y ) and the conﬁdence is at least as high as the ﬁrst decision problem. 4.5 Applying Regularization to Rankboost It follows from the above theorem that to have consistency in preference and conﬁdence we desire ranking functions that are monotonic and concave. In rankboost H(x) = ∑ wt ht (x) (Eq. 1). To make H an increasing and concave function of constituent rankings y j = f j (x) we need to constrain the weak ranking learners. If the learners themselves are monotonically increasing and concave functions of y j , then linearly combining them with positive weights will give an H that is also an increasing and concave function of y j . 799 M ELNIK , VARDI AND Z HANG In this paper, we apply the “third method” (Freund et al., 2003) to setting a wt weight value. That is, weak learners are selected on their ability to maximize r from Eq. 4 and then wt = 0.5 ln ((1 + rmax ) / (1 − rmax )) . (5) Therefore, using monotonic and concave weak learners we select only ones that rankboost gives a positive r value to, which renders a positive wt weight. If no r values are positive the rankboost algorithm stops. We mention that a ranking function can be construed as the negative of a score function, that is, G(y) = −S(y). For score functions these regularization properties become monotonically decreasing, and convex (Melnik et al., 2004). 5. Minimum Weighted Group Ranks The functional structure of the new learner we propose is h(y) = min {γ1 y1 , . . . , γn yn , 1} , (6) where y = (y1 , . . . , yn ) = ( f1 (x), . . . , fn (x)) the vector of ranking features, and the γ j are learned positive coefﬁcients. Note that the function’s range is in [0, 1] due to the 1 term in the min function. Using rankings as our features, the learner function (Eq. 6) is monotonically increasing. It is also a concave function in y. Thus if these learners are linearly combined with positive weights, the resulting ranking function, H, will have three-point consistency in conﬁdence and preference. To gain some intuition, the functional form of the learner is related to the Highest Rank combination method (Ho, 1992) that assigns combined ranks as the best rank each class receives from any ranker. This is a conﬁdence based approach, as Highest Rank bets on the classiﬁer that is most conﬁdent, the one giving the best rank. As such, a single learner can potentially be error prone. But as we combine many of these learners during boosting, it becomes more powerful, allowing the conﬁdence of different classiﬁers to be weighed with preference for their potential accuracy. 5.1 Learning At each boosting iteration, rather than selecting from all possible weak learners of form (Eq. 6), we limit our choices by building new weak learners out of the ones that have been previously trained. Let F = { f1 (x), . . . , fn (x)} be the set of ranking features. Recall that in rankboost H(x) = (t) (t) (t) ∑t wt ht (x), where ht (x) = min γ1 f1 (x), . . . , γn fn (x), 1 and γ j are learned coefﬁcients. We set (s) (s) Ht = h(x) h(x) = min γ1 f1 (x), . . . , γn fn (x) , s ≤ t and select ht+1 (x) from weak learners of the form hnew (x) = min αh(x), β f (x), 1 (7) (t+1) with h(x) ∈ Ht and f (x) ∈ F . This learner can be rewritten in the form of Eq. 6 with the γ j derived from the learned α, β, h(x) and f (x). Thus, at each iteration we look at combinations of 800 C ONCAVE L EARNERS FOR R ANKBOOST the features and existing learners. As discussed in the next section, we can either consider all such combinations, or use a heuristic selection strategy. We propose to optimize α and β separately, in stages. That is, given a value for one of the variables we optimize the other. As α and β are symmetric we show how to optimize β given α. We need to ﬁnd a value of β that maximizes r in Eq. 4. Freund et al. (2003) pointed out that this equation can be rewritten as: r = ∑ D(t) (x , x )(h(x ) − h(x )) x ,x = ∑ π(x)h(x) x where π(x) = ∑x D(t) (x , x) − D(t) (x, x ) . Given the form of Eq. 7 we can write r as a function of β, ∑ (β f (x) − 1) π(x) + ∑ r (β) = αh(x) − 1 π(x). β f j (x)≤min(αh(x),1) (8) αh(x) < 1 , then O(βl+1 ) = O(βl ) − π(xl ), P(βl+1 ) = P(βl ) − f (xl )π(xl ), Q(βl+1 ) = Q(βl ) +W π(xl ), R(βl+1 ) = R(βl ) +W αh(xl )π(xl ). Combining these formulas gives algorithm 2 for optimizing β. Algorithm 2 Algorithm for optimizing β Given α, h(x) ∈ Ht , f (x) ∈ F and the training instances. For all x’s generate and sort the set of candidate βs, B, such that β 1 ≤ β2 ≤ · · · ≤ β|B| and βl = min(αh(xl ),1) . f (xl ) O = ∑xl π(xl ) P = ∑xl f (xl )π(xl ) Q=0 R=0 rbest = 0 for j = 1 . . . |B| do r = βl P − O + R − Q if r > rbest then rbest = r end if O = O − π(xl ), P = P − f (xl )π(xl ) if αh(xl ) < 1 then Q = Q + π(xl ), R = R + αh(xl )π(xl ) end if end for 5.2 Heuristics for Learner Selection If at each boosting iteration we select from all combinations of h(x) and f (x) we end up with an O(T 2 ) algorithm, where T is the number of iterations. However, we can sacriﬁce accuracy for speed by only evaluating and selecting from a ﬁxed sized pool of previously trained learners h(x) and features f (x), where at each iteration the pool is randomly chosen from the full Ht and F . To improve performance, instead of using a uniform sampling distribution we can apply a heuristic to focus the distribution on combinations with better potential. As Eq. 8 is composed of two sums, for r to be large the terms f (x)π(x) and h(x)π(x) need to be large. We can consider s f = ∑x f (x)π(x) and sh = ∑x h(x)π(x) as indicators of how well these components work with π(x). Thus, we might expect larger r values to occur when these two score values are larger. Of course, we are discounting interactions, which is the reason for the combination. Using these score values, we can order all h(x) and f (x) separately, and sample such that learners and features with better scores are more likely to be selected. We opted for a polynomial weighting 802 C ONCAVE L EARNERS FOR R ANKBOOST Training error on Dup II 9 P=1 P=1/2 P=1/4 Avg rank of correct class 8.8 8.6 8.4 8.2 8 7.8 7.6 7.4 7.2 1 2 3 4 5 6 7 8 9 Iteration number Figure 2: These plots show convergence of training error for 3 values of the heuristic selection pressure value p. These plots are averages over 10 runs and are typical of the other training data sets as well. As can be seen the heuristic improves the convergence rate of the training error. method. Thus, for example, all f ∈ F are sorted by their score and are assigned a number based on the rank of these scores,(maxrank − rank)/maxrank, that gives each f an equally sized bin in the range 0 and 1. Given a random number ξ ∼ U(0, 1), we calculate ξ p and select the f that corresponds to the bin this number falls into. Here p < 1 can be construed as a selection pressure, where bins corresponding to higher scores are more likely to be selected. Figure 2 demonstrates the effect of different values of the p parameter in one of our experiments. 6. Face Recognition Experiments We present experiments on the combination of face recognizers, comparing the binary learner with the MWGR learner. Given an image of a face, a face recognizer assigns a similarity score to each of the faces it is trained to recognize. These scores give a linear order or ranking to the gallery of faces. Different face recognition algorithms have different performance characteristics. Thus, we explore how combining the outputs of multiple face recognizers can improve recognition accuracy. 6.1 Algorithm Methods We consider a data set I of face images (queries) to train on. For each query image, i in I, we need to rank all u ∈ U faces in the gallery. In rankboost the favor function, Φ, that speciﬁes output loss, is a function of the query and the item to be ranked. Therefore, the notational convention is to combine the query, i, and the item to be ranked, u, as an instance, x ≡ (i, u). As such, f j (x) = f j ((i, u)) is the 803 M ELNIK , VARDI AND Z HANG Error convergence 9 Train error on dup II Test error on dup I Avg rank of correct class 8.5 8 7.5 7 6.5 6 0 10 20 30 40 50 60 70 80 90 100 Iteration number Figure 3: This plot is typical of the convergence behavior of rankboost with MWGR on the FERET data. Both training and test errors tended to converge within 10-30 iterations of boosting with no signiﬁcant post-convergence divergence. rank assigned to identity u for query image i by recognition algorithm j. As there is only one correct identity, we only care about the ranking of the one correct identity for each query. We set the favor function as stated by Freund et al. (2003) for this type of output loss. Let u ∗ be the the correct identity for training image i, then Φ ((i, u) , (i, u∗ )) = +1 and Φ ((i, u∗ ) , (i, u)) = −1 for all u = u∗ , setting all remaining elements of Φ (x , x ) = 0. That is, the correct identity of a query image is given positive favor compared to all other identities for that image, while all other rankings, including interactions between training images, are given zero favor. Note that since there is no favor interaction between queries (different i’s); Φ (x , x ) is effectively a function of 3 variables, (i, u , u ). Both weak learners were trained for 100 iterations, giving them ample time to converge. See Figure 3 for an illustration of convergence times. The binary learner was trained as speciﬁed by Freund et al. (2003). At each iteration the MWGR learner was selected from a pool of candidate combinations of f (x) and h(x), with a selection pressure of p = 0.5. For each candidate, ﬁrst β was optimized with α = 1, then α was optimized using the optimized β. The candidate with the most positive r value was always selected. This is summarized in algorithm 3. 6.2 Experimental Setup FERET (Phillips et al., 2000) was a government sponsored program for the evaluation of face recognition algorithms. In this program commercial and academic algorithms were evaluated on their ability to differentiate between 1,196 individuals. The test consisted of different data sets of varying difﬁculty, for a total of 3,816 different images. The data sets, in order of perceived difﬁculty, are: the fafb data set of 1,195 images which consists of pictures taken the same day with different facial 804 C ONCAVE L EARNERS FOR R ANKBOOST Algorithm 3 Algorithm for selecting learner from pool. Given poolsize and selection pressure. Calculate s f j and sh for all rank features and existing learners. for p = 1 . . . poolsize do Generate d f ∼ U(0, 1) and dh ∼ U(0, 1) Select which h = min αh(x), β f (x), 1 to try, using s f j , sh , u f , uh . Setting α = 1, optimize β (algorithm 2) Keeping the optimized β, optimize α (algorithm 2) Get r of learner with this α, β if r > 0 and r > rbest then rbest = r, hbest = h, hbest = min αh(x), β f (x) end if end for ht = hbest, Ht+1 = Ht ∪ hbest expressions; the fafc data set of 194 images that contains pictures taken with different cameras and lighting conditions; the dup I data set of 488 images that has duplicate pictures taken within a year of the initial photo; and the most difﬁcult, the dup II data set of 234 images which contains duplicate pictures taken more than a year later. Note that in our experiments we separate the images of dup II from the dup I data set, unlike the FERET study where dup II was also a subset of dup I. The FERET study evaluated 10 baseline and proprietary face recognition algorithms. The baseline algorithms consisted of a correlation-based method and a number of eigenfaces (principle components) methods that differ in the internal metric they use. Of the proprietary algorithms, most were from different academic institutions and one was commercial. Of the 10 algorithms we selected three dominant algorithms. From the baseline algorithms we chose to use the ANM algorithm which uses a Mahalanobis distance variation on angular distances for eigenfaces (Moon and Phillips, 2001). While this algorithm’s performance is not distinctive, within the class of baseline algorithms it was strong. Moreover, in accuracy with respect to average rank of the correct class on the dup I data set it demonstrated superior performance to all other algorithms. The other two algorithms we used were the University of Maryland’s 1997 test submission (UMD) and the University of Southern California’s 1997 test submission (USC). These algorithms clearly outperformed the other algorithms. UMD is based on a discriminant analysis of eigenfaces (Zhao et al., 1998), and USC is an elastic bunch graph matching approach (Wiskott et al., 1997). The outputs of the 10 face recognizers on the four FERET data sets, fafb, fafc, dup I and dup II were the data for the experiments. Thus, we never had access to the actual classiﬁers, only to data on how they ranked the different faces in these data sets. We conducted experiments based on homogeneous and heterogeneous data sets, testing the efﬁciency and robustness (adaptivity) of the MWGR procedure. For the homogeneous case we took all 4 FERET data sets and randomly shufﬂed them together. We call this the homogeneous data set as both the training and testing data are selected from the same combined pool. On this combined data set we did 4-fold cross validation. For each fold 75% of the data was used for training and the rest for testing. We combined the results of all four runs together for evaluation purposes. 805 M ELNIK , VARDI AND Z HANG For the heterogeneous case, in each experiment one of the FERET data sets was selected as a training set and another data set was selected for testing. This gave 12 experiments (not including training and testing on the same data set) per group of face recognizers, where we get combinations of training on easy data sets and testing on hard data sets, training on hard and testing on easy data sets, and training and testing on hard data sets. To reduce noise in our experiments the training ranks were truncated at 150, and outliers were removed. In face recognition and other classiﬁcation applications usually only the top ranks are important. Thus, in evaluating the results we focused on the top 30 ranks. All ranks returned by a boosted combiner for the correct class above 30 were truncated to 30. In evaluating the performance of the combiners not all the test data are equally useful. We consider the following two cases as non informative. When the two best face recognizers, UMD and USC both give the correct class a rank of 1 there is very little reason for the combined rank to be different. Also when both the binary-learner-based combiner and the MWGR-based combiner give the correct class a rank greater than the truncation value (30) it makes little sense to compare between the combiners. The testing data was ﬁltered to remove these cases, and the results are presented without them. Before presenting the results, it should be said that rankboost with both learner types gives ranking functions that signiﬁcantly outperform all the individual face recognition algorithms. In addition, in our tests both learners also clearly outperformed other standard rank combination methods, such as the Borda count, highest rank and logistic regression (Ho et al., 1992). We present two sets of experiments—the combination of the 3 selected classiﬁers (ANM, UMD and USC) and the combination of all 10 classiﬁers. These are qualitatively different tasks. In combining 3, we seek to capitalize on the unique strengths of each classiﬁer. In combining 10, we are introducing classiﬁers which may be correlated and classiﬁers which are comparably much noisier. The size of the pool was 6 when we combined 3 classiﬁers and 20 when combining all 10 classiﬁers. For all experiments we measure the average rank of the correct class for both learners: A= 1 N ∑ min {Rank (xi∗ ) , 30} , N i=1 where Rank (xi∗ ) is the rank of the correct class for query i, and the sum is over all useful test queries, as described above. The average rank difference between the learners is calculated to show the improvement in performance. To evaluate the signiﬁcance of the improvement we ran paired one-sided t-tests and evaluated the signiﬁcance of the p-value (a value less than 0.05 is signiﬁcant). In addition we show the standard deviation of the rank difference. 6.3 Results In the experiments with the homogeneous data sets, combining all classiﬁers gives an improvement in the average rank of the correct class of 0.296 for MWGR, with a standard deviation of 3.9, a paired t-test statistic of 1.76 and p-value of 0.039, where combining the ANM, UMD and USC classiﬁers gives an average rank improvement of 0.1 for MWGR, with standard deviation of 3.6, a paired t-test statistic of 0.68 and p-value of 0.24. Table 2 contains the results for combining the 3 classiﬁers in the experiments with heterogeneous data sets (compare the combiner results in columns bin mean and mwgr mean with the aver806 C ONCAVE L EARNERS FOR R ANKBOOST ANM ARL EFAVG EFML1 EFML2 EXCA MSU RUT UMD USC dup i 9.52 16.85 15.02 18.23 15.85 11.9 17.64 17.87 13.21 12.44 dup ii 18.14 16.96 20.61 22.21 20.33 16.28 23.44 16.48 13.74 6.85 fafb 10.88 8.94 14.43 16.23 12.21 11.67 6.81 12.93 4.57 5.84 fafc 19.71 28.02 26.17 17.39 19.78 20.30 14.27 22.79 8.96 5.17 Table 1: The best average rank of the correct class on the different data sets for all constituent face recognition systems. age rank of the correct class for all constituent classiﬁers in Table 1). The diff mean column contains the improvement of MWGR over the binary learner in terms of average rank of the correct class. Of the 12 experiments, we see an improvement in 10 cases. Six of those 10 have signiﬁcant p-values. The two no improvement experiments do not have signiﬁcant p-values. Table 3 contains the results for combining all 10 classiﬁers. Of the 12 experiments, we see an improvement for MWGR in 11 cases. Eight or nine of those 11 have signiﬁcant p-values. The one no improvement experiment does not have a signiﬁcant p-value. It is interesting to note that we do not seem to see overﬁtting when increasing the number of constituents. In some case we see improvement, in others we see slight degradation, but all in all the combiner seems resilient to the noise of adding less informative constituents. All sets of experiments, homogeneous data, heterogeneous data sets, combining 3 select recognizers and combining all 10 recognizers at once yielded signiﬁcant improvements in accuracy, as is visible in the change in the average rank of the correct class and the signiﬁcance of the statistical tests. 7. Information Retrieval Experiments The annual Text REtrieval Conference (TREC) generates high-quality retrieval results of different systems on different retrieval tasks (Voorhees and Harman, 2001). We use the result data sets of the TREC-2001 web ad hoc task that uses actual web queries taken from web logs. This task has been used in other rank fusion experiments (Renda and Straccia, 2003). As did Renda and Straccia (2003) we combine the results of the following top 12 systems: iit01m, ok10wtnd1, csiro0mwal, ﬂabxtd, UniNEn7d, fub01be2, hum01tdlx, JuruFull, kuadhoc, ricMM, jscbtawtl4, apl10wd. Similar to other TREC information retrieval tasks, the TREC-2001 ad hoc task consists of 50 queries. For each query a list of relevant documents is supplied. Each system returns an ordered list of 1,000 documents for each query. The fusion goal is to combine these 12 individual lists into one list of 1,000 documents, which hopefully has greater precision than the individual systems. For the 807 M ELNIK , VARDI AND Z HANG test set dup i dup i dup i dup ii dup ii dup ii fafb fafb fafb fafc fafc fafc train set dup ii fafb fafc dup i fafb fafc dup i dup ii fafc dup i dup ii fafb bin mean 7.19 7.36 8.31 5.25 5.15 5.19 2.62 3.38 2.60 2.85 2.40 2.56 mwgr mean 5.96 6.21 6.27 5.38 4.4 4.95 2.22 2.60 2.28 2.78 2.43 2.03 diff mean 1.22 1.14 2.04 -.12 0.74 0.23 0.39 0.78 0.32 0.06 -.03 0.52 diff std 5.22 5.15 5.51 4.0 4.61 5.1 3.09 3.79 2.81 3.33 2.27 2.57 pval .7e-4 .001 .1e-6 .658 .018 .275 .115 .029 .142 .424 .537 .028 Table 2: Results of combining the ANM, UMD, and USC classiﬁers using individual FERET data sets. test set dup i dup i dup i dup ii dup ii dup ii fafb fafb fafb fafc fafc fafc train set dup ii fafb fafc dup i fafb fafc dup i dup ii fafc dup i dup ii fafb bin mean 6.73 8.06 6.68 5.75 6.24 5.86 2.67 3.56 2.68 3.36 3.17 2.22 mwgr mean 5.89 7.09 4.87 5.67 5.47 5.31 2.07 2.45 2.17 2.51 2.95 2.23 diff mean 0.84 0.96 1.81 0.08 0.76 0.55 0.59 1.11 0.51 0.85 0.22 -0.01 diff std 4.79 6.01 5.24 4.61 4.22 4.87 2.97 4.51 2.03 3.23 3.95 2.08 pval .009 .014 .5e-6 .408 .009 .074 .033 .012 .01 .007 .3 .52 Table 3: Results of combining all 10 classiﬁers using individual FERET data sets. 50 queries of the TREC-2001 ad hoc task, the number of relevant documents that intersect with the union of system results range between 662 and 2664. 7.1 Methods In the information retrieval task the favor function needs to show favor for relevant documents while disregarding other documents. Thus, we can set the favor function similarly to the way it was set in 808 C ONCAVE L EARNERS FOR R ANKBOOST JuruFull 0.759 hum01tdlx 0.760 UniNEn7d 0.763 iit01m 0.762 apl10wd 0.735 jscbtawt14 0.761 csiro0mwa1 0.721 kuadhoc2001 0.727 ﬂabxtd 0.741 ok10wtnd1 0.745 fub01be2 0.734 ricMM 0.765 Table 4: Normalized mean average precision for each constituent. the face recognition classiﬁcation task. Consider a data set with I queries to train on. For each query, i in I, we need to rank all u ∈ U documents. An instance in this case is a pair, x ≡ (i, u). For each u∗ a relevant document and u an irrelevant document for query i, we set Φ ((i, u) , (i, u ∗ )) = +1 and Φ ((i, u∗ ) , (i, u)) = −1, setting all remaining elements of Φ (x0 , x1 ) = 0. Thus, all relevant documents are given a positive favor with respect to irrelevant documents, while all other rankings, including interactions between relevant documents and interactions between irrelevant documents are given zero favor. As the task has only 50 queries, rather than separating the data into train and test, we opted to do a cross validation performance evaluation. We use 5-fold cross validation on the normalized mean average precision measure (Salton and McGill, 1983), a standard IR measure which accounts for the precision (quantity of correct documents retrieved) and their rank position in the list. It is described by the following equation: AveP = ∑N (Prec(r) × rel(r)) r=1 number o f relevant documents where r is the rank, N is the number of documents retrieved, rel() is a binary function of the relevance of the document at rank r,and Prec is the precision ( [number of relevant documents retrieved] / [number of documents retrieved]) at a given cut-off rank. It is clear that higher values of AveP are better. Note that for purposes of normalization we assigned unretrieved relevant documents a constant rank. As the binary and MWGR weak learners had signiﬁcantly different convergence properties on this task (see Figure 4) MWGR was trained for 100 iterations and the binary learner for 300 iterations. As in the FERET experiments, MWGR was optimized using algorithm 3, with a selection pressure of p = 0.5. 7.2 Results As seen in Figure 4, the MWGR learner converges in signiﬁcantly less iterations than the binary learner. This could possibly be attributed to the fact that the MWGR is a more complex function that can incorporate the rankings of multiple classiﬁers in each learner. Also the MWGR function is not tuned to particular rank cutoffs, whereas the binary learner is, so the MWGR can better accommodate the variety in the 1000 ranks being considered. The normalized mean average precision for the MWGR after 100 iterations was 0.8537 and it was 0.8508 for the binary learner after 300 iterations. Compare these results with the precision of the constituents in Table 4. Both weak learners had a performance rate of change of approximately 3 ∗ 10−5 on their ﬁnal iteration (better for MWGR). A paired t-test on the cross validation results of the two learners gives a statistically signiﬁcant p-value of 0.007 in favor of MWGR. 809 M ELNIK , VARDI AND Z HANG Cross Validation Performance 0.88 MWGR Binary 0.87 0.86 0.85 0.84 Mean Avg Precision 0.83 0.82 0.81 0.8 0.79 0.78 0.77 0.76 0.75 0.74 0.73 0.72 0.71 0.7 0 50 100 150 200 Iteration Number 250 300 Figure 4: The cross validation mean average precision score of the two weak learners, MWGR and binary, as a function of boosting iteration number. 8. Discussion The question of how to combine ordinal data has become an active focus of research in machine learning, as applications in pattern recognition, information retrieval and other domains have come to the forefront. A particular question of importance is how can the structure of ranks be correctly exploited to maximize performance. The semi parametric nature of rankboost offers the possibility to generate arbitrarily ﬂexible ranking functions. But as observed this ﬂexibility comes at a cost of signiﬁcant overﬁtting without further regularization. Freund et al. (2003) demonstrate that successful generalization only occurs when the resulting ranking functions are constrained to be monotonic. This constraint can be thought of as a regularization that incorporates prior knowledge on the interpretation of ranks and as such, how they can be combined. We present a regularization framework based on the concept of consistency in conﬁdence and preference. Ranking functions with this property show a consistency in how they treat the preference and relative conﬁdence exhibited by constituents. We prove that under a natural interpretation of preference and conﬁdence for ranks, this consistency property of the combiner is equivalent to monotonicity and concavity of its ranking function. We enhance rankboost by designing a weak ranking learner that exhibits consistency in preference and conﬁdence. A computational advantage of this weak learner, called minimum weighted group ranks (MWGR) is that its parameters can be individually optimized readily with respect to the rankboost criteria, allowing it to be tested on real-world data. In our ﬁrst experiments we compare the original rankboost binary weak learner with MWGR on a task of combining the output of multiple face recognition algorithms from the FERET study. We conducted experiments on homogeneous data, testing the intrinsic efﬁciency of the MWGR proce810 C ONCAVE L EARNERS FOR R ANKBOOST dure. We also conducted experiments on heterogeneous data, testing the robustness or adaptivity of the procedure. In almost all cases we see that MWGR shows improved performance compared with the binary weak learner, whether combining three or all of the face recognizers, conﬁrming the utility of this monotonic and concave learner. Our second experiment was on an Information Retrieval task taken from the TREC conference. In this task we see MWGR converges in signiﬁcantly less iterations and generates statistically signiﬁcant improved performance. Final Words Ofer Melnik and Cun-Hui Zhang are very saddened that our colleague and friend Yehuda Vardi passed away before he could give this paper his ﬁnal stamp of approval. He was very enthusiastic about this research and would have been very pleased to see it come to fruition. Acknowledgments This research is partially supported by NSF Grants DMS 04-05202, DMS 05-04387 and, DMS 0604571, ONR Grant N00014-02-1-056 and NSA Grant H98230-04-1-0041. Ofer Melnik would also like to thank DIMACS for their support in this research. The authors are also grateful to the editor and reviewers for their constructive suggestions which improved the presentation of the results. Appendix A. In this paper we showed how three-point consistency in preference and conﬁdence implies concave and monotonic ranking functions. For two decision problems involving two pairs of rank vectors, the four-point consistency property implies the following constraints for a ranking function   G(y) < G(y )   yB − y B ≤ 0 < y A − y A  zA ≤ yA , yA − yA ≤ zA − zA ⇒ G(z ) − G(z) > G(y ) − G(y)   yB ≤ z B , zB − z B ≤ y B − y B (11) where y and y are the rank vectors from the ﬁrst decision problem and z and z are the rank vectors from the second decision problem. Unlike the three-point case, the four-point consistency property does not imply a clearly recognizable functional form for the ranking function. What we can say about it though is that as the constraints are linear, in the same way that concavity and monotonicity in the weak learner conferred the same properties to the ranking function, a weak learner that satisﬁes Eq. 11 will also confer those properties to the ranking function that uses it with positive weights. References K. Arrow. Social Choice and Individual Values. Wiley, 1951. C. Dwork, R. Kumar, M. Naor, and D. Sivakumar. Rank aggregation methods for the web. In Proc. 10th Intl. World Wide Web Conf., pages 613–622, 2001. 811 M ELNIK , VARDI AND Z HANG Y. Freund, R. Iyer, R.E. Schapire, and Y. Singer. An efﬁcient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933–969, 2003. Y. Freund and R.E. Schapire. Experiments with a new boosting algorithm. In Proceedings of the Thirteenth International Conference on Machine Learning, pages 148–156, 1996. T. K. Ho. A Theory of Multiple Classiﬁer Systems and Its Application to Visual Word Recognition. PhD thesis, State University of New York at Buffalo, May 1992. T. K. Ho, J. J. Hull, and S. N. Srihari. Combination of decisions by multiple classiﬁers. In H. S. Baird, H. Bunke, and K. Yamamoto (Eds.), editors, Structured Document Image Analysis, pages 188–202. Springer-Verlag, Heidelberg, 1992. J. Kittler and F. Roli, editors. Multiple Classiﬁer Systems, Lecture Notes in Computer Science 1857, 2000. Springer. R. Meir and G. Ratsch. Advanced Lectures in Machine Learning, Lecture Notes in Computer Science 2600, chapter An introduction to boosting and leveraging, pages 119–184. Springer, 2003. O. Melnik, Y. Vardi, and C-H. Zhang. Mixed group ranks: Preference and conﬁdence in classiﬁer combination. IEEE Pattern Analysis and Machine Intelligence, 26(8):973–981, 2004. H. Moon and P.J. Phillips. Computational and performance aspects of PCA-based face-recognition algorithms. Perception, 30:303–321, 2001. P.J. Phillips, H. Moon, S.A. Rizvi, and P.J. Rauss. The FERET evaluation methodology for facerecognition algorithms. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22:1090– 1104, 2000. M.E. Renda and U. Straccia. Web metasearch: Rank vs. score based rank aggregation methods. In 18th Annual ACM Symposium on Applied Computing (SAC-03), pages 841–846, Melbourne, Florida, USA, 2003. ACM. G. Salton and J.M. McGill. Introduction to Modern Information Retrieval. Addison Wesley Publ. Co., 1983. E.M. Voorhees and D.K. Harman, editors. NIST Special Publication 500-250: The Tenth Text REtrieval Conference (TREC 2001), number SN003-003-03750-8, 2001. Department of Commerce, National Institute of Standards and Technology, Government Printing Ofﬁce. URL http://trec.nist.gov. L. Wiskott, J.-M. Fellous, N. Kruger, and C. von der Malsburg. Face recognition by elastic bunch graph matching. IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(7):775– 779, 1997. W. Zhao, A. Krishnaswamy, R. Chellappa, D. Swets, and J. Weng. Face Recognition: From Theory to Applications, chapter Discriminant Analysis of Principal Components, pages 73–86. SpringerVerlag, Berlin, 1998. 812</p><p>4 0.37640145 <a title="4-lsi-4" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>Author: Art B. Owen</p><p>Abstract: In binary classiﬁcation problems it is common for the two classes to be imbalanced: one case is very rare compared to the other. In this paper we consider the inﬁnitely imbalanced case where one class has a ﬁnite sample size and the other class’s sample size grows without bound. For logistic regression, the inﬁnitely imbalanced case often has a useful solution. Under mild conditions, the intercept diverges as expected, but the rest of the coefﬁcient vector approaches a non trivial and useful limit. That limit can be expressed in terms of exponential tilting and is the minimum of a convex objective function. The limiting form of logistic regression suggests a computational shortcut for fraud detection problems. Keywords: classiﬁcation, drug discovery, fraud detection, rare events, unbalanced data</p><p>5 0.32409707 <a title="4-lsi-5" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>Author: Jaime S. Cardoso, Joaquim F. Pinto da Costa</p><p>Abstract: Classiﬁcation of ordinal data is one of the most important tasks of relation learning. This paper introduces a new machine learning paradigm speciﬁcally intended for classiﬁcation problems where the classes have a natural order. The technique reduces the problem of classifying ordered classes to the standard two-class problem. The introduced method is then mapped into support vector machines and neural networks. Generalization bounds of the proposed ordinal classiﬁer are also provided. An experimental study with artiﬁcial and real data sets, including an application to gene expression analysis, veriﬁes the usefulness of the proposed approach. Keywords: classiﬁcation, ordinal data, support vector machines, neural networks</p><p>6 0.28057447 <a title="4-lsi-6" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>7 0.24129382 <a title="4-lsi-7" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>8 0.22099353 <a title="4-lsi-8" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>9 0.21901861 <a title="4-lsi-9" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>10 0.21253714 <a title="4-lsi-10" href="./jmlr-2007-Distances_between_Data_Sets_Based_on_Summary_Statistics.html">27 jmlr-2007-Distances between Data Sets Based on Summary Statistics</a></p>
<p>11 0.20649181 <a title="4-lsi-11" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>12 0.20447117 <a title="4-lsi-12" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>13 0.1962014 <a title="4-lsi-13" href="./jmlr-2007-Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning.html">85 jmlr-2007-Transfer Learning via Inter-Task Mappings for Temporal Difference Learning</a></p>
<p>14 0.18429442 <a title="4-lsi-14" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>15 0.17546649 <a title="4-lsi-15" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>16 0.16865095 <a title="4-lsi-16" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>17 0.16484325 <a title="4-lsi-17" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>18 0.16467403 <a title="4-lsi-18" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>19 0.15753883 <a title="4-lsi-19" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>20 0.15603854 <a title="4-lsi-20" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.014), (10, 0.014), (12, 0.011), (28, 0.037), (40, 0.038), (45, 0.03), (48, 0.605), (60, 0.017), (85, 0.027), (98, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93544954 <a title="4-lda-1" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>Author: Carine Hue, Marc Boullé</p><p>Abstract: In this paper, we consider the supervised learning task which consists in predicting the normalized rank of a numerical variable. We introduce a novel probabilistic approach to estimate the posterior distribution of the target rank conditionally to the predictors. We turn this learning task into a model selection problem. For that, we deﬁne a 2D partitioning family obtained by discretizing numerical variables and grouping categorical ones and we derive an analytical criterion to select the partition with the highest posterior probability. We show how these partitions can be used to build univariate predictors and multivariate ones under a naive Bayes assumption. We also propose a new evaluation criterion for probabilistic rank estimators. Based on the logarithmic score, we show that such criterion presents the advantage to be minored, which is not the case of the logarithmic score computed for probabilistic value estimator. A ﬁrst set of experimentations on synthetic data shows the good properties of the proposed criterion and of our partitioning approach. A second set of experimentations on real data shows competitive performance of the univariate and selective naive Bayes rank estimators projected on the value range compared to methods submitted to a recent challenge on probabilistic metric regression tasks. Our approach is applicable for all regression problems with categorical or numerical predictors. It is particularly interesting for those with a high number of predictors as it automatically detects the variables which contain predictive information. It builds pertinent predictors of the normalized rank of the numerical target from one or several predictors. As the criteria selection is regularized by the presence of a prior and a posterior term, it does not suffer from overﬁtting. Keywords: rank regression, probabilistic approach, 2D partitioning, non parametric estimation, Bayesian model selection</p><p>2 0.92800212 <a title="4-lda-2" href="./jmlr-2007-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">80 jmlr-2007-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>Author: Margarita Osadchy, Yann Le Cun, Matthew L. Miller</p><p>Abstract: We describe a novel method for simultaneously detecting faces and estimating their pose in real time. The method employs a convolutional network to map images of faces to points on a lowdimensional manifold parametrized by pose, and images of non-faces to points far away from that manifold. Given an image, detecting a face and estimating its pose is viewed as minimizing an energy function with respect to the face/non-face binary variable and the continuous pose parameters. The system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones. The system is designed to handle very large range of poses without retraining. The performance of the system was tested on three standard data sets—for frontal views, rotated faces, and proﬁles— is comparable to previous systems that are designed to handle a single one of these data sets. We show that a system trained simuiltaneously for detection and pose estimation is more accurate on both tasks than similar systems trained for each task separately.1 Keywords: face detection, pose estimation, convolutional networks, energy based models, object recognition</p><p>3 0.80300027 <a title="4-lda-3" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>Author: Masashi Sugiyama</p><p>Abstract: Reducing the dimensionality of data without losing intrinsic information is an important preprocessing step in high-dimensional data analysis. Fisher discriminant analysis (FDA) is a traditional technique for supervised dimensionality reduction, but it tends to give undesired results if samples in a class are multimodal. An unsupervised dimensionality reduction method called localitypreserving projection (LPP) can work well with multimodal data due to its locality preserving property. However, since LPP does not take the label information into account, it is not necessarily useful in supervised learning scenarios. In this paper, we propose a new linear supervised dimensionality reduction method called local Fisher discriminant analysis (LFDA), which effectively combines the ideas of FDA and LPP. LFDA has an analytic form of the embedding transformation and the solution can be easily computed just by solving a generalized eigenvalue problem. We demonstrate the practical usefulness and high scalability of the LFDA method in data visualization and classiﬁcation tasks through extensive simulation studies. We also show that LFDA can be extended to non-linear dimensionality reduction scenarios by applying the kernel trick. Keywords: dimensionality reduction, supervised learning, Fisher discriminant analysis, locality preserving projection, afﬁnity matrix</p><p>4 0.40133807 <a title="4-lda-4" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>5 0.39195696 <a title="4-lda-5" href="./jmlr-2007-Local_Discriminant_Wavelet_Packet_Coordinates_for_Face_Recognition.html">50 jmlr-2007-Local Discriminant Wavelet Packet Coordinates for Face Recognition</a></p>
<p>Author: Chao-Chun Liu, Dao-Qing Dai, Hong Yan</p><p>Abstract: Face recognition is a challenging problem due to variations in pose, illumination, and expression. Techniques that can provide effective feature representation with enhanced discriminability are crucial. Wavelets have played an important role in image processing for its ability to capture localized spatial-frequency information of images. In this paper, we propose a novel local discriminant coordinates method based on wavelet packet for face recognition to compensate for these variations. Traditional wavelet-based methods for face recognition select or operate on the most discriminant subband, and neglect the scattered characteristic of discriminant features. The proposed method selects the most discriminant coordinates uniformly from all spatial frequency subbands to overcome the deﬁciency of traditional wavelet-based methods. To measure the discriminability of coordinates, a new dilation invariant entropy and a maximum a posterior logistic model are put forward. Moreover, a new triangle square ratio criterion is used to improve classiﬁcation using the Euclidean distance and the cosine criterion. Experimental results show that the proposed method is robust for face recognition under variations in illumination, pose and expression. Keywords: local discriminant coordinates, invariant entropy, logistic model, wavelet packet, face recognition, illumination, pose and expression variations</p><p>6 0.39056998 <a title="4-lda-6" href="./jmlr-2007-Handling_Missing_Values_when_Applying_Classification_Models.html">39 jmlr-2007-Handling Missing Values when Applying Classification Models</a></p>
<p>7 0.37518409 <a title="4-lda-7" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>8 0.36147889 <a title="4-lda-8" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>9 0.36003008 <a title="4-lda-9" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>10 0.35862884 <a title="4-lda-10" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>11 0.34912434 <a title="4-lda-11" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>12 0.33733106 <a title="4-lda-12" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>13 0.33485568 <a title="4-lda-13" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>14 0.32934794 <a title="4-lda-14" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>15 0.32903248 <a title="4-lda-15" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>16 0.3279824 <a title="4-lda-16" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>17 0.32546085 <a title="4-lda-17" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>18 0.32023606 <a title="4-lda-18" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>19 0.31939226 <a title="4-lda-19" href="./jmlr-2007-Truncating_the_Loop_Series_Expansion_for_Belief_Propagation.html">86 jmlr-2007-Truncating the Loop Series Expansion for Belief Propagation</a></p>
<p>20 0.31559786 <a title="4-lda-20" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
