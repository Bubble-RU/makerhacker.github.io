<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-73" href="#">jmlr2007-73</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</h1>
<br/><p>Source: <a title="jmlr-2007-73-pdf" href="http://jmlr.org/papers/volume8/hussain07a/hussain07a.pdf">pdf</a></p><p>Author: Zakria Hussain, François Laviolette, Mario Marchand, John Shawe-Taylor, Spencer Charles Brubaker, Matthew D. Mullin</p><p>Abstract: Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classiﬁer achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classiﬁer achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassiﬁcations. Keywords: set covering machines, sample-compression, loss bounds</p><p>Reference: <a title="jmlr-2007-73-reference" href="../jmlr2007_reference/jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A generalization error bound is an upper bound on the expected test set performance that holds with high probability over the (random) choice of the training set. [sent-31, score-0.28]
</p><p>2 The classiﬁer output by the SCM is described by a small subset of the training data called the compression set. [sent-39, score-0.202]
</p><p>3 , the number of examples) of the compression set of the SCM classiﬁer. [sent-42, score-0.181]
</p><p>4 None of these loss bounds, however, depend on the observed fraction of positive examples in the training set and on the fraction of positive examples used for the compression set of the ﬁnal classiﬁer. [sent-44, score-0.391]
</p><p>5 We then propose, in Section 4, a loss bound which is valid for any sample-compression learning algorithm (including the SCM) and that depends on the observed fraction of positive examples and on what the classiﬁer achieves on the positive training examples. [sent-48, score-0.28]
</p><p>6 The proof of this new loss bound turns out to be much more involved than all other sample-compression loss bounds that do not depend on the observed fraction of positive examples (as in Marchand and Sokolova, 2005). [sent-49, score-0.325]
</p><p>7 The novelty of this paper is that we correct a bound that was found to be wrong, but in doing so, we derive a more general form that allows any learning algorithm, relying on sample compression schemes, to be upper bounded. [sent-51, score-0.318]
</p><p>8 Separately bounding the positive and negative errors also gives rise to a natural extension—namely asymmetric loss of sample compression risk bounds. [sent-52, score-0.283]
</p><p>9 To determine the radius ρ of ball hi,ρ , we will use another training example x j , called the ball border of hi,ρ , such that ρ = d(x j , xi ). [sent-76, score-0.212]
</p><p>10 The union of these two sets gives the compression set of the SCM. [sent-78, score-0.181]
</p><p>11 Following Littlestone and Warmuth (1986) and Floyd and Warmuth (1995), the compression set is a small subset of the 2535  H USSAIN , L AVIOLETTE , M ARCHAND , S HAWE -TAYLOR , B RUBAKER AND M ULLIN  training set which identiﬁes a classiﬁer (here a SCM). [sent-79, score-0.202]
</p><p>12 The function that maps arbitrary compression sets to classiﬁers is called the reconstruction function. [sent-80, score-0.206]
</p><p>13 Hence, the error probability of classiﬁer f on P -examples and on N -examples, that we call respectively the expected P -loss and the expected N -loss, are given by def  erP ( f ) = P{ f (x) = y (x, y) ∈ P } , def  erN ( f ) = P{ f (x) = y (x, y) ∈ N } . [sent-85, score-0.34]
</p><p>14 Hence def  ˆ erP ( f , S) = |{(x, y) ∈ SP : f (x) = y}| , def  ˆ erN ( f , S) =  {(x, y) ∈ SN : f (x) = y} . [sent-87, score-0.34]
</p><p>15 We will consider the general case where the loss lP of misclassifying a positive example can differ from the loss l N of misclassifying a negative example. [sent-90, score-0.173]
</p><p>16 In this case, the expected loss E[l(A(S))] of classiﬁer A(S) is deﬁned as def  E[l(A(S))] = lP · pP · erP [A(S)] + lN · pN · erN [A(S)] . [sent-92, score-0.209]
</p><p>17 Incorrect Bound The Theorem 5 of Marchand and Shawe-Taylor (2002) gives the following loss bound for the SCM with the symmetric loss case of lP = lN = 1. [sent-94, score-0.2]
</p><p>18 Given the above deﬁnitions, let A be any learning algorithm that builds a SCM with datadependent balls with the constraint that the returned function A(S) always correctly classiﬁes every example in the compression set. [sent-95, score-0.181]
</p><p>19 Similarly, c p and cn are the number of positive and negative ball centers contained in classiﬁer A(S) whereas b denotes the number of ball borders 1 in classiﬁer A(S). [sent-97, score-0.272]
</p><p>20 Finally m p and mn denote the number of positive and negative examples in training set S. [sent-98, score-0.315]
</p><p>21 Let us take the B expression only and look more closely at the number of ways of choosing the errors on SP and SN : m p − c p − b mn − c n . [sent-99, score-0.236]
</p><p>22 kp kn The bound on the expected loss given above will be small only if each factor is small. [sent-100, score-0.311]
</p><p>23 In particular, the product of these two factors will be small for a small value of kn (say, kn = 0) and a large value of k p (say, k p = m p − c p − b). [sent-102, score-0.17]
</p><p>24 In this case, the denominator of the bound given above will become m − c p − b − c n − k p − kn = mn − cn , and will be large whenever mn cn . [sent-103, score-0.717]
</p><p>25 Consequently, the bound given by Theorem 5 of Marchand and Shawe-Taylor (2002) will be small for classiﬁers having a small compression set and making a large number of errors on SP and a small number of errors on SN . [sent-104, score-0.335]
</p><p>26 In order to derive a loss bound where the issue of imbalanced misclassiﬁcations can be handled, the errors for positive and negative examples must be bounded separately. [sent-106, score-0.3]
</p><p>27 This equality is tantamount to writing that for any ﬁxed classiﬁer f: ˆ P S ∈ X : er( f , S) = 0 |SP | = m p = (1 − erP ( f ))m p (1 − erN ( f ))mn ×  m m pP p (1 − pP )mn mp  (false) ,  where pP denotes the probability of occurrence of a P -example. [sent-108, score-0.581]
</p><p>28 More formally, this means that there exists a reconstruction function Φ that produces a classiﬁer f = Φ(Λ, σ) when given an arbitrary compression set Λ and message string σ. [sent-118, score-0.307]
</p><p>29 We can thus consider that the learning algorithm A, trained on S, returns a compression set Λ(S) and a message string σ(S). [sent-119, score-0.282]
</p><p>30 For any training sample S and compression set Λ, consisting of a subset Λ P of positive examples and a subset ΛN of negative examples, we use the notation Λ(S) = (ΛP (S), ΛN (S)). [sent-121, score-0.276]
</p><p>31 Any further partitioning of the compression set Λ can be performed by the message string σ. [sent-122, score-0.282]
</p><p>32 For example, in the set covering machine, σ speciﬁes for each point in ΛP , whether it is a ball center or a ball border (not already used as a center). [sent-123, score-0.235]
</p><p>33 As explained by Marchand and Shawe-Taylor (2002), this is the only additional information required to obtain a SCM consistent with the compression set. [sent-124, score-0.181]
</p><p>34 (4)  mN (S, A(S)) =  Hence, the predicate mP (S, A(S)) = mP means that |S| = m, |SP | = m p , |SN | = mn , |ΛP (S)| = d p , ˆ |ΛN (S)| = dn , erP (A(S), S) = k p . [sent-128, score-0.304]
</p><p>35 We will also use BP (mP ) and BN (mN ) deﬁned as def  mp dp  mn dn  mp − dp , kp  def  mp dp  mn dn  mn − d n . [sent-130, score-3.106]
</p><p>36 kn  BP (mP ) = BN (mN ) =  The proposed loss bound will hold uniformly for all possible messages that can be chosen by A. [sent-131, score-0.275]
</p><p>37 To obtain a smaller loss bound, we will therefore permit M to be dependent on the compression set chosen by A. [sent-133, score-0.22]
</p><p>38 In fact, the loss bound will depend on a prior distribution PΛ (σ) of message strings over the set MΛ of possible messages that can be used with a compression set Λ. [sent-134, score-0.453]
</p><p>39 Given a compression set Λ = (ΛP , ΛN ) of size (|ΛP |, |ΛN |) = (d p , dn ), recall that each example in ΛN is a ball center whereas each example in ΛP can either be a ball border or a ball center. [sent-137, score-0.511]
</p><p>40 Consequently, if b(σ) denotes the number of ball borders speciﬁed by message string σ, we can choose PΛ (σ) = ζ(b(σ)) ·  dp b(σ)  −1  (SCM case) ,  (5)  where, for any non-negative integer b, we deﬁne def  ζ(b) =  6 (b + 1)−2 . [sent-149, score-0.426]
</p><p>41 The proposed loss bound will make use of the following functions: def  1 1 ln (BP (mP )) + ln mp − dp − kp β  def  1 1 ln BN (mN ) + ln mn − d n − k n β  εP (mP , β) = 1 − exp − εN (mN , β) = 1 − exp −  ,  (7)  . [sent-151, score-1.929]
</p><p>42 (8)  Theorem 2 Given the above deﬁnitions, let A be any learning algorithm having a reconstruction function that maps compression sets and message strings to classiﬁers. [sent-152, score-0.288]
</p><p>43 The summation over m P stands for m  m p m−m p m p −d p  ∑(·) = ∑ ∑ ∑ ∑ def  mP  (·) . [sent-156, score-0.17]
</p><p>44 m p =0 d p =0 dn =0 k p =0  Note that the summation over k p stops at m p − d p because, as we will see later in the proof, we can upper bound the risk of a sample-compressed classiﬁer only from the training errors it makes on the examples that are not used for the compression set. [sent-157, score-0.45]
</p><p>45 For S ∈ X , we deﬁne Si as def  Si =((xi1 , yi1 ), . [sent-165, score-0.17]
</p><p>46 Therefore, given a training set S and vectors i p and in , the subset Si p ,in will denote a compression set. [sent-172, score-0.202]
</p><p>47 Let def  P =P S ∈ X : ∃σ ∈ MSi p ,in : erP [Φ(Si p ,in , σ)] ≥ ε mP , Si p ,in , σ , mP (S, A(S)) = mP  . [sent-178, score-0.17]
</p><p>48 P  ≤  m m pP p (1 − pP )m−m p sup P S ∈ X : ∃σ ∈ MSi p ,in : mp b∈Bm p erP [Φ(Si p ,in , σ)] ≥ ε mP , Si p ,in , σ , mP (S, A(S)) = mP | b(S) = b . [sent-182, score-0.581]
</p><p>49 Consequently, under this condition, we can compute the above probability by ﬁrst conditioning on the compression set Si p ,in and then performing the expectation over Si p ,in . [sent-184, score-0.181]
</p><p>50 We will now stratify this last probability by the set of possible errors that classiﬁer Φ(S i p ,in , σ) can perform on the training examples that are not in the compression set S i p ,in . [sent-187, score-0.245]
</p><p>51 Now, under the condition b(S) = b with a ﬁxed compression set Si p ,in , this last probability is obtained for the random draws of the training examples that are not in Si p ,in . [sent-194, score-0.263]
</p><p>52 Consequently, this last probability is at most equal to the probability that a ﬁxed classiﬁer, having erP ≥ ε(mP , Si p ,in , σ), makes no errors on m p − d p − k p positive examples that are not in the compression set Si p ,in . [sent-195, score-0.247]
</p><p>53 Doing so yields the following bounds: 1 1 ln (BP (mP )) + ln mp − dp − kp gP (S)δ  ≥ 1 − δ,  1 1 P S ∈ X : erN [A(S)] ≤ ln BN (mN ) + ln mn − d n − k n gN (S)δ  ≥ 1−δ. [sent-205, score-1.428]
</p><p>54 Now that we have a bound on both erP [A(S)] and erN [A(S)], to bound the expected loss E[l(A(S))] of Equation 1 we now need to upper bound the probabilities p P and pN . [sent-209, score-0.42]
</p><p>55 In order to obtain a tight loss bound for both balanced and imbalanced data sets, we have decided to use the binomial distribution without any approximation. [sent-213, score-0.229]
</p><p>56 Recall that the probability Bin(m, k, p) of having at most k successes among m Bernoulli trials, each having probability of success p, is given by the binomial tail def  Bin(m, k, p) =  k  ∑  i=0  m i p (1 − p)m−i . [sent-214, score-0.203]
</p><p>57 In other words, def  Bin (m, k, δ) = sup p : Bin (m, k, p) ≥ δ . [sent-216, score-0.17]
</p><p>58 2543  (11)  H USSAIN , L AVIOLETTE , M ARCHAND , S HAWE -TAYLOR , B RUBAKER AND M ULLIN  From this deﬁnition, it follows that Bin (m, mn , δ) is the smallest upper bound on pN , which holds with probability at least 1 − δ, over the random draws of m examples. [sent-217, score-0.391]
</p><p>59 Theorem 4 Given the above deﬁnitions, let A be any learning algorithm having a reconstruction function that maps compression sets and message strings to classiﬁers. [sent-221, score-0.288]
</p><p>60 With probability 1 − δ over the random draws of a training set S, we have E[l(A(S))] ≤ lP · Bin m, m p ,  δ δ · εP mP , gP (S) 4 4 + lN · Bin m, mn ,  δ δ · εN mN , gN (S) , 4 4  where mP = mP (S, A(S)) and mN = mN (S, A(S)) are deﬁned by Equations 3 and 4. [sent-222, score-0.275]
</p><p>61 Consider the def def frequencies pP =m p /m and pN =mn /m. [sent-224, score-0.34]
</p><p>62 2544  S AMPLE -C OMPRESSION L OSS B OUNDS FOR I MBALANCED DATA  Theorem 5 Given the above deﬁnitions, let A be any learning algorithm having a reconstruction function that maps compression sets and message strings to classiﬁers. [sent-231, score-0.288]
</p><p>63 For any real numbers a, b, c, let a · |c| if c ≥ 0 def Ψ(a; b; c) = b · |c| if c ≤ 0 . [sent-232, score-0.17]
</p><p>64 To compare the bound given by Theorem 5 with the bound given by Theorem 4, let us assume that lN εN ≥ lP εP . [sent-234, score-0.244]
</p><p>65 The bound of Theorem 4 minus the bound of Theorem 5 then gives (lP pP εP + lN pN εN ) − (lP pP εP + lN pN εN + (pN − pN )(lN εN − lP εP )) ˆ ˆ ˆ ˆ ˆ ˆ = (pP − pP )lP εP + (pN − pN )lN εN − (pN − pN )(lN εN − lP εP ) = (pP − pP + pN − pN )lP εP ˆ ˆ = (pP + pN − 1)lP εP . [sent-237, score-0.244]
</p><p>66 Hence, the bound of Theorem 5 can be signiﬁcantly better than the the bound of Theorem 4. [sent-245, score-0.244]
</p><p>67 Furthermore, the risk bound on one class depends on what the classiﬁer achieves on the training 2545  H USSAIN , L AVIOLETTE , M ARCHAND , S HAWE -TAYLOR , B RUBAKER AND M ULLIN  examples of that class. [sent-252, score-0.17]
</p><p>68 Thus, making the bound more data-dependent then the usual bounds on the true error. [sent-253, score-0.172]
</p><p>69 A small compression scheme is evidence of simplicity in the structure of the classiﬁer, but one that is related to the training distribution rather than a priori determined. [sent-257, score-0.202]
</p><p>70 Any algorithm that uses a compression scheme can use the bounds that we have proposed and take advantage of asymmetrical loss and cases of imbalanced data sets. [sent-258, score-0.347]
</p><p>71 In order to show the merits of our bound we must now compare numerically against more common sample compression bounds and the bound found to be incorrect. [sent-264, score-0.475]
</p><p>72 All the compared bounds are specialized to the set covering machine compression scheme that uses data-dependent balls. [sent-266, score-0.275]
</p><p>73 Here each ball is constructed from two data points—one that deﬁnes the center of the ball and another that helps deﬁne the radius of the ball (known as the border point). [sent-267, score-0.262]
</p><p>74 Hence to build a classiﬁer from the compression set, we also need an informative message string to discriminate between the border points and the centers. [sent-268, score-0.331]
</p><p>75 The ﬁrst bound we compare against is taken from the original set covering machine paper by Marchand and Shawe-Taylor (2001) and is similar to the Littlestone and Warmuth (1986) bound but with more specialization for the SCM compression set deﬁned from the set of data-dependent balls. [sent-273, score-0.469]
</p><p>76 All these bounds will also be compared against the incorrect bound given in Marchand and Shawe-Taylor (2002). [sent-275, score-0.219]
</p><p>77 Please note that traditional sample compression bounds, such as that given by Theorem 6. [sent-276, score-0.181]
</p><p>78 This implies the need for side information to discriminate between centers and border points, something that traditional sample compression bounds do not cater for. [sent-279, score-0.298]
</p><p>79 All generalization error bounds detailed below will make use of the following deﬁnitions: d n = cn , d p = c p + b, d = d p + dn and k = k p + kn . [sent-281, score-0.238]
</p><p>80 • incorrect bound (Theorem 5 of Marchand and Shawe-Taylor, 2002). [sent-285, score-0.169]
</p><p>81 2 of Marchand and Shawe-Taylor, 2001): ε(m, d, c p , k, δ) = 1 − exp  −1 m 2d m − 2d ln + ln + ln + m − 2d − k 2d cp k ln  2m2 d δ  . [sent-288, score-0.534]
</p><p>82 • MS05 bound (Equation 10 of Marchand and Sokolova, 2005): ε(m, d, d p , b, k, δ) = 1 − exp  −1 m m−d dp ln + ln + ln + m−d −k d k b 1 , ln ζ(d)ζ(k)ζ(b)δ  where ζ(a) is given by Equation 6. [sent-289, score-0.684]
</p><p>83 2 Discussion of Results The numerical comparisons of these four bounds (new bound, incorrect bound, MS01 bound and MS05 bound) are shown in Figure 1 and Figure 2. [sent-291, score-0.219]
</p><p>84 Each plot contains the number of positive examples m p , the number of negative examples mn , the number of positive centers c p , the number of negative centers cn and the number of borders b. [sent-292, score-0.469]
</p><p>85 Finally, the empirical error was also included in each plot—which is simply the number of examples misclassiﬁed divided by the number of examples, that is, (k p + kn )/(m p + mn ). [sent-297, score-0.332]
</p><p>86 We clearly see that the incorrect bound becomes erroneous when the number k p of errors on the positive training examples approaches the total number m p of positive training examples. [sent-299, score-0.314]
</p><p>87 We also see that the new bound is tighter than the MS01 and MS05 bounds when the k p differs greatly from kn . [sent-300, score-0.279]
</p><p>88 However, the latter bound is slightly tighter than the new bound when k p = kn . [sent-301, score-0.351]
</p><p>89 Indeed, the MS01 and MS05 loss bounds are slightly smaller than the new bound when k p /m p is similar to kn /mn , but the new bound becomes smaller when these two quantities greatly differ. [sent-304, score-0.418]
</p><p>90 As we would expect, the new bound is smaller when one class of examples is more abundant than the other. [sent-306, score-0.165]
</p><p>91 9 new bound incorrect bound MS05 bound MS01 bound empirical error  0. [sent-309, score-0.535]
</p><p>92 2  0  0  500 1000 1500 number of positive examples misclassified (kp > 0, kn = 0)  0. [sent-323, score-0.173]
</p><p>93 1  2000  new bound incorrect bound MS05 bound MS01 bound empirical error 0  500 1000 1500 2000 number of positive examples misclassified (kp > 0, kn = 500)  Figure 1: Bound values for the SCM when m p = 2020, mn = 1980, c p = 5, cn = 5, b = 10. [sent-324, score-0.963]
</p><p>94 7 new bound incorrect bound MS05 bound MS01 bound empirical error  0. [sent-327, score-0.535]
</p><p>95 4  new bound incorrect bound MS05 bound MS01 bound empirical error  0. [sent-338, score-0.535]
</p><p>96 1  0  200 400 600 800 number of positive examples misclassified (kp > 0, kn = 0)  0  1000  0  200 400 600 800 1000 number of positive examples misclassified (kp > 0, kn = 500)  Figure 2: Bound values for the SCM when m p = 1000, mn = 3000, c p = 5, cn = 5, b = 10. [sent-339, score-0.601]
</p><p>97 Conclusion We have observed that the SCM loss bound proposed by Marchand and Shawe-Taylor (2002) is incorrect and, in fact, becomes erroneous in the limit where the number of errors on the positive training examples approaches the total number of positive training examples. [sent-341, score-0.353]
</p><p>98 This new bound captures the spirit of Marchand and Shawe-Taylor (2002) with very similar tightness in the regimes in which the bound could hold. [sent-343, score-0.264]
</p><p>99 This is shown in numerical comparisons of the loss bound proposed in this paper with all of the earlier bounds that can be applied to the SCM. [sent-344, score-0.211]
</p><p>100 As mentioned above, an advantage of the bound is its ability to take into account the observed number of positive examples in the training set in order to arrive at tighter estimates. [sent-345, score-0.215]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mp', 0.581), ('pp', 0.302), ('erp', 0.292), ('mn', 0.22), ('scm', 0.215), ('lp', 0.196), ('marchand', 0.193), ('compression', 0.181), ('def', 0.17), ('si', 0.142), ('pn', 0.14), ('ln', 0.127), ('sp', 0.124), ('bound', 0.122), ('ern', 0.104), ('msi', 0.088), ('kn', 0.085), ('hawe', 0.075), ('rubaker', 0.075), ('ullin', 0.075), ('ussain', 0.075), ('archand', 0.072), ('ball', 0.071), ('dn', 0.068), ('kp', 0.065), ('mbalanced', 0.064), ('aviolette', 0.064), ('bin', 0.06), ('message', 0.06), ('ompression', 0.057), ('sn', 0.056), ('dp', 0.054), ('ounds', 0.052), ('oss', 0.052), ('bounds', 0.05), ('gp', 0.05), ('im', 0.049), ('imbalanced', 0.049), ('border', 0.049), ('sokolova', 0.048), ('imn', 0.047), ('incorrect', 0.047), ('covering', 0.044), ('er', 0.043), ('hi', 0.042), ('ample', 0.041), ('string', 0.041), ('mario', 0.04), ('loss', 0.039), ('misclassified', 0.038), ('gn', 0.035), ('cn', 0.035), ('draws', 0.034), ('bm', 0.032), ('misclassi', 0.032), ('borders', 0.03), ('messages', 0.029), ('bp', 0.028), ('asymmetrical', 0.028), ('brubaker', 0.028), ('examples', 0.027), ('imbalance', 0.026), ('cp', 0.026), ('reconstruction', 0.025), ('theorem', 0.025), ('fraction', 0.025), ('negative', 0.024), ('misclassifying', 0.024), ('hussain', 0.024), ('esi', 0.024), ('francois', 0.024), ('laviolette', 0.024), ('positive', 0.023), ('tighter', 0.022), ('strings', 0.022), ('littlestone', 0.021), ('training', 0.021), ('tightness', 0.02), ('binomial', 0.019), ('zakria', 0.019), ('warmuth', 0.018), ('centers', 0.018), ('classi', 0.018), ('nitions', 0.018), ('conjunction', 0.018), ('predicate', 0.016), ('abundant', 0.016), ('gatech', 0.016), ('ucl', 0.016), ('reasearch', 0.016), ('ulaval', 0.016), ('errors', 0.016), ('bn', 0.016), ('consequently', 0.015), ('upper', 0.015), ('georgia', 0.014), ('erroneous', 0.014), ('spencer', 0.014), ('successes', 0.014), ('floyd', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="73-tfidf-1" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>Author: Zakria Hussain, François Laviolette, Mario Marchand, John Shawe-Taylor, Spencer Charles Brubaker, Matthew D. Mullin</p><p>Abstract: Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classiﬁer achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classiﬁer achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassiﬁcations. Keywords: set covering machines, sample-compression, loss bounds</p><p>2 0.21948202 <a title="73-tfidf-2" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>Author: François Laviolette, Mario Marchand</p><p>Abstract: We propose a PAC-Bayes theorem for the sample-compression setting where each classiﬁer is described by a compression subset of the training data and a message string of additional information. This setting, which is the appropriate one to describe many learning algorithms, strictly generalizes the usual data-independent setting where classiﬁers are represented only by data-independent message strings (or parameters taken from a continuous set). The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classiﬁer vanishes. For posteriors having all their weights on a single sample-compressed classiﬁer, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. (2005). Finally, we extend our results to the case where each sample-compressed classiﬁer of a data-dependent ensemble may abstain of predicting a class label. Keywords: PAC-Bayes, risk bounds, sample-compression, set covering machines, decision list machines</p><p>3 0.13690047 <a title="73-tfidf-3" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>Author: Jean-Yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, ﬁrst, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester (1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand (see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds. Keywords: statistical learning theory, PAC-Bayes theorems, generalization error bounds</p><p>4 0.093494125 <a title="73-tfidf-4" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>Author: András György, Tamás Linder, Gábor Lugosi, György Ottucsák</p><p>Abstract: The on-line shortest path problem is considered under various models of partial monitoring. Given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (deﬁned as the sum of the weights of its composing edges) be as small as possible. In a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. For this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is √ proportional to 1/ n and depends only polynomially on the number of edges of the graph. The algorithm can be implemented with complexity that is linear in the number of rounds n (i.e., the average complexity per round is constant) and in the number of edges. An extension to the so-called label efﬁcient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m n time instances. Another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. A version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. Applications to routing in packet switched networks along with simulation results are also presented. Keywords: on-line learning, shortest path problem, multi-armed bandit problem c 2007 Andr´ s Gy¨ rgy, Tam´ s Linder, G´ bor Lugosi and Gy¨ rgy Ottucs´ k. a o a a o a ¨ ´ G Y ORGY, L INDER , L UGOSI AND OTTUCS AK</p><p>5 0.078593753 <a title="73-tfidf-5" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>Author: Sanjoy Dasgupta, Leonard Schulman</p><p>Abstract: We show that, given data from a mixture of k well-separated spherical Gaussians in R d , a simple two-round variant of EM will, with high probability, learn the parameters of the Gaussians to nearoptimal precision, if the dimension is high (d ln k). We relate this to previous theoretical and empirical work on the EM algorithm. Keywords: expectation maximization, mixtures of Gaussians, clustering, unsupervised learning, probabilistic analysis</p><p>6 0.060729083 <a title="73-tfidf-6" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>7 0.059400707 <a title="73-tfidf-7" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>8 0.058282297 <a title="73-tfidf-8" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>9 0.053328995 <a title="73-tfidf-9" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>10 0.037294362 <a title="73-tfidf-10" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>11 0.036125384 <a title="73-tfidf-11" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>12 0.032418698 <a title="73-tfidf-12" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>13 0.031455766 <a title="73-tfidf-13" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>14 0.031244662 <a title="73-tfidf-14" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>15 0.030046133 <a title="73-tfidf-15" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>16 0.027885297 <a title="73-tfidf-16" href="./jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</a></p>
<p>17 0.026049586 <a title="73-tfidf-17" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>18 0.025750462 <a title="73-tfidf-18" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>19 0.023631627 <a title="73-tfidf-19" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>20 0.021355411 <a title="73-tfidf-20" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.18), (1, -0.128), (2, -0.252), (3, -0.314), (4, 0.099), (5, -0.082), (6, 0.161), (7, -0.261), (8, 0.042), (9, 0.102), (10, -0.023), (11, 0.199), (12, -0.082), (13, 0.015), (14, 0.101), (15, 0.006), (16, -0.009), (17, -0.006), (18, -0.019), (19, 0.069), (20, -0.102), (21, -0.141), (22, 0.068), (23, -0.101), (24, -0.105), (25, 0.01), (26, -0.034), (27, -0.006), (28, -0.081), (29, -0.045), (30, 0.0), (31, 0.029), (32, -0.068), (33, -0.047), (34, -0.024), (35, -0.093), (36, -0.03), (37, -0.064), (38, 0.004), (39, 0.013), (40, -0.011), (41, -0.018), (42, -0.045), (43, -0.064), (44, -0.026), (45, -0.007), (46, -0.096), (47, 0.094), (48, -0.018), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96737581 <a title="73-lsi-1" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>Author: Zakria Hussain, François Laviolette, Mario Marchand, John Shawe-Taylor, Spencer Charles Brubaker, Matthew D. Mullin</p><p>Abstract: Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classiﬁer achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classiﬁer achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassiﬁcations. Keywords: set covering machines, sample-compression, loss bounds</p><p>2 0.73008347 <a title="73-lsi-2" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>Author: François Laviolette, Mario Marchand</p><p>Abstract: We propose a PAC-Bayes theorem for the sample-compression setting where each classiﬁer is described by a compression subset of the training data and a message string of additional information. This setting, which is the appropriate one to describe many learning algorithms, strictly generalizes the usual data-independent setting where classiﬁers are represented only by data-independent message strings (or parameters taken from a continuous set). The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classiﬁer vanishes. For posteriors having all their weights on a single sample-compressed classiﬁer, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. (2005). Finally, we extend our results to the case where each sample-compressed classiﬁer of a data-dependent ensemble may abstain of predicting a class label. Keywords: PAC-Bayes, risk bounds, sample-compression, set covering machines, decision list machines</p><p>3 0.49978808 <a title="73-lsi-3" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>Author: Jean-Yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, ﬁrst, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester (1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand (see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds. Keywords: statistical learning theory, PAC-Bayes theorems, generalization error bounds</p><p>4 0.45173338 <a title="73-lsi-4" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>Author: Sanjoy Dasgupta, Leonard Schulman</p><p>Abstract: We show that, given data from a mixture of k well-separated spherical Gaussians in R d , a simple two-round variant of EM will, with high probability, learn the parameters of the Gaussians to nearoptimal precision, if the dimension is high (d ln k). We relate this to previous theoretical and empirical work on the EM algorithm. Keywords: expectation maximization, mixtures of Gaussians, clustering, unsupervised learning, probabilistic analysis</p><p>5 0.29452035 <a title="73-lsi-5" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>Author: Sébastien Gadat, Laurent Younes</p><p>Abstract: We introduce a new model addressing feature selection from a large dictionary of variables that can be computed from a signal or an image. Features are extracted according to an efﬁciency criterion, on the basis of speciﬁed classiﬁcation or recognition tasks. This is done by estimating a probability distribution P on the complete dictionary, which distributes its mass over the more efﬁcient, or informative, components. We implement a stochastic gradient descent algorithm, using the probability as a state variable and optimizing a multi-task goodness of ﬁt criterion for classiﬁers based on variable randomly chosen according to P. We then generate classiﬁers from the optimal distribution of weights learned on the training set. The method is ﬁrst tested on several pattern recognition problems including face detection, handwritten digit recognition, spam classiﬁcation and micro-array analysis. We then compare our approach with other step-wise algorithms like random forests or recursive feature elimination. Keywords: stochastic learning algorithms, Robbins-Monro application, pattern recognition, classiﬁcation algorithm, feature selection</p><p>6 0.27418029 <a title="73-lsi-6" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>7 0.26258078 <a title="73-lsi-7" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>8 0.25827366 <a title="73-lsi-8" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>9 0.23204376 <a title="73-lsi-9" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>10 0.21986236 <a title="73-lsi-10" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>11 0.15877265 <a title="73-lsi-11" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>12 0.14283082 <a title="73-lsi-12" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>13 0.13402089 <a title="73-lsi-13" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>14 0.12913196 <a title="73-lsi-14" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>15 0.12376716 <a title="73-lsi-15" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>16 0.12321502 <a title="73-lsi-16" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>17 0.11736168 <a title="73-lsi-17" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>18 0.11730242 <a title="73-lsi-18" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>19 0.11493388 <a title="73-lsi-19" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>20 0.1124652 <a title="73-lsi-20" href="./jmlr-2007-General_Polynomial_Time_Decomposition_Algorithms_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">35 jmlr-2007-General Polynomial Time Decomposition Algorithms     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.144), (8, 0.011), (10, 0.012), (12, 0.015), (28, 0.047), (40, 0.034), (48, 0.014), (60, 0.474), (85, 0.049), (98, 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.90301394 <a title="73-lda-1" href="./jmlr-2007-General_Polynomial_Time_Decomposition_Algorithms_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">35 jmlr-2007-General Polynomial Time Decomposition Algorithms     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Nikolas List, Hans Ulrich Simon</p><p>Abstract: We present a general decomposition algorithm that is uniformly applicable to every (suitably normalized) instance of Convex Quadratic Optimization and efﬁciently approaches an optimal solution. The number of iterations required to be within ε of optimality grows linearly with 1/ε and quadratically with the number m of variables. The working set selection can be performed in polynomial time. If we restrict our considerations to instances of Convex Quadratic Optimization with at most k0 equality constraints for some ﬁxed constant k0 plus some so-called box-constraints (conditions that hold for most variants of SVM-optimization), the working set is found in linear time. Our analysis builds on a generalization of the concept of rate certifying pairs that was introduced by Hush and Scovel. In order to extend their results to arbitrary instances of Convex Quadratic Optimization, we introduce the general notion of a rate certifying q-set. We improve on the results by Hush and Scovel (2003) in several ways. First our result holds for Convex Quadratic Optimization whereas the results by Hush and Scovel are specialized to SVM-optimization. Second, we achieve a higher rate of convergence even for the special case of SVM-optimization (despite the generality of our approach). Third, our analysis is technically simpler. We prove furthermore that the strategy for working set selection which is based on rate certifying sets coincides with a strategy which is based on a so-called “sparse witness of sub-optimality”. Viewed from this perspective, our main result improves on convergence results by List and Simon (2004) and Simon (2004) by providing convergence rates (and by holding under more general conditions). Keywords: convex quadratic optimization, decomposition algorithms, support vector machines</p><p>same-paper 2 0.87934172 <a title="73-lda-2" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>Author: Zakria Hussain, François Laviolette, Mario Marchand, John Shawe-Taylor, Spencer Charles Brubaker, Matthew D. Mullin</p><p>Abstract: Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classiﬁer achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classiﬁer achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassiﬁcations. Keywords: set covering machines, sample-compression, loss bounds</p><p>3 0.83779132 <a title="73-lda-3" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>Author: Peter L. Bartlett, Mikhail Traskin</p><p>Abstract: The risk, or probability of error, of the classiﬁer produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after n1−ε iterations—for sample size n and ε ∈ (0, 1)—the sequence of risks of the classiﬁers it produces approaches the Bayes risk. Keywords: boosting, adaboost, consistency</p><p>4 0.83526409 <a title="73-lda-4" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>Author: Niels Landwehr, Kristian Kersting, Luc De Raedt</p><p>Abstract: A novel relational learning approach that tightly integrates the na¨ve Bayes learning scheme with ı the inductive logic programming rule-learner FOIL is presented. In contrast to previous combinations that have employed na¨ve Bayes only for post-processing the rule sets, the presented approach ı employs the na¨ve Bayes criterion to guide its search directly. The proposed technique is impleı mented in the N FOIL and T FOIL systems, which employ standard na¨ve Bayes and tree augmented ı na¨ve Bayes models respectively. We show that these integrated approaches to probabilistic model ı and rule learning outperform post-processing approaches. They also yield signiﬁcantly more accurate models than simple rule learning and are competitive with more sophisticated ILP systems. Keywords: rule learning, na¨ve Bayes, statistical relational learning, inductive logic programming ı</p><p>5 0.58273286 <a title="73-lda-5" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>Author: François Laviolette, Mario Marchand</p><p>Abstract: We propose a PAC-Bayes theorem for the sample-compression setting where each classiﬁer is described by a compression subset of the training data and a message string of additional information. This setting, which is the appropriate one to describe many learning algorithms, strictly generalizes the usual data-independent setting where classiﬁers are represented only by data-independent message strings (or parameters taken from a continuous set). The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classiﬁer vanishes. For posteriors having all their weights on a single sample-compressed classiﬁer, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. (2005). Finally, we extend our results to the case where each sample-compressed classiﬁer of a data-dependent ensemble may abstain of predicting a class label. Keywords: PAC-Bayes, risk bounds, sample-compression, set covering machines, decision list machines</p><p>6 0.53603965 <a title="73-lda-6" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>7 0.51599813 <a title="73-lda-7" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>8 0.51494372 <a title="73-lda-8" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>9 0.51208222 <a title="73-lda-9" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>10 0.50009418 <a title="73-lda-10" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>11 0.49961814 <a title="73-lda-11" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>12 0.49323392 <a title="73-lda-12" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>13 0.49125057 <a title="73-lda-13" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>14 0.4300935 <a title="73-lda-14" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>15 0.42590836 <a title="73-lda-15" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>16 0.41443443 <a title="73-lda-16" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>17 0.41372851 <a title="73-lda-17" href="./jmlr-2007-A_Complete_Characterization_of_a_Family_of_Solutions_to_a_Generalized_Fisher_Criterion.html">2 jmlr-2007-A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion</a></p>
<p>18 0.40586889 <a title="73-lda-18" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>19 0.40447935 <a title="73-lda-19" href="./jmlr-2007-Large_Margin_Semi-supervised_Learning.html">44 jmlr-2007-Large Margin Semi-supervised Learning</a></p>
<p>20 0.40224969 <a title="73-lda-20" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
