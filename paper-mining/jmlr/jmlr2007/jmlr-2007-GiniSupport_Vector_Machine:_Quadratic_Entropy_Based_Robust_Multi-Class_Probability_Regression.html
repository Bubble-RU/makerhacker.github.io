<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-37" href="#">jmlr2007-37</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</h1>
<br/><p>Source: <a title="jmlr-2007-37-pdf" href="http://jmlr.org/papers/volume8/chakrabartty07a/chakrabartty07a.pdf">pdf</a></p><p>Author: Shantanu Chakrabartty, Gert Cauwenberghs</p><p>Abstract: Many classiﬁcation tasks require estimation of output class probabilities for use as conﬁdence scores or for inference integrated with other models. Probability estimates derived from large margin classiﬁers such as support vector machines (SVMs) are often unreliable. We extend SVM large margin classiﬁcation to GiniSVM maximum entropy multi-class probability regression. GiniSVM combines a quadratic (Gini-Simpson) entropy based agnostic model with a kernel based similarity model. A form of Huber loss in the GiniSVM primal formulation elucidates a connection to robust estimation, further corroborated by the impulsive noise ﬁltering property of the reverse water-ﬁlling procedure to arrive at normalized classiﬁcation margins. The GiniSVM normalized classiﬁcation margins directly provide estimates of class conditional probabilities, approximating kernel logistic regression (KLR) but at reduced computational cost. As with other SVMs, GiniSVM produces a sparse kernel expansion and is trained by solving a quadratic program under linear constraints. GiniSVM training is efﬁciently implemented by sequential minimum optimization or by growth transformation on probability functions. Results on synthetic and benchmark data, including speaker veriﬁcation and face detection data, show improved classiﬁcation performance and increased tolerance to imprecision over soft-margin SVM and KLR. Keywords: support vector machines, large margin classiﬁers, kernel regression, probabilistic models, quadratic entropy, Gini index, growth transformation</p><p>Reference: <a title="jmlr-2007-37-reference" href="../jmlr2007_reference/jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 GiniSVM combines a quadratic (Gini-Simpson) entropy based agnostic model with a kernel based similarity model. [sent-6, score-0.279]
</p><p>2 A form of Huber loss in the GiniSVM primal formulation elucidates a connection to robust estimation, further corroborated by the impulsive noise ﬁltering property of the reverse water-ﬁlling procedure to arrive at normalized classiﬁcation margins. [sent-7, score-0.198]
</p><p>3 Results on synthetic and benchmark data, including speaker veriﬁcation and face detection data, show improved classiﬁcation performance and increased tolerance to imprecision over soft-margin SVM and KLR. [sent-11, score-0.195]
</p><p>4 Keywords: support vector machines, large margin classiﬁers, kernel regression, probabilistic models, quadratic entropy, Gini index, growth transformation  1. [sent-12, score-0.211]
</p><p>5 For instance text-independent speaker veriﬁcation systems require normalized classiﬁer scores to be integrated over several speech frames in an utterance (Auckenthaler et al. [sent-23, score-0.304]
</p><p>6 Even though SVMs have been successfully applied for the task of speaker veriﬁcation (Schmidt and Gish, 1996; Gu and Thomas, 2001), the cumulative scores generated by SVMs are susceptible to corruption by impulse noise, which increases false acceptance rate. [sent-25, score-0.216]
</p><p>7 Previous work in this area used Shannon entropy in a large margin framework (Jebara, 2001) which led directly to KLR and hence inherited its potential disadvantages of non-sparsity. [sent-42, score-0.169]
</p><p>8 One of the important contributions of the paper is exploration of links between maximum entropy based learning techniques and large margin classiﬁers with extensions to quadratic based impurity functions. [sent-43, score-0.193]
</p><p>9 Within this framework the paper introduces the Gini Support Vector Machine (GiniSVM) (Chakrabartty and Cauwenberghs, 2002), a large margin classiﬁer based on a quadratic entropy formulation combined with kernel based quadratic distance. [sent-44, score-0.325]
</p><p>10 Section 6 compares the performance of GiniSVM for benchmark UCI databases, a face detection and a text-independent speaker veriﬁcation task. [sent-51, score-0.195]
</p><p>11 Also provided to the learner is a set of soft (or possibly hard) labels that represent conditional probability measures yik = P(Ck |xi ) deﬁned over a discrete set of classes Ck , k = 1, . [sent-58, score-0.235]
</p><p>12 The labels therefore are normalized and satisfy ∑M yik = 1. [sent-61, score-0.254]
</p><p>13 Since the prior labels y ik are available only for the training set, the learner also deﬁnes an agnostic (non-informative) distance metric D I : RM × RM → R which does not assume any knowledge of the training set. [sent-66, score-0.363]
</p><p>14 The embedded agnostic prior is consistent with maximum entropy principles (Jaynes, 1957; Pietra and Pietra, 1993) and enforces smoothness constraints on the the function Pk (x) by avoiding solutions that over-ﬁt to the training set. [sent-67, score-0.255]
</p><p>15 P  P  (1)  Here Y : R|T | × RM is a matrix of prior labels yik , i = 1, . [sent-69, score-0.254]
</p><p>16 , M under the distribution P to an equivalent measure under the prior distribution yik . [sent-83, score-0.254]
</p><p>17 This ﬁrst constraint can be written to express equivalence between average estimated probabilities and empirical frequencies for each class over the training set N  N  i=1  i=1  ∑ Pk (xi ) = ∑ yik ,  k = 1, . [sent-84, score-0.291]
</p><p>18 D I (P,U) is a distance that deﬁnes an agnostic model when any prior knowledge about prior distribution is absent. [sent-102, score-0.171]
</p><p>19 ∂Pk (x) ∂Pk (x)  (5)  Here bk represent Lagrange multipliers corresponding to frequency constraints (2), β k (x) ≥ 0 are Lagrange multipliers for the inequality constraints (3), and Lagrange multipliers z(x) correspond to the normalization constraint (4). [sent-112, score-0.281]
</p><p>20 The Legendre transformation is commonly used in the dual formulation of support vector machines ¨ and other kernel machines (Vapnik, 1995; Scholkopf and Smola, 2001), and we refer to ∇Ψ−1 (. [sent-122, score-0.261]
</p><p>21 A popular metric is a quadratic ¨ distance extensively used in kernel methods (Scholkopf et al. [sent-130, score-0.159]
</p><p>22 ) given by Equation (8) the ﬁrst order conditions (7) can be written as 1 Pk (x) = ∇Ψ−1 [ fk (x) − z(x) + βk (x)] (9) γ where N  fk (x) = ∑ λi K(xi , x) + bk k i=1  with inference parameters λi = C[yik − Pk (xi )]. [sent-140, score-0.383]
</p><p>23 Like the primal (1), minimization of the dual Hd is subject to linear constraints (2)-(4) rewritten in terms of the inference parameters as M  ∑ λik  = 0,  i = 1, . [sent-148, score-0.296]
</p><p>24 The other Lagrange multiplier z(x) in (13) is determined by expressing the normalization condition ∑M Pk (x) = 1 which leads to a logistic model k=1 Pk (x) = exp  M 1 1 fk (x))/ ∑ exp( f p (x) . [sent-164, score-0.234]
</p><p>25 ) in the general form (10) directly leads to the dual cost function M  He =  ∑  k=1  N 1 N N i ∑ ∑ λk Qi j λkj + γ ∑ (yik − λik /C) log(yik − λik /C) 2C i=1 j=1 i=1  (15)  subject to the dual constraints (11). [sent-169, score-0.319]
</p><p>26 1 KLR Primal Reformulation The dual (15) derived from the general maximum entropy form (1) is identical to the dual formulation of another, closely related primal cost function for kernel logistic regression as formulated in Jaakkola and Haussler (1999). [sent-171, score-0.553]
</p><p>27 ), the decision functions f k (x) are linked to a set of M hyperplanes N  fk (x) =  ∑ λik K(xi , x) + bk  (16)  ∑ λik Φ(xi ) · Φ(x) + bk  (17)  i=1 N  =  i=1  = wk · Φ(x) + bk 818  G INI S UPPORT V ECTOR M ACHINE  where wk = ∑N λi Φ(xi ) represent the parameters of the hyperplanes. [sent-174, score-0.48]
</p><p>28 The following proposition i=1 k links the kernel logistic regression dual (15) with its equivalent primal formulation (Jaakkola and Haussler, 1999). [sent-175, score-0.375]
</p><p>29 Proposition I: The kernel logistic regression objective function (15) is the dual derived from a primal objective function with regularized loss function M  Le =  N  1  yik  i=1 k=1 N M  k=1  =  M  ∑ 2 |wk |2 +C ∑ ∑ yik log Pk (xi ) 1  ∑ 2 |wk |2 −C ∑ [ ∑ yik fk (xi ) − log(e f (x ) + . [sent-176, score-1.176]
</p><p>30 The primal uses the Kullback-Leibler (KL) divergence ΨKL (P,U) = P log(P/U) between distributions yik and Pk (xi ) as loss function in the regularized form (18) (Wahba, 1998; Zhu and Hastie, 2002). [sent-182, score-0.313]
</p><p>31 One of the disadvantages of the kernel logistic dual is that the KL divergence distance metric strongly penalizes solutions far away from the agnostic distribution U, leading to a non-sparse kernel expansion. [sent-183, score-0.452]
</p><p>32 A Gini form of entropy as agnostic distance metric provides the connection between support vector machines and probability regression, studied next. [sent-185, score-0.249]
</p><p>33 GiniSVM and Margin Normalization Instead of KL divergence, a natural choice for an agnostic distance metric D I is a quadratic form of entropy similar to the quadratic form of the prior distance metric D Q . [sent-187, score-0.406]
</p><p>34 1 The Gini quadratic form of entropy ΨGini (P,U) = 2 (P −U)2 with uniform agnostic distribution Uik ≡ 1/M leads to an agnostic distance metric DI (P,U) = =  1 M ∑ ∑ (Pk (x) −Uik )2 2 k=1 x∈T 1 M ∑ ∑ Pk (x)2 + cst 2 k=1 x∈T  where the constant term cst = −N/2M drops in the minimization. [sent-191, score-0.455]
</p><p>35 ) in the general form (10) leads to the dual GiniSVM cost function M  Hg =  ∑  k=1  γ N 1 N N i j λk Qi j λk + ∑ (yik − λi /C)2 ∑∑ k 2C i=1 j=1 2 i=1  (19)  under constraints (11). [sent-196, score-0.182]
</p><p>36 In contrast to the KL distance in the KLR dual (15), the quadratic distance in the GiniSVM dual (19) allows sparse kernel expansions, where several of the inference parameters λ i are driven k 819  C HAKRABARTTY AND C AUWENBERGHS  Figure 2: Illustration of reverse water-ﬁlling procedure. [sent-197, score-0.489]
</p><p>37 to zero by the inequality constraints in (11) corresponding to a majority of labels for which y ik = 0. [sent-199, score-0.185]
</p><p>38 Even sparser kernel expansions could be obtained with soft-margin support vector machines for classiﬁcation, owing to its slightly different quadratic cost function under linear constraints which further favors sparsity. [sent-200, score-0.174]
</p><p>39 To ensure positive probabilities according to constraints (3), the Lagrange parameters βk (x) produce rectiﬁed linear probability estimates 1 Pk (x) = [ fk (x) − z(x)]+ γ  (20)  where [x]+ = max(x, 0) denotes a hinge function. [sent-210, score-0.224]
</p><p>40 The remaining Lagrange parameter z(x) is determined through a subtractive normalization procedure which solves for the normalization constraint (4) M  ∑ [ fk (x) − z(x)]+ = γ. [sent-211, score-0.307]
</p><p>41 We refer to the procedure solving for z(x) given conﬁdence scores f k (x) in (21) as margin normalization, because of similarities between the normalization parameter γ and the margin of multi-class soft-margin support vector machines (Weston and Watkins, 1998). [sent-214, score-0.304]
</p><p>42 , M a set of GiniSVM decision functions satisfying the reverse water-ﬁlling conditions ∑M [ fk (x) − z1 (x)]+ = γ1 and ∑M [ fk (x) − z2 (x)]+ = γ2 . [sent-220, score-0.315]
</p><p>43 Proof: The two reverse water-ﬁlling conditions lead to M  ∑ ([ fk (x) − z1 (x)]+ − [ fk (x) − z2 (x)]+) = γ1 − γ2 > 0. [sent-222, score-0.315]
</p><p>44 Therefore the margin parameter γ directly controls the sparsity m of the probability estimates, assigning the probability mass to a smaller fraction of more conﬁdent classes with larger fk (x) as γ is decreased. [sent-232, score-0.22]
</p><p>45 Margin: In the limit γ → 0, the normalization factor z(x) → max k fk (x). [sent-234, score-0.191]
</p><p>46 Based on this principle a multi-class probability margin can be deﬁned based on the multi-class decision functions f k (x) as fk (x) = z(x) + γ. [sent-236, score-0.22]
</p><p>47 Robustness: The subtractive margin normalization (20) and (21) is inherently robust to impulsive noise, since components f k (z) in the kernel expansion smaller than a threshold z(x) (at most a margin γ below the largest value) do not contribute to the output. [sent-255, score-0.375]
</p><p>48 2 GiniSVM Primal Reformulation In this section we derive an equivalent primal reformulation of the GiniSVM dual (19), analogous to the derivation in Section 3. [sent-260, score-0.241]
</p><p>49 , N and its corresponding prior probability distributions yik ∈ R : yik ≥ 0; ∑M yik = 1, GiniSVM in its primal reformulation of minimizes a regularization k=1 factor proportional to the L2 norm of the weight vectors wk , k = 1, . [sent-267, score-0.895]
</p><p>50 822  G INI S UPPORT V ECTOR M ACHINE  lg according to Lg =  N M 1 M ∑ |wk |2 + ∑ ∑ lg (wk , bk , zi ) 2 k=1 i=1 k=1  (22)  with loss function lg (xi ) for each training vector xi given by lg (xi ) =  1 γC yik − [ fk (xi ) − zi ]+ 2 γ  2  ,  and where zi , i = 1, . [sent-272, score-0.962]
</p><p>51 Pk (xi ) = 1 [ fk (xi ) − z∗ ]+ with fk (xi ) = w∗ · xi + b∗ for a given data xi is a valid condii k k γ tional probability measure over classes k ∈ 1, . [sent-280, score-0.346]
</p><p>52 The dual cost function corresponding to the primal cost function (22) is the GiniSVM dual (19). [sent-284, score-0.352]
</p><p>53 With the kernel expansion of f (x) expressed in reduced form 3 N  f (x) = ∑ λi yi K(xi , x) + b,  (24)  i=1  the GiniSVM dual objective function (19) and linear constraints (11) reduce to Hb = min λi  1 C  N  N  N  ∑ ∑ λi λ j yi y j K(xi , x j ) − ∑ G(λi )  i=1 j=1  (25)  i=1  3. [sent-291, score-0.312]
</p><p>54 This choice of kernel expansion is consistent with binary soft-margin SVM and binary KLR, with identical dual formulation under constraints, and with the only difference in the form of the dual potential function G(λ) (26). [sent-292, score-0.448]
</p><p>55 under constraints M  ∑ λi yi = 0,  i=1  0 ≤ λi ≤ C with dual potential function  λ λ (26) G(λ) = γC (1 − ). [sent-294, score-0.238]
</p><p>56 Figure 4 graphically illustrates the relationship between the binary GiniSVM dual potential function G, inference parameters λi , probability estimates Pi , and primal reformulation loss function lg . [sent-296, score-0.431]
</p><p>57 The dual potential is linked to the probability estimate through the Legendre transform ∇Ψ −1 (. [sent-297, score-0.167]
</p><p>58 Since the dual potential function is symmetric Ψ(P+1 ) = Ψ(1 − P+1 ) = Ψ(P−1 ), it follows that the Legendre transform ∇Ψ−1 (. [sent-299, score-0.167]
</p><p>59 Symmetry in the dual potential function thus leads to unbiased probability estimates that are centered around the discrimination boundary, P+1 = P−1 = 1/2 for f (x) = 0. [sent-301, score-0.188]
</p><p>60 The Legendre transform also links the dual potential function to the primal reformulation cost function. [sent-302, score-0.271]
</p><p>61 Note the relationship between the parameter γ scaling the dual potential function, and the location of the margin in the loss function and probability estimate indicated by M in Figure 4. [sent-303, score-0.248]
</p><p>62 Hence the parameter γ can be seen both to control the strength of the agnostic metric D I , and to control a measure of margin in the probability regression. [sent-304, score-0.231]
</p><p>63 The regularization parameter C also scales the dual potential function, but controls regularization by scaling the primal loss function by the same factor C without affecting margin, as in soft-margin SVM classiﬁcation. [sent-305, score-0.245]
</p><p>64 8 Inference Parameter λ  1  (b)  Figure 5: Primal and dual formulation of logistic regression, GiniSVM regression and soft-margin SVM classiﬁcation. [sent-319, score-0.228]
</p><p>65 Figure 5(b) also shows the binary KLR dual potential function given by Shannon’s binary entropy (Jaakkola and Haussler, 1999) G(λ) = γC  λ λ λ λ log( ) + (1 − ) log(1 − ) . [sent-327, score-0.265]
</p><p>66 C C C C  Like GiniSVM, the binary KLR dual potential function is symmetric with respect to the separating hyperplane in Figure 5(a), and hence also produces unbiased estimates of conditional probabilities. [sent-328, score-0.208]
</p><p>67 The binary ¨ GiniSVM dual bears similarity to the quadratic SVM dual (Scholkopf et al. [sent-330, score-0.348]
</p><p>68 4 Relation to Robust Estimation and Logistic Regression The GiniSVM dual (19) relates to the kernel logistic regression dual (15) through a lower-bound on Shannon entropy. [sent-333, score-0.391]
</p><p>69 It can be shown 2 825  C HAKRABARTTY AND C AUWENBERGHS  that the solution obtained by minimizing the GiniSVM dual Hg in Equation (19) is an over-estimate of the solution obtained by minimizing kernel logistic dual He given by Equation (15). [sent-337, score-0.369]
</p><p>70 Thus the GiniSVM dual Hg can also be used for approximately solving the kernel logistic dual He . [sent-339, score-0.369]
</p><p>71 Therefore the parameter γ in GiniSVM can be interpreted as a noise margin in the Huber formulation, consistent with its interpretation as noise threshold in the reverse water-ﬁlling procedure for margin normalization. [sent-350, score-0.199]
</p><p>72 , 1998) which directly generates normalized scores from the margin variable f (x). [sent-353, score-0.167]
</p><p>73 The second algorithm uses the polynomial nature of the dual resulting into a novel multiplicative update algorithm based on growth transformation on probabilities. [sent-360, score-0.181]
</p><p>74 In the case of the GiniSVM dual function (19), at least four inference parameters need to be chosen to satisfy two sets of equality constraints (11). [sent-366, score-0.218]
</p><p>75 1 2 1 2 • Update λ1 , λ1 , λ2 and λ2 such that the dual (19) is minimized subject to constraints (11). [sent-380, score-0.182]
</p><p>76 Also, let Pik the value of the probability distribution th iteration then at m M  m+1 m m Pik ← Pik δm / ∑ Pik δm ik ik k=1  where N  m m δm = C ∑ Qi j [Pik − yik ] + γPik + Γ ik j=1  and Γ = C (N + 1) Qmax . [sent-408, score-0.655]
</p><p>77 2 Speaker Veriﬁcation Experiments The beneﬁt of normalized scores generated by GiniSVM is demonstrated for the task of textindependent speaker veriﬁcation. [sent-501, score-0.235]
</p><p>78 The task entails verifying a particular speaker from possible imposters without any knowledge of the text spoken. [sent-502, score-0.181]
</p><p>79 A YOHO speaker veriﬁcation database was chosen for training and testing the speaker veriﬁcation system. [sent-505, score-0.316]
</p><p>80 To reduce the total number of training points, a K-means clustering was performed for each speaker to obtain 1000 cluster points for the correct speaker, and 100 cluster points for each imposter speaker. [sent-528, score-0.167]
</p><p>81 For each speaker (101-200), this procedure was repeated to obtain a training set of 10, 900 MFCC vectors. [sent-529, score-0.167]
</p><p>82 06  Figure 9: Comparison of ROC obtained for a speaker veriﬁcation system based on soft-margin SVM and GiniSVM classiﬁcation for speaker id: 148. [sent-558, score-0.298]
</p><p>83 This demonstrates that the normalization procedure used by GiniSVM improves the accuracy of a text-independent speaker veriﬁcation system. [sent-563, score-0.201]
</p><p>84 In particular, GiniSVM produces direct estimates of conditional probabilities that approximate kernel logistic regression (KLR) at reduced computational cost, incurring quadratic programming under linear constraints as with standard SVM training. [sent-568, score-0.256]
</p><p>85 Unlike a baseline soft-margin SVM based system with calibrated probabilities, GiniSVM produces unbiased probability estimates owing to symmetry in the agnostic distance metric in the maximum entropy formulation. [sent-569, score-0.247]
</p><p>86 The probability estimates are sparse, where the number of non-zero probabilities is controlled by a single parameter γ, which acts as a margin in the normalization of probability scores. [sent-570, score-0.173]
</p><p>87 The margin parameter γ is distinct from the regularization parameter C also found in soft-margin SVM and KLR, even though both C and γ weigh the agnostic metric relative to the prior metric in the maximum entropy primal cost function. [sent-571, score-0.421]
</p><p>88 GiniSVM also successfully trained on a task of textindependent speaker veriﬁcation, by integrating normalized probability scores over time. [sent-575, score-0.235]
</p><p>89 The maximum entropy framework for large-margin kernel probability regression introduced for GiniSVM is general and can be extended to other classiﬁcation and regression tasks based on polynomial entropy. [sent-577, score-0.174]
</p><p>90 Kernel Logistic Regression Primal and Dual Formulation Proof of Proposition I: Deﬁne Le as the regularized log-likelihood/cross entropy for kernel logistic regression (Wahba, 1998; Zhu and Hastie, 2002) M  Le =  N M 1 ||wk ||2 −C ∑ [ ∑ yik fk (xi ) − log(e f1 (xi ) + . [sent-583, score-0.549]
</p><p>91 ∑ i=1 k=1 k=1 2  (30)  First order conditions with respect to parameters wk and bk in fk (x) = wk . [sent-587, score-0.342]
</p><p>92 x + bk yield e fk (xi ) ] xi , ∑M e f p (xi ) p  N  wk = C ∑ [yik − i=1 N  0 = C ∑[yik − n  e fk (xi ) ]. [sent-588, score-0.448]
</p><p>93 ∑M e f p (xi ) p  Denote λn = C[yik − k  e fk (xi ) ∑M e f p (xi ) p  ]  (31)  (32)  in the ﬁrst-order conditions (31) to arrive at the kernel expansion (17) with linear constraint fk (x) =  ∑ λn K(xi , x) + bk , k  (33)  n  0 =  ∑ λn . [sent-589, score-0.444]
</p><p>94 k=1 k Legendre transformation of the primal objective function (30) in w k and bk leads to a dual formulation directly in terms of the coefﬁcients λn (Jaakkola and Haussler, 1999). [sent-591, score-0.31]
</p><p>95 To prove the second part of the proposition let 1 λi = Cyik − [ fk (xi ) − zi + µik ]). [sent-598, score-0.201]
</p><p>96 Substitution in (36) yields an expansion of wk which re-substituted in the primal yields the dual ﬁrst-order condition  ∑ Qi j λkj + bk − zi + µik − γ(yik − λik /C) = 0. [sent-600, score-0.422]
</p><p>97 j  Along with constraints (11) the corresponding dual reduces to the GiniSVM dual Hg (19), which completes proof of the proposition. [sent-601, score-0.319]
</p><p>98 1 Further substitution of λi = yi λi , λi = −yi λi , yi,+1 = 1 (1 + yi ) and yi,−1 = 2 (1 − yi ) into the +1 −1 2 binary GiniSVM dual cost function (42) N 1 Hg = ∑ λi λ j yi y j Qi j − γ ∑ C ij i=1  1 2  2  1 λi − − 2 C  2  which is equivalent to the form Hb (25). [sent-607, score-0.261]
</p><p>99 For the interior points denoted by its training index i the following condition k is satisﬁed bk − zi + gik = 0 which is over-complete in parameters bk , k = 1, . [sent-622, score-0.201]
</p><p>100 A text-independent speaker veriﬁcation system using support vector machines classiﬁer. [sent-756, score-0.172]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ginisvm', 0.727), ('pik', 0.25), ('yik', 0.235), ('pk', 0.163), ('speaker', 0.149), ('ik', 0.14), ('fk', 0.139), ('dual', 0.137), ('klr', 0.128), ('agnostic', 0.115), ('auwenberghs', 0.098), ('ector', 0.098), ('hakrabartty', 0.098), ('ini', 0.098), ('gini', 0.097), ('achine', 0.083), ('lg', 0.083), ('upport', 0.083), ('margin', 0.081), ('primal', 0.078), ('svm', 0.072), ('bk', 0.069), ('dq', 0.069), ('chakrabartty', 0.068), ('wk', 0.067), ('scores', 0.067), ('entropy', 0.058), ('quadratic', 0.054), ('cauwenberghs', 0.053), ('normalization', 0.052), ('kernel', 0.052), ('smo', 0.049), ('lling', 0.047), ('legendre', 0.046), ('subtractive', 0.045), ('zi', 0.045), ('constraints', 0.045), ('veri', 0.044), ('qi', 0.044), ('logistic', 0.043), ('lagrange', 0.041), ('speech', 0.039), ('hg', 0.039), ('eer', 0.038), ('impulsive', 0.038), ('mfcc', 0.038), ('yoho', 0.038), ('reverse', 0.037), ('inference', 0.036), ('metric', 0.035), ('huber', 0.034), ('xi', 0.034), ('entails', 0.032), ('pietra', 0.032), ('kl', 0.031), ('potential', 0.03), ('cst', 0.03), ('nsv', 0.03), ('uik', 0.03), ('utterance', 0.03), ('face', 0.027), ('platt', 0.027), ('svms', 0.026), ('mismatch', 0.026), ('expansion', 0.026), ('reformulation', 0.026), ('yi', 0.026), ('formulation', 0.026), ('decoding', 0.024), ('jaakkola', 0.024), ('growth', 0.024), ('machines', 0.023), ('haussler', 0.023), ('cyik', 0.023), ('gopalakrishnan', 0.023), ('shantanu', 0.023), ('ynk', 0.023), ('roc', 0.022), ('regression', 0.022), ('estimates', 0.021), ('polynomial', 0.02), ('di', 0.02), ('binary', 0.02), ('constraint', 0.019), ('normalized', 0.019), ('osuna', 0.019), ('hd', 0.019), ('prior', 0.019), ('detection', 0.019), ('probabilities', 0.019), ('sequential', 0.018), ('frame', 0.018), ('classi', 0.018), ('distance', 0.018), ('training', 0.018), ('proposition', 0.017), ('multipliers', 0.017), ('unity', 0.017), ('gert', 0.017), ('burges', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="37-tfidf-1" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>Author: Shantanu Chakrabartty, Gert Cauwenberghs</p><p>Abstract: Many classiﬁcation tasks require estimation of output class probabilities for use as conﬁdence scores or for inference integrated with other models. Probability estimates derived from large margin classiﬁers such as support vector machines (SVMs) are often unreliable. We extend SVM large margin classiﬁcation to GiniSVM maximum entropy multi-class probability regression. GiniSVM combines a quadratic (Gini-Simpson) entropy based agnostic model with a kernel based similarity model. A form of Huber loss in the GiniSVM primal formulation elucidates a connection to robust estimation, further corroborated by the impulsive noise ﬁltering property of the reverse water-ﬁlling procedure to arrive at normalized classiﬁcation margins. The GiniSVM normalized classiﬁcation margins directly provide estimates of class conditional probabilities, approximating kernel logistic regression (KLR) but at reduced computational cost. As with other SVMs, GiniSVM produces a sparse kernel expansion and is trained by solving a quadratic program under linear constraints. GiniSVM training is efﬁciently implemented by sequential minimum optimization or by growth transformation on probability functions. Results on synthetic and benchmark data, including speaker veriﬁcation and face detection data, show improved classiﬁcation performance and increased tolerance to imprecision over soft-margin SVM and KLR. Keywords: support vector machines, large margin classiﬁers, kernel regression, probabilistic models, quadratic entropy, Gini index, growth transformation</p><p>2 0.051629525 <a title="37-tfidf-2" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>Author: Yann Guermeur</p><p>Abstract: In the context of discriminant analysis, Vapnik’s statistical learning theory has mainly been developed in three directions: the computation of dichotomies with binary-valued functions, the computation of dichotomies with real-valued functions, and the computation of polytomies with functions taking their values in ﬁnite sets, typically the set of categories itself. The case of classes of vectorvalued functions used to compute polytomies has seldom been considered independently, which is unsatisfactory, for three main reasons. First, this case encompasses the other ones. Second, it cannot be treated appropriately through a na¨ve extension of the results devoted to the computation of ı dichotomies. Third, most of the classiﬁcation problems met in practice involve multiple categories. In this paper, a VC theory of large margin multi-category classiﬁers is introduced. Central in this theory are generalized VC dimensions called the γ-Ψ-dimensions. First, a uniform convergence bound on the risk of the classiﬁers of interest is derived. The capacity measure involved in this bound is a covering number. This covering number can be upper bounded in terms of the γ-Ψdimensions thanks to generalizations of Sauer’s lemma, as is illustrated in the speciﬁc case of the scale-sensitive Natarajan dimension. A bound on this latter dimension is then computed for the class of functions on which multi-class SVMs are based. This makes it possible to apply the structural risk minimization inductive principle to those machines. Keywords: multi-class discriminant analysis, large margin classiﬁers, uniform strong laws of large numbers, generalized VC dimensions, multi-class SVMs, structural risk minimization inductive principle, model selection</p><p>3 0.04663831 <a title="37-tfidf-3" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>Author: Ryan M. Rifkin, Ross A. Lippert</p><p>Abstract: Regularization is an approach to function learning that balances ﬁt and smoothness. In practice, we search for a function f with a ﬁnite representation f = ∑i ci φi (·). In most treatments, the ci are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning. Keywords: kernel machines, duality, optimization, convex analysis, kernel learning</p><p>4 0.043424949 <a title="37-tfidf-4" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Kwangmoo Koh, Seung-Jean Kim, Stephen Boyd</p><p>Abstract: Logistic regression with 1 regularization has been proposed as a promising method for feature selection in classiﬁcation problems. In this paper we describe an efﬁcient interior-point method for solving large-scale 1 -regularized logistic regression problems. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC; medium sized problems, with tens of thousands of features and examples, can be solved in tens of seconds (assuming some sparsity in the data). A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve very large problems, with a million features and examples (e.g., the 20 Newsgroups data set), in a few minutes, on a PC. Using warm-start techniques, a good approximation of the entire regularization path can be computed much more efﬁciently than by solving a family of problems independently. Keywords: logistic regression, feature selection, 1 regularization, regularization path, interiorpoint methods.</p><p>5 0.04102546 <a title="37-tfidf-5" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>Author: Robert Tibshirani, Trevor Hastie</p><p>Abstract: We propose a method for the classiﬁcation of more than two classes, from high-dimensional features. Our approach is to build a binary decision tree in a top-down manner, using the optimal margin classiﬁer at each split. We implement an exact greedy algorithm for this task, and compare its performance to less greedy procedures based on clustering of the matrix of pairwise margins. We compare the performance of the “margin tree” to the closely related “all-pairs” (one versus one) support vector machine, and nearest centroids on a number of cancer microarray data sets. We also develop a simple method for feature selection. We ﬁnd that the margin tree has accuracy that is competitive with other methods and offers additional interpretability in its putative grouping of the classes. Keywords: maximum margin classiﬁer, support vector machine, decision tree, CART</p><p>6 0.037148617 <a title="37-tfidf-6" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>7 0.037070863 <a title="37-tfidf-7" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>8 0.035708357 <a title="37-tfidf-8" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>9 0.033874512 <a title="37-tfidf-9" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>10 0.033024956 <a title="37-tfidf-10" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>11 0.032508031 <a title="37-tfidf-11" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>12 0.031297918 <a title="37-tfidf-12" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>13 0.030521644 <a title="37-tfidf-13" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>14 0.030050125 <a title="37-tfidf-14" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>15 0.029678246 <a title="37-tfidf-15" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>16 0.02886408 <a title="37-tfidf-16" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>17 0.02785453 <a title="37-tfidf-17" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>18 0.026952049 <a title="37-tfidf-18" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>19 0.026663836 <a title="37-tfidf-19" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>20 0.025443817 <a title="37-tfidf-20" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.159), (1, -0.004), (2, 0.036), (3, 0.03), (4, 0.056), (5, 0.001), (6, -0.138), (7, 0.012), (8, -0.013), (9, 0.052), (10, 0.07), (11, 0.031), (12, -0.001), (13, 0.076), (14, 0.102), (15, -0.054), (16, -0.078), (17, -0.115), (18, -0.075), (19, -0.09), (20, 0.014), (21, -0.007), (22, 0.074), (23, 0.093), (24, -0.091), (25, -0.004), (26, -0.02), (27, 0.129), (28, 0.122), (29, -0.062), (30, 0.221), (31, 0.075), (32, 0.219), (33, -0.041), (34, -0.008), (35, 0.084), (36, -0.246), (37, 0.282), (38, 0.015), (39, -0.064), (40, -0.08), (41, 0.041), (42, 0.222), (43, 0.238), (44, -0.312), (45, -0.016), (46, 0.033), (47, 0.418), (48, -0.287), (49, -0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93823981 <a title="37-lsi-1" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>Author: Shantanu Chakrabartty, Gert Cauwenberghs</p><p>Abstract: Many classiﬁcation tasks require estimation of output class probabilities for use as conﬁdence scores or for inference integrated with other models. Probability estimates derived from large margin classiﬁers such as support vector machines (SVMs) are often unreliable. We extend SVM large margin classiﬁcation to GiniSVM maximum entropy multi-class probability regression. GiniSVM combines a quadratic (Gini-Simpson) entropy based agnostic model with a kernel based similarity model. A form of Huber loss in the GiniSVM primal formulation elucidates a connection to robust estimation, further corroborated by the impulsive noise ﬁltering property of the reverse water-ﬁlling procedure to arrive at normalized classiﬁcation margins. The GiniSVM normalized classiﬁcation margins directly provide estimates of class conditional probabilities, approximating kernel logistic regression (KLR) but at reduced computational cost. As with other SVMs, GiniSVM produces a sparse kernel expansion and is trained by solving a quadratic program under linear constraints. GiniSVM training is efﬁciently implemented by sequential minimum optimization or by growth transformation on probability functions. Results on synthetic and benchmark data, including speaker veriﬁcation and face detection data, show improved classiﬁcation performance and increased tolerance to imprecision over soft-margin SVM and KLR. Keywords: support vector machines, large margin classiﬁers, kernel regression, probabilistic models, quadratic entropy, Gini index, growth transformation</p><p>2 0.26458964 <a title="37-lsi-2" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>Author: Gaëlle Loosli, Stéphane Canu</p><p>Abstract: In a recently published paper in JMLR, Tsang et al. (2005) present an algorithm for SVM called Core Vector Machines (CVM) and illustrate its performances through comparisons with other SVM solvers. After reading the CVM paper we were surprised by some of the reported results. In order to clarify the matter, we decided to reproduce some of the experiments. It turns out that to some extent, our results contradict those reported. Reasons of these different behaviors are given through the analysis of the stopping criterion. Keywords: SVM, CVM, large scale, KKT gap, stopping condition, stopping criteria</p><p>3 0.23651841 <a title="37-lsi-3" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>Author: Charles Sutton, Andrew McCallum, Khashayar Rohanimanesh</p><p>Abstract: In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random ﬁelds (DCRFs), a generalization of linear-chain conditional random ﬁelds (CRFs) in which each time slice contains a set of state variables and edges—a distributed state representation as in dynamic Bayesian networks (DBNs)—and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data. In addition to maximum conditional likelihood, we present two alternative approaches for training DCRFs: marginal likelihood training, for when we are primarily interested in predicting only a subset of the variables, and cascaded training, for when we have a distinct data set for each state variable, as in transfer learning. We evaluate marginal training and cascaded training on both synthetic data and real-world text data, ﬁnding that marginal training can improve accuracy when uncertainty exists over the latent variables, and that for transfer learning, a DCRF trained in a cascaded fashion performs better than a linear-chain CRF that predicts the ﬁnal task directly. Keywords: conditional random ﬁelds, graphical models, sequence labeling</p><p>4 0.21749628 <a title="37-lsi-4" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>Author: Yann Guermeur</p><p>Abstract: In the context of discriminant analysis, Vapnik’s statistical learning theory has mainly been developed in three directions: the computation of dichotomies with binary-valued functions, the computation of dichotomies with real-valued functions, and the computation of polytomies with functions taking their values in ﬁnite sets, typically the set of categories itself. The case of classes of vectorvalued functions used to compute polytomies has seldom been considered independently, which is unsatisfactory, for three main reasons. First, this case encompasses the other ones. Second, it cannot be treated appropriately through a na¨ve extension of the results devoted to the computation of ı dichotomies. Third, most of the classiﬁcation problems met in practice involve multiple categories. In this paper, a VC theory of large margin multi-category classiﬁers is introduced. Central in this theory are generalized VC dimensions called the γ-Ψ-dimensions. First, a uniform convergence bound on the risk of the classiﬁers of interest is derived. The capacity measure involved in this bound is a covering number. This covering number can be upper bounded in terms of the γ-Ψdimensions thanks to generalizations of Sauer’s lemma, as is illustrated in the speciﬁc case of the scale-sensitive Natarajan dimension. A bound on this latter dimension is then computed for the class of functions on which multi-class SVMs are based. This makes it possible to apply the structural risk minimization inductive principle to those machines. Keywords: multi-class discriminant analysis, large margin classiﬁers, uniform strong laws of large numbers, generalized VC dimensions, multi-class SVMs, structural risk minimization inductive principle, model selection</p><p>5 0.1744065 <a title="37-lsi-5" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We present a uniﬁed and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including 1 , 2 , 2 , and 1 + 2 style regularization. We propose an algorithm solving a large and 2 2 general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and uniﬁes techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models. Keywords: maximum entropy, density estimation, regularization, iterative scaling, species distribution modeling</p><p>6 0.15618446 <a title="37-lsi-6" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>7 0.15520239 <a title="37-lsi-7" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>8 0.14741445 <a title="37-lsi-8" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>9 0.14691682 <a title="37-lsi-9" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>10 0.13602655 <a title="37-lsi-10" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>11 0.12402725 <a title="37-lsi-11" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>12 0.12179417 <a title="37-lsi-12" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>13 0.1193772 <a title="37-lsi-13" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>14 0.11456252 <a title="37-lsi-14" href="./jmlr-2007-Covariate_Shift_Adaptation_by_Importance_Weighted_Cross_Validation.html">25 jmlr-2007-Covariate Shift Adaptation by Importance Weighted Cross Validation</a></p>
<p>15 0.11291678 <a title="37-lsi-15" href="./jmlr-2007-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">80 jmlr-2007-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>16 0.11042731 <a title="37-lsi-16" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>17 0.10814403 <a title="37-lsi-17" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>18 0.10127522 <a title="37-lsi-18" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>19 0.1004025 <a title="37-lsi-19" href="./jmlr-2007-Concave_Learners_for_Rankboost.html">23 jmlr-2007-Concave Learners for Rankboost</a></p>
<p>20 0.10010449 <a title="37-lsi-20" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.014), (8, 0.017), (9, 0.365), (10, 0.023), (12, 0.026), (15, 0.022), (22, 0.01), (28, 0.06), (40, 0.059), (45, 0.016), (48, 0.043), (60, 0.058), (80, 0.015), (85, 0.048), (98, 0.125)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.70329988 <a title="37-lda-1" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>Author: Shantanu Chakrabartty, Gert Cauwenberghs</p><p>Abstract: Many classiﬁcation tasks require estimation of output class probabilities for use as conﬁdence scores or for inference integrated with other models. Probability estimates derived from large margin classiﬁers such as support vector machines (SVMs) are often unreliable. We extend SVM large margin classiﬁcation to GiniSVM maximum entropy multi-class probability regression. GiniSVM combines a quadratic (Gini-Simpson) entropy based agnostic model with a kernel based similarity model. A form of Huber loss in the GiniSVM primal formulation elucidates a connection to robust estimation, further corroborated by the impulsive noise ﬁltering property of the reverse water-ﬁlling procedure to arrive at normalized classiﬁcation margins. The GiniSVM normalized classiﬁcation margins directly provide estimates of class conditional probabilities, approximating kernel logistic regression (KLR) but at reduced computational cost. As with other SVMs, GiniSVM produces a sparse kernel expansion and is trained by solving a quadratic program under linear constraints. GiniSVM training is efﬁciently implemented by sequential minimum optimization or by growth transformation on probability functions. Results on synthetic and benchmark data, including speaker veriﬁcation and face detection data, show improved classiﬁcation performance and increased tolerance to imprecision over soft-margin SVM and KLR. Keywords: support vector machines, large margin classiﬁers, kernel regression, probabilistic models, quadratic entropy, Gini index, growth transformation</p><p>2 0.41646731 <a title="37-lda-2" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We present a uniﬁed and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including 1 , 2 , 2 , and 1 + 2 style regularization. We propose an algorithm solving a large and 2 2 general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and uniﬁes techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models. Keywords: maximum entropy, density estimation, regularization, iterative scaling, species distribution modeling</p><p>3 0.41555354 <a title="37-lda-3" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>4 0.41181362 <a title="37-lda-4" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>Author: Jean-Yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, ﬁrst, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester (1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand (see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds. Keywords: statistical learning theory, PAC-Bayes theorems, generalization error bounds</p><p>5 0.40859169 <a title="37-lda-5" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>6 0.40750894 <a title="37-lda-6" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>7 0.40606368 <a title="37-lda-7" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>8 0.4051525 <a title="37-lda-8" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>9 0.40503687 <a title="37-lda-9" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>10 0.40405041 <a title="37-lda-10" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>11 0.40321752 <a title="37-lda-11" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>12 0.40115908 <a title="37-lda-12" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>13 0.40105215 <a title="37-lda-13" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>14 0.39926657 <a title="37-lda-14" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>15 0.39847934 <a title="37-lda-15" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>16 0.39836693 <a title="37-lda-16" href="./jmlr-2007-Stagewise_Lasso.html">77 jmlr-2007-Stagewise Lasso</a></p>
<p>17 0.39771566 <a title="37-lda-17" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>18 0.39649802 <a title="37-lda-18" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>19 0.39525968 <a title="37-lda-19" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>20 0.39362609 <a title="37-lda-20" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
