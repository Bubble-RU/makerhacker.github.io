<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-12" href="#">jmlr2007-12</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</h1>
<br/><p>Source: <a title="jmlr-2007-12-pdf" href="http://jmlr.org/papers/volume8/feldman07a/feldman07a.pdf">pdf</a></p><p>Author: Vitaly Feldman</p><p>Abstract: We consider the problems of attribute-efﬁcient PAC learning of two well-studied concept classes: parity functions and DNF expressions over {0, 1}n . We show that attribute-efﬁcient learning of parities with respect to the uniform distribution is equivalent to decoding high-rate random linear codes from low number of errors, a long-standing open problem in coding theory. This is the ﬁrst evidence that attribute-efﬁcient learning of a natural PAC learnable concept class can be computationally hard. An algorithm is said to use membership queries (MQs) non-adaptively if the points at which the algorithm asks MQs do not depend on the target concept. Using a simple non-adaptive parity learning algorithm and a modiﬁcation of Levin’s algorithm for locating a weakly-correlated parity due to Bshouty et al. (1999), we give the ﬁrst non-adaptive and attribute-efﬁcient algorithm for ˜ learning DNF with respect to the uniform distribution. Our algorithm runs in time O(ns4 /ε) and ˜ uses O(s4 · log2 n/ε) non-adaptive MQs, where s is the number of terms in the shortest DNF representation of the target concept. The algorithm improves on the best previous algorithm for learning DNF (of Bshouty et al., 1999) and can also be easily modiﬁed to tolerate random persistent classiﬁcation noise in MQs. Keywords: attribute-efﬁcient, non-adaptive, membership query, DNF, parity function, random linear code</p><p>Reference: <a title="jmlr-2007-12-reference" href="../jmlr2007_reference/jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  School of Engineering and Applied Sciences Harvard University Cambridge, MA 02138  Editor: Peter Auer  Abstract We consider the problems of attribute-efﬁcient PAC learning of two well-studied concept classes: parity functions and DNF expressions over {0, 1}n . [sent-3, score-0.428]
</p><p>2 We show that attribute-efﬁcient learning of parities with respect to the uniform distribution is equivalent to decoding high-rate random linear codes from low number of errors, a long-standing open problem in coding theory. [sent-4, score-0.547]
</p><p>3 Using a simple non-adaptive parity learning algorithm and a modiﬁcation of Levin’s algorithm for locating a weakly-correlated parity due to Bshouty et al. [sent-7, score-0.712]
</p><p>4 Keywords: attribute-efﬁcient, non-adaptive, membership query, DNF, parity function, random linear code  1. [sent-12, score-0.525]
</p><p>5 Introduction The problems of PAC learning parity functions and DNF expressions are among the most fundamental and well-studied problems in machine learning theory. [sent-13, score-0.4]
</p><p>6 F ELDMAN  Learning of DNF expressions and attribute-efﬁcient learning of parities from random examples with respect to the uniform distribution are both long-standing challenges in learning theory. [sent-24, score-0.421]
</p><p>7 The most well-studied such model is one in which a membership query oracle is given to the learner in addition to the example oracle. [sent-26, score-0.299]
</p><p>8 Jackson (1997) gave the ﬁrst algorithm that learns DNF from membership queries (MQs) under the uniform distribution and later Bshouty, Jackson, and Tamon (1999) gave a more efﬁcient and attribute-efﬁcient algorithm for learning DNF in the same setting. [sent-28, score-0.323]
</p><p>9 The ﬁrst algorithm for attribute-efﬁcient learning of parities using MQs is due to Blum et al. [sent-29, score-0.305]
</p><p>10 An immediate advantage of this model (over the usual MQ model) is the fact that the queries to the membership oracle can be parallelized. [sent-37, score-0.352]
</p><p>11 (1995) were the ﬁrst to ask whether parities are learnable attribute-efﬁciently (in the related on-line mistake-bound model). [sent-55, score-0.38]
</p><p>12 They also presented the ﬁrst algorithm to learn parity functions attribute-efﬁciently using MQs. [sent-56, score-0.356]
</p><p>13 (1997) gave several algorithms for attribute-efﬁcient learning of parities that again used adaptiveness in an essential way. [sent-63, score-0.335]
</p><p>14 naMQ algorithm for learning parities based on BCH error-correcting codes. [sent-65, score-0.305]
</p><p>15 When learning the class of parities on at most k variables his algorithm has running time of O(kn) and uses O(k log n) non-adaptive MQs. [sent-66, score-0.436]
</p><p>16 Little previous work has been published on attribute-efﬁcient learning of parities from random examples only. [sent-68, score-0.337]
</p><p>17 They prove that parity functions on at most k variables are learnable 1 in polynomial time using O(n1− k log n) examples. [sent-70, score-0.545]
</p><p>18 naMQ learning of parities (Theorem 9) and provide a transformation that converts a non-adaptive parity learning algorithm into an algorithm for ﬁnding signiﬁcant Fourier coefﬁcients of a function while preserving attribute-efﬁciency and nonadaptiveness (Theorem 13). [sent-86, score-0.661]
</p><p>19 Thus we may consider this equivalence as evidence of the hardness of attribute-efﬁcient learning of parities from random examples only. [sent-101, score-0.364]
</p><p>20 The connection between attribute-efﬁcient learning of parities by membership queries and linear codes was earlier observed by Hofmeister (1999). [sent-104, score-0.553]
</p><p>21 His result allows to derive attribute-efﬁcient parity learning algorithms from efﬁciently decodable linear codes with appropriate parameters. [sent-105, score-0.412]
</p><p>22 The restriction to the uniform distribution allows us to prove the connection in the other direction, giving the above-mentioned negative result for attribute-efﬁcient learning of parities from random examples only. [sent-107, score-0.377]
</p><p>23 In Section 3, we give the required background on binary linear codes and prove the equivalence between attributeefﬁcient learning of parities from random uniform examples and decoding high-rate random linear codes from a low number of errors. [sent-110, score-0.602]
</p><p>24 Section 5 gives a way to convert a non-adaptive parity learning algorithm into an algorithm for ﬁnding signiﬁcant Fourier coefﬁcients of a function while preserving attribute-efﬁciency and non-adaptiveness, yielding an ae. [sent-113, score-0.356]
</p><p>25 Our main interest are the classes of parity functions and DNF expressions. [sent-139, score-0.356]
</p><p>26 A parity function χa (x) for a vector a ∈ {0, 1}n is deﬁned as χa (x) = (−1)a·x . [sent-140, score-0.356]
</p><p>27 We refer to the vector associated with a parity function as its index and the Hamming weight of the vector as the length of the parity function. [sent-141, score-0.712]
</p><p>28 We denote the concept class of parity functions {χ a | a ∈ {0, 1}n } by PAR and the class of all the parities of length at most k by PAR(k). [sent-142, score-0.689]
</p><p>29 We represent a parity function by listing all the variables on which it depends. [sent-143, score-0.356]
</p><p>30 This representation for a parity of length k requires θ(k log n) bits. [sent-144, score-0.438]
</p><p>31 In this model, for a concept c and distribution D over X, an example oracle EXD (c) is an oracle that upon request returns an example x, c(x) where x is chosen randomly with respect to D , independently of any previous examples. [sent-149, score-0.348]
</p><p>32 A membership query oracle MEM(c) is the oracle that, given any point x ∈ {0, 1} n , returns the value c(x). [sent-162, score-0.459]
</p><p>33 In this model for any η ≤ 1/2 called the noise rate the regular example oracle EX D (c) is replaced with the faulty oracle EXη (c). [sent-173, score-0.359]
</p><p>34 2 F OURIER T RANSFORM The Fourier transform is a technique for learning with respect to the uniform distribution (primarily) based on the fact that the set of all parity functions {χ a (x)}a∈{0,1}n forms an orthonormal basis of the linear space of real-valued function over {0, 1} n . [sent-185, score-0.396]
</p><p>35 (1986) imply that if one-way functions exist then the concept class of all polynomial circuits is not learnable even with respect to U and with access to a MQ oracle (Kearns and Valiant, 1994). [sent-213, score-0.353]
</p><p>36 Similarly, by placing the encoding of the circuit in some location that is encoded in a ﬁxed location, one can create a function class learnable by adaptive membership queries but not learnable by the non-adaptive ones (if one-way functions exist). [sent-215, score-0.374]
</p><p>37 Learning of Parities and Binary Linear Codes In this section we show that attribute-efﬁcient learning of parities with respect to the uniform distribution from random examples only is likely to be hard by proving that it is equivalent to an open 1437  F ELDMAN  problem in coding theory. [sent-218, score-0.41]
</p><p>38 Unlike in the rest of the paper in this section and the following section ˙ parity functions will be functions to {0, 1}. [sent-219, score-0.356]
</p><p>39 (2006) have proved that when learning parities from random and uniform examples, random classiﬁcation noise of rate η is as hard as adversarial noise of rate η (up to a polynomial blowup in the running time). [sent-241, score-0.568]
</p><p>40 The only known non-trivial algorithm for learning parities with noise is a slightly subexponential algorithm by Blum et al. [sent-242, score-0.344]
</p><p>41 2 The Reduction The equivalence of attribute-efﬁcient learning of parities with respect to the uniform distribution and decoding of random linear codes relies on two simple lemmas. [sent-255, score-0.514]
</p><p>42 Then A learns PAR(w) over {0, 1} m given the values of an unknown parity on the columns of H. [sent-260, score-0.387]
</p><p>43 Therefore ﬁnding an error vector e of weight at most w using the syndrome yH is the same as ﬁnding a parity of length at most w given the values of the unknown parity on the columns of H. [sent-266, score-0.769]
</p><p>44 naMQ algorithm for learning parities that uses the columns of the parity check matrix of BCH code as MQs. [sent-268, score-0.711]
</p><p>45 We note that the converse of this lemma is only true if the learning algorithm is proper, that is, produces a parity function in PAR(w) as a hypothesis. [sent-269, score-0.403]
</p><p>46 We can now prove that decoding of random linear codes implies attribute-efﬁcient learning of parities from random examples only. [sent-310, score-0.506]
</p><p>47 ˙ Proof Let χe ∈ PAR(w) be the unknown parity function and z1 , z2 , . [sent-313, score-0.356]
</p><p>48 By the deﬁnition of x, e = xG ⊕ y, giving us the desired parity function. [sent-324, score-0.356]
</p><p>49 Therefore if the number of errors that RandDec can correct is α errors for some constant α > 0 then the sample complexity of learning a parity of at least w = d length at most w over m variables would equal O(w1/α log m). [sent-330, score-0.438]
</p><p>50 We have noted previously that using a parity learning algorithm to obtain a syndrome decoding algorithm requires the parity learning algorithm to be proper. [sent-332, score-0.85]
</p><p>51 When a distribution over examples is not restricted it is unknown whether proper learning of parities is harder than non-proper. [sent-333, score-0.305]
</p><p>52 Fortunately, when learning with respect to the uniform distribution any learning algorithm for parities can be converted to a proper and exact one (that is, with a hypothesis equal to the target function). [sent-334, score-0.378]
</p><p>53 Let h be the output of A when running on an unknown parity χe ∈ PAR(k) with ε = 1/5. [sent-339, score-0.405]
</p><p>54 This means that making O(k log n) MQs will take O(nk log n log (k log n)) = O(nk) steps. [sent-348, score-0.328]
</p><p>55 We can now assume that algorithms for learning parity with respect to the uniform distribution are proper and exact (and in particular do not require parameter ε) and use this to obtain the other direction of the equivalence. [sent-350, score-0.396]
</p><p>56 naMQ algorithm for learning parities is due to Hofmeister (1999) and is a deterministic algorithm based on constructing and decoding of BCH binary linear codes (see also Section 3. [sent-369, score-0.442]
</p><p>57 The basic idea of our algorithm is to use a distribution over {0, 1} n for which each attribute is correlated with the parity function if and only if it is present in the parity. [sent-372, score-0.407]
</p><p>58 4k 4k  Our second observation is that for any set of indices B ⊆ [n] and the corresponding parity function ˙ χb , |B| 1 ˙ PrD 1 [χb (x) = 1] ≤ 1 − PrD 1 [∀i ∈ B, xi = 0] = 1 − (1 − )|B| ≤ . [sent-379, score-0.356]
</p><p>59 Therefore ﬁnding a signiﬁcant Fourier coefﬁcient of f is sometimes called weak parity learning (Jackson, 1997). [sent-402, score-0.46]
</p><p>60 It can also be interpreted as a learning algorithm for parities in the agnostic learning framework of Haussler (1992) and Kearns et al. [sent-403, score-0.33]
</p><p>61 Given θ > 0 and access to MEM( f ), the weak parity learning problem consists of ﬁnding a vector z such that fˆ(z) is θ/2-heavy. [sent-407, score-0.518]
</p><p>62 We will only consider algorithms for weak parity learning that are efﬁcient, that is, produce the result in time polynomial in n, and θ−1 . [sent-408, score-0.492]
</p><p>63 In addition we are interested in weak parity learning algorithms that are attribute-efﬁcient. [sent-409, score-0.46]
</p><p>64 1443  F ELDMAN  We follow the presentation of Levin’s weak parity algorithm given by Bshouty et al. [sent-411, score-0.46]
</p><p>65 Another key element of the weak parity algorithm is the following equation (Bshouty et al. [sent-422, score-0.46]
</p><p>66 Therefore only a non-adaptive membership query algorithm for learning parities can be used. [sent-434, score-0.444]
</p><p>67 naMQ algorithm for learning parities that runs in time t(n, k) and uses q(n, k) MQs. [sent-440, score-0.305]
</p><p>68 There exists an attribute-efﬁcient and non-adaptive algorithm AEBoundedSieveB (θ, k) that, with probability at least 1 − δ, solves the weak parity learning problem. [sent-441, score-0.46]
</p><p>69 If the output of B (k) is a parity function χa of length at most k then we test that (i) : | fR (z)| ≥ 3θ/4 and (ii) : aRT = z. [sent-448, score-0.356]
</p><p>70 By plugging AEParityStat(k) algorithm (Theorem 9) into Theorem 13 we obtain our weak parity learning algorithm. [sent-482, score-0.46]
</p><p>71 Corollary 14 There exists an attribute-efﬁcient and non-adaptive weak parity learning algorithm AEBoundedSieve(θ, k) that succeeds with probability at least 1 − δ, runs in time ˜ ˜ O nk2 θ−2 log (1/δ) , and asks O k2 log2 n · θ−2 log (1/δ) MQs. [sent-483, score-0.705]
</p><p>72 Jackson (1997) has proved that for every distribution D , every DNF formula f has a parity function that weakly approximates f with respect to D . [sent-484, score-0.391]
</p><p>73 A reﬁned version of this claim by Bshouty and Feldman (2002) shows that f has a short parity that weakly approximates f if the distribution is not too far from the uniform. [sent-485, score-0.391]
</p><p>74 Lemma 15 For any Boolean function f of DNF-size s and a distribution D over {0, 1} n there exists a parity function χa such that |ED [ f χa ]| ≥  1 and wt(a) ≤ log ((2s + 1)L∞ (2n D )) . [sent-487, score-0.438]
</p><p>75 Proof Lemma 15 implies that there exists a parity χa on at most log (2s + 1) variables such that 1 1 |EU [ f χa ]| = | fˆ(a)| ≥ 2s+1 . [sent-491, score-0.438]
</p><p>76 The parity χa or its negation 1 1 ( 2 − 4(2s+1) )-approximates f . [sent-494, score-0.381]
</p><p>77 1 Weak DNF Learning with Respect to Any Distribution The ﬁrst step in Jackson’s approach is to generalize a weak parity algorithm to work for any realvalued function. [sent-506, score-0.46]
</p><p>78 As in Jackson’s work we use the generalized weak parity algorithm to obtain an algorithm that weakly learns DNF expressions with respect to any distribution. [sent-517, score-0.57]
</p><p>79 Furthermore, 2 s WeakDNF(s, B) ˜ • runs in time O(ns2 B log (1/δ)); ˜ • asks O(s2 log2 n · B log (1/δ)) non-adaptive MQs; • returns a parity function of length at most O(log (sB)) or its negation. [sent-520, score-0.601]
</p><p>80 The WeakDNF algorithm also requires oracle access to Di (x) and therefore we will use a boosting algorithm in the same way. [sent-551, score-0.32]
</p><p>81 Accordingly the distribution function generated at stage i depends on random coin ﬂips and the ﬁnal hypothesis is a majority vote over hypotheses from the weak learner and random coin ﬂips. [sent-572, score-0.427]
</p><p>82 Speciﬁcally, we note that evaluation of the distribution function Di (x) at boosting stage i involves evaluation of i − 1 previous hypotheses on x and therefore, in a general case, for a sample of size q will require Ω(i · q) steps, making the last stages of boosting noticeably slower. [sent-587, score-0.34]
</p><p>83 The idea of the speed-up is to use Equation (5) together with the facts that weak hypotheses are parities and MQs of WeakDNF come from a “small” number of low-dimension linear subspaces. [sent-589, score-0.49]
</p><p>84 Let g be a function that is equal to a linear combination of short parity functions. [sent-590, score-0.356]
</p><p>85 WeakDNF produces a short parity function (or its negation) as the hypothesis. [sent-657, score-0.356]
</p><p>86 Then, given MEM( f ) and ,i an oracle access to NFilt (x), the set of pairs S = { x, λD Comb (x) | x ∈ W } for some constant ,i ˜ λ ∈ [2/3, 4/3], can be computed, with probability at least 1 − δ, in time O((i + s2 B) log2 n log (1/δ)). [sent-667, score-0.3]
</p><p>87 By revisiting the proof of Theorem 13 we can see that our weak parity algorithm asks queries on Y = {pR | p ∈ {0, 1} m } for a randomly chosen R and then for each query z of a ae. [sent-669, score-0.698]
</p><p>88 naMQ parity algorithm it asks queries on points of the set Yz = {z ⊕ y | y ∈ Y }. [sent-670, score-0.542]
</p><p>89 ˜ linear subspaces of dimension m = log T for T = O(s Our second observation is given by Theorem 18 and states that for each j ≤ i, χ c j is a parity on at most log sB variables. [sent-674, score-0.52]
</p><p>90 Together with the ﬁrst observation this implies that computing NF1 (x) for all points in W can be done in time ˜ ˜ ˜ O(log2 (sB) log n log (1/δ))O(i · log (sB) log n + s2 B log n) = O((i + s2 B) log2 n log (1/δ)) . [sent-680, score-0.492]
</p><p>91 WeakDNF(s, B) 1 returns ( 2 − Ω( 1 ))-approximate hypotheses and therefore each hypothesis generated by F1 is a mas jority vote of O(γ− 2) = O(s2 ) short parities (or their negations). [sent-692, score-0.419]
</p><p>92 A majority vote of these parities and their negations is simply the sign of their sum, and in particular is determined by a linear combination of parity functions. [sent-693, score-0.661]
</p><p>93 • The use of attribute-efﬁcient weak learning improves the total sample complexity from ˜ ˜ O ns4 /ε2 to O s4 log2 n/ε2 and the same running time is achieved without assumptions on the MQ oracle (see Theorem 16). [sent-719, score-0.313]
</p><p>94 The goal of a weak DNF learning algorithm at stage i of boosting is to ﬁnd a parity correlated with the function 2n Di (x) f (x) given an oracle access to values of Di (x) and the oracle for f with noise of rate η < 1/2 instead of MEM( f ). [sent-752, score-1.034]
</p><p>95 Therefore Lemma 27 gives a way to ﬁnd heavy Fourier coefﬁcients using an oracle for Φ η (x) instead of the membership query oracle for f (x). [sent-764, score-0.459]
</p><p>96 Given the oracle for D Comb (x, b) and ,i ,i oracle access to Φη (x) we use AEBoundedSieveRV(θ, k, V) on Ψg,η (x) in the same way it was used on ψ(x) by WeakDNF(s, B) (see the proof of Theorem 18). [sent-773, score-0.378]
</p><p>97 They also imply that ˜ WeakDNF(s, B) will produce parities on log (s2 /(ε(1 − 2η))) variables (this change is absorbed by O notation). [sent-779, score-0.387]
</p><p>98 Conclusions and Open Problems In this work we have demonstrated equivalence of attribute-efﬁcient learning of parities from random and uniform examples and decoding of random linear binary codes. [sent-781, score-0.49]
</p><p>99 Our results show that some of the most important concepts classes that are learnable attribute efﬁciently with respect to the unform distribution using membership queries are also learnable by signiﬁcantly weaker non-adaptive MQs. [sent-785, score-0.393]
</p><p>100 Noise-tolerant learning, the parity problem, and the statistical query model. [sent-822, score-0.408]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('dnf', 0.399), ('parity', 0.356), ('parities', 0.305), ('mqs', 0.277), ('bshouty', 0.197), ('fourier', 0.189), ('weakdnf', 0.184), ('comb', 0.176), ('oracle', 0.16), ('jackson', 0.156), ('eldman', 0.126), ('nfilt', 0.126), ('embership', 0.117), ('prd', 0.109), ('queries', 0.105), ('weak', 0.104), ('boosting', 0.102), ('mem', 0.099), ('fficient', 0.099), ('ueries', 0.099), ('membership', 0.087), ('feldman', 0.084), ('log', 0.082), ('asks', 0.081), ('hypotheses', 0.081), ('decoding', 0.081), ('par', 0.076), ('vn', 0.076), ('learnable', 0.075), ('aenalearndnf', 0.075), ('mq', 0.07), ('hofmeister', 0.067), ('persistent', 0.064), ('boolean', 0.061), ('randdec', 0.059), ('access', 0.058), ('levin', 0.058), ('syndrome', 0.057), ('codes', 0.056), ('stage', 0.055), ('wl', 0.054), ('query', 0.052), ('attribute', 0.051), ('code', 0.05), ('fft', 0.05), ('klivans', 0.05), ('di', 0.049), ('running', 0.049), ('lemma', 0.047), ('yh', 0.046), ('coef', 0.046), ('coin', 0.045), ('xg', 0.044), ('expressions', 0.044), ('hamming', 0.043), ('pr', 0.042), ('bch', 0.042), ('pac', 0.041), ('sb', 0.041), ('servedio', 0.041), ('uniform', 0.04), ('earning', 0.04), ('noise', 0.039), ('gr', 0.038), ('freund', 0.038), ('randomized', 0.036), ('goldreich', 0.035), ('weakly', 0.035), ('fr', 0.035), ('kearns', 0.034), ('aeboundedsieverv', 0.034), ('aeparitystat', 0.034), ('estexprel', 0.034), ('executions', 0.034), ('guijarro', 0.034), ('un', 0.033), ('estimations', 0.033), ('hypothesis', 0.033), ('coding', 0.033), ('adaptive', 0.032), ('polynomial', 0.032), ('random', 0.032), ('stoc', 0.032), ('learns', 0.031), ('claimed', 0.03), ('gave', 0.03), ('rank', 0.029), ('concept', 0.028), ('wt', 0.028), ('gh', 0.027), ('corrupted', 0.027), ('hardness', 0.027), ('execution', 0.026), ('agnostic', 0.025), ('fc', 0.025), ('dnfs', 0.025), ('hellerstein', 0.025), ('negation', 0.025), ('uehara', 0.025), ('vitaly', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="12-tfidf-1" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Vitaly Feldman</p><p>Abstract: We consider the problems of attribute-efﬁcient PAC learning of two well-studied concept classes: parity functions and DNF expressions over {0, 1}n . We show that attribute-efﬁcient learning of parities with respect to the uniform distribution is equivalent to decoding high-rate random linear codes from low number of errors, a long-standing open problem in coding theory. This is the ﬁrst evidence that attribute-efﬁcient learning of a natural PAC learnable concept class can be computationally hard. An algorithm is said to use membership queries (MQs) non-adaptively if the points at which the algorithm asks MQs do not depend on the target concept. Using a simple non-adaptive parity learning algorithm and a modiﬁcation of Levin’s algorithm for locating a weakly-correlated parity due to Bshouty et al. (1999), we give the ﬁrst non-adaptive and attribute-efﬁcient algorithm for ˜ learning DNF with respect to the uniform distribution. Our algorithm runs in time O(ns4 /ε) and ˜ uses O(s4 · log2 n/ε) non-adaptive MQs, where s is the number of terms in the shortest DNF representation of the target concept. The algorithm improves on the best previous algorithm for learning DNF (of Bshouty et al., 1999) and can also be easily modiﬁed to tolerate random persistent classiﬁcation noise in MQs. Keywords: attribute-efﬁcient, non-adaptive, membership query, DNF, parity function, random linear code</p><p>2 0.20298149 <a title="12-tfidf-2" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ariel Elbaz, Homin K. Lee, Rocco A. Servedio, Andrew Wan</p><p>Abstract: We consider a natural framework of learning from correlated data, in which successive examples used for learning are generated according to a random walk over the space of possible examples. A recent paper by Bshouty et al. (2003) shows that the class of polynomial-size DNF formulas is efﬁciently learnable in this random walk model; this result suggests that the Random Walk model is more powerful than comparable standard models of learning from independent examples, in which similarly efﬁcient DNF learning algorithms are not known. We give strong evidence that the Random Walk model is indeed more powerful than the standard model, by showing that if any cryptographic one-way function exists (a universally held belief in cryptography), then there is a class of functions that can be learned efﬁciently in the Random Walk setting but not in the standard setting where all examples are independent. Keywords: random walks, uniform distribution learning, cryptographic hardness, correlated data, PAC learning</p><p>3 0.10265323 <a title="12-tfidf-3" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>Author: Manu Chhabra, Robert A. Jacobs, Daniel Štefankovič</p><p>Abstract: In a search problem, an agent uses the membership oracle of a target concept to ﬁnd a positive example of the concept. In a shaped search problem the agent is aided by a sequence of increasingly restrictive concepts leading to the target concept (analogous to behavioral shaping). The concepts are given by membership oracles, and the agent has to ﬁnd a positive example of the target concept while minimizing the total number of oracle queries. We show that for the concept class of intervals on the real line an agent using a bounded number of queries per oracle exists. In contrast, for the concept class of unions of two intervals on the real line no agent with a bounded number of queries per oracle exists. We then investigate the (amortized) number of queries per oracle needed for the shaped search problem over other concept classes. We explore the following methods to obtain efﬁcient agents. For axis-parallel rectangles we use a bootstrapping technique to obtain gradually better approximations of the target concept. We show that given rectangles R ⊆ A ⊆ R d one can obtain a rectangle A ⊇ R with vol(A )/vol(R) ≤ 2, using only O(d · vol(A)/vol(R)) random samples from A. For ellipsoids of bounded eccentricity in Rd we analyze a deterministic ray-shooting process which uses a sequence of rays to get close to the centroid. Finally, we use algorithms for generating random points in convex bodies (Dyer et al., 1991; Kannan et al., 1997) to give a randomized agent for the concept class of convex bodies. In the ﬁnal section, we explore connections between our bootstrapping method and active learning. Speciﬁcally, we use the bootstrapping technique for axis-parallel rectangles to active learn axis-parallel rectangles under the uniform distribution in O(d ln(1/ε)) labeled samples. Keywords: computational learning theory, behavioral shaping, active learning</p><p>4 0.063376814 <a title="12-tfidf-4" href="./jmlr-2007-Distances_between_Data_Sets_Based_on_Summary_Statistics.html">27 jmlr-2007-Distances between Data Sets Based on Summary Statistics</a></p>
<p>Author: Nikolaj Tatti</p><p>Abstract: The concepts of similarity and distance are crucial in data mining. We consider the problem of deﬁning the distance between two data sets by comparing summary statistics computed from the data sets. The initial deﬁnition of our distance is based on geometrical notions of certain sets of distributions. We show that this distance can be computed in cubic time and that it has several intuitive properties. We also show that this distance is the unique Mahalanobis distance satisfying certain assumptions. We also demonstrate that if we are dealing with binary data sets, then the distance can be represented naturally by certain parity functions, and that it can be evaluated in linear time. Our empirical tests with real world data show that the distance works well. Keywords: data mining theory, complex data, binary data, itemsets</p><p>5 0.042927057 <a title="12-tfidf-5" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>Author: Peter L. Bartlett, Mikhail Traskin</p><p>Abstract: The risk, or probability of error, of the classiﬁer produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after n1−ε iterations—for sample size n and ε ∈ (0, 1)—the sequence of risks of the classiﬁers it produces approaches the Bayes risk. Keywords: boosting, adaboost, consistency</p><p>6 0.040248182 <a title="12-tfidf-6" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>7 0.039994475 <a title="12-tfidf-7" href="./jmlr-2007-Concave_Learners_for_Rankboost.html">23 jmlr-2007-Concave Learners for Rankboost</a></p>
<p>8 0.037636951 <a title="12-tfidf-8" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>9 0.03581905 <a title="12-tfidf-9" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>10 0.034348942 <a title="12-tfidf-10" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>11 0.032705691 <a title="12-tfidf-11" href="./jmlr-2007-Multi-class_Protein_Classification_Using_Adaptive_Codes.html">57 jmlr-2007-Multi-class Protein Classification Using Adaptive Codes</a></p>
<p>12 0.031939592 <a title="12-tfidf-12" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>13 0.031912837 <a title="12-tfidf-13" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>14 0.031618286 <a title="12-tfidf-14" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>15 0.030268226 <a title="12-tfidf-15" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>16 0.030262819 <a title="12-tfidf-16" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>17 0.029086988 <a title="12-tfidf-17" href="./jmlr-2007-A_Complete_Characterization_of_a_Family_of_Solutions_to_a_Generalized_Fisher_Criterion.html">2 jmlr-2007-A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion</a></p>
<p>18 0.028674589 <a title="12-tfidf-18" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>19 0.028086558 <a title="12-tfidf-19" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>20 0.027819311 <a title="12-tfidf-20" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.184), (1, -0.002), (2, -0.109), (3, -0.053), (4, -0.089), (5, 0.128), (6, 0.07), (7, 0.332), (8, 0.009), (9, -0.211), (10, -0.361), (11, 0.1), (12, 0.155), (13, 0.06), (14, 0.193), (15, -0.105), (16, 0.048), (17, 0.017), (18, -0.071), (19, 0.06), (20, 0.062), (21, -0.119), (22, 0.165), (23, 0.137), (24, 0.002), (25, -0.094), (26, -0.031), (27, 0.003), (28, 0.04), (29, -0.071), (30, 0.014), (31, 0.048), (32, 0.008), (33, -0.024), (34, -0.026), (35, 0.014), (36, 0.034), (37, 0.016), (38, -0.032), (39, -0.017), (40, -0.01), (41, 0.052), (42, -0.047), (43, 0.013), (44, -0.006), (45, -0.052), (46, -0.096), (47, 0.03), (48, 0.108), (49, -0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95188367 <a title="12-lsi-1" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Vitaly Feldman</p><p>Abstract: We consider the problems of attribute-efﬁcient PAC learning of two well-studied concept classes: parity functions and DNF expressions over {0, 1}n . We show that attribute-efﬁcient learning of parities with respect to the uniform distribution is equivalent to decoding high-rate random linear codes from low number of errors, a long-standing open problem in coding theory. This is the ﬁrst evidence that attribute-efﬁcient learning of a natural PAC learnable concept class can be computationally hard. An algorithm is said to use membership queries (MQs) non-adaptively if the points at which the algorithm asks MQs do not depend on the target concept. Using a simple non-adaptive parity learning algorithm and a modiﬁcation of Levin’s algorithm for locating a weakly-correlated parity due to Bshouty et al. (1999), we give the ﬁrst non-adaptive and attribute-efﬁcient algorithm for ˜ learning DNF with respect to the uniform distribution. Our algorithm runs in time O(ns4 /ε) and ˜ uses O(s4 · log2 n/ε) non-adaptive MQs, where s is the number of terms in the shortest DNF representation of the target concept. The algorithm improves on the best previous algorithm for learning DNF (of Bshouty et al., 1999) and can also be easily modiﬁed to tolerate random persistent classiﬁcation noise in MQs. Keywords: attribute-efﬁcient, non-adaptive, membership query, DNF, parity function, random linear code</p><p>2 0.89154565 <a title="12-lsi-2" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ariel Elbaz, Homin K. Lee, Rocco A. Servedio, Andrew Wan</p><p>Abstract: We consider a natural framework of learning from correlated data, in which successive examples used for learning are generated according to a random walk over the space of possible examples. A recent paper by Bshouty et al. (2003) shows that the class of polynomial-size DNF formulas is efﬁciently learnable in this random walk model; this result suggests that the Random Walk model is more powerful than comparable standard models of learning from independent examples, in which similarly efﬁcient DNF learning algorithms are not known. We give strong evidence that the Random Walk model is indeed more powerful than the standard model, by showing that if any cryptographic one-way function exists (a universally held belief in cryptography), then there is a class of functions that can be learned efﬁciently in the Random Walk setting but not in the standard setting where all examples are independent. Keywords: random walks, uniform distribution learning, cryptographic hardness, correlated data, PAC learning</p><p>3 0.4458276 <a title="12-lsi-3" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>Author: Manu Chhabra, Robert A. Jacobs, Daniel Štefankovič</p><p>Abstract: In a search problem, an agent uses the membership oracle of a target concept to ﬁnd a positive example of the concept. In a shaped search problem the agent is aided by a sequence of increasingly restrictive concepts leading to the target concept (analogous to behavioral shaping). The concepts are given by membership oracles, and the agent has to ﬁnd a positive example of the target concept while minimizing the total number of oracle queries. We show that for the concept class of intervals on the real line an agent using a bounded number of queries per oracle exists. In contrast, for the concept class of unions of two intervals on the real line no agent with a bounded number of queries per oracle exists. We then investigate the (amortized) number of queries per oracle needed for the shaped search problem over other concept classes. We explore the following methods to obtain efﬁcient agents. For axis-parallel rectangles we use a bootstrapping technique to obtain gradually better approximations of the target concept. We show that given rectangles R ⊆ A ⊆ R d one can obtain a rectangle A ⊇ R with vol(A )/vol(R) ≤ 2, using only O(d · vol(A)/vol(R)) random samples from A. For ellipsoids of bounded eccentricity in Rd we analyze a deterministic ray-shooting process which uses a sequence of rays to get close to the centroid. Finally, we use algorithms for generating random points in convex bodies (Dyer et al., 1991; Kannan et al., 1997) to give a randomized agent for the concept class of convex bodies. In the ﬁnal section, we explore connections between our bootstrapping method and active learning. Speciﬁcally, we use the bootstrapping technique for axis-parallel rectangles to active learn axis-parallel rectangles under the uniform distribution in O(d ln(1/ε)) labeled samples. Keywords: computational learning theory, behavioral shaping, active learning</p><p>4 0.25387424 <a title="12-lsi-4" href="./jmlr-2007-Distances_between_Data_Sets_Based_on_Summary_Statistics.html">27 jmlr-2007-Distances between Data Sets Based on Summary Statistics</a></p>
<p>Author: Nikolaj Tatti</p><p>Abstract: The concepts of similarity and distance are crucial in data mining. We consider the problem of deﬁning the distance between two data sets by comparing summary statistics computed from the data sets. The initial deﬁnition of our distance is based on geometrical notions of certain sets of distributions. We show that this distance can be computed in cubic time and that it has several intuitive properties. We also show that this distance is the unique Mahalanobis distance satisfying certain assumptions. We also demonstrate that if we are dealing with binary data sets, then the distance can be represented naturally by certain parity functions, and that it can be evaluated in linear time. Our empirical tests with real world data show that the distance works well. Keywords: data mining theory, complex data, binary data, itemsets</p><p>5 0.20881516 <a title="12-lsi-5" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>Author: David Mease, Abraham J. Wyner, Andreas Buja</p><p>Abstract: The standard by which binary classiﬁers are usually judged, misclassiﬁcation error, assumes equal costs of misclassifying the two classes or, equivalently, classifying at the 1/2 quantile of the conditional class probability function P[y = 1|x]. Boosted classiﬁcation trees are known to perform quite well for such problems. In this article we consider the use of standard, off-the-shelf boosting for two more general problems: 1) classiﬁcation with unequal costs or, equivalently, classiﬁcation at quantiles other than 1/2, and 2) estimation of the conditional class probability function P[y = 1|x]. We ﬁrst examine whether the latter problem, estimation of P[y = 1|x], can be solved with LogitBoost, and with AdaBoost when combined with a natural link function. The answer is negative: both approaches are often ineffective because they overﬁt P[y = 1|x] even though they perform well as classiﬁers. A major negative point of the present article is the disconnect between class probability estimation and classiﬁcation. Next we consider the practice of over/under-sampling of the two classes. We present an algorithm that uses AdaBoost in conjunction with Over/Under-Sampling and Jittering of the data (“JOUS-Boost”). This algorithm is simple, yet successful, and it preserves the advantage of relative protection against overﬁtting, but for arbitrary misclassiﬁcation costs and, equivalently, arbitrary quantile boundaries. We then use collections of classiﬁers obtained from a grid of quantiles to form estimators of class probabilities. The estimates of the class probabilities compare favorably to those obtained by a variety of methods across both simulated and real data sets. Keywords: boosting algorithms, LogitBoost, AdaBoost, class probability estimation, over-sampling, under-sampling, stratiﬁcation, data jittering</p><p>6 0.19917688 <a title="12-lsi-6" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>7 0.18500064 <a title="12-lsi-7" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>8 0.17348219 <a title="12-lsi-8" href="./jmlr-2007-Multi-class_Protein_Classification_Using_Adaptive_Codes.html">57 jmlr-2007-Multi-class Protein Classification Using Adaptive Codes</a></p>
<p>9 0.17091253 <a title="12-lsi-9" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>10 0.16925517 <a title="12-lsi-10" href="./jmlr-2007-Concave_Learners_for_Rankboost.html">23 jmlr-2007-Concave Learners for Rankboost</a></p>
<p>11 0.16826321 <a title="12-lsi-11" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>12 0.1668472 <a title="12-lsi-12" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>13 0.16622362 <a title="12-lsi-13" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>14 0.15098515 <a title="12-lsi-14" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>15 0.15044264 <a title="12-lsi-15" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>16 0.14676502 <a title="12-lsi-16" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>17 0.14594334 <a title="12-lsi-17" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>18 0.14285439 <a title="12-lsi-18" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>19 0.14003257 <a title="12-lsi-19" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>20 0.13781904 <a title="12-lsi-20" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.025), (8, 0.017), (10, 0.027), (12, 0.027), (15, 0.014), (16, 0.418), (22, 0.011), (28, 0.086), (40, 0.023), (48, 0.023), (60, 0.036), (66, 0.035), (80, 0.017), (85, 0.054), (98, 0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.67122996 <a title="12-lda-1" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Vitaly Feldman</p><p>Abstract: We consider the problems of attribute-efﬁcient PAC learning of two well-studied concept classes: parity functions and DNF expressions over {0, 1}n . We show that attribute-efﬁcient learning of parities with respect to the uniform distribution is equivalent to decoding high-rate random linear codes from low number of errors, a long-standing open problem in coding theory. This is the ﬁrst evidence that attribute-efﬁcient learning of a natural PAC learnable concept class can be computationally hard. An algorithm is said to use membership queries (MQs) non-adaptively if the points at which the algorithm asks MQs do not depend on the target concept. Using a simple non-adaptive parity learning algorithm and a modiﬁcation of Levin’s algorithm for locating a weakly-correlated parity due to Bshouty et al. (1999), we give the ﬁrst non-adaptive and attribute-efﬁcient algorithm for ˜ learning DNF with respect to the uniform distribution. Our algorithm runs in time O(ns4 /ε) and ˜ uses O(s4 · log2 n/ε) non-adaptive MQs, where s is the number of terms in the shortest DNF representation of the target concept. The algorithm improves on the best previous algorithm for learning DNF (of Bshouty et al., 1999) and can also be easily modiﬁed to tolerate random persistent classiﬁcation noise in MQs. Keywords: attribute-efﬁcient, non-adaptive, membership query, DNF, parity function, random linear code</p><p>2 0.38241285 <a title="12-lda-2" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ariel Elbaz, Homin K. Lee, Rocco A. Servedio, Andrew Wan</p><p>Abstract: We consider a natural framework of learning from correlated data, in which successive examples used for learning are generated according to a random walk over the space of possible examples. A recent paper by Bshouty et al. (2003) shows that the class of polynomial-size DNF formulas is efﬁciently learnable in this random walk model; this result suggests that the Random Walk model is more powerful than comparable standard models of learning from independent examples, in which similarly efﬁcient DNF learning algorithms are not known. We give strong evidence that the Random Walk model is indeed more powerful than the standard model, by showing that if any cryptographic one-way function exists (a universally held belief in cryptography), then there is a class of functions that can be learned efﬁciently in the Random Walk setting but not in the standard setting where all examples are independent. Keywords: random walks, uniform distribution learning, cryptographic hardness, correlated data, PAC learning</p><p>3 0.33387312 <a title="12-lda-3" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>Author: Marta Arias, Roni Khardon, Jérôme Maloberti</p><p>Abstract: The paper introduces L OG A N -H —a system for learning ﬁrst-order function-free Horn expressions from interpretations. The system is based on an algorithm that learns by asking questions and that was proved correct in previous work. The current paper shows how the algorithm can be implemented in a practical system, and introduces a new algorithm based on it that avoids interaction and learns from examples only. The L OG A N -H system implements these algorithms and adds several facilities and optimizations that allow efﬁcient applications in a wide range of problems. As one of the important ingredients, the system includes several fast procedures for solving the subsumption problem, an NP-complete problem that needs to be solved many times during the learning process. We describe qualitative and quantitative experiments in several domains. The experiments demonstrate that the system can deal with varied problems, large amounts of data, and that it achieves good classiﬁcation accuracy. Keywords: inductive logic programming, subsumption, bottom-up learning, learning with queries</p><p>4 0.32594639 <a title="12-lda-4" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>Author: Francesco Dinuzzo, Marta Neve, Giuseppe De Nicolao, Ugo Pietro Gianazza</p><p>Abstract: Support Vector Regression (SVR) for discrete data is considered. An alternative formulation of the representer theorem is derived. This result is based on the newly introduced notion of pseudoresidual and the use of subdifferential calculus. The representer theorem is exploited to analyze the sensitivity properties of ε-insensitive SVR and introduce the notion of approximate degrees of freedom. The degrees of freedom are shown to play a key role in the evaluation of the optimism, that is the difference between the expected in-sample error and the expected empirical risk. In this way, it is possible to deﬁne a C p -like statistic that can be used for tuning the parameters of SVR. The proposed tuning procedure is tested on a simulated benchmark problem and on a real world problem (Boston Housing data set). Keywords: statistical learning, reproducing kernel Hilbert spaces, support vector machines, representer theorem, regularization theory</p><p>5 0.32401794 <a title="12-lda-5" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>Author: Onur C. Hamsici, Aleix M. Martinez</p><p>Abstract: Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere S p−1 . Such representations should then be modeled using spherical distributions. However, the difﬁculty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead—as if the data were represented in R p rather than S p−1 . This opens the question to whether the classiﬁcation results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases (which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to deﬁne optimal classiﬁers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classiﬁers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classiﬁcation of images of objects, gene expression sequences, and text data. Keywords: directional data, spherical distributions, normal distributions, norm normalization, linear and non-linear classiﬁers, computer vision</p><p>6 0.3236205 <a title="12-lda-6" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>7 0.32253504 <a title="12-lda-7" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>8 0.32071602 <a title="12-lda-8" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>9 0.3189207 <a title="12-lda-9" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>10 0.31828028 <a title="12-lda-10" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>11 0.31781766 <a title="12-lda-11" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>12 0.3173593 <a title="12-lda-12" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>13 0.31537461 <a title="12-lda-13" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>14 0.31513685 <a title="12-lda-14" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>15 0.31480068 <a title="12-lda-15" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>16 0.31412628 <a title="12-lda-16" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>17 0.3134234 <a title="12-lda-17" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>18 0.31308138 <a title="12-lda-18" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>19 0.3126049 <a title="12-lda-19" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>20 0.31258452 <a title="12-lda-20" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
