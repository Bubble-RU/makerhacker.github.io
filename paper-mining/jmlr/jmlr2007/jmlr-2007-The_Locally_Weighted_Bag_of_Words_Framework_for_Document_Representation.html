<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-81" href="#">jmlr2007-81</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</h1>
<br/><p>Source: <a title="jmlr-2007-81-pdf" href="http://jmlr.org/papers/volume8/lebanon07a/lebanon07a.pdf">pdf</a></p><p>Author: Guy Lebanon, Yi Mao, Joshua Dillon</p><p>Abstract: The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efﬁcient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks. Keywords: text processing, local smoothing</p><p>Reference: <a title="jmlr-2007-81-reference" href="../jmlr2007_reference/jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In contrast to n-grams, which keep track of frequently occurring patterns independent of their positions, lowbow keeps track of changes in the word histogram as it sweeps through the document from beginning to end. [sent-53, score-1.053]
</p><p>2 The entire lowbow framework may be generalized to deﬁne locally weighted n-grams in a straightforward manner. [sent-56, score-0.807]
</p><p>3 Section 3 describes the mechanics of using the lowbow framework in document modeling. [sent-58, score-0.94]
</p><p>4 2409  L EBANON , M AO AND D ILLON  Deﬁnition 6 The locally weighted bag of words or lowbow representation of the word sequence y is γ(y) = {γµ (y) : µ ∈ [0, 1]} where γµ (y) ∈ PV −1 is the local word histogram at µ deﬁned by [γµ (y)] j =  Z 1 0  ϕ(δc (y))(t, j) Kµ,σ (t)dt. [sent-156, score-1.266]
</p><p>5 j∈V 0  0  ϕ(δc (y))(t, j)Kµ,σ (t) dt =  0  Kµ,σ (t) ∑ ϕ(δc (y))(t, j) dt j∈V  Geometrically, the lowbow representation of documents is equivalent to parameterized curves in the simplex. [sent-158, score-1.176]
</p><p>6 The following theorem establishes the continuity and smoothness of these curves which enables the use of differential geometry in the analysis of the lowbow representation and its properties. [sent-159, score-1.002]
</p><p>7 Theorem 1 The lowbow representation is a continuous and differentiable parameterized curve in the simplex, in both the Euclidean and the Fisher geometry. [sent-160, score-0.947]
</p><p>8 Proof We prove below only the continuity of the lowbow representation. [sent-161, score-0.833]
</p><p>9 It is important to note that the parameterized curve that corresponds to the lowbow representation consists of two parts: the geometric ﬁgure {γµ (y) : µ ∈ [0, 1]} ⊂ PV −1 and the parameterization function µ → γµ (y) that ties the local histogram to a location µ in the normalized document. [sent-167, score-1.056]
</p><p>10 While 2410  T HE L OCALLY W EIGHTED BAG OF W ORDS F RAMEWORK  it is easy to ignore the parameterization function when dealing with parameterized curves, one must be aware that different lowbow representations may share similar geometric ﬁgures but possess different parameterization speeds. [sent-168, score-0.884]
</p><p>11 As the following theorem shows, if σ → ∞ the lowbow curve degenerates into a single point corresponding to the bow representation. [sent-172, score-1.022]
</p><p>12 As a consequence we view the popular bag of words representation (3) as a special case of the lowbow representation. [sent-173, score-1.034]
</p><p>13 Then for σ → ∞, the lowbow curve γ(y) degenerates into a single point corresponding to the bow representation of (3). [sent-175, score-1.063]
</p><p>14 By varying σ between 0 and ∞, the lowbow representation interpolates between these two extreme cases and captures sequential detail at different resolutions. [sent-184, score-0.919]
</p><p>15 Figures 2-3 illustrate the curve resulting from the lowbow representation and its dependency on the kernel scale parameter and the smoothing coefﬁcient. [sent-186, score-1.097]
</p><p>16 It is useful to have a quantitative characterization of the complexity of the lowbow representation as a function of the chosen kernel and σ. [sent-189, score-0.895]
</p><p>17 To this end, the kernel’s complexity, deﬁned below, serves as a bound for variations in the lowbow curve. [sent-190, score-0.807]
</p><p>18 2, c = 1  Figure 2: The curve in P1 resulting from the lowbow representation of the word sequence 1 1 1 2 2 1 1 1 2 1 1 . [sent-269, score-1.013]
</p><p>19 2, c = 1  Figure 3: The curve in P2 resulting from the lowbow representation of the word sequence 1 3 3 3 2 2 1 3 3 . [sent-287, score-1.013]
</p><p>20 The theorem below proves that the lowbow curve is Lipschitz continuous with a Lipschitz constant  O (K), thus connecting the curve complexity with the shape and the scale of the kernel. [sent-301, score-1.02]
</p><p>21 Theorem 3 The lowbow curve γ(y) satisﬁes γµ (y) − γτ (y)  2  ≤ |µ − τ| O (K),  ∀µ, τ ∈ [0, 1]. [sent-302, score-0.885]
</p><p>22 Modeling of Simplicial Curves Modeling functional data such as lowbow curves is known in the statistics literature as functional data analysis (e. [sent-305, score-0.858]
</p><p>23 In this section we discuss some issues concerning generative and conditional modeling of lowbow curves. [sent-309, score-0.858]
</p><p>24 Additional information regarding the practical use of lowbow curves in a number of text processing tasks may be found in Section 5. [sent-310, score-0.917]
</p><p>25 [0,1] Geometrically, a lowbow curve is a point in an inﬁnite product of simplices PV −1 that is naturally equipped with the product topology and geometry of the individual simplices. [sent-311, score-0.934]
</p><p>26 , µl ∈ [0, 1] provides a ﬁnite dimensional lowbow representation equivalent l to a point in the product space PV −1 . [sent-316, score-0.848]
</p><p>27 Given a Riemannian metric g on the simplex, its product form gθ (u, v) =  Z 1 0  gθ(t) (u(t), v(t)) dt  deﬁnes a corresponding metric on lowbow curves. [sent-318, score-0.945]
</p><p>28 Using the integrated distance formula (9) we can easily adapt distance-based algorithms to the lowbow representation. [sent-325, score-0.807]
</p><p>29 In a similar way, we can also apply kernel-based algorithms such as SVM to documents using [0,1] the lowbow representation by considering a kernel over PV −1 . [sent-334, score-0.984]
</p><p>30 We show in Section 5 that its lowbow version (10) further improves upon those results. [sent-342, score-0.807]
</p><p>31 In addition to classiﬁcation, the lowbow diffusion kernel may prove useful for other tasks such as dimensionality reduction using kernel PCA, support vector regression, and semi-supervised learning. [sent-343, score-0.944]
</p><p>32 The lowbow representation may also be used to construct generative models for text that generalize the naive Bayes or multinomial model. [sent-344, score-0.952]
</p><p>33 In contrast to the multinomial model which ignores the sequential progression in a document, lowbow curves γ may be considered as a semiparametric generative model assigning the probability vector γµ to the generation of words around the document location µN. [sent-346, score-1.183]
</p><p>34 Lebanon and Zhao (2007) describe a local likelihood model that is essentially equivalent to the generative lowbow model described above. [sent-358, score-0.839]
</p><p>35 In contrast to the model of Blei and Lafferty (2006) the lowbow generative model is not based on latent topics and is inherently smooth. [sent-359, score-0.807]
</p><p>36 The differential characteristics of the lowbow curve convey signiﬁcant information and deserve closer inspection. [sent-360, score-0.913]
</p><p>37 Further details concerning differential operators and their role in visualizing lowbow curves may be found in Mao et al. [sent-367, score-0.937]
</p><p>38 Kernel Smoothing, Bias-Variance Tradeoff and Generalization Error Bounds The choice of the kernel scale parameter σ or the amount of smoothing is essential for the success of the lowbow framework. [sent-378, score-0.978]
</p><p>39 Further details concerning the statistical properties of the lowbow estimator as a local likelihood model for streaming data may be found in Lebanon and Zhao (2007). [sent-383, score-0.859]
</p><p>40 1 Bias and Variance Tradeoff We discuss the bias and variance of the lowbow model γ(y) as an estimator for an underlying semiparametric model {θt : t ∈ [0, 1]} ⊂ PV −1 which we assume generated the observed document y. [sent-386, score-0.961]
</p><p>41 ˆ The bias and variance of the the lowbow estimator γ(y) = θ(y), reveal the expected tradeoff by ˆ considering their dependence on the kernel scale σ. [sent-393, score-0.932]
</p><p>42 However, despite the lack of an asymptotic trend N → ∞ we can still gain insight from analyzing the dependency of the bias and variance of the lowbow estimator as a function of the kernel scale parameter σ. [sent-402, score-0.911]
</p><p>43 01  0 0  10  20  30  40  50  60  70  80  90  100  Kernel Support L  ˆ Figure 4: Squared bias, variance and mean squared error of the lowbow estimator θi j as a function of a triangular kernel support, that is, L in (13). [sent-415, score-0.854]
</p><p>44 The problem of selecting a particular weight vector w or kernel K for the lowbow estimator that minimizes the mean squared error is related to the problem of bandwidth kernel selection in local regression and density estimation. [sent-429, score-0.933]
</p><p>45 A lowbow representation with a small σ would lead to a low empirical risk since it results in a richer and more accurate expression of the data. [sent-447, score-0.848]
</p><p>46 The theorem and bounds below are expressed for continuous lowbow representation and continuous linear classiﬁers. [sent-478, score-0.89]
</p><p>47 The same results hold with analogous proofs in the more practical case of ﬁnite dimensional linear classiﬁers and discretized lowbow representations. [sent-479, score-0.833]
</p><p>48 1 Text Classiﬁcation using Nearest Neighbor We start by examining lowbow and its properties in the context of text classiﬁcation using a nearest neighbor classiﬁer. [sent-503, score-0.866]
</p><p>49 The continuous quantities in the lowbow calculation were approximated by a discrete sample of 5 equally spaced points in the interval [0, 1] turning the integrals into simple sums. [sent-510, score-0.85]
</p><p>50 As a result, the computational complexity associated with the lowbow representation is simply the number of sampling points times the complexity of the corresponding bow classiﬁer. [sent-511, score-0.985]
</p><p>51 test set error rate comparing the bow geodesic (lowbow with σ → ∞) (dashed) and the lowbow geodesic distance. [sent-534, score-1.032]
</p><p>52 The lowbow geodesic distance for an intermediate scale is denoted by err 1 and for σ → ∞ is denoted by err2 . [sent-541, score-0.887]
</p><p>53 The experiments indicate that lowbow geodesic clearly outperforms, for most values of σ, the standard tf-cosine similarity and Euclidean distance for bow (represented by err 3 , err4 ). [sent-543, score-0.988]
</p><p>54 In addition they also indicate that in general, the best scale parameter for lowbow is an intermediate one, rather than the standard bow model σ → ∞ thus validating the hypothesis that we can leverage sequential 2422  T HE L OCALLY W EIGHTED BAG OF W ORDS F RAMEWORK  0. [sent-544, score-1.051]
</p><p>55 err1 is obtained using the lowbow geodesic distance with the optimal kernel scale. [sent-690, score-0.898]
</p><p>56 2423  L EBANON , M AO AND D ILLON  information using the lowbow framework to improve on global bow models. [sent-692, score-0.944]
</p><p>57 In our experiments we examined the classiﬁcation performance of SVM with the Fisher diffusion kernel for bow (Lafferty and Lebanon, 2005) and its corresponding product version for lowbow (10) (which reverts to the kernel of Lafferty and Lebanon (2005) for σ → ∞). [sent-702, score-1.081]
</p><p>58 Notice that in general, the ˆ lowbow σopt signiﬁcantly outperforms the standard bow approach. [sent-711, score-0.944]
</p><p>59 In all the classiﬁcation tasks, lowbow performs substantially better than bow. [sent-729, score-0.807]
</p><p>60 The error bars indicate one standard deviation from the mean, and support experimentally the assertion that lowbow has lower variance. [sent-730, score-0.807]
</p><p>61 3 Dynamic Time Warping of Lowbow Curves As presented in the previous sections, the lowbow framework normalizes the time interval [1, N] to [0,1] [0, 1] thus achieving an embedding of documents of varying lengths in PV −1 . [sent-734, score-0.918]
</p><p>62 Such cases can be modeled by introducing time-warping or re-parameterization functions that match the individual temporal domains of lowbow curves to a unique canonical parameterization. [sent-1107, score-0.899]
</p><p>63 Before proceeding to discuss such re-parameterization in the context of lowbow curves we brieﬂy review their use in speech recondition and functional data analysis. [sent-1108, score-0.896]
</p><p>64 In contrast to the smoothness and monotonic nature of the re-parameterization class I in speech recognition and functional data analysis, it seems reasonable to allow some amount of discontinuity in lowbow re-parameterization. [sent-1114, score-0.845]
</p><p>65 Somewhat surprisingly, adding dynamic time warping or registration to lowbow classiﬁcation resulted in only a marginal modiﬁcation of the distances and consequently only a marginal improvement in classiﬁcation performance. [sent-1132, score-0.906]
</p><p>66 Second, the local smoothing inherent in the lowbow representation makes it fairly robust to some amount of temporal misalignment. [sent-1136, score-1.009]
</p><p>67 Although surprising, this is indeed a positive result as it indicates that the lowbow representation is relatively robust to different time parameterization, at least when applied to documents sharing similar structure such as news stories in RCV1 corpus or webpages in the WebKB data set. [sent-1138, score-1.045]
</p><p>68 Our approach in this section is not to carefully construct a state-of-the-art text segmentation system but rather to demonstrate the usefulness of the continuous lowbow representation in this context. [sent-1143, score-0.967]
</p><p>69 In the context of lowbow curves, this would correspond to sudden dramatic shifts in the curve location. [sent-1147, score-0.885]
</p><p>70 Due to the continuity of the lowbow curves, such sudden movements may be discovered by considering the gradient vector ﬁeld γ˙µ along the lowbow curve. [sent-1148, score-1.659]
</p><p>71 We examine the behavior of the lowbow curve and its gradient for two documents created in this fashion. [sent-1153, score-0.993]
</p><p>72 8  1  Figure 10: Velocity of the lowbow curve as a function of t. [sent-1195, score-0.885]
</p><p>73 The right panel of Figure 10 displays the ˙ gradient norm γµ 2 for the corresponding lowbow curve. [sent-1213, score-0.826]
</p><p>74 The lowbow curve itself carries additional information beyond the gradient norm for segmentation purposes. [sent-1224, score-0.943]
</p><p>75 6  Figure 11: 2D embeddings of the lowbow curve representing the three successive RCV1 stories (see text for more details) using PCA (left, σ = 0. [sent-1267, score-0.985]
</p><p>76 Figure 11 shows the 2D projection of the lowbow curve for the three concatenated RCV1 stories mentioned above. [sent-1272, score-0.926]
</p><p>77 The distance between successive points near the segment boundaries is relatively large which demonstrates the high speed of the lowbow curve at these points (compare it with the gradient norm in right panel of Figure 10). [sent-1279, score-0.929]
</p><p>78 Additional visualization applications of the lowbow framework may be found in Mao et al. [sent-1283, score-0.853]
</p><p>79 ˙ The gradient norm γ(t) 2 of the lowbow curve of this paper is displayed in Figure 12. [sent-1285, score-0.925]
</p><p>80 5) are excluded from the generation of the lowbow curve and equations were replaced by special markers. [sent-1294, score-0.885]
</p><p>81 8  1  Figure 12: Velocity of the lowbow curve computed for this paper as a function of µ (σ = 0. [sent-1323, score-0.885]
</p><p>82 Figure 13 depicts 2D projections of the lowbow curve corresponding to Sections 5. [sent-1329, score-0.885]
</p><p>83 As previously demonstrated, the lowbow curve nicely reveals three clusters corresponding to the different subsections with the exception of not distinguishing between the nearest neighbor and SVM experiments. [sent-1335, score-0.885]
</p><p>84 Using interactive graphics it is possible to extract more information from the lowbow curves by examining the 3D PCA projection, displayed in Figure 14. [sent-1336, score-0.879]
</p><p>85 6  Figure 13: 2D embeddings of the lowbow curve computed for Section 5. [sent-1378, score-0.885]
</p><p>86 Figure 14: 3D embeddings of the lowbow curve computed for Section 5. [sent-1383, score-0.885]
</p><p>87 , 2007) to consider dynamic time warping which is highly relevant to the problem of matching two lowbow curves. [sent-1399, score-0.883]
</p><p>88 Modeling functional data such as lowbow curves in statistics has been studied in the context of functional data analysis. [sent-1400, score-0.858]
</p><p>89 Wand and Jones (1995) and Loader (1999) provide a recent overview of local smoothing in statistics which is closely related to the lowbow framework. [sent-1402, score-0.927]
</p><p>90 Discussion The lowbow representation is a promising new direction in text modeling. [sent-1413, score-0.907]
</p><p>91 On the other hand, the lowbow novelty is orthogonal to n-gram as it is possible to construct lowbow curves over n-gram counts. [sent-1419, score-1.665]
</p><p>92 Under our current model, two different lowbow curves are compared in a point-wise manner. [sent-1420, score-0.858]
</p><p>93 This is due partly to the nature of the data and partly to the robustness of the lowbow representation being insensible to time-misalignment by adapting the scale parameter. [sent-1422, score-0.884]
</p><p>94 2435  L EBANON , M AO AND D ILLON  In this paper we have focused on analyzing lowbow curves at one particular scale σ. [sent-1427, score-0.894]
</p><p>95 An alternative and potentially more powerful approach is to consider a family of lowbow curves corresponding to a collection of scale parameters σ. [sent-1428, score-0.894]
</p><p>96 The lowbow framework is aesthetically pleasing, and achieves good results both in terms of numeric classiﬁcation accuracy and in terms of presenting a convenient visual text representation to a user. [sent-1431, score-0.907]
</p><p>97 In contrast to categorical smoothing methods employed by n-grams (such as back-off and interpolation) lowbow employs temporal smoothing which is potentially more powerful due to its ordinal nature. [sent-1433, score-1.05]
</p><p>98 Since the simplex is the space of bow representations, its geometry is crucial to the lowbow representation. [sent-1439, score-1.081]
</p><p>99 In addressing the question of modeling the lowbow curves, the geometry of the simplex plays a central role. [sent-1461, score-0.975]
</p><p>100 Understanding the relationship between these geometric notions and g θ is a necessary prerequisite for modeling documents using the lowbow representation. [sent-1465, score-0.946]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lowbow', 0.807), ('bag', 0.157), ('bow', 0.137), ('document', 0.133), ('ao', 0.123), ('ebanon', 0.123), ('illon', 0.123), ('ocally', 0.123), ('ords', 0.123), ('pv', 0.109), ('ramework', 0.104), ('dt', 0.094), ('lebanon', 0.092), ('documents', 0.089), ('simplex', 0.088), ('smoothing', 0.088), ('word', 0.087), ('eighted', 0.079), ('curve', 0.078), ('opt', 0.074), ('sequential', 0.071), ('text', 0.059), ('curves', 0.051), ('lafferty', 0.049), ('geometry', 0.049), ('ramsay', 0.048), ('warping', 0.048), ('kernel', 0.047), ('visualization', 0.046), ('multinomial', 0.045), ('news', 0.044), ('geodesic', 0.044), ('diffusion', 0.043), ('representation', 0.041), ('stories', 0.041), ('story', 0.041), ('temporal', 0.041), ('segmentation', 0.039), ('speech', 0.038), ('scale', 0.036), ('te', 0.035), ('fisher', 0.033), ('local', 0.032), ('covering', 0.032), ('euclidean', 0.031), ('modeling', 0.031), ('webkb', 0.031), ('visualizing', 0.031), ('beta', 0.031), ('parameterization', 0.029), ('words', 0.029), ('differential', 0.028), ('pm', 0.028), ('yn', 0.028), ('dynamic', 0.028), ('wand', 0.027), ('discretized', 0.026), ('purdue', 0.026), ('continuity', 0.026), ('categorical', 0.026), ('histogram', 0.026), ('segment', 0.025), ('trends', 0.025), ('location', 0.024), ('blei', 0.024), ('mao', 0.024), ('corpus', 0.023), ('kass', 0.023), ('rro', 0.023), ('loocv', 0.023), ('silverman', 0.023), ('registration', 0.023), ('progression', 0.023), ('lengths', 0.022), ('smooth', 0.022), ('spaced', 0.022), ('curvature', 0.022), ('metric', 0.022), ('pca', 0.022), ('riemannian', 0.022), ('continuous', 0.021), ('tradeoff', 0.021), ('maxima', 0.021), ('displayed', 0.021), ('bias', 0.021), ('smoothed', 0.021), ('beeferman', 0.021), ('dtdt', 0.021), ('dtw', 0.021), ('sakoe', 0.021), ('simplicial', 0.021), ('lipschitz', 0.02), ('concerning', 0.02), ('segments', 0.019), ('gradient', 0.019), ('geometric', 0.019), ('amari', 0.019), ('rv', 0.019), ('iz', 0.019), ('histograms', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="81-tfidf-1" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>Author: Guy Lebanon, Yi Mao, Joshua Dillon</p><p>Abstract: The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efﬁcient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks. Keywords: text processing, local smoothing</p><p>2 0.1216514 <a title="81-tfidf-2" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>Author: Evgeniy Gabrilovich, Shaul Markovitch</p><p>Abstract: Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-speciﬁc and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two signiﬁcant problems in natural language processing—synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets conﬁrm improved performance compared to the bag of words document representation. Keywords: feature generation, text classiﬁcation, background knowledge</p><p>3 0.074310362 <a title="81-tfidf-3" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis. Keywords: embedding algorithms, manifold learning, exponential families, multidimensional scaling, matrix factorization, semideﬁnite programming</p><p>4 0.047233973 <a title="81-tfidf-4" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: In numerous domains it is useful to represent a single example by the set of the local features or parts that comprise it. However, this representation poses a challenge to many conventional machine learning techniques, since sets may vary in cardinality and elements lack a meaningful ordering. Kernel methods can learn complex functions, but a kernel over unordered set inputs must somehow solve for correspondences—generally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function called the pyramid match that measures partial match similarity in time linear in the number of features. The pyramid match maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in order to ﬁnd implicit correspondences based on the ﬁnest resolution histogram cell where a matched pair ﬁrst appears. We show the pyramid match yields a Mercer kernel, and we prove bounds on its error relative to the optimal partial matching cost. We demonstrate our algorithm on both classiﬁcation and regression tasks, including object recognition, 3-D human pose inference, and time of publication estimation for documents, and we show that the proposed method is accurate and signiﬁcantly more efﬁcient than current approaches. Keywords: kernel, sets of features, histogram intersection, multi-resolution histogram pyramid, approximate matching, object recognition</p><p>5 0.039276686 <a title="81-tfidf-5" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>Author: Marc Teboulle</p><p>Abstract: Center-based partitioning clustering algorithms rely on minimizing an appropriately formulated objective function, and different formulations suggest different possible algorithms. In this paper, we start with the standard nonconvex and nonsmooth formulation of the partitioning clustering problem. We demonstrate that within this elementary formulation, convex analysis tools and optimization theory provide a unifying language and framework to design, analyze and extend hard and soft center-based clustering algorithms, through a generic algorithm which retains the computational simplicity of the popular k-means scheme. We show that several well known and more recent center-based clustering algorithms, which have been derived either heuristically, or/and have emerged from intuitive analogies in physics, statistical techniques and information theoretic perspectives can be recovered as special cases of the proposed analysis and we streamline their relationships. Keywords: clustering, k-means algorithm, convex analysis, support and asymptotic functions, distance-like functions, Bregman and Csiszar divergences, nonlinear means, nonsmooth optimization, smoothing algorithms, ﬁxed point methods, deterministic annealing, expectation maximization, information theory and entropy methods</p><p>6 0.03764696 <a title="81-tfidf-6" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>7 0.037135106 <a title="81-tfidf-7" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>8 0.02690565 <a title="81-tfidf-8" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>9 0.02564613 <a title="81-tfidf-9" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>10 0.025616992 <a title="81-tfidf-10" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>11 0.023844725 <a title="81-tfidf-11" href="./jmlr-2007-Distances_between_Data_Sets_Based_on_Summary_Statistics.html">27 jmlr-2007-Distances between Data Sets Based on Summary Statistics</a></p>
<p>12 0.023253024 <a title="81-tfidf-12" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>13 0.022147091 <a title="81-tfidf-13" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>14 0.021871518 <a title="81-tfidf-14" href="./jmlr-2007-Measuring_Differentiability%3A__Unmasking_Pseudonymous_Authors.html">54 jmlr-2007-Measuring Differentiability:  Unmasking Pseudonymous Authors</a></p>
<p>15 0.021700758 <a title="81-tfidf-15" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>16 0.021374321 <a title="81-tfidf-16" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>17 0.020995907 <a title="81-tfidf-17" href="./jmlr-2007-Very_Fast_Online_Learning_of_Highly_Non_Linear_Problems.html">91 jmlr-2007-Very Fast Online Learning of Highly Non Linear Problems</a></p>
<p>18 0.020859065 <a title="81-tfidf-18" href="./jmlr-2007-Covariate_Shift_Adaptation_by_Importance_Weighted_Cross_Validation.html">25 jmlr-2007-Covariate Shift Adaptation by Importance Weighted Cross Validation</a></p>
<p>19 0.020722419 <a title="81-tfidf-19" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>20 0.020711916 <a title="81-tfidf-20" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.147), (1, 0.06), (2, 0.026), (3, 0.097), (4, -0.086), (5, 0.026), (6, -0.016), (7, -0.072), (8, 0.077), (9, -0.005), (10, -0.061), (11, 0.065), (12, -0.335), (13, 0.219), (14, -0.047), (15, -0.086), (16, 0.193), (17, 0.123), (18, 0.113), (19, -0.1), (20, 0.003), (21, -0.027), (22, -0.11), (23, -0.021), (24, 0.119), (25, -0.011), (26, 0.017), (27, -0.07), (28, 0.093), (29, -0.081), (30, -0.049), (31, -0.143), (32, 0.129), (33, -0.037), (34, -0.095), (35, -0.015), (36, -0.042), (37, 0.076), (38, -0.157), (39, 0.126), (40, -0.248), (41, -0.059), (42, 0.137), (43, -0.012), (44, 0.133), (45, -0.156), (46, 0.026), (47, -0.056), (48, -0.107), (49, 0.09)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94906843 <a title="81-lsi-1" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>Author: Guy Lebanon, Yi Mao, Joshua Dillon</p><p>Abstract: The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efﬁcient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks. Keywords: text processing, local smoothing</p><p>2 0.74525654 <a title="81-lsi-2" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>Author: Evgeniy Gabrilovich, Shaul Markovitch</p><p>Abstract: Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-speciﬁc and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two signiﬁcant problems in natural language processing—synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets conﬁrm improved performance compared to the bag of words document representation. Keywords: feature generation, text classiﬁcation, background knowledge</p><p>3 0.3808634 <a title="81-lsi-3" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis. Keywords: embedding algorithms, manifold learning, exponential families, multidimensional scaling, matrix factorization, semideﬁnite programming</p><p>4 0.21414205 <a title="81-lsi-4" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: In numerous domains it is useful to represent a single example by the set of the local features or parts that comprise it. However, this representation poses a challenge to many conventional machine learning techniques, since sets may vary in cardinality and elements lack a meaningful ordering. Kernel methods can learn complex functions, but a kernel over unordered set inputs must somehow solve for correspondences—generally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function called the pyramid match that measures partial match similarity in time linear in the number of features. The pyramid match maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in order to ﬁnd implicit correspondences based on the ﬁnest resolution histogram cell where a matched pair ﬁrst appears. We show the pyramid match yields a Mercer kernel, and we prove bounds on its error relative to the optimal partial matching cost. We demonstrate our algorithm on both classiﬁcation and regression tasks, including object recognition, 3-D human pose inference, and time of publication estimation for documents, and we show that the proposed method is accurate and signiﬁcantly more efﬁcient than current approaches. Keywords: kernel, sets of features, histogram intersection, multi-resolution histogram pyramid, approximate matching, object recognition</p><p>5 0.20036909 <a title="81-lsi-5" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>Author: Sébastien Gadat, Laurent Younes</p><p>Abstract: We introduce a new model addressing feature selection from a large dictionary of variables that can be computed from a signal or an image. Features are extracted according to an efﬁciency criterion, on the basis of speciﬁed classiﬁcation or recognition tasks. This is done by estimating a probability distribution P on the complete dictionary, which distributes its mass over the more efﬁcient, or informative, components. We implement a stochastic gradient descent algorithm, using the probability as a state variable and optimizing a multi-task goodness of ﬁt criterion for classiﬁers based on variable randomly chosen according to P. We then generate classiﬁers from the optimal distribution of weights learned on the training set. The method is ﬁrst tested on several pattern recognition problems including face detection, handwritten digit recognition, spam classiﬁcation and micro-array analysis. We then compare our approach with other step-wise algorithms like random forests or recursive feature elimination. Keywords: stochastic learning algorithms, Robbins-Monro application, pattern recognition, classiﬁcation algorithm, feature selection</p><p>6 0.14899805 <a title="81-lsi-6" href="./jmlr-2007-Covariate_Shift_Adaptation_by_Importance_Weighted_Cross_Validation.html">25 jmlr-2007-Covariate Shift Adaptation by Importance Weighted Cross Validation</a></p>
<p>7 0.14848578 <a title="81-lsi-7" href="./jmlr-2007-Very_Fast_Online_Learning_of_Highly_Non_Linear_Problems.html">91 jmlr-2007-Very Fast Online Learning of Highly Non Linear Problems</a></p>
<p>8 0.1435371 <a title="81-lsi-8" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>9 0.12775469 <a title="81-lsi-9" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>10 0.124861 <a title="81-lsi-10" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>11 0.1190179 <a title="81-lsi-11" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>12 0.11607017 <a title="81-lsi-12" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>13 0.11461629 <a title="81-lsi-13" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>14 0.11444148 <a title="81-lsi-14" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>15 0.11331686 <a title="81-lsi-15" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>16 0.10389958 <a title="81-lsi-16" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>17 0.10372517 <a title="81-lsi-17" href="./jmlr-2007-Large_Margin_Semi-supervised_Learning.html">44 jmlr-2007-Large Margin Semi-supervised Learning</a></p>
<p>18 0.1031165 <a title="81-lsi-18" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>19 0.10280501 <a title="81-lsi-19" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>20 0.10206809 <a title="81-lsi-20" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.017), (8, 0.028), (10, 0.05), (12, 0.055), (15, 0.08), (22, 0.013), (28, 0.052), (40, 0.031), (41, 0.014), (42, 0.285), (45, 0.011), (48, 0.052), (60, 0.019), (77, 0.013), (80, 0.031), (85, 0.059), (98, 0.098)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69722497 <a title="81-lda-1" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>Author: Guy Lebanon, Yi Mao, Joshua Dillon</p><p>Abstract: The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efﬁcient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks. Keywords: text processing, local smoothing</p><p>2 0.45885456 <a title="81-lda-2" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis. Keywords: embedding algorithms, manifold learning, exponential families, multidimensional scaling, matrix factorization, semideﬁnite programming</p><p>3 0.45714492 <a title="81-lda-3" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>4 0.45651194 <a title="81-lda-4" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>Author: Wei Pan, Xiaotong Shen</p><p>Abstract: Variable selection in clustering analysis is both challenging and important. In the context of modelbased clustering analysis with a common diagonal covariance matrix, which is especially suitable for “high dimension, low sample size” settings, we propose a penalized likelihood approach with an L1 penalty function, automatically realizing variable selection via thresholding and delivering a sparse solution. We derive an EM algorithm to ﬁt our proposed model, and propose a modiﬁed BIC as a model selection criterion to choose the number of components and the penalization parameter. A simulation study and an application to gene function prediction with gene expression proﬁles demonstrate the utility of our method. Keywords: BIC, EM, mixture model, penalized likelihood, soft-thresholding, shrinkage</p><p>5 0.4536958 <a title="81-lda-5" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>Author: Robert Tibshirani, Trevor Hastie</p><p>Abstract: We propose a method for the classiﬁcation of more than two classes, from high-dimensional features. Our approach is to build a binary decision tree in a top-down manner, using the optimal margin classiﬁer at each split. We implement an exact greedy algorithm for this task, and compare its performance to less greedy procedures based on clustering of the matrix of pairwise margins. We compare the performance of the “margin tree” to the closely related “all-pairs” (one versus one) support vector machine, and nearest centroids on a number of cancer microarray data sets. We also develop a simple method for feature selection. We ﬁnd that the margin tree has accuracy that is competitive with other methods and offers additional interpretability in its putative grouping of the classes. Keywords: maximum margin classiﬁer, support vector machine, decision tree, CART</p><p>6 0.45320234 <a title="81-lda-6" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>7 0.45245355 <a title="81-lda-7" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>8 0.44897604 <a title="81-lda-8" href="./jmlr-2007-Handling_Missing_Values_when_Applying_Classification_Models.html">39 jmlr-2007-Handling Missing Values when Applying Classification Models</a></p>
<p>9 0.44674152 <a title="81-lda-9" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>10 0.44496801 <a title="81-lda-10" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>11 0.44310784 <a title="81-lda-11" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>12 0.43945917 <a title="81-lda-12" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>13 0.43819818 <a title="81-lda-13" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>14 0.43734819 <a title="81-lda-14" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>15 0.43522519 <a title="81-lda-15" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>16 0.43408778 <a title="81-lda-16" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>17 0.43226552 <a title="81-lda-17" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>18 0.43162841 <a title="81-lda-18" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>19 0.43028155 <a title="81-lda-19" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>20 0.42995176 <a title="81-lda-20" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
