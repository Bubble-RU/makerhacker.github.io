<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>71 jmlr-2007-Refinable Kernels</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-71" href="#">jmlr2007-71</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>71 jmlr-2007-Refinable Kernels</h1>
<br/><p>Source: <a title="jmlr-2007-71-pdf" href="http://jmlr.org/papers/volume8/xu07a/xu07a.pdf">pdf</a></p><p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: Motivated by mathematical learning from training data, we introduce the notion of reﬁnable kernels. Various characterizations of reﬁnable kernels are presented. The concept of reﬁnable kernels leads to the introduction of wavelet-like reproducing kernels. We also investigate a reﬁnable kernel that forms a Riesz basis. In particular, we characterize reﬁnable translation invariant kernels, and reﬁnable kernels deﬁned by reﬁnable functions. This study leads to multiresolution analysis of reproducing kernel Hilbert spaces. Keywords: reﬁnable kernels, reﬁnable feature maps, wavelet-like reproducing kernels, dual kernels, learning with kernels, reproducing kernel Hilbert spaces, Riesz bases</p><p>Reference: <a title="jmlr-2007-71-reference" href="../jmlr2007_reference/jmlr-2007-Refinable_Kernels_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The concept of reﬁnable kernels leads to the introduction of wavelet-like reproducing kernels. [sent-5, score-0.113]
</p><p>2 In particular, we characterize reﬁnable translation invariant kernels, and reﬁnable kernels deﬁned by reﬁnable functions. [sent-7, score-0.126]
</p><p>3 This study leads to multiresolution analysis of reproducing kernel Hilbert spaces. [sent-8, score-0.123]
</p><p>4 Keywords: reﬁnable kernels, reﬁnable feature maps, wavelet-like reproducing kernels, dual kernels, learning with kernels, reproducing kernel Hilbert spaces, Riesz bases  1. [sent-9, score-0.149]
</p><p>5 Introduction The main purpose of this paper is to introduce the notion of reﬁnable kernels, wavelet-like reproducing kernels and multiresolution analysis of a reproducing kernel Hilbert space. [sent-10, score-0.236]
</p><p>6 , 1991; Daubechies, 1992), multiresolution analysis of L2 (R) (Mallat, 1989; Meyer, 1992) and kernels constructed by wavelet functions (Amato et al. [sent-12, score-0.132]
</p><p>7 A kernel K on X corresponds to a Hilbert space  HK := span {K(·, y) : y ∈ X}  (3)  of functions on X with an inner product determined by (K(·, y), K(·, x))HK = K(x, y), x, y ∈ X. [sent-32, score-0.104]
</p><p>8 (4)  The space HK is a reproducing kernel Hilbert space (RKHS), that is, point evaluations are continuous linear functionals on HK (Aronszajn, 1950). [sent-33, score-0.102]
</p><p>9 (5)  Due to Equation (5), K is often interpreted as the reproducing kernel of H K . [sent-35, score-0.09]
</p><p>10 A RKHS has exactly one reproducing kernel (Aronszajn, 1950). [sent-36, score-0.09]
</p><p>11 Speciﬁcally, we demand kernels K having the feature that there is a cheap way of updating K to a new kernel K such that HK HK . [sent-50, score-0.125]
</p><p>12 We shall study characterizations of a reﬁnable kernel, fundamental properties of reﬁnable kernels and wavelet-like reproducing kernels, and multiscale structures of a RKHS induced by reﬁnable kernels. [sent-58, score-0.172]
</p><p>13 It is important to note that the concept of wavelet-like reproducing kernels differs from that of “wavelet kernels” in Amato et al. [sent-59, score-0.113]
</p><p>14 The earlier means the kernels deﬁned by the difference of kernels at two consecutive scales while the latter means the kernels deﬁned by a linear combination of dilations and translations of wavelet functions. [sent-62, score-0.247]
</p><p>15 Section 3 is devoted to wavelet-like reproducing kernels and a multiscale decomposition of the RKHS of a reﬁnable kernel. [sent-65, score-0.133]
</p><p>16 In Section 4, we investigate reﬁnable kernels of a Riesz type and we also introduce the notion of a multiresolution analysis for a RKHS. [sent-66, score-0.107]
</p><p>17 As concrete examples of reﬁnable kernels, we formulate in Sections 5 and 6, respectively, conditions for translation invariant kernels and kernels deﬁned by a reﬁnable function to be reﬁnable. [sent-67, score-0.2]
</p><p>18 2085  X U AND Z HANG  Theorem 1 If K is a kernel on X, then for each n ∈ Z, the RKHS of kernel Kn is  HKn = { f ◦ γn : f ∈ HK } with inner product  (9)  ( f , g)HKn := λ−n ( f ◦ γ−n , g ◦ γ−n )HK , f , g ∈ HKn . [sent-80, score-0.119]
</p><p>19 This implies that Kn is the reproducing kernel for Hn . [sent-87, score-0.09]
</p><p>20 A direct consequence of Theorem 1 is that if K is a γ-reﬁnable kernel then for each n ∈ Z, the kernel Kn is γ-reﬁnable. [sent-89, score-0.102]
</p><p>21 Finally, we verify by (12) for each f , g ∈ HK that ( f ◦ γ−1 , g ◦ γ−1 )HK = ( f−1 , g−1 )HK = lim ( fn ◦ γ−1 , gn ◦ γ−1 )HK = λ lim ( fn , gn )HK = λ( f , g)HK . [sent-121, score-0.13]
</p><p>22 Another characterization of γ-reﬁnable kernels K is in terms of a feature map for K. [sent-124, score-0.11]
</p><p>23 To state the result, we denote by Φ(X) the image of X under Φ, span Φ(X) the closure of span Φ(X) in W , and PΦ the orthogonal projection from W onto span Φ(X). [sent-129, score-0.122]
</p><p>24 In the application of Lemma 5, it is always convenient to assume that there holds span Φ(X) = W  (18)  since otherwise W can be replaced by span Φ(X). [sent-137, score-0.085]
</p><p>25 Recall that a linear operator A on W is isometric if for all u ∈ W , Au W = u W . [sent-142, score-0.088]
</p><p>26 One can see that A is isometric if and only if A∗ A is equal to the identity operator on W , where A∗ denotes the adjoint operator of A. [sent-143, score-0.16]
</p><p>27 2088  R EFINABLE K ERNELS  Proof Suppose that Φ is γ-reﬁnable, that is, it satisﬁes (20) for some bounded operator T on W , and suppose that T ∗ is isometric. [sent-147, score-0.084]
</p><p>28 Let A denote the map u → vu and observe that A is a linear operator on W . [sent-161, score-0.097]
</p><p>29 Then HK−1 is a proper subspace of HK if and only if the operator T in (20) is not injective. [sent-180, score-0.096]
</p><p>30 By a property of reproducing kernels (see, Aronszajn, 1950, page 345), the sum of Kn and the kernel of Wn is equal to Kn+1 . [sent-199, score-0.164]
</p><p>31 Therefore, G is the kernel of W0 , and it is observed 2090  R EFINABLE K ERNELS  by (8) that the kernel of Wn is Gn . [sent-200, score-0.102]
</p><p>32 We call G n the wavelet-like reproducing kernels and in particular, G the initial wavelet-like kernel. [sent-205, score-0.113]
</p><p>33 It is clear that the initial wavelet-like kernel G is nontrivial if and only if HK−1 is a proper subspace of HK . [sent-206, score-0.126]
</p><p>34 (30)  One should notice the difference between wavelet-like reproducing kernels that we introduce here and the “wavelet kernels” studied in Amato et al. [sent-208, score-0.113]
</p><p>35 The latter are a class of Hilbert-Schmidt kernels deﬁned as a superposition of dilations and translations of a wavelet function. [sent-211, score-0.099]
</p><p>36 Examples of nontrivial reﬁnable kernels on Rd will be given in Sections 5 and 6. [sent-257, score-0.11]
</p><p>37 This leads to consideration of requiring KX to be a frame or a Riesz basis for HK . [sent-261, score-0.118]
</p><p>38 A family of elements {ϕ j : j ∈ J} in a Hilbert space W forms a frame if there exist 0 < α ≤ β < ∞ such that for all f ∈ W α f 2 ≤ ∑ |( f , ϕ j )W |2 ≤ β f 2 . [sent-265, score-0.084]
</p><p>39 W W  (36)  j∈J  The constants α, β are called the frame bounds for {ϕ j : j ∈ J}. [sent-266, score-0.084]
</p><p>40 We call a frame {ϕ j : j ∈ J} a tight frame when its two frame bounds are equal, that is, α = β. [sent-267, score-0.252]
</p><p>41 A Riesz basis {ϕ j : j ∈ J} is equivalent to an orthonormal basis {ψ j : j ∈ J} for W , namely, there exists a bounded linear operator L on W having a bounded inverse such that L(ψ j ) = ϕ j , j ∈ J. [sent-269, score-0.14]
</p><p>42 An arbitrary element in W can be expressed as a linear combination of a frame {ϕ j : j ∈ J} for W . [sent-270, score-0.084]
</p><p>43 Deﬁne the frame operator F from W to 2 (J) by setting for all f ∈ W , (F f ) j := ( f , ϕ j )W , for all j ∈ J. [sent-272, score-0.141]
</p><p>44 Then F ∗ F is a bounded positive self-adjoint operator on W with a bounded inverse and ˜ ϕ j := (F ∗ F)−1 ϕ j : j ∈ J 2093  X U AND Z HANG  is a frame for W . [sent-273, score-0.141]
</p><p>45 We shall refer to it as the dual frame of {ϕ j : j ∈ J} since we have for all f ∈ W ˜ ˜ f = ∑ ( f , ϕ j )W ϕ j = ∑ ( f , ϕ j )W ϕ j . [sent-274, score-0.106]
</p><p>46 For a kernel K on X, we are interested in the conditions for which the set KZ := {K(·, z j ) : j ∈ J} is a Riesz basis for HK . [sent-278, score-0.085]
</p><p>47 Proposition 11 The family KZ is a Riesz basis for HK with frame bounds 0 < α ≤ β < ∞ if and only if for every ﬁnite subset X ⊆ Z , K[X ] is nonsingular, and for all f ∈ H K α f 2 K ≤ ∑ | f (z j )|2 ≤ β f 2 K . [sent-280, score-0.118]
</p><p>48 If X is a topological space, we call a kernel K on X a universal kernel if for all compact X ⊆ X, the linear span of {K(·, y) : y ∈ X } is dense in C(X ), the Banach space of continuous functions on X . [sent-282, score-0.173]
</p><p>49 Various characterizations of universal kernels are studied in Micchelli et al. [sent-283, score-0.091]
</p><p>50 The third characterization is in terms of a feature map for the kernel K. [sent-296, score-0.087]
</p><p>51 Then KZ is a Riesz basis for HK if and only if Φ(Z ) is a Riesz basis for span Φ(X). [sent-298, score-0.104]
</p><p>52 Since the operator Γ deﬁned by (19) is an isomorphism from H K onto W , KZ is a Riesz basis if and only if Γ(KZ ) is a Riesz basis for Γ(HK ) = W . [sent-300, score-0.125]
</p><p>53 Proposition 15 Suppose that KZ is a Riesz basis for HK with frame bounds 0 < α ≤ β < ∞. [sent-312, score-0.118]
</p><p>54 Then for each n ∈ Z, {φn, j : j ∈ J} is a Riesz basis for HKn with the same frame bounds α, β. [sent-313, score-0.118]
</p><p>55 This assumption implies that the linear operator S on HK deﬁned by S f := ∑ f (z j )K(·, z j ), f ∈ HK  (40)  j∈J  is bounded positive self-adjoint and so is its inverse operator S −1 on HK (see, for example, Daubechies, 1992, pages 58–59). [sent-315, score-0.114]
</p><p>56 2095  X U AND Z HANG  Proposition 16 Suppose that KZ is a Riesz basis for HK and let S be the operator on HK deﬁned by (40). [sent-318, score-0.091]
</p><p>57 ˜ Theorem 17 If KZ is a Riesz basis for HK , then for each n ∈ Z, the dual Riesz basis {φn, j : j ∈ J} of {φn, j : j ∈ J} for HKn has the form ˜ ˜ φn, j = ∑ Λk, j φn,k , j ∈ J  (44)  k∈J  and S−1 f = ∑  j∈J  ˜ ∑ Λ j,k f (zk )  ˜ ∑ Λl, j K(·, zl )  k∈J  , f ∈ HK . [sent-328, score-0.09]
</p><p>58 By (10) and (5), we have for all j, k ∈ J that ˜ ˜ ˜ ˜ ˜ (φn, j , φn,k )HKn = (φ0, j , K(·, zk ))HK = φ0, j (zk ) = ∑ Λl, j K(zk , zl ) = ∑ Λk,l Λl, j = δ j,k , l∈J  2096  l∈J  R EFINABLE K ERNELS  ˜ which shows that {φn, j : j ∈ J} is the dual Riesz basis of {φn, j : j ∈ J} in HKn . [sent-333, score-0.088]
</p><p>59 (48)  The Riesz basis provides a characterization of γ-reﬁnable kernels in terms of the sampling operator, which we present next. [sent-345, score-0.124]
</p><p>60 We get by (5) for each x ∈ X that g(x) = (g, K(·, x))HK = lim ( fn , K(·, x))HK = lim fn (x) = lim ( fn , K−1 (·, x))HK n→∞  n→∞  n→∞  −1  = f (x). [sent-357, score-0.144]
</p><p>61 Therefore, f ∈ HK , and by (53) ( f , f ) HK  −1  = lim ( fn , fn )HK n→∞  −1  = lim ( fn , fn )HK = (g, g)HK = ( f , f )HK . [sent-358, score-0.15]
</p><p>62 In the rest of this section, we construct a frame for the RKHS H G of the wavelet-like kernel G := K1 − K. [sent-365, score-0.135]
</p><p>63 Lemma 19 Suppose that K is γ-reﬁnable and KZ is a Riesz basis for HK with frame bounds 0 < α ≤ β < ∞. [sent-366, score-0.118]
</p><p>64 Then ψ0, j := λ−1/2 G(·, γ−1 (z j )), j ∈ J form a frame for HG with the same frame bounds α, β. [sent-367, score-0.168]
</p><p>65 Applying Proposition 15 and f HG = f HK yields that {ψ0, j : j ∈ J} is a frame for HG with frame bounds α, β. [sent-371, score-0.168]
</p><p>66 1  The frame for the RKHS HG is now translated to a frame for the RKHSs HGn . [sent-372, score-0.168]
</p><p>67 2098  R EFINABLE K ERNELS  Proposition 20 Suppose that K is γ-reﬁnable and KZ is a Riesz basis for HK with frame bounds 0 < α ≤ β < ∞. [sent-373, score-0.118]
</p><p>68 Then for each n ∈ Z, ψn, j := λn/2 ψ0, j ◦ γn , j ∈ J form a frame for HGn with the same frame bounds α, β. [sent-374, score-0.168]
</p><p>69 For each n ∈ Z, the functions ˜ ˜ ˜ (58) ψn, j := φn+1, j − ∑ Ck, j φn,k , j ∈ J k∈J  constitute a frame for HGn and have the representation ˜ ψn, j =  ˜ ˜ ∑ D j,k φn+1,k . [sent-384, score-0.084]
</p><p>70 Arguments similar to those in the proof of ˜ Proposition 20 yield that ψn, j , j ∈ J form a frame for HGn with the same frame bounds as those of ˜ {φ0, j : j ∈ J} for HK . [sent-390, score-0.188]
</p><p>71 The RKHS HK has a multiresolution analysis if and only if Φ is reﬁnable, that is, it satisﬁes (20) for some bounded linear operator T on W whose adjoint T ∗ is isometric, T has the property (32) and there exists a countable subset Z of X such that Φ(Z ) is a Riesz basis for W . [sent-401, score-0.139]
</p><p>72 Reﬁnable Translation Invariant Kernels In this section, we consider reﬁnable kernels with specializing our input space to R d , d ∈ N, the mapping γ to the dilation mapping x → 2x in Rd and kernels to translation invariant kernels K on Rd , that is, for all x, y, a ∈ Rd K(x − a, y − a) = K(x, y). [sent-423, score-0.274]
</p><p>73 In other words, the main purpose of this section is to characterize reﬁnable translation invariant kernels on Rd . [sent-424, score-0.126]
</p><p>74 R ˆ Speciﬁcally, we shall characterize nonnegative k ∈ L1 (Rd ) for which the kernel K given by K(x, y) = k(x − y) =  Z  Rd  ˆ ei(x−y,t) k(t)dt, x, y ∈ Rd  (67)  is reﬁnable, that is, there holds  HK−1  HK . [sent-434, score-0.1]
</p><p>75 ˆ Theorem 23 Let K be the translation invariant kernel given in (67) through a nonnegative k ∈ 1 (Rd ) and let Ω be deﬁned by (72). [sent-447, score-0.117]
</p><p>76 ˆ Corollary 24 Let K be a reﬁnable translation invariant kernel deﬁned by (67). [sent-458, score-0.103]
</p><p>77 Proof Since the Gaussian kernels can be represented as Gσ (x, y) =  1 (2π)d  Z  Rd  π σ  d/2  ei(x−y,t) e−  t 2 4σ  dt,  · 2  and e− 4σ , supported on the whole Rd , is clearly continuous at t = 0, by Corollary 24, they are not reﬁnable. [sent-471, score-0.086]
</p><p>78 We now present a nontrivial reﬁnable translation invariant kernel. [sent-472, score-0.088]
</p><p>79 λ=2 We next characterize when HK−1 is a proper subspace of HK if K is a reﬁnable translation invariant kernel. [sent-477, score-0.091]
</p><p>80 We next present a necessary and sufﬁcient condition for KZd to be a Riesz basis for HK if K is a ˆ translation invariant kernel deﬁned by (67) through a nonnegative k ∈ L1 (Rd ). [sent-517, score-0.151]
</p><p>81 ˆ Theorem 30 Let K be a translation invariant kernel deﬁned by (67) through a nonnegative k ∈ 1 (Rd ) and Ω be deﬁned by (72). [sent-518, score-0.117]
</p><p>82 ˆ In the next theorem, we construct k that satisfy conditions (73), (74), (84) and (85) to obtain a d with K reﬁnable kernel K on R Zd being a Riesz basis for HK . [sent-529, score-0.085]
</p><p>83 Since (84) and (89) are true, we obtain by direct computation for all m, n ∈ Z d that 1 (K(·, n), K(·, m))HK η(2π)d  1 ei(m−n,t) dt (2π)d ZΩ 1 = ei(m−n,t) χΩ (t)dt (2π)d ZRd 1 ei(m−n,t) ∑ χΩ (· + 2lπ)dt = (2π)d [0,2π]d l∈Zd Z 1 = ei(m−n,t) dt = δm,n . [sent-545, score-0.092]
</p><p>84 By noting that (84) can be interpreted as that Ω + 2nπ, n ∈ Z d , form a tiling of Rd , we construct examples of reﬁnable kernels K such that KZd are Riesz bases for HK . [sent-547, score-0.094]
</p><p>85 Then the kernel K deﬁned by (67) with k having the form (89) is a reﬁnable kernel such that KZd is a Riesz basis for HK . [sent-551, score-0.136]
</p><p>86 2π 3  ≤α≤ (91)  ˆ Then the kernel K deﬁned by (67) with k having the form (89) with η = 1 is a reﬁnable kernel such that KZd is a Riesz basis for HK . [sent-556, score-0.136]
</p><p>87 Let ϕ be a compactly supported continuous function on R d that is reﬁnable, namely, there exists h := [hn : n ∈ Zd ] such that · ϕ = ∑ hn ϕ(· − n). [sent-564, score-0.144]
</p><p>88 (95)  ψ(x − n)ψ(y − n)  (96)  n∈Zd  constructed by a reﬁnable function ψ were considered in Opfer (2006), and kernels deﬁned as a superposition of frame elements in RKHS were discussed in Gao et al. [sent-570, score-0.158]
</p><p>89 The next proposition shows that kernels in the form (95) are more general than those in the degenerate form (96) and in general cannot be written in the degenerate form. [sent-576, score-0.11]
</p><p>90 Proof Since the operator A satisﬁes (93), there exists a bounded positive self-adjoint operator A 1/2 on 2 (Zd ) such that A1/2 A1/2 = A (see, Conway, 1990, ,page 240). [sent-598, score-0.114]
</p><p>91 Proof By Theorem 7, HK−1 is a proper subspace of HK if and only if the null space N (T ) of operator T deﬁned by (101) contains nonzero elements in 2 (Zd ), which is equivalent to that  N (H) = {0}. [sent-618, score-0.096]
</p><p>92 The last proposition provides a class of reﬁnable kernels given by (95) that never degenerate to the form (96). [sent-644, score-0.11]
</p><p>93 If there exists at least one n ∈ Zd such that the real part Re (hn ) of hn is not zero then A satisﬁes (97) if and only if a = 0. [sent-646, score-0.113]
</p><p>94 Then (80) holds true if and only if hn = 0, for at least one n ∈ Zd \ 2Zd . [sent-659, score-0.126]
</p><p>95 By condition (32) in Theorem 9, (80) holds true if and only if lim T j c 2 (Zd ) = 0, for all c ∈ 2 (Zd ), (108) j→∞  where T is the operator deﬁned by (101). [sent-663, score-0.091]
</p><p>96 (112)  If hn = 0 for each n ∈ Zd \ 2Zd then m0 (· + πν j ) = m0 for all j ∈ N2d . [sent-669, score-0.113]
</p><p>97 A Discussion of Applications and Conclusions For the completeness of the paper, in this section we discuss how reﬁnable kernels can be used to efﬁciently update kernels for learning from increasing training data. [sent-696, score-0.148]
</p><p>98 If K is reﬁnable on X = R then we update the kernel K to a new kernel K1 := λK(2·, 2·). [sent-716, score-0.102]
</p><p>99 Tight frame expansions of multiscale reproducing kernels in Sobolev spaces. [sent-979, score-0.217]
</p><p>100 Support vector machines, reproducing kernel Hilbert spaces and the randomized GACV. [sent-1060, score-0.09]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hk', 0.544), ('nable', 0.51), ('zd', 0.451), ('hkn', 0.217), ('riesz', 0.203), ('rd', 0.121), ('hgn', 0.117), ('hn', 0.113), ('frame', 0.084), ('kz', 0.084), ('efinable', 0.081), ('kdt', 0.077), ('kernels', 0.074), ('ernels', 0.069), ('kzd', 0.068), ('hang', 0.065), ('operator', 0.057), ('kn', 0.055), ('re', 0.053), ('kernel', 0.051), ('dt', 0.046), ('rkhs', 0.045), ('rakotomamonjy', 0.041), ('reproducing', 0.039), ('bm', 0.039), ('micchelli', 0.038), ('nontrivial', 0.036), ('proposition', 0.036), ('span', 0.036), ('daubechies', 0.034), ('basis', 0.034), ('multiresolution', 0.033), ('zk', 0.032), ('hg', 0.032), ('ei', 0.032), ('smale', 0.031), ('isometric', 0.031), ('translation', 0.03), ('hilbert', 0.029), ('fn', 0.027), ('suppose', 0.027), ('wavelet', 0.025), ('ah', 0.025), ('theorem', 0.025), ('dense', 0.023), ('walder', 0.023), ('invariant', 0.022), ('shall', 0.022), ('zl', 0.022), ('lim', 0.021), ('vu', 0.02), ('multiscale', 0.02), ('map', 0.02), ('subspace', 0.02), ('bases', 0.02), ('proof', 0.02), ('lemma', 0.02), ('lebesgue', 0.019), ('conversely', 0.019), ('ensures', 0.019), ('compactly', 0.019), ('proper', 0.019), ('amato', 0.018), ('augmentation', 0.018), ('ezd', 0.018), ('nability', 0.018), ('syracuse', 0.018), ('inclusion', 0.018), ('corollary', 0.017), ('characterizations', 0.017), ('mallat', 0.017), ('gn', 0.017), ('inner', 0.017), ('vn', 0.016), ('fourier', 0.016), ('characterization', 0.016), ('ha', 0.015), ('orthonormal', 0.015), ('nm', 0.015), ('adjoint', 0.015), ('predictor', 0.014), ('reconstruction', 0.014), ('pn', 0.014), ('nonnegative', 0.014), ('orthogonal', 0.014), ('cavaretta', 0.014), ('conway', 0.014), ('haizhang', 0.014), ('opfer', 0.014), ('yuesheng', 0.014), ('holds', 0.013), ('satis', 0.013), ('xu', 0.013), ('zhou', 0.013), ('sch', 0.013), ('chen', 0.013), ('cucker', 0.013), ('canu', 0.013), ('equation', 0.012), ('continuous', 0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="71-tfidf-1" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: Motivated by mathematical learning from training data, we introduce the notion of reﬁnable kernels. Various characterizations of reﬁnable kernels are presented. The concept of reﬁnable kernels leads to the introduction of wavelet-like reproducing kernels. We also investigate a reﬁnable kernel that forms a Riesz basis. In particular, we characterize reﬁnable translation invariant kernels, and reﬁnable kernels deﬁned by reﬁnable functions. This study leads to multiresolution analysis of reproducing kernel Hilbert spaces. Keywords: reﬁnable kernels, reﬁnable feature maps, wavelet-like reproducing kernels, dual kernels, learning with kernels, reproducing kernel Hilbert spaces, Riesz bases</p><p>2 0.26561344 <a title="71-tfidf-2" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>Author: Yiming Ying, Ding-Xuan Zhou</p><p>Abstract: Gaussian kernels with ﬂexible variances provide a rich family of Mercer kernels for learning algorithms. We show that the union of the unit balls of reproducing kernel Hilbert spaces generated by Gaussian kernels with ﬂexible variances is a uniform Glivenko-Cantelli (uGC) class. This result conﬁrms a conjecture concerning learnability of Gaussian kernels and veriﬁes the uniform convergence of many learning algorithms involving Gaussians with changing variances. Rademacher averages and empirical covering numbers are used to estimate sample errors of multi-kernel regularization schemes associated with general loss functions. It is then shown that the regularization error associated with the least square loss and the Gaussian kernels can be greatly improved when ﬂexible variances are allowed. Finally, for regularization schemes generated by Gaussian kernels with ﬂexible variances we present explicit learning rates for regression with least square loss and classiﬁcation with hinge loss. Keywords: Gaussian kernel, ﬂexible variances, learning theory, Glivenko-Cantelli class, regularization scheme, empirical covering number</p><p>3 0.20223817 <a title="71-tfidf-3" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>Author: Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee, Robert L. Wolpert</p><p>Abstract: Kernel methods have been very popular in the machine learning literature in the last ten years, mainly in the context of Tikhonov regularization algorithms. In this paper we study a coherent Bayesian kernel model based on an integral operator deﬁned as the convolution of a kernel with a signed measure. Priors on the random signed measures correspond to prior distributions on the functions mapped by the integral operator. We study several classes of signed measures and their image mapped by the integral operator. In particular, we identify a general class of measures whose image is dense in the reproducing kernel Hilbert space (RKHS) induced by the kernel. A consequence of this result is a function theoretic foundation for using non-parametric prior speciﬁcations in Bayesian modeling, such as Gaussian process and Dirichlet process prior distributions. We discuss the construction of priors on spaces of signed measures using Gaussian and L´ vy processes, e with the Dirichlet processes being a special case the latter. Computational issues involved with sampling from the posterior distribution are outlined for a univariate regression and a high dimensional classiﬁcation problem. Keywords: reproducing kernel Hilbert space, non-parametric Bayesian methods, L´ vy processes, e Dirichlet processes, integral operator, Gaussian processes c 2007 Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee and Robert L. Wolpert. P ILLAI , W U , L IANG , M UKHERJEE AND W OLPERT</p><p>4 0.060328469 <a title="71-tfidf-4" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>Author: Philippe Rigollet</p><p>Abstract: We consider semi-supervised classiﬁcation when part of the available data is unlabeled. These unlabeled data can be useful for the classiﬁcation problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples. Keywords: semi-supervised learning, statistical learning theory, classiﬁcation, cluster assumption, generalization bounds</p><p>5 0.046992745 <a title="71-tfidf-5" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>6 0.045500968 <a title="71-tfidf-6" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>7 0.043245897 <a title="71-tfidf-7" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>8 0.039882269 <a title="71-tfidf-8" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>9 0.031797666 <a title="71-tfidf-9" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>10 0.031514596 <a title="71-tfidf-10" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>11 0.029898657 <a title="71-tfidf-11" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>12 0.026193403 <a title="71-tfidf-12" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>13 0.021865707 <a title="71-tfidf-13" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>14 0.020358929 <a title="71-tfidf-14" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>15 0.019201368 <a title="71-tfidf-15" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>16 0.016933322 <a title="71-tfidf-16" href="./jmlr-2007-Local_Discriminant_Wavelet_Packet_Coordinates_for_Face_Recognition.html">50 jmlr-2007-Local Discriminant Wavelet Packet Coordinates for Face Recognition</a></p>
<p>17 0.016057698 <a title="71-tfidf-17" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>18 0.015608085 <a title="71-tfidf-18" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>19 0.015264783 <a title="71-tfidf-19" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>20 0.014943683 <a title="71-tfidf-20" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, -0.357), (2, 0.318), (3, 0.149), (4, 0.03), (5, 0.084), (6, 0.263), (7, -0.086), (8, -0.223), (9, -0.049), (10, -0.137), (11, -0.026), (12, 0.067), (13, 0.027), (14, -0.079), (15, -0.023), (16, 0.071), (17, -0.051), (18, 0.054), (19, -0.039), (20, -0.163), (21, -0.074), (22, -0.106), (23, -0.027), (24, -0.007), (25, -0.015), (26, -0.017), (27, -0.006), (28, -0.015), (29, 0.04), (30, 0.119), (31, -0.011), (32, -0.075), (33, -0.072), (34, 0.049), (35, -0.028), (36, -0.058), (37, -0.014), (38, 0.005), (39, 0.021), (40, -0.054), (41, 0.024), (42, 0.007), (43, 0.026), (44, -0.026), (45, 0.007), (46, 0.05), (47, -0.003), (48, 0.036), (49, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.98115313 <a title="71-lsi-1" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: Motivated by mathematical learning from training data, we introduce the notion of reﬁnable kernels. Various characterizations of reﬁnable kernels are presented. The concept of reﬁnable kernels leads to the introduction of wavelet-like reproducing kernels. We also investigate a reﬁnable kernel that forms a Riesz basis. In particular, we characterize reﬁnable translation invariant kernels, and reﬁnable kernels deﬁned by reﬁnable functions. This study leads to multiresolution analysis of reproducing kernel Hilbert spaces. Keywords: reﬁnable kernels, reﬁnable feature maps, wavelet-like reproducing kernels, dual kernels, learning with kernels, reproducing kernel Hilbert spaces, Riesz bases</p><p>2 0.76309639 <a title="71-lsi-2" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>Author: Yiming Ying, Ding-Xuan Zhou</p><p>Abstract: Gaussian kernels with ﬂexible variances provide a rich family of Mercer kernels for learning algorithms. We show that the union of the unit balls of reproducing kernel Hilbert spaces generated by Gaussian kernels with ﬂexible variances is a uniform Glivenko-Cantelli (uGC) class. This result conﬁrms a conjecture concerning learnability of Gaussian kernels and veriﬁes the uniform convergence of many learning algorithms involving Gaussians with changing variances. Rademacher averages and empirical covering numbers are used to estimate sample errors of multi-kernel regularization schemes associated with general loss functions. It is then shown that the regularization error associated with the least square loss and the Gaussian kernels can be greatly improved when ﬂexible variances are allowed. Finally, for regularization schemes generated by Gaussian kernels with ﬂexible variances we present explicit learning rates for regression with least square loss and classiﬁcation with hinge loss. Keywords: Gaussian kernel, ﬂexible variances, learning theory, Glivenko-Cantelli class, regularization scheme, empirical covering number</p><p>3 0.74564934 <a title="71-lsi-3" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>Author: Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee, Robert L. Wolpert</p><p>Abstract: Kernel methods have been very popular in the machine learning literature in the last ten years, mainly in the context of Tikhonov regularization algorithms. In this paper we study a coherent Bayesian kernel model based on an integral operator deﬁned as the convolution of a kernel with a signed measure. Priors on the random signed measures correspond to prior distributions on the functions mapped by the integral operator. We study several classes of signed measures and their image mapped by the integral operator. In particular, we identify a general class of measures whose image is dense in the reproducing kernel Hilbert space (RKHS) induced by the kernel. A consequence of this result is a function theoretic foundation for using non-parametric prior speciﬁcations in Bayesian modeling, such as Gaussian process and Dirichlet process prior distributions. We discuss the construction of priors on spaces of signed measures using Gaussian and L´ vy processes, e with the Dirichlet processes being a special case the latter. Computational issues involved with sampling from the posterior distribution are outlined for a univariate regression and a high dimensional classiﬁcation problem. Keywords: reproducing kernel Hilbert space, non-parametric Bayesian methods, L´ vy processes, e Dirichlet processes, integral operator, Gaussian processes c 2007 Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee and Robert L. Wolpert. P ILLAI , W U , L IANG , M UKHERJEE AND W OLPERT</p><p>4 0.20703089 <a title="71-lsi-4" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>5 0.19950743 <a title="71-lsi-5" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>Author: Philippe Rigollet</p><p>Abstract: We consider semi-supervised classiﬁcation when part of the available data is unlabeled. These unlabeled data can be useful for the classiﬁcation problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples. Keywords: semi-supervised learning, statistical learning theory, classiﬁcation, cluster assumption, generalization bounds</p><p>6 0.18361139 <a title="71-lsi-6" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>7 0.17606288 <a title="71-lsi-7" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>8 0.14884387 <a title="71-lsi-8" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>9 0.14804152 <a title="71-lsi-9" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>10 0.12518838 <a title="71-lsi-10" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>11 0.10536564 <a title="71-lsi-11" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>12 0.098317415 <a title="71-lsi-12" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>13 0.097213365 <a title="71-lsi-13" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>14 0.094290443 <a title="71-lsi-14" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>15 0.092077032 <a title="71-lsi-15" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>16 0.091317259 <a title="71-lsi-16" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>17 0.087825686 <a title="71-lsi-17" href="./jmlr-2007-Concave_Learners_for_Rankboost.html">23 jmlr-2007-Concave Learners for Rankboost</a></p>
<p>18 0.087079316 <a title="71-lsi-18" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>19 0.078325741 <a title="71-lsi-19" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>20 0.065923773 <a title="71-lsi-20" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.017), (8, 0.027), (10, 0.019), (12, 0.045), (28, 0.107), (40, 0.073), (41, 0.39), (48, 0.019), (60, 0.056), (85, 0.032), (98, 0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72896284 <a title="71-lda-1" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: Motivated by mathematical learning from training data, we introduce the notion of reﬁnable kernels. Various characterizations of reﬁnable kernels are presented. The concept of reﬁnable kernels leads to the introduction of wavelet-like reproducing kernels. We also investigate a reﬁnable kernel that forms a Riesz basis. In particular, we characterize reﬁnable translation invariant kernels, and reﬁnable kernels deﬁned by reﬁnable functions. This study leads to multiresolution analysis of reproducing kernel Hilbert spaces. Keywords: reﬁnable kernels, reﬁnable feature maps, wavelet-like reproducing kernels, dual kernels, learning with kernels, reproducing kernel Hilbert spaces, Riesz bases</p><p>2 0.69032842 <a title="71-lda-2" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: In numerous domains it is useful to represent a single example by the set of the local features or parts that comprise it. However, this representation poses a challenge to many conventional machine learning techniques, since sets may vary in cardinality and elements lack a meaningful ordering. Kernel methods can learn complex functions, but a kernel over unordered set inputs must somehow solve for correspondences—generally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function called the pyramid match that measures partial match similarity in time linear in the number of features. The pyramid match maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in order to ﬁnd implicit correspondences based on the ﬁnest resolution histogram cell where a matched pair ﬁrst appears. We show the pyramid match yields a Mercer kernel, and we prove bounds on its error relative to the optimal partial matching cost. We demonstrate our algorithm on both classiﬁcation and regression tasks, including object recognition, 3-D human pose inference, and time of publication estimation for documents, and we show that the proposed method is accurate and signiﬁcantly more efﬁcient than current approaches. Keywords: kernel, sets of features, histogram intersection, multi-resolution histogram pyramid, approximate matching, object recognition</p><p>3 0.36133796 <a title="71-lda-3" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>Author: Marta Arias, Roni Khardon, Jérôme Maloberti</p><p>Abstract: The paper introduces L OG A N -H —a system for learning ﬁrst-order function-free Horn expressions from interpretations. The system is based on an algorithm that learns by asking questions and that was proved correct in previous work. The current paper shows how the algorithm can be implemented in a practical system, and introduces a new algorithm based on it that avoids interaction and learns from examples only. The L OG A N -H system implements these algorithms and adds several facilities and optimizations that allow efﬁcient applications in a wide range of problems. As one of the important ingredients, the system includes several fast procedures for solving the subsumption problem, an NP-complete problem that needs to be solved many times during the learning process. We describe qualitative and quantitative experiments in several domains. The experiments demonstrate that the system can deal with varied problems, large amounts of data, and that it achieves good classiﬁcation accuracy. Keywords: inductive logic programming, subsumption, bottom-up learning, learning with queries</p><p>4 0.35971043 <a title="71-lda-4" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>Author: Yiming Ying, Ding-Xuan Zhou</p><p>Abstract: Gaussian kernels with ﬂexible variances provide a rich family of Mercer kernels for learning algorithms. We show that the union of the unit balls of reproducing kernel Hilbert spaces generated by Gaussian kernels with ﬂexible variances is a uniform Glivenko-Cantelli (uGC) class. This result conﬁrms a conjecture concerning learnability of Gaussian kernels and veriﬁes the uniform convergence of many learning algorithms involving Gaussians with changing variances. Rademacher averages and empirical covering numbers are used to estimate sample errors of multi-kernel regularization schemes associated with general loss functions. It is then shown that the regularization error associated with the least square loss and the Gaussian kernels can be greatly improved when ﬂexible variances are allowed. Finally, for regularization schemes generated by Gaussian kernels with ﬂexible variances we present explicit learning rates for regression with least square loss and classiﬁcation with hinge loss. Keywords: Gaussian kernel, ﬂexible variances, learning theory, Glivenko-Cantelli class, regularization scheme, empirical covering number</p><p>5 0.34629121 <a title="71-lda-5" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>Author: David Mease, Abraham J. Wyner, Andreas Buja</p><p>Abstract: The standard by which binary classiﬁers are usually judged, misclassiﬁcation error, assumes equal costs of misclassifying the two classes or, equivalently, classifying at the 1/2 quantile of the conditional class probability function P[y = 1|x]. Boosted classiﬁcation trees are known to perform quite well for such problems. In this article we consider the use of standard, off-the-shelf boosting for two more general problems: 1) classiﬁcation with unequal costs or, equivalently, classiﬁcation at quantiles other than 1/2, and 2) estimation of the conditional class probability function P[y = 1|x]. We ﬁrst examine whether the latter problem, estimation of P[y = 1|x], can be solved with LogitBoost, and with AdaBoost when combined with a natural link function. The answer is negative: both approaches are often ineffective because they overﬁt P[y = 1|x] even though they perform well as classiﬁers. A major negative point of the present article is the disconnect between class probability estimation and classiﬁcation. Next we consider the practice of over/under-sampling of the two classes. We present an algorithm that uses AdaBoost in conjunction with Over/Under-Sampling and Jittering of the data (“JOUS-Boost”). This algorithm is simple, yet successful, and it preserves the advantage of relative protection against overﬁtting, but for arbitrary misclassiﬁcation costs and, equivalently, arbitrary quantile boundaries. We then use collections of classiﬁers obtained from a grid of quantiles to form estimators of class probabilities. The estimates of the class probabilities compare favorably to those obtained by a variety of methods across both simulated and real data sets. Keywords: boosting algorithms, LogitBoost, AdaBoost, class probability estimation, over-sampling, under-sampling, stratiﬁcation, data jittering</p><p>6 0.34346822 <a title="71-lda-6" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>7 0.34030011 <a title="71-lda-7" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>8 0.33664581 <a title="71-lda-8" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>9 0.3339158 <a title="71-lda-9" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>10 0.33195189 <a title="71-lda-10" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>11 0.32838428 <a title="71-lda-11" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>12 0.32468188 <a title="71-lda-12" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>13 0.32418355 <a title="71-lda-13" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>14 0.32341057 <a title="71-lda-14" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>15 0.32320467 <a title="71-lda-15" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>16 0.32307887 <a title="71-lda-16" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>17 0.3211827 <a title="71-lda-17" href="./jmlr-2007-Stagewise_Lasso.html">77 jmlr-2007-Stagewise Lasso</a></p>
<p>18 0.32077289 <a title="71-lda-18" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>19 0.31993926 <a title="71-lda-19" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>20 0.31974149 <a title="71-lda-20" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
