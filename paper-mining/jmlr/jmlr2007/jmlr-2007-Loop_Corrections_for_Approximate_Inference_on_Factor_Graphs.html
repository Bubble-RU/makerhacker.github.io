<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 jmlr-2007-Loop Corrections for Approximate Inference on Factor Graphs</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-51" href="#">jmlr2007-51</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 jmlr-2007-Loop Corrections for Approximate Inference on Factor Graphs</h1>
<br/><p>Source: <a title="jmlr-2007-51-pdf" href="http://jmlr.org/papers/volume8/mooij07a/mooij07a.pdf">pdf</a></p><p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a method to improve approximate inference methods by correcting for the inﬂuence of loops in the graphical model. The method is a generalization and alternative implementation of a recent idea from Montanari and Rizzo (2005). It is applicable to arbitrary factor graphs, provided that the size of the Markov blankets is not too large. It consists of two steps: (i) an approximate inference method, for example, belief propagation, is used to approximate cavity distributions for each variable (i.e., probability distributions on the Markov blanket of a variable for a modiﬁed graphical model in which the factors involving that variable have been removed); (ii) all cavity distributions are improved by a message-passing algorithm that cancels out approximation errors by imposing certain consistency constraints. This loop correction (LC) method usually gives signiﬁcantly better results than the original, uncorrected, approximate inference algorithm that is used to estimate the effect of loops. Indeed, we often observe that the loop-corrected error is approximately the square of the error of the uncorrected approximate inference method. In this article, we compare different variants of the loop correction method with other approximate inference methods on a variety of graphical models, including “real world” networks, and conclude that the LC method generally obtains the most accurate results. Keywords: loop corrections, approximate inference, graphical models, factor graphs, belief propagation</p><p>Reference: <a title="jmlr-2007-51-reference" href="../jmlr2007_reference/jmlr-2007-Loop_Corrections_for_Approximate_Inference_on_Factor_Graphs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 NL  Department of Biophysics Radboud University Nijmegen 6525 EZ Nijmegen, The Netherlands  Editor: Michael Jordan  Abstract We propose a method to improve approximate inference methods by correcting for the inﬂuence of loops in the graphical model. [sent-9, score-0.422]
</p><p>2 It consists of two steps: (i) an approximate inference method, for example, belief propagation, is used to approximate cavity distributions for each variable (i. [sent-12, score-0.78]
</p><p>3 This loop correction (LC) method usually gives signiﬁcantly better results than the original, uncorrected, approximate inference algorithm that is used to estimate the effect of loops. [sent-15, score-0.317]
</p><p>4 In this article, we compare different variants of the loop correction method with other approximate inference methods on a variety of graphical models, including “real world” networks, and conclude that the LC method generally obtains the most accurate results. [sent-17, score-0.429]
</p><p>5 Keywords: loop corrections, approximate inference, graphical models, factor graphs, belief propagation  1. [sent-18, score-0.42]
</p><p>6 However, if the inﬂuence of loops is large, the approximate marginals calculated by BP can have large errors and the quality of the BP results may not be satisfactory. [sent-31, score-0.355]
</p><p>7 However, choosing a good set of outer clusters is highly nontrivial, and in general this method will only work if the clusters do not have many intersections, or in other words, if the loops do not have many intersections (see also Welling et al. [sent-39, score-0.28]
</p><p>8 However, loops that consist of two or more interactions that are not part of the base tree are approximated in a similar way as in BP. [sent-45, score-0.296]
</p><p>9 In this article we propose a method that takes into account all the loops in the graphical model in an approximate way and therefore obtains more accurate results in many cases. [sent-47, score-0.377]
</p><p>10 A cavity distribution of some variable is the probability distribution on its Markov blanket (all its neighboring variables) of a modiﬁed graphical model, in which all factors involving that variable have been removed. [sent-50, score-0.684]
</p><p>11 The removal of the factors breaks all the loops in which that variable takes part. [sent-51, score-0.276]
</p><p>12 This allows an approximate inference algorithm to estimate the strength of these loops in terms of effective interactions or correlations between the variables of the Markov blanket. [sent-52, score-0.468]
</p><p>13 Even more accuracy is obtained by imposing certain consistency relations between the cavity distributions, which results in a cancellation of errors to some extent. [sent-54, score-0.525]
</p><p>14 On the other hand, using better estimates of the effective interactions in the cavity distributions yields accurate loop-corrected results. [sent-56, score-0.653]
</p><p>15 The cavity distribution Z \i (x∂i ) is the (unnormalized) marginal on x∂i of the probability distribution corresponding to the cavity graph (b). [sent-111, score-1.062]
</p><p>16 2 Cavity Networks and Loop Corrections The notion of a cavity stems from statistical physics, where it was used originally to calculate properties of random ensembles of certain graphical models (M´ zard et al. [sent-115, score-0.585]
</p><p>17 A cavity is e obtained by removing one variable from the graphical model, together with all the factors in which that variable participates. [sent-117, score-0.663]
</p><p>18 In our context, we deﬁne cavity networks as follows (see also Figure 1): Deﬁnition 2. [sent-118, score-0.504]
</p><p>19 1 Given a graphical model (V , F , {ψI }I∈F ) and a variable i ∈ V , the cavity network of variable i is the graphical model (V \ i, F \ Ni , {ψI }I∈F \Ni ). [sent-119, score-0.7]
</p><p>20 The probability distribution corresponding to the cavity network of variable i is thus proportional to: Ψ\Ni (x\i ) = ∏ ψI (xI ). [sent-120, score-0.538]
</p><p>21 I∈F i∈I  Summing out all the variables, except for the neighbors ∂i of i, gives what we will call the cavity distribution: 1116  L OOP C ORRECTIONS FOR A PPROXIMATE I NFERENCE ON FACTOR G RAPHS  Deﬁnition 2. [sent-121, score-0.504]
</p><p>22 2 Given a graphical model (V , F , {ψI }I∈F ) and a variable i ∈ V , the cavity distribution of i is Z \i (x∂i ) := ∑ Ψ\Ni (x\i ). [sent-122, score-0.585]
</p><p>23 (2)  x\∆i  Thus the cavity distribution of i is proportional to the marginal of the cavity network of i on the Markov blanket ∂i. [sent-123, score-1.085]
</p><p>24 The cavity distribution describes the effective interactions (or correlations) induced by the cavity network on the neighbors ∂i of variable i. [sent-124, score-1.14]
</p><p>25 (3)  Thus, given the cavity distribution Z \i (x∂i ), one can calculate the marginal distribution of the original graphical model P on x∆i , provided that the cardinality of X∆i is not too large. [sent-126, score-0.607]
</p><p>26 In practice, exact cavity distributions are not known, and the only way to proceed is to use approximate cavity distributions. [sent-127, score-1.123]
</p><p>27 The latter approach generally gives more accurate results, since the complexity of the cavity network is less than that of the original network. [sent-131, score-0.569]
</p><p>28 In particular, the cavity network of variable i contains no loops involving that variable, since all factors in which i participates have been removed (e. [sent-132, score-0.835]
</p><p>29 , the loop i − J − l − O − m − K − i in the original network, Figure 1(a), is not present in the cavity network, Figure 1(b)). [sent-134, score-0.618]
</p><p>30 It does not, however, take into account the other loops in the original graphical model. [sent-136, score-0.279]
</p><p>31 The basic idea of the loop correction approach of Montanari and Rizzo (2005) is to use the latter approach for all variables in the network, but to adjust the approximate cavity distributions in order to cancel out approximation errors before (3) is used to obtain the ﬁnal approximate marginals. [sent-137, score-0.881]
</p><p>32 3 Combining Approximate Cavity Distributions to Cancel Out Errors \i  Suppose that we have obtained an initial approximation ζ 0 (x∂i ) of the (exact) cavity distribution Z \i (x∂i ), for each i ∈ V . [sent-146, score-0.546]
</p><p>33 Let i ∈ V and consider the approximation error of the cavity distribution of i, that is, the exact cavity distribution of i divided by its approximation: Z \i (x∂i ) \i  ζ0 (x∂i ) 1117  . [sent-147, score-1.081]
</p><p>34 It turns out that the error factors can indeed be calculated by exploiting the redundancy of the in\i formation in the initial cavity approximations {ζ0 }i∈V . [sent-153, score-0.663]
</p><p>35 Using (2) and summing over all xk for k ∈ Y \ i, we obtain the following equation, which holds for the exact cavity distributions Z \i and Z \ j :  ∑ ∑ ΨN \Y Z \i = ∑ ∑ ΨN \Y Z \ j . [sent-160, score-0.552]
</p><p>36 The input consists \i of the initial approximations {ζ0 }i∈V to the cavity distributions. [sent-174, score-0.56]
</p><p>37 It calculates the error factors that satisfy (5) by ﬁxed point iteration and from the ﬁxed point, it calculates improved approximations of the cavity distributions {ζ\i }i∈V using Equation (4). [sent-175, score-0.661]
</p><p>38 1 From the improved cavity distributions, the loop-corrected approximations to the single-variable marginals of the original probability distribution (1) can be calculated as follows: Pi (xi ) ≈ bi (xi ) ∝ ∑ ΨNi (x∆i )ζ\i (x∂i ),  (6)  x∂i  where the factor ψY is now included. [sent-176, score-0.675]
</p><p>39 Alternatively, one could formulate the updates directly in terms of the cavity distributions {ζ \i }. [sent-183, score-0.524]
</p><p>40 4 A Special Case: Factorized Cavity Distributions In the previous subsection we have discussed how to improve approximations of cavity distributions. [sent-190, score-0.538]
</p><p>41 We will show that if the factor graph does not contain short loops consisting of four nodes, ﬁxed points of the standard BP algorithm are also ﬁxed points of Algorithm 1. [sent-193, score-0.301]
</p><p>42 If all factors involve at most two variables, one can easily arrange for the factor graph to have no loops of four nodes. [sent-196, score-0.379]
</p><p>43 See Figure 1(a) for an example of a factor graph which has no loops of four nodes. [sent-197, score-0.301]
</p><p>44 1 If the factor graph corresponding to (1) has no loops of exactly four nodes, and all initial approximate cavity distributions factorize in the following way: \i  ζ0 (x∂i ) =  ∏ ξI (xI\i ) \i  I∈Ni  ∀i ∈ V ,  (8)  then ﬁxed points of the BP algorithm can be mapped to ﬁxed points of Algorithm 1. [sent-200, score-0.937]
</p><p>45 If the factor graph does contain loops of four nodes, we usually observe that the ﬁxed point of Algorithm 1 coincides with the solution of the “minimal” CVM approximation when using factorized initial cavity approximations as in (8). [sent-218, score-0.881]
</p><p>46 The minimal CVM approximation uses all maximal factors as outer clusters (a maximal factor is a factor deﬁned on a domain which is not a strict subset of the domain of another factor). [sent-219, score-0.295]
</p><p>47 5 Obtaining Initial Approximate Cavity Distributions \i  There is no principled way to obtain the initial cavity approximations ζ 0 (x∂i ). [sent-223, score-0.56]
</p><p>48 In the previous subsection, we investigated the results of applying the LC algorithm on factorizing initial cavity approximations. [sent-224, score-0.526]
</p><p>49 Here, we will describe one method, which uses BP on clamped cavity networks. [sent-226, score-0.561]
</p><p>50 This method captures all interactions in the cavity distribution of i in an approximate way and can lead to very accurate results. [sent-227, score-0.7]
</p><p>51 One could also choose the method for each cavity separately, trading accuracy versus computation time. [sent-231, score-0.533]
</p><p>52 For each possible state of x ∂i , run BP on the \i cavity network clamped to that state x∂i and calculate the corresponding Bethe free energy FBethe (x∂i ) (Yedidia et al. [sent-234, score-0.595]
</p><p>53 Then, take the following initial approximate cavity distribution: \i  \i  ζ0 (x∂i ) ∝ e−FBethe (x∂i ) . [sent-236, score-0.593]
</p><p>54 However, many networks encountered in applications are relatively sparse and have limited cavity size and the computational cost may be acceptable. [sent-238, score-0.504]
</p><p>55 This particular way of obtaining initial cavity distributions has the following interesting property: in case the factor graph contains only a single loop and assuming that the ﬁxed point is unique, the ﬁnal beliefs (7) resulting from Algorithm 1 are exact. [sent-239, score-0.801]
</p><p>56 First, consider the case that i is part of the loop; removing i will break the loop and the remaining cavity network will be singly connected. [sent-242, score-0.652]
</p><p>57 The cavity distribution approximated by BP will thus be exact. [sent-243, score-0.504]
</p><p>58 In a draft version of this work (Mooij and Kappen, 2006), we conjectured that the result of Algorithm 1, when initialized with factorizing initial cavity approximations, would always coincide with the minimal CVM approximation. [sent-245, score-0.526]
</p><p>59 6 Differences with the Original Implementation As mentioned before, the idea of estimating the cavity distributions and imposing certain consistency relations amongst them has been ﬁrst presented in Montanari and Rizzo (2005). [sent-252, score-0.545]
</p><p>60 An important difference is that Montanari and Rizzo (2005) suggest to deform the initial approximate cavity distributions by altering certain cumulants (also called “connected correlations”), instead of altering certain interactions. [sent-261, score-0.784]
</p><p>61 Montanari and Rizzo (2005) propose to approximate the cavity distributions by estimating the pair cumulants and assuming higher-order cumulants to be zero. [sent-267, score-0.933]
</p><p>62 The assumption suggested in Montanari and Rizzo (2005) that higher-order cumulants are zero is the most important difference with our method, which instead takes into account effective interactions in the cavity distribution of all orders. [sent-274, score-0.773]
</p><p>63 A minor difference lies in the method to obtain initial approximations to the cavity distributions. [sent-276, score-0.56]
</p><p>64 This difference is not very important, since one could also use BP on clamped cavity networks instead, which turns out to give almost identical results. [sent-278, score-0.561]
</p><p>65 We generalized the method of choosing the base tree described in Minka and Qi (2004) to multiple variable factors as follows: when estimating the mutual information between xi and x j , we take the product of the marginals on {i, j} of all the factors that involve xi and/or x j . [sent-288, score-0.29]
</p><p>66 LCBP-Cum The original cumulant-based loop correction scheme by Montanari and Rizzo (2005), using response propagation (also known as linear response) to approximate the initial pairwise cavity cumulants. [sent-294, score-0.841]
</p><p>67 CVM-Loopk A double-loop implementation of CVM, using as outer clusters all (maximal) factors together with all loops in the factor graph that consist of up to k different variables (for k = 3, 4, 5, 6, 8). [sent-305, score-0.489]
</p><p>68 For each approximate inference method, we report the maximum ∞ error of the approximate single-variable marginals bi , calculated as follows: Error := max max |bi (xi ) − P(xi )| i∈V xi ∈Xi  where P(xi ) is the exact marginal calculated using the JunctionTree method. [sent-312, score-0.385]
</p><p>69 The loop-corrected versions are the result of Algorithm 1, initialized with approximate cavity distributions obtained by the procedure described in Section 2. [sent-347, score-0.591]
</p><p>70 This is mainly due to the fact that LCBP also takes into account effective triple interactions in the initial estimates of the approximate cavity distributions. [sent-377, score-0.691]
</p><p>71 We speculate that the reason for the break-down of LCBP-Cum and LCBP-Cum-Lin for strong interactions is due to the choice of cumulants instead of interactions. [sent-396, score-0.269]
</p><p>72 We believe that something similar happens for LCBP-Cum (and LCBP-Cum-Lin): for strong interactions, the approximate pair cumulants in the cavity are strong, and even tiny errors can lead to inconsistencies which prevent convergence. [sent-400, score-0.742]
</p><p>73 The CVMLoop methods, with clusters reﬂecting the short loops present in the factor graph, do indeed improve 1128  L OOP C ORRECTIONS FOR A PPROXIMATE I NFERENCE ON FACTOR G RAPHS  10 Time (s)  100  10−2 Max. [sent-402, score-0.296]
</p><p>74 We conclude that the CVM-Loop approach to loop correction is not very efﬁcient if there are many loops present. [sent-468, score-0.372]
</p><p>75 5 S CALING  WITH  d  It is also interesting to see how various methods scale with d, the variable degree, which is directly related to the cavity size. [sent-484, score-0.504]
</p><p>76 1 that for these binary, pairwise graphical models, LCBP is the best method for obtaining high accuracy marginals if the graphs are sparse, LCBP-Cum-Lin is the best method if the graphs are dense and LCBP-Cum shows no clear advantages over either method. [sent-502, score-0.276]
</p><p>77 2 Multi-variable Factors We now go beyond pairwise interactions and study a class of random factor graphs with binary variables and uniform factor degree |I| = k (for all I ∈ F ) with k > 2. [sent-504, score-0.35]
</p><p>78 The factor graphs are constructed by starting from an empty / / graphical model (V , 0, 0) and adding M random factors, where each factor is obtained in the following way: a subset I = {I1 , . [sent-506, score-0.271]
</p><p>79 The reason that we require the factor graph to be connected is that not all our approximate inference method implementations currently support connected factor graphs that consist of more than one connected component. [sent-516, score-0.365]
</p><p>80 In addition to the usual approximate inference methods, we have compared with GBPMin, a GBP implementation of the minimal CVM approximation that uses maximal factors as outer clusters. [sent-533, score-0.295]
</p><p>81 412 · 10−05  Table 1: Results for the ALARM network based loop LCBP methods are not available, due to the presence of factors involving more than two variables and variables that can take more than two values. [sent-575, score-0.284]
</p><p>82 , BP) by correcting for the inﬂuence of loops in the factor graph. [sent-632, score-0.269]
</p><p>83 We have proved that the method is a generalization of BP if the initial approximate cavity distributions factorize and the factor graph does not contain short loops of exactly four nodes. [sent-633, score-0.937]
</p><p>84 If the factor graph does contain such short loops, we observe in many cases that the method reduces to the minimal CVM approximation if one applies it on factorized initial approximate cavity distributions. [sent-634, score-0.716]
</p><p>85 If, on the other hand, the LC method is applied in combination with BP estimates of the effective cavity interactions, we have seen that the loop-corrected error is approximately the square of the uncorrected BP error. [sent-635, score-0.577]
</p><p>86 For practical purposes, we suggest to apply loop corrections to BP (“LCBP”), because the loop correction approach requires many runs of the approximate inference method and BP is well suited for this job because of its speed. [sent-637, score-0.462]
</p><p>87 On sparse factor graphs, TreeEP can obtain signiﬁcant improvements over BP by correcting for loops that consist of part of the base tree and one additional interaction, using little computation time. [sent-641, score-0.298]
</p><p>88 The original implementations work with cumulants instead of interactions and we believe that this explains the observed convergence difﬁculties of LCBP-Cum and LCBP-CumLin in the regime of strong interactions. [sent-652, score-0.269]
</p><p>89 This is mainly due to the fact that LCBP estimates the higher-order effective interactions in the cavity distributions. [sent-654, score-0.602]
</p><p>90 The reason is that we have intentionally selected experiments for which exact inference is still feasible, in order to be able to compare the quality of various approximate inference methods. [sent-662, score-0.271]
</p><p>91 The work presented here provides some intuition that may be helpful for constructing a general and fast loop correction method that is applicable to arbitrary factor graphs that can have large Markov blankets. [sent-678, score-0.293]
</p><p>92 Summarizing, the approach to loop corrections by Chertkov and Chernyak (2006b) takes a subset of loops into account in an exact way, whereas the loop correction approach presented in this article takes all loops into account in an approximate way. [sent-690, score-0.81]
</p><p>93 Summarizing, we have proposed a method to correct approximate inference methods for the inﬂuence of loops in the factor graph. [sent-692, score-0.412]
</p><p>94 The rather large computation time required is an issue which deserves further consideration; it may be possible to use additional approximations on top of the loop correction framework that trade quality for computation time. [sent-695, score-0.29]
</p><p>95 i∈V j∈∂i  Let i ∈ V and consider the corresponding cavity network of i. [sent-704, score-0.538]
</p><p>96 12 The cavity cumulants (also called “connected correlations”) C A are related to the moments in the following way: \i MA =  ∑  ∏ CE  \i  B ∈Part(A ) E ∈B  where Part(A ) is the set of partitions of A . [sent-706, score-0.7]
</p><p>97 \i  tiA MA  (10)  On the other hand, the same expectation value can also be expressed in terms of cavity moments of j as follows: \j \j tanh θ j ∑ t j B MB + ∑ t j B MB  ∑  A ∈P+ (∂ j\i)  A ∈P+ (∂ j\i)  \j  t jB MB + tanh θ j  A ∈P− (∂ j\i)  ∑  A ∈P− (∂ j\i)  \j  t j B MB  . [sent-710, score-0.645]
</p><p>98 In Montanari and Rizzo (2005), the notation CA is used for the cavity moment MA . [sent-713, score-0.504]
</p><p>99 1 Neglecting Higher-order Cumulants \i  Montanari and Rizzo proceed by neglecting cavity cumulants C A with |A | > 2. [sent-716, score-0.675]
</p><p>100 Thus, neglecting higherorder cavity cumulants amounts to the following approximation: \i MA ≈  ∑  ∏ CE . [sent-718, score-0.675]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cavity', 0.504), ('bp', 0.401), ('lcbp', 0.394), ('treeep', 0.233), ('loops', 0.198), ('cumulants', 0.171), ('montanari', 0.145), ('rizzo', 0.139), ('cvm', 0.119), ('loop', 0.114), ('ni', 0.103), ('orrections', 0.103), ('interactions', 0.098), ('tia', 0.089), ('pproximate', 0.087), ('raphs', 0.087), ('appen', 0.087), ('ooij', 0.087), ('oop', 0.087), ('graphical', 0.081), ('factors', 0.078), ('inference', 0.076), ('promedas', 0.075), ('nference', 0.071), ('factor', 0.071), ('mooij', 0.07), ('approximate', 0.067), ('marginals', 0.066), ('correction', 0.06), ('mf', 0.058), ('tanh', 0.058), ('mb', 0.057), ('clamped', 0.057), ('bethe', 0.052), ('linearized', 0.048), ('uncorrected', 0.048), ('graphs', 0.048), ('damping', 0.046), ('kappen', 0.046), ('belief', 0.046), ('junctiontree', 0.041), ('propagation', 0.041), ('ndings', 0.039), ('beliefs', 0.038), ('ji', 0.037), ('alarm', 0.036), ('chertkov', 0.035), ('couplings', 0.035), ('linearization', 0.034), ('network', 0.034), ('xi', 0.034), ('approximations', 0.034), ('pairwise', 0.033), ('graph', 0.032), ('interaction', 0.032), ('converged', 0.032), ('lc', 0.031), ('yedidia', 0.031), ('corrections', 0.031), ('accurate', 0.031), ('blankets', 0.029), ('gbp', 0.029), ('minka', 0.029), ('variables', 0.029), ('computation', 0.029), ('xy', 0.028), ('qi', 0.028), ('exact', 0.028), ('outer', 0.028), ('clusters', 0.027), ('denser', 0.027), ('lcmf', 0.027), ('lctreeep', 0.027), ('markov', 0.027), ('nodes', 0.027), ('implementation', 0.026), ('elds', 0.025), ('error', 0.025), ('moments', 0.025), ('quality', 0.024), ('heskes', 0.024), ('factorize', 0.023), ('chernyak', 0.023), ('kikuchi', 0.023), ('kschischang', 0.023), ('parisi', 0.023), ('update', 0.022), ('patient', 0.022), ('marginal', 0.022), ('initial', 0.022), ('consistency', 0.021), ('diagnoses', 0.021), ('cumulant', 0.021), ('blanket', 0.021), ('ijgp', 0.021), ('participates', 0.021), ('approximation', 0.02), ('distributions', 0.02), ('ma', 0.019), ('mechanics', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="51-tfidf-1" href="./jmlr-2007-Loop_Corrections_for_Approximate_Inference_on_Factor_Graphs.html">51 jmlr-2007-Loop Corrections for Approximate Inference on Factor Graphs</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a method to improve approximate inference methods by correcting for the inﬂuence of loops in the graphical model. The method is a generalization and alternative implementation of a recent idea from Montanari and Rizzo (2005). It is applicable to arbitrary factor graphs, provided that the size of the Markov blankets is not too large. It consists of two steps: (i) an approximate inference method, for example, belief propagation, is used to approximate cavity distributions for each variable (i.e., probability distributions on the Markov blanket of a variable for a modiﬁed graphical model in which the factors involving that variable have been removed); (ii) all cavity distributions are improved by a message-passing algorithm that cancels out approximation errors by imposing certain consistency constraints. This loop correction (LC) method usually gives signiﬁcantly better results than the original, uncorrected, approximate inference algorithm that is used to estimate the effect of loops. Indeed, we often observe that the loop-corrected error is approximately the square of the error of the uncorrected approximate inference method. In this article, we compare different variants of the loop correction method with other approximate inference methods on a variety of graphical models, including “real world” networks, and conclude that the LC method generally obtains the most accurate results. Keywords: loop corrections, approximate inference, graphical models, factor graphs, belief propagation</p><p>2 0.42689797 <a title="51-tfidf-2" href="./jmlr-2007-Truncating_the_Loop_Series_Expansion_for_Belief_Propagation.html">86 jmlr-2007-Truncating the Loop Series Expansion for Belief Propagation</a></p>
<p>Author: Vicenç Gómez, Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: Recently, Chertkov and Chernyak (2006b) derived an exact expression for the partition sum (normalization constant) corresponding to a graphical model, which is an expansion around the belief propagation (BP) solution. By adding correction terms to the BP free energy, one for each “generalized loop” in the factor graph, the exact partition sum is obtained. However, the usually enormous number of generalized loops generally prohibits summation over all correction terms. In this article we introduce truncated loop series BP (TLSBP), a particular way of truncating the loop series of Chertkov & Chernyak by considering generalized loops as compositions of simple loops. We analyze the performance of TLSBP in different scenarios, including the Ising model on square grids and regular random graphs, and on PROMEDAS, a large probabilistic medical diagnostic system. We show that TLSBP often improves upon the accuracy of the BP solution, at the expense of increased computation time. We also show that the performance of TLSBP strongly depends on the degree of interaction between the variables. For weak interactions, truncating the series leads to signiﬁcant improvements, whereas for strong interactions it can be ineffective, even if a high number of terms is considered. Keywords: belief propagation, loop calculus, approximate inference, partition function, Ising grid, random regular graphs, medical diagnosis</p><p>3 0.10061456 <a title="51-tfidf-3" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>Author: Gaëlle Loosli, Stéphane Canu</p><p>Abstract: In a recently published paper in JMLR, Tsang et al. (2005) present an algorithm for SVM called Core Vector Machines (CVM) and illustrate its performances through comparisons with other SVM solvers. After reading the CVM paper we were surprised by some of the reported results. In order to clarify the matter, we decided to reproduce some of the experiments. It turns out that to some extent, our results contradict those reported. Reasons of these different behaviors are given through the analysis of the stopping criterion. Keywords: SVM, CVM, large scale, KKT gap, stopping condition, stopping criteria</p><p>4 0.082313992 <a title="51-tfidf-4" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>Author: Charles Sutton, Andrew McCallum, Khashayar Rohanimanesh</p><p>Abstract: In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random ﬁelds (DCRFs), a generalization of linear-chain conditional random ﬁelds (CRFs) in which each time slice contains a set of state variables and edges—a distributed state representation as in dynamic Bayesian networks (DBNs)—and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data. In addition to maximum conditional likelihood, we present two alternative approaches for training DCRFs: marginal likelihood training, for when we are primarily interested in predicting only a subset of the variables, and cascaded training, for when we have a distinct data set for each state variable, as in transfer learning. We evaluate marginal training and cascaded training on both synthetic data and real-world text data, ﬁnding that marginal training can improve accuracy when uncertainty exists over the latent variables, and that for transfer learning, a DCRF trained in a cascaded fashion performs better than a linear-chain CRF that predicts the ﬁnal task directly. Keywords: conditional random ﬁelds, graphical models, sequence labeling</p><p>5 0.041137531 <a title="51-tfidf-5" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>Author: Sofus A. Macskassy, Foster Provost</p><p>Abstract: paper1 This is about classifying entities that are interlinked with entities for which the class is known. After surveying prior work, we present NetKit, a modular toolkit for classiﬁcation in networked data, and a case-study of its application to networked data used in prior machine learning research. NetKit is based on a node-centric framework in which classiﬁers comprise a local classiﬁer, a relational classiﬁer, and a collective inference procedure. Various existing node-centric relational learning algorithms can be instantiated with appropriate choices for these components, and new combinations of components realize new algorithms. The case study focuses on univariate network classiﬁcation, for which the only information used is the structure of class linkage in the network (i.e., only links and some class labels). To our knowledge, no work previously has evaluated systematically the power of class-linkage alone for classiﬁcation in machine learning benchmark data sets. The results demonstrate that very simple network-classiﬁcation models perform quite well—well enough that they should be used regularly as baseline classiﬁers for studies of learning with networked data. The simplest method (which performs remarkably well) highlights the close correspondence between several existing methods introduced for different purposes—that is, Gaussian-ﬁeld classiﬁers, Hopﬁeld networks, and relational-neighbor classiﬁers. The case study also shows that there are two sets of techniques that are preferable in different situations, namely when few versus many labels are known initially. We also demonstrate that link selection plays an important role similar to traditional feature selection. Keywords: relational learning, network learning, collective inference, collective classiﬁcation, networked data, probabilistic relational models, network analysis, network data</p><p>6 0.032362994 <a title="51-tfidf-6" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>7 0.029377092 <a title="51-tfidf-7" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>8 0.026471315 <a title="51-tfidf-8" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>9 0.024587367 <a title="51-tfidf-9" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>10 0.024401555 <a title="51-tfidf-10" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>11 0.023858517 <a title="51-tfidf-11" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>12 0.023820287 <a title="51-tfidf-12" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>13 0.021491403 <a title="51-tfidf-13" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>14 0.020940358 <a title="51-tfidf-14" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>15 0.019824931 <a title="51-tfidf-15" href="./jmlr-2007-Very_Fast_Online_Learning_of_Highly_Non_Linear_Problems.html">91 jmlr-2007-Very Fast Online Learning of Highly Non Linear Problems</a></p>
<p>16 0.018889267 <a title="51-tfidf-16" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>17 0.01837408 <a title="51-tfidf-17" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>18 0.017421555 <a title="51-tfidf-18" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>19 0.016910288 <a title="51-tfidf-19" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>20 0.016339011 <a title="51-tfidf-20" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.14), (1, 0.282), (2, 0.493), (3, -0.531), (4, 0.024), (5, -0.046), (6, 0.005), (7, 0.056), (8, -0.026), (9, 0.021), (10, -0.042), (11, 0.006), (12, -0.011), (13, -0.011), (14, -0.027), (15, 0.001), (16, -0.0), (17, 0.019), (18, 0.03), (19, -0.009), (20, 0.0), (21, 0.019), (22, -0.012), (23, -0.004), (24, -0.007), (25, 0.009), (26, 0.022), (27, -0.045), (28, 0.011), (29, 0.015), (30, 0.0), (31, 0.008), (32, -0.011), (33, 0.024), (34, -0.054), (35, 0.053), (36, 0.029), (37, 0.024), (38, 0.015), (39, 0.015), (40, -0.018), (41, 0.004), (42, -0.007), (43, -0.016), (44, 0.003), (45, 0.054), (46, 0.048), (47, -0.039), (48, 0.093), (49, -0.01)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98161173 <a title="51-lsi-1" href="./jmlr-2007-Truncating_the_Loop_Series_Expansion_for_Belief_Propagation.html">86 jmlr-2007-Truncating the Loop Series Expansion for Belief Propagation</a></p>
<p>Author: Vicenç Gómez, Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: Recently, Chertkov and Chernyak (2006b) derived an exact expression for the partition sum (normalization constant) corresponding to a graphical model, which is an expansion around the belief propagation (BP) solution. By adding correction terms to the BP free energy, one for each “generalized loop” in the factor graph, the exact partition sum is obtained. However, the usually enormous number of generalized loops generally prohibits summation over all correction terms. In this article we introduce truncated loop series BP (TLSBP), a particular way of truncating the loop series of Chertkov & Chernyak by considering generalized loops as compositions of simple loops. We analyze the performance of TLSBP in different scenarios, including the Ising model on square grids and regular random graphs, and on PROMEDAS, a large probabilistic medical diagnostic system. We show that TLSBP often improves upon the accuracy of the BP solution, at the expense of increased computation time. We also show that the performance of TLSBP strongly depends on the degree of interaction between the variables. For weak interactions, truncating the series leads to signiﬁcant improvements, whereas for strong interactions it can be ineffective, even if a high number of terms is considered. Keywords: belief propagation, loop calculus, approximate inference, partition function, Ising grid, random regular graphs, medical diagnosis</p><p>same-paper 2 0.96652162 <a title="51-lsi-2" href="./jmlr-2007-Loop_Corrections_for_Approximate_Inference_on_Factor_Graphs.html">51 jmlr-2007-Loop Corrections for Approximate Inference on Factor Graphs</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a method to improve approximate inference methods by correcting for the inﬂuence of loops in the graphical model. The method is a generalization and alternative implementation of a recent idea from Montanari and Rizzo (2005). It is applicable to arbitrary factor graphs, provided that the size of the Markov blankets is not too large. It consists of two steps: (i) an approximate inference method, for example, belief propagation, is used to approximate cavity distributions for each variable (i.e., probability distributions on the Markov blanket of a variable for a modiﬁed graphical model in which the factors involving that variable have been removed); (ii) all cavity distributions are improved by a message-passing algorithm that cancels out approximation errors by imposing certain consistency constraints. This loop correction (LC) method usually gives signiﬁcantly better results than the original, uncorrected, approximate inference algorithm that is used to estimate the effect of loops. Indeed, we often observe that the loop-corrected error is approximately the square of the error of the uncorrected approximate inference method. In this article, we compare different variants of the loop correction method with other approximate inference methods on a variety of graphical models, including “real world” networks, and conclude that the LC method generally obtains the most accurate results. Keywords: loop corrections, approximate inference, graphical models, factor graphs, belief propagation</p><p>3 0.36394945 <a title="51-lsi-3" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>Author: Gaëlle Loosli, Stéphane Canu</p><p>Abstract: In a recently published paper in JMLR, Tsang et al. (2005) present an algorithm for SVM called Core Vector Machines (CVM) and illustrate its performances through comparisons with other SVM solvers. After reading the CVM paper we were surprised by some of the reported results. In order to clarify the matter, we decided to reproduce some of the experiments. It turns out that to some extent, our results contradict those reported. Reasons of these different behaviors are given through the analysis of the stopping criterion. Keywords: SVM, CVM, large scale, KKT gap, stopping condition, stopping criteria</p><p>4 0.35756829 <a title="51-lsi-4" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>Author: Charles Sutton, Andrew McCallum, Khashayar Rohanimanesh</p><p>Abstract: In sequence modeling, we often wish to represent complex interaction between labels, such as when performing multiple, cascaded labeling tasks on the same sequence, or when long-range dependencies exist. We present dynamic conditional random ﬁelds (DCRFs), a generalization of linear-chain conditional random ﬁelds (CRFs) in which each time slice contains a set of state variables and edges—a distributed state representation as in dynamic Bayesian networks (DBNs)—and parameters are tied across slices. Since exact inference can be intractable in such models, we perform approximate inference using several schedules for belief propagation, including tree-based reparameterization (TRP). On a natural-language chunking task, we show that a DCRF performs better than a series of linear-chain CRFs, achieving comparable performance using only half the training data. In addition to maximum conditional likelihood, we present two alternative approaches for training DCRFs: marginal likelihood training, for when we are primarily interested in predicting only a subset of the variables, and cascaded training, for when we have a distinct data set for each state variable, as in transfer learning. We evaluate marginal training and cascaded training on both synthetic data and real-world text data, ﬁnding that marginal training can improve accuracy when uncertainty exists over the latent variables, and that for transfer learning, a DCRF trained in a cascaded fashion performs better than a linear-chain CRF that predicts the ﬁnal task directly. Keywords: conditional random ﬁelds, graphical models, sequence labeling</p><p>5 0.14098558 <a title="51-lsi-5" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>6 0.12918723 <a title="51-lsi-6" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>7 0.11715817 <a title="51-lsi-7" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>8 0.10588423 <a title="51-lsi-8" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>9 0.1026646 <a title="51-lsi-9" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>10 0.09969347 <a title="51-lsi-10" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>11 0.098687597 <a title="51-lsi-11" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>12 0.097418606 <a title="51-lsi-12" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>13 0.0971798 <a title="51-lsi-13" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>14 0.088070817 <a title="51-lsi-14" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>15 0.08639805 <a title="51-lsi-15" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>16 0.08043921 <a title="51-lsi-16" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>17 0.079730503 <a title="51-lsi-17" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>18 0.079597451 <a title="51-lsi-18" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>19 0.072534911 <a title="51-lsi-19" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>20 0.072108522 <a title="51-lsi-20" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.012), (8, 0.015), (10, 0.02), (12, 0.046), (15, 0.015), (22, 0.01), (28, 0.04), (35, 0.412), (40, 0.037), (45, 0.02), (48, 0.035), (60, 0.022), (80, 0.014), (85, 0.067), (89, 0.061), (98, 0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7114166 <a title="51-lda-1" href="./jmlr-2007-Loop_Corrections_for_Approximate_Inference_on_Factor_Graphs.html">51 jmlr-2007-Loop Corrections for Approximate Inference on Factor Graphs</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a method to improve approximate inference methods by correcting for the inﬂuence of loops in the graphical model. The method is a generalization and alternative implementation of a recent idea from Montanari and Rizzo (2005). It is applicable to arbitrary factor graphs, provided that the size of the Markov blankets is not too large. It consists of two steps: (i) an approximate inference method, for example, belief propagation, is used to approximate cavity distributions for each variable (i.e., probability distributions on the Markov blanket of a variable for a modiﬁed graphical model in which the factors involving that variable have been removed); (ii) all cavity distributions are improved by a message-passing algorithm that cancels out approximation errors by imposing certain consistency constraints. This loop correction (LC) method usually gives signiﬁcantly better results than the original, uncorrected, approximate inference algorithm that is used to estimate the effect of loops. Indeed, we often observe that the loop-corrected error is approximately the square of the error of the uncorrected approximate inference method. In this article, we compare different variants of the loop correction method with other approximate inference methods on a variety of graphical models, including “real world” networks, and conclude that the LC method generally obtains the most accurate results. Keywords: loop corrections, approximate inference, graphical models, factor graphs, belief propagation</p><p>2 0.51293373 <a title="51-lda-2" href="./jmlr-2007-Truncating_the_Loop_Series_Expansion_for_Belief_Propagation.html">86 jmlr-2007-Truncating the Loop Series Expansion for Belief Propagation</a></p>
<p>Author: Vicenç Gómez, Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: Recently, Chertkov and Chernyak (2006b) derived an exact expression for the partition sum (normalization constant) corresponding to a graphical model, which is an expansion around the belief propagation (BP) solution. By adding correction terms to the BP free energy, one for each “generalized loop” in the factor graph, the exact partition sum is obtained. However, the usually enormous number of generalized loops generally prohibits summation over all correction terms. In this article we introduce truncated loop series BP (TLSBP), a particular way of truncating the loop series of Chertkov & Chernyak by considering generalized loops as compositions of simple loops. We analyze the performance of TLSBP in different scenarios, including the Ising model on square grids and regular random graphs, and on PROMEDAS, a large probabilistic medical diagnostic system. We show that TLSBP often improves upon the accuracy of the BP solution, at the expense of increased computation time. We also show that the performance of TLSBP strongly depends on the degree of interaction between the variables. For weak interactions, truncating the series leads to signiﬁcant improvements, whereas for strong interactions it can be ineffective, even if a high number of terms is considered. Keywords: belief propagation, loop calculus, approximate inference, partition function, Ising grid, random regular graphs, medical diagnosis</p><p>3 0.33585101 <a title="51-lda-3" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>Author: Jennifer Neville, David Jensen</p><p>Abstract: Recent work on graphical models for relational data has demonstrated signiﬁcant improvements in classiﬁcation and inference when models represent the dependencies among instances. Despite its use in conventional statistical models, the assumption of instance independence is contradicted by most relational data sets. For example, in citation data there are dependencies among the topics of a paper’s references, and in genomic data there are dependencies among the functions of interacting proteins. In this paper, we present relational dependency networks (RDNs), graphical models that are capable of expressing and reasoning with such dependencies in a relational setting. We discuss RDNs in the context of relational Bayes networks and relational Markov networks and outline the relative strengths of RDNs—namely, the ability to represent cyclic dependencies, simple methods for parameter estimation, and efﬁcient structure learning techniques. The strengths of RDNs are due to the use of pseudolikelihood learning techniques, which estimate an efﬁcient approximation of the full joint distribution. We present learned RDNs for a number of real-world data sets and evaluate the models in a prediction context, showing that RDNs identify and exploit cyclic relational dependencies to achieve signiﬁcant performance gains over conventional conditional models. In addition, we use synthetic data to explore model performance under various relational data characteristics, showing that RDN learning and inference techniques are accurate over a wide range of conditions. Keywords: relational learning, probabilistic relational models, knowledge discovery, graphical models, dependency networks, pseudolikelihood estimation</p><p>4 0.29499066 <a title="51-lda-4" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>5 0.29448301 <a title="51-lda-5" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>Author: Guy Lebanon, Yi Mao, Joshua Dillon</p><p>Abstract: The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efﬁcient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks. Keywords: text processing, local smoothing</p><p>6 0.2912527 <a title="51-lda-6" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>7 0.28846145 <a title="51-lda-7" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>8 0.28773755 <a title="51-lda-8" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>9 0.28606248 <a title="51-lda-9" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>10 0.28581893 <a title="51-lda-10" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>11 0.28569013 <a title="51-lda-11" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>12 0.28419605 <a title="51-lda-12" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>13 0.28333825 <a title="51-lda-13" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>14 0.28241515 <a title="51-lda-14" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>15 0.28194594 <a title="51-lda-15" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>16 0.28175652 <a title="51-lda-16" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>17 0.28138232 <a title="51-lda-17" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>18 0.28075439 <a title="51-lda-18" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>19 0.28037053 <a title="51-lda-19" href="./jmlr-2007-Stagewise_Lasso.html">77 jmlr-2007-Stagewise Lasso</a></p>
<p>20 0.28020823 <a title="51-lda-20" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
