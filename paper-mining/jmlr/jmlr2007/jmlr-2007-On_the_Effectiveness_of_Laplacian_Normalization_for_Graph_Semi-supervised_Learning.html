<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-62" href="#">jmlr2007-62</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</h1>
<br/><p>Source: <a title="jmlr-2007-62-pdf" href="http://jmlr.org/papers/volume8/johnson07a/johnson07a.pdf">pdf</a></p><p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>Reference: <a title="jmlr-2007-62-reference" href="../jmlr2007_reference/jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. [sent-11, score-0.473]
</p><p>2 Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. [sent-13, score-0.514]
</p><p>3 The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. [sent-14, score-0.428]
</p><p>4 Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian  1. [sent-17, score-0.471]
</p><p>5 In spectral clustering, a traditional starting point is to ﬁnd a partition of a graph that minimizes a certain deﬁnition of “graph cut” that quantiﬁes the quality of the partition. [sent-20, score-0.253]
</p><p>6 The cut is the objective one attempts to minimize. [sent-21, score-0.305]
</p><p>7 J OHNSON AND Z HANG  universally acceptable criterion, it is difﬁcult to argue that one cut deﬁnition is better than another just based on heuristics. [sent-30, score-0.305]
</p><p>8 If a universally agreeable standard does exist, then one should focus on that criterion instead of an artiﬁcially deﬁned cut problem. [sent-31, score-0.305]
</p><p>9 For example, in the context of spectral clustering, there are two well-known types of graph cut, the ratio cut (Hagen and Kahng, 1992) and the normalized cut (Shi and Malik, 2000). [sent-32, score-0.906]
</p><p>10 Approximate optimization of the ratio cut leads to eigenvector computation of the unnormalized graph Laplacian matrix (which we will deﬁne later), and that of the normalized cut involves the normalized graph Laplacian matrix (normalized using node degrees). [sent-33, score-1.341]
</p><p>11 Although a number of empirical studies indicate that the normalized cut often leads to better clustering results, there isn’t any direct theoretical proof except for some implicit evidence. [sent-34, score-0.397]
</p><p>12 As another example, the deﬁnition of graph Laplacian in the spectral graph theory in Chung (1998) is normalized, but that is for graph theoretical reasons instead of statistical reasons. [sent-35, score-0.541]
</p><p>13 Similarly, some analysis of spectral clustering employs normalized cut (Meil˘ et al. [sent-39, score-0.506]
</p><p>14 a These can be regarded as implicit evidence for preferring the normalized Laplacian over the unnormalized Laplacian. [sent-41, score-0.277]
</p><p>15 In spectral clustering or graph based semi-supervised learning, one starts with similarity graphs that link similar data points. [sent-51, score-0.366]
</p><p>16 If the graph is fully connected within each class and disconnected between the classes, then appropriate cut minimization leads to perfect classiﬁcation. [sent-53, score-0.528]
</p><p>17 (2002) that one may ﬁrst project these data points into the eigenspace corresponding to the largest eigenvalues of a normalized adjacency matrix of the graph and then use the standard k-means method to perform clustering. [sent-55, score-0.33]
</p><p>18 The relationship of kernel design and graph learning was investigated in Zhang and Ando (2006), where it was argued that quadratic regularization-based graph learning can be regarded as kernel design in the spectral domain. [sent-80, score-0.493]
</p><p>19 First we present a model for transductive learning on graphs and develop a margin analysis for multi-class graph learning. [sent-85, score-0.26]
</p><p>20 We then analyze graph learning using graph properties such as graph-cut and a concept we call pure subgraph. [sent-86, score-0.42]
</p><p>21 We will consider the kernel matrix induced by the graph Laplacian, which we shall deﬁne later in the paper. [sent-140, score-0.256]
</p><p>22 Then ∀p > 0, the expected generalization error of the learning method (1) over the training samples Z n , uniformly drawn without replacement from graph nodes {1, . [sent-168, score-0.27]
</p><p>23 Margin and Graph Cut Consider an undirected graph G = (V, E) deﬁned on the nodes V = {v j : j = 1, . [sent-182, score-0.246]
</p><p>24 Let deg j (G) = ∑m=1 w j, j be the degree of / / j node j of graph G. [sent-193, score-0.291]
</p><p>25 Deﬁnition 2 Consider a graph G = (V, E) of m nodes with weights w j, j ( j, j = 1, . [sent-195, score-0.27]
</p><p>26 The unnormalized Laplacian matrix L (G) ∈ Rm×m is deﬁned as: L j, j (G) = −w j, j if j = j ; deg j (G) otherwise. [sent-199, score-0.372]
</p><p>27 1 Generalization Analysis Using Graph-Cut We will adapt Theorem 1 in Section 2 to analyze graph learning using graph properties such as graph-cut. [sent-214, score-0.288]
</p><p>28 We now introduce a learning theoretical deﬁnition of S-normalized graph cut as follows. [sent-215, score-0.449]
</p><p>29 ,m on V , we deﬁne the cut for the S-normalized Laplacian LS in Deﬁnition 2 as: cut(LS , y) =  ∑  j, j :y j =y j  w j, j 2  1 1 + Sj Sj  +  ∑  j, j :y j =y j  w j, j 2  1 − Sj  1 Sj  2  . [sent-219, score-0.305]
</p><p>30 For unnormalized Laplacian, the second term on the right hand side of Deﬁnition 3 vanishes, which means that it only penalizes weights corresponding to edges connecting nodes with different labels. [sent-222, score-0.392]
</p><p>31 It is worth noting that in our framework, cut is used to indicate the absolute amount of perturbation from the idealized case with zero-cut. [sent-224, score-0.305]
</p><p>32 In spectral clustering, the absolute cut is often scaled and the resulting quantity is used as a quality measure for the clusters. [sent-225, score-0.443]
</p><p>33 In particular, the unnormalized Laplacian is used in spectral clustering to approximately minimize the ratio cut = ∑ j∈A, j ∈B w j, j /(|A| · |B|) (Hagen and Kahng, 1992) instead of ∑ j∈A, j ∈B w j, j . [sent-227, score-0.727]
</p><p>34 The scaling in the ratio cut (when K = 2) corresponds to the normalization of a speciﬁc encoding of the target vectors (f ·,k ∈ Rm which encodes the true output values in our setting); it is used in the computation of the second smallest eigenvalue of the unnormalized Laplacian. [sent-228, score-0.786]
</p><p>35 As such, the discrepancy of cut deﬁnition is merely due to a difference in representation of the target vectors, and both lead to the same unnormalized Laplacian matrix algorithmically. [sent-230, score-0.577]
</p><p>36 This means that the normalization method we propose in the paper might be useful in spectral clustering as well. [sent-232, score-0.289]
</p><p>37 Our goal is to better understand the quantity (αs + cut(LS , y)) p/(p+1) tr p (K) p/(p+1) using graph properties, which gives better understanding of graph based learning. [sent-260, score-0.365]
</p><p>38 2 Zero-cut and Geometric Margin Separation We consider an application of Theorem 4 for the unnormalized Laplacian under the zero-cut assumption that each connected component of the graph has a single label. [sent-264, score-0.496]
</p><p>39 ac n ac nm1  Proof Since the graph has q connected components, L has q eigenvectors v ( = 1, . [sent-270, score-0.301]
</p><p>40 This means that graph semi-supervised learning can take advantage of the new quantity q to characterize its generalization performance, and this quantity cannot be used by standard supervised learning. [sent-294, score-0.284]
</p><p>41 In particular, if the data are generated in a way such that the number of connected components q is small, and each connected component belongs to a single class, then graph based semi-supervised learning can work better than supervised kernel learning. [sent-296, score-0.447]
</p><p>42 3 Non-Zero Cut and Pure Components It is often too restrictive to assume that each connected component has only one label (that is, the cut is zero). [sent-306, score-0.423]
</p><p>43 A pure subgraph 1497  J OHNSON AND Z HANG  q  q  G = ∪ =1 G of G divides V into q disjoint sets V = ∪ =1V such that each subgraph G = (V , E ) is a pure component. [sent-310, score-0.364]
</p><p>44 For instance, if we remove all edges of G that connect nodes with different labels, then the resulting subgraph is a pure subgraph (though it may not be the only one). [sent-312, score-0.403]
</p><p>45 We use it together with graph cut to derive a generalization bound. [sent-316, score-0.473]
</p><p>46 It quantitatively illustrates the importance of analyzing graph learning using a partition of the original graph into well-connected pure components. [sent-322, score-0.42]
</p><p>47 We thus have the condition λ2 (G )/m ≥ u(G ) for some constant u(G ) that does not depend on the size of the pure components (but only how q well-connected each pure component is). [sent-329, score-0.303]
</p><p>48 This observation motivates a scaling matrix S that compensates for the unbalanced pure component sizes, which we will investigate next. [sent-336, score-0.263]
</p><p>49 4 Optimal Normalization for Near-zero-cut Partition As discussed in the introduction, the common practice of the normalization of the adjacency matrix (W) or the graph Laplacian (D − W) is based on degrees, which corresponds to setting S = D. [sent-338, score-0.352]
</p><p>50 If scaling factors S j are approximately constant within each pure component, then using the Laplacian in Deﬁnition 2, we have a small regularization penalty for the edges within a pure component and between the nodes that have close output values (i. [sent-343, score-0.574]
</p><p>51 Therefore, in the following we focus on ﬁnding the optimal scaling matrix S such that S j is constant within each pure component V , and assume that S is quantiﬁed by q numbers [s ] =1,. [sent-346, score-0.263]
</p><p>52 That is, if cut(G , y) is small, then we can choose scaling factor s ∝ m for each pure component ¯ so that the generalization performance is approximately (ac) −1 b · q/n, which is of the order O(1/n). [sent-355, score-0.279]
</p><p>53 Note that this requirement is not enforced by the standard degree-based normalization method S j = deg j (G) because a well-connected pure component may contain nodes with quite different degrees. [sent-361, score-0.504]
</p><p>54 In this model, a pure component is completely connected, and each node connects to all other nodes and itself with edge weight w j, j = 1. [sent-365, score-0.385]
</p><p>55 For such a model, the degree-based normalization can fail because the deg j (G ) within each pure component G is not approximately constant, and it may not be proportional to m . [sent-369, score-0.432]
</p><p>56 Our analysis suggests that it is necessary to modify the degree-based scaling method S j = deg j (G) so that the scaling factor is approximately a constant within each pure component, which should be proportional to m . [sent-371, score-0.37]
</p><p>57 Let K = (αI + L )−1 be the kernel matrix corresponding to the unnormalized Laplacian. [sent-374, score-0.32]
</p><p>58 To our best knowledge, no one has proposed this normalization 1500  O N THE E FFECTIVENESS OF L APLACIAN N ORMALIZATION FOR G RAPH S EMI - SUPERVISED L EARNING  method in the graph learning setting before. [sent-381, score-0.275]
</p><p>59 5 Dimension Reduction Normalization and dimension reduction have been commonly used in spectral clustering such as Ng et al. [sent-389, score-0.337]
</p><p>60 For semi-supervised learning, dimension reduction (without normalization) is known to improve performance (Belkin and Niyogi, 2004, Zhang and Ando, 2006) while the degree-based normalization (without dimension reduction) has also been explored (Zhou et al. [sent-391, score-0.395]
</p><p>61 In this section, we present a brief high-level argument that an appropriate combination of normalization and dimension reduction (as commonly used in spectral clustering) can improve classiﬁcation performance. [sent-393, score-0.419]
</p><p>62 (6)  The beneﬁt of dimension reduction in graph learning has been investigated in Zhang and Ando (2006), under the spectral kernel design framework. [sent-398, score-0.48]
</p><p>63 Because of this, dimension reduction, which makes kernel eigenvalues better match the decay of target spectral coefﬁcients, will become helpful. [sent-403, score-0.273]
</p><p>64 For Laplacian regularization investigated here, we may regard noise as edges connecting pure components of different classes, which increase the cut in Deﬁnition 3. [sent-404, score-0.498]
</p><p>65 Although “the scaling factor S j = m ” might be reminiscent of the ratio cut in spectral clustering, note that, as mentioned earlier, the ratio cut corresponds to the unnormalized Laplacian. [sent-407, score-1.007]
</p><p>66 K-scaling suggested here normalizes the Laplacian matrix, which is in the same spirit of normalized cut (degree-scaling) but ﬁxes some of its short-comings based on learning theoretical insights developed here. [sent-408, score-0.348]
</p><p>67 (c, e) in the table indicates that for each node, we randomly chose c nodes of the same class and connect it to them, and we randomly chose e nodes of other classes (introducing errors) and connect it to them. [sent-410, score-0.278]
</p><p>68 Therefore, under certain conditions, dimension reduction can reduce noise (corresponding to a small cut), which essentially makes normalization more effective as shown in Section 3. [sent-417, score-0.31]
</p><p>69 We show our formal analysis of the combination of dimension reduction (as in (6) above) and normalization of Laplacian, for completeness, in Appendix C and empirical results in the next section. [sent-419, score-0.31]
</p><p>70 Experiments We experiment with the Laplacian regularization with the normalization methods discussed above, on synthesized data sets generated by controlling graph properties as well as three real-world data sets. [sent-421, score-0.304]
</p><p>71 1 Experimental Framework The Laplacian matrix L is generated from a graph G so that L j, j = −w j, j for j = j and L j, j = deg j (G). [sent-423, score-0.282]
</p><p>72 2 Controlled Data Experiments The purpose of the controlled data experiments is to observe the correlation of the effectiveness of the normalization methods with graph properties. [sent-449, score-0.275]
</p><p>73 We observe that on these graphs, both K-scaling and L-scaling signiﬁcantly improve classiﬁcation accuracy over the unnormalized baseline. [sent-454, score-0.264]
</p><p>74 Satellite nodes are relatively weakly connected to core nodes of the same class. [sent-460, score-0.329]
</p><p>75 The satellite nodes are also connected to some other classes’ satellite nodes (i. [sent-461, score-0.413]
</p><p>76 For every core node, we randomly choose 10 other core nodes of the same class and connect it to them with edge weight 1 (that is, each core node is connected to at least 10 core nodes of the same class). [sent-468, score-0.616]
</p><p>77 For every satellite node, we randomly choose one core node of the same class and connect them with edge weight 0. [sent-469, score-0.26]
</p><p>78 For graphs generated in this manner, degrees vary within the same class since the satellite nodes have smaller degrees than the core nodes. [sent-479, score-0.327]
</p><p>79 On the other hand, the theory suggests that when the graph has large error (large cut), the beneﬁt of normalization is less clear (since the derivation of K-scaling assumes near-zero cut). [sent-486, score-0.275]
</p><p>80 Note that the performance without dimension reduction (Figure 5) is signiﬁcantly worse than the performance with dimension reduction (Figure 2). [sent-490, score-0.358]
</p><p>81 Given the original graph as in Figure 6 (a), Figure 6 (b)–(d) show the graphs corresponding to the scaled adjacency matrices S−1/2 WS−1/2 where S is derived from L-scaling, K-scaling with α = 0. [sent-493, score-0.247]
</p><p>82 We observe that, compared with the unnormalized case, K-scaling and L-scaling essentially balance the edge weights between the two classes (i. [sent-496, score-0.323]
</p><p>83 For the two-class core-satellite graph in (a), L-scaling makes the weights of error edges larger than the edge weights between the core nodes and satellite nodes of the same class as in (b). [sent-603, score-0.604]
</p><p>84 (a) MNIST, dim and alpha determined by cross validation  (b) MNIST, w/ optimum dim and optimum alpha  85  85  80  80  75  75 accuracy (%)  accuracy (%)  K-scaling (w/ dim reduction)  70  L-scaling (w/ dim reduction) Unnormalized (w/ dim redu. [sent-617, score-1.593]
</p><p>85 The performance of the unnormalized Laplacian (with dimension reduction) is roughly consistent with the performance with similar (m, n) with heuristic dimension selection in Belkin and Niyogi (2004). [sent-629, score-0.404]
</p><p>86 Although without dimension reduction, L-scaling 1507  J OHNSON AND Z HANG  70  (a) RCV1, RBF, dim and alpha determined by cross validation  75  65  K-scaling (w/ dim reduction)  70  60  (b) RCV1, RBF, w/ optimum dim and optimum alpha  65  L-scaling (w/ dim reduction) Unnormalized (w/ dim redu. [sent-630, score-1.618]
</p><p>87 Performance differences of the best performing method ‘K-scaling (w/ dim reduction)’ from ‘L-scaling (w/ dim reduction)’ and ‘Unnormalized (w/ dim redu. [sent-637, score-0.684]
</p><p>88 80  (a) RCV1, linear, dim and alpha determined by cross validation  85  K-scaling (w/ dim reduction)  80  70  L-scaling (w/ dim reduction)  75  75  Unnormalized (w/ dim redu. [sent-640, score-1.094]
</p><p>89 Performance differences of the best performing method ‘K-scaling (w/ dim reduction)’ from the second and third best ‘L-scaling (w/ dim reduction)’ and ‘Unnormalized (w/ dim redu. [sent-646, score-0.684]
</p><p>90 and K-scaling still improve performance over the unnormalized Laplacian, the best performance is always obtained by K-scaling with dimension reduction (the bold line with circles). [sent-656, score-0.413]
</p><p>91 In Figure 8 (a), the unnormalized Laplacian with dimension reduction underperforms the unnormalized Laplacian without dimension reduction, indicating that dimension reduction rather degrades performance in this case. [sent-657, score-0.911]
</p><p>92 As observed, if the optimum dimensionality is known (as in (b)), dimension reduction improves performance either with or without normalization by K-scaling and L-scaling, and that all the transductive conﬁgurations outperform the supervised baseline. [sent-660, score-0.546]
</p><p>93 We also note that the comparison of Figure 8 (a) and (b) shows that choosing good dimensionality by cross validation is much harder than choosing α by cross validation especially when the number of labeled examples is small. [sent-661, score-0.306]
</p><p>94 In the setting of Figure 9 (a) where the dimensionality and α were determined by cross validation, K-scaling with dimension reduction generally performs the best. [sent-666, score-0.3]
</p><p>95 Its performance differences from the second and third best ‘L-scaling (w/ dim reduction)’ 1509  J OHNSON AND Z HANG  and ‘Unnormalized (w/ dim redu. [sent-671, score-0.456]
</p><p>96 Nevertheless, when the dimensionality and α are set to the optimum (Figure 11 (b)), again, K-scaling with dimension reduction performs the best. [sent-678, score-0.305]
</p><p>97 Its differences from the second and the third best methods (K-scaling without dimension reduction and L-scaling without dimension reduction) are statistically signiﬁcant (p ≤ 0. [sent-679, score-0.264]
</p><p>98 In particular, we used this analysis to obtain a better understanding of the role of normalization of the graph Laplacian matrix. [sent-698, score-0.275]
</p><p>99 A min-max cut algorithm for graph partitioning and data clustering. [sent-822, score-0.449]
</p><p>100 New spectral methods for ratio cut partitioning and clustering. [sent-832, score-0.414]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zn', 0.391), ('ls', 0.319), ('cut', 0.305), ('laplacian', 0.238), ('unnormalized', 0.234), ('dim', 0.228), ('aplacian', 0.151), ('ffectiveness', 0.151), ('ohnson', 0.151), ('ormalization', 0.151), ('graph', 0.144), ('err', 0.14), ('pure', 0.132), ('normalization', 0.131), ('yin', 0.128), ('raph', 0.114), ('hang', 0.114), ('emi', 0.114), ('spectral', 0.109), ('nodes', 0.102), ('deg', 0.1), ('hin', 0.097), ('reduction', 0.094), ('ft', 0.093), ('alpha', 0.091), ('dimension', 0.085), ('connected', 0.079), ('umist', 0.076), ('sj', 0.073), ('qk', 0.069), ('dimensionality', 0.066), ('pr', 0.066), ('satellite', 0.065), ('edge', 0.065), ('graphs', 0.064), ('optimum', 0.06), ('labeled', 0.058), ('supervised', 0.058), ('rmk', 0.055), ('cross', 0.055), ('scaling', 0.054), ('transductive', 0.052), ('malik', 0.052), ('mnist', 0.052), ('ando', 0.052), ('shi', 0.052), ('subgraph', 0.05), ('clustering', 0.049), ('kernel', 0.048), ('tr', 0.048), ('earning', 0.048), ('node', 0.047), ('core', 0.046), ('rie', 0.046), ('normalized', 0.043), ('hagen', 0.043), ('vt', 0.042), ('eigenvalue', 0.04), ('adjacency', 0.039), ('rbf', 0.039), ('component', 0.039), ('theorem', 0.038), ('matrix', 0.038), ('connect', 0.037), ('validation', 0.036), ('ut', 0.035), ('gin', 0.035), ('eigenspace', 0.035), ('eigenvectors', 0.034), ('kahng', 0.032), ('prs', 0.032), ('edges', 0.032), ('eigenvalues', 0.031), ('remedy', 0.031), ('tong', 0.031), ('accuracy', 0.03), ('chung', 0.03), ('approximately', 0.03), ('ng', 0.029), ('quantity', 0.029), ('regularization', 0.029), ('rk', 0.028), ('kin', 0.027), ('intuitions', 0.027), ('zhang', 0.027), ('rm', 0.027), ('belkin', 0.026), ('shall', 0.026), ('trend', 0.025), ('zhou', 0.025), ('degrees', 0.025), ('squares', 0.025), ('weights', 0.024), ('generalization', 0.024), ('factors', 0.024), ('baseline', 0.023), ('diagonal', 0.023), ('entries', 0.023), ('ac', 0.022), ('smallest', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="62-tfidf-1" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>2 0.17467725 <a title="62-tfidf-2" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Matthias Hein, Jean-Yves Audibert, Ulrike von Luxburg</p><p>Abstract: Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. Keywords: graphs, graph Laplacians, semi-supervised learning, spectral clustering, dimensionality reduction</p><p>3 0.131504 <a title="62-tfidf-3" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A speciﬁc instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a ﬁnal parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A speciﬁc instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystr¨ m extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker o sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are brieﬂy summarized at the end. Keywords: Markov decision processes, reinforcement learning, value function approximation, manifold learning, spectral graph theory</p><p>4 0.070843987 <a title="62-tfidf-4" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>Author: Ryan M. Rifkin, Ross A. Lippert</p><p>Abstract: Regularization is an approach to function learning that balances ﬁt and smoothness. In practice, we search for a function f with a ﬁnite representation f = ∑i ci φi (·). In most treatments, the ci are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning. Keywords: kernel machines, duality, optimization, convex analysis, kernel learning</p><p>5 0.059936892 <a title="62-tfidf-5" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>Author: Jean-Yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, ﬁrst, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester (1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand (see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds. Keywords: statistical learning theory, PAC-Bayes theorems, generalization error bounds</p><p>6 0.057520755 <a title="62-tfidf-6" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>7 0.053199615 <a title="62-tfidf-7" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>8 0.052277077 <a title="62-tfidf-8" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>9 0.044242263 <a title="62-tfidf-9" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>10 0.043075882 <a title="62-tfidf-10" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>11 0.0421785 <a title="62-tfidf-11" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>12 0.040250544 <a title="62-tfidf-12" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>13 0.038096741 <a title="62-tfidf-13" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>14 0.038088847 <a title="62-tfidf-14" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>15 0.037361521 <a title="62-tfidf-15" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>16 0.037204348 <a title="62-tfidf-16" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>17 0.036957815 <a title="62-tfidf-17" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>18 0.036409661 <a title="62-tfidf-18" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>19 0.036281887 <a title="62-tfidf-19" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>20 0.035871625 <a title="62-tfidf-20" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.248), (1, -0.0), (2, 0.08), (3, 0.052), (4, -0.207), (5, -0.003), (6, 0.131), (7, 0.094), (8, 0.207), (9, 0.135), (10, 0.268), (11, 0.069), (12, -0.08), (13, -0.187), (14, 0.17), (15, -0.067), (16, -0.148), (17, 0.009), (18, 0.038), (19, 0.013), (20, -0.074), (21, 0.011), (22, -0.146), (23, 0.058), (24, 0.099), (25, -0.173), (26, 0.077), (27, -0.015), (28, 0.094), (29, -0.012), (30, 0.007), (31, 0.127), (32, -0.029), (33, 0.063), (34, 0.05), (35, -0.082), (36, -0.071), (37, 0.01), (38, -0.012), (39, 0.015), (40, 0.044), (41, 0.033), (42, -0.015), (43, -0.004), (44, -0.017), (45, 0.032), (46, -0.036), (47, -0.038), (48, -0.032), (49, 0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96656895 <a title="62-lsi-1" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>2 0.83530754 <a title="62-lsi-2" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Matthias Hein, Jean-Yves Audibert, Ulrike von Luxburg</p><p>Abstract: Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. Keywords: graphs, graph Laplacians, semi-supervised learning, spectral clustering, dimensionality reduction</p><p>3 0.63713712 <a title="62-lsi-3" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A speciﬁc instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a ﬁnal parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A speciﬁc instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystr¨ m extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker o sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are brieﬂy summarized at the end. Keywords: Markov decision processes, reinforcement learning, value function approximation, manifold learning, spectral graph theory</p><p>4 0.27984735 <a title="62-lsi-4" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>Author: Markus Kalisch, Peter Bühlmann</p><p>Abstract: We consider the PC-algorithm (Spirtes et al., 2000) for estimating the skeleton and equivalence class of a very high-dimensional directed acyclic graph (DAG) with corresponding Gaussian distribution. The PC-algorithm is computationally feasible and often very fast for sparse problems with many nodes (variables), and it has the attractive property to automatically achieve high computational efﬁciency as a function of sparseness of the true underlying DAG. We prove uniform consistency of the algorithm for very high-dimensional, sparse DAGs where the number of nodes is allowed to quickly grow with sample size n, as fast as O(na ) for any 0 < a < ∞. The sparseness assumption is rather minimal requiring only that the neighborhoods in the DAG are of lower order than sample size n. We also demonstrate the PC-algorithm for simulated data. Keywords: asymptotic consistency, DAG, graphical model, PC-algorithm, skeleton</p><p>5 0.2482073 <a title="62-lsi-5" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: One of the nice properties of kernel classiﬁers such as SVMs is that they often produce sparse solutions. However, the decision functions of these classiﬁers cannot always be used to estimate the conditional probability of the class label. We investigate the relationship between these two properties and show that these are intimately related: sparseness does not occur when the conditional probabilities can be unambiguously estimated. We consider a family of convex loss functions and derive sharp asymptotic results for the fraction of data that becomes support vectors. This enables us to characterize the exact trade-off between sparseness and the ability to estimate conditional probabilities for these loss functions. Keywords: kernel methods, support vector machines, sparseness, estimating conditional probabilities</p><p>6 0.24132764 <a title="62-lsi-6" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>7 0.23196878 <a title="62-lsi-7" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>8 0.20680737 <a title="62-lsi-8" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>9 0.20331928 <a title="62-lsi-9" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>10 0.19603892 <a title="62-lsi-10" href="./jmlr-2007-Covariate_Shift_Adaptation_by_Importance_Weighted_Cross_Validation.html">25 jmlr-2007-Covariate Shift Adaptation by Importance Weighted Cross Validation</a></p>
<p>11 0.19240838 <a title="62-lsi-11" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>12 0.1903666 <a title="62-lsi-12" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>13 0.18994211 <a title="62-lsi-13" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>14 0.18664005 <a title="62-lsi-14" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>15 0.18333308 <a title="62-lsi-15" href="./jmlr-2007-Structure_and_Majority_Classes_in_Decision_Tree_Learning.html">79 jmlr-2007-Structure and Majority Classes in Decision Tree Learning</a></p>
<p>16 0.17965955 <a title="62-lsi-16" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>17 0.17901208 <a title="62-lsi-17" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>18 0.17745137 <a title="62-lsi-18" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>19 0.1768024 <a title="62-lsi-19" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>20 0.1701348 <a title="62-lsi-20" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.027), (8, 0.033), (10, 0.023), (12, 0.102), (15, 0.024), (28, 0.05), (40, 0.05), (45, 0.016), (48, 0.065), (60, 0.035), (80, 0.02), (85, 0.055), (88, 0.318), (98, 0.103)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72364146 <a title="62-lda-1" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>2 0.4801414 <a title="62-lda-2" href="./jmlr-2007-Dynamics_and_Generalization_Ability_of_LVQ_Algorithms.html">30 jmlr-2007-Dynamics and Generalization Ability of LVQ Algorithms</a></p>
<p>Author: Michael Biehl, Anarta Ghosh, Barbara Hammer</p><p>Abstract: Learning vector quantization (LVQ) schemes constitute intuitive, powerful classiﬁcation heuristics with numerous successful applications but, so far, limited theoretical background. We study LVQ rigorously within a simplifying model situation: two competing prototypes are trained from a sequence of examples drawn from a mixture of Gaussians. Concepts from statistical physics and the theory of on-line learning allow for an exact description of the training dynamics in highdimensional feature space. The analysis yields typical learning curves, convergence properties, and achievable generalization abilities. This is also possible for heuristic training schemes which do not relate to a cost function. We compare the performance of several algorithms, including Kohonen’s LVQ1 and LVQ+/-, a limiting case of LVQ2.1. The former shows close to optimal performance, while LVQ+/- displays divergent behavior. We investigate how early stopping can overcome this difﬁculty. Furthermore, we study a crisp version of robust soft LVQ, which was recently derived from a statistical formulation. Surprisingly, it exhibits relatively poor generalization. Performance improves if a window for the selection of data is introduced; the resulting algorithm corresponds to cost function based LVQ2. The dependence of these results on the model parameters, for example, prior class probabilities, is investigated systematically, simulations conﬁrm our analytical ﬁndings. Keywords: prototype based classiﬁcation, learning vector quantization, Winner-Takes-All algorithms, on-line learning, competitive learning</p><p>3 0.45861465 <a title="62-lda-3" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Matthias Hein, Jean-Yves Audibert, Ulrike von Luxburg</p><p>Abstract: Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. Keywords: graphs, graph Laplacians, semi-supervised learning, spectral clustering, dimensionality reduction</p><p>4 0.45001516 <a title="62-lda-4" href="./jmlr-2007-Structure_and_Majority_Classes_in_Decision_Tree_Learning.html">79 jmlr-2007-Structure and Majority Classes in Decision Tree Learning</a></p>
<p>Author: Ray J. Hickey</p><p>Abstract: To provide good classification accuracy on unseen examples, a decision tree, learned by an algorithm such as ID3, must have sufficient structure and also identify the correct majority class in each of its leaves. If there are inadequacies in respect of either of these, the tree will have a percentage classification rate below that of the maximum possible for the domain, namely (100 Bayes error rate). An error decomposition is introduced which enables the relative contributions of deficiencies in structure and in incorrect determination of majority class to be isolated and quantified. A sub-decomposition of majority class error permits separation of the sampling error at the leaves from the possible bias introduced by the attribute selection method of the induction algorithm. It is shown that sampling error can extend to 25% when there are more than two classes. Decompositions are obtained from experiments on several data sets. For ID3, the effect of selection bias is shown to vary from being statistically non-significant to being quite substantial, with the latter appearing to be associated with a simple underlying model. Keywords: decision tree learning, error decomposition, majority classes, sampling error, attribute selection bias 1</p><p>5 0.44025838 <a title="62-lda-5" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>Author: Guy Lebanon, Yi Mao, Joshua Dillon</p><p>Abstract: The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efﬁcient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks. Keywords: text processing, local smoothing</p><p>6 0.433676 <a title="62-lda-6" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>7 0.43345338 <a title="62-lda-7" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>8 0.4280329 <a title="62-lda-8" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>9 0.42705292 <a title="62-lda-9" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>10 0.42609689 <a title="62-lda-10" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>11 0.42482644 <a title="62-lda-11" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>12 0.42347246 <a title="62-lda-12" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>13 0.4217501 <a title="62-lda-13" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>14 0.42161012 <a title="62-lda-14" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>15 0.4161694 <a title="62-lda-15" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>16 0.41517669 <a title="62-lda-16" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>17 0.41515052 <a title="62-lda-17" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>18 0.41473514 <a title="62-lda-18" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>19 0.41383839 <a title="62-lda-19" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>20 0.41334587 <a title="62-lda-20" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
