<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-40" href="#">jmlr2007-40</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</h1>
<br/><p>Source: <a title="jmlr-2007-40-pdf" href="http://jmlr.org/papers/volume8/gabrilovich07a/gabrilovich07a.pdf">pdf</a></p><p>Author: Evgeniy Gabrilovich, Shaul Markovitch</p><p>Abstract: Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-speciﬁc and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two signiﬁcant problems in natural language processing—synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets conﬁrm improved performance compared to the bag of words document representation. Keywords: feature generation, text classiﬁcation, background knowledge</p><p>Reference: <a title="jmlr-2007-40-reference" href="../jmlr2007_reference/jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 IL  Department of Computer Science Technion—Israel Institute of Technology 32000 Haifa, Israel  Editor: Andrew McCallum  Abstract Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. [sent-5, score-0.61]
</p><p>2 In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. [sent-8, score-0.41]
</p><p>3 Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. [sent-11, score-0.917]
</p><p>4 Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. [sent-12, score-0.462]
</p><p>5 Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. [sent-14, score-0.395]
</p><p>6 Experimental results over a range of data sets conﬁrm improved performance compared to the bag of words document representation. [sent-16, score-0.422]
</p><p>7 The features commonly used are the individual words appearing in the training documents (while their order within the document is ignored). [sent-28, score-0.499]
</p><p>8 This representation scheme treats each document as a bag of the words it contains, and is therefore known as the bag of words (BOW) approach (Salton and McGill, 1983). [sent-30, score-0.64]
</p><p>9 The bag of words method is very effective in easy to medium difﬁculty categorization tasks where the category of a document can be identiﬁed by several easily distinguishable keywords. [sent-31, score-0.685]
</p><p>10 While the basic approaches are able to identify commonalities between documents based on word identity, and more advanced approaches can recognize synonyms, there are cases where identifying commonality between documents requires recognition of more elaborated semantic relations between terms. [sent-60, score-0.483]
</p><p>11 2298  K NOWLEDGE -BASED F EATURE G ENERATION  For illustration, consider document #15264 in Reuters-21578, which is one of the most frequently used data sets in text categorization research. [sent-61, score-0.568]
</p><p>12 Prior to text categorization, we employ a feature generator that uses common-sense and domain-speciﬁc knowledge to enrich the bag of words with new, more informative and discriminating features. [sent-72, score-0.639]
</p><p>13 Given an existing knowledge hierarchy (ODP in this case), the 2299  G ABRILOVICH AND M ARKOVITCH  feature generator examines documents and enriches their representation in a completely mechanical way. [sent-91, score-0.478]
</p><p>14 Second, we propose a novel kind of contextual analysis performed during feature generation, which views the document text as a sequence of local contexts, and performs implicit word sense disambiguation. [sent-94, score-0.559]
</p><p>15 Performing feature generation using external knowledge effectively capitalizes on human knowledge (as encoded by the editors of the Open Directory), leveraging information that cannot be deduced solely from the texts being classiﬁed. [sent-96, score-0.396]
</p><p>16 We also intend to apply the feature generation methodology to additional natural language processing tasks, as well as to study its applicability beyond text processing. [sent-100, score-0.486]
</p><p>17 Section 3 describes how our feature generation methodology uses repositories of human knowledge to overcome these limitations. [sent-104, score-0.389]
</p><p>18 Problems in the Bag of Words Approach Since the majority of existing text categorization systems employ the bag of words approach to represent documents, we begin by analyzing typical problems and limitations of this method. [sent-110, score-0.582]
</p><p>19 Words that appear in testing documents but not in training documents are completely ignored by the basic BOW approach that does not use external data to compensate for such vocabulary mismatch. [sent-112, score-0.387]
</p><p>20 2301  G ABRILOVICH AND M ARKOVITCH  Some of these limitations are due to data sparsity—after all, if we had inﬁnite amounts of text on every imaginable topic, the bag of words would perform much better. [sent-147, score-0.408]
</p><p>21 The feature generator is then invoked prior to text categorization to assign each document with a number of relevant concepts. [sent-172, score-0.753]
</p><p>22 Finally, we use traditional text categorization techniques to learn a text categorizer in the new feature space. [sent-176, score-0.618]
</p><p>23 This is a very natural thing to do, as text categorization is all about assigning documents or parts thereof to a predeﬁned set of categories (concepts in our case). [sent-202, score-0.686]
</p><p>24 Here each directory node deﬁnes a concept, and crawling the Web sites cataloged under the node provides a collection of textual objects for that node. [sent-221, score-0.627]
</p><p>25 By the very virtue of their deﬁnition, each hierarchy node can be associated with the text of books cataloged under the node. [sent-240, score-0.402]
</p><p>26 ” Again, we avoid using the term “features,” which is reserved for denoting individual entries of document vectors in text categorization per se. [sent-249, score-0.568]
</p><p>27 3 Building a Feature Generator The ﬁrst step in our methodology is preprocessing, performed once for all future text categorization tasks. [sent-251, score-0.408]
</p><p>28 We induce a hierarchical text classiﬁer that maps pieces of text onto relevant knowledge concepts, which later serve as generated features. [sent-252, score-0.459]
</p><p>29 The feature generator represents concepts as vectors of their most characteristic words, which we call attributes (reserving the term features to denote the properties of documents in text categorization). [sent-254, score-0.704]
</p><p>30 The feature generator operates similarly to a regular text classiﬁer—it ﬁrst learns a classiﬁcation model in the space of concept attributes, and then identiﬁes a set of concepts that are most appropriate to describe the contents of the input document. [sent-255, score-0.52]
</p><p>31 Observe that the number of concepts to which the feature generator classiﬁes document text is huge, as suitable knowledge repositories may contain tens and even hundreds of thousands of concepts. [sent-256, score-0.81]
</p><p>32 4 Contextual Feature Generation Feature generation precedes text categorization, that is, before the induction algorithm is invoked to build the text categorizer, the documents are fed to the feature generator. [sent-280, score-0.774]
</p><p>33 In the case of text processing, however, important information about word ordering will be lost if the traditional approach is applied to the bag of words. [sent-282, score-0.436]
</p><p>34 Therefore, we argue that feature generation becomes much more powerful when it operates on the raw document text. [sent-283, score-0.425]
</p><p>35 But should the generator always analyze the whole document as a single unit, as do regular text classiﬁers? [sent-284, score-0.515]
</p><p>36 1 A NALYZING L OCAL C ONTEXTS We believe that considering the document as a single unit can often be misleading: its text might be too diverse to be readily mapped to the right set of concepts, while notions mentioned only brieﬂy may be overlooked. [sent-293, score-0.394]
</p><p>37 Each context is classiﬁed into a number of concepts in the knowledge base, and pooling these concepts together to describe the entire document results in multi-faceted classiﬁcation. [sent-295, score-0.464]
</p><p>38 At the same time, enriching document representation with high-level concepts and their generalizations addresses the problem of synonymy, as the enhanced representation can easily recognize that two (or more) documents actually talk about related issues, albeit using different vocabularies. [sent-308, score-0.484]
</p><p>39 For each of these concepts we generate one feature that represents the concept itself, as well an additional group of features that represent ancestors of this concept in the hierarchy of the knowledge repository. [sent-311, score-0.416]
</p><p>40 Note also that the categories to which the documents are categorized most likely correspond to a mix of knowledge repository concepts rather than a single one. [sent-328, score-0.475]
</p><p>41 Therefore, as the feature generator maps documents to a large set of related concepts, it is up to feature selection to retain only those that are relevant to the particular categorization task in hand. [sent-329, score-0.596]
</p><p>42 3 F EATURE VALUATION In regular text categorization, each word occurrence in document text is initially counted as a unit, and then feature valuation is performed, usually by subjecting these counts to TF. [sent-332, score-0.749]
</p><p>43 To augment the bag of words with generated features and to use a single uniﬁed feature set, we need to assign weights to generated features in a compatible manner. [sent-334, score-0.424]
</p><p>44 While building the feature generator at the preprocessing stage, our system crawls the Web sites cataloged under mining-related ODP concepts such as B USINESS /M INING AND D RILLING, S CIENCE /T ECHNOLOGY /M INING and B USINESS /I NDUSTRIAL G OODS AND S ERVICES /M ATERIALS / M ETALS. [sent-342, score-0.547]
</p><p>45 During feature generation, the document is segmented into a sequence of contexts The feature generator analyzes these contexts and uses their words (e. [sent-349, score-0.644]
</p><p>46 Observe that the training documents for the category “copper” underwent similar processing when a text classiﬁer was induced. [sent-355, score-0.452]
</p><p>47 Consequently, features based on these concepts were selected during feature selection and retained in document vectors, thanks to their high predictive capacity. [sent-356, score-0.424]
</p><p>48 It is due to these features that the document is now categorized correctly, while without feature generation it consistently caused BOW classiﬁers to err. [sent-357, score-0.474]
</p><p>49 We therefore eliminate categories with fewer than 10 URLs or those situated below depth level 7 (the textual content of pruned categories is assigned to their parents). [sent-410, score-0.419]
</p><p>50 3 L EARNING THE F EATURE G ENERATOR In our current implementation, the feature generator works as a centroid-based classiﬁer (Han and Karypis, 2000), which represents each category as a centroid vector of the pool of textual objects associated with it. [sent-451, score-0.395]
</p><p>51 Following common practice, we used the ModApte split (9603 training, 3299 testing documents) and two category sets, 10 largest categories and 90 categories with at least one training and testing example. [sent-469, score-0.387]
</p><p>52 , 2004), with over 800,000 documents and three orthogonal category sets, presents a new challenge for text categorization. [sent-472, score-0.452]
</p><p>53 We used support vector machines6 as our learning algorithm to build text categorizers, since prior studies found SVMs to have the best performance for text categorization (Sebastiani, 2002; Dumais et al. [sent-495, score-0.554]
</p><p>54 3 T EXT C ATEGORIZATION We conducted the experiments using a text categorization platform of our own design and development named H OGWARTS 9 (Davidov et al. [sent-547, score-0.394]
</p><p>55 H OGWARTS facilitates full-cycle text categorization including text preprocessing, feature extraction, construction, selection and valuation, followed by actual classiﬁcation with cross-validation of experiments. [sent-550, score-0.618]
</p><p>56 5 text categorization algorithms, and computes all standard measures of categorization performance. [sent-555, score-0.538]
</p><p>57 The bag of words is next merged with the set of features generated for the document by analyzing its contexts as explained in Section 3. [sent-564, score-0.53]
</p><p>58 2315  G ABRILOVICH AND M ARKOVITCH  Since earlier studies found that most BOW features are indeed useful for SVM text categorization (Section 3. [sent-573, score-0.413]
</p><p>59 The word “Anakin” has been selected as an attribute for this concept due to its numerous occurrences in the cataloged Web sites such as http://www. [sent-610, score-0.44]
</p><p>60 While analyzing document contexts it also uses other words such as “Central Bank of Kenya” and “devaluation” to correctly map the document to ODP concepts S OCIETY /G OVERNMENT /F INANCE, S CIENCE /S OCIAL S CIENCES /E CONOMICS and B USINESS /F INANCIAL S ERVICES /BANKING S ERVICES. [sent-644, score-0.647]
</p><p>61 Similarly, document #18748 discusses Italy’s balance of payments and belongs to the category “trade” (interpreted as an economic indicator), while the word “trade” itself does not occur in this short document. [sent-646, score-0.394]
</p><p>62 These features, which were also generated for training documents in this category (notably, document #271 on Japanese trade surplus, document #312 on South Korea’s account surplus, document #354 on tariff cuts in Taiwan and document #718 on U. [sent-648, score-1.078]
</p><p>63 Table 4 shows the results of using feature generation for text categorization, with signiﬁcant improvements (p < 0. [sent-699, score-0.411]
</p><p>64 7%  Table 4: Text categorization with and without feature generation categorization performance was improved for all data sets, with notably high improvements for Reuters RCV1, OHSUMED and Movies. [sent-824, score-0.569]
</p><p>65 5 The Effect of Contextual Analysis We now explore the various possibilities for deﬁning document contexts for feature generation, that is, chunks of document text that are classiﬁed onto the ODP to construct features. [sent-827, score-0.721]
</p><p>66 The Baseline line represents text categorization without feature generation. [sent-831, score-0.428]
</p><p>67 In light of this, it could be expected that restricting the feature generator to a particular ODP branch that corresponds to the scope of the test collection would result in much better categorization accuracy due to the elimination of noise in “unused” ODP branches. [sent-850, score-0.39]
</p><p>68 2, feature generation constructed approximately 4–5 times as many features as are in the bag of words (after rare features that occurred in less than 3 documents were removed). [sent-857, score-0.71]
</p><p>69 4 that feature generation greatly improves text categorization for smaller categories, as can be evidenced in the greater improvements in macro-BEP. [sent-971, score-0.585]
</p><p>70 To explore this phenomenon further, we depict in Figures 7 and 8 the relation between the category size and the improvement due to feature generation for RCV1 (the number of categories in each bin appears in parentheses above the bars). [sent-972, score-0.459]
</p><p>71 For all other data sets, we created a short document from each original document by taking only the title of the latter (with the exception of Movie Reviews, where documents do not have titles). [sent-989, score-0.625]
</p><p>72 Thus, taking all the categories of the original documents to be “genuine” categories of the title is often misleading. [sent-993, score-0.515]
</p><p>73 As we can see, in the majority of cases (except for RCV1 Topic category sets), feature generation leads to greater improvement on short documents than on regular documents. [sent-997, score-0.483]
</p><p>74 This extra computation includes the (one-time) preprocessing step where the feature generator is built, as well as the actual feature generation performed on documents prior to text categorization. [sent-1001, score-0.769]
</p><p>75 11 These times constitute the additional overhead required by feature generation compared with regular text categorization. [sent-1018, score-0.411]
</p><p>76 7%  Table 6: Text categorization of short documents with and without feature generation. [sent-1182, score-0.411]
</p><p>77 In operational text categorization systems, documents rarely arrive in huge batches of hundreds of thousands at a time. [sent-1191, score-0.537]
</p><p>78 However, this approach does not enhance queries with high-level concepts beyond words or phrases (as this would require indexing the entire document collection accordingly). [sent-1201, score-0.415]
</p><p>79 However, even though feature generation is an established research area in machine learning, only a few works have applied it to text pro12. [sent-1208, score-0.411]
</p><p>80 Mikheev (1999) used a feature collocation lattice as a feature generation engine within a maximum entropy framework and applied it to document categorization, sentence boundary detection, and part-of-speech tagging. [sent-1216, score-0.537]
</p><p>81 Scott (1998) completely replaced a bag of words with a bag of synsets 13 . [sent-1228, score-0.409]
</p><p>82 (2003) used Medical Subject Headings (MeSH) (MeSH, 2003) to replace the bag of words with canonical medical terms; Bloehdorn and Hotho (2004) used a similar approach to augment Reuters21578 documents with WordNet synsets and OHSUMED medical documents with MeSH terms. [sent-1234, score-0.722]
</p><p>83 The unlabeled documents are used to deﬁne a cosine similarity metric, which is then used by the KNN algorithm for actual text categorization. [sent-1256, score-0.398]
</p><p>84 The addition of unlabeled documents signiﬁcantly increases the amount of data on which word cooccurrence statistics is estimated, thus providing a solution to text categorization problems where training data is particularly scarce. [sent-1262, score-0.673]
</p><p>85 There have also been other studies (notably, using semi-supervised learning methodology) that augmented the bag of words approach to text categorization with external knowledge distilled from unlabelled data (Goldberg and Zhu, 2006; Ando and Zhang, 2005a,b; Blei et al. [sent-1264, score-0.669]
</p><p>86 ESA uses the same basic feature generation methodology that we presented herein, but represents texts in the space of all available concepts (discarding the bag of words altogether), rather then augmenting the bag of words with a few top scoring concepts. [sent-1281, score-0.85]
</p><p>87 In our methodology, we ﬁrst learn a text classiﬁer that maps local document contexts onto ODP concepts, and then use this classiﬁer for feature generation in other learning tasks. [sent-1295, score-0.674]
</p><p>88 Conclusions and Future Work In this paper we proposed a feature generation methodology for text categorization. [sent-1310, score-0.455]
</p><p>89 The feature generator analyzes documents prior to text categorization and augments the conventional bag of words representation with relevant concepts from the knowledge repository. [sent-1313, score-1.093]
</p><p>90 The enriched representation contains information that cannot be deduced from the document text alone. [sent-1314, score-0.394]
</p><p>91 We also described multi-resolution analysis, which examines the document text at several levels of linguistic abstraction and performs feature generation at each level. [sent-1319, score-0.615]
</p><p>92 Furthermore, when the document text is processed at several levels of granularity, even brieﬂy mentioned aspects can be identiﬁed and used. [sent-1322, score-0.394]
</p><p>93 Empirical evaluation deﬁnitively conﬁrmed that knowledge-based feature generation brings text categorization to a new level of performance. [sent-1324, score-0.585]
</p><p>94 Interestingly, the sheer breadth and depth of the ODP, further boosted by crawling the URLs cataloged in the directory, brought about improvements both in regular text categorization as well as in the (non-topical) sentiment classiﬁcation task. [sent-1325, score-0.611]
</p><p>95 Indeed, to date we applied our feature generation methodology for improving the 2332  K NOWLEDGE -BASED F EATURE G ENERATION  performance of text categorization. [sent-1335, score-0.455]
</p><p>96 Finally, we conjecture that knowledge-based feature generation will also be useful for other information retrieval tasks beyond text categorization, and we intend to investigate this in our future work. [sent-1341, score-0.473]
</p><p>97 Current approaches to word sense disambiguation represent contexts that contain ambiguous words using the bag of words augmented with part-of-speech information. [sent-1346, score-0.48]
</p><p>98 To this end, we believe representation of such contexts can be greatly improved if we use feature generation to map such contexts into relevant knowledge concepts. [sent-1347, score-0.385]
</p><p>99 Parameterized generation of labeled datasets for text categorization based on a hierarchical directory. [sent-1481, score-0.554]
</p><p>100 RCV1: A new benchmark collection for text categorization research. [sent-1633, score-0.395]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('odp', 0.367), ('document', 0.204), ('text', 0.19), ('categorization', 0.174), ('documents', 0.173), ('bep', 0.166), ('generation', 0.157), ('usiness', 0.155), ('directory', 0.151), ('eneration', 0.149), ('categories', 0.149), ('bag', 0.145), ('abrilovich', 0.138), ('cataloged', 0.138), ('nowledge', 0.138), ('ociety', 0.138), ('eature', 0.122), ('wordnet', 0.122), ('textual', 0.121), ('generator', 0.121), ('arkovitch', 0.117), ('sites', 0.117), ('ohsumed', 0.115), ('op', 0.111), ('urls', 0.109), ('concepts', 0.107), ('bow', 0.103), ('word', 0.101), ('ining', 0.098), ('web', 0.096), ('category', 0.089), ('attack', 0.078), ('repositories', 0.078), ('gabrilovich', 0.075), ('hierarchy', 0.074), ('words', 0.073), ('cience', 0.069), ('crawling', 0.069), ('rilling', 0.069), ('feature', 0.064), ('ssues', 0.063), ('retrieval', 0.062), ('contexts', 0.059), ('cominco', 0.057), ('ealth', 0.057), ('egional', 0.057), ('macro', 0.057), ('mesh', 0.052), ('reuters', 0.052), ('crawled', 0.052), ('features', 0.049), ('dumais', 0.049), ('sentence', 0.048), ('knowledge', 0.046), ('attribute', 0.046), ('evgeniy', 0.046), ('gen', 0.046), ('merica', 0.046), ('orth', 0.046), ('rts', 0.046), ('synsets', 0.046), ('teck', 0.046), ('methodology', 0.044), ('augment', 0.044), ('micro', 0.044), ('title', 0.044), ('movies', 0.042), ('texts', 0.042), ('external', 0.041), ('copper', 0.04), ('croft', 0.04), ('etals', 0.04), ('sentiment', 0.04), ('shaul', 0.039), ('concept', 0.038), ('ci', 0.037), ('semantic', 0.036), ('unlabeled', 0.035), ('assault', 0.034), ('echnology', 0.034), ('html', 0.034), ('iseases', 0.034), ('raina', 0.034), ('rumsfeld', 0.034), ('markovitch', 0.034), ('medical', 0.034), ('sentences', 0.033), ('lewis', 0.033), ('collections', 0.033), ('hierarchical', 0.033), ('names', 0.032), ('healthy', 0.032), ('collection', 0.031), ('language', 0.031), ('companies', 0.03), ('development', 0.03), ('yang', 0.029), ('disambiguation', 0.029), ('ancer', 0.029), ('onditions', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000029 <a title="40-tfidf-1" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>Author: Evgeniy Gabrilovich, Shaul Markovitch</p><p>Abstract: Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-speciﬁc and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two signiﬁcant problems in natural language processing—synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets conﬁrm improved performance compared to the bag of words document representation. Keywords: feature generation, text classiﬁcation, background knowledge</p><p>2 0.1216514 <a title="40-tfidf-2" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>Author: Guy Lebanon, Yi Mao, Joshua Dillon</p><p>Abstract: The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efﬁcient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks. Keywords: text processing, local smoothing</p><p>3 0.085616998 <a title="40-tfidf-3" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis. Keywords: embedding algorithms, manifold learning, exponential families, multidimensional scaling, matrix factorization, semideﬁnite programming</p><p>4 0.0594116 <a title="40-tfidf-4" href="./jmlr-2007-Measuring_Differentiability%3A__Unmasking_Pseudonymous_Authors.html">54 jmlr-2007-Measuring Differentiability:  Unmasking Pseudonymous Authors</a></p>
<p>Author: Moshe Koppel, Jonathan Schler, Elisheva Bonchek-Dokow</p><p>Abstract: In the authorship verification problem, we are given examples of the writing of a single author and are asked to determine if given long texts were or were not written by this author. We present a new learning-based method for adducing the “depth of difference” between two example sets and offer evidence that this method solves the authorship verification problem with very high accuracy. The underlying idea is to test the rate of degradation of the accuracy of learned models as the best features are iteratively dropped from the learning process. Keywords: authorship attribution, one-class learning, unmasking 1</p><p>5 0.05691456 <a title="40-tfidf-5" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>Author: Sofus A. Macskassy, Foster Provost</p><p>Abstract: paper1 This is about classifying entities that are interlinked with entities for which the class is known. After surveying prior work, we present NetKit, a modular toolkit for classiﬁcation in networked data, and a case-study of its application to networked data used in prior machine learning research. NetKit is based on a node-centric framework in which classiﬁers comprise a local classiﬁer, a relational classiﬁer, and a collective inference procedure. Various existing node-centric relational learning algorithms can be instantiated with appropriate choices for these components, and new combinations of components realize new algorithms. The case study focuses on univariate network classiﬁcation, for which the only information used is the structure of class linkage in the network (i.e., only links and some class labels). To our knowledge, no work previously has evaluated systematically the power of class-linkage alone for classiﬁcation in machine learning benchmark data sets. The results demonstrate that very simple network-classiﬁcation models perform quite well—well enough that they should be used regularly as baseline classiﬁers for studies of learning with networked data. The simplest method (which performs remarkably well) highlights the close correspondence between several existing methods introduced for different purposes—that is, Gaussian-ﬁeld classiﬁers, Hopﬁeld networks, and relational-neighbor classiﬁers. The case study also shows that there are two sets of techniques that are preferable in different situations, namely when few versus many labels are known initially. We also demonstrate that link selection plays an important role similar to traditional feature selection. Keywords: relational learning, network learning, collective inference, collective classiﬁcation, networked data, probabilistic relational models, network analysis, network data</p><p>6 0.05588989 <a title="40-tfidf-6" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>7 0.055814441 <a title="40-tfidf-7" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>8 0.055704866 <a title="40-tfidf-8" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>9 0.047667917 <a title="40-tfidf-9" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>10 0.040895347 <a title="40-tfidf-10" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>11 0.040088553 <a title="40-tfidf-11" href="./jmlr-2007-Polynomial_Identification_in_the_Limit_of_Substitutable_Context-free_Languages.html">67 jmlr-2007-Polynomial Identification in the Limit of Substitutable Context-free Languages</a></p>
<p>12 0.038167119 <a title="40-tfidf-12" href="./jmlr-2007-The_Need_for_Open_Source_Software_in_Machine_Learning.html">82 jmlr-2007-The Need for Open Source Software in Machine Learning</a></p>
<p>13 0.036270417 <a title="40-tfidf-13" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>14 0.035276297 <a title="40-tfidf-14" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>15 0.035178676 <a title="40-tfidf-15" href="./jmlr-2007-Concave_Learners_for_Rankboost.html">23 jmlr-2007-Concave Learners for Rankboost</a></p>
<p>16 0.034551613 <a title="40-tfidf-16" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>17 0.033763677 <a title="40-tfidf-17" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>18 0.032247402 <a title="40-tfidf-18" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>19 0.030718323 <a title="40-tfidf-19" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>20 0.029120892 <a title="40-tfidf-20" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.177), (1, 0.176), (2, -0.024), (3, 0.102), (4, -0.053), (5, 0.117), (6, 0.027), (7, -0.069), (8, 0.046), (9, -0.043), (10, -0.039), (11, 0.1), (12, -0.308), (13, 0.304), (14, -0.088), (15, -0.095), (16, 0.213), (17, 0.129), (18, 0.067), (19, -0.12), (20, 0.024), (21, -0.037), (22, -0.061), (23, -0.06), (24, 0.095), (25, 0.102), (26, 0.014), (27, -0.018), (28, 0.103), (29, -0.046), (30, 0.054), (31, -0.048), (32, 0.035), (33, 0.103), (34, -0.047), (35, -0.048), (36, 0.026), (37, 0.007), (38, -0.074), (39, -0.005), (40, -0.092), (41, -0.018), (42, 0.038), (43, 0.01), (44, 0.059), (45, -0.071), (46, -0.022), (47, -0.109), (48, -0.074), (49, 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96356922 <a title="40-lsi-1" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>Author: Evgeniy Gabrilovich, Shaul Markovitch</p><p>Abstract: Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-speciﬁc and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two signiﬁcant problems in natural language processing—synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets conﬁrm improved performance compared to the bag of words document representation. Keywords: feature generation, text classiﬁcation, background knowledge</p><p>2 0.82526672 <a title="40-lsi-2" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>Author: Guy Lebanon, Yi Mao, Joshua Dillon</p><p>Abstract: The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efﬁcient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks. Keywords: text processing, local smoothing</p><p>3 0.41317454 <a title="40-lsi-3" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis. Keywords: embedding algorithms, manifold learning, exponential families, multidimensional scaling, matrix factorization, semideﬁnite programming</p><p>4 0.34585217 <a title="40-lsi-4" href="./jmlr-2007-Measuring_Differentiability%3A__Unmasking_Pseudonymous_Authors.html">54 jmlr-2007-Measuring Differentiability:  Unmasking Pseudonymous Authors</a></p>
<p>Author: Moshe Koppel, Jonathan Schler, Elisheva Bonchek-Dokow</p><p>Abstract: In the authorship verification problem, we are given examples of the writing of a single author and are asked to determine if given long texts were or were not written by this author. We present a new learning-based method for adducing the “depth of difference” between two example sets and offer evidence that this method solves the authorship verification problem with very high accuracy. The underlying idea is to test the rate of degradation of the accuracy of learned models as the best features are iteratively dropped from the learning process. Keywords: authorship attribution, one-class learning, unmasking 1</p><p>5 0.31634611 <a title="40-lsi-5" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: In numerous domains it is useful to represent a single example by the set of the local features or parts that comprise it. However, this representation poses a challenge to many conventional machine learning techniques, since sets may vary in cardinality and elements lack a meaningful ordering. Kernel methods can learn complex functions, but a kernel over unordered set inputs must somehow solve for correspondences—generally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function called the pyramid match that measures partial match similarity in time linear in the number of features. The pyramid match maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in order to ﬁnd implicit correspondences based on the ﬁnest resolution histogram cell where a matched pair ﬁrst appears. We show the pyramid match yields a Mercer kernel, and we prove bounds on its error relative to the optimal partial matching cost. We demonstrate our algorithm on both classiﬁcation and regression tasks, including object recognition, 3-D human pose inference, and time of publication estimation for documents, and we show that the proposed method is accurate and signiﬁcantly more efﬁcient than current approaches. Keywords: kernel, sets of features, histogram intersection, multi-resolution histogram pyramid, approximate matching, object recognition</p><p>6 0.27084661 <a title="40-lsi-6" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>7 0.25974655 <a title="40-lsi-7" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>8 0.2441382 <a title="40-lsi-8" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>9 0.23494036 <a title="40-lsi-9" href="./jmlr-2007-Polynomial_Identification_in_the_Limit_of_Substitutable_Context-free_Languages.html">67 jmlr-2007-Polynomial Identification in the Limit of Substitutable Context-free Languages</a></p>
<p>10 0.2004154 <a title="40-lsi-10" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>11 0.19963033 <a title="40-lsi-11" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>12 0.19179679 <a title="40-lsi-12" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>13 0.18460555 <a title="40-lsi-13" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>14 0.18254584 <a title="40-lsi-14" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>15 0.18234983 <a title="40-lsi-15" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>16 0.16222747 <a title="40-lsi-16" href="./jmlr-2007-The_Need_for_Open_Source_Software_in_Machine_Learning.html">82 jmlr-2007-The Need for Open Source Software in Machine Learning</a></p>
<p>17 0.15804878 <a title="40-lsi-17" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>18 0.15667398 <a title="40-lsi-18" href="./jmlr-2007-Concave_Learners_for_Rankboost.html">23 jmlr-2007-Concave Learners for Rankboost</a></p>
<p>19 0.14969862 <a title="40-lsi-19" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>20 0.14343512 <a title="40-lsi-20" href="./jmlr-2007-Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning.html">85 jmlr-2007-Transfer Learning via Inter-Task Mappings for Temporal Difference Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.011), (10, 0.022), (12, 0.016), (15, 0.601), (22, 0.025), (28, 0.031), (40, 0.024), (45, 0.016), (48, 0.023), (60, 0.02), (80, 0.032), (85, 0.036), (98, 0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92897999 <a title="40-lda-1" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>Author: Evgeniy Gabrilovich, Shaul Markovitch</p><p>Abstract: Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-speciﬁc and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two signiﬁcant problems in natural language processing—synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets conﬁrm improved performance compared to the bag of words document representation. Keywords: feature generation, text classiﬁcation, background knowledge</p><p>2 0.86014676 <a title="40-lda-2" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>Author: Robert Tibshirani, Trevor Hastie</p><p>Abstract: We propose a method for the classiﬁcation of more than two classes, from high-dimensional features. Our approach is to build a binary decision tree in a top-down manner, using the optimal margin classiﬁer at each split. We implement an exact greedy algorithm for this task, and compare its performance to less greedy procedures based on clustering of the matrix of pairwise margins. We compare the performance of the “margin tree” to the closely related “all-pairs” (one versus one) support vector machine, and nearest centroids on a number of cancer microarray data sets. We also develop a simple method for feature selection. We ﬁnd that the margin tree has accuracy that is competitive with other methods and offers additional interpretability in its putative grouping of the classes. Keywords: maximum margin classiﬁer, support vector machine, decision tree, CART</p><p>3 0.34707785 <a title="40-lda-3" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>Author: Guy Lebanon, Yi Mao, Joshua Dillon</p><p>Abstract: The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efﬁcient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks. Keywords: text processing, local smoothing</p><p>4 0.32154936 <a title="40-lda-4" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>Author: Dima Kuzmin, Manfred K. Warmuth</p><p>Abstract: n Maximum concept classes of VC dimension d over n domain points have size ≤d , and this is an upper bound on the size of any concept class of VC dimension d over n points. We give a compression scheme for any maximum class that represents each concept by a subset of up to d unlabeled domain points and has the property that for any sample of a concept in the class, the representative of exactly one of the concepts consistent with the sample is a subset of the domain of the sample. This allows us to compress any sample of a concept in the class to a subset of up to d unlabeled sample points such that this subset represents a concept consistent with the entire original sample. Unlike the previously known compression scheme for maximum classes (Floyd and Warmuth, 1995) which compresses to labeled subsets of the sample of size equal d, our new scheme is tight in the sense that the number of possible unlabeled compression sets of size at most d equals the number of concepts in the class. Keywords: compression schemes, VC dimension, maximum classes, one-inclusion graph</p><p>5 0.3122637 <a title="40-lda-5" href="./jmlr-2007-Handling_Missing_Values_when_Applying_Classification_Models.html">39 jmlr-2007-Handling Missing Values when Applying Classification Models</a></p>
<p>Author: Maytal Saar-Tsechansky, Foster Provost</p><p>Abstract: Much work has studied the effect of different treatments of missing values on model induction, but little work has analyzed treatments for the common case of missing values at prediction time. This paper ﬁrst compares several different methods—predictive value imputation, the distributionbased imputation used by C4.5, and using reduced models—for applying classiﬁcation trees to instances with missing values (and also shows evidence that the results generalize to bagged trees and to logistic regression). The results show that for the two most popular treatments, each is preferable under different conditions. Strikingly the reduced-models approach, seldom mentioned or used, consistently outperforms the other two methods, sometimes by a large margin. The lack of attention to reduced modeling may be due in part to its (perceived) expense in terms of computation or storage. Therefore, we then introduce and evaluate alternative, hybrid approaches that allow users to balance between more accurate but computationally expensive reduced modeling and the other, less accurate but less computationally expensive treatments. The results show that the hybrid methods can scale gracefully to the amount of investment in computation/storage, and that they outperform imputation even for small investments. Keywords: missing data, classiﬁcation, classiﬁcation trees, decision trees, imputation</p><p>6 0.31030187 <a title="40-lda-6" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>7 0.30455545 <a title="40-lda-7" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>8 0.30042553 <a title="40-lda-8" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>9 0.29909912 <a title="40-lda-9" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>10 0.29524633 <a title="40-lda-10" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>11 0.28331512 <a title="40-lda-11" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>12 0.27504471 <a title="40-lda-12" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>13 0.26969549 <a title="40-lda-13" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>14 0.26222706 <a title="40-lda-14" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>15 0.25655425 <a title="40-lda-15" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>16 0.25157571 <a title="40-lda-16" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>17 0.24472174 <a title="40-lda-17" href="./jmlr-2007-Large_Margin_Semi-supervised_Learning.html">44 jmlr-2007-Large Margin Semi-supervised Learning</a></p>
<p>18 0.24249594 <a title="40-lda-18" href="./jmlr-2007-Covariate_Shift_Adaptation_by_Importance_Weighted_Cross_Validation.html">25 jmlr-2007-Covariate Shift Adaptation by Importance Weighted Cross Validation</a></p>
<p>19 0.24227454 <a title="40-lda-19" href="./jmlr-2007-Nonlinear_Boosting_Projections_for_Ensemble_Construction.html">59 jmlr-2007-Nonlinear Boosting Projections for Ensemble Construction</a></p>
<p>20 0.23577622 <a title="40-lda-20" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
