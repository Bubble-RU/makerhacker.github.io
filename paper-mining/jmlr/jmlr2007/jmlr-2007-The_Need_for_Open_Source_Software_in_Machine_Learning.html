<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>82 jmlr-2007-The Need for Open Source Software in Machine Learning</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-82" href="#">jmlr2007-82</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>82 jmlr-2007-The Need for Open Source Software in Machine Learning</h1>
<br/><p>Source: <a title="jmlr-2007-82-pdf" href="http://jmlr.org/papers/volume8/sonnenburg07a/sonnenburg07a.pdf">pdf</a></p><p>Author: Sören Sonnenburg, Mikio L. Braun, Cheng Soon Ong, Samy Bengio, Leon Bottou, Geoffrey Holmes, Yann LeCun, Klaus-Robert Müller, Fernando Pereira, Carl Edward Rasmussen, Gunnar Rätsch, Bernhard Schölkopf, Alexander Smola, Pascal Vincent, Jason Weston, Robert Williamson</p><p>Abstract: Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the ﬁeld of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be signiﬁcantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientiﬁc community. Keywords: machine learning, open source, reproducibility, creditability, algorithms, software 2444 M ACHINE L EARNING O PEN S OURCE S OFTWARE</p><p>Reference: <a title="jmlr-2007-82-reference" href="../jmlr2007_reference/jmlr-2007-The_Need_for_Open_Source_Software_in_Machine_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. [sent-54, score-0.571]
</p><p>2 We argue that this situation can be signiﬁcantly improved by increasing incentives for researchers to publish their software under an open source model. [sent-55, score-1.133]
</p><p>3 Keywords: machine learning, open source, reproducibility, creditability, algorithms, software  2444  M ACHINE L EARNING O PEN S OURCE S OFTWARE  1. [sent-58, score-0.71]
</p><p>4 However, few machine learning researchers currently publish the software and/or source code associated with their papers (Thimbleby, 2003). [sent-62, score-1.098]
</p><p>5 This contrasts for instance with the practices of the bioinformatics community, where open source software has been the foundation of further research (Strajich and Lapp, 2006). [sent-63, score-1.065]
</p><p>6 We believe that open source sharing of machine learning software can play a very important role in removing that obstacle. [sent-65, score-1.035]
</p><p>7 The open source model has many advantages which will lead to better reproducibility of experimental results: quicker detection of errors, innovative applications, and faster adoption of machine learning methods in other disciplines and in industry. [sent-66, score-0.692]
</p><p>8 However, incentives for polishing and publishing software are at present lacking. [sent-67, score-0.575]
</p><p>9 This paper is structured as follows: First, we brieﬂy explain the idea behind open source software (Section 2). [sent-71, score-1.035]
</p><p>10 Finally, we propose a new, separate, ongoing track for machine learning open source software in JMLR (JMLR-MLOSS) in Section 5. [sent-74, score-1.035]
</p><p>11 We provide an overview about open source licenses in Appendix A and guidelines for good machine learning software in Appendix B. [sent-75, score-1.316]
</p><p>12 —Sir Isaac Newton (1642–1727)  The basic idea of open source software is very simple; programmers or users can read, modify and redistribute the source code of a piece of software (Gacek and Arief, 2004). [sent-78, score-2.15]
</p><p>13 While there are various licenses of open source software (cf. [sent-79, score-1.264]
</p><p>14 The open source model replaces central control with collaborative networks of contributors. [sent-82, score-0.539]
</p><p>15 The Open Source Initiative (OSI)1 deﬁnes open source software as work that satisﬁes the criteria spelled out in Table 1. [sent-84, score-1.035]
</p><p>16 To achieve this, the supporting software and data should be distributed under a suitable open source license along with the scientiﬁc paper. [sent-116, score-1.359]
</p><p>17 On the contrary, open source software has created numerous new opportunities for businesses (Riehle, 2007). [sent-138, score-1.035]
</p><p>18 Also, simply using an open source program on a day to day basis has little legal implications for a user provided they comply with the terms of their license. [sent-139, score-0.643]
</p><p>19 Users are free to copy and distribute the software as is. [sent-140, score-0.57]
</p><p>20 Most issues arise when users, playing the role of a developer, modify the software or incorporate it in their own programs and distribute a modiﬁed product. [sent-141, score-0.558]
</p><p>21 A variety of open source licenses exists, which protect different aspects of the software with beneﬁts for the initial developer or for developers creating derived work (Laurent, 2004). [sent-142, score-1.54]
</p><p>22 A developer who wants to give away the source code in exchange for proper credit for derivative works, even closed-source ones, could choose the BSD license. [sent-147, score-0.742]
</p><p>23 A developer who wants to give away the source code, is comfortable with his source being incorporated into a closed-source product but still wants to receive bug-ﬁxes and changes that are necessary to his source when integrating the code could choose the GNU Lesser General Public License (LGPL). [sent-151, score-1.397]
</p><p>24 This developer could be someone who wants to keep developing his software, and by publishing his software basically invites the community to contribute to the software. [sent-152, score-0.838]
</p><p>25 A developer who wants to give away the source code and make sure that his program stays open source, that is, any extension (or integration) will require both the original and the derived code to be released as open source, could choose the GNU General Public License (GPL). [sent-156, score-1.372]
</p><p>26 Here, the developer could be a researcher who has further plans with his software and wants to make sure that no closed-source product, not even one of his own if it includes changes of external developers, is beneﬁting from his software. [sent-157, score-0.796]
</p><p>27 A comparison of open source software licenses listed as “with strong communities” on http://opensource. [sent-178, score-1.264]
</p><p>28 All of the open source licenses allow for derivative works (item two in Table 1). [sent-182, score-0.768]
</p><p>29 In addition it is not possible to limit an open source product to a particular use, for example, to non-commercial or academic use, as it conﬂicts with item six in Table 1. [sent-183, score-0.539]
</p><p>30 In a brief summary of common open source licenses, Table 2 shows the rights of a developer to distribute a modiﬁed product. [sent-184, score-0.798]
</p><p>31 8 The CC project was started in 2001 to supply the analog to open source for less technical forms of expression (Coates, 2007) and extends to all kinds of media like text documents, photographs, video and music. [sent-192, score-0.539]
</p><p>32 All CC licenses allow copying, distribution, and public performance and display of the work without any license payments. [sent-193, score-0.649]
</p><p>33 It therefore conﬂicts with the a a non-discrimination provision in the open source deﬁnition (Table 1). [sent-197, score-0.539]
</p><p>34 Applied to the area of science, Creative Commons advocates not only a a having open source methods, but also open source data and results. [sent-203, score-1.078]
</p><p>35 Open Source in Machine Learning This section of the paper aims to provide a brief overview of open source software and its relationship to scientiﬁc activity, speciﬁcally machine learning. [sent-207, score-1.035]
</p><p>36 The truth is that it is extremely difﬁcult to obtain hard evidence on the debate between proprietary systems and open source software. [sent-209, score-0.584]
</p><p>37 11 We argue from moral, ethical and social grounds that open source should be the preferred software publication option for machine learning research and refer the reader to the many advantages of the open source software development (Raymond, 2000). [sent-210, score-2.12]
</p><p>38 Here, we focus on the speciﬁc advantages of open source software for machine learning research, which combines the needs and requirements both of being a scientiﬁc endeavor, as well as being a producer and consumer of software. [sent-212, score-1.035]
</p><p>39 So, it follows that an open source approach would be ideally suited to this challenge. [sent-254, score-0.539]
</p><p>40 2 Quicker Detection and Correction of Bugs An important feature that has contributed much to the success of open source software is that with the availability of the source code, it is much easier to spot and ﬁx bugs in software. [sent-284, score-1.439]
</p><p>41 The only question is what can be done about a particular instance of software failure, and that is where having the source matters. [sent-288, score-0.821]
</p><p>42 Therefore, the availability of open source implementations can help speed up scientiﬁc progress signiﬁcantly. [sent-298, score-0.657]
</p><p>43 4 Long Term Availability and Support For the individual researcher, open source may provide a means of ensuring that he will be able to use his research even after changing his employer. [sent-300, score-0.539]
</p><p>44 By releasing code under an open source license the chances of having long-term support are dramatically increased. [sent-311, score-1.043]
</p><p>45 6 Faster Adoption in Machine Learning, Other Disciplines and Industry Availability of high-quality open source implementations can ease adoption by other machine learning researchers, users in other disciplines and developers in industry for the following reasons: 1. [sent-326, score-0.78]
</p><p>46 Open source software can be used without cost in teaching. [sent-327, score-0.821]
</p><p>47 Publishing software as open source might also be the 15. [sent-335, score-1.035]
</p><p>48 There are also impressive precedents of open source software leading to the creation of multi-billion dollar companies and industries. [sent-343, score-1.068]
</p><p>49 Now with the publication of toolboxes according to an open source model, it becomes possible for individual projects to move towards standardization in a collaborative, distributed manner. [sent-349, score-0.634]
</p><p>50 Current Obstacles to an Open Source Community While there exist many advantages to publishing implementations according to the open source model, this option is currently not taken often. [sent-363, score-0.658]
</p><p>51 org as the platform for machine learning open source software (MLOSS) to openly discuss design decisions and to host and announce MLOSS. [sent-373, score-1.07]
</p><p>52 1 Publishing Software is Not Considered a Scientiﬁc Contribution Some researchers may not consider the extra effort to create a usable piece of software out of machine learning methods to be science. [sent-387, score-0.62]
</p><p>53 In reality, careful selection of a suitable open source license would satisfy the requirements of most researchers and their employer. [sent-401, score-0.914]
</p><p>54 For example, using the concept of dual licensing one could release the source code to the public under a open source license with strong reciprocal obligations (like the GNU GPL), and at the same time sell it commercially in a closed-source product. [sent-402, score-1.635]
</p><p>55 3 The Incentive for Publishing Open Source Software is not High Enough Unlike writing a journal article, releasing a piece of software is only the beginning. [sent-405, score-0.599]
</p><p>56 Maintaining a software package, ﬁxing bugs or writing further documentation requires time and commitment from the developer, and this contribution is also rarely acknowledged. [sent-406, score-0.578]
</p><p>57 As a result, researchers tend to not acknowledge software used in their published research, and the effort which has to be expended to turn a piece of code for personal research into a software product that can be used, understood, and extended by others is not sufﬁciently acknowledged. [sent-410, score-1.266]
</p><p>58 Therefore, at ﬁrst glance, making the source code for a particular machine learning paper public may seem counterproductive for the researcher, as other researchers can more easily ﬁnd problems with the proposed method, and possibly even discredit the approach. [sent-422, score-0.622]
</p><p>59 Therefore, the already altruistic behavior of publishing papers should be complemented by also providing open source code as the same great beneﬁts can be expected if many other researchers follow this path and also distribute accompanying open source software. [sent-425, score-1.417]
</p><p>60 Proposal In summary, providing open source code would help the whole community in accelerating research. [sent-435, score-0.719]
</p><p>61 Arguably, the best way to build an open source community of scientists in machine learning is to promote open source software through the existing reward system based on citation of archival sources (journals, conferences). [sent-436, score-1.631]
</p><p>62 We would like to initiate this process by giving researchers the opportunity to publish their machine learning open source software, thereby setting an example of how to deal with this kind of publication media. [sent-439, score-0.687]
</p><p>63 The proposed new JMLR track on machine learning open source software with review guidelines specially tailored to the needs of software is designed to serve that purpose. [sent-440, score-1.583]
</p><p>64 The software must adhere to a recognized open source license (http://www. [sent-443, score-1.359]
</p><p>65 Since we speciﬁcally want to honor the effort of turning a method into a highly usable piece of software, prior publication of the method is admissible, as long as the software has not been published elsewhere. [sent-447, score-0.619]
</p><p>66 In summary, preparing research software for publication is a signiﬁcant extra effort which should also be rewarded as such. [sent-449, score-0.546]
</p><p>67 It is hoped that the open source track will motivate the machine learning community towards open science, where open access publishing, open data standards and open source software foster research progress. [sent-450, score-2.326]
</p><p>68 1 Format We invite submissions of descriptions of high quality machine learning open source software implementations. [sent-452, score-1.08]
</p><p>69 A cover letter stating that the submission is intended for the machine learning open source software section, the open source license the software is released under, the web address of the project, and the software version to be reviewed. [sent-454, score-2.933]
</p><p>70 The quality of the user documentation (should enable new users to quickly apply the software to other problems, including a tutorial and several non-trivial examples of how the software can be used). [sent-474, score-1.056]
</p><p>71 After acceptance, the abstract including the link to the software project website, the four page description and the reviewed version of the software will be published on the JMLR-MLOSS website http://www. [sent-482, score-0.992]
</p><p>72 Conclusion We have argued that the adoption of the open source model of sharing information for implementations of machine learning software can be highly beneﬁcial for the whole ﬁeld. [sent-487, score-1.13]
</p><p>73 The open source model has many advantages, such as improved reproducibility of experimental results, quicker detection of errors, accelerated scientiﬁc progress, and faster adoption of machine learning methods in other disciplines and in the industry. [sent-488, score-0.692]
</p><p>74 As the incentives for publishing open source software are 2457  S ONNENBURG , B RAUN , O NG , ET AL . [sent-489, score-1.114]
</p><p>75 currently insufﬁcient, we outlined a platform for publishing software for machine learning. [sent-490, score-0.575]
</p><p>76 If machine learning is to solve real scientiﬁc and technological problems, the community needs to build on each others’ open source software tools. [sent-495, score-1.065]
</p><p>77 Hence, we believe that there is an urgent need for machine learning open source software. [sent-496, score-0.539]
</p><p>78 Such software will fulﬁll several concurrent roles: a better means for reproducing results; a mechanism for providing academic recognition for quality software implementations; and acceleration of the research process by allowing the standing on shoulders of others (not necessarily giants! [sent-497, score-0.992]
</p><p>79 As discussed in Section 2, most issues regarding the use of open source software arise when one wants to distribute a modiﬁed or derived product. [sent-515, score-1.104]
</p><p>80 With the proliferation of open source software, various licenses have been put forward, confusing a developer who just wants to release his program to the public. [sent-517, score-1.076]
</p><p>81 Whilst the choice of license might be considered a boring legal/management detail, it is actually very important to get it right— the choice of certain licenses may signiﬁcantly limit the impact a piece of software may have. [sent-518, score-1.122]
</p><p>82 Different licenses protect different aspects of the software with beneﬁts for the initial developer or developers creating derived work (Laurent, 2004). [sent-527, score-1.001]
</p><p>83 Signiﬁcant licensing issues may arise when open source software (OSS) is combined with proprietary code. [sent-528, score-1.159]
</p><p>84 Licenses which demand that subsequent modiﬁcations of the software be released under the same license are called “copyleft” licenses Wikipedia (2007a), the most famous of which is the GNU General Public License (GPL). [sent-530, score-1.092]
</p><p>85 Then there are the “in between” licenses, like Lesser GNU General Public  Figure 1: An illustration of open source licenses with respect to the rights for the initial developer and the developer creating derived works. [sent-533, score-1.191]
</p><p>86 This is referred to as dual licensing and allows a developer to release his code to the public under the GPL and at the same time sell it commercially in a closed-source product. [sent-538, score-0.551]
</p><p>87 1 Some Complexities As an illustration of some of the difﬁculties, let us consider the issue of conﬂicting open source licenses and the issue of reciprocal obligations. [sent-544, score-0.805]
</p><p>88 1 O PEN S OURCE L ICENSES MAY C ONFLICT When releasing a program as “open source” it is not obvious that although the program is now “open source” it still may have a license that conﬂicts with many other open source licenses. [sent-547, score-0.979]
</p><p>89 The OSI currently lists 60 open source licenses21 and the consequence of this license proliferation22 means that the simple inclusion BSD ⊂ LGPL ⊂ GPL as shown in Figure 1 does not hold for other licenses. [sent-549, score-0.863]
</p><p>90 While this can be used to purposely generate conﬂicts, as a general rule, one should refrain from doing so as it will make code exchange between open source projects impossible and may limit distribution and thus success of a open source project. [sent-551, score-1.262]
</p><p>91 Researchers aspiring to a wide developer audience for their software should consider GPL compatible licenses,24 or select one with a strong community. [sent-553, score-0.69]
</p><p>92 Also note that this is a one-way street, that is, BSD licensed software cannot merge code from LGPL/GPL and LGPL cannot merge software from GPL projects 24. [sent-565, score-1.142]
</p><p>93 2 Reciprocal Obligations Another issue is the one of reciprocal obligations: any modiﬁcations to a piece of open source software may need to be available to the original authors. [sent-573, score-1.145]
</p><p>94 While a certain lack of organization, documentation, and robustness can be tolerated when the software is used internally, it can make the software next to useless for others. [sent-594, score-0.992]
</p><p>95 Table 4: Six features of useful machine learning software Good machine learning software should ﬁrst of all be a good piece of software (Table 4). [sent-609, score-1.561]
</p><p>96 One should follow general rules for developing open source software (see also the discussion by Levesque, 2004, which highlights common failure modes for open source software development): • The software should be structured well and logically such that its usability is high. [sent-613, score-2.566]
</p><p>97 Open source software development as a special type of academic research (critique of vulgar Raymondism). [sent-665, score-0.821]
</p><p>98 Legal issues relating to free and open source software. [sent-714, score-0.583]
</p><p>99 Open source licenses and the creative commons framework: License selection and comparison. [sent-776, score-0.696]
</p><p>100 The economic motivation of open source software: Stakeholder perspectives. [sent-814, score-0.566]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('software', 0.496), ('source', 0.325), ('license', 0.324), ('licenses', 0.229), ('open', 0.214), ('developer', 0.194), ('code', 0.15), ('mpl', 0.141), ('gpl', 0.115), ('lgpl', 0.115), ('onnenburg', 0.106), ('ource', 0.106), ('raun', 0.106), ('oftware', 0.097), ('public', 0.096), ('yes', 0.092), ('scienti', 0.083), ('achine', 0.082), ('developers', 0.082), ('pen', 0.08), ('publishing', 0.079), ('licensing', 0.079), ('piece', 0.073), ('commons', 0.071), ('creative', 0.071), ('researcher', 0.067), ('cpl', 0.062), ('legal', 0.061), ('reproducibility', 0.061), ('bsd', 0.06), ('al', 0.059), ('adoption', 0.055), ('obligations', 0.053), ('guidelines', 0.052), ('gnu', 0.052), ('access', 0.052), ('researchers', 0.051), ('publication', 0.05), ('journals', 0.047), ('publish', 0.047), ('bugs', 0.045), ('toolboxes', 0.045), ('submissions', 0.045), ('proprietary', 0.045), ('progress', 0.044), ('programmers', 0.044), ('routines', 0.044), ('sonnenburg', 0.044), ('free', 0.044), ('program', 0.043), ('released', 0.043), ('library', 0.041), ('ng', 0.041), ('implementations', 0.04), ('commercial', 0.04), ('wants', 0.039), ('disciplines', 0.037), ('documentation', 0.037), ('reciprocal', 0.037), ('bug', 0.035), ('collobert', 0.035), ('jurisdiction', 0.035), ('openly', 0.035), ('osi', 0.035), ('rights', 0.035), ('samy', 0.035), ('silent', 0.035), ('jmlr', 0.035), ('exchange', 0.034), ('availability', 0.034), ('icts', 0.033), ('companies', 0.033), ('interfaces', 0.033), ('libraries', 0.033), ('release', 0.032), ('bengio', 0.032), ('programs', 0.032), ('gunnar', 0.031), ('frameworks', 0.031), ('earning', 0.031), ('community', 0.03), ('http', 0.03), ('distribute', 0.03), ('practices', 0.03), ('citations', 0.03), ('releasing', 0.03), ('wikipedia', 0.03), ('germany', 0.029), ('papers', 0.029), ('standards', 0.028), ('algebra', 0.027), ('users', 0.027), ('citation', 0.027), ('opening', 0.027), ('ong', 0.027), ('economic', 0.027), ('cc', 0.027), ('accepted', 0.027), ('apache', 0.026), ('citing', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000032 <a title="82-tfidf-1" href="./jmlr-2007-The_Need_for_Open_Source_Software_in_Machine_Learning.html">82 jmlr-2007-The Need for Open Source Software in Machine Learning</a></p>
<p>Author: Sören Sonnenburg, Mikio L. Braun, Cheng Soon Ong, Samy Bengio, Leon Bottou, Geoffrey Holmes, Yann LeCun, Klaus-Robert Müller, Fernando Pereira, Carl Edward Rasmussen, Gunnar Rätsch, Bernhard Schölkopf, Alexander Smola, Pascal Vincent, Jason Weston, Robert Williamson</p><p>Abstract: Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the ﬁeld of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be signiﬁcantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientiﬁc community. Keywords: machine learning, open source, reproducibility, creditability, algorithms, software 2444 M ACHINE L EARNING O PEN S OURCE S OFTWARE</p><p>2 0.062785223 <a title="82-tfidf-2" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>3 0.055640657 <a title="82-tfidf-3" href="./jmlr-2007-Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning.html">85 jmlr-2007-Transfer Learning via Inter-Task Mappings for Temporal Difference Learning</a></p>
<p>Author: Matthew E. Taylor, Peter Stone, Yaxin Liu</p><p>Abstract: Temporal difference (TD) learning (Sutton and Barto, 1998) has become a popular reinforcement learning technique in recent years. TD methods, relying on function approximators to generalize learning to novel situations, have had some experimental successes and have been shown to exhibit some desirable properties in theory, but the most basic algorithms have often been found slow in practice. This empirical result has motivated the development of many methods that speed up reinforcement learning by modifying a task for the learner or helping the learner better generalize to novel situations. This article focuses on generalizing across tasks, thereby speeding up learning, via a novel form of transfer using handcoded task relationships. We compare learning on a complex task with three function approximators, a cerebellar model arithmetic computer (CMAC), an artiﬁcial neural network (ANN), and a radial basis function (RBF), and empirically demonstrate that directly transferring the action-value function can lead to a dramatic speedup in learning with all three. Using transfer via inter-task mapping (TVITM), agents are able to learn one task and then markedly reduce the time it takes to learn a more complex task. Our algorithms are fully implemented and tested in the RoboCup soccer Keepaway domain. This article contains and extends material published in two conference papers (Taylor and Stone, 2005; Taylor et al., 2005). Keywords: transfer learning, reinforcement learning, temporal difference methods, value function approximation, inter-task mapping</p><p>4 0.039629292 <a title="82-tfidf-4" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis. Keywords: embedding algorithms, manifold learning, exponential families, multidimensional scaling, matrix factorization, semideﬁnite programming</p><p>5 0.038167119 <a title="82-tfidf-5" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>Author: Evgeniy Gabrilovich, Shaul Markovitch</p><p>Abstract: Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-speciﬁc and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two signiﬁcant problems in natural language processing—synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets conﬁrm improved performance compared to the bag of words document representation. Keywords: feature generation, text classiﬁcation, background knowledge</p><p>6 0.031403981 <a title="82-tfidf-6" href="./jmlr-2007-Multi-class_Protein_Classification_Using_Adaptive_Codes.html">57 jmlr-2007-Multi-class Protein Classification Using Adaptive Codes</a></p>
<p>7 0.02540607 <a title="82-tfidf-7" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>8 0.023754232 <a title="82-tfidf-8" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>9 0.021497566 <a title="82-tfidf-9" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>10 0.021085897 <a title="82-tfidf-10" href="./jmlr-2007-Very_Fast_Online_Learning_of_Highly_Non_Linear_Problems.html">91 jmlr-2007-Very Fast Online Learning of Highly Non Linear Problems</a></p>
<p>11 0.019512165 <a title="82-tfidf-11" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>12 0.019402914 <a title="82-tfidf-12" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>13 0.019168949 <a title="82-tfidf-13" href="./jmlr-2007-Nonlinear_Boosting_Projections_for_Ensemble_Construction.html">59 jmlr-2007-Nonlinear Boosting Projections for Ensemble Construction</a></p>
<p>14 0.019046977 <a title="82-tfidf-14" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>15 0.018978039 <a title="82-tfidf-15" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>16 0.018473733 <a title="82-tfidf-16" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>17 0.017770916 <a title="82-tfidf-17" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>18 0.01747006 <a title="82-tfidf-18" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>19 0.016849915 <a title="82-tfidf-19" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>20 0.016324706 <a title="82-tfidf-20" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.112), (1, 0.08), (2, -0.007), (3, 0.019), (4, -0.065), (5, 0.057), (6, -0.008), (7, -0.04), (8, -0.026), (9, -0.034), (10, -0.032), (11, -0.121), (12, -0.068), (13, 0.162), (14, 0.014), (15, -0.028), (16, 0.039), (17, -0.062), (18, -0.113), (19, 0.079), (20, 0.174), (21, -0.13), (22, -0.029), (23, -0.008), (24, -0.063), (25, 0.003), (26, 0.041), (27, 0.128), (28, -0.054), (29, 0.056), (30, -0.036), (31, 0.19), (32, -0.311), (33, 0.151), (34, 0.402), (35, -0.259), (36, -0.047), (37, 0.351), (38, -0.045), (39, -0.043), (40, 0.029), (41, -0.056), (42, -0.205), (43, 0.133), (44, -0.031), (45, 0.023), (46, 0.198), (47, -0.134), (48, -0.075), (49, -0.157)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99426717 <a title="82-lsi-1" href="./jmlr-2007-The_Need_for_Open_Source_Software_in_Machine_Learning.html">82 jmlr-2007-The Need for Open Source Software in Machine Learning</a></p>
<p>Author: Sören Sonnenburg, Mikio L. Braun, Cheng Soon Ong, Samy Bengio, Leon Bottou, Geoffrey Holmes, Yann LeCun, Klaus-Robert Müller, Fernando Pereira, Carl Edward Rasmussen, Gunnar Rätsch, Bernhard Schölkopf, Alexander Smola, Pascal Vincent, Jason Weston, Robert Williamson</p><p>Abstract: Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the ﬁeld of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be signiﬁcantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientiﬁc community. Keywords: machine learning, open source, reproducibility, creditability, algorithms, software 2444 M ACHINE L EARNING O PEN S OURCE S OFTWARE</p><p>2 0.34620398 <a title="82-lsi-2" href="./jmlr-2007-Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning.html">85 jmlr-2007-Transfer Learning via Inter-Task Mappings for Temporal Difference Learning</a></p>
<p>Author: Matthew E. Taylor, Peter Stone, Yaxin Liu</p><p>Abstract: Temporal difference (TD) learning (Sutton and Barto, 1998) has become a popular reinforcement learning technique in recent years. TD methods, relying on function approximators to generalize learning to novel situations, have had some experimental successes and have been shown to exhibit some desirable properties in theory, but the most basic algorithms have often been found slow in practice. This empirical result has motivated the development of many methods that speed up reinforcement learning by modifying a task for the learner or helping the learner better generalize to novel situations. This article focuses on generalizing across tasks, thereby speeding up learning, via a novel form of transfer using handcoded task relationships. We compare learning on a complex task with three function approximators, a cerebellar model arithmetic computer (CMAC), an artiﬁcial neural network (ANN), and a radial basis function (RBF), and empirically demonstrate that directly transferring the action-value function can lead to a dramatic speedup in learning with all three. Using transfer via inter-task mapping (TVITM), agents are able to learn one task and then markedly reduce the time it takes to learn a more complex task. Our algorithms are fully implemented and tested in the RoboCup soccer Keepaway domain. This article contains and extends material published in two conference papers (Taylor and Stone, 2005; Taylor et al., 2005). Keywords: transfer learning, reinforcement learning, temporal difference methods, value function approximation, inter-task mapping</p><p>3 0.32523152 <a title="82-lsi-3" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>4 0.18356872 <a title="82-lsi-4" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis. Keywords: embedding algorithms, manifold learning, exponential families, multidimensional scaling, matrix factorization, semideﬁnite programming</p><p>5 0.15234704 <a title="82-lsi-5" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>Author: Evgeniy Gabrilovich, Shaul Markovitch</p><p>Abstract: Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-speciﬁc and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two signiﬁcant problems in natural language processing—synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets conﬁrm improved performance compared to the bag of words document representation. Keywords: feature generation, text classiﬁcation, background knowledge</p><p>6 0.14977951 <a title="82-lsi-6" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>7 0.14423762 <a title="82-lsi-7" href="./jmlr-2007-Multi-class_Protein_Classification_Using_Adaptive_Codes.html">57 jmlr-2007-Multi-class Protein Classification Using Adaptive Codes</a></p>
<p>8 0.093096174 <a title="82-lsi-8" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>9 0.089266896 <a title="82-lsi-9" href="./jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</a></p>
<p>10 0.088118583 <a title="82-lsi-10" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>11 0.079712585 <a title="82-lsi-11" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>12 0.078915961 <a title="82-lsi-12" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>13 0.077731177 <a title="82-lsi-13" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>14 0.073403239 <a title="82-lsi-14" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>15 0.070344843 <a title="82-lsi-15" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>16 0.069013692 <a title="82-lsi-16" href="./jmlr-2007-Nonlinear_Boosting_Projections_for_Ensemble_Construction.html">59 jmlr-2007-Nonlinear Boosting Projections for Ensemble Construction</a></p>
<p>17 0.068733104 <a title="82-lsi-17" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>18 0.068438768 <a title="82-lsi-18" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>19 0.066170737 <a title="82-lsi-19" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>20 0.062932983 <a title="82-lsi-20" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.013), (10, 0.022), (12, 0.016), (15, 0.021), (22, 0.595), (28, 0.026), (40, 0.027), (45, 0.01), (48, 0.028), (60, 0.023), (80, 0.021), (85, 0.042), (98, 0.075)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90873301 <a title="82-lda-1" href="./jmlr-2007-The_Need_for_Open_Source_Software_in_Machine_Learning.html">82 jmlr-2007-The Need for Open Source Software in Machine Learning</a></p>
<p>Author: Sören Sonnenburg, Mikio L. Braun, Cheng Soon Ong, Samy Bengio, Leon Bottou, Geoffrey Holmes, Yann LeCun, Klaus-Robert Müller, Fernando Pereira, Carl Edward Rasmussen, Gunnar Rätsch, Bernhard Schölkopf, Alexander Smola, Pascal Vincent, Jason Weston, Robert Williamson</p><p>Abstract: Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the ﬁeld of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be signiﬁcantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientiﬁc community. Keywords: machine learning, open source, reproducibility, creditability, algorithms, software 2444 M ACHINE L EARNING O PEN S OURCE S OFTWARE</p><p>2 0.23762402 <a title="82-lda-2" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>3 0.21748823 <a title="82-lda-3" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis. Keywords: embedding algorithms, manifold learning, exponential families, multidimensional scaling, matrix factorization, semideﬁnite programming</p><p>4 0.2133964 <a title="82-lda-4" href="./jmlr-2007-Multi-class_Protein_Classification_Using_Adaptive_Codes.html">57 jmlr-2007-Multi-class Protein Classification Using Adaptive Codes</a></p>
<p>Author: Iain Melvin, Eugene Ie, Jason Weston, William Stafford Noble, Christina Leslie</p><p>Abstract: Predicting a protein’s structural class from its amino acid sequence is a fundamental problem in computational biology. Recent machine learning work in this domain has focused on developing new input space representations for protein sequences, that is, string kernels, some of which give state-of-the-art performance for the binary prediction task of discriminating between one class and all the others. However, the underlying protein classiﬁcation problem is in fact a huge multiclass problem, with over 1000 protein folds and even more structural subcategories organized into a hierarchy. To handle this challenging many-class problem while taking advantage of progress on the binary problem, we introduce an adaptive code approach in the output space of one-vsthe-rest prediction scores. Speciﬁcally, we use a ranking perceptron algorithm to learn a weighting of binary classiﬁers that improves multi-class prediction with respect to a ﬁxed set of output codes. We use a cross-validation set-up to generate output vectors for training, and we deﬁne codes that capture information about the protein structural hierarchy. Our code weighting approach signiﬁcantly improves on the standard one-vs-all method for two difﬁcult multi-class protein classiﬁcation problems: remote homology detection and fold recognition. Our algorithm also outperforms a previous code learning approach due to Crammer and Singer, trained here using a perceptron, when the dimension of the code vectors is high and the number of classes is large. Finally, we compare against PSI-BLAST, one of the most widely used methods in protein sequence analysis, and ﬁnd that our method strongly outperforms it on every structure clas∗. The ﬁrst two authors contributed equally to this work. c 2007 Iain Melvin, Eugene Ie, Jason Weston, William Stafford Noble and Christina Leslie. M ELVIN , I E , W ESTON , N OBLE AND L ESLIE siﬁcation problem that we consider. Supplementary data and source code are available at http: //www.cs</p><p>5 0.20413013 <a title="82-lda-5" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>Author: Simon Günter, Nicol N. Schraudolph, S. V. N. Vishwanathan</p><p>Abstract: We develop gain adaptation methods that improve convergence of the kernel Hebbian algorithm (KHA) for iterative kernel PCA (Kim et al., 2005). KHA has a scalar gain parameter which is either held constant or decreased according to a predetermined annealing schedule, leading to slow convergence. We accelerate it by incorporating the reciprocal of the current estimated eigenvalues as part of a gain vector. An additional normalization term then allows us to eliminate a tuning parameter in the annealing schedule. Finally we derive and apply stochastic meta-descent (SMD) gain vector adaptation (Schraudolph, 1999, 2002) in reproducing kernel Hilbert space to further speed up convergence. Experimental results on kernel PCA and spectral clustering of USPS digits, motion capture and image denoising, and image super-resolution tasks conﬁrm that our methods converge substantially faster than conventional KHA. To demonstrate scalability, we perform kernel PCA on the entire MNIST data set. Keywords: step size adaptation, gain vector adaptation, stochastic meta-descent, kernel Hebbian algorithm, online learning</p><p>6 0.19472551 <a title="82-lda-6" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>7 0.18755895 <a title="82-lda-7" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>8 0.18616182 <a title="82-lda-8" href="./jmlr-2007-Loop_Corrections_for_Approximate_Inference_on_Factor_Graphs.html">51 jmlr-2007-Loop Corrections for Approximate Inference on Factor Graphs</a></p>
<p>9 0.18537554 <a title="82-lda-9" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>10 0.18536234 <a title="82-lda-10" href="./jmlr-2007-Covariate_Shift_Adaptation_by_Importance_Weighted_Cross_Validation.html">25 jmlr-2007-Covariate Shift Adaptation by Importance Weighted Cross Validation</a></p>
<p>11 0.17111982 <a title="82-lda-11" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>12 0.16537592 <a title="82-lda-12" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>13 0.16521493 <a title="82-lda-13" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>14 0.16334721 <a title="82-lda-14" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>15 0.16299358 <a title="82-lda-15" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>16 0.16298839 <a title="82-lda-16" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>17 0.16298032 <a title="82-lda-17" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>18 0.16277146 <a title="82-lda-18" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>19 0.16175407 <a title="82-lda-19" href="./jmlr-2007-Handling_Missing_Values_when_Applying_Classification_Models.html">39 jmlr-2007-Handling Missing Values when Applying Classification Models</a></p>
<p>20 0.16172403 <a title="82-lda-20" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
