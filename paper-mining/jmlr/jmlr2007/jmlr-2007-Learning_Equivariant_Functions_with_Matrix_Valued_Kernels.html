<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-46" href="#">jmlr2007-46</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</h1>
<br/><p>Source: <a title="jmlr-2007-46-pdf" href="http://jmlr.org/papers/volume8/reisert07a/reisert07a.pdf">pdf</a></p><p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>Reference: <a title="jmlr-2007-46-reference" href="../jmlr2007_reference/jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 DE  LMB, Georges-Koehler-Allee 52 Albert-Ludwig University 79110 Freiburg, Germany  Editor: Leslie Pack Kaelbing  Abstract This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. [sent-5, score-1.274]
</p><p>2 Matrix valued kernels are a natural generalization of the common notion of a kernel. [sent-6, score-0.337]
</p><p>3 We set the theoretical foundations of so called equivariant matrix valued kernels. [sent-7, score-0.937]
</p><p>4 We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. [sent-8, score-0.8]
</p><p>5 The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. [sent-9, score-0.418]
</p><p>6 We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. [sent-11, score-0.924]
</p><p>7 Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series  1. [sent-12, score-0.314]
</p><p>8 The notion of a kernel as a kind of similarity between two given patterns can be naturally generalized to matrix valued kernels. [sent-17, score-0.391]
</p><p>9 A matrix valued kernel can carry more information than only similarity, like information about the relative pose or conﬁguration of the two patterns. [sent-18, score-0.44]
</p><p>10 First Burbea and Masani (1984) studied the theory of vector-valued reproducing kernel Hilbert spaces leading to the notion of a matrix (or operator) valued kernel. [sent-20, score-0.391]
</p><p>11 In this paper we introduce a new class of matrix valued kernels with a speciﬁc transformation behavior. [sent-22, score-0.39]
</p><p>12 The new type of kernel is motivated by a vector valued regression problem. [sent-23, score-0.338]
</p><p>13 In our words the term ’time-invariant’ means that the ﬁlter is equivariant to the group of time-shifts. [sent-26, score-0.764]
</p><p>14 We give a constructive way to obtain kernels, whose linear combinations lead to equivariant functions. [sent-30, score-0.662]
</p><p>15 The paper is organized as follows: In Section 2 we give a ﬁrst motivation for our kernels by solving an equivariant regression problem. [sent-36, score-0.777]
</p><p>16 We give an illustrative interpretation of the kernels and present an equivariant Representer Theorem, which justiﬁes our lax motivation from the beginning. [sent-37, score-0.777]
</p><p>17 In Section 3 we give rules how to construct matrix kernels out of matrix kernels and give constraints how to preserve equivariance. [sent-39, score-0.336]
</p><p>18 Section 4 introduces the notion of irreducibility for kernels and show how the traces of matrix valued kernels are related to scalar valued kernels. [sent-40, score-0.897]
</p><p>19 Two possible application of matrix valued kernels are given in Section 5: a rotation equivariant non-linear ﬁlter and a rotation invariant object detector. [sent-41, score-1.312]
</p><p>20 Then we give an interpretation of the proposed kernels and present an equivariant Representer Theorem. [sent-46, score-0.777]
</p><p>21 An equivariant function is a function fulﬁlling f(gx) = gf(x). [sent-60, score-0.662]
</p><p>22 Due to the unitary group representation the inner product is invariant in the sense gx1 |gx2 X = x1 |x2 X . [sent-63, score-0.463]
</p><p>23 A scalar valued kernel is a symmetric, positive deﬁnite function k : X × X → R fulﬁlling the Mercer property. [sent-64, score-0.476]
</p><p>24 n} in an equivariant manner, that is, the function has to satisfy f(gx) = gf(x) for all g ∈ G , while f(x i ) = yi should be fulﬁlled as accurate as possible. [sent-73, score-0.662]
</p><p>25 To obtain an equivariant behavior we pretend to present the training samples in all possible poses {(gx i , gyi )|i = 1. [sent-75, score-0.662]
</p><p>26 1 Every function of form (2) is an equivariant function with respect to G . [sent-81, score-0.662]
</p><p>27 Proof Due to the invariance of the kernel we have f(gx) =  Z  ∑ k(x, g−1g xi ) g ai dg ,  G i  and using the unimodularity of G we reparametrize the integral by h = g −1 g and obtain the desired result f(gx) =  Z  ∑ k(x, hxi ) ghai dh = gf(x),  G i  using the linearity of the group representation. [sent-82, score-0.593]
</p><p>28 So we have a constructive way to obtain equivariant functions. [sent-83, score-0.662]
</p><p>29 Since we can exchange summation with integration and the group action is linear we can rewrite (2) by n  f(x) = ∑  i=1  Z  G  k(x, gxi ) ρg dg ai ,  where we can interpret the expression inside the brackets as a matrix valued kernel. [sent-84, score-0.696]
</p><p>30 The following properties hold for a function above, a) For every x1 , x2 ∈ X and g, h ∈ G , we have that K(gx1 , hx2 ) = ρg K(x1 , x2 ) ρ† , h that is, K is equivariant in the ﬁrst argument and anti-equivariant in the second, we say K is equivariant. [sent-87, score-0.662]
</p><p>31 For b) we use the invariance and symmetry of k and the unimodularity of G and get K(x1 , x2 ) =  Z  G  k(g x1 , x2 ) ρg dg = −1  Z  G  k(x2 , gx1 ) ρg−1 dg = K(x2 , x1 )† ,  as asserted. [sent-91, score-0.518]
</p><p>32 There are basically two ways to introduce matrix valued kernels in almost the same manner as for scalar-valued kernels. [sent-92, score-0.415]
</p><p>33 As the upper expression is linear in y one can deﬁne a linear operator in Y , the so called matrix valued kernel K(x1 , x2 ) ∈ L(Y ), by the following K(x1 , x2 )y := (Kx2 y)(x1 ) equation. [sent-95, score-0.428]
</p><p>34 A second possibility to introduce the notion of a matrix valued kernel is to demand the existence of a feature mapping into a certain feature space. [sent-96, score-0.391]
</p><p>35 For example for scalar valued kernels this is done by Shawe-Taylor and Cristianini (2004). [sent-97, score-0.475]
</p><p>36 For matrix valued kernels the feature space can be identiﬁed as the tensor product space of linear mappings on Y times an arbitrary high dimensional feature space. [sent-98, score-0.48]
</p><p>37 Proof To show this, we give the feature mapping Ψ by the use of the feature mapping Φ corresponding to the scalar kernel k and show that the GIM-kernel can be written as a positive matrix valued inner product in the new feature space H = F ⊗ L(Y ). [sent-127, score-0.611]
</p><p>38 The matrix valued inner product in H is given by the rule Φ1 ⊗ ρ1 |Φ2 ⊗ ρ2 H := ρ† ρ2 Φ1 |Φ2 F 1 R  and its linear extension, that is, it is a sesqui-linear mapping of type H × H → L(Y ). [sent-129, score-0.357]
</p><p>39 Using the above rule for the inner product we can compute Ψ(x1 )|Ψ(x2 ) H  =  =  1 µ(G ) 1 µ(G )  Z  Φ(gx1 ) ⊗ ρg |Φ(hx2 ) ⊗ ρh H dg dh  Z  Φ(gx1 )|Φ(hx2 )  G2  G2  389  F  ρg−1 h dg dh. [sent-133, score-0.537]
</p><p>40 Y  ≥0  R EISERT AND B URKHARDT  Inserting the scalar kernel k and reparametrizing by g = g−1 h gives 1 Ψ(x1 )|Ψ(x2 ) H = µ(G )  Z  k(x1 , g x2 )ρg dg dh =  G2  Z  G  k(x1 , gx2 )ρg dg = K(x1 , x2 )  which is the desired result. [sent-134, score-0.709]
</p><p>41 3 Examples and Interpretation of Equivariant Kernels To get more intuition how equivariant kernels, not necessarily GIM-kernels, behave, we want to sketch how such kernels can be interpreted as estimates of the relative pose of two objects. [sent-136, score-0.85]
</p><p>42 First we consider the probably most simple equivariant kernel, the complex hermitian inner product itself. [sent-137, score-0.771]
</p><p>43 If we deﬁne the representation of U(1) acting on the Cn by a simple scalar multiplication with a complex unit number, then the ordinary inner product is obviously equivariant, that is g1 x1 |g2 x2 = eiφ1 x1 |eiφ2 x2 = eiφ1 x1 |x2 e−iφ2 = g1 x1 |x2 g† . [sent-139, score-0.36]
</p><p>44 Actually this result can be generalized for equivariant kernels. [sent-145, score-0.662]
</p><p>45 Interpreting this result, we can say that the sum of the singular values of a equivariant kernel expresses the similarity of the two given objects, while the unitary parts U and V contain information about the relative pose of the objects. [sent-149, score-0.995]
</p><p>46 The alignment with respect to those is a very common procedure in image processing applications to obtain complete invariant feature sets for the cyclic translation group Cn or other abelian groups like the two dimensional rotations SO(2) (see, for example, Canterakis, 1986). [sent-166, score-0.402]
</p><p>47 We did not clarify whether this is the optimal solution to obtain an equivariant behavior. [sent-171, score-0.662]
</p><p>48 It is easy to check that the subspace E ⊂ Zk of equivariant functions is a linear subspace. [sent-173, score-0.662]
</p><p>49 Proof Applying the projection on (1) yields (ΠE f)(x) = =  1 µ(G ) 1 µ(G )  n  Z  1 ∑ k(gx, xi ) g ai dg = µ(G ) i=1  Z  n  ∑ k(x, hxi )hai dh,  G  −1  Z  ∑ k(x, g−1xi ) g−1 ai dg  G i=1  n  G i=1  where we used the substitution h = g−1 . [sent-180, score-0.544]
</p><p>50 In fact, this projection is an unitary projection with respect to the naturally induced scalar product in the feature space. [sent-181, score-0.414]
</p><p>51 Before stating this more precisely we show how vector valued functions 391  R EISERT AND B URKHARDT  which are based on scalar kernels (as introduced in Eq. [sent-182, score-0.475]
</p><p>52 (5)  ˜ By the use of this we deﬁne a new operator ΠE acting on F ⊗ Y and show that it corresponds to ΠE and that it is unitary with respect to the inner product given in Eq. [sent-196, score-0.323]
</p><p>53 7 If ΠE is given by ˜ ΠE Ψf :=  1 gΨf dg µ(G ) G Z  ˜ ˜ then ΠE Ψf = ΨΠE f holds and ΠE is a unitary projection. [sent-199, score-0.381]
</p><p>54 ˜ In conclusion, due to the unitartity the projection ΠE gives the best equivariant approximation of an arbitrary function in Zk . [sent-206, score-0.698]
</p><p>55 The space of equivariant function E is nothing else than the space of ﬁx points with respect to the group action deﬁned in Eq. [sent-207, score-0.764]
</p><p>56 Then each equivariant minimizer f ∈ Zk of R(f) = c(x1 , y1 , f(x1 ), . [sent-213, score-0.662]
</p><p>57 Every equivariant function in Zk is a projection of the form ΠE f. [sent-222, score-0.698]
</p><p>58 Note that the above theorem makes only statements for vector valued function spaces induced by a scalar kernel. [sent-230, score-0.36]
</p><p>59 There are also matrix valued kernels that are not based on a scalar kernels, that is, GIM-kernels are not the only way to obtain equivariant kernels. [sent-231, score-1.19]
</p><p>60 In Section 4 we will work out that one important class of equivariant kernels are GIM-kernels, namely those kernels whose corresponding group representation is irreducible. [sent-232, score-1.037]
</p><p>61 In fact, Volterra series of degree n can be modeled with polynomial matrix kernels of degree n, that is, the scalar basis kernel is k(x1 , x2 ) = (1 + x1 |x2 )n . [sent-246, score-0.422]
</p><p>62 Constructing Kernels Similar to scalar valued kernels there are also several building rules to obtain new matrix and scalar valued kernels from existing ones. [sent-253, score-1.003]
</p><p>63 In particular, the rule for building a kernel out of two kernels by multiplying them splits into two rules, either using the tensor product or the matrix product. [sent-255, score-0.35]
</p><p>64 1 (Closure Properties) Let K1 : X × X → L(Y ) and K2 : X × X → L(Y ) be matrix valued kernels and A ∈ L(V , Y ) with full row-rank, then the following functions are kernels. [sent-257, score-0.39]
</p><p>65 4, where we mentioned that the trace of a positive matrix valued product is an ordinary positive scalar inner product. [sent-269, score-0.527]
</p><p>66 Let us have a look how such rules can be applied to equivariant kernels and under what circumstances equivariance is preserved. [sent-270, score-0.861]
</p><p>67 Since ρg is again an unitary representation, K is an equivariant kernel. [sent-274, score-0.83]
</p><p>68 Rule c) can also be used to construct equivariant kernels. [sent-275, score-0.662]
</p><p>69 Supposing an equivariant kernel K1 and an invariant scalar kernel k in sense that k(x1 , gx2 ) = k(x1 , x2 ) for all g ∈ G , then rule c) implies that K = K1 ⊗ k = K1 k is also a kernel and in fact equivariant. [sent-276, score-1.216]
</p><p>70 2 If K is a normal equivariant kernel, then k = tr(K † K) is an invariant kernel in the following sense k(x1 , gx2 ) = k(g x1 , x2 ) = k(x1 , x2 ) for all g, g ∈ G . [sent-284, score-0.846]
</p><p>71 A matrix kernel which is diagonal seems to be the most appropriate; this means that the kernel has to be diagonal for every pair of input patterns. [sent-291, score-0.335]
</p><p>72 For abelian groups the basis function on which the irreducible representation are working have the same formal appearance as the irreducible representations itself which might be confusing. [sent-308, score-0.528]
</p><p>73 If an equivariant kernel transforms by an irreducible representation, then it can be deduced that also the kernel itself is irreducible. [sent-316, score-1.07]
</p><p>74 But the opposite direction is not true in general, otherwise any equivariant kernel would be a GIM-kernel. [sent-317, score-0.778]
</p><p>75 2 If the representation ρ of a strictly positive deﬁnite, equivariant kernel K is irreducible then K is also irreducible. [sent-321, score-0.997]
</p><p>76 / Since we know from representation theory (Gaal, 1973) that any unitary representation can be decomposed in a direct sum of irreducible representations we can make a similar statement for kernels. [sent-328, score-0.493]
</p><p>77 3 Any GIM-kernel K can be decomposed in a direct sum of irreducible GIM-kernels associated with its irreducible representations. [sent-330, score-0.352]
</p><p>78 By the Peter-Weyl-Theorem (Gaal, 1973) we know that the entries of the irreducible representations of a compact group G form a basis for the space of square-integrable function on G . [sent-340, score-0.341]
</p><p>79 4 If the representation ρ of an equivariant kernel K is irreducible then K is a GIM-kernel. [sent-344, score-0.997]
</p><p>80 Proof We deﬁne the corresponding scalar kernel by k=  n tr(K), µ(G )  where n is the dimensionality of the associated group representation. [sent-345, score-0.356]
</p><p>81 g µ(G ) G Z  By the orthogonality relations for irreducible group representations we know that n tr(K(x1 , x2 )ρ† )ρg dg = K(x1 , x2 ). [sent-347, score-0.554]
</p><p>82 Any scalar kernel can be written in terms of its irreducible GIM-kernel expansion. [sent-349, score-0.43]
</p><p>83 3 Proof Due to the Peter-Weyl-Theorem we can expand the scalar kernel in terms of the irreducible representation of G , where the expansion coefﬁcients are by deﬁnition the corresponding GIMkernels. [sent-355, score-0.5]
</p><p>84 Hence we have a one-to-one correspondence between the scalar basis kernel k and the GIM(l) kernels Kl formed by the irreducible group representations ρg . [sent-361, score-0.688]
</p><p>85 1 Non GIM-kernels There is another very simple way to construct equivariant kernels. [sent-365, score-0.662]
</p><p>86 For example assume that X = Y then K(x1 , x2 ) = |x1 x2 | is obviously an equivariant kernel. [sent-366, score-0.691]
</p><p>87 6 Let K be an irreducible equivariant kernel and let its corresponding representation be reducible, then K is not a GIM-kernel. [sent-373, score-0.997]
</p><p>88 To characterize the kernel response for such patterns we deﬁne a linear unitary projection on the space of U -symmetric patterns as follows 1 πU x := gx dg. [sent-384, score-0.439]
</p><p>89 If x2 is U2 -symmetric then  K(x1 , x2 ) =  =  1 µ(G )  Z  G  1 k(x1 , gx2 )ρg dg = µ(G )  1 µ(G /U2 )  Z  k(x1 , gx2 ) dg  G /U2  Z  Z  k(x1 , gux2 )ρgu du dg  G /U2 U2  1  Z  µ(U2 ) U2  ρu du . [sent-391, score-0.639]
</p><p>90 The irreducible representations of 2D-rotations are e inφ and the irreducible subspaces correspond to the Fourier representation of the function. [sent-426, score-0.436]
</p><p>91 Using this kernel as X a scalar base kernel the matrix-valued feature space looks very simple. [sent-433, score-0.37]
</p><p>92 The induced matrix valued bilinear product in this space is the ordinary Kn (x1 , x2 ) = ∑[Ψ(x1 )]nl [Ψ(x2 )]∗ . [sent-435, score-0.343]
</p><p>93 The scalar basis kernel k for the GIM-kernel expansion of f is chosen by k(xz , xz ) = (1 + xz |xz X )2 . [sent-456, score-0.659]
</p><p>94 As already mentioned the irreducible kernels of the 2D-rotation group act on the Fourier representation of functions, so we denote p(φ|x) by the vector p(x), where its components are the Fourier coefﬁcients of p(φ|x) in respect to φ. [sent-527, score-0.436]
</p><p>95 Then the equivariant least-square minimizer p(x 0 ) is proportional to the Fourier transformed histogram of the relative angles φ i + ϕi . [sent-534, score-0.662]
</p><p>96 2  The scalar basis kernel is chosen to be the Gaussian kernel k(x 1 , x2 ) = e−λ||x1 −x2 || . [sent-541, score-0.37]
</p><p>97 The results are satisfying, the voting function p(x) obviously behaves in an equivariant manner and is robust against small distortions. [sent-556, score-0.755]
</p><p>98 2 to compute an invariant kernel and modify our matrix kernel by 405  R EISERT AND B URKHARDT  a)  b)  c)  Figure 6: A test image of a leaf with background and small clutter. [sent-569, score-0.465]
</p><p>99 Conclusion and Outlook We presented a new type of matrix valued kernel for learning equivariant functions. [sent-576, score-1.053]
</p><p>100 Another simple task for the equivariant ﬁlter would be to gaps in road networks. [sent-581, score-0.662]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('equivariant', 0.662), ('valued', 0.222), ('dg', 0.213), ('xz', 0.189), ('irreducible', 0.176), ('unitary', 0.168), ('scalar', 0.138), ('volterra', 0.137), ('eisert', 0.126), ('urkhardt', 0.126), ('gx', 0.119), ('kernel', 0.116), ('quivariant', 0.116), ('kernels', 0.115), ('group', 0.102), ('atrix', 0.098), ('ernels', 0.098), ('fourier', 0.097), ('lter', 0.087), ('image', 0.085), ('equivariance', 0.084), ('tr', 0.082), ('unctions', 0.08), ('object', 0.074), ('wiener', 0.071), ('invariant', 0.068), ('src', 0.063), ('detector', 0.061), ('rotation', 0.059), ('rectangle', 0.057), ('rotations', 0.054), ('matrix', 0.053), ('burkhardt', 0.053), ('dest', 0.053), ('reisert', 0.053), ('unimodularity', 0.053), ('pose', 0.049), ('inner', 0.046), ('fft', 0.044), ('blurred', 0.044), ('iy', 0.044), ('reducible', 0.044), ('representation', 0.043), ('bz', 0.042), ('gxi', 0.042), ('haasdonk', 0.042), ('hough', 0.042), ('unimodular', 0.042), ('ai', 0.041), ('representations', 0.041), ('images', 0.041), ('invariance', 0.039), ('voting', 0.039), ('zk', 0.038), ('operator', 0.037), ('groups', 0.037), ('neighborhood', 0.036), ('iz', 0.036), ('acting', 0.036), ('earning', 0.036), ('product', 0.036), ('projection', 0.036), ('representer', 0.034), ('kn', 0.033), ('ein', 0.032), ('hilbert', 0.032), ('ordinary', 0.032), ('rkhs', 0.032), ('abelian', 0.032), ('filter', 0.032), ('gaal', 0.032), ('irreducibility', 0.032), ('pixel', 0.031), ('interest', 0.03), ('tensor', 0.03), ('obviously', 0.029), ('dh', 0.029), ('kl', 0.028), ('lemma', 0.028), ('expansion', 0.027), ('kx', 0.027), ('leaf', 0.027), ('convolution', 0.027), ('actions', 0.027), ('gf', 0.027), ('gai', 0.027), ('schoelkopf', 0.027), ('hermitian', 0.027), ('diagonal', 0.025), ('coef', 0.025), ('manner', 0.025), ('want', 0.024), ('gk', 0.024), ('kernelized', 0.024), ('freiburg', 0.024), ('dimensional', 0.024), ('integration', 0.023), ('appearance', 0.023), ('know', 0.022), ('cients', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="46-tfidf-1" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>2 0.062007565 <a title="46-tfidf-2" href="./jmlr-2007-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">80 jmlr-2007-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>Author: Margarita Osadchy, Yann Le Cun, Matthew L. Miller</p><p>Abstract: We describe a novel method for simultaneously detecting faces and estimating their pose in real time. The method employs a convolutional network to map images of faces to points on a lowdimensional manifold parametrized by pose, and images of non-faces to points far away from that manifold. Given an image, detecting a face and estimating its pose is viewed as minimizing an energy function with respect to the face/non-face binary variable and the continuous pose parameters. The system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones. The system is designed to handle very large range of poses without retraining. The performance of the system was tested on three standard data sets—for frontal views, rotated faces, and proﬁles— is comparable to previous systems that are designed to handle a single one of these data sets. We show that a system trained simuiltaneously for detection and pose estimation is more accurate on both tasks than similar systems trained for each task separately.1 Keywords: face detection, pose estimation, convolutional networks, energy based models, object recognition</p><p>3 0.058098771 <a title="46-tfidf-3" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>Author: Onur C. Hamsici, Aleix M. Martinez</p><p>Abstract: Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere S p−1 . Such representations should then be modeled using spherical distributions. However, the difﬁculty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead—as if the data were represented in R p rather than S p−1 . This opens the question to whether the classiﬁcation results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases (which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to deﬁne optimal classiﬁers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classiﬁers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classiﬁcation of images of objects, gene expression sequences, and text data. Keywords: directional data, spherical distributions, normal distributions, norm normalization, linear and non-linear classiﬁers, computer vision</p><p>4 0.048240211 <a title="46-tfidf-4" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: In numerous domains it is useful to represent a single example by the set of the local features or parts that comprise it. However, this representation poses a challenge to many conventional machine learning techniques, since sets may vary in cardinality and elements lack a meaningful ordering. Kernel methods can learn complex functions, but a kernel over unordered set inputs must somehow solve for correspondences—generally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function called the pyramid match that measures partial match similarity in time linear in the number of features. The pyramid match maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in order to ﬁnd implicit correspondences based on the ﬁnest resolution histogram cell where a matched pair ﬁrst appears. We show the pyramid match yields a Mercer kernel, and we prove bounds on its error relative to the optimal partial matching cost. We demonstrate our algorithm on both classiﬁcation and regression tasks, including object recognition, 3-D human pose inference, and time of publication estimation for documents, and we show that the proposed method is accurate and signiﬁcantly more efﬁcient than current approaches. Keywords: kernel, sets of features, histogram intersection, multi-resolution histogram pyramid, approximate matching, object recognition</p><p>5 0.046992745 <a title="46-tfidf-5" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: Motivated by mathematical learning from training data, we introduce the notion of reﬁnable kernels. Various characterizations of reﬁnable kernels are presented. The concept of reﬁnable kernels leads to the introduction of wavelet-like reproducing kernels. We also investigate a reﬁnable kernel that forms a Riesz basis. In particular, we characterize reﬁnable translation invariant kernels, and reﬁnable kernels deﬁned by reﬁnable functions. This study leads to multiresolution analysis of reproducing kernel Hilbert spaces. Keywords: reﬁnable kernels, reﬁnable feature maps, wavelet-like reproducing kernels, dual kernels, learning with kernels, reproducing kernel Hilbert spaces, Riesz bases</p><p>6 0.045363769 <a title="46-tfidf-6" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>7 0.043528493 <a title="46-tfidf-7" href="./jmlr-2007-A_Complete_Characterization_of_a_Family_of_Solutions_to_a_Generalized_Fisher_Criterion.html">2 jmlr-2007-A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion</a></p>
<p>8 0.041415431 <a title="46-tfidf-8" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>9 0.040277787 <a title="46-tfidf-9" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>10 0.037816159 <a title="46-tfidf-10" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>11 0.036589194 <a title="46-tfidf-11" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>12 0.036163889 <a title="46-tfidf-12" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>13 0.03581905 <a title="46-tfidf-13" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>14 0.034063835 <a title="46-tfidf-14" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>15 0.033791423 <a title="46-tfidf-15" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>16 0.032899607 <a title="46-tfidf-16" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>17 0.031159943 <a title="46-tfidf-17" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>18 0.026820464 <a title="46-tfidf-18" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>19 0.0265733 <a title="46-tfidf-19" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>20 0.026052129 <a title="46-tfidf-20" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.157), (1, -0.046), (2, 0.083), (3, 0.093), (4, -0.167), (5, 0.002), (6, -0.0), (7, -0.019), (8, -0.064), (9, 0.051), (10, -0.08), (11, 0.093), (12, -0.037), (13, -0.097), (14, -0.049), (15, -0.058), (16, 0.026), (17, -0.12), (18, -0.01), (19, 0.022), (20, -0.003), (21, 0.091), (22, 0.189), (23, 0.084), (24, 0.019), (25, 0.087), (26, -0.041), (27, 0.007), (28, -0.212), (29, -0.101), (30, -0.199), (31, 0.203), (32, 0.065), (33, -0.039), (34, -0.002), (35, -0.084), (36, -0.036), (37, 0.094), (38, 0.278), (39, -0.043), (40, -0.231), (41, 0.081), (42, 0.003), (43, -0.283), (44, -0.026), (45, -0.05), (46, 0.109), (47, 0.211), (48, 0.223), (49, 0.227)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95698428 <a title="46-lsi-1" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>2 0.36944422 <a title="46-lsi-2" href="./jmlr-2007-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">80 jmlr-2007-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>Author: Margarita Osadchy, Yann Le Cun, Matthew L. Miller</p><p>Abstract: We describe a novel method for simultaneously detecting faces and estimating their pose in real time. The method employs a convolutional network to map images of faces to points on a lowdimensional manifold parametrized by pose, and images of non-faces to points far away from that manifold. Given an image, detecting a face and estimating its pose is viewed as minimizing an energy function with respect to the face/non-face binary variable and the continuous pose parameters. The system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones. The system is designed to handle very large range of poses without retraining. The performance of the system was tested on three standard data sets—for frontal views, rotated faces, and proﬁles— is comparable to previous systems that are designed to handle a single one of these data sets. We show that a system trained simuiltaneously for detection and pose estimation is more accurate on both tasks than similar systems trained for each task separately.1 Keywords: face detection, pose estimation, convolutional networks, energy based models, object recognition</p><p>3 0.31417841 <a title="46-lsi-3" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>Author: Kristen Grauman, Trevor Darrell</p><p>Abstract: In numerous domains it is useful to represent a single example by the set of the local features or parts that comprise it. However, this representation poses a challenge to many conventional machine learning techniques, since sets may vary in cardinality and elements lack a meaningful ordering. Kernel methods can learn complex functions, but a kernel over unordered set inputs must somehow solve for correspondences—generally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function called the pyramid match that measures partial match similarity in time linear in the number of features. The pyramid match maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in order to ﬁnd implicit correspondences based on the ﬁnest resolution histogram cell where a matched pair ﬁrst appears. We show the pyramid match yields a Mercer kernel, and we prove bounds on its error relative to the optimal partial matching cost. We demonstrate our algorithm on both classiﬁcation and regression tasks, including object recognition, 3-D human pose inference, and time of publication estimation for documents, and we show that the proposed method is accurate and signiﬁcantly more efﬁcient than current approaches. Keywords: kernel, sets of features, histogram intersection, multi-resolution histogram pyramid, approximate matching, object recognition</p><p>4 0.26287305 <a title="46-lsi-4" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>Author: Onur C. Hamsici, Aleix M. Martinez</p><p>Abstract: Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere S p−1 . Such representations should then be modeled using spherical distributions. However, the difﬁculty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead—as if the data were represented in R p rather than S p−1 . This opens the question to whether the classiﬁcation results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases (which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to deﬁne optimal classiﬁers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classiﬁers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classiﬁcation of images of objects, gene expression sequences, and text data. Keywords: directional data, spherical distributions, normal distributions, norm normalization, linear and non-linear classiﬁers, computer vision</p><p>5 0.21922162 <a title="46-lsi-5" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>Author: Kenji Fukumizu, Francis R. Bach, Arthur Gretton</p><p>Abstract: While kernel canonical correlation analysis (CCA) has been applied in many contexts, the convergence of ﬁnite sample estimates of the associated functions to their population counterparts has not yet been established. This paper gives a mathematical proof of the statistical convergence of kernel CCA, providing a theoretical justiﬁcation for the method. The proof uses covariance operators deﬁned on reproducing kernel Hilbert spaces, and analyzes the convergence of their empirical estimates of ﬁnite rank to their population counterparts, which can have inﬁnite rank. The result also gives a sufﬁcient condition for convergence on the regularization coefﬁcient involved in kernel CCA: this should decrease as n−1/3 , where n is the number of data. Keywords: canonical correlation analysis, kernel, consistency, regularization, Hilbert space</p><p>6 0.21121418 <a title="46-lsi-6" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>7 0.20964208 <a title="46-lsi-7" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>8 0.1768999 <a title="46-lsi-8" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>9 0.16890395 <a title="46-lsi-9" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>10 0.16694745 <a title="46-lsi-10" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>11 0.1653361 <a title="46-lsi-11" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>12 0.16011627 <a title="46-lsi-12" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>13 0.15925249 <a title="46-lsi-13" href="./jmlr-2007-From_External_to_Internal_Regret_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">34 jmlr-2007-From External to Internal Regret     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>14 0.15844014 <a title="46-lsi-14" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>15 0.1568273 <a title="46-lsi-15" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>16 0.15654238 <a title="46-lsi-16" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>17 0.1559433 <a title="46-lsi-17" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>18 0.14857371 <a title="46-lsi-18" href="./jmlr-2007-Large_Margin_Semi-supervised_Learning.html">44 jmlr-2007-Large Margin Semi-supervised Learning</a></p>
<p>19 0.14502865 <a title="46-lsi-19" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>20 0.14197209 <a title="46-lsi-20" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.037), (8, 0.025), (10, 0.025), (12, 0.044), (22, 0.01), (28, 0.06), (40, 0.042), (45, 0.013), (48, 0.074), (60, 0.033), (83, 0.381), (85, 0.054), (98, 0.085)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71082658 <a title="46-lda-1" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>2 0.36640352 <a title="46-lda-2" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>Author: Masashi Sugiyama</p><p>Abstract: Reducing the dimensionality of data without losing intrinsic information is an important preprocessing step in high-dimensional data analysis. Fisher discriminant analysis (FDA) is a traditional technique for supervised dimensionality reduction, but it tends to give undesired results if samples in a class are multimodal. An unsupervised dimensionality reduction method called localitypreserving projection (LPP) can work well with multimodal data due to its locality preserving property. However, since LPP does not take the label information into account, it is not necessarily useful in supervised learning scenarios. In this paper, we propose a new linear supervised dimensionality reduction method called local Fisher discriminant analysis (LFDA), which effectively combines the ideas of FDA and LPP. LFDA has an analytic form of the embedding transformation and the solution can be easily computed just by solving a generalized eigenvalue problem. We demonstrate the practical usefulness and high scalability of the LFDA method in data visualization and classiﬁcation tasks through extensive simulation studies. We also show that LFDA can be extended to non-linear dimensionality reduction scenarios by applying the kernel trick. Keywords: dimensionality reduction, supervised learning, Fisher discriminant analysis, locality preserving projection, afﬁnity matrix</p><p>3 0.34205091 <a title="46-lda-3" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>4 0.33910179 <a title="46-lda-4" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>Author: Onur C. Hamsici, Aleix M. Martinez</p><p>Abstract: Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere S p−1 . Such representations should then be modeled using spherical distributions. However, the difﬁculty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead—as if the data were represented in R p rather than S p−1 . This opens the question to whether the classiﬁcation results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases (which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to deﬁne optimal classiﬁers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classiﬁers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classiﬁcation of images of objects, gene expression sequences, and text data. Keywords: directional data, spherical distributions, normal distributions, norm normalization, linear and non-linear classiﬁers, computer vision</p><p>5 0.33784956 <a title="46-lda-5" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>6 0.3320989 <a title="46-lda-6" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>7 0.33007586 <a title="46-lda-7" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>8 0.32839772 <a title="46-lda-8" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>9 0.32604814 <a title="46-lda-9" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>10 0.32532388 <a title="46-lda-10" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>11 0.3244099 <a title="46-lda-11" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>12 0.32140589 <a title="46-lda-12" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>13 0.32075799 <a title="46-lda-13" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>14 0.32050633 <a title="46-lda-14" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>15 0.32010496 <a title="46-lda-15" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>16 0.31979221 <a title="46-lda-16" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>17 0.31863704 <a title="46-lda-17" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>18 0.31829607 <a title="46-lda-18" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>19 0.31820488 <a title="46-lda-19" href="./jmlr-2007-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">80 jmlr-2007-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>20 0.31787339 <a title="46-lda-20" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
