<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-5" href="#">jmlr2007-5</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</h1>
<br/><p>Source: <a title="jmlr-2007-5-pdf" href="http://jmlr.org/papers/volume8/li07a/li07a.pdf">pdf</a></p><p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>Reference: <a title="jmlr-2007-5-reference" href="../jmlr2007_reference/jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. [sent-15, score-0.862]
</p><p>2 Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density  1. [sent-25, score-0.8]
</p><p>3 The merit function reﬂects the general belief about good clustering, that is, objects in the same cluster should be similar to each other while those in different clusters be as distinct as possible. [sent-45, score-0.657]
</p><p>4 The clustering procedure involves ﬁrst ﬁtting a mixture model, usually by the EM algorithm, and then computing the posterior probability of each mixture component given a data point. [sent-51, score-0.579]
</p><p>5 Furthermore, data are allowed to reveal a nonparametric distribution for each cluster as part of the clustering procedure. [sent-78, score-0.606]
</p><p>6 Our new clustering algorithm groups data points into one cluster if they are associated with the same hilltop. [sent-84, score-0.571]
</p><p>7 Speciﬁcally, every cluster is ensured to be associated with a hill, and every sample point in the cluster can be moved to the corresponding hilltop along an ascending path without crossing any “valley” that separates two hills. [sent-89, score-0.685]
</p><p>8 Measures for the pairwise separability between clusters are proposed using ridgelines. [sent-102, score-0.586]
</p><p>9 A cluster merging algorithm to enhance modal clustering is developed. [sent-103, score-0.927]
</p><p>10 Comparisons are made with several other clustering algorithms such as linkage clustering, k-means, and Gaussian mixture modeling. [sent-105, score-0.587]
</p><p>11 The MEM and REM algorithms, the cluster diagnosis tool, and cluster merging procedure built upon REM are unique to our work. [sent-113, score-0.782]
</p><p>12 In addition, several measures of pairwise separability between clusters are deﬁned, which lead to the derivation of a new cluster merging algorithm. [sent-148, score-1.077]
</p><p>13 These density functions facilitate soft clustering as well as cluster assignment of samples outside the data set. [sent-234, score-0.658]
</p><p>14 It is known in the literature of mixture modeling that if the density of a cluster is estimated using only points assigned to this cluster, the variance tends to be under estimated, although the effect on clustering may be small (Celeux and Govaert, 1993). [sent-238, score-0.821]
</p><p>15 With g k (x) in (2) as the initial cluster density, compute the posterior of cluster k given each x i by pi,k ∝ |Ck | gk (x), n |G|  k = 1, . [sent-244, score-0.692]
</p><p>16 On the other hand, if the maximum a posteriori clustering based on the ﬁnal gk (x) differs signiﬁcantly from the result of ˜ modal clustering, this procedure may have defeated the very purpose of modal clustering and turned it into merely an initialization scheme. [sent-252, score-0.957]
</p><p>17 At any bandwidth σl , the cluster representatives in Gl−1 obtained from the preceding bandwidth are input into MAC using the density f (x|S, σ2 ). [sent-265, score-0.571]
</p><p>18 That is, the cluster of xi at level l is determined by its cluster representative in Gl−1 . [sent-294, score-0.68]
</p><p>19 In linkage clustering, at every level, only the two clusters with the minimum pairwise distance are merged. [sent-296, score-0.584]
</p><p>20 In HMAC, however, at any level, the merging of clusters is conducted globally and the effect of every original data point on clustering is retained through the density f (x|S, σ2 ). [sent-299, score-0.933]
</p><p>21 Among the 20 different σ l ’s, only 6 of them result in clustering different from σl−1 , reﬂecting the fact that the bandwidth needs to increase by a sufﬁcient amount to drive the merging of some existing cluster representatives. [sent-311, score-0.835]
</p><p>22 Analysis of Cluster Separability via Ridgelines A measure for the separability between clusters is useful for gaining deeper understanding of clustering structures in data. [sent-345, score-0.818]
</p><p>23 Although the separability measure is a diagnostic tool and the cluster merging method can adjust the number of clusters, in this paper, we do not pursue the problem of choosing the number of clusters fully automatically. [sent-352, score-1.049]
</p><p>24 The separability measure we deﬁne here exploits the geometry of the density functions of two clusters in a comprehensive manner. [sent-354, score-0.645]
</p><p>25 (b) The MEM ascending paths from the modes at level 2 (crosses) to the modes at level 3 (squares), and the contours of the density estimate at level 3. [sent-373, score-0.591]
</p><p>26 1 Ridgeline The ridgeline between two clusters with density g1 (x) and g2 (x) is  L = {x(α) : (1 − α)∇ log g1 (x) + α∇ log g2 (x) = 0, 0 ≤ α ≤ 1} . [sent-381, score-0.69]
</p><p>27 The task of identifying insigniﬁcant clusters in Figure 1(c) is not particularly challenging because the two smallest clusters are “singletons” (containing only one sample). [sent-446, score-0.732]
</p><p>28 The low separability of cluster 9 is caused by its proximity to cluster 1, the largest cluster which accounts for 60% of the data. [sent-455, score-1.065]
</p><p>29 However, an enlarged bandwidth may cause prominent clusters to be clumped while leaving undesirable small “noisy” clusters unchanged. [sent-473, score-0.872]
</p><p>30 Merging clusters according to the separability measure is one possible approach to eliminate “noisy” clusters without losing important clustering structures found at a small bandwidth. [sent-474, score-1.184]
</p><p>31 We denote the pairwise separability between cluster zi and z j in short by Si, j , where Si, j = S(zi , z j ). [sent-480, score-0.581]
</p><p>32 The main idea of the merging algorithm is to have clusters with a higher signiﬁcance index absorb other clusters that are not well separated from them and are less dominant (lower signiﬁcance index). [sent-487, score-0.983]
</p><p>33 First, a cluster z i may be weakly separated from several other clusters with higher signiﬁcance indices. [sent-489, score-0.682]
</p><p>34 Second, two weakly separated clusters zi and z j may have the same signiﬁcance indices, that is, δ(zi ) = δ(z j ); and hence it is ambiguous which cluster should be treated as the dominant one. [sent-491, score-0.752]
</p><p>35 Clique: Cluster zi and z j are in the same clique if (a) zi and z j are tied, or (b) there is a cluster zk such that zi and zk are in the same clique, and z j and zk are in the same clique. [sent-501, score-0.585]
</p><p>36 Since all the clusters in ci have equal signiﬁcance indices, we let δ(ci ) = δ(zi ), where zi is any cluster included in ci . [sent-516, score-0.841]
</p><p>37 We call the merging procedure conducted based on the separability measure stage I merging and that based on coverage rate stage II merging. [sent-555, score-0.711]
</p><p>38 (a) The clustering result after merging clusters in the same cliques. [sent-634, score-0.826]
</p><p>39 In a nutshell, linkage clustering forms clusters by progressively merging a pair of current clusters. [sent-640, score-1.016]
</p><p>40 Our merging algorithm is a particular kind of linkage clustering where the elements to be clustered are cliques and the distance between the cliques is the separability. [sent-647, score-0.832]
</p><p>41 We may call this linkage clustering 1702  N ONPARAMETRIC M ODAL C LUSTERING  algorithm directional single linkage for reasons that will be self-evident shortly. [sent-650, score-0.667]
</p><p>42 An alternative is to apply the directional single linkage clustering and stop merging when a desired number of clusters is achieved. [sent-656, score-1.043]
</p><p>43 Figure 2(a) shows the clustering after merging clusters in the same clique. [sent-666, score-0.826]
</p><p>44 To highlight the relationship between the merged clusters and the original clusters, the list of updated symbols for each original cluster is given in every scatter plot. [sent-668, score-0.717]
</p><p>45 At ρ = 95%, only the cluster of size 1 is marked as an outlier cluster, and is merged with the cluster of size 6. [sent-674, score-0.674]
</p><p>46 Because clusters with low separability are apt to be grouped together when the kernel bandwidth increases, it is not surprising for the clustering result obtained by the merging algorithm to agree with clustering at a higher level of HMAC. [sent-675, score-1.497]
</p><p>47 On the other hand, examining separability and identifying outlier clusters enhance the robustness of clustering results, a valuable trait especially in high dimensions. [sent-676, score-0.85]
</p><p>48 Instead, we can apply the merging algorithm to a relatively large number of clusters obtained at an early level and reduce the number of clusters to the desired value. [sent-679, score-1.005]
</p><p>49 We start by comparing linkage clustering with mixture modeling using two data sets. [sent-797, score-0.653]
</p><p>50 In average linkage, if cluster z 2 and z3 are merged into z4 , the distance between z1 and z4 is calculated as d(z1 , z4 ) = n2n2 3 d(z1 , z2 ) + +n n3 n2 +n3 d(z1 , z3 ), where n2 and n3 are the cluster sizes of z2 and z3 respectively. [sent-803, score-0.642]
</p><p>51 In this example, the mixture model is initialized by the clustering obtained from k-means; and the covariance matrices of the two clusters are not restricted. [sent-818, score-0.763]
</p><p>52 We apply HMAC to both data sets and show the clustering results obtained at the level where two clusters are formed. [sent-830, score-0.719]
</p><p>53 The two clusters obtained by average linkage clustering and HMAC are identical to the original ones. [sent-834, score-0.816]
</p><p>54 The above two data sets exemplify situations in which either the average linkage clustering or mixture modeling (or k-means) performs well but not both. [sent-840, score-0.653]
</p><p>55 Mixture modeling favors elliptical clusters because of the Gaussian assumption, and k-means favors spherical clusters due to extra restrictions on Gaussian components. [sent-844, score-0.778]
</p><p>56 The greedy pairwise merging in average linkage becomes rather arbitrary when clusters are not well separated. [sent-847, score-0.784]
</p><p>57 Chipman and Tibshirani (2006) noted that bottom-up agglomerative clustering methods, such as average linkage, tend to generate good small clusters but suffer at extracting a few large clusters. [sent-866, score-0.626]
</p><p>58 A hybrid approach is proposed in that paper to combine the advantages of bottom-up and topdown clustering, which ﬁrst seeks mutual clusters by bottom-up linkage clustering and then applies top-down clustering to the mutual clusters. [sent-868, score-1.076]
</p><p>59 For HMAC, the level of the dendrogram yielding two clusters is chosen to create the partition of the data. [sent-877, score-0.621]
</p><p>60 In Mclust, the mixture model is initialized using the suggested default option, that is, to initialize the partition of data by an agglomerative hierarchical clustering approach, an extension of linkage clustering based on Gaussian mixtures (Fraley and Raftery, 2006). [sent-881, score-0.896]
</p><p>61 This initialization may be of advantage especially to data in this study because as shown previously, linkage clustering generates perfect clustering for the noisy curve data set in the ﬁrst example. [sent-882, score-0.771]
</p><p>62 The dendrogram and the table show that 3 clusters containing more than 5 points are formed at the ﬁrst level. [sent-958, score-0.575]
</p><p>63 At level 4, two of the 3 prominent clusters are merged, leaving 2 prominent clusters which are further merged at level 7. [sent-960, score-1.05]
</p><p>64 One remedy is to apply the cluster merging algorithm to a larger number of clusters formed at a lower level. [sent-969, score-0.884]
</p><p>65 In Figure 6(d), three clusters are formed by applying stage II merging based on coverage rate. [sent-977, score-0.679]
</p><p>66 Merging based on separability is not conducted because we attempt to get 3 clusters and the 3 largest clusters at this level already account for close to 95% of the data. [sent-979, score-0.997]
</p><p>67 This clustering result is much more preferable than the 3 clusters directly generated by HMAC at level 1710  N ONPARAMETRIC M ODAL C LUSTERING  Level 10  Level 7  Level 3  Level 1  (a) 0. [sent-980, score-0.699]
</p><p>68 (d), (e) Three (two) clusters obtained by applying the merging algorithm to clusters generated by HMAC at level 3. [sent-1034, score-1.005]
</p><p>69 4 (stage I) and merging based on coverage rate with ρ = 95% (stage II), we obtain two clusters shown in Figure 6(e). [sent-1038, score-0.619]
</p><p>70 The ridgeline between the two major clusters at level 2 is computed, and the density function along this ridgeline is plotted in Figure 8(c). [sent-1059, score-0.935]
</p><p>71 (c) Density function along the ridgeline between the two major clusters at level 2. [sent-1076, score-0.659]
</p><p>72 (d) Two clusters obtained by applying the merging algorithm to clusters generated by HMAC. [sent-1077, score-0.932]
</p><p>73 (e) The modal curves of the two main clusters obtained by HMAC at level 2. [sent-1078, score-0.615]
</p><p>74 Just as in the clustering by HMAC, one cluster had longer attention time to the initial stimulus in both occasions. [sent-1101, score-0.578]
</p><p>75 Instead, we provide a summarized version of the dendrogram emphasizing prominent clusters in Figure 9(a). [sent-1122, score-0.604]
</p><p>76 (b) The density function along the ridgeline between the two major clusters at level 12. [sent-1151, score-0.746]
</p><p>77 This dendrogram strongly suggests there are two major clusters in this data set because before level 8, the clusters formed are too small and the percentage of data not covered by the clusters is too high. [sent-1162, score-1.451]
</p><p>78 To examine the separation of the two clusters at this level, we calculate the ridgeline between them and plot the density function along the ridgeline in Figure 9(b). [sent-1165, score-0.831]
</p><p>79 The mode heights of the two clusters are close, and the cluster separation is strong. [sent-1166, score-0.744]
</p><p>80 If we specify the range for the number of clusters as 2 to 10 and let Mclust choose the best number and the best covariance structure using BIC, the number of clusters chosen is 3. [sent-1207, score-0.732]
</p><p>81 On the other hand, if we ﬁx the number of clusters at 2 and run Mclust, the clustering accuracy becomes 94%. [sent-1213, score-0.626]
</p><p>82 Under this principle, we declare a level of the dendrogram too low (small bandwidth) if all the clusters are small and a level too high (large bandwidth) if a very large portion of the data (e. [sent-1220, score-0.714]
</p><p>83 A more profound effect on the clustering result comes from the merging procedure based on separability which involves choosing a separability threshold. [sent-1226, score-0.844]
</p><p>84 2, the merging process based on separability is essentially the directional single linkage clustering which stops when all the between-cluster separability measures are above the threshold. [sent-1228, score-1.061]
</p><p>85 Hence in situations where we have a targeted number of clusters to create, we can avoid choosing the threshold and simply stop the directional single linkage when the given number is reached. [sent-1230, score-0.603]
</p><p>86 Of course, if at a certain level of the dendrogram, there exist precisely the targeted number of valid clusters, we can use that level directly rather than applying merging on clusters at a lower level. [sent-1231, score-0.732]
</p><p>87 Whether the HMAC dendrogram clearly suggests the right number of clusters can be highly data dependent. [sent-1232, score-0.568]
</p><p>88 For instance, in the document clustering example, the dendrogram in Figure 9(a) strongly suggests two clusters because at all the acceptable levels there are two reasonably large clusters. [sent-1233, score-0.808]
</p><p>89 If precisely two clusters merge at every increased level of a dendrogram, that is, the number of clusters decreases exactly by one, the dendrogram will have n levels, where n is the data size. [sent-1239, score-1.041]
</p><p>90 In practice, since the targeted number of clusters is normally much smaller than the data size, it is easy to ﬁnd a relatively low level at which the number of clusters exceeds the target. [sent-1247, score-0.845]
</p><p>91 We can then apply the separability based directional single linkage clustering at that level, and achieve any number of clusters smaller than the starting value. [sent-1248, score-1.035]
</p><p>92 A basic approach to image segmentation is to cluster the pixel color components and label pixels in the same cluster as one region (Li and Gray, 2000). [sent-1251, score-0.665]
</p><p>93 (d) Starting from the ﬁrst level of the i=1 dendrogram formed by HMAC, apply the cluster merging algorithm described in Section 4. [sent-1267, score-0.773]
</p><p>94 If the number of clusters after merging is smaller than or equal to the given targeted number of segments, stop and output the clustering results at this level. [sent-1269, score-0.846]
</p><p>95 K-means clusters the pixels and computes the centroid vector for each cluster according to the criterion of minimizing the mean squared distance between the original vectors and the centroid vectors. [sent-1292, score-0.657]
</p><p>96 For HMAC, the mode color vector of each cluster is shown as a color bar; and for K-means, the mean vector of each cluster is shown. [sent-1301, score-0.669]
</p><p>97 A hierarchical clustering algorithm, HMAC, is developed by gradually increasing the bandwidth of the kernel functions and by recursively treating modes acquired at a smaller bandwidth as points to be clustered when a larger bandwidth is used. [sent-1310, score-0.718]
</p><p>98 A separability measure between two clusters is deﬁned based on the ridgeline, which takes comprehensive consideration of the exact densities of the clusters. [sent-1312, score-0.587]
</p><p>99 A cluster merging method based on pairwise separability is developed, which addresses the competing factors of using a small bandwidth to retain major clustering structures and using a large one to achieve a low number of clusters. [sent-1313, score-1.086]
</p><p>100 The HMAC clustering algorithm and its combination with the cluster merging algorithm are tested using both simulated and real data sets. [sent-1314, score-0.793]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hmac', 0.43), ('clusters', 0.366), ('cluster', 0.291), ('clustering', 0.26), ('merging', 0.2), ('separability', 0.192), ('linkage', 0.19), ('ridgeline', 0.189), ('dendrogram', 0.182), ('modal', 0.176), ('mem', 0.138), ('mixture', 0.137), ('mclust', 0.132), ('indsay', 0.117), ('odal', 0.117), ('modes', 0.104), ('onparametric', 0.099), ('mode', 0.087), ('density', 0.087), ('gk', 0.085), ('clique', 0.084), ('bandwidth', 0.084), ('ay', 0.081), ('ascending', 0.077), ('lustering', 0.075), ('level', 0.073), ('rem', 0.072), ('infants', 0.072), ('zi', 0.07), ('cliques', 0.068), ('head', 0.067), ('segmentation', 0.06), ('merged', 0.06), ('infant', 0.059), ('ci', 0.057), ('prominent', 0.056), ('sc', 0.055), ('coverage', 0.053), ('em', 0.047), ('modeling', 0.046), ('pk', 0.046), ('clustered', 0.046), ('mac', 0.044), ('visualization', 0.043), ('glass', 0.042), ('ridgelines', 0.039), ('discriminant', 0.038), ('pl', 0.038), ('li', 0.037), ('grouped', 0.035), ('parametric', 0.035), ('nonparametric', 0.035), ('fk', 0.035), ('hi', 0.035), ('bandwidths', 0.035), ('ck', 0.034), ('merge', 0.034), ('gl', 0.034), ('leung', 0.033), ('occasion', 0.033), ('stage', 0.033), ('lindsay', 0.033), ('outlier', 0.032), ('circle', 0.031), ('major', 0.031), ('gaussian', 0.03), ('colors', 0.03), ('xii', 0.03), ('directed', 0.029), ('densities', 0.029), ('hierarchical', 0.029), ('pairwise', 0.028), ('fraley', 0.028), ('directional', 0.027), ('stimulus', 0.027), ('kernel', 0.027), ('formed', 0.027), ('absorb', 0.026), ('hilltop', 0.026), ('mink', 0.026), ('minnotte', 0.026), ('posterior', 0.025), ('xi', 0.025), ('separated', 0.025), ('representatives', 0.025), ('loop', 0.024), ('log', 0.024), ('ray', 0.024), ('image', 0.023), ('cance', 0.023), ('projection', 0.022), ('simulated', 0.022), ('directions', 0.022), ('noisy', 0.021), ('jain', 0.02), ('tied', 0.02), ('zm', 0.02), ('data', 0.02), ('raftery', 0.02), ('targeted', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="5-tfidf-1" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>2 0.20398107 <a title="5-tfidf-2" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>Author: Wei Pan, Xiaotong Shen</p><p>Abstract: Variable selection in clustering analysis is both challenging and important. In the context of modelbased clustering analysis with a common diagonal covariance matrix, which is especially suitable for “high dimension, low sample size” settings, we propose a penalized likelihood approach with an L1 penalty function, automatically realizing variable selection via thresholding and delivering a sparse solution. We derive an EM algorithm to ﬁt our proposed model, and propose a modiﬁed BIC as a model selection criterion to choose the number of components and the penalization parameter. A simulation study and an application to gene function prediction with gene expression proﬁles demonstrate the utility of our method. Keywords: BIC, EM, mixture model, penalized likelihood, soft-thresholding, shrinkage</p><p>3 0.13822326 <a title="5-tfidf-3" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>Author: Sanjoy Dasgupta, Leonard Schulman</p><p>Abstract: We show that, given data from a mixture of k well-separated spherical Gaussians in R d , a simple two-round variant of EM will, with high probability, learn the parameters of the Gaussians to nearoptimal precision, if the dimension is high (d ln k). We relate this to previous theoretical and empirical work on the EM algorithm. Keywords: expectation maximization, mixtures of Gaussians, clustering, unsupervised learning, probabilistic analysis</p><p>4 0.12118074 <a title="5-tfidf-4" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>Author: Robert Tibshirani, Trevor Hastie</p><p>Abstract: We propose a method for the classiﬁcation of more than two classes, from high-dimensional features. Our approach is to build a binary decision tree in a top-down manner, using the optimal margin classiﬁer at each split. We implement an exact greedy algorithm for this task, and compare its performance to less greedy procedures based on clustering of the matrix of pairwise margins. We compare the performance of the “margin tree” to the closely related “all-pairs” (one versus one) support vector machine, and nearest centroids on a number of cancer microarray data sets. We also develop a simple method for feature selection. We ﬁnd that the margin tree has accuracy that is competitive with other methods and offers additional interpretability in its putative grouping of the classes. Keywords: maximum margin classiﬁer, support vector machine, decision tree, CART</p><p>5 0.11185756 <a title="5-tfidf-5" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>Author: Philippe Rigollet</p><p>Abstract: We consider semi-supervised classiﬁcation when part of the available data is unlabeled. These unlabeled data can be useful for the classiﬁcation problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples. Keywords: semi-supervised learning, statistical learning theory, classiﬁcation, cluster assumption, generalization bounds</p><p>6 0.10828663 <a title="5-tfidf-6" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>7 0.080860965 <a title="5-tfidf-7" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>8 0.048297901 <a title="5-tfidf-8" href="./jmlr-2007-Multi-Task_Learning_for_Classification_with_Dirichlet_Process_Priors.html">56 jmlr-2007-Multi-Task Learning for Classification with Dirichlet Process Priors</a></p>
<p>9 0.044045281 <a title="5-tfidf-9" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>10 0.043863602 <a title="5-tfidf-10" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>11 0.0421785 <a title="5-tfidf-11" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>12 0.038062461 <a title="5-tfidf-12" href="./jmlr-2007-Dynamics_and_Generalization_Ability_of_LVQ_Algorithms.html">30 jmlr-2007-Dynamics and Generalization Ability of LVQ Algorithms</a></p>
<p>13 0.037145335 <a title="5-tfidf-13" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>14 0.036925156 <a title="5-tfidf-14" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>15 0.033442993 <a title="5-tfidf-15" href="./jmlr-2007-Local_Discriminant_Wavelet_Packet_Coordinates_for_Face_Recognition.html">50 jmlr-2007-Local Discriminant Wavelet Packet Coordinates for Face Recognition</a></p>
<p>16 0.032696996 <a title="5-tfidf-16" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>17 0.031844076 <a title="5-tfidf-17" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>18 0.031142378 <a title="5-tfidf-18" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>19 0.030868549 <a title="5-tfidf-19" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>20 0.029377092 <a title="5-tfidf-20" href="./jmlr-2007-Loop_Corrections_for_Approximate_Inference_on_Factor_Graphs.html">51 jmlr-2007-Loop Corrections for Approximate Inference on Factor Graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.237), (1, 0.136), (2, 0.014), (3, 0.132), (4, 0.16), (5, -0.476), (6, 0.141), (7, 0.143), (8, 0.047), (9, -0.002), (10, -0.025), (11, -0.054), (12, 0.052), (13, 0.112), (14, -0.064), (15, -0.078), (16, 0.003), (17, -0.071), (18, -0.008), (19, 0.021), (20, -0.078), (21, 0.11), (22, 0.061), (23, -0.037), (24, 0.051), (25, -0.015), (26, 0.001), (27, 0.045), (28, 0.238), (29, 0.048), (30, -0.161), (31, -0.02), (32, -0.021), (33, 0.048), (34, 0.06), (35, 0.033), (36, -0.083), (37, 0.001), (38, 0.028), (39, 0.011), (40, 0.023), (41, 0.018), (42, 0.033), (43, -0.038), (44, -0.068), (45, -0.032), (46, 0.004), (47, -0.037), (48, 0.084), (49, -0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97965866 <a title="5-lsi-1" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>2 0.85120165 <a title="5-lsi-2" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>Author: Wei Pan, Xiaotong Shen</p><p>Abstract: Variable selection in clustering analysis is both challenging and important. In the context of modelbased clustering analysis with a common diagonal covariance matrix, which is especially suitable for “high dimension, low sample size” settings, we propose a penalized likelihood approach with an L1 penalty function, automatically realizing variable selection via thresholding and delivering a sparse solution. We derive an EM algorithm to ﬁt our proposed model, and propose a modiﬁed BIC as a model selection criterion to choose the number of components and the penalization parameter. A simulation study and an application to gene function prediction with gene expression proﬁles demonstrate the utility of our method. Keywords: BIC, EM, mixture model, penalized likelihood, soft-thresholding, shrinkage</p><p>3 0.50220454 <a title="5-lsi-3" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>Author: Marc Teboulle</p><p>Abstract: Center-based partitioning clustering algorithms rely on minimizing an appropriately formulated objective function, and different formulations suggest different possible algorithms. In this paper, we start with the standard nonconvex and nonsmooth formulation of the partitioning clustering problem. We demonstrate that within this elementary formulation, convex analysis tools and optimization theory provide a unifying language and framework to design, analyze and extend hard and soft center-based clustering algorithms, through a generic algorithm which retains the computational simplicity of the popular k-means scheme. We show that several well known and more recent center-based clustering algorithms, which have been derived either heuristically, or/and have emerged from intuitive analogies in physics, statistical techniques and information theoretic perspectives can be recovered as special cases of the proposed analysis and we streamline their relationships. Keywords: clustering, k-means algorithm, convex analysis, support and asymptotic functions, distance-like functions, Bregman and Csiszar divergences, nonlinear means, nonsmooth optimization, smoothing algorithms, ﬁxed point methods, deterministic annealing, expectation maximization, information theory and entropy methods</p><p>4 0.47346708 <a title="5-lsi-4" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>Author: Philippe Rigollet</p><p>Abstract: We consider semi-supervised classiﬁcation when part of the available data is unlabeled. These unlabeled data can be useful for the classiﬁcation problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples. Keywords: semi-supervised learning, statistical learning theory, classiﬁcation, cluster assumption, generalization bounds</p><p>5 0.46886599 <a title="5-lsi-5" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>Author: Sanjoy Dasgupta, Leonard Schulman</p><p>Abstract: We show that, given data from a mixture of k well-separated spherical Gaussians in R d , a simple two-round variant of EM will, with high probability, learn the parameters of the Gaussians to nearoptimal precision, if the dimension is high (d ln k). We relate this to previous theoretical and empirical work on the EM algorithm. Keywords: expectation maximization, mixtures of Gaussians, clustering, unsupervised learning, probabilistic analysis</p><p>6 0.37363437 <a title="5-lsi-6" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>7 0.33861825 <a title="5-lsi-7" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>8 0.21310411 <a title="5-lsi-8" href="./jmlr-2007-Multi-Task_Learning_for_Classification_with_Dirichlet_Process_Priors.html">56 jmlr-2007-Multi-Task Learning for Classification with Dirichlet Process Priors</a></p>
<p>9 0.20878568 <a title="5-lsi-9" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>10 0.20768578 <a title="5-lsi-10" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>11 0.19502525 <a title="5-lsi-11" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>12 0.18536125 <a title="5-lsi-12" href="./jmlr-2007-Dynamics_and_Generalization_Ability_of_LVQ_Algorithms.html">30 jmlr-2007-Dynamics and Generalization Ability of LVQ Algorithms</a></p>
<p>13 0.18300515 <a title="5-lsi-13" href="./jmlr-2007-Local_Discriminant_Wavelet_Packet_Coordinates_for_Face_Recognition.html">50 jmlr-2007-Local Discriminant Wavelet Packet Coordinates for Face Recognition</a></p>
<p>14 0.17358135 <a title="5-lsi-14" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>15 0.15441437 <a title="5-lsi-15" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>16 0.15381956 <a title="5-lsi-16" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>17 0.1530063 <a title="5-lsi-17" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>18 0.14596853 <a title="5-lsi-18" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>19 0.13873351 <a title="5-lsi-19" href="./jmlr-2007-Loop_Corrections_for_Approximate_Inference_on_Factor_Graphs.html">51 jmlr-2007-Loop Corrections for Approximate Inference on Factor Graphs</a></p>
<p>20 0.13187692 <a title="5-lsi-20" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.017), (7, 0.011), (8, 0.092), (10, 0.047), (12, 0.03), (15, 0.042), (22, 0.044), (28, 0.056), (39, 0.271), (40, 0.031), (45, 0.015), (48, 0.052), (60, 0.034), (80, 0.021), (85, 0.053), (98, 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73809195 <a title="5-lda-1" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>2 0.48367858 <a title="5-lda-2" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>Author: Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee, Robert L. Wolpert</p><p>Abstract: Kernel methods have been very popular in the machine learning literature in the last ten years, mainly in the context of Tikhonov regularization algorithms. In this paper we study a coherent Bayesian kernel model based on an integral operator deﬁned as the convolution of a kernel with a signed measure. Priors on the random signed measures correspond to prior distributions on the functions mapped by the integral operator. We study several classes of signed measures and their image mapped by the integral operator. In particular, we identify a general class of measures whose image is dense in the reproducing kernel Hilbert space (RKHS) induced by the kernel. A consequence of this result is a function theoretic foundation for using non-parametric prior speciﬁcations in Bayesian modeling, such as Gaussian process and Dirichlet process prior distributions. We discuss the construction of priors on spaces of signed measures using Gaussian and L´ vy processes, e with the Dirichlet processes being a special case the latter. Computational issues involved with sampling from the posterior distribution are outlined for a univariate regression and a high dimensional classiﬁcation problem. Keywords: reproducing kernel Hilbert space, non-parametric Bayesian methods, L´ vy processes, e Dirichlet processes, integral operator, Gaussian processes c 2007 Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee and Robert L. Wolpert. P ILLAI , W U , L IANG , M UKHERJEE AND W OLPERT</p><p>3 0.4750458 <a title="5-lda-3" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>Author: Sanjoy Dasgupta, Leonard Schulman</p><p>Abstract: We show that, given data from a mixture of k well-separated spherical Gaussians in R d , a simple two-round variant of EM will, with high probability, learn the parameters of the Gaussians to nearoptimal precision, if the dimension is high (d ln k). We relate this to previous theoretical and empirical work on the EM algorithm. Keywords: expectation maximization, mixtures of Gaussians, clustering, unsupervised learning, probabilistic analysis</p><p>4 0.44996285 <a title="5-lda-4" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>Author: Wei Pan, Xiaotong Shen</p><p>Abstract: Variable selection in clustering analysis is both challenging and important. In the context of modelbased clustering analysis with a common diagonal covariance matrix, which is especially suitable for “high dimension, low sample size” settings, we propose a penalized likelihood approach with an L1 penalty function, automatically realizing variable selection via thresholding and delivering a sparse solution. We derive an EM algorithm to ﬁt our proposed model, and propose a modiﬁed BIC as a model selection criterion to choose the number of components and the penalization parameter. A simulation study and an application to gene function prediction with gene expression proﬁles demonstrate the utility of our method. Keywords: BIC, EM, mixture model, penalized likelihood, soft-thresholding, shrinkage</p><p>5 0.44884136 <a title="5-lda-5" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis. Keywords: embedding algorithms, manifold learning, exponential families, multidimensional scaling, matrix factorization, semideﬁnite programming</p><p>6 0.4449873 <a title="5-lda-6" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>7 0.44489977 <a title="5-lda-7" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>8 0.4367345 <a title="5-lda-8" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>9 0.43546617 <a title="5-lda-9" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>10 0.43502757 <a title="5-lda-10" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>11 0.42505509 <a title="5-lda-11" href="./jmlr-2007-Handling_Missing_Values_when_Applying_Classification_Models.html">39 jmlr-2007-Handling Missing Values when Applying Classification Models</a></p>
<p>12 0.4228152 <a title="5-lda-12" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>13 0.4218016 <a title="5-lda-13" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>14 0.42124364 <a title="5-lda-14" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>15 0.41876778 <a title="5-lda-15" href="./jmlr-2007-Bayesian_Quadratic_Discriminant_Analysis.html">13 jmlr-2007-Bayesian Quadratic Discriminant Analysis</a></p>
<p>16 0.41725791 <a title="5-lda-16" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>17 0.41692716 <a title="5-lda-17" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>18 0.41593832 <a title="5-lda-18" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>19 0.41541362 <a title="5-lda-19" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>20 0.4147917 <a title="5-lda-20" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
