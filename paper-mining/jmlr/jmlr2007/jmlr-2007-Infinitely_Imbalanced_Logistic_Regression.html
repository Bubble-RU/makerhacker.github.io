<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 jmlr-2007-Infinitely Imbalanced Logistic Regression</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-42" href="#">jmlr2007-42</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>42 jmlr-2007-Infinitely Imbalanced Logistic Regression</h1>
<br/><p>Source: <a title="jmlr-2007-42-pdf" href="http://jmlr.org/papers/volume8/owen07a/owen07a.pdf">pdf</a></p><p>Author: Art B. Owen</p><p>Abstract: In binary classiﬁcation problems it is common for the two classes to be imbalanced: one case is very rare compared to the other. In this paper we consider the inﬁnitely imbalanced case where one class has a ﬁnite sample size and the other class’s sample size grows without bound. For logistic regression, the inﬁnitely imbalanced case often has a useful solution. Under mild conditions, the intercept diverges as expected, but the rest of the coefﬁcient vector approaches a non trivial and useful limit. That limit can be expressed in terms of exponential tilting and is the minimum of a convex objective function. The limiting form of logistic regression suggests a computational shortcut for fraud detection problems. Keywords: classiﬁcation, drug discovery, fraud detection, rare events, unbalanced data</p><p>Reference: <a title="jmlr-2007-42-reference" href="../jmlr2007_reference/jmlr-2007-Infinitely_Imbalanced_Logistic_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 EDU  Department of Statistics Stanford University Stanford CA, 94305, USA  Editor: Yi Lin  Abstract In binary classiﬁcation problems it is common for the two classes to be imbalanced: one case is very rare compared to the other. [sent-4, score-0.171]
</p><p>2 In this paper we consider the inﬁnitely imbalanced case where one class has a ﬁnite sample size and the other class’s sample size grows without bound. [sent-5, score-0.411]
</p><p>3 For logistic regression, the inﬁnitely imbalanced case often has a useful solution. [sent-6, score-0.739]
</p><p>4 Under mild conditions, the intercept diverges as expected, but the rest of the coefﬁcient vector approaches a non trivial and useful limit. [sent-7, score-0.259]
</p><p>5 That limit can be expressed in terms of exponential tilting and is the minimum of a convex objective function. [sent-8, score-0.168]
</p><p>6 The limiting form of logistic regression suggests a computational shortcut for fraud detection problems. [sent-9, score-0.76]
</p><p>7 Keywords: classiﬁcation, drug discovery, fraud detection, rare events, unbalanced data  1. [sent-10, score-0.45]
</p><p>8 Introduction In many applications of logistic regression one of the two classes is extremely rare. [sent-11, score-0.439]
</p><p>9 In political science, the occurrence of wars, coups, vetos and the decisions of citizens to run for ofﬁce have been modelled as rare events; see King and Zeng (2001). [sent-12, score-0.209]
</p><p>10 Bolton and Hand (2002) consider fraud detection, and Zhu et al. [sent-13, score-0.189]
</p><p>11 In other examples the rare event might correspond to people with a rare disease, customer conversions at an e-commerce web site, or false positives among a set of emails marked as spam. [sent-15, score-0.342]
</p><p>12 We will let Y ∈ {0, 1} denote a random response with the observed value of Y being y = 1 in the rare case and y = 0 in the common case. [sent-16, score-0.2]
</p><p>13 We will suppose that the number of observations with y = 0 is so large that we have a satisfactory representation of the distribution of predictors in that setting. [sent-17, score-0.143]
</p><p>14 It is no surprise that the intercept term in the logistic regression typically tends to −∞ in this limit. [sent-19, score-0.651]
</p><p>15 The main result (Theorem 8 below) is that under reasonable conditions, the intercept term tends to −∞ like − log(N) plus a constant, while the limiting logistic regression coefﬁcient β = β(N) satisﬁes R xβ e x dF0 (x) x= R xβ ¯  e  c 2007 Art B. [sent-21, score-0.731]
</p><p>16 ¯ 0  N→∞  (2)  Equation (2) reminds one of a well known derivation for logistic regression. [sent-26, score-0.328]
</p><p>17 If the conditional distribution of X given that Y = y is N(µy , Σ) then the coefﬁcient of X in logistic regression is Σ−1 (µ1 − µ0 ). [sent-27, score-0.439]
</p><p>18 It outlines the results of Silvapulle (1981) who completely characterizes the conditions under which unique logistic regression estimates exist in the ﬁnite sample case. [sent-36, score-0.439]
</p><p>19 (2005), we can ¯ replace all data points with y = 1 by a single one at (x, 1) with minimal effect on the estimated ¯ coefﬁcient, apart from the intercept term. [sent-43, score-0.241]
</p><p>20 Section 6 discusses how these results can be used in deciding which unlabelled data points to label, and it shows how the inﬁnitely imbalanced setting may lead to computational savings. [sent-44, score-0.458]
</p><p>21 We conclude this introduction by relating the present work to the literature on imbalanced data. [sent-45, score-0.411]
</p><p>22 To study this case we use logistic regression on (x i , yi ) = (Φ−1 ((i − 1/2)/N), 0) for i = 1, . [sent-56, score-0.439]
</p><p>23 As N increases the problem becomes more imbalanced and the N points used produce an ever better approximation to the normal distribution. [sent-61, score-0.411]
</p><p>24 0003  Table 1: Logistic regression intercept α and coefﬁcient β for imbalanced data described in the text. [sent-77, score-0.734]
</p><p>25 From this table it seems clear that as N → ∞, the intercept term is diverging like − log(N) while the coefﬁcient of X is approaching the value 1 that we would get from Equation (2). [sent-81, score-0.254]
</p><p>26 The Cauchy distribution has tails far heavier than the logistic distribution. [sent-87, score-0.423]
</p><p>27 The heavy tails of the Cauchy distribution make it fail a condition of Theorem 8. [sent-91, score-0.171]
</p><p>28 We need the point x to be surrounded ¯ by the distribution of X given Y = 0 as deﬁned in Section 3. [sent-102, score-0.141]
</p><p>29 9513  Table 2: Logistic regression intercept α and coefﬁcient β for imbalanced data described in the text. [sent-123, score-0.734]
</p><p>30 14  Table 3: Logistic regression intercept α and coefﬁcient β for imbalanced data described in the text. [sent-145, score-0.734]
</p><p>31 The logistic regression model is Pr(Y = 1 | X = x) = eα+x β /(1 + eα+x β ) for α ∈ R and β ∈ Rd . [sent-163, score-0.439]
</p><p>32 The log-likelihood in logistic regression is n  ∑  i=1  N  α + x1i β − log(1 + eα+x1i β ) − ∑ log(1 + eα+x0i β ). [sent-164, score-0.439]
</p><p>33 With a bit of foresight we also center the logistic regression around the average x = ∑n xi /n of ¯ i=1 the predictor values for cases with Y = 1. [sent-168, score-0.587]
</p><p>34 ¯ i=1 ˆ ˆ When the centered log likelihood has an MLE (α0 , β) we can recover the MLE of the uncenˆ remains unchanged while α in the uncentered version is α0 − x β. [sent-170, score-0.17]
</p><p>35 The ˆ ˆ tered log likelihood easily: β ¯ˆ numerical examples in Section 2 used uncentered logistic regression. [sent-171, score-0.498]
</p><p>36 1 Silvapulle’s Results It is well known that the MLE in the usual logistic regression setting can fail to be ﬁnite when the x values where y = 1 are linearly separable from those where y = 0. [sent-181, score-0.468]
</p><p>37 The existence and uniqueness of MLE’s for linear logistic regression has been completely characterized by Silvapulle (1981). [sent-182, score-0.439]
</p><p>38 Then the logistic regression model has Pr(Y = 1 | X = x) = exp(z θ)/(1 + exp(z θ)) where of course z = z(x) = (1, x ) . [sent-193, score-0.439]
</p><p>39 Silvapulle (1981) employs two convex cones: nj  Cj =  ∑ k ji z ji | k ji > 0  ,  j ∈ {0, 1}. [sent-194, score-0.596]
</p><p>40 i=1  Theorem 1 For data as described above, assume that the n 0 + n1 by d + 1 matrix with rows taken / from z ji for j = 0, 1 and i = 1, . [sent-195, score-0.173]
</p><p>41 If C0 ∩ C1 = 0 then a unique ﬁnite logistic ˆ ˆ = (α, β ) exists. [sent-199, score-0.328]
</p><p>42 Theorem 1 also holds when the logistic CDF G(t) = exp(t)/(1 + exp(t)) is replaced by the standard normal one (for probit analysis) or by the U(0, 1) CDF. [sent-203, score-0.328]
</p><p>43 A more readily interpretable condition is that the relative interior (as explained below) of the convex hull of the x’s for y = 0 intersects that for y = 1. [sent-207, score-0.211]
</p><p>44 / That is H0 ∩ H1 = 0 where nj  nj  i=1  Hj =  i=1  ∑ λ ji x ji | λ ji > 0, ∑ λ ji = 1 . [sent-208, score-0.75]
</p><p>45 When the x ji span Rd then H j is the interior of the convex hull of x ji . [sent-209, score-0.515]
</p><p>46 When x ji lie in a lower dimensional afﬁne subspace of Rd then the interior of their convex hull is the empty set. [sent-210, score-0.342]
</p><p>47 , n j , then the desired relative interior of the convex hull of x j1 , . [sent-215, score-0.169]
</p><p>48 Then we may write n0  z0 = ∑ k0i i=1  1 x0i  n1  = ∑ k1i i=1  1 , x1i  where each k ji > 0. [sent-223, score-0.173]
</p><p>49 Let K denote that value, and put λ ji = k ji /K for j = 0, 1 and i = 1, . [sent-225, score-0.346]
</p><p>50 Deﬁnition 3 The distribution F on Rd has the point x∗ surrounded if Z  (x−x∗ ) ω> ε  dF(x) > δ  (5)  holds for some ε > 0, some δ > 0 and all ω ∈ Ω. [sent-237, score-0.141]
</p><p>51 This implies that having at least one point surrounded by F0 will be enough to avoid rank deﬁciency. [sent-242, score-0.18]
</p><p>52 In Theorem 1 it follows from Lemma 2 that we only need there to be some point x ∗ that is ˆ ˆ ˆ surrounded by both F0 and F1 where Fj is the empirical distribution of x j1 , . [sent-243, score-0.141]
</p><p>53 ) In the inﬁnitely imbalanced setting we expect that F0 will ordinarily surround every single one of x1 , . [sent-249, score-0.584]
</p><p>54 We do not need F0 to surround them all but it is not enough to just have some point x ∗ ˆ exist that is surrounded by both F0 and F1 . [sent-253, score-0.258]
</p><p>55 We do not need ¯ ˆ to assume that F1 surrounds x, a condition that fails when the xi are conﬁned to an afﬁne subset of ¯ Rd as they necessarily are for n < d. [sent-255, score-0.19]
</p><p>56 There is an interesting case in which F0 can fail to surround x. [sent-256, score-0.146]
</p><p>57 The predictor X may contain ¯ a component that is itself an imbalanced binary variable, and that component might never take the value 1 in the y = 1 sample. [sent-257, score-0.489]
</p><p>58 Then x is right on the boundary of the support of F0 and we cannot be ¯ sure of a ﬁnite β in either the ﬁnite sample case or the inﬁnitely imbalanced case. [sent-258, score-0.411]
</p><p>59 , xn ∈ Rd be given, and assume that the distribution F0 surrounds x = ∑n xi /n and that 0 < N < ∞. [sent-269, score-0.19]
</p><p>60 Then the log likelihood (α, β) given by (4) has a unique ﬁnite ¯ i=1 ˆ ˆ maximizer (α, β). [sent-270, score-0.185]
</p><p>61 Proof: The log likelihood is strictly concave in (α, β). [sent-271, score-0.142]
</p><p>62 As illustrated in Section 2, inﬁnitely imbalanced logistic regression will be degenerate if F0 has tails that are too heavy. [sent-303, score-0.945]
</p><p>63 , xn ∈ Rd be ﬁxed and suppose that F0 satisﬁes the tail condiˆ ˆ tion (11) and surrounds x = ∑n xi /n as described at (5). [sent-309, score-0.268]
</p><p>64 413  Table 4: This table shows logistic regression coefﬁcients for the chemical compound data set described in the text. [sent-340, score-0.472]
</p><p>65 The fourth row shows standard errors for the ordinary logistic regression ¯ coefﬁcients in the top row. [sent-344, score-0.439]
</p><p>66 Therefore the MLEs satisfy R  lim R  N→∞  ˆ ¯ x ex β [1 + eα+(x−x) β ]−1 dF0 (x) ˆ  ˆ  ˆ ¯ ex β [1 + eα+(x−x) β ]−1 dF0 (x) ˆ  ˆ  = x. [sent-346, score-0.25]
</p><p>67 ¯  (13)  The denominator of (13) is at most ex β dF0 (x) and is at least Z  R  ˆ  ˆ ¯ ex β (1 − eα+(x−x) β )dF0 (x) → ˆ  ˆ  Z  ex β dF0 (x) ˆ  as N → ∞ because α → −∞ and e2x β dF0 (x) < ∞ by the tail condition (11). [sent-347, score-0.316]
</p><p>68 Therefore the denomR ˆ inator of (13) has the same limit as ex β dF0 (x) as N → ∞. [sent-348, score-0.143]
</p><p>69 Similarly the numerator has the same R ˆ limit as ex β x dF0 (x). [sent-349, score-0.143]
</p><p>70 Illustration It is perhaps surprising that in the N → ∞ limit, the logistic regression depends on x 1 , . [sent-352, score-0.47]
</p><p>71 They study a data set with 29,812 chemical compounds on which 6 predictor variables were measured. [sent-360, score-0.174]
</p><p>72 Table 4 shows the logistic regression coefﬁcients for this data, as well as what happens to them when we replace the 608 data points (x, y) with y = 1 by a single point at (x, 1), or by 608 points ¯ equal to (x, 1). [sent-362, score-0.468]
</p><p>73 In a centered logistic regression the point (x, 1) becomes (x − x, 1) = (0, . [sent-363, score-0.439]
</p><p>74 The intercept changes a lot when we reduce the rare cases from 608 to 1 but otherwise the coefﬁcients do not change importantly. [sent-367, score-0.414]
</p><p>75 Interestingly the single point version has a β vector closer 769  OWEN  to the original logistic regression than has the version with 608 points at (x, 1). [sent-368, score-0.439]
</p><p>76 ) The correlation between the linear ¯ predictor from logistic regression to that ﬁt with all xi = x is 0. [sent-372, score-0.556]
</p><p>77 The correlation between the ¯ linear predictor from logistic regression to that ﬁt with just one (x, 1) data point is still higher, at ¯ 0. [sent-374, score-0.517]
</p><p>78 1 shows these ﬁndings lead to greater understanding of how logistic regression works or fails and how to improve it. [sent-384, score-0.479]
</p><p>79 4 describes how using inﬁnitely imbalanced logistic regression may lead to cost savings in fraud detection settings. [sent-390, score-1.124]
</p><p>80 1 Insight Into Logistic Regression In the inﬁnitely imbalanced limit, logistic regression only uses the y = 1 data points through their average feature vector x. [sent-392, score-0.85]
</p><p>81 This limiting behavior is a property of logistic regression, not of any ¯ particular data set. [sent-393, score-0.408]
</p><p>82 It holds equally well in those problems for which logistic regression works badly as it does in problems where the Bayes rule is a logistic regression. [sent-394, score-0.767]
</p><p>83 In the illustrative example we got almost the same logistic regression after replacing all the rare cases by a single point at x. [sent-395, score-0.641]
</p><p>84 But knowing that those parameters are very strongly tied to the d components of x gives us insight into how logistic regression works on ¯ imbalanced problems. [sent-400, score-0.879]
</p><p>85 It is reasonable to expect better results from logistic regression when the x 1i are in a single tight cluster near x than when there are outliers, or when the x 1i points are in two well ¯ separated clusters in different directions from the bulk of F0 . [sent-401, score-0.469]
</p><p>86 When we detect sharp ¯ clusters among x1i then we might ﬁt one logistic regression per cluster, separating that cluster from the x0i ’s, and predict for new points by pooling the cluster speciﬁc results. [sent-404, score-0.499]
</p><p>87 2 Nontrivial Limiting Predictions ˆ In the inﬁnitely imbalanced limit with N → ∞ we often ﬁnd that β converges to a ﬁnite limit while ˆ α → −∞. [sent-407, score-0.511]
</p><p>88 Pr(Y = 1 | X = x) For example if we are presented with a number of cases of potential fraud to investigate and have limited resources then we can rank them by x β and investigate as many of the most likely ones as time or other costs allow. [sent-410, score-0.362]
</p><p>89 If the values of uncovering fraud in the two cases are v and v, respectively, then we might prefer to investigate the former when ve x β > vex β . [sent-412, score-0.335]
</p><p>90 If the costs of investigation are c and c then we might prefer the former when vex β /c > vex β /c. [sent-413, score-0.205]
</p><p>91 In the selective setting, the investigator has a mix of labelled cases (both x and y known) and unlabelled cases (x known but y unknown), and must choose which of the unlabelled cases to get a label for. [sent-419, score-0.187]
</p><p>92 In a rare event setting, ﬁnding the cases most likely to have y = 1 is a reasonable proxy for ﬁnding the most informative cases, and one could then allocate a large part of the labelling budget to cases with high values of x β. [sent-421, score-0.233]
</p><p>93 The effective sample size of an imbalanced data set is often considered to be simply the number of rare outcomes. [sent-424, score-0.582]
</p><p>94 If at least one of the Σk has full rank then F0 will surround the point where λk > 0 and x. [sent-432, score-0.156]
</p><p>95 By contrast, each step in iteratively reweighted least squares ﬁtting of logistic regression takes O((n + N)d 2 ) work. [sent-444, score-0.439]
</p><p>96 But after the ﬁrst iteration there can be substantial computational savings for solving (15) instead of doing logistic regression, when n/d is large. [sent-447, score-0.361]
</p><p>97 When there is one common class and there are numerous rare classes, such as types of fraud or different targets against which a drug might be active, then the cost of approximating F0 can be shared over the set of uncommon classes. [sent-448, score-0.423]
</p><p>98 In fraud detection problems we might expect that the distribution F0 for legitimate data points is slowly changing while the patterns in the fraudulent points change rapidly in response to improved detection. [sent-449, score-0.27]
</p><p>99 These vectors can be for different known types of fraud, for fraud over shorter time intervals, or even individual fraud cases. [sent-451, score-0.378]
</p><p>100 Editorial: special issue on learning from imbalanced data sets. [sent-485, score-0.411]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('imbalanced', 0.411), ('logistic', 0.328), ('intercept', 0.212), ('silvapulle', 0.194), ('fraud', 0.189), ('owen', 0.188), ('ji', 0.173), ('rare', 0.171), ('nfinitely', 0.167), ('cauchy', 0.154), ('mle', 0.147), ('rd', 0.144), ('surrounded', 0.141), ('mbalanced', 0.141), ('cdf', 0.125), ('surround', 0.117), ('surrounds', 0.111), ('regression', 0.111), ('egression', 0.107), ('ogistic', 0.107), ('nitely', 0.107), ('tails', 0.095), ('ex', 0.093), ('coef', 0.086), ('vex', 0.083), ('limiting', 0.08), ('predictor', 0.078), ('log', 0.075), ('maximizer', 0.071), ('df', 0.07), ('tilting', 0.07), ('interior', 0.069), ('chawla', 0.067), ('strati', 0.067), ('lim', 0.064), ('drug', 0.063), ('japkowicz', 0.063), ('compounds', 0.063), ('lemma', 0.06), ('nontrivial', 0.06), ('bolton', 0.056), ('ordinarily', 0.056), ('uncentered', 0.056), ('detection', 0.052), ('predictors', 0.052), ('hull', 0.052), ('cients', 0.051), ('active', 0.051), ('observations', 0.05), ('limit', 0.05), ('overlap', 0.05), ('pr', 0.05), ('convex', 0.048), ('heavy', 0.047), ('unlabelled', 0.047), ('zh', 0.047), ('diverges', 0.047), ('supn', 0.047), ('ze', 0.047), ('stanford', 0.043), ('nonsingular', 0.042), ('tilt', 0.042), ('intersects', 0.042), ('approaching', 0.042), ('suppose', 0.041), ('xn', 0.04), ('fails', 0.04), ('zhu', 0.04), ('likelihood', 0.039), ('xi', 0.039), ('rank', 0.039), ('costs', 0.039), ('political', 0.038), ('jn', 0.038), ('tail', 0.037), ('nite', 0.034), ('chemical', 0.033), ('ray', 0.033), ('savings', 0.033), ('cohn', 0.033), ('king', 0.033), ('investigate', 0.032), ('events', 0.032), ('af', 0.031), ('surprising', 0.031), ('cases', 0.031), ('cluster', 0.03), ('mixture', 0.03), ('replace', 0.029), ('response', 0.029), ('insight', 0.029), ('nj', 0.029), ('fail', 0.029), ('concave', 0.028), ('gaussians', 0.028), ('stronger', 0.028), ('establishing', 0.027), ('unbalanced', 0.027), ('theorem', 0.027), ('aaai', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="42-tfidf-1" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>Author: Art B. Owen</p><p>Abstract: In binary classiﬁcation problems it is common for the two classes to be imbalanced: one case is very rare compared to the other. In this paper we consider the inﬁnitely imbalanced case where one class has a ﬁnite sample size and the other class’s sample size grows without bound. For logistic regression, the inﬁnitely imbalanced case often has a useful solution. Under mild conditions, the intercept diverges as expected, but the rest of the coefﬁcient vector approaches a non trivial and useful limit. That limit can be expressed in terms of exponential tilting and is the minimum of a convex objective function. The limiting form of logistic regression suggests a computational shortcut for fraud detection problems. Keywords: classiﬁcation, drug discovery, fraud detection, rare events, unbalanced data</p><p>2 0.13924295 <a title="42-tfidf-2" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>Author: Ping Li, Trevor J. Hastie, Kenneth W. Church</p><p>Abstract: For1 dimension reduction in the l1 norm, the method of Cauchy random projections multiplies the original data matrix A ∈ Rn×D with a random matrix R ∈ RD×k (k D) whose entries are i.i.d. samples of the standard Cauchy C(0, 1). Because of the impossibility result, one can not hope to recover the pairwise l1 distances in A from B = A × R ∈ Rn×k , using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O log n sufﬁces with the constants explicitly ε2 given. Asymptotically (as k → ∞), both the sample median and the geometric mean estimators are about 80% efﬁcient compared to the MLE. We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian. Keywords: dimension reduction, l1 norm, Johnson-Lindenstrauss (JL) lemma, Cauchy random projections</p><p>3 0.10593387 <a title="42-tfidf-3" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Kwangmoo Koh, Seung-Jean Kim, Stephen Boyd</p><p>Abstract: Logistic regression with 1 regularization has been proposed as a promising method for feature selection in classiﬁcation problems. In this paper we describe an efﬁcient interior-point method for solving large-scale 1 -regularized logistic regression problems. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC; medium sized problems, with tens of thousands of features and examples, can be solved in tens of seconds (assuming some sparsity in the data). A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve very large problems, with a million features and examples (e.g., the 20 Newsgroups data set), in a few minutes, on a PC. Using warm-start techniques, a good approximation of the entire regularization path can be computed much more efﬁciently than by solving a family of problems independently. Keywords: logistic regression, feature selection, 1 regularization, regularization path, interiorpoint methods.</p><p>4 0.080679722 <a title="42-tfidf-4" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>Author: Carine Hue, Marc Boullé</p><p>Abstract: In this paper, we consider the supervised learning task which consists in predicting the normalized rank of a numerical variable. We introduce a novel probabilistic approach to estimate the posterior distribution of the target rank conditionally to the predictors. We turn this learning task into a model selection problem. For that, we deﬁne a 2D partitioning family obtained by discretizing numerical variables and grouping categorical ones and we derive an analytical criterion to select the partition with the highest posterior probability. We show how these partitions can be used to build univariate predictors and multivariate ones under a naive Bayes assumption. We also propose a new evaluation criterion for probabilistic rank estimators. Based on the logarithmic score, we show that such criterion presents the advantage to be minored, which is not the case of the logarithmic score computed for probabilistic value estimator. A ﬁrst set of experimentations on synthetic data shows the good properties of the proposed criterion and of our partitioning approach. A second set of experimentations on real data shows competitive performance of the univariate and selective naive Bayes rank estimators projected on the value range compared to methods submitted to a recent challenge on probabilistic metric regression tasks. Our approach is applicable for all regression problems with categorical or numerical predictors. It is particularly interesting for those with a high number of predictors as it automatically detects the variables which contain predictive information. It builds pertinent predictors of the normalized rank of the numerical target from one or several predictors. As the criteria selection is regularized by the presence of a prior and a posterior term, it does not suffer from overﬁtting. Keywords: rank regression, probabilistic approach, 2D partitioning, non parametric estimation, Bayesian model selection</p><p>5 0.062225182 <a title="42-tfidf-5" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>Author: David Mease, Abraham J. Wyner, Andreas Buja</p><p>Abstract: The standard by which binary classiﬁers are usually judged, misclassiﬁcation error, assumes equal costs of misclassifying the two classes or, equivalently, classifying at the 1/2 quantile of the conditional class probability function P[y = 1|x]. Boosted classiﬁcation trees are known to perform quite well for such problems. In this article we consider the use of standard, off-the-shelf boosting for two more general problems: 1) classiﬁcation with unequal costs or, equivalently, classiﬁcation at quantiles other than 1/2, and 2) estimation of the conditional class probability function P[y = 1|x]. We ﬁrst examine whether the latter problem, estimation of P[y = 1|x], can be solved with LogitBoost, and with AdaBoost when combined with a natural link function. The answer is negative: both approaches are often ineffective because they overﬁt P[y = 1|x] even though they perform well as classiﬁers. A major negative point of the present article is the disconnect between class probability estimation and classiﬁcation. Next we consider the practice of over/under-sampling of the two classes. We present an algorithm that uses AdaBoost in conjunction with Over/Under-Sampling and Jittering of the data (“JOUS-Boost”). This algorithm is simple, yet successful, and it preserves the advantage of relative protection against overﬁtting, but for arbitrary misclassiﬁcation costs and, equivalently, arbitrary quantile boundaries. We then use collections of classiﬁers obtained from a grid of quantiles to form estimators of class probabilities. The estimates of the class probabilities compare favorably to those obtained by a variety of methods across both simulated and real data sets. Keywords: boosting algorithms, LogitBoost, AdaBoost, class probability estimation, over-sampling, under-sampling, stratiﬁcation, data jittering</p><p>6 0.048543632 <a title="42-tfidf-6" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>7 0.039742243 <a title="42-tfidf-7" href="./jmlr-2007-Very_Fast_Online_Learning_of_Highly_Non_Linear_Problems.html">91 jmlr-2007-Very Fast Online Learning of Highly Non Linear Problems</a></p>
<p>8 0.038269389 <a title="42-tfidf-8" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>9 0.037738729 <a title="42-tfidf-9" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>10 0.037145335 <a title="42-tfidf-10" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>11 0.036320604 <a title="42-tfidf-11" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>12 0.036125384 <a title="42-tfidf-12" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>13 0.035752892 <a title="42-tfidf-13" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>14 0.034845993 <a title="42-tfidf-14" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>15 0.034647718 <a title="42-tfidf-15" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>16 0.03278726 <a title="42-tfidf-16" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>17 0.03247859 <a title="42-tfidf-17" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>18 0.032156743 <a title="42-tfidf-18" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>19 0.031839054 <a title="42-tfidf-19" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>20 0.031797666 <a title="42-tfidf-20" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.211), (1, -0.024), (2, 0.021), (3, 0.033), (4, 0.078), (5, -0.041), (6, -0.084), (7, -0.004), (8, 0.092), (9, -0.103), (10, -0.073), (11, -0.142), (12, -0.044), (13, -0.093), (14, 0.336), (15, 0.334), (16, 0.2), (17, -0.002), (18, -0.017), (19, -0.177), (20, -0.126), (21, -0.038), (22, 0.071), (23, 0.036), (24, -0.121), (25, 0.104), (26, 0.211), (27, 0.033), (28, -0.045), (29, 0.045), (30, 0.018), (31, -0.152), (32, -0.034), (33, 0.004), (34, 0.076), (35, 0.035), (36, -0.008), (37, 0.044), (38, 0.127), (39, 0.035), (40, -0.037), (41, 0.099), (42, 0.105), (43, -0.117), (44, 0.096), (45, -0.043), (46, 0.098), (47, -0.025), (48, -0.034), (49, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96775538 <a title="42-lsi-1" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>Author: Art B. Owen</p><p>Abstract: In binary classiﬁcation problems it is common for the two classes to be imbalanced: one case is very rare compared to the other. In this paper we consider the inﬁnitely imbalanced case where one class has a ﬁnite sample size and the other class’s sample size grows without bound. For logistic regression, the inﬁnitely imbalanced case often has a useful solution. Under mild conditions, the intercept diverges as expected, but the rest of the coefﬁcient vector approaches a non trivial and useful limit. That limit can be expressed in terms of exponential tilting and is the minimum of a convex objective function. The limiting form of logistic regression suggests a computational shortcut for fraud detection problems. Keywords: classiﬁcation, drug discovery, fraud detection, rare events, unbalanced data</p><p>2 0.67107916 <a title="42-lsi-2" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>Author: Ping Li, Trevor J. Hastie, Kenneth W. Church</p><p>Abstract: For1 dimension reduction in the l1 norm, the method of Cauchy random projections multiplies the original data matrix A ∈ Rn×D with a random matrix R ∈ RD×k (k D) whose entries are i.i.d. samples of the standard Cauchy C(0, 1). Because of the impossibility result, one can not hope to recover the pairwise l1 distances in A from B = A × R ∈ Rn×k , using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O log n sufﬁces with the constants explicitly ε2 given. Asymptotically (as k → ∞), both the sample median and the geometric mean estimators are about 80% efﬁcient compared to the MLE. We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian. Keywords: dimension reduction, l1 norm, Johnson-Lindenstrauss (JL) lemma, Cauchy random projections</p><p>3 0.5262785 <a title="42-lsi-3" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Kwangmoo Koh, Seung-Jean Kim, Stephen Boyd</p><p>Abstract: Logistic regression with 1 regularization has been proposed as a promising method for feature selection in classiﬁcation problems. In this paper we describe an efﬁcient interior-point method for solving large-scale 1 -regularized logistic regression problems. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC; medium sized problems, with tens of thousands of features and examples, can be solved in tens of seconds (assuming some sparsity in the data). A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve very large problems, with a million features and examples (e.g., the 20 Newsgroups data set), in a few minutes, on a PC. Using warm-start techniques, a good approximation of the entire regularization path can be computed much more efﬁciently than by solving a family of problems independently. Keywords: logistic regression, feature selection, 1 regularization, regularization path, interiorpoint methods.</p><p>4 0.37448671 <a title="42-lsi-4" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>Author: Carine Hue, Marc Boullé</p><p>Abstract: In this paper, we consider the supervised learning task which consists in predicting the normalized rank of a numerical variable. We introduce a novel probabilistic approach to estimate the posterior distribution of the target rank conditionally to the predictors. We turn this learning task into a model selection problem. For that, we deﬁne a 2D partitioning family obtained by discretizing numerical variables and grouping categorical ones and we derive an analytical criterion to select the partition with the highest posterior probability. We show how these partitions can be used to build univariate predictors and multivariate ones under a naive Bayes assumption. We also propose a new evaluation criterion for probabilistic rank estimators. Based on the logarithmic score, we show that such criterion presents the advantage to be minored, which is not the case of the logarithmic score computed for probabilistic value estimator. A ﬁrst set of experimentations on synthetic data shows the good properties of the proposed criterion and of our partitioning approach. A second set of experimentations on real data shows competitive performance of the univariate and selective naive Bayes rank estimators projected on the value range compared to methods submitted to a recent challenge on probabilistic metric regression tasks. Our approach is applicable for all regression problems with categorical or numerical predictors. It is particularly interesting for those with a high number of predictors as it automatically detects the variables which contain predictive information. It builds pertinent predictors of the normalized rank of the numerical target from one or several predictors. As the criteria selection is regularized by the presence of a prior and a posterior term, it does not suffer from overﬁtting. Keywords: rank regression, probabilistic approach, 2D partitioning, non parametric estimation, Bayesian model selection</p><p>5 0.27923504 <a title="42-lsi-5" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>Author: David Mease, Abraham J. Wyner, Andreas Buja</p><p>Abstract: The standard by which binary classiﬁers are usually judged, misclassiﬁcation error, assumes equal costs of misclassifying the two classes or, equivalently, classifying at the 1/2 quantile of the conditional class probability function P[y = 1|x]. Boosted classiﬁcation trees are known to perform quite well for such problems. In this article we consider the use of standard, off-the-shelf boosting for two more general problems: 1) classiﬁcation with unequal costs or, equivalently, classiﬁcation at quantiles other than 1/2, and 2) estimation of the conditional class probability function P[y = 1|x]. We ﬁrst examine whether the latter problem, estimation of P[y = 1|x], can be solved with LogitBoost, and with AdaBoost when combined with a natural link function. The answer is negative: both approaches are often ineffective because they overﬁt P[y = 1|x] even though they perform well as classiﬁers. A major negative point of the present article is the disconnect between class probability estimation and classiﬁcation. Next we consider the practice of over/under-sampling of the two classes. We present an algorithm that uses AdaBoost in conjunction with Over/Under-Sampling and Jittering of the data (“JOUS-Boost”). This algorithm is simple, yet successful, and it preserves the advantage of relative protection against overﬁtting, but for arbitrary misclassiﬁcation costs and, equivalently, arbitrary quantile boundaries. We then use collections of classiﬁers obtained from a grid of quantiles to form estimators of class probabilities. The estimates of the class probabilities compare favorably to those obtained by a variety of methods across both simulated and real data sets. Keywords: boosting algorithms, LogitBoost, AdaBoost, class probability estimation, over-sampling, under-sampling, stratiﬁcation, data jittering</p><p>6 0.21424599 <a title="42-lsi-6" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>7 0.19370182 <a title="42-lsi-7" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>8 0.18731755 <a title="42-lsi-8" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>9 0.18136594 <a title="42-lsi-9" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>10 0.17866597 <a title="42-lsi-10" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>11 0.17012581 <a title="42-lsi-11" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>12 0.16866599 <a title="42-lsi-12" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>13 0.1638588 <a title="42-lsi-13" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>14 0.16290167 <a title="42-lsi-14" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>15 0.15726949 <a title="42-lsi-15" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>16 0.15405822 <a title="42-lsi-16" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>17 0.15191162 <a title="42-lsi-17" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>18 0.149114 <a title="42-lsi-18" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>19 0.14389245 <a title="42-lsi-19" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>20 0.14269908 <a title="42-lsi-20" href="./jmlr-2007-Covariate_Shift_Adaptation_by_Importance_Weighted_Cross_Validation.html">25 jmlr-2007-Covariate Shift Adaptation by Importance Weighted Cross Validation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.015), (8, 0.018), (10, 0.011), (12, 0.012), (28, 0.04), (40, 0.667), (48, 0.028), (60, 0.023), (80, 0.01), (85, 0.027), (98, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9596417 <a title="42-lda-1" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>Author: Art B. Owen</p><p>Abstract: In binary classiﬁcation problems it is common for the two classes to be imbalanced: one case is very rare compared to the other. In this paper we consider the inﬁnitely imbalanced case where one class has a ﬁnite sample size and the other class’s sample size grows without bound. For logistic regression, the inﬁnitely imbalanced case often has a useful solution. Under mild conditions, the intercept diverges as expected, but the rest of the coefﬁcient vector approaches a non trivial and useful limit. That limit can be expressed in terms of exponential tilting and is the minimum of a convex objective function. The limiting form of logistic regression suggests a computational shortcut for fraud detection problems. Keywords: classiﬁcation, drug discovery, fraud detection, rare events, unbalanced data</p><p>2 0.88355714 <a title="42-lda-2" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>Author: Ryan M. Rifkin, Ross A. Lippert</p><p>Abstract: Regularization is an approach to function learning that balances ﬁt and smoothness. In practice, we search for a function f with a ﬁnite representation f = ∑i ci φi (·). In most treatments, the ci are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning. Keywords: kernel machines, duality, optimization, convex analysis, kernel learning</p><p>3 0.51153135 <a title="42-lda-3" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>Author: Miroslav Dudík, Steven J. Phillips, Robert E. Schapire</p><p>Abstract: We present a uniﬁed and complete account of maximum entropy density estimation subject to constraints represented by convex potential functions or, alternatively, by convex regularization. We provide fully general performance guarantees and an algorithm with a complete convergence proof. As special cases, we easily derive performance guarantees for many known regularization types, including 1 , 2 , 2 , and 1 + 2 style regularization. We propose an algorithm solving a large and 2 2 general subclass of generalized maximum entropy problems, including all discussed in the paper, and prove its convergence. Our approach generalizes and uniﬁes techniques based on information geometry and Bregman divergences as well as those based more directly on compactness. Our work is motivated by a novel application of maximum entropy to species distribution modeling, an important problem in conservation biology and ecology. In a set of experiments on real-world data, we demonstrate the utility of maximum entropy in this setting. We explore effects of different feature types, sample sizes, and regularization levels on the performance of maxent, and discuss interpretability of the resulting models. Keywords: maximum entropy, density estimation, regularization, iterative scaling, species distribution modeling</p><p>4 0.45618618 <a title="42-lda-4" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Kwangmoo Koh, Seung-Jean Kim, Stephen Boyd</p><p>Abstract: Logistic regression with 1 regularization has been proposed as a promising method for feature selection in classiﬁcation problems. In this paper we describe an efﬁcient interior-point method for solving large-scale 1 -regularized logistic regression problems. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC; medium sized problems, with tens of thousands of features and examples, can be solved in tens of seconds (assuming some sparsity in the data). A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve very large problems, with a million features and examples (e.g., the 20 Newsgroups data set), in a few minutes, on a PC. Using warm-start techniques, a good approximation of the entire regularization path can be computed much more efﬁciently than by solving a family of problems independently. Keywords: logistic regression, feature selection, 1 regularization, regularization path, interiorpoint methods.</p><p>5 0.41034806 <a title="42-lda-5" href="./jmlr-2007-Stagewise_Lasso.html">77 jmlr-2007-Stagewise Lasso</a></p>
<p>Author: Peng Zhao, Bin Yu</p><p>Abstract: Many statistical machine learning algorithms minimize either an empirical loss function as in AdaBoost, or a penalized empirical loss as in Lasso or SVM. A single regularization tuning parameter controls the trade-off between ﬁdelity to the data and generalizability, or equivalently between bias and variance. When this tuning parameter changes, a regularization “path” of solutions to the minimization problem is generated, and the whole path is needed to select a tuning parameter to optimize the prediction or interpretation performance. Algorithms such as homotopy-Lasso or LARS-Lasso and Forward Stagewise Fitting (FSF) (aka e-Boosting) are of great interest because of their resulted sparse models for interpretation in addition to prediction. In this paper, we propose the BLasso algorithm that ties the FSF (e-Boosting) algorithm with the Lasso method that minimizes the L1 penalized L2 loss. BLasso is derived as a coordinate descent method with a ﬁxed stepsize applied to the general Lasso loss function (L1 penalized convex loss). It consists of both a forward step and a backward step. The forward step is similar to e-Boosting or FSF, but the backward step is new and revises the FSF (or e-Boosting) path to approximate the Lasso path. In the cases of a ﬁnite number of base learners and a bounded Hessian of the loss function, the BLasso path is shown to converge to the Lasso path when the stepsize goes to zero. For cases with a larger number of base learners than the sample size and when the true model is sparse, our simulations indicate that the BLasso model estimates are sparser than those from FSF with comparable or slightly better prediction performance, and that the the discrete stepsize of BLasso and FSF has an additional regularization effect in terms of prediction and sparsity. Moreover, we introduce the Generalized BLasso algorithm to minimize a general convex loss penalized by a general convex function. Since the (Generalized) BLasso relies only on differences</p><p>6 0.39593127 <a title="42-lda-6" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>7 0.38555765 <a title="42-lda-7" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>8 0.36308566 <a title="42-lda-8" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>9 0.36174899 <a title="42-lda-9" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>10 0.35986716 <a title="42-lda-10" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>11 0.3595444 <a title="42-lda-11" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>12 0.35627088 <a title="42-lda-12" href="./jmlr-2007-Handling_Missing_Values_when_Applying_Classification_Models.html">39 jmlr-2007-Handling Missing Values when Applying Classification Models</a></p>
<p>13 0.35172749 <a title="42-lda-13" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>14 0.35050189 <a title="42-lda-14" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>15 0.34779599 <a title="42-lda-15" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>16 0.33859801 <a title="42-lda-16" href="./jmlr-2007-Bayesian_Quadratic_Discriminant_Analysis.html">13 jmlr-2007-Bayesian Quadratic Discriminant Analysis</a></p>
<p>17 0.33775368 <a title="42-lda-17" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>18 0.33247498 <a title="42-lda-18" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>19 0.33016261 <a title="42-lda-19" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>20 0.32389513 <a title="42-lda-20" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
