<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-89" href="#">jmlr2007-89</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2007-89-pdf" href="http://jmlr.org/papers/volume8/guermeur07a/guermeur07a.pdf">pdf</a></p><p>Author: Yann Guermeur</p><p>Abstract: In the context of discriminant analysis, Vapnik’s statistical learning theory has mainly been developed in three directions: the computation of dichotomies with binary-valued functions, the computation of dichotomies with real-valued functions, and the computation of polytomies with functions taking their values in ﬁnite sets, typically the set of categories itself. The case of classes of vectorvalued functions used to compute polytomies has seldom been considered independently, which is unsatisfactory, for three main reasons. First, this case encompasses the other ones. Second, it cannot be treated appropriately through a na¨ve extension of the results devoted to the computation of ı dichotomies. Third, most of the classiﬁcation problems met in practice involve multiple categories. In this paper, a VC theory of large margin multi-category classiﬁers is introduced. Central in this theory are generalized VC dimensions called the γ-Ψ-dimensions. First, a uniform convergence bound on the risk of the classiﬁers of interest is derived. The capacity measure involved in this bound is a covering number. This covering number can be upper bounded in terms of the γ-Ψdimensions thanks to generalizations of Sauer’s lemma, as is illustrated in the speciﬁc case of the scale-sensitive Natarajan dimension. A bound on this latter dimension is then computed for the class of functions on which multi-class SVMs are based. This makes it possible to apply the structural risk minimization inductive principle to those machines. Keywords: multi-class discriminant analysis, large margin classiﬁers, uniform strong laws of large numbers, generalized VC dimensions, multi-class SVMs, structural risk minimization inductive principle, model selection</p><p>Reference: <a title="jmlr-2007-89-reference" href="../jmlr2007_reference/jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Keywords: multi-class discriminant analysis, large margin classiﬁers, uniform strong laws of large numbers, generalized VC dimensions, multi-class SVMs, structural risk minimization inductive principle, model selection  1. [sent-15, score-0.327]
</p><p>2 , 1995), and ] large margin classiﬁers computing dichotomies (Alon et al. [sent-21, score-0.246]
</p><p>3 G UERMEUR  In this paper, we unify two complementary and well established theories, the theory of large margin (bi-class) classiﬁers and the theory of multi-class [[ 1, Q ]-valued classiﬁers, to lay the bases ] of a simple theory of large margin multi-class classiﬁers. [sent-27, score-0.354]
</p><p>4 Section 2 introduces the notion of multi-class margin and margin risk for multi-class discriminant models, as well as the capacity measure that will appear in the conﬁdence interval of the basic guaranteed risk, a covering number. [sent-52, score-0.592]
</p><p>5 The extension of Sauer’s lemma relating the covering number of interest to one of the γ-Ψ-dimensions, the margin Natarajan dimension, is established in Section 5. [sent-55, score-0.364]
</p><p>6 Section 7 is devoted to the computation of a bound on the margin Natarajan dimension of the architecture shared by all the M-SVMs. [sent-57, score-0.306]
</p><p>7 10 for a detailed study of the question in a similar context), plus the constraint Q ∑k=1 gk = 0 (the purpose of this constraint will appear later). [sent-78, score-0.237]
</p><p>8 g assigns x ∈ X to the category l if and only if gl (x) > maxk=l gk (x). [sent-79, score-0.237]
</p><p>9 Deﬁnition 2 (Expected risk) The expected risk of a function g ∈ G , R(g), is given by: R(g) = E [ (Y, g (X))] =  Z  Z  1 {gy (x)≤maxk=y gk (x)} dP(z). [sent-87, score-0.306]
</p><p>10 ∑ 1l m i=1 {gYi (Xi )≤maxk=Yi gk (Xi )}  When needed, the m-sample used will be speciﬁed, by writing for instance R Dm (g) in place of Rm (g). [sent-90, score-0.237]
</p><p>11 In the sequel, Rzn (g) will designate the frequency of errors 1 ∑n 1 {gy (xi )≤maxk=y gk (xi )} . [sent-92, score-0.296]
</p><p>12 To take this margin into account, the following operators are introduced: 2554  VC T HEORY OF L ARGE M ARGIN M ULTI -C ATEGORY C LASSIFIERS  Deﬁnition 6 (∆ operator) Deﬁne ∆ as an operator on G such that: ∆ : G −→ ∆G , g → ∆g = (∆gk )1≤k≤Q , ∀x ∈ X , ∆g(x) = (M (g(x), k))1≤k≤Q . [sent-105, score-0.233]
</p><p>13 Deﬁnition 7 (∆∗ operator) Deﬁne ∆∗ as an operator on G such that: ∆∗ : G −→ ∆∗ G g → ∆∗ g = (∆∗ gk )1≤k≤Q ∀x ∈ X , ∆∗ g(x) = (max (∆gk (x), −M (g(x), . [sent-108, score-0.293]
</p><p>14 Let us consider the ﬁrst case, and let k ∗ = argmax1≤k≤Q ∆gk (x) = argmax1≤k≤Q gk (x) (∆gk∗ (x) = M (g(x), . [sent-112, score-0.237]
</p><p>15 This is precisely to in G ˜ ensure the existence of this one-to-one map that the constraint ∑Q gk = 0 has been introduced. [sent-136, score-0.237]
</p><p>16 As a consequence, one can consider that when implementing a ˜ ˜ large margin bi-class classiﬁer, the functions effectively handled are the component functions ∆g 1 (or equivalently the component functions ∆∗ g1 ). [sent-138, score-0.279]
</p><p>17 l 2555  G UERMEUR  With these deﬁnitions at hand, the margin risk is deﬁned as follows. [sent-142, score-0.246]
</p><p>18 The risk with margin γ of a function g of G , + Rγ (g), is deﬁned as: Rγ (g) = E 1 {∆# gY (X)<γ} . [sent-144, score-0.246]
</p><p>19 l The empirical risk with margin γ of g, Rγ,m (g) (or Rγ,Dm (g) if the sample needs to be speciﬁed), and the frequency of errors with margin γ, Rγ,zn (g), are deﬁned accordingly. [sent-145, score-0.423]
</p><p>20 A consequence of the deﬁnition of the margin risk is the fact that knowing the exact behaviour of the component functions ∆# gk below −γ and over γ is useless. [sent-146, score-0.558]
</p><p>21 For a sequence xn = (xi )1≤i≤n ∈ X n , deﬁne the pseudo-metric dxn on G as: ∀(g, g ) ∈ G 2 , dxn (g, g ) = max g(xi ) − g (xi ) 1≤i≤n  ∞  . [sent-169, score-0.412]
</p><p>22 In this section, the γ-Ψ-dimensions are introduced as the generalized VC dimensions suited for large margin multi-category classiﬁers. [sent-222, score-0.233]
</p><p>23 The basic result relating a covering number (precisely the growth function) to the VC dimension is the Sauer-Shelah lemma (Vapnik and Chervonenkis, 1971; Sauer, 1972; Shelah, 1972). [sent-225, score-0.267]
</p><p>24 As stated in the introduction, extensions of the standard VC theory, which only deals with the computation of dichotomies with indicator functions, have mainly been proposed for large margin bi-class discriminant models and multi-class discriminant models taking their values in ﬁnite sets. [sent-226, score-0.38]
</p><p>25 For large margin bi-class discriminant models, the generalization of the VC dimension which has given birth to the richest set of theoretical results is a scale-sensitive variant called the fat-shattering dimension (Kearns and Schapire, 1994). [sent-229, score-0.389]
</p><p>26 A subset sX n = {xi : 1 ≤ i ≤ n} of X is said to be Ψ-shattered by F if there is a mapping ψn = ψ(i) 1≤i≤n in Ψn such that for each vector vy in {−1, 1}n , there is a function f y in F satisfying ψ(i) ◦ fy (xi )  1≤i≤n  = vy . [sent-240, score-0.284]
</p><p>27 In that context, the motivation for the choice of one particular dimension (set Ψ) utterly rests on the possibility to derive two tight bounds: a generalized Sauer-Shelah lemma and a bound on the dimension itself. [sent-246, score-0.35]
</p><p>28 For γ ∈ R∗ , a subset sX n = {xi : 1 ≤ i ≤ n} of X is said to be γ-shattered + by G if there is a vector vb = (bi ) ∈ Rn such that, for each vector vy = (yi ) in {−1, 1}n , there is a function gy in G satisfying ∀i ∈ [[ 1, n ]], yi (gy (xi ) − bi ) ≥ γ. [sent-259, score-0.867]
</p><p>29 (1) The fat-shattering dimension with margin γ, or Pγ dimension, of the class G , Pγ -dim (G ), is the maximal cardinality of a subset of X γ-shattered by G , if this cardinality is ﬁnite. [sent-260, score-0.47]
</p><p>30 if yi = −1, ∃l : ψ(i) (l) = −1 ∧ ∆# gy,l (xi ) + bi ≥ γ  (2)  The γ-Ψ-dimension, or Ψ-dimension with margin γ, of ∆# G , denoted by Ψ-dim ∆# G , γ , is the maximal cardinality of a subset of X γ-Ψ-shattered by ∆# G , if this cardinality is ﬁnite. [sent-266, score-0.748]
</p><p>31 Given the deﬁnitions of the Natarajan dimension and the scale-sensitive Ψdimensions, it can be formulated as: Deﬁnition 29 (Natarajan dimension with margin γ) Let G be a class of functions on a set X taking their values in RQ . [sent-270, score-0.371]
</p><p>32 if yi = −1, ∆# gy,i2 (xi ) (xi ) + bi ≥ γ  The Natarajan dimension with margin γ of the class ∆# G , N-dim ∆# G , γ , is the maximal cardinality of a subset of X γ-N-shattered by ∆# G , if this cardinality is ﬁnite. [sent-272, score-0.828]
</p><p>33 If no such maximum exists, ∆# G is said to have inﬁnite Natarajan dimension with margin γ. [sent-273, score-0.284]
</p><p>34 2561  G UERMEUR  Proposition 30 The deﬁnition of the Natarajan dimension with margin γ is not affected by the introduction of the additional constraint: ∀i ∈ [[ 1, n ], i1 (xi ) < i2 (xi ). [sent-283, score-0.257]
</p><p>35 ] Proof Let Gy be a subset of G of cardinality 2n such that ∆# Gy γ-N-shatters sX n with respect to I(sX n ) and vb . [sent-284, score-0.306]
</p><p>36 Let vb = (bi ) be the vector of Rn deduced from vb as follows: ∀i ∈ [[ 1, n ], bi = bi if (i1 (xi ), i2 (xi )) = ] (i1 (xi ), i2 (xi )), bi = −bi otherwise. [sent-286, score-1.221]
</p><p>37 For any vector vy = (yi ) of {−1, 1}n , let gy be the function in Gy such that ∆# gy “contributes” to the γ-N-shattering of sX n with respect to I(sX n ) and vb for a value of the binary vector equal to vy = (yi ), where yi = yi if (i1 (xi ), i2 (xi )) = (i1 (xi ), i2 (xi )), yi = −yi otherwise. [sent-288, score-0.952]
</p><p>38 According to Deﬁnition 29, if yi = 1, ∆# gy ,i1 (xi ) (xi ) − bi ≥ γ ∀i ∈ [[ 1, n ]], . [sent-289, score-0.479]
</p><p>39 if yi = −1, ∆# gy ,i2 (xi ) (xi ) + bi ≥ γ As a consequence, for the set of indexes i such that (i1 (xi ), i2 (xi )) = (i1 (xi ), i2 (xi )), if yi = 1, ∆# gy ,i1 (xi ) (xi ) − bi ≥ γ . [sent-290, score-1.067]
</p><p>40 if yi = −1, ∆# gy ,i2 (xi ) (xi ) + bi ≥ γ  (3)  Furthermore, for the set of indexes i such that (i1 (xi ), i2 (xi )) = (i2 (xi ), i1 (xi )), if yi = −1, ∆# gy ,i2 (xi ) (xi ) + bi ≥ γ . [sent-291, score-1.067]
</p><p>41 if yi = 1, ∆# gy ,i1 (xi ) (xi ) − bi ≥ γ This is exactly (3), which thus holds true for all values of i in [[ 1, n ]] (whether the couple (i 1 (xi ), i2 (xi )) is equal to (i1 (xi ), i2 (xi )) or equal to (i2 (xi ), i1 (xi ))). [sent-292, score-0.516]
</p><p>42 According to Deﬁnition 29, the function ∆# gy thus contributes to the γ-N-shattering of sX n with respect to I (sX n ) and vb for a value of the binary vector equal to vy . [sent-293, score-0.443]
</p><p>43 But since the vector vy has been chosen arbitrarily in {−1, 1}n , this implies that ∆# Gy γ-N-shatters sX n with respect to I (sX n ) and vb , which, by construction of I (sX n ), concludes the proof. [sent-294, score-0.35]
</p><p>44 As a consequence, (2) simpliﬁes into: if yi = 1, ∆# gy,1 (xi ) − bi ≥ γ ∀i ∈ [[ 1, n ]], . [sent-302, score-0.358]
</p><p>45 (4) if yi = −1, ∆# gy,2 (xi ) + bi ≥ γ  Since we have seen in Section 2. [sent-303, score-0.358]
</p><p>46 Relating the Covering Number and the Margin Natarajan Dimension This section is devoted to the formulation of an upper bound on the covering number of interest in terms of the margin Natarajan dimension. [sent-309, score-0.301]
</p><p>47 )(η) : ∆# G −→ ∆# G  (η)  ∆# g → ∆# g  ∀x ∈ X , ∆# g  (η)  (x) =  (η)  , ∆# g k  =  (η) 1≤k≤Q  ∆# gk (x) η  sign ∆# gk (x) ·  ,  1≤k≤Q  where the function . [sent-333, score-0.474]
</p><p>48 (xi ) + bi ≥ 1  , SN-dim  dinality of a subset of X strongly N-shattered by ∆# G (η)  (η)  M η  ∆# G  (η)  , is the maximal car-  , if this cardinality is ﬁnite. [sent-339, score-0.45]
</p><p>49 Suppose further, without loss of generality, that max k fk (x) ≥ maxk fk (x) and let ∗ (x). [sent-352, score-0.303]
</p><p>50 But ∗ ∗ f ∗ (x) = 0 and maxk fk (x) ≥ maxk fk (x) implies that f ∗ (x) = 0, which is in contradiction with the  ≥ 2. [sent-356, score-0.42]
</p><p>51 l0 is simply the index of a component of g (x) satisfying gl0 (x) = 0 ∗ maxk=k0 gk (x). [sent-358, score-0.276]
</p><p>52 If fl∗ (x) = maxk fk (x), 0 0 ∗ ∗ ∗ ∗ ∗ then fk0 (x) = maxk fk (x) and fl∗ (x) = − fk0 (x). [sent-364, score-0.42]
</p><p>53 Thus, the couple ∗ I ({x}) = {(k0 , l0 )} , vb = fk0 (x) − 1 witnesses the strong N-shattering of {x} by { f , f }. [sent-366, score-0.25]
</p><p>54 Proof Let us say that the class F strongly N-shatters a triplet (sD , I(sD ), vb ) (for a nonempty subset sD of D , a set of couples of indexes I(sD ) and a vector of biases vb ) if F strongly Nshatters sD according to I(sD ) and vb . [sent-399, score-1.011]
</p><p>55 For all integers l ≥ 2 and |D | ≥ 1, let t(l, |D |) denote the maximum number t such that, for every set Fl ∗ of l pairwise separated functions in F ∗ , Fl = f ∈ F : f ∗ ∈ Fl ∗ strongly N-shatters at least t triplets (sD , I(sD ), vb ). [sent-400, score-0.428]
</p><p>56 The number of triplets (sD , I(sD ), vb ) that could be shattered and for which the cardinality of sD does not exceed d ≥ 1 is less than ∑d |D | i=1 i i  Q 2  i  (2q − 1) , since for sD of size i > 0, there  are strictly less than Q (2q − 1) possibilities to choose the couple (I(sD ), vb ). [sent-402, score-0.596]
</p><p>57 For all these pairs, the correspond-  ing sets { f , f } all shatter {x0 } (shatter at least one triplet of the form ({x0 } , I ({x0 }) , vb )). [sent-417, score-0.297]
</p><p>58 Hence, by deﬁnition of the function t, F+ strongly N-shatters at least t (2p, |D | − 1) triplets (sD , I(sD ), vb ) with sD ⊆ D \ {x0 }, and the same holds for F− . [sent-424, score-0.318]
</p><p>59 Let (sD , I(sD ), vb ) be a triplet strongly N-shattered both by F+ and by F− . [sent-429, score-0.328]
</p><p>60 Since, once more by construction, neither F+ nor F− strongly N-shatters {x0 } sD (whatever S the pair (I({x0 } sD ), vb ) may be), it follows that t 2p |D | Q2 (Q − 1) q2 , |D | ≥ 2t(2p, |D | − 1), ¯ which is precisely (8). [sent-432, score-0.278]
</p><p>61 Since the number of distinct functions in F ∗ is bounded from above by (Qq + 1)|D | , F ∗ cannot contain a set of pairwise separated functions of cardinality larger than this number and hence, by deﬁnition of t, log2 (φ(d,|D |)) , |D | = ∞. [sent-440, score-0.237]
</p><p>62 Proof ∀xn ∈ X n , applying Lemma 56 (right-hand side inequality) to ∆∗ G gives:  N (p) (ε, ∆∗ G , dxn ) ≤ M (ε, ∆∗ G , dxn ) . [sent-449, score-0.378]
</p><p>63 Setting η = ε/3 in Proposition 2 of Lemma 57, one obtains:  N (p) (ε, ∆∗ G , dxn ) ≤ M 2, (∆∗ G )(ε/3) , dxn . [sent-450, score-0.378]
</p><p>64 Since  M 2, (∆∗ G )(ε/3) , dxn = M 2, (∆∗ G )(ε/3)  Dn  , dx n ,  (10) implies:  N (p) (ε, ∆∗ G , dxn ) ≤ M 2, (∆∗ G )(ε/3) The packing numbers of (∆∗ G )(ε/3)  Dn  Dn  , dx n . [sent-453, score-0.412]
</p><p>65 6 Discussion To sum up, in this section, we have derived a bound on the covering number of interest in terms of one of the γ-Ψ-dimensions, the margin Natarajan dimension. [sent-467, score-0.301]
</p><p>66 1, the choice of one particular variant of the VC dimension rests on the search for an optimal compromise between two requirements that can be contradictory: the need for a tight bound on the capacity measure in terms of the VC dimension, and the need for a tight bound on the VC dimension itself. [sent-472, score-0.3]
</p><p>67 Deriving a bound on the margin Natarajan dimension of the M-SVMs can be performed very simply, by extending in a straightforward way the reasoning of the proof of the standard bound on the fat-shattering dimension of the perceptron (or pattern recognition SVM). [sent-474, score-0.435]
</p><p>68 Proposition 41 (Almost sure convergence) lim sup P sup sup R(g) − Rγ,n (g) > ε  m−→+∞ P  n≥m g∈G  = 0. [sent-482, score-0.255]
</p><p>69 , 1992; Cortes and Vapnik, 1995) as nonlinear extensions of the maximal margin hyperplane (Vapnik, 1982). [sent-500, score-0.234]
</p><p>70 Indeed, the notion of multi-class margin given by Deﬁnition 5 involves differences between outputs, which suggests to use such penalty terms as maxk < i2 (xi ). [sent-565, score-0.328]
</p><p>71 This means that for all vector vy = (yi ) in {−1, 1}n ,  at least p =  2575  G UERMEUR  ¯ ¯ there is a function hy in H characterized by the vector wy = (wy,k )1≤k≤Q such that: ∀i ∈ [[ 1, p ]],  ¯ if yi = 1, ∆hy,k0 (xi ) − bi ≥ ε . [sent-569, score-0.499]
</p><p>72 wy,l0 − wy,k0 , Φ(xi ) + bi ≥ ε  (18)  Consider now any partition of sX p into two subsets s1 and s2 . [sent-571, score-0.265]
</p><p>73 Consider any vector vy in {−1, 1}n such that yi = 1 if xi ∈ s1 and yi = −1 if xi ∈ s2 . [sent-572, score-0.619]
</p><p>74 It results from (18) that: 1 1 wy,k0 − wy,l0 , ∑ Φ(xi ) − ∑ bi + wy,l0 − wy,k0 , ∑ Φ(xi ) + ∑ bi ≥ |sX p | ε 2 2 xi ∈s1 xi ∈s2 xi ∈s1 xi ∈s2 which simpliﬁes into 1 wy,k0 − wy,l0 , ∑ Φ(xi ) − ∑ Φ(xi ) − ∑ bi + ∑ bi ≥ pε. [sent-573, score-1.708]
</p><p>75 2 xi ∈s1 xi ∈s2 xi ∈s1 xi ∈s2 Conversely, consider any vector vy such that yi = −1 if xi ∈ s1 and yi = 1 if xi ∈ s2 . [sent-574, score-1.267]
</p><p>76 We have: 1 wy,l0 − wy,k0 , ∑ Φ(xi ) − ∑ Φ(xi ) + ∑ bi − ∑ bi ≥ pε. [sent-575, score-0.53]
</p><p>77 Q 2  (20)  Applying the Cauchy-Schwarz inequality to (19) and (20) yields 1 wy,k0 − wy,l0 2  ∑ Φ(xi ) − ∑ Φ(xi )  xi ∈s1  xi ∈s2  ≥  n Q 2  ε,  which thus holds true irrespective of the value of ∑xi ∈s1 bi − ∑xi ∈s2 bi . [sent-577, score-0.854]
</p><p>78 Let Q denote the following event:  Q = zm = ((xi , yi ))1≤i≤m ∈ Z m : sup R(g) − Rγ,zm (g) > ε . [sent-588, score-0.315]
</p><p>79 z m  (23)  I  I is an integral which is calculated for a ﬁxed  zm  satisfying  sup R(g) − Rγ,zm (g) > ε. [sent-590, score-0.261]
</p><p>80 m  Lemma 53 (Hoeffding’s inequality, Hoeffding, 1963) For n ∈ N∗ , let (Ti )1≤i≤n be a sequence of n independent random variables with zero means and bounded ranges: a i ≤ Ti ≤ bi . [sent-614, score-0.265]
</p><p>81 for every real number ε satisfying ε ≥ 3η and every x n = (xi )1≤i≤n ∈ X n ,  M (ε, ∆∗ G , dxn ) ≤ M 2, (∆∗ G )(η) , dxn . [sent-632, score-0.417]
</p><p>82 Proof To prove the ﬁrst proposition, it is enough to establish that any set strongly N-shattered by (∆G )(η) is also N-shattered with margin η/2 by ∆G . [sent-633, score-0.242]
</p><p>83 1) bi ≥ 0 and yi = 1  (η)  ∆gy,i1 (xi )  (η)  (xi ) − bi ≥ 1 =⇒ ∆gy,i1 (xi ) (xi ) −  (xi ) + bi ≥ 1 =⇒ ∆gy,i2 (xi ) (xi ) + bi ≥ η/2. [sent-635, score-1.153]
</p><p>84 To that end, four cases must  ∆gy,i1 (xi ) thus  (xi ) − bi ≥ 1 . [sent-636, score-0.265]
</p><p>85 (xi ) + bi ≥ 1  (η)  (η)  (xi ) > 0 =⇒ η ∆gy,i1 (xi )  (η)  (xi ) ≤ ∆gy,i1 (xi ) (xi )  (xi ) − bi ≥ 1 =⇒ ∆gy,i1 (xi ) (xi ) − η(bi + 1/2) ≥ η/2. [sent-637, score-0.53]
</p><p>86 2) bi ≥ 0 and yi = −1 ∆gy,i2 (xi )  (η)  (xi ) + bi ≥ 1 =⇒ ∆gy,i2 (xi ) (xi ) + ηbi ≥ 0 2586  VC T HEORY OF L ARGE M ARGIN M ULTI -C ATEGORY C LASSIFIERS  or equivalently ∆gy,i2 (xi )  (η)  (xi ) + bi ≥ 1 =⇒ ∆gy,i2 (xi ) (xi ) + η(bi + 1/2) ≥ η/2. [sent-638, score-0.888]
</p><p>87 3) bi < 0 and yi = 1 ∆gy,i1 (xi )  (η)  (xi ) − bi ≥ 1 =⇒ ∆gy,i1 (xi ) (xi ) − ηbi ≥ 0  or equivalently ∆gy,i1 (xi )  (η)  (xi ) − bi ≥ 1 =⇒ ∆gy,i1 (xi ) (xi ) − η(bi − 1/2) ≥ η/2. [sent-639, score-0.888]
</p><p>88 4) bi < 0 and yi = −1 ∆gy,i2 (xi ) thus ∆gy,i2 (xi )  (η)  (η)  (xi ) > 0 =⇒ η ∆gy,i2 (xi )  (η)  (xi ) ≤ ∆gy,i2 (xi ) (xi )  (xi ) + bi ≥ 1 =⇒ ∆gy,i2 (xi ) (xi ) + η(bi − 1/2) ≥ η/2. [sent-640, score-0.623]
</p><p>89 To sum up, a satisfactory solution consists in setting bi = η(bi + 1/2) if bi ≥ 0 and bi = η(bi − 1/2) otherwise. [sent-641, score-0.795]
</p><p>90 By deﬁnition, the set of functions ∆gy , for vy in {−1, 1}n , N-shatters sX n with margin η/2, for a set of couples of indexes and a vector of “biases” respectively equal to I(s X n ) and vb = (bi )1≤i≤n . [sent-642, score-0.725]
</p><p>91 As a consequence, any set strongly N-shattered by (∆G ) (η) is also N-shattered with margin η/2 by ∆G , which is precisely our claim. [sent-643, score-0.242]
</p><p>92 To prove the second proposition, let us ﬁrst notice that: ∀(g, g ) ∈ G 2 , ∀x ∈ X , ∀k ∈ [[ 1, Q ]], ∀η ∈ (0, M], ∆∗ gk (x) − ∆∗ gk (x) ≥ 3η =⇒ (∆∗ gk )(η) (x) − ∆∗ gk  (η)  (x) ≥ 2. [sent-644, score-0.948]
</p><p>93 Indeed, without loss of generality, we can make the hypothesis that ∆ ∗ gk (x) > ∆∗ gk (x). [sent-645, score-0.474]
</p><p>94 Then, ∆∗ g k  (η)  (x) − 1 η < ∆∗ gk (x) < ∆∗ gk (x) < (∆∗ gk )(η) (x) + 1 η. [sent-646, score-0.711]
</p><p>95 Thus (∆∗ gk )(η) (x) + 1 η −  (η)  ∆∗ g k  (x) − 1 η > 3η  and ﬁnally (∆∗ gk )(η) (x) − ∆∗ gk  (η)  (x) > 1,  from which the desired result springs directly, keeping in mind that the η-discretizations are integer (η) (η) (x) ≥ 2 . [sent-647, score-0.741]
</p><p>96 (x) > 1 =⇒ (∆∗ gk )(η) (x) − ∆∗ gk numbers (∆∗ gk )(η) (x) − ∆∗ gk ∗ G in the pseudo-metric d n . [sent-648, score-0.948]
</p><p>97 Note that a more interesting second proposition could have resulted from using a different deﬁnition # (η) k (x) = ∆ gη (x) irrespective of the sign of ∆# gk (x), of the η-discretization. [sent-651, score-0.277]
</p><p>98 Indeed, setting ∆# gk one can easily establish that the following proposition, with a dependence between ε and η identical to the one of Alon et al. [sent-652, score-0.237]
</p><p>99 (1997), holds true: for every ε ≥ 2η and every xn ∈ X n , M (ε, ∆∗ G , dxn ) ≤ M (2, (∆∗ G )(η) , dxn ). [sent-653, score-0.378]
</p><p>100 The reason for our choice is to get an additional useful property, namely: ∀η ∈ (0, M], ∆# gl (x) = −∆# gk (x) =⇒ ∆# gl  (η)  (x) = − ∆# gk  (η)  (x). [sent-654, score-0.474]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('natarajan', 0.288), ('bi', 0.265), ('gk', 0.237), ('vc', 0.23), ('sx', 0.218), ('vb', 0.213), ('uermeur', 0.208), ('dxn', 0.189), ('margin', 0.177), ('sd', 0.174), ('ategory', 0.169), ('heory', 0.169), ('xi', 0.162), ('maxk', 0.151), ('lassifiers', 0.143), ('zm', 0.137), ('ulti', 0.128), ('gy', 0.121), ('pdm', 0.119), ('arge', 0.117), ('argin', 0.117), ('lemma', 0.112), ('indexes', 0.109), ('rzm', 0.109), ('vy', 0.109), ('rdm', 0.101), ('cardinality', 0.093), ('yi', 0.093), ('rq', 0.09), ('sup', 0.085), ('couples', 0.083), ('dimension', 0.08), ('covering', 0.075), ('dichotomies', 0.069), ('risk', 0.069), ('alon', 0.069), ('dpm', 0.067), ('dn', 0.066), ('strongly', 0.065), ('fk', 0.059), ('designate', 0.059), ('operator', 0.056), ('bartlett', 0.053), ('vapnik', 0.052), ('discriminant', 0.052), ('triplet', 0.05), ('guermeur', 0.05), ('kolmogorov', 0.05), ('qq', 0.05), ('bound', 0.049), ('separated', 0.048), ('sauer', 0.045), ('wk', 0.044), ('endowed', 0.042), ('gyi', 0.042), ('dd', 0.042), ('symmetrization', 0.042), ('fl', 0.042), ('capacity', 0.042), ('consequence', 0.041), ('proposition', 0.04), ('pigeonhole', 0.04), ('tihomirov', 0.04), ('triplets', 0.04), ('mappings', 0.04), ('nition', 0.04), ('satisfying', 0.039), ('couple', 0.037), ('sequel', 0.036), ('functions', 0.034), ('max', 0.034), ('mrdm', 0.034), ('packing', 0.034), ('shatter', 0.034), ('ln', 0.033), ('devroye', 0.032), ('hy', 0.032), ('extensions', 0.03), ('multi', 0.03), ('exposed', 0.03), ('categories', 0.03), ('berlinet', 0.03), ('emq', 0.03), ('polytomies', 0.03), ('pseudometric', 0.03), ('sauershelah', 0.03), ('springs', 0.03), ('generalized', 0.029), ('pairwise', 0.028), ('concludes', 0.028), ('learnability', 0.028), ('svms', 0.028), ('maximal', 0.027), ('dimensions', 0.027), ('said', 0.027), ('capabilities', 0.026), ('master', 0.026), ('hk', 0.025), ('dm', 0.025), ('theorem', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999887 <a title="89-tfidf-1" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>Author: Yann Guermeur</p><p>Abstract: In the context of discriminant analysis, Vapnik’s statistical learning theory has mainly been developed in three directions: the computation of dichotomies with binary-valued functions, the computation of dichotomies with real-valued functions, and the computation of polytomies with functions taking their values in ﬁnite sets, typically the set of categories itself. The case of classes of vectorvalued functions used to compute polytomies has seldom been considered independently, which is unsatisfactory, for three main reasons. First, this case encompasses the other ones. Second, it cannot be treated appropriately through a na¨ve extension of the results devoted to the computation of ı dichotomies. Third, most of the classiﬁcation problems met in practice involve multiple categories. In this paper, a VC theory of large margin multi-category classiﬁers is introduced. Central in this theory are generalized VC dimensions called the γ-Ψ-dimensions. First, a uniform convergence bound on the risk of the classiﬁers of interest is derived. The capacity measure involved in this bound is a covering number. This covering number can be upper bounded in terms of the γ-Ψdimensions thanks to generalizations of Sauer’s lemma, as is illustrated in the speciﬁc case of the scale-sensitive Natarajan dimension. A bound on this latter dimension is then computed for the class of functions on which multi-class SVMs are based. This makes it possible to apply the structural risk minimization inductive principle to those machines. Keywords: multi-class discriminant analysis, large margin classiﬁers, uniform strong laws of large numbers, generalized VC dimensions, multi-class SVMs, structural risk minimization inductive principle, model selection</p><p>2 0.10852787 <a title="89-tfidf-2" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>Author: Yiming Ying, Ding-Xuan Zhou</p><p>Abstract: Gaussian kernels with ﬂexible variances provide a rich family of Mercer kernels for learning algorithms. We show that the union of the unit balls of reproducing kernel Hilbert spaces generated by Gaussian kernels with ﬂexible variances is a uniform Glivenko-Cantelli (uGC) class. This result conﬁrms a conjecture concerning learnability of Gaussian kernels and veriﬁes the uniform convergence of many learning algorithms involving Gaussians with changing variances. Rademacher averages and empirical covering numbers are used to estimate sample errors of multi-kernel regularization schemes associated with general loss functions. It is then shown that the regularization error associated with the least square loss and the Gaussian kernels can be greatly improved when ﬂexible variances are allowed. Finally, for regularization schemes generated by Gaussian kernels with ﬂexible variances we present explicit learning rates for regression with least square loss and classiﬁcation with hinge loss. Keywords: Gaussian kernel, ﬂexible variances, learning theory, Glivenko-Cantelli class, regularization scheme, empirical covering number</p><p>3 0.091651231 <a title="89-tfidf-3" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>Author: Robert Tibshirani, Trevor Hastie</p><p>Abstract: We propose a method for the classiﬁcation of more than two classes, from high-dimensional features. Our approach is to build a binary decision tree in a top-down manner, using the optimal margin classiﬁer at each split. We implement an exact greedy algorithm for this task, and compare its performance to less greedy procedures based on clustering of the matrix of pairwise margins. We compare the performance of the “margin tree” to the closely related “all-pairs” (one versus one) support vector machine, and nearest centroids on a number of cancer microarray data sets. We also develop a simple method for feature selection. We ﬁnd that the margin tree has accuracy that is competitive with other methods and offers additional interpretability in its putative grouping of the classes. Keywords: maximum margin classiﬁer, support vector machine, decision tree, CART</p><p>4 0.089851916 <a title="89-tfidf-4" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>5 0.088988505 <a title="89-tfidf-5" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>Author: Peter L. Bartlett, Mikhail Traskin</p><p>Abstract: The risk, or probability of error, of the classiﬁer produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after n1−ε iterations—for sample size n and ε ∈ (0, 1)—the sequence of risks of the classiﬁers it produces approaches the Bayes risk. Keywords: boosting, adaboost, consistency</p><p>6 0.075553164 <a title="89-tfidf-6" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>7 0.06772083 <a title="89-tfidf-7" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>8 0.062173534 <a title="89-tfidf-8" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>9 0.061043508 <a title="89-tfidf-9" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>10 0.058480434 <a title="89-tfidf-10" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>11 0.055908535 <a title="89-tfidf-11" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>12 0.053428877 <a title="89-tfidf-12" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>13 0.053116206 <a title="89-tfidf-13" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>14 0.051629525 <a title="89-tfidf-14" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>15 0.050900016 <a title="89-tfidf-15" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>16 0.046461619 <a title="89-tfidf-16" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>17 0.04600336 <a title="89-tfidf-17" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>18 0.043863602 <a title="89-tfidf-18" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>19 0.043278608 <a title="89-tfidf-19" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>20 0.0420742 <a title="89-tfidf-20" href="./jmlr-2007-Multi-Task_Learning_for_Classification_with_Dirichlet_Process_Priors.html">56 jmlr-2007-Multi-Task Learning for Classification with Dirichlet Process Priors</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.256), (1, -0.161), (2, 0.055), (3, -0.003), (4, 0.138), (5, 0.033), (6, -0.025), (7, 0.049), (8, 0.001), (9, 0.077), (10, 0.067), (11, 0.088), (12, 0.073), (13, 0.11), (14, 0.102), (15, -0.166), (16, -0.178), (17, 0.057), (18, -0.06), (19, -0.219), (20, 0.183), (21, 0.162), (22, -0.001), (23, -0.051), (24, -0.132), (25, 0.299), (26, -0.043), (27, -0.143), (28, 0.043), (29, 0.015), (30, -0.017), (31, 0.011), (32, -0.006), (33, -0.027), (34, 0.026), (35, 0.059), (36, -0.054), (37, -0.002), (38, 0.019), (39, -0.041), (40, 0.025), (41, -0.052), (42, 0.027), (43, -0.103), (44, -0.028), (45, 0.004), (46, 0.101), (47, -0.005), (48, -0.007), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95430768 <a title="89-lsi-1" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>Author: Yann Guermeur</p><p>Abstract: In the context of discriminant analysis, Vapnik’s statistical learning theory has mainly been developed in three directions: the computation of dichotomies with binary-valued functions, the computation of dichotomies with real-valued functions, and the computation of polytomies with functions taking their values in ﬁnite sets, typically the set of categories itself. The case of classes of vectorvalued functions used to compute polytomies has seldom been considered independently, which is unsatisfactory, for three main reasons. First, this case encompasses the other ones. Second, it cannot be treated appropriately through a na¨ve extension of the results devoted to the computation of ı dichotomies. Third, most of the classiﬁcation problems met in practice involve multiple categories. In this paper, a VC theory of large margin multi-category classiﬁers is introduced. Central in this theory are generalized VC dimensions called the γ-Ψ-dimensions. First, a uniform convergence bound on the risk of the classiﬁers of interest is derived. The capacity measure involved in this bound is a covering number. This covering number can be upper bounded in terms of the γ-Ψdimensions thanks to generalizations of Sauer’s lemma, as is illustrated in the speciﬁc case of the scale-sensitive Natarajan dimension. A bound on this latter dimension is then computed for the class of functions on which multi-class SVMs are based. This makes it possible to apply the structural risk minimization inductive principle to those machines. Keywords: multi-class discriminant analysis, large margin classiﬁers, uniform strong laws of large numbers, generalized VC dimensions, multi-class SVMs, structural risk minimization inductive principle, model selection</p><p>2 0.52504683 <a title="89-lsi-2" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>3 0.47406939 <a title="89-lsi-3" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>Author: Robert Tibshirani, Trevor Hastie</p><p>Abstract: We propose a method for the classiﬁcation of more than two classes, from high-dimensional features. Our approach is to build a binary decision tree in a top-down manner, using the optimal margin classiﬁer at each split. We implement an exact greedy algorithm for this task, and compare its performance to less greedy procedures based on clustering of the matrix of pairwise margins. We compare the performance of the “margin tree” to the closely related “all-pairs” (one versus one) support vector machine, and nearest centroids on a number of cancer microarray data sets. We also develop a simple method for feature selection. We ﬁnd that the margin tree has accuracy that is competitive with other methods and offers additional interpretability in its putative grouping of the classes. Keywords: maximum margin classiﬁer, support vector machine, decision tree, CART</p><p>4 0.42774945 <a title="89-lsi-4" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>Author: Dima Kuzmin, Manfred K. Warmuth</p><p>Abstract: n Maximum concept classes of VC dimension d over n domain points have size ≤d , and this is an upper bound on the size of any concept class of VC dimension d over n points. We give a compression scheme for any maximum class that represents each concept by a subset of up to d unlabeled domain points and has the property that for any sample of a concept in the class, the representative of exactly one of the concepts consistent with the sample is a subset of the domain of the sample. This allows us to compress any sample of a concept in the class to a subset of up to d unlabeled sample points such that this subset represents a concept consistent with the entire original sample. Unlike the previously known compression scheme for maximum classes (Floyd and Warmuth, 1995) which compresses to labeled subsets of the sample of size equal d, our new scheme is tight in the sense that the number of possible unlabeled compression sets of size at most d equals the number of concepts in the class. Keywords: compression schemes, VC dimension, maximum classes, one-inclusion graph</p><p>5 0.41366804 <a title="89-lsi-5" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>Author: Jaime S. Cardoso, Joaquim F. Pinto da Costa</p><p>Abstract: Classiﬁcation of ordinal data is one of the most important tasks of relation learning. This paper introduces a new machine learning paradigm speciﬁcally intended for classiﬁcation problems where the classes have a natural order. The technique reduces the problem of classifying ordered classes to the standard two-class problem. The introduced method is then mapped into support vector machines and neural networks. Generalization bounds of the proposed ordinal classiﬁer are also provided. An experimental study with artiﬁcial and real data sets, including an application to gene expression analysis, veriﬁes the usefulness of the proposed approach. Keywords: classiﬁcation, ordinal data, support vector machines, neural networks</p><p>6 0.40874597 <a title="89-lsi-6" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>7 0.33161113 <a title="89-lsi-7" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>8 0.32803094 <a title="89-lsi-8" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>9 0.32654867 <a title="89-lsi-9" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>10 0.32047662 <a title="89-lsi-10" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>11 0.31435513 <a title="89-lsi-11" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>12 0.3134816 <a title="89-lsi-12" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>13 0.29885158 <a title="89-lsi-13" href="./jmlr-2007-Large_Margin_Semi-supervised_Learning.html">44 jmlr-2007-Large Margin Semi-supervised Learning</a></p>
<p>14 0.27976039 <a title="89-lsi-14" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>15 0.26076722 <a title="89-lsi-15" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>16 0.25398126 <a title="89-lsi-16" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>17 0.23596239 <a title="89-lsi-17" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>18 0.22126976 <a title="89-lsi-18" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>19 0.22027276 <a title="89-lsi-19" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>20 0.21398915 <a title="89-lsi-20" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.016), (7, 0.4), (8, 0.041), (10, 0.015), (12, 0.025), (15, 0.03), (28, 0.098), (40, 0.034), (48, 0.025), (60, 0.072), (85, 0.05), (98, 0.101)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71095002 <a title="89-lda-1" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>Author: Yann Guermeur</p><p>Abstract: In the context of discriminant analysis, Vapnik’s statistical learning theory has mainly been developed in three directions: the computation of dichotomies with binary-valued functions, the computation of dichotomies with real-valued functions, and the computation of polytomies with functions taking their values in ﬁnite sets, typically the set of categories itself. The case of classes of vectorvalued functions used to compute polytomies has seldom been considered independently, which is unsatisfactory, for three main reasons. First, this case encompasses the other ones. Second, it cannot be treated appropriately through a na¨ve extension of the results devoted to the computation of ı dichotomies. Third, most of the classiﬁcation problems met in practice involve multiple categories. In this paper, a VC theory of large margin multi-category classiﬁers is introduced. Central in this theory are generalized VC dimensions called the γ-Ψ-dimensions. First, a uniform convergence bound on the risk of the classiﬁers of interest is derived. The capacity measure involved in this bound is a covering number. This covering number can be upper bounded in terms of the γ-Ψdimensions thanks to generalizations of Sauer’s lemma, as is illustrated in the speciﬁc case of the scale-sensitive Natarajan dimension. A bound on this latter dimension is then computed for the class of functions on which multi-class SVMs are based. This makes it possible to apply the structural risk minimization inductive principle to those machines. Keywords: multi-class discriminant analysis, large margin classiﬁers, uniform strong laws of large numbers, generalized VC dimensions, multi-class SVMs, structural risk minimization inductive principle, model selection</p><p>2 0.37239432 <a title="89-lda-2" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>Author: Marta Arias, Roni Khardon, Jérôme Maloberti</p><p>Abstract: The paper introduces L OG A N -H —a system for learning ﬁrst-order function-free Horn expressions from interpretations. The system is based on an algorithm that learns by asking questions and that was proved correct in previous work. The current paper shows how the algorithm can be implemented in a practical system, and introduces a new algorithm based on it that avoids interaction and learns from examples only. The L OG A N -H system implements these algorithms and adds several facilities and optimizations that allow efﬁcient applications in a wide range of problems. As one of the important ingredients, the system includes several fast procedures for solving the subsumption problem, an NP-complete problem that needs to be solved many times during the learning process. We describe qualitative and quantitative experiments in several domains. The experiments demonstrate that the system can deal with varied problems, large amounts of data, and that it achieves good classiﬁcation accuracy. Keywords: inductive logic programming, subsumption, bottom-up learning, learning with queries</p><p>3 0.36792946 <a title="89-lda-3" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>4 0.36667687 <a title="89-lda-4" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>Author: Francesco Dinuzzo, Marta Neve, Giuseppe De Nicolao, Ugo Pietro Gianazza</p><p>Abstract: Support Vector Regression (SVR) for discrete data is considered. An alternative formulation of the representer theorem is derived. This result is based on the newly introduced notion of pseudoresidual and the use of subdifferential calculus. The representer theorem is exploited to analyze the sensitivity properties of ε-insensitive SVR and introduce the notion of approximate degrees of freedom. The degrees of freedom are shown to play a key role in the evaluation of the optimism, that is the difference between the expected in-sample error and the expected empirical risk. In this way, it is possible to deﬁne a C p -like statistic that can be used for tuning the parameters of SVR. The proposed tuning procedure is tested on a simulated benchmark problem and on a real world problem (Boston Housing data set). Keywords: statistical learning, reproducing kernel Hilbert spaces, support vector machines, representer theorem, regularization theory</p><p>5 0.36343229 <a title="89-lda-5" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>Author: Yuesheng Xu, Haizhang Zhang</p><p>Abstract: Motivated by mathematical learning from training data, we introduce the notion of reﬁnable kernels. Various characterizations of reﬁnable kernels are presented. The concept of reﬁnable kernels leads to the introduction of wavelet-like reproducing kernels. We also investigate a reﬁnable kernel that forms a Riesz basis. In particular, we characterize reﬁnable translation invariant kernels, and reﬁnable kernels deﬁned by reﬁnable functions. This study leads to multiresolution analysis of reproducing kernel Hilbert spaces. Keywords: reﬁnable kernels, reﬁnable feature maps, wavelet-like reproducing kernels, dual kernels, learning with kernels, reproducing kernel Hilbert spaces, Riesz bases</p><p>6 0.36291236 <a title="89-lda-6" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>7 0.36277407 <a title="89-lda-7" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>8 0.3614803 <a title="89-lda-8" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>9 0.3598904 <a title="89-lda-9" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>10 0.35704482 <a title="89-lda-10" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>11 0.3564091 <a title="89-lda-11" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>12 0.3563211 <a title="89-lda-12" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>13 0.3556304 <a title="89-lda-13" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>14 0.35472697 <a title="89-lda-14" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>15 0.35184354 <a title="89-lda-15" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>16 0.34996185 <a title="89-lda-16" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>17 0.34962624 <a title="89-lda-17" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>18 0.3483873 <a title="89-lda-18" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>19 0.34785068 <a title="89-lda-19" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>20 0.34716487 <a title="89-lda-20" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
