<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-78" href="#">jmlr2007-78</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</h1>
<br/><p>Source: <a title="jmlr-2007-78-pdf" href="http://jmlr.org/papers/volume8/fukumizu07a/fukumizu07a.pdf">pdf</a></p><p>Author: Kenji Fukumizu, Francis R. Bach, Arthur Gretton</p><p>Abstract: While kernel canonical correlation analysis (CCA) has been applied in many contexts, the convergence of ﬁnite sample estimates of the associated functions to their population counterparts has not yet been established. This paper gives a mathematical proof of the statistical convergence of kernel CCA, providing a theoretical justiﬁcation for the method. The proof uses covariance operators deﬁned on reproducing kernel Hilbert spaces, and analyzes the convergence of their empirical estimates of ﬁnite rank to their population counterparts, which can have inﬁnite rank. The result also gives a sufﬁcient condition for convergence on the regularization coefﬁcient involved in kernel CCA: this should decrease as n−1/3 , where n is the number of data. Keywords: canonical correlation analysis, kernel, consistency, regularization, Hilbert space</p><p>Reference: <a title="jmlr-2007-78-reference" href="../jmlr2007_reference/jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('hx', 0.445), ('yy', 0.414), ('xx', 0.399), ('cca', 0.391), ('hy', 0.252), ('vy', 0.197), ('fn', 0.156), ('kx', 0.142), ('ky', 0.132), ('bach', 0.118), ('nocco', 0.114), ('px', 0.096), ('py', 0.094), ('pxy', 0.092), ('coco', 0.085), ('hs', 0.079), ('retton', 0.078), ('ukumizu', 0.078), ('kernel', 0.077), ('rkhs', 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="78-tfidf-1" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>Author: Kenji Fukumizu, Francis R. Bach, Arthur Gretton</p><p>Abstract: While kernel canonical correlation analysis (CCA) has been applied in many contexts, the convergence of ﬁnite sample estimates of the associated functions to their population counterparts has not yet been established. This paper gives a mathematical proof of the statistical convergence of kernel CCA, providing a theoretical justiﬁcation for the method. The proof uses covariance operators deﬁned on reproducing kernel Hilbert spaces, and analyzes the convergence of their empirical estimates of ﬁnite rank to their population counterparts, which can have inﬁnite rank. The result also gives a sufﬁcient condition for convergence on the regularization coefﬁcient involved in kernel CCA: this should decrease as n−1/3 , where n is the number of data. Keywords: canonical correlation analysis, kernel, consistency, regularization, Hilbert space</p><p>2 0.10613068 <a title="78-tfidf-2" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: Binary classiﬁcation is a well studied special case of the classiﬁcation problem. Statistical properties of binary classiﬁers, such as consistency, have been investigated in a variety of settings. Binary classiﬁcation methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classiﬁcation method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufﬁcient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature. Keywords: multiclass classiﬁcation, consistency, Bayes risk</p><p>3 0.087474369 <a title="78-tfidf-3" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>Author: Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee, Robert L. Wolpert</p><p>Abstract: Kernel methods have been very popular in the machine learning literature in the last ten years, mainly in the context of Tikhonov regularization algorithms. In this paper we study a coherent Bayesian kernel model based on an integral operator deﬁned as the convolution of a kernel with a signed measure. Priors on the random signed measures correspond to prior distributions on the functions mapped by the integral operator. We study several classes of signed measures and their image mapped by the integral operator. In particular, we identify a general class of measures whose image is dense in the reproducing kernel Hilbert space (RKHS) induced by the kernel. A consequence of this result is a function theoretic foundation for using non-parametric prior speciﬁcations in Bayesian modeling, such as Gaussian process and Dirichlet process prior distributions. We discuss the construction of priors on spaces of signed measures using Gaussian and L´ vy processes, e with the Dirichlet processes being a special case the latter. Computational issues involved with sampling from the posterior distribution are outlined for a univariate regression and a high dimensional classiﬁcation problem. Keywords: reproducing kernel Hilbert space, non-parametric Bayesian methods, L´ vy processes, e Dirichlet processes, integral operator, Gaussian processes c 2007 Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee and Robert L. Wolpert. P ILLAI , W U , L IANG , M UKHERJEE AND W OLPERT</p><p>4 0.084454067 <a title="78-tfidf-4" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>Author: Ryan M. Rifkin, Ross A. Lippert</p><p>Abstract: Regularization is an approach to function learning that balances ﬁt and smoothness. In practice, we search for a function f with a ﬁnite representation f = ∑i ci φi (·). In most treatments, the ci are the primary objects of study. We consider value regularization, constructing optimization problems in which the predicted values at the training points are the primary variables, and therefore the central objects of study. Although this is a simple change, it has profound consequences. From convex conjugacy and the theory of Fenchel duality, we derive separate optimality conditions for the regularization and loss portions of the learning problem; this technique yields clean and short derivations of standard algorithms. This framework is ideally suited to studying many other phenomena at the intersection of learning theory and optimization. We obtain a value-based variant of the representer theorem, which underscores the transductive nature of regularization in reproducing kernel Hilbert spaces. We unify and extend previous results on learning kernel functions, with very simple proofs. We analyze the use of unregularized bias terms in optimization problems, and low-rank approximations to kernel matrices, obtaining new results in these areas. In summary, the combination of value regularization and Fenchel duality are valuable tools for studying the optimization problems in machine learning. Keywords: kernel machines, duality, optimization, convex analysis, kernel learning</p><p>5 0.059027564 <a title="78-tfidf-5" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>6 0.054742411 <a title="78-tfidf-6" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>7 0.050836854 <a title="78-tfidf-7" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>8 0.04228054 <a title="78-tfidf-8" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>9 0.035459291 <a title="78-tfidf-9" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>10 0.032174349 <a title="78-tfidf-10" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>11 0.030330339 <a title="78-tfidf-11" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>12 0.029555151 <a title="78-tfidf-12" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>13 0.028846823 <a title="78-tfidf-13" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>14 0.027763451 <a title="78-tfidf-14" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>15 0.027289489 <a title="78-tfidf-15" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>16 0.027051855 <a title="78-tfidf-16" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>17 0.02698181 <a title="78-tfidf-17" href="./jmlr-2007-General_Polynomial_Time_Decomposition_Algorithms_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">35 jmlr-2007-General Polynomial Time Decomposition Algorithms     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>18 0.025322441 <a title="78-tfidf-18" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>19 0.02519525 <a title="78-tfidf-19" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>20 0.024736188 <a title="78-tfidf-20" href="./jmlr-2007-Undercomplete_Blind_Subspace_Deconvolution.html">87 jmlr-2007-Undercomplete Blind Subspace Deconvolution</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.131), (1, -0.173), (2, 0.168), (3, 0.003), (4, -0.003), (5, -0.024), (6, -0.031), (7, -0.002), (8, -0.02), (9, 0.073), (10, 0.012), (11, 0.004), (12, 0.006), (13, 0.066), (14, -0.017), (15, -0.144), (16, -0.095), (17, -0.011), (18, -0.031), (19, 0.206), (20, 0.173), (21, -0.307), (22, -0.07), (23, -0.043), (24, 0.019), (25, 0.022), (26, 0.29), (27, 0.146), (28, 0.049), (29, -0.247), (30, 0.042), (31, -0.118), (32, 0.048), (33, 0.153), (34, -0.151), (35, -0.109), (36, -0.029), (37, 0.036), (38, -0.094), (39, -0.032), (40, 0.101), (41, 0.124), (42, -0.072), (43, -0.161), (44, -0.092), (45, 0.115), (46, 0.028), (47, -0.031), (48, 0.156), (49, -0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95483547 <a title="78-lsi-1" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>Author: Kenji Fukumizu, Francis R. Bach, Arthur Gretton</p><p>Abstract: While kernel canonical correlation analysis (CCA) has been applied in many contexts, the convergence of ﬁnite sample estimates of the associated functions to their population counterparts has not yet been established. This paper gives a mathematical proof of the statistical convergence of kernel CCA, providing a theoretical justiﬁcation for the method. The proof uses covariance operators deﬁned on reproducing kernel Hilbert spaces, and analyzes the convergence of their empirical estimates of ﬁnite rank to their population counterparts, which can have inﬁnite rank. The result also gives a sufﬁcient condition for convergence on the regularization coefﬁcient involved in kernel CCA: this should decrease as n−1/3 , where n is the number of data. Keywords: canonical correlation analysis, kernel, consistency, regularization, Hilbert space</p><p>2 0.52118003 <a title="78-lsi-2" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: Binary classiﬁcation is a well studied special case of the classiﬁcation problem. Statistical properties of binary classiﬁers, such as consistency, have been investigated in a variety of settings. Binary classiﬁcation methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classiﬁcation method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufﬁcient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature. Keywords: multiclass classiﬁcation, consistency, Bayes risk</p><p>3 0.32990018 <a title="78-lsi-3" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>4 0.27418423 <a title="78-lsi-4" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>Author: Simon Günter, Nicol N. Schraudolph, S. V. N. Vishwanathan</p><p>Abstract: We develop gain adaptation methods that improve convergence of the kernel Hebbian algorithm (KHA) for iterative kernel PCA (Kim et al., 2005). KHA has a scalar gain parameter which is either held constant or decreased according to a predetermined annealing schedule, leading to slow convergence. We accelerate it by incorporating the reciprocal of the current estimated eigenvalues as part of a gain vector. An additional normalization term then allows us to eliminate a tuning parameter in the annealing schedule. Finally we derive and apply stochastic meta-descent (SMD) gain vector adaptation (Schraudolph, 1999, 2002) in reproducing kernel Hilbert space to further speed up convergence. Experimental results on kernel PCA and spectral clustering of USPS digits, motion capture and image denoising, and image super-resolution tasks conﬁrm that our methods converge substantially faster than conventional KHA. To demonstrate scalability, we perform kernel PCA on the entire MNIST data set. Keywords: step size adaptation, gain vector adaptation, stochastic meta-descent, kernel Hebbian algorithm, online learning</p><p>5 0.25802806 <a title="78-lsi-5" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>Author: Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee, Robert L. Wolpert</p><p>Abstract: Kernel methods have been very popular in the machine learning literature in the last ten years, mainly in the context of Tikhonov regularization algorithms. In this paper we study a coherent Bayesian kernel model based on an integral operator deﬁned as the convolution of a kernel with a signed measure. Priors on the random signed measures correspond to prior distributions on the functions mapped by the integral operator. We study several classes of signed measures and their image mapped by the integral operator. In particular, we identify a general class of measures whose image is dense in the reproducing kernel Hilbert space (RKHS) induced by the kernel. A consequence of this result is a function theoretic foundation for using non-parametric prior speciﬁcations in Bayesian modeling, such as Gaussian process and Dirichlet process prior distributions. We discuss the construction of priors on spaces of signed measures using Gaussian and L´ vy processes, e with the Dirichlet processes being a special case the latter. Computational issues involved with sampling from the posterior distribution are outlined for a univariate regression and a high dimensional classiﬁcation problem. Keywords: reproducing kernel Hilbert space, non-parametric Bayesian methods, L´ vy processes, e Dirichlet processes, integral operator, Gaussian processes c 2007 Natesh S. Pillai, Qiang Wu, Feng Liang, Sayan Mukherjee and Robert L. Wolpert. P ILLAI , W U , L IANG , M UKHERJEE AND W OLPERT</p><p>6 0.23121133 <a title="78-lsi-6" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>7 0.1951316 <a title="78-lsi-7" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>8 0.17007445 <a title="78-lsi-8" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>9 0.16780934 <a title="78-lsi-9" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>10 0.1574236 <a title="78-lsi-10" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>11 0.151484 <a title="78-lsi-11" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>12 0.12756477 <a title="78-lsi-12" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>13 0.11905243 <a title="78-lsi-13" href="./jmlr-2007-Undercomplete_Blind_Subspace_Deconvolution.html">87 jmlr-2007-Undercomplete Blind Subspace Deconvolution</a></p>
<p>14 0.11430603 <a title="78-lsi-14" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>15 0.10866971 <a title="78-lsi-15" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>16 0.1071149 <a title="78-lsi-16" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>17 0.10451418 <a title="78-lsi-17" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>18 0.095534407 <a title="78-lsi-18" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>19 0.093648933 <a title="78-lsi-19" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>20 0.086144485 <a title="78-lsi-20" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.024), (10, 0.449), (19, 0.02), (21, 0.01), (23, 0.036), (31, 0.021), (32, 0.073), (33, 0.021), (69, 0.034), (70, 0.017), (94, 0.117), (95, 0.039)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73143804 <a title="78-lda-1" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>Author: Kenji Fukumizu, Francis R. Bach, Arthur Gretton</p><p>Abstract: While kernel canonical correlation analysis (CCA) has been applied in many contexts, the convergence of ﬁnite sample estimates of the associated functions to their population counterparts has not yet been established. This paper gives a mathematical proof of the statistical convergence of kernel CCA, providing a theoretical justiﬁcation for the method. The proof uses covariance operators deﬁned on reproducing kernel Hilbert spaces, and analyzes the convergence of their empirical estimates of ﬁnite rank to their population counterparts, which can have inﬁnite rank. The result also gives a sufﬁcient condition for convergence on the regularization coefﬁcient involved in kernel CCA: this should decrease as n−1/3 , where n is the number of data. Keywords: canonical correlation analysis, kernel, consistency, regularization, Hilbert space</p><p>2 0.31438139 <a title="78-lda-2" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ambuj Tewari, Peter L. Bartlett</p><p>Abstract: Binary classiﬁcation is a well studied special case of the classiﬁcation problem. Statistical properties of binary classiﬁers, such as consistency, have been investigated in a variety of settings. Binary classiﬁcation methods can be generalized in many ways to handle multiple classes. It turns out that one can lose consistency in generalizing a binary classiﬁcation method to deal with multiple classes. We study a rich family of multiclass methods and provide a necessary and sufﬁcient condition for their consistency. We illustrate our approach by applying it to some multiclass methods proposed in the literature. Keywords: multiclass classiﬁcation, consistency, Bayes risk</p><p>3 0.304649 <a title="78-lda-3" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>Author: Amir Globerson, Gal Chechik, Fernando Pereira, Naftali Tishby</p><p>Abstract: Embedding algorithms search for a low dimensional continuous representation of data, but most algorithms only handle objects of a single type for which pairwise distances are speciﬁed. This paper describes a method for embedding objects of different types, such as images and text, into a single common Euclidean space, based on their co-occurrence statistics. The joint distributions are modeled as exponentials of Euclidean distances in the low-dimensional embedding space, which links the problem to convex optimization over positive semideﬁnite matrices. The local structure of the embedding corresponds to the statistical correlations via random walks in the Euclidean space. We quantify the performance of our method on two text data sets, and show that it consistently and signiﬁcantly outperforms standard methods of statistical correspondence modeling, such as multidimensional scaling, IsoMap and correspondence analysis. Keywords: embedding algorithms, manifold learning, exponential families, multidimensional scaling, matrix factorization, semideﬁnite programming</p><p>4 0.30260104 <a title="78-lda-4" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>Author: Peter L. Bartlett, Mikhail Traskin</p><p>Abstract: The risk, or probability of error, of the classiﬁer produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after n1−ε iterations—for sample size n and ε ∈ (0, 1)—the sequence of risks of the classiﬁers it produces approaches the Bayes risk. Keywords: boosting, adaboost, consistency</p><p>5 0.29668331 <a title="78-lda-5" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>6 0.29355323 <a title="78-lda-6" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>7 0.29154018 <a title="78-lda-7" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>8 0.28996494 <a title="78-lda-8" href="./jmlr-2007-Distances_between_Data_Sets_Based_on_Summary_Statistics.html">27 jmlr-2007-Distances between Data Sets Based on Summary Statistics</a></p>
<p>9 0.28853443 <a title="78-lda-9" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>10 0.28795767 <a title="78-lda-10" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>11 0.28785321 <a title="78-lda-11" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>12 0.28601679 <a title="78-lda-12" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>13 0.28348947 <a title="78-lda-13" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>14 0.28288895 <a title="78-lda-14" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>15 0.28244779 <a title="78-lda-15" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>16 0.28215182 <a title="78-lda-16" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>17 0.27940121 <a title="78-lda-17" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>18 0.27909186 <a title="78-lda-18" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>19 0.27681687 <a title="78-lda-19" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>20 0.27663937 <a title="78-lda-20" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
