<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>35 jmlr-2007-General Polynomial Time Decomposition Algorithms     (Special Topic on the Conference on Learning Theory 2005)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-35" href="#">jmlr2007-35</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>35 jmlr-2007-General Polynomial Time Decomposition Algorithms     (Special Topic on the Conference on Learning Theory 2005)</h1>
<br/><p>Source: <a title="jmlr-2007-35-pdf" href="http://jmlr.org/papers/volume8/list07a/list07a.pdf">pdf</a></p><p>Author: Nikolas List, Hans Ulrich Simon</p><p>Abstract: We present a general decomposition algorithm that is uniformly applicable to every (suitably normalized) instance of Convex Quadratic Optimization and efﬁciently approaches an optimal solution. The number of iterations required to be within ε of optimality grows linearly with 1/ε and quadratically with the number m of variables. The working set selection can be performed in polynomial time. If we restrict our considerations to instances of Convex Quadratic Optimization with at most k0 equality constraints for some ﬁxed constant k0 plus some so-called box-constraints (conditions that hold for most variants of SVM-optimization), the working set is found in linear time. Our analysis builds on a generalization of the concept of rate certifying pairs that was introduced by Hush and Scovel. In order to extend their results to arbitrary instances of Convex Quadratic Optimization, we introduce the general notion of a rate certifying q-set. We improve on the results by Hush and Scovel (2003) in several ways. First our result holds for Convex Quadratic Optimization whereas the results by Hush and Scovel are specialized to SVM-optimization. Second, we achieve a higher rate of convergence even for the special case of SVM-optimization (despite the generality of our approach). Third, our analysis is technically simpler. We prove furthermore that the strategy for working set selection which is based on rate certifying sets coincides with a strategy which is based on a so-called “sparse witness of sub-optimality”. Viewed from this perspective, our main result improves on convergence results by List and Simon (2004) and Simon (2004) by providing convergence rates (and by holding under more general conditions). Keywords: convex quadratic optimization, decomposition algorithms, support vector machines</p><p>Reference: <a title="jmlr-2007-35-reference" href="../jmlr2007_reference/jmlr-2007-General_Polynomial_Time_Decomposition_Algorithms_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 The working set selection can be performed in polynomial time. [sent-7, score-0.157]
</p><p>2 If we restrict our considerations to instances of Convex Quadratic Optimization with at most k0 equality constraints for some ﬁxed constant k0 plus some so-called box-constraints (conditions that hold for most variants of SVM-optimization), the working set is found in linear time. [sent-8, score-0.233]
</p><p>3 Our analysis builds on a generalization of the concept of rate certifying pairs that was introduced by Hush and Scovel. [sent-9, score-0.599]
</p><p>4 In order to extend their results to arbitrary instances of Convex Quadratic Optimization, we introduce the general notion of a rate certifying q-set. [sent-10, score-0.598]
</p><p>5 We prove furthermore that the strategy for working set selection which is based on rate certifying sets coincides with a strategy which is based on a so-called “sparse witness of sub-optimality”. [sent-15, score-0.784]
</p><p>6 Given an instance of CQO, this method keeps track of a current feasible solution which is iteratively improved. [sent-32, score-0.292]
</p><p>7 The success of the method depends in a quite sensitive manner on the policy for the selection of the working set I (whose size is typically much smaller than m). [sent-41, score-0.138]
</p><p>8 Ideally, the selection procedure should be computationally efﬁcient and, at the same time, effective in the sense that the resulting feasible solutions become cost-optimal in the limit (with high speed of convergence). [sent-42, score-0.214]
</p><p>9 They introduced the notion of an “α-rate certifying pair” and showed that every decomposition algorithm for SVO that always inserts an α-rate certifying pair in its current working set comes within ε of optimality after O(1/(εα2 )) iterations. [sent-44, score-1.423]
</p><p>10 (2000), they presented furthermore an algorithm that constructs an 1/m 2 -rate certifying pair in O(m log m) steps. [sent-46, score-0.573]
</p><p>11 1 Combining these results, we see that the decomposition algorithm of Hush and Scovel for problem SVO is within ε of optimality after O(m4 /ε) iterations. [sent-47, score-0.166]
</p><p>12 We ﬁrst deﬁne the general notion of an α-rate certifying q-set and show (with a simpliﬁed analysis) that it basically ﬁts the same purpose for CQO as the α-rate certifying pair for SVO, where the number of iterations needed to be within ε of optimality is proportional to q/(εα 2 ). [sent-49, score-1.24]
</p><p>13 Given an instance with k equality constraints and m variables, it ﬁnds an 1/m-rate certifying (k + 1)-set in polynomial time. [sent-51, score-0.714]
</p><p>14 2 Combining these results, we are within ε of optimality after O(km 2 /ε) iterations of our decomposition algorithm. [sent-52, score-0.197]
</p><p>15 List and Simon (2004) suggested a strategy for working set selection that is based on a so-called “sparse witnesses of sub-optimality”. [sent-57, score-0.219]
</p><p>16 They were able to prove that this strategy leads to feasible solutions of minimum cost in the limit. [sent-58, score-0.214]
</p><p>17 In this paper, we prove (by means of Linear Programming duality) that the strategy for working set selection which is based on rate certifying sets coincides with the strategy of List and Simon (2004). [sent-61, score-0.744]
</p><p>18 304  G ENERAL P OLYNOMIAL T IME D ECOMPOSITION A LGORITHMS  rithms following the approach of maximally KKT-violating pairs are within ε of optimality after only O(log 1/ε) iterations. [sent-69, score-0.139]
</p><p>19 , 2003) which tries to iteratively include the support vectors in the working set. [sent-74, score-0.138]
</p><p>20 The main difference between SimpleSVM and decomposition algorithms is the size of the working set which can grow-up to the number of support vectors in the former case and is kept constant in the latter. [sent-77, score-0.214]
</p><p>21 1), we brieﬂy recall the main result by Hush and Scovel (2003) about SVO and rate certifying pairs (Section 2. [sent-81, score-0.599]
</p><p>22 (2005, 2006) for a generalization of this result to similar but more general policies for working set selection. [sent-95, score-0.138]
</p><p>23 In the sequel, R(P ) = {x ∈ Rm | Ax = b, l ≤ x ≤ r} denotes the compact and convex set of feasible points for P . [sent-99, score-0.243]
</p><p>24 The well-known ﬁrst-order optimality condition for convex optimization (see, for example, Boyd and Vandenberghe, 2004) (valid for an arbitrary convex cost function) states that x ∈ R(P ) is an optimal feasible solution iff ∀x ∈ R(P ) : ∇ f (x) (x − x) ≥ 0 . [sent-100, score-0.364]
</p><p>25 By the compactness of the region of feasible points, we may also put a suitable upper bound on each slack variable such that ﬁnally all linear inequalities take the form of box-constraints. [sent-102, score-0.232]
</p><p>26 A decomposition algorithm for CQO with working sets of size at most q (where we allow that q depends on k) proceeds iteratively as follows: given an instance P of CQO and a feasible solution x ∈ R(P ) (chosen arbitrarily in the beginning), a so-called working set I ⊆ {1, . [sent-109, score-0.644]
</p><p>27 The policy for working set selection is a critical issue that we discuss in the next sections. [sent-115, score-0.138]
</p><p>28 Similarly, parameter q (possibly dependent on k) always represents the (maximal) size of the working set. [sent-117, score-0.138]
</p><p>29 • For a decomposition algorithm A , we denote the current feasible solution obtained after n iterations by xn (such that x0 is the feasible solution A starts with4 ). [sent-121, score-0.591]
</p><p>30 x∗ always denotes an optimal feasible solution such that ∆n := f (xn ) − f (x∗ )  (2)  denotes the difference between the value of the current solution and the optimal value. [sent-122, score-0.27]
</p><p>31 Let x ∈ R(P0 ) be a feasible solution and x∗ ∈ R(P0 ) an optimal feasible solution. [sent-125, score-0.456]
</p><p>32 In the sequel, we will often use the following ﬁrst-order approximation of the maximal distance (with regard to the value of the objective function) between a given point x and any other feasible solution x σ(x) :=  sup ∇ f (x) (x − x ) . [sent-126, score-0.262]
</p><p>33 Since we are dealing with working sets whose size is bounded by a (small) parameter q, it is natural to restrict the range of x to feasible solutions that differ from x in at most q coordinates. [sent-131, score-0.352]
</p><p>34 sup x ∈R(P0 ):xi =xi  for i=i1 ,i2  The following notion is crucial: (i1 , i2 ) is called an α-rate certifying pair for x if σ(x|i1 , i2 ) ≥ α( f (x) − f (x∗ )) . [sent-133, score-0.589]
</p><p>35 An α-rate certifying algorithm is a decomposition algorithm for SVO that, for every m and every input instance P 0 with m variables, always includes an α(m)-rate certifying pair in the current working set. [sent-135, score-1.364]
</p><p>36 The problem of ﬁnding an initial feasible solution can be cast as an instance of Linear Programming. [sent-137, score-0.292]
</p><p>37 For function α given by α(m) = 1/m2 , there exists an α-rate certifying algorithm. [sent-148, score-0.55]
</p><p>38 It constructs a working set (given P0 and a sub-optimal feasible solution x) in O(m log m) steps (or in O(m) steps when the method of Simon (2004) is applied). [sent-149, score-0.403]
</p><p>39 As suggested by List and Simon (2004), the selection of a working set can be guided by a function family (CI (x))I⊆{1,. [sent-153, score-0.138]
</p><p>40 ,m} as follows: Strict Rule: Given the current feasible solution x, choose the next working set I ⊆ {1, . [sent-156, score-0.38]
</p><p>41 Relaxed Rule: Given the current feasible solution x, choose the next working set I ⊆ {1, . [sent-160, score-0.38]
</p><p>42 (C2) If |I| ≤ q and x is an optimal solution for the subproblem induced by the current feasible solution x and working set I, then CI (x ) = 0. [sent-168, score-0.449]
</p><p>43 The working set selection problem induced by the function family CI (x) from (4) is NPcomplete if we insist on the strict rule. [sent-179, score-0.16]
</p><p>44 If (CI (x)) is a q-sparse witness of sub-optimality and the working set is always selected according to the relaxed rule, then the resulting feasible solutions become cost-optimal in the limit. [sent-187, score-0.413]
</p><p>45 1, we present the new notion of rate certifying sets and state the main convergence theorem whose proof is given in Sections 3. [sent-196, score-0.623]
</p><p>46 4, we reveal the relation between rate certifying sets and sparse witnesses of sub-optimality. [sent-200, score-0.681]
</p><p>47 1 Rate Certifying Sets The notion of rate certifying sets generalizes the notion of rate certifying pairs from Section 2. [sent-202, score-1.216]
</p><p>48 However, since CQO deals with several equality constraints, one can in general not expect to ﬁnd rate certifying pairs. [sent-206, score-0.637]
</p><p>49 sup x ∈R(P ):xi =xi  (6)  for i∈I /  I is called an α-rate certifying q-set if |I| ≤ q and σ(x|I) ≥ α( f (x) − f (x∗ )) . [sent-211, score-0.57]
</p><p>50 (7)  I is called a strong α-rate certifying q-set (implying that it is an α-rate certifying q-set) if |I| ≤ q and σ(x|I) ≥ ασ(x) . [sent-212, score-1.1]
</p><p>51 A (strong) (α, q)-rate certifying algorithm is a decomposition algorithm for CQO that, for every m, k and any problem instance P with m variables and k equality constraints, includes a (strong) α(m)-rate certifying q(k)-set in the current working set. [sent-214, score-1.422]
</p><p>52 Then, A is within ε of optimality after 2 2qLmax Smax 2 ∆0 + max 0, ln (8) 2ε α α ε 2 iterations. [sent-219, score-0.149]
</p><p>53 For functions α, q given by α(m) = 1/m and q(k) = k + 1, there exists a strong (α, q)-rate certifying algorithm A . [sent-222, score-0.55]
</p><p>54 7 It constructs a working set (given P and a sub-optimal feasible solution x) in polynomial time. [sent-223, score-0.422]
</p><p>55 Moreover, if we restrict its application to instances of CQO with at most k0 equality constraints for some ﬁxed constant k0 , there is a linear time bound for the construction of the working set. [sent-224, score-0.233]
</p><p>56 The algorithm A from the preceding statement is within ε of optimality after 2 2(k + 1)m2 Lmax Smax ∆0 + max 0, 2m ln ε ε 2 iterations. [sent-226, score-0.175]
</p><p>57 One might be tempted to think that an ( α, q)rate certifying algorithm decreases (an upper bound on) the distance between the current feasible solution and the best feasible solution (with regard to the objective value) roughly by factor 1 − α (for α := α(m)). [sent-232, score-1.034]
</p><p>58 We will see in the proof of Theorem 4 that a run of an (α, q)-rate certifying algorithm can be decomposed into two phases. [sent-235, score-0.55]
</p><p>59 2 Proof of the 1st Statement in Theorem 4 We will use the notation introduced in Theorem 4 and consider an arbitrary iteration of the ( α, q)rate certifying algorithm A when it is applied on input P . [sent-248, score-0.568]
</p><p>60 To this end, let x denote a feasible but sub-optimal solution, and let x∗ be an optimal feasible solution. [sent-249, score-0.428]
</p><p>61 Let I be the subset of the working set that satisﬁes |I| ≤ q and (7). [sent-250, score-0.138]
</p><p>62 Let x be a feasible solution that satisﬁes xi = xi for every i ∈ I and / σ(x|I) = ∇ f (x) (x − x ) = ∇ f (x) d ,  (9)  where d = x − x . [sent-251, score-0.242]
</p><p>63 For some parameter 0 ≤ λ ≤ 1 (suitably chosen later), consider the feasible solution x = x − λd on the line segment between x and x . [sent-253, score-0.242]
</p><p>64 Thus, x = x − λd is a feasible solution that / coincides with x outside the current working set. [sent-256, score-0.407]
</p><p>65 Thus, A (ﬁnding an optimal feasible solution that coincides with x outside the working set) achieves in the next iteration a “cost reduction” of at least f (x) − f (x ). [sent-257, score-0.425]
</p><p>66 “Achieving a cost reduction of a in the next iteration” means that the next iteration decreases the distance between the value of the current feasible solution and the value of an optimal feasible solution by at least a. [sent-267, score-0.502]
</p><p>67 Recall from (2) that the sequence (∆n )n≥0 keeps track of the difference between the value of the current feasible solution and the value of an optimal solution. [sent-275, score-0.242]
</p><p>68 Thus the total number of iterations in both phases needed to be within ε of optimality is bounded by n0 +  1 γε  (12),(13)  ≤  2 ∆0 2U + max 0, ln 2ε α α ε  iterations. [sent-287, score-0.18]
</p><p>69 Thus, we are within ε of optimality after max 0,  2 ∆0 ln α ε  iterations. [sent-292, score-0.149]
</p><p>70 A vector that satisﬁes the constraints is called a feasible solution. [sent-297, score-0.251]
</p><p>71 The instance is called feasible if it has a feasible solution. [sent-298, score-0.478]
</p><p>72 If a basic solution happens to be feasible, it is called a basic feasible solution. [sent-320, score-0.242]
</p><p>73 It is well-known that, for every feasible and bounded instance of Linear Programming in standard form, there exists a basic feasible solution that is optimal (among all feasible solutions). [sent-321, score-0.72]
</p><p>74 Let x be a feasible but sub-optimal solution for P . [sent-324, score-0.242]
</p><p>75 From Px , we derive (basically by aggregating several equality constraints into a single-one) the instance Px , whose optimal basic solution will represent a working set I of size at most k + 1. [sent-329, score-0.311]
</p><p>76 Its optimal basic feasible solution has therefore at most k + 1 non-zero components. [sent-348, score-0.242]
</p><p>77 The following observations (where d, d + , d − are related according to (16)) are easy to verify: d+ is a feasible solution for Px with value p, then d− for Px with value p/m. [sent-349, score-0.242]
</p><p>78 If  d+ is a feasible solution for Px with value p, then d− for Px with value p. [sent-351, score-0.242]
</p><p>79 If  1 m  d+ d−  d+ d−  is a feasible solution  is also a feasible solution  Recall that σ(x) is the value of an optimal solution for Px . [sent-353, score-0.512]
</p><p>80 Now consider an d+ optimal basic feasible solution for Px with value σ (x). [sent-355, score-0.242]
</p><p>81 Since d = d + − d − is a feasible solution for P (still with value σ (x)) that differs from x only in coordinates from I, we may conclude that σ(x|I) ≥ σ (x) ≥  1 σ(x) . [sent-361, score-0.242]
</p><p>82 d∈Rq  This is an (obviously feasible and bounded) instance of Linear Programming with the following inequality constraints Idq x −l d≤ I I , − Idq rI − x I where Idq ∈ Rq×q denotes the identity-matrix. [sent-397, score-0.301]
</p><p>83 • σ(x|I) was designed as a generalization of the corresponding function for rate certifying pairs. [sent-402, score-0.579]
</p><p>84 Deﬁnition 6 (WSS) The problem “Working Set Selection for CQO”, denoted brieﬂy as WSS(CQO), is the following computational problem: given an instance of CQO (see Deﬁnition 1) augmented by a feasible solution x and an upper bound q ≥ k + 1 on the size of the working set, ﬁnd a working set I∗ ⊆ {1, . [sent-403, score-0.568]
</p><p>85 An approximation algorithm for WSS(CQO) is called ρ-optimal 13 if, for every instance of WSS(CQO), it ﬁnds a working set I0 such that |I0 | ≤ q and σ(x|I0 ) ≥ ρ maxI:|I|≤q σ(x|I). [sent-407, score-0.188]
</p><p>86 Rate certifying algorithms and approximation algorithms for WSS(CQO) are closely related: Corollary 7 The following holds for WSS(CQO):14 1. [sent-409, score-0.55]
</p><p>87 A strong (α, k +1)-rate certifying algorithm can be viewed as an α(m)-optimal approximation algorithm for WSS(CQO). [sent-410, score-0.55]
</p><p>88 An approximation algorithm for WSS(CQO) with performance ratio ρ can be viewed as a strong (α, k + 1)-rate certifying algorithm where α(m) = ρ/m. [sent-412, score-0.55]
</p><p>89 A strong (α, k +1)-rate certifying algorithm applied to an instance of WSS(CQO) constructs a working set I0 of size at most k +1 such that σ(x|I0 ) ≥ α(m)σ(x). [sent-414, score-0.761]
</p><p>90 An approximation algorithm for WSS(CQO) can be used as the procedure within a decomposition algorithm that performs the working set selection. [sent-418, score-0.24]
</p><p>91 It then constructs a working set I0 such that |I0 | ≤ q and σ(x|I0 ) ≥ ρ maxI:|I|≤k+1 σ(x|I). [sent-420, score-0.161]
</p><p>92 317  L IST AND S IMON  Corollary 7 immediately implies that the strong (1/m, k + 1)-rate certifying algorithm described in Section 3. [sent-427, score-0.55]
</p><p>93 This result can however be strengthened: Corollary 8 The strong (1/m, k + 1)-rate certifying algorithm described in Section 3. [sent-429, score-0.55]
</p><p>94 Every instance of WSS(CQO) can be cast as an instance Px,q resulting from an instance Px by adding the constraint that the total number of non-zero components in d + , d − is bounded by q. [sent-433, score-0.15]
</p><p>95 The corollary is now an immediate consequence of the following observations: d+ is a feasible solution for Px,q with value p, then d− for Px with value p/q. [sent-436, score-0.242]
</p><p>96 If  1 q  d+ d−  d+ is a basic feasible solution for Px with value p, then d− solution for Px,q with value p. [sent-438, score-0.27]
</p><p>97 If  is a feasible solution  d+ d−  is also a feasible  Our results nicely strengthen (or complement) what was known before about working set selection w. [sent-440, score-0.594]
</p><p>98 CI (x): • For general q−sparse witnesses CI (x) List and Simon (2004) have shown that the “strict rule” of always selecting a working set I∗ that maximizes CI (x) subject to |I| ≤ q leads to costoptimal feasible solutions in the limit (but no convergence rate was provided). [sent-443, score-0.487]
</p><p>99 Conclusions and Open Problems We have presented an analysis of a decomposition algorithm that leads to the to-date strongest theoretical performance guarantees within the “rate certifying pair” approach. [sent-450, score-0.652]
</p><p>100 Our results can be seen as an extension of (and an improvement on) earlier work by Hush and Scovel (2003) about rate certifying pairs, and by List and Simon (2004) and Simon (2004) about sparse witnesses of sub-optimality. [sent-452, score-0.681]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('certifying', 0.55), ('cqo', 0.442), ('svo', 0.287), ('wss', 0.215), ('feasible', 0.214), ('smax', 0.191), ('px', 0.161), ('hush', 0.149), ('working', 0.138), ('simon', 0.135), ('lmax', 0.131), ('scovel', 0.127), ('eneral', 0.108), ('imon', 0.108), ('olynomial', 0.091), ('ecomposition', 0.081), ('witnesses', 0.081), ('decomposition', 0.076), ('ime', 0.074), ('lgorithms', 0.074), ('ist', 0.068), ('optimality', 0.064), ('equality', 0.058), ('ci', 0.054), ('qd', 0.051), ('instance', 0.05), ('nikolas', 0.048), ('ulrich', 0.048), ('phase', 0.044), ('subproblem', 0.041), ('ai', 0.041), ('mangasarian', 0.04), ('pessimistic', 0.04), ('witness', 0.04), ('list', 0.04), ('constraints', 0.037), ('rq', 0.036), ('rk', 0.036), ('idq', 0.036), ('olvi', 0.036), ('qlmax', 0.036), ('steiglitz', 0.036), ('vishwanthan', 0.036), ('hans', 0.033), ('lp', 0.033), ('keerthi', 0.032), ('ln', 0.032), ('quadratic', 0.031), ('iterations', 0.031), ('smo', 0.031), ('papadimitriou', 0.03), ('sathiya', 0.03), ('sequel', 0.029), ('maximally', 0.029), ('rate', 0.029), ('convex', 0.029), ('ri', 0.028), ('solution', 0.028), ('max', 0.027), ('di', 0.027), ('maxi', 0.027), ('coincides', 0.027), ('within', 0.026), ('chang', 0.026), ('li', 0.026), ('statement', 0.026), ('convergence', 0.025), ('programming', 0.025), ('dual', 0.024), ('bochum', 0.024), ('chiranjib', 0.024), ('dunn', 0.024), ('krishnaj', 0.024), ('lmi', 0.024), ('npcomplete', 0.024), ('rub', 0.024), ('shirish', 0.024), ('ax', 0.023), ('hsu', 0.023), ('constructs', 0.023), ('strict', 0.022), ('suitably', 0.022), ('primal', 0.021), ('sparse', 0.021), ('program', 0.021), ('relaxed', 0.021), ('aj', 0.02), ('simplesvm', 0.02), ('clint', 0.02), ('osuna', 0.02), ('sup', 0.02), ('pairs', 0.02), ('alexander', 0.02), ('notion', 0.019), ('polynomial', 0.019), ('lin', 0.019), ('iteration', 0.018), ('slack', 0.018), ('saunders', 0.018), ('bhattacharyya', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="35-tfidf-1" href="./jmlr-2007-General_Polynomial_Time_Decomposition_Algorithms_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">35 jmlr-2007-General Polynomial Time Decomposition Algorithms     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Nikolas List, Hans Ulrich Simon</p><p>Abstract: We present a general decomposition algorithm that is uniformly applicable to every (suitably normalized) instance of Convex Quadratic Optimization and efﬁciently approaches an optimal solution. The number of iterations required to be within ε of optimality grows linearly with 1/ε and quadratically with the number m of variables. The working set selection can be performed in polynomial time. If we restrict our considerations to instances of Convex Quadratic Optimization with at most k0 equality constraints for some ﬁxed constant k0 plus some so-called box-constraints (conditions that hold for most variants of SVM-optimization), the working set is found in linear time. Our analysis builds on a generalization of the concept of rate certifying pairs that was introduced by Hush and Scovel. In order to extend their results to arbitrary instances of Convex Quadratic Optimization, we introduce the general notion of a rate certifying q-set. We improve on the results by Hush and Scovel (2003) in several ways. First our result holds for Convex Quadratic Optimization whereas the results by Hush and Scovel are specialized to SVM-optimization. Second, we achieve a higher rate of convergence even for the special case of SVM-optimization (despite the generality of our approach). Third, our analysis is technically simpler. We prove furthermore that the strategy for working set selection which is based on rate certifying sets coincides with a strategy which is based on a so-called “sparse witness of sub-optimality”. Viewed from this perspective, our main result improves on convergence results by List and Simon (2004) and Simon (2004) by providing convergence rates (and by holding under more general conditions). Keywords: convex quadratic optimization, decomposition algorithms, support vector machines</p><p>2 0.049805805 <a title="35-tfidf-2" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: One of the nice properties of kernel classiﬁers such as SVMs is that they often produce sparse solutions. However, the decision functions of these classiﬁers cannot always be used to estimate the conditional probability of the class label. We investigate the relationship between these two properties and show that these are intimately related: sparseness does not occur when the conditional probabilities can be unambiguously estimated. We consider a family of convex loss functions and derive sharp asymptotic results for the fraction of data that becomes support vectors. This enables us to characterize the exact trade-off between sparseness and the ability to estimate conditional probabilities for these loss functions. Keywords: kernel methods, support vector machines, sparseness, estimating conditional probabilities</p><p>3 0.043459285 <a title="35-tfidf-3" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>4 0.039347485 <a title="35-tfidf-4" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Kwangmoo Koh, Seung-Jean Kim, Stephen Boyd</p><p>Abstract: Logistic regression with 1 regularization has been proposed as a promising method for feature selection in classiﬁcation problems. In this paper we describe an efﬁcient interior-point method for solving large-scale 1 -regularized logistic regression problems. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC; medium sized problems, with tens of thousands of features and examples, can be solved in tens of seconds (assuming some sparsity in the data). A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve very large problems, with a million features and examples (e.g., the 20 Newsgroups data set), in a few minutes, on a PC. Using warm-start techniques, a good approximation of the entire regularization path can be computed much more efﬁciently than by solving a family of problems independently. Keywords: logistic regression, feature selection, 1 regularization, regularization path, interiorpoint methods.</p><p>5 0.032415245 <a title="35-tfidf-5" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>Author: Philippe Rigollet</p><p>Abstract: We consider semi-supervised classiﬁcation when part of the available data is unlabeled. These unlabeled data can be useful for the classiﬁcation problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples. Keywords: semi-supervised learning, statistical learning theory, classiﬁcation, cluster assumption, generalization bounds</p><p>6 0.031856347 <a title="35-tfidf-6" href="./jmlr-2007-Value_Regularization_and_Fenchel_Duality.html">90 jmlr-2007-Value Regularization and Fenchel Duality</a></p>
<p>7 0.031356107 <a title="35-tfidf-7" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>8 0.029348768 <a title="35-tfidf-8" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>9 0.028729301 <a title="35-tfidf-9" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>10 0.02717999 <a title="35-tfidf-10" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>11 0.026138879 <a title="35-tfidf-11" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>12 0.0257401 <a title="35-tfidf-12" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>13 0.024670959 <a title="35-tfidf-13" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>14 0.022979628 <a title="35-tfidf-14" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>15 0.022524824 <a title="35-tfidf-15" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>16 0.02164422 <a title="35-tfidf-16" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>17 0.019251781 <a title="35-tfidf-17" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>18 0.018733863 <a title="35-tfidf-18" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>19 0.017828036 <a title="35-tfidf-19" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>20 0.017759325 <a title="35-tfidf-20" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.12), (1, -0.055), (2, 0.01), (3, -0.029), (4, 0.024), (5, 0.004), (6, -0.077), (7, 0.051), (8, 0.033), (9, 0.005), (10, 0.064), (11, 0.01), (12, -0.045), (13, 0.053), (14, -0.019), (15, 0.047), (16, 0.004), (17, -0.037), (18, -0.057), (19, 0.02), (20, 0.138), (21, 0.091), (22, 0.087), (23, -0.052), (24, 0.112), (25, 0.044), (26, -0.121), (27, 0.387), (28, -0.137), (29, 0.102), (30, 0.27), (31, -0.033), (32, 0.188), (33, -0.073), (34, 0.263), (35, 0.035), (36, -0.179), (37, -0.273), (38, -0.152), (39, 0.069), (40, -0.164), (41, -0.104), (42, -0.294), (43, -0.194), (44, 0.017), (45, 0.093), (46, -0.137), (47, -0.003), (48, 0.068), (49, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.963664 <a title="35-lsi-1" href="./jmlr-2007-General_Polynomial_Time_Decomposition_Algorithms_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">35 jmlr-2007-General Polynomial Time Decomposition Algorithms     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Nikolas List, Hans Ulrich Simon</p><p>Abstract: We present a general decomposition algorithm that is uniformly applicable to every (suitably normalized) instance of Convex Quadratic Optimization and efﬁciently approaches an optimal solution. The number of iterations required to be within ε of optimality grows linearly with 1/ε and quadratically with the number m of variables. The working set selection can be performed in polynomial time. If we restrict our considerations to instances of Convex Quadratic Optimization with at most k0 equality constraints for some ﬁxed constant k0 plus some so-called box-constraints (conditions that hold for most variants of SVM-optimization), the working set is found in linear time. Our analysis builds on a generalization of the concept of rate certifying pairs that was introduced by Hush and Scovel. In order to extend their results to arbitrary instances of Convex Quadratic Optimization, we introduce the general notion of a rate certifying q-set. We improve on the results by Hush and Scovel (2003) in several ways. First our result holds for Convex Quadratic Optimization whereas the results by Hush and Scovel are specialized to SVM-optimization. Second, we achieve a higher rate of convergence even for the special case of SVM-optimization (despite the generality of our approach). Third, our analysis is technically simpler. We prove furthermore that the strategy for working set selection which is based on rate certifying sets coincides with a strategy which is based on a so-called “sparse witness of sub-optimality”. Viewed from this perspective, our main result improves on convergence results by List and Simon (2004) and Simon (2004) by providing convergence rates (and by holding under more general conditions). Keywords: convex quadratic optimization, decomposition algorithms, support vector machines</p><p>2 0.34834334 <a title="35-lsi-2" href="./jmlr-2007-Sparseness_vs_Estimating_Conditional_Probabilities%3A_Some_Asymptotic_Results.html">75 jmlr-2007-Sparseness vs Estimating Conditional Probabilities: Some Asymptotic Results</a></p>
<p>Author: Peter L. Bartlett, Ambuj Tewari</p><p>Abstract: One of the nice properties of kernel classiﬁers such as SVMs is that they often produce sparse solutions. However, the decision functions of these classiﬁers cannot always be used to estimate the conditional probability of the class label. We investigate the relationship between these two properties and show that these are intimately related: sparseness does not occur when the conditional probabilities can be unambiguously estimated. We consider a family of convex loss functions and derive sharp asymptotic results for the fraction of data that becomes support vectors. This enables us to characterize the exact trade-off between sparseness and the ability to estimate conditional probabilities for these loss functions. Keywords: kernel methods, support vector machines, sparseness, estimating conditional probabilities</p><p>3 0.25829625 <a title="35-lsi-3" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>Author: Kwangmoo Koh, Seung-Jean Kim, Stephen Boyd</p><p>Abstract: Logistic regression with 1 regularization has been proposed as a promising method for feature selection in classiﬁcation problems. In this paper we describe an efﬁcient interior-point method for solving large-scale 1 -regularized logistic regression problems. Small problems with up to a thousand or so features and examples can be solved in seconds on a PC; medium sized problems, with tens of thousands of features and examples, can be solved in tens of seconds (assuming some sparsity in the data). A variation on the basic method, that uses a preconditioned conjugate gradient method to compute the search step, can solve very large problems, with a million features and examples (e.g., the 20 Newsgroups data set), in a few minutes, on a PC. Using warm-start techniques, a good approximation of the entire regularization path can be computed much more efﬁciently than by solving a family of problems independently. Keywords: logistic regression, feature selection, 1 regularization, regularization path, interiorpoint methods.</p><p>4 0.22193813 <a title="35-lsi-4" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>5 0.22025406 <a title="35-lsi-5" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>Author: Philippe Rigollet</p><p>Abstract: We consider semi-supervised classiﬁcation when part of the available data is unlabeled. These unlabeled data can be useful for the classiﬁcation problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples. Keywords: semi-supervised learning, statistical learning theory, classiﬁcation, cluster assumption, generalization bounds</p><p>6 0.20617479 <a title="35-lsi-6" href="./jmlr-2007-Comments_on_the_%22Core_Vector_Machines%3A_Fast_SVM_Training_on_Very_Large_Data_Sets%22.html">21 jmlr-2007-Comments on the "Core Vector Machines: Fast SVM Training on Very Large Data Sets"</a></p>
<p>7 0.14848825 <a title="35-lsi-7" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>8 0.1361625 <a title="35-lsi-8" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>9 0.12341141 <a title="35-lsi-9" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>10 0.1173211 <a title="35-lsi-10" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>11 0.10692817 <a title="35-lsi-11" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>12 0.10507771 <a title="35-lsi-12" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>13 0.1016848 <a title="35-lsi-13" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>14 0.098131076 <a title="35-lsi-14" href="./jmlr-2007-Hierarchical_Average_Reward_Reinforcement_Learning.html">41 jmlr-2007-Hierarchical Average Reward Reinforcement Learning</a></p>
<p>15 0.094038755 <a title="35-lsi-15" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>16 0.092416286 <a title="35-lsi-16" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>17 0.089815967 <a title="35-lsi-17" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>18 0.086988956 <a title="35-lsi-18" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>19 0.085696772 <a title="35-lsi-19" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>20 0.083314538 <a title="35-lsi-20" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.016), (10, 0.021), (12, 0.018), (28, 0.029), (40, 0.035), (48, 0.013), (60, 0.612), (80, 0.012), (85, 0.018), (98, 0.102)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93580425 <a title="35-lda-1" href="./jmlr-2007-General_Polynomial_Time_Decomposition_Algorithms_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">35 jmlr-2007-General Polynomial Time Decomposition Algorithms     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Nikolas List, Hans Ulrich Simon</p><p>Abstract: We present a general decomposition algorithm that is uniformly applicable to every (suitably normalized) instance of Convex Quadratic Optimization and efﬁciently approaches an optimal solution. The number of iterations required to be within ε of optimality grows linearly with 1/ε and quadratically with the number m of variables. The working set selection can be performed in polynomial time. If we restrict our considerations to instances of Convex Quadratic Optimization with at most k0 equality constraints for some ﬁxed constant k0 plus some so-called box-constraints (conditions that hold for most variants of SVM-optimization), the working set is found in linear time. Our analysis builds on a generalization of the concept of rate certifying pairs that was introduced by Hush and Scovel. In order to extend their results to arbitrary instances of Convex Quadratic Optimization, we introduce the general notion of a rate certifying q-set. We improve on the results by Hush and Scovel (2003) in several ways. First our result holds for Convex Quadratic Optimization whereas the results by Hush and Scovel are specialized to SVM-optimization. Second, we achieve a higher rate of convergence even for the special case of SVM-optimization (despite the generality of our approach). Third, our analysis is technically simpler. We prove furthermore that the strategy for working set selection which is based on rate certifying sets coincides with a strategy which is based on a so-called “sparse witness of sub-optimality”. Viewed from this perspective, our main result improves on convergence results by List and Simon (2004) and Simon (2004) by providing convergence rates (and by holding under more general conditions). Keywords: convex quadratic optimization, decomposition algorithms, support vector machines</p><p>2 0.87107021 <a title="35-lda-2" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>Author: Peter L. Bartlett, Mikhail Traskin</p><p>Abstract: The risk, or probability of error, of the classiﬁer produced by the AdaBoost algorithm is investigated. In particular, we consider the stopping strategy to be used in AdaBoost to achieve universal consistency. We show that provided AdaBoost is stopped after n1−ε iterations—for sample size n and ε ∈ (0, 1)—the sequence of risks of the classiﬁers it produces approaches the Bayes risk. Keywords: boosting, adaboost, consistency</p><p>3 0.85744208 <a title="35-lda-3" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>Author: Zakria Hussain, François Laviolette, Mario Marchand, John Shawe-Taylor, Spencer Charles Brubaker, Matthew D. Mullin</p><p>Abstract: Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classiﬁer achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classiﬁer achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassiﬁcations. Keywords: set covering machines, sample-compression, loss bounds</p><p>4 0.85224336 <a title="35-lda-4" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>Author: Niels Landwehr, Kristian Kersting, Luc De Raedt</p><p>Abstract: A novel relational learning approach that tightly integrates the na¨ve Bayes learning scheme with ı the inductive logic programming rule-learner FOIL is presented. In contrast to previous combinations that have employed na¨ve Bayes only for post-processing the rule sets, the presented approach ı employs the na¨ve Bayes criterion to guide its search directly. The proposed technique is impleı mented in the N FOIL and T FOIL systems, which employ standard na¨ve Bayes and tree augmented ı na¨ve Bayes models respectively. We show that these integrated approaches to probabilistic model ı and rule learning outperform post-processing approaches. They also yield signiﬁcantly more accurate models than simple rule learning and are competitive with more sophisticated ILP systems. Keywords: rule learning, na¨ve Bayes, statistical relational learning, inductive logic programming ı</p><p>5 0.52955973 <a title="35-lda-5" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>Author: Roland Nilsson, José M. Peña, Johan Björkegren, Jesper Tegnér</p><p>Abstract: We analyze two different feature selection problems: ﬁnding a minimal feature set optimal for classiﬁcation (MINIMAL - OPTIMAL) vs. ﬁnding all features relevant to the target variable (ALL RELEVANT ). The latter problem is motivated by recent applications within bioinformatics, particularly gene expression analysis. For both problems, we identify classes of data distributions for which there exist consistent, polynomial-time algorithms. We also prove that ALL - RELEVANT is much harder than MINIMAL - OPTIMAL and propose two consistent, polynomial-time algorithms. We argue that the distribution classes considered are reasonable in many practical cases, so that our results simplify feature selection in a wide range of machine learning tasks. Keywords: learning theory, relevance, classiﬁcation, Markov blanket, bioinformatics</p><p>6 0.52829909 <a title="35-lda-6" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>7 0.51992911 <a title="35-lda-7" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>8 0.50592417 <a title="35-lda-8" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>9 0.5034095 <a title="35-lda-9" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>10 0.49383429 <a title="35-lda-10" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>11 0.49005428 <a title="35-lda-11" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>12 0.48165682 <a title="35-lda-12" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>13 0.41198879 <a title="35-lda-13" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>14 0.41190231 <a title="35-lda-14" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>15 0.40807459 <a title="35-lda-15" href="./jmlr-2007-Large_Margin_Semi-supervised_Learning.html">44 jmlr-2007-Large Margin Semi-supervised Learning</a></p>
<p>16 0.40404028 <a title="35-lda-16" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>17 0.39348355 <a title="35-lda-17" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>18 0.39185384 <a title="35-lda-18" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>19 0.38809425 <a title="35-lda-19" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>20 0.37828964 <a title="35-lda-20" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
