<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-69" href="#">jmlr2007-69</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</h1>
<br/><p>Source: <a title="jmlr-2007-69-pdf" href="http://jmlr.org/papers/volume8/mahadevan07a/mahadevan07a.pdf">pdf</a></p><p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A speciﬁc instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a ﬁnal parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A speciﬁc instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystr¨ m extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker o sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are brieﬂy summarized at the end. Keywords: Markov decision processes, reinforcement learning, value function approximation, manifold learning, spectral graph theory</p><p>Reference: <a title="jmlr-2007-69-reference" href="../jmlr2007_reference/jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Introduction This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) (Puterman, 1994) where both the underlying representation or basis functions and (approximate) optimal policies within the (linear) span of these basis functions are simultaneously learned. [sent-10, score-0.378]
</p><p>2 In the simplest setting, the diffusion model is deﬁned by the combinatorial graph Laplacian matrix L = D − W , where W is a symmetrized weight matrix, and D is a diagonal matrix whose entries are the row sums of W . [sent-18, score-0.371]
</p><p>3 The similarity between value functions and the eigenvectors of the graph Laplacian sometimes can be remarkable, leading to a highly compact encoding (measured in terms of the number of basis functions needed to encode a value function). [sent-22, score-0.4]
</p><p>4 Laplacian basis functions can be used in conjunction with a standard “black box” parameter estimation method, such as Qlearning (Watkins, 1989) or least-squares policy iteration (LSPI) (Lagoudakis and Parr, 2003) to ﬁnd the best policy representable within the space of the chosen basis functions. [sent-23, score-0.595]
</p><p>5 The fundamental idea is to construct basis functions for solving MDPs by diagonalizing symmetric diffusion operators on an empirically learned graph representing the underlying state space. [sent-25, score-0.539]
</p><p>6 A key advantage of diffusion models is their simplicity: it can be signiﬁcantly easier to estimate a “weak” diffusion model, such as the undirected random walk Pr or the combinatorial Laplacian L, than to learn the true underlying transition matrix Pπ of a policy π. [sent-29, score-0.742]
</p><p>7 , 2005; Kveton and Hauskrecht, 2006), dynamically allocating new parametric basis functions based on state space trajectories (Kretchmar and Anderson, 1999), or generating basis functions using the Bellman error in approximating a speciﬁc value function (Keller et al. [sent-32, score-0.434]
</p><p>8 In this paper, basis functions are constructed from spectral analysis of diffusion operators where the resulting representations are constructed without explicitly taking rewards into account. [sent-42, score-0.367]
</p><p>9 Furthermore, basis functions constructed using spectral analysis reﬂect global geometric properties, such as bottlenecks and symmetries in state spaces, that are invariant across multiple MDPs on the same state (action) space. [sent-48, score-0.463]
</p><p>10 Since eigenvectors of the graph Laplacian form “global” basis functions whose support is the entire state space, each eigenvector induces a real-valued mapping over the state space. [sent-55, score-0.612]
</p><p>11 Figure 2 shows a set of samples produced by doing a random walk in the inverted pendulum task. [sent-74, score-0.477]
</p><p>12 Instead of placing basis functions uniformly in all regions of the state space, the proposed framework recovers the underlying manifold by building a graph based on the samples collected over a period of exploratory activity. [sent-76, score-0.4]
</p><p>13 The basis functions are then computed by diagonalizing a diffusion operator (the Laplacian) on the space of functions on the graph, and are thereby customized to the manifold represented by the state (action) space of a particular control task. [sent-77, score-0.607]
</p><p>14 The additional power obtained from knowledge of the underlying state space graph or manifold comes at a potentially signiﬁcant cost: the manifold representation needs to be learned, and furthermore, basis functions need to be computed from it. [sent-87, score-0.457]
</p><p>15 5  Angle  Figure 2: Left: Samples from a series of random walks in an inverted pendulum task. [sent-103, score-0.417]
</p><p>16 Section 4 describes a concrete instance of the RPI framework using least-squares policy iteration (LSPI) (Lagoudakis and Parr, 2003) as the underlying control learning method, and compares PVFs with two parametric bases—radial basis functions (RBFs) and polynomials—on small discrete MDPs. [sent-128, score-0.397]
</p><p>17 , 2007), multiscale diffusion wavelet bases (Mahadevan and Maggioni, 2006), and more elaborate diffusion models using directed graphs where actions are part of the representation (Johns and Mahadevan, 2007; Osentoski and Mahadevan, 2007). [sent-141, score-0.517]
</p><p>18 This process involves ﬁnding the eigenvectors of a symmetrized graph operator such as the graph Laplacian. [sent-149, score-0.365]
</p><p>19 In the simplest case of discrete MDPs, construct an undirected weighted graph G from D by connecting state i to state j if the pair (i, j) form temporally successive states ∈ S. [sent-195, score-0.429]
</p><p>20 The state action bases φ(s, a) can be generated from rows of this matrix by duplicating the state bases φ(s) |A| times, and setting all the elements of this vector to 0 except for the ones corresponding to the chosen action. [sent-200, score-0.591]
</p><p>21 , Q-learning or LSPI), ﬁnd an ε-optimal policy π that maximizes the action value function Qπ = Φwπ within the linear span of the bases Φ using the training data in D . [sent-204, score-0.38]
</p><p>22 • 20 eigenvectors corresponding to the smallest eigenvalues of L (duplicated 4 times, one set for each action) are chosen as the columns of the state action basis matrix Φ. [sent-215, score-0.461]
</p><p>23 Given a policy π : S → A mapping states to actions, its corresponding value function V π speciﬁes the expected long-term discounted sum of rewards received by the agent in any given state s when actions are chosen using the policy. [sent-255, score-0.435]
</p><p>24 Let Pπ represent an |S| × |S| transition matrix of a (deterministic) policy π : S → A mapping each state s ∈ S to a desired action a = π(s). [sent-262, score-0.457]
</p><p>25 The basis function matrix Φ is an |S| × k matrix, where each column is a particular basis function evaluated over the state space, and each row is the set of all possible basis functions evaluated on a particular state. [sent-281, score-0.494]
</p><p>26 , 2001), Our approach to the problem of control learning involves ﬁnding a suitable set of basis functions by diagonalizing a learned diffusion model from sample trajectories, and to use projections in the Hilbert space deﬁned by the diffusion model for policy evaluation and improvement. [sent-306, score-0.654]
</p><p>27 We ﬁrst introduce the Fourier approach of ﬁnding basis functions by diagonalization, and then describe how diffusion models are used as a substitute for transition models. [sent-307, score-0.352]
</p><p>28 Although transition matrices for general MDPs are not reversible, and their spectral analysis is more delicate, it will still be a useful starting point to understand diffusion matrices such as the graph Laplacian. [sent-316, score-0.423]
</p><p>29 Another way to express the above property is to write the transition matrix as a sum of projection matrices associated with each eigenvalue:  n  Pπ = ∑ λπ φπ (φπ )T , i i i i=1  where the eigenvectors φπ form a complete orthogonal basis (i. [sent-323, score-0.36]
</p><p>30 Since the basis matrix Φ spans all vectors on the i i i state space S, we can express the reward vector Rπ in terms of this basis as R π = Φ π απ ,  (2)  where απ is a vector of scalar weights. [sent-329, score-0.422]
</p><p>31 1 If the transition matrix Pπ and reward function Rπ are both known, one can of course construct basis functions by diagonalizing Pπ and choosing eigenvectors “out-of-order” (that is, pick eigenvectors with the largest βk coefﬁcients above). [sent-339, score-0.618]
</p><p>32 Figure 7: Top: A simple diffusion model given by an undirected unweighted graph connecting each state to neighbors that are reachable using a single (reversible) action. [sent-361, score-0.379]
</p><p>33 4 From Transition Matrices to Diffusion Models We now develop a line of analysis where a graph is induced from the state space of an MDP, by sampling from a policy such as a random walk. [sent-365, score-0.371]
</p><p>34 The random walk matrix Pr = D−1W is called a diffusion model because given any function f on t the underlying graph G, the powers of Pr f determine how quickly the random walk will “mix” and converge to the long term distribution (Chung, 1997). [sent-373, score-0.449]
</p><p>35 Hence, the random walk operator D−1W is similar to I − L , so both have the same eigenvalues, and the eigenvectors of the random walk operator are the eigenvectors of I − L point-wise multiplied 1 by D− 2 . [sent-402, score-0.522]
</p><p>36 We will describe a speciﬁc instantiation of the RPI framework described previously, which comprises of an outer loop for learning basis functions and an inner loop for estimating the optimal policy representable within the given set of basis functions. [sent-453, score-0.421]
</p><p>37 Each row of Φ speciﬁes all the basis functions for a particular state action pair (s, a), and each column represents the value of a particular basis function over all state action pairs. [sent-465, score-0.589]
</p><p>38 a (c) Optional basis adaptation step: Prune the basis matrix Φ by discarding basis functions (columns) whose coefﬁcients are smaller than µ. [sent-487, score-0.392]
</p><p>39 11 PVFs are evaluated along a number of dimensions, including the number of bases used, and its relative performance compared to parametric bases such as polynomials and radial basis functions. [sent-499, score-0.441]
</p><p>40 Figure 9 also compares the performance of PVFs with unit vector bases (table lookup), showing that PVFs with 25 bases closely tracks the performance of unit vector bases on this task. [sent-512, score-0.411]
</p><p>41 Figure 9 evaluates the effectiveness of polynomial bases and radial basis functions in the two room MDP. [sent-519, score-0.374]
</p><p>42 Many RL domains lead to factored representations where the state space is generated as the Cartesian product of the values of state variables (Boutilier et al. [sent-567, score-0.428]
</p><p>43 For the cylinder on the right, the eigenvectors were generated as the Kronecker product of the eigenvectors of the combinatorial Laplacian for a 10 state closed chain and a 5 state open chain. [sent-624, score-0.536]
</p><p>44 2 Factored Representation Policy Iteration for Structured Domains We derive the update rule for a factored form of RPI (and LSPI) for structured domains when the basis functions can be represented as Kronecker products of elementary basis functions on simpler state spaces. [sent-671, score-0.565]
</p><p>45 Basis functions are column eigenvectors of the diagonalized representation of a graph operator, whereas embeddings φ(s) are row vectors representing the ﬁrst k basis functions evaluated on state s. [sent-672, score-0.527]
</p><p>46 Right: a factored (combinatorial) Laplacian approximation using basis functions constructed by taking Kronecker products of basis functions for chain graphs (of length corresponding to row and column sizes). [sent-694, score-0.467]
</p><p>47 In a grid world with 106 states, “ﬂat” proto-value functions require k × 106 space and time proportional to (106 )3 to be computed, whereas the factored basis functions only require space k × 10 3 to store with much less computational cost to ﬁnd. [sent-705, score-0.375]
</p><p>48 17 The basis functions for the overall Blocker state space were computed as Kronecker products of the basis functions over each agent’s state space. [sent-718, score-0.496]
</p><p>49 The overall basis functions were then constructed as Kronecker products of Laplacian basis functions for each learned (irregular) state grid. [sent-721, score-0.394]
</p><p>50 Figure 16 compares the performance of the factored Laplacian bases with a set of radial basis functions (RBFs) for the ﬁrst Blocker domain (shown on the left in Figure 15). [sent-722, score-0.453]
</p><p>51 Left: the 3rd eigenvector of the Laplacian plotted on a set of samples (shown as ﬁlled dots) drawn from a random walk in the inverted pendulum ¨ domain, as well as its Nystr¨ m interpolated values. [sent-776, score-0.53]
</p><p>52 Compute the k “smoothest” eigenvectors of O on the sub-sampled graph Ds , and collect them as columns of the basis function matrix Φ, a |Ds | × k matrix. [sent-800, score-0.354]
</p><p>53 For example, in the mountain car task, around 700 samples are sufﬁcient to form the basis functions, whereas usually > 7000 samples are needed to learn a close to optimal policy. [sent-875, score-0.356]
</p><p>54 By “fully interleaved”, we mean that the overall learning run is divided into a set of discrete episodes of sample collection, basis construction, and policy learning. [sent-892, score-0.424]
</p><p>55 At the end of each episode, a set of additional samples is collected using either a random walk (off-policy) or the currently best performing policy (on-policy), and then basis functions are then recomputed and a new policy is learned. [sent-893, score-0.58]
</p><p>56 1 Three Control Tasks We explored the effectiveness and stability of proto-value functions in three continuous domains— the Acrobot task, the inverted pendulum task, and the mountain car task—that have long been viewed as benchmarks in the ﬁeld. [sent-897, score-0.72]
</p><p>57 The Inverted Pendulum: The inverted pendulum problem requires balancing a pendulum of unknown mass and length by applying force to the cart to which the pendulum is attached. [sent-899, score-0.883]
</p><p>58 The agent is given a reward of 0 as long as the absolute value of the angle of the pendulum does not exceed π/2. [sent-912, score-0.384]
</p><p>59 The maximum number of episodes the pendulum was allowed to balance was ﬁxed at 3000 steps. [sent-916, score-0.358]
</p><p>60 Mountain Car: The goal of the mountain car task is to get a simulated car to the top of a hill as quickly as possible (Sutton and Barto, 1998). [sent-918, score-0.382]
</p><p>61 One reason for this difference is the nature of the underlying manifold: the samples in the inverted pendulum are in a relatively narrow region around the 45 degree line. [sent-968, score-0.391]
</p><p>62 In contrast, the samples in the mountain car domain are distributed across a wider region of the state space. [sent-969, score-0.357]
</p><p>63 In the inverted pendulum and Acrobot domains, the initial state was always set the same, with the pole starting from the vertical position at rest, or the arm at rest. [sent-972, score-0.493]
</p><p>64 In the mountain car domain, however, starting the car from a position of rest at the bottom of the hill produced poorer results than starting from the bottom with the velocities initialized randomly. [sent-973, score-0.382]
</p><p>65 The effect of varying k was most pronounced in the inverted pendulum domain, with less tangible results in the mountain car and Acrobot domains. [sent-981, score-0.646]
</p><p>66 Note that in the inverted pendulum domain, the differences between k = 25 and k = 50 are negligible, and the corresponding runs tightly overlap. [sent-982, score-0.391]
</p><p>67 In the inverted pendulum domain, 10 PVFs was signiﬁcantly better than using 30 PVFs, but was closely matched by using 60 PVFs. [sent-986, score-0.391]
</p><p>68 In both the Acrobot and mountain car domains, the normalized Laplacian operator produced signiﬁcantly better results than the combinatorial Laplacian. [sent-990, score-0.366]
</p><p>69 However, in the inverted pendulum domain, the combinatorial Laplacian was better than the normalized Laplacian operator. [sent-991, score-0.441]
</p><p>70 Note that in both the Acrobot and mountain car domains, the manifold is signiﬁcantly more spread out spatially than the inverted pendulum task. [sent-993, score-0.703]
</p><p>71 If the policy learned in the current round of RPI improved on the best-performing policy thus far, samples were collected in the next iteration of RPI using the newly learned policy (which was then viewed as the best performing policy in subsequent runs). [sent-997, score-0.696]
</p><p>72 For the Acrobot domain, the number of PVFs was set at 100, whereas in the mountain car and inverted pendulum tasks, the number of PVFs was set to 30. [sent-1003, score-0.646]
</p><p>73 The number of nearest neighbors k = 25 in the Acrobot and inverted pendulum domains, and k = 30 in the mountain car domain. [sent-1007, score-0.646]
</p><p>74 Figure 24 shows the results of these two modiﬁcations in the Acrobot domain, whereas Figure 25 and Figure 27 show the corresponding results from the inverted pendulum and mountain car domains. [sent-1008, score-0.646]
</p><p>75 4 Comparing PVFs with RBFs on Continuous MDPs In this section, we compare the performance of PVFs with radial basis functions (RBFs), which are a popular choice of basis functions for both discrete and continuous MDPS. [sent-1011, score-0.384]
</p><p>76 We restrict our comparison of PVFs and RBFs in this section to the inverted pendulum and mountain car domains. [sent-1012, score-0.646]
</p><p>77 In the Acrobot task, 100 PVFs were used, whereas 30 basis functions were used in the mountain car task, and 10 basis functions were used in the inverted pendulum task. [sent-1015, score-0.938]
</p><p>78 choose a suitable set of parameters for RBFs, we initially relied on the values chosen in the published study of LSPI for the inverted pendulum domain (Lagoudakis and Parr, 2003). [sent-1016, score-0.391]
</p><p>79 Generally speaking, the results demonstrate that PVFs are signiﬁcantly quicker to converge, by almost a factor of two in both the inverted pendulum and mountain car domains. [sent-1019, score-0.646]
</p><p>80 Inverted Pendulum: We begin by comparing the performance of PVFs with a linear RBF approximation architecture for the inverted pendulum domain. [sent-1027, score-0.391]
</p><p>81 Figure 25 plots the effect of varying the kernel width for RBFs in the inverted pendulum domain (left plot). [sent-1028, score-0.391]
</p><p>82 Mountain Car: As with the inverted pendulum, we were able to improve the performance of RBFs by ﬁne-tuning the kernel width, although the differences are less signiﬁcant than in the inverted pendulum domain. [sent-1034, score-0.536]
</p><p>83 Figure 27 plots the effect of varying the kernel width for RBFs using 13 basis functions in the mountain car domain (left plot). [sent-1035, score-0.401]
</p><p>84 However, as with the inverted pendulum results, PVFs converge signiﬁcantly quicker, and clearly outperform RBFs for smaller numbers of samples. [sent-1039, score-0.391]
</p><p>85 As in the inverted pendulum, we note that PVFs clearly converge more quickly to a more stable performance than RBFs, although the differences are not as dramatic as in the inverted pendulum domain. [sent-1041, score-0.536]
</p><p>86 3θ2 )  Table 1: Parameter values (as deﬁned in Figure 18) for Acrobot, inverted pendulum and mountain car domains. [sent-1050, score-0.646]
</p><p>87 Right: A comparison of 15 PVFs with several choices of RBFs on the inverted pendulum task, focusing on the initial 100 episodes averaged over 100 runs. [sent-1058, score-0.503]
</p><p>88 05  Table 2: RBF parameter settings for inverted pendulum and mountain car experiments. [sent-1076, score-0.646]
</p><p>89 For example, Petrik (2007) proposes combining reward-speciﬁc Krylov bases with Laplacian bases as a way of integrating localized high-frequency reward-speciﬁc bases with more global long-term eigenvector bases such as PVFs. [sent-1137, score-0.601]
</p><p>90 It is clear from the experiments presented in Section 7 that RPI converges extremely quickly in problems like the inverted pendulum, whereas in other problems such as the mountain car or Acrobot, convergence takes signiﬁcantly longer. [sent-1148, score-0.4]
</p><p>91 , 2007), we have been able to signiﬁcantly reduce the size of the random walk weight matrices for the inverted pendulum, mountain car, and the Acrobot tasks with modest loss in performance compared to the full matrix. [sent-1183, score-0.392]
</p><p>92 For example, in the Acrobot task, the original basis matrix is compressed by a factor of 36 : 1, which resulted in a policy slightly worse than the original larger basis matrix. [sent-1184, score-0.42]
</p><p>93 In Mahadevan and Maggioni (2006) we compare the performance of diffusion wavelet bases and Laplacian bases on a variety of simple MDPs. [sent-1197, score-0.412]
</p><p>94 In Maggioni and Mahadevan (2006), we present an efﬁcient direct method for policy evaluation by using the multiscale diffusion bases to invert the Bellman matrix I − γP π . [sent-1198, score-0.493]
</p><p>95 One recent approach studied in Petrik (2007) assumes that the reward function Rπ and policy transition matrix Pπ are known, and combines Laplacian PVF bases with Krlyov bases. [sent-1203, score-0.497]
</p><p>96 While making bases sensitive to rewards can lead to superior results, if the reward function or policy is modiﬁed, reward-sensitive basis functions would need to be re-learned. [sent-1218, score-0.531]
</p><p>97 7 Learning State Action PVFs In our paper, the basis functions φ(s) are originally deﬁned over states, and then extended to state action pairs φ(s, a) by duplicating the state embedding |A| times and “zeroing” out elements of the state-action embedding corresponding to actions not taken. [sent-1221, score-0.465]
</p><p>98 State action graphs are naturally highly directional, and we used the directed Laplacian to compute basis functions over state action graphs. [sent-1226, score-0.444]
</p><p>99 In particular, many of the continuous MDPs we studied, including the inverted pendulum and the Acrobot, deﬁne continuous manifolds that have been extensively studied in mathematics (Baker, 2001) and robotics (Lavalle, 2006). [sent-1232, score-0.449]
</p><p>100 discrete graph Laplacian operator I − D Ravindran and Barto (2003) explore the use of group homomorphisms on state action spaces to abstract semi-MDPs, which can be combined with PVFs as a way of solving large SMDPs. [sent-1249, score-0.364]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pvfs', 0.444), ('laplacian', 0.308), ('pendulum', 0.246), ('rpi', 0.201), ('rbfs', 0.177), ('policy', 0.174), ('mdps', 0.155), ('acrobot', 0.148), ('inverted', 0.145), ('factored', 0.144), ('diffusion', 0.138), ('bases', 0.137), ('mountain', 0.128), ('aggioni', 0.127), ('arkov', 0.127), ('ontrol', 0.127), ('car', 0.127), ('kronecker', 0.126), ('eigenvectors', 0.114), ('mdp', 0.114), ('episodes', 0.112), ('ecision', 0.108), ('ahadevan', 0.108), ('epresentation', 0.108), ('state', 0.102), ('basis', 0.101), ('rocesses', 0.096), ('graph', 0.095), ('mahadevan', 0.09), ('walk', 0.086), ('reward', 0.074), ('lspi', 0.074), ('action', 0.069), ('transition', 0.068), ('maggioni', 0.066), ('room', 0.065), ('agent', 0.064), ('operator', 0.061), ('parr', 0.06), ('diagonalizing', 0.058), ('manifold', 0.057), ('spectral', 0.056), ('blocker', 0.053), ('lagoudakis', 0.053), ('eigenvector', 0.053), ('combinatorial', 0.05), ('proto', 0.049), ('states', 0.049), ('actions', 0.046), ('functions', 0.045), ('undirected', 0.044), ('matrix', 0.044), ('subsampling', 0.043), ('eigenfunctions', 0.043), ('petrik', 0.041), ('smoothest', 0.041), ('bellman', 0.04), ('earning', 0.04), ('grid', 0.04), ('barto', 0.04), ('parametric', 0.04), ('reinforcement', 0.038), ('nystr', 0.038), ('nystrom', 0.038), ('chung', 0.037), ('discrete', 0.037), ('st', 0.036), ('pr', 0.035), ('matrices', 0.033), ('osentoski', 0.033), ('pvf', 0.033), ('walls', 0.033), ('graphs', 0.031), ('rbf', 0.031), ('eigenvalues', 0.031), ('eigenvalue', 0.031), ('policies', 0.03), ('factorization', 0.029), ('bottlenecks', 0.029), ('diagonalizable', 0.029), ('sigma', 0.029), ('continuous', 0.029), ('cylinder', 0.028), ('coifman', 0.028), ('approximators', 0.028), ('symmetries', 0.028), ('fourier', 0.027), ('directed', 0.027), ('domains', 0.027), ('representations', 0.027), ('product', 0.026), ('steps', 0.026), ('radial', 0.026), ('transitions', 0.026), ('walks', 0.026), ('markov', 0.025), ('vol', 0.025), ('embeddings', 0.025), ('blockers', 0.025), ('cheeger', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="69-tfidf-1" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A speciﬁc instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a ﬁnal parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A speciﬁc instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystr¨ m extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker o sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are brieﬂy summarized at the end. Keywords: Markov decision processes, reinforcement learning, value function approximation, manifold learning, spectral graph theory</p><p>2 0.18580393 <a title="69-tfidf-2" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Matthias Hein, Jean-Yves Audibert, Ulrike von Luxburg</p><p>Abstract: Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. Keywords: graphs, graph Laplacians, semi-supervised learning, spectral clustering, dimensionality reduction</p><p>3 0.131504 <a title="69-tfidf-3" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>4 0.13033883 <a title="69-tfidf-4" href="./jmlr-2007-Hierarchical_Average_Reward_Reinforcement_Learning.html">41 jmlr-2007-Hierarchical Average Reward Reinforcement Learning</a></p>
<p>Author: Mohammad Ghavamzadeh, Sridhar Mahadevan</p><p>Abstract: Hierarchical reinforcement learning (HRL) is a general framework for scaling reinforcement learning (RL) to problems with large state and action spaces by using the task (or action) structure to restrict the space of policies. Prior work in HRL including HAMs, options, MAXQ, and PHAMs has been limited to the discrete-time discounted reward semi-Markov decision process (SMDP) model. The average reward optimality criterion has been recognized to be more appropriate for a wide class of continuing tasks than the discounted framework. Although average reward RL has been studied for decades, prior work has been largely limited to ﬂat policy representations. In this paper, we develop a framework for HRL based on the average reward optimality criterion. We investigate two formulations of HRL based on the average reward SMDP model, both for discrete-time and continuous-time. These formulations correspond to two notions of optimality that have been previously explored in HRL: hierarchical optimality and recursive optimality. We present algorithms that learn to ﬁnd hierarchically and recursively optimal average reward policies under discrete-time and continuous-time average reward SMDP models. We use two automated guided vehicle (AGV) scheduling tasks as experimental testbeds to study the empirical performance of the proposed algorithms. The ﬁrst problem is a relatively simple AGV scheduling task, in which the hierarchically and recursively optimal policies are different. We compare the proposed algorithms with three other HRL methods, including a hierarchically optimal discounted reward algorithm and a recursively optimal discounted reward algorithm on this problem. The second problem is a larger AGV scheduling task. We model this problem using both discrete-time and continuous-time models. We use a hierarchical task decomposition in which the hierarchically and recursively optimal policies are the same for this problem. We compare the performance of the proposed algorithms</p><p>5 0.09287028 <a title="69-tfidf-5" href="./jmlr-2007-Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning.html">85 jmlr-2007-Transfer Learning via Inter-Task Mappings for Temporal Difference Learning</a></p>
<p>Author: Matthew E. Taylor, Peter Stone, Yaxin Liu</p><p>Abstract: Temporal difference (TD) learning (Sutton and Barto, 1998) has become a popular reinforcement learning technique in recent years. TD methods, relying on function approximators to generalize learning to novel situations, have had some experimental successes and have been shown to exhibit some desirable properties in theory, but the most basic algorithms have often been found slow in practice. This empirical result has motivated the development of many methods that speed up reinforcement learning by modifying a task for the learner or helping the learner better generalize to novel situations. This article focuses on generalizing across tasks, thereby speeding up learning, via a novel form of transfer using handcoded task relationships. We compare learning on a complex task with three function approximators, a cerebellar model arithmetic computer (CMAC), an artiﬁcial neural network (ANN), and a radial basis function (RBF), and empirically demonstrate that directly transferring the action-value function can lead to a dramatic speedup in learning with all three. Using transfer via inter-task mapping (TVITM), agents are able to learn one task and then markedly reduce the time it takes to learn a more complex task. Our algorithms are fully implemented and tested in the RoboCup soccer Keepaway domain. This article contains and extends material published in two conference papers (Taylor and Stone, 2005; Taylor et al., 2005). Keywords: transfer learning, reinforcement learning, temporal difference methods, value function approximation, inter-task mapping</p><p>6 0.057647649 <a title="69-tfidf-6" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>7 0.055433065 <a title="69-tfidf-7" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>8 0.049705792 <a title="69-tfidf-8" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>9 0.04755548 <a title="69-tfidf-9" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>10 0.043722712 <a title="69-tfidf-10" href="./jmlr-2007-A_Complete_Characterization_of_a_Family_of_Solutions_to_a_Generalized_Fisher_Criterion.html">2 jmlr-2007-A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion</a></p>
<p>11 0.04281126 <a title="69-tfidf-11" href="./jmlr-2007-From_External_to_Internal_Regret_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">34 jmlr-2007-From External to Internal Regret     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>12 0.042672113 <a title="69-tfidf-12" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>13 0.039894983 <a title="69-tfidf-13" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>14 0.037816159 <a title="69-tfidf-14" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>15 0.036991276 <a title="69-tfidf-15" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>16 0.034074076 <a title="69-tfidf-16" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>17 0.03207849 <a title="69-tfidf-17" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>18 0.029863916 <a title="69-tfidf-18" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>19 0.02690565 <a title="69-tfidf-19" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>20 0.026549233 <a title="69-tfidf-20" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.208), (1, 0.025), (2, 0.028), (3, 0.026), (4, -0.451), (5, 0.043), (6, 0.228), (7, 0.122), (8, 0.145), (9, 0.176), (10, 0.182), (11, -0.173), (12, 0.015), (13, -0.028), (14, 0.107), (15, 0.017), (16, -0.1), (17, -0.016), (18, 0.144), (19, 0.005), (20, -0.087), (21, -0.066), (22, -0.045), (23, -0.061), (24, -0.015), (25, 0.004), (26, -0.04), (27, -0.009), (28, -0.003), (29, 0.04), (30, 0.035), (31, -0.016), (32, 0.119), (33, -0.042), (34, -0.017), (35, -0.015), (36, 0.028), (37, 0.006), (38, 0.005), (39, -0.041), (40, 0.005), (41, -0.038), (42, -0.02), (43, 0.009), (44, 0.039), (45, -0.021), (46, -0.051), (47, 0.023), (48, 0.023), (49, -0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96592832 <a title="69-lsi-1" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A speciﬁc instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a ﬁnal parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A speciﬁc instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystr¨ m extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker o sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are brieﬂy summarized at the end. Keywords: Markov decision processes, reinforcement learning, value function approximation, manifold learning, spectral graph theory</p><p>2 0.71279711 <a title="69-lsi-2" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Matthias Hein, Jean-Yves Audibert, Ulrike von Luxburg</p><p>Abstract: Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. Keywords: graphs, graph Laplacians, semi-supervised learning, spectral clustering, dimensionality reduction</p><p>3 0.60734564 <a title="69-lsi-3" href="./jmlr-2007-Hierarchical_Average_Reward_Reinforcement_Learning.html">41 jmlr-2007-Hierarchical Average Reward Reinforcement Learning</a></p>
<p>Author: Mohammad Ghavamzadeh, Sridhar Mahadevan</p><p>Abstract: Hierarchical reinforcement learning (HRL) is a general framework for scaling reinforcement learning (RL) to problems with large state and action spaces by using the task (or action) structure to restrict the space of policies. Prior work in HRL including HAMs, options, MAXQ, and PHAMs has been limited to the discrete-time discounted reward semi-Markov decision process (SMDP) model. The average reward optimality criterion has been recognized to be more appropriate for a wide class of continuing tasks than the discounted framework. Although average reward RL has been studied for decades, prior work has been largely limited to ﬂat policy representations. In this paper, we develop a framework for HRL based on the average reward optimality criterion. We investigate two formulations of HRL based on the average reward SMDP model, both for discrete-time and continuous-time. These formulations correspond to two notions of optimality that have been previously explored in HRL: hierarchical optimality and recursive optimality. We present algorithms that learn to ﬁnd hierarchically and recursively optimal average reward policies under discrete-time and continuous-time average reward SMDP models. We use two automated guided vehicle (AGV) scheduling tasks as experimental testbeds to study the empirical performance of the proposed algorithms. The ﬁrst problem is a relatively simple AGV scheduling task, in which the hierarchically and recursively optimal policies are different. We compare the proposed algorithms with three other HRL methods, including a hierarchically optimal discounted reward algorithm and a recursively optimal discounted reward algorithm on this problem. The second problem is a larger AGV scheduling task. We model this problem using both discrete-time and continuous-time models. We use a hierarchical task decomposition in which the hierarchically and recursively optimal policies are the same for this problem. We compare the performance of the proposed algorithms</p><p>4 0.60142368 <a title="69-lsi-4" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>5 0.47318968 <a title="69-lsi-5" href="./jmlr-2007-Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning.html">85 jmlr-2007-Transfer Learning via Inter-Task Mappings for Temporal Difference Learning</a></p>
<p>Author: Matthew E. Taylor, Peter Stone, Yaxin Liu</p><p>Abstract: Temporal difference (TD) learning (Sutton and Barto, 1998) has become a popular reinforcement learning technique in recent years. TD methods, relying on function approximators to generalize learning to novel situations, have had some experimental successes and have been shown to exhibit some desirable properties in theory, but the most basic algorithms have often been found slow in practice. This empirical result has motivated the development of many methods that speed up reinforcement learning by modifying a task for the learner or helping the learner better generalize to novel situations. This article focuses on generalizing across tasks, thereby speeding up learning, via a novel form of transfer using handcoded task relationships. We compare learning on a complex task with three function approximators, a cerebellar model arithmetic computer (CMAC), an artiﬁcial neural network (ANN), and a radial basis function (RBF), and empirically demonstrate that directly transferring the action-value function can lead to a dramatic speedup in learning with all three. Using transfer via inter-task mapping (TVITM), agents are able to learn one task and then markedly reduce the time it takes to learn a more complex task. Our algorithms are fully implemented and tested in the RoboCup soccer Keepaway domain. This article contains and extends material published in two conference papers (Taylor and Stone, 2005; Taylor et al., 2005). Keywords: transfer learning, reinforcement learning, temporal difference methods, value function approximation, inter-task mapping</p><p>6 0.21433432 <a title="69-lsi-6" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>7 0.18706265 <a title="69-lsi-7" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>8 0.17230099 <a title="69-lsi-8" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>9 0.17210694 <a title="69-lsi-9" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>10 0.17210363 <a title="69-lsi-10" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>11 0.1710062 <a title="69-lsi-11" href="./jmlr-2007-A_Complete_Characterization_of_a_Family_of_Solutions_to_a_Generalized_Fisher_Criterion.html">2 jmlr-2007-A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion</a></p>
<p>12 0.16568142 <a title="69-lsi-12" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>13 0.16175827 <a title="69-lsi-13" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>14 0.15604912 <a title="69-lsi-14" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>15 0.15498115 <a title="69-lsi-15" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>16 0.14663069 <a title="69-lsi-16" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>17 0.13528965 <a title="69-lsi-17" href="./jmlr-2007-From_External_to_Internal_Regret_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">34 jmlr-2007-From External to Internal Regret     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>18 0.13400547 <a title="69-lsi-18" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>19 0.13182297 <a title="69-lsi-19" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>20 0.12013432 <a title="69-lsi-20" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.031), (8, 0.035), (10, 0.022), (12, 0.084), (14, 0.011), (15, 0.018), (28, 0.039), (30, 0.365), (40, 0.033), (45, 0.013), (48, 0.042), (60, 0.022), (80, 0.017), (85, 0.059), (98, 0.111)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73308492 <a title="69-lda-1" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A speciﬁc instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a ﬁnal parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A speciﬁc instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystr¨ m extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker o sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are brieﬂy summarized at the end. Keywords: Markov decision processes, reinforcement learning, value function approximation, manifold learning, spectral graph theory</p><p>2 0.40120107 <a title="69-lda-2" href="./jmlr-2007-Dynamics_and_Generalization_Ability_of_LVQ_Algorithms.html">30 jmlr-2007-Dynamics and Generalization Ability of LVQ Algorithms</a></p>
<p>Author: Michael Biehl, Anarta Ghosh, Barbara Hammer</p><p>Abstract: Learning vector quantization (LVQ) schemes constitute intuitive, powerful classiﬁcation heuristics with numerous successful applications but, so far, limited theoretical background. We study LVQ rigorously within a simplifying model situation: two competing prototypes are trained from a sequence of examples drawn from a mixture of Gaussians. Concepts from statistical physics and the theory of on-line learning allow for an exact description of the training dynamics in highdimensional feature space. The analysis yields typical learning curves, convergence properties, and achievable generalization abilities. This is also possible for heuristic training schemes which do not relate to a cost function. We compare the performance of several algorithms, including Kohonen’s LVQ1 and LVQ+/-, a limiting case of LVQ2.1. The former shows close to optimal performance, while LVQ+/- displays divergent behavior. We investigate how early stopping can overcome this difﬁculty. Furthermore, we study a crisp version of robust soft LVQ, which was recently derived from a statistical formulation. Surprisingly, it exhibits relatively poor generalization. Performance improves if a window for the selection of data is introduced; the resulting algorithm corresponds to cost function based LVQ2. The dependence of these results on the model parameters, for example, prior class probabilities, is investigated systematically, simulations conﬁrm our analytical ﬁndings. Keywords: prototype based classiﬁcation, learning vector quantization, Winner-Takes-All algorithms, on-line learning, competitive learning</p><p>3 0.39580825 <a title="69-lda-3" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>4 0.38607836 <a title="69-lda-4" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>Author: Guy Lebanon, Yi Mao, Joshua Dillon</p><p>Abstract: The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efﬁcient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks. Keywords: text processing, local smoothing</p><p>5 0.37566018 <a title="69-lda-5" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Matthias Hein, Jean-Yves Audibert, Ulrike von Luxburg</p><p>Abstract: Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. Keywords: graphs, graph Laplacians, semi-supervised learning, spectral clustering, dimensionality reduction</p><p>6 0.37286726 <a title="69-lda-6" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>7 0.37061894 <a title="69-lda-7" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>8 0.37059188 <a title="69-lda-8" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>9 0.36704975 <a title="69-lda-9" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>10 0.36681312 <a title="69-lda-10" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>11 0.36636326 <a title="69-lda-11" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>12 0.36628819 <a title="69-lda-12" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>13 0.3644852 <a title="69-lda-13" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>14 0.3639085 <a title="69-lda-14" href="./jmlr-2007-Structure_and_Majority_Classes_in_Decision_Tree_Learning.html">79 jmlr-2007-Structure and Majority Classes in Decision Tree Learning</a></p>
<p>15 0.36211771 <a title="69-lda-15" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>16 0.36116877 <a title="69-lda-16" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>17 0.36003822 <a title="69-lda-17" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>18 0.3596848 <a title="69-lda-18" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>19 0.35913992 <a title="69-lda-19" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>20 0.35804278 <a title="69-lda-20" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
