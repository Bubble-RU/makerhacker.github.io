<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-65" href="#">jmlr2007-65</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</h1>
<br/><p>Source: <a title="jmlr-2007-65-pdf" href="http://jmlr.org/papers/volume8/laviolette07a/laviolette07a.pdf">pdf</a></p><p>Author: François Laviolette, Mario Marchand</p><p>Abstract: We propose a PAC-Bayes theorem for the sample-compression setting where each classiﬁer is described by a compression subset of the training data and a message string of additional information. This setting, which is the appropriate one to describe many learning algorithms, strictly generalizes the usual data-independent setting where classiﬁers are represented only by data-independent message strings (or parameters taken from a continuous set). The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classiﬁer vanishes. For posteriors having all their weights on a single sample-compressed classiﬁer, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. (2005). Finally, we extend our results to the case where each sample-compressed classiﬁer of a data-dependent ensemble may abstain of predicting a class label. Keywords: PAC-Bayes, risk bounds, sample-compression, set covering machines, decision list machines</p><p>Reference: <a title="jmlr-2007-65-reference" href="../jmlr2007_reference/jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Warmuth  Abstract We propose a PAC-Bayes theorem for the sample-compression setting where each classiﬁer is described by a compression subset of the training data and a message string of additional information. [sent-8, score-0.336]
</p><p>2 The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classiﬁer vanishes. [sent-10, score-0.253]
</p><p>3 For posteriors having all their weights on a single sample-compressed classiﬁer, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. [sent-11, score-0.258]
</p><p>4 A remarkable result that came out from this line of research, known as the “PAC-Bayes theorem”, provides a tight upper bound on the risk of a stochastic classiﬁer (deﬁned on the posterior Q) called the Gibbs classiﬁer. [sent-17, score-0.263]
</p><p>5 Consequently, a Gibbs classiﬁer with a posterior Q having all its weight on a single classiﬁer will have a larger risk bound than another Gibbs classiﬁer, making the same amount of training errors, using a “broader” posterior Q that gives weight to many classiﬁers. [sent-21, score-0.318]
</p><p>6 In the samplecompression setting, each classiﬁer is represented by two independent sources of information: a compression set which consists of a small subset of the training data, and a message string of the additional information needed to obtain a classiﬁer. [sent-31, score-0.308]
</p><p>7 In the limit where the compression set vanishes, each classiﬁer is identiﬁed only by a message string and the new PAC-Bayes theorem reduces to the “usual” PAC-Bayes theorem of Seeger (2002) and Langford (2005). [sent-32, score-0.362]
</p><p>8 However, new quantities appear in the risk bound when classiﬁers are also described by their compression sets. [sent-33, score-0.281]
</p><p>9 Nevertheless, in the limit where the posterior Q puts all its weight on a single samplecompressed classiﬁer, the new PAC-Bayes risk bound reduces to a bound similar to the tight samplecompression bound of Laviolette et al. [sent-35, score-0.369]
</p><p>10 In that section, we also present a bound for the “intermediate” case where the posterior has all its weight on a single compression sequence 1462  S AMPLE -C OMPRESSED PAC-BAYES B OUNDS  and non-zero weight on several messages. [sent-50, score-0.335]
</p><p>11 In Section 7, we provide an alternative formulation of Theorem 3 by including the training errors into the compression sequence. [sent-53, score-0.179]
</p><p>12 , (xm , ym ) of m examples, the empirical risk RS ( f ) on S, of any classiﬁer f , is deﬁned according to def  RS ( f ) =  1 m def E ∑ I( f (xi ) = yi ) = (x,y)∼S I( f (x) = y) . [sent-67, score-0.304]
</p><p>13 The PAC-Bayes Theorem in the Data-Independent Setting The PAC-Bayes theorem provides tight upper and lower bounds on the risk of a stochastic classiﬁer called the Gibbs classiﬁer. [sent-77, score-0.215]
</p><p>14 The risk of G Q is deﬁned as the expected risk of classiﬁers drawn according to Q. [sent-80, score-0.168]
</p><p>15 Similarly, the empirical risk RS (GQ ) of GQ , on a training sequence S of m examples, is given by def  RS (GQ ) = E RS (h) = E h∼Q  h∼Q  1 m ∑ I(h(xi ) = yi ) . [sent-82, score-0.253]
</p><p>16 p 1− p  It is rarely mentioned that this theorem provides both an upper bound and a lower bound on the true risk R(GQ ) based on its empirical risk RS (GQ ). [sent-87, score-0.309]
</p><p>17 With probability at least 1 − δ over the random draws of S, R(GQ ) is upper-bounded by sup R : kl(RS (GQ ) R) ≤  m+1 1 KL(Q P) + ln m δ  and lower-bounded by inf R : kl(RS (GQ ) R) ≤  1 m+1 KL(Q P) + ln m δ  . [sent-88, score-0.234]
</p><p>18 The upper bound given by the PAC-Bayes theorem for the risk of Gibbs classiﬁers can be turned into an upper bound for the risk of majority-vote classiﬁers (often called Bayes classiﬁers) in the following way. [sent-91, score-0.322]
</p><p>19 Finally, for certain distributions Q, a bound for R(BQ ) can be turned into a bound for the risk of a single classiﬁer whenever there exists h∗ ∈ H such that h∗ (x) = BQ (x) ∀x ∈ X . [sent-100, score-0.164]
</p><p>20 , zm of m examples, each classiﬁer is described entirely by two complementary sources of information: a subsequence S i of S, called the compression sequence, and a message σ which represents the additional information needed to obtain a classiﬁer from the compression sequence. [sent-109, score-0.416]
</p><p>21 Given a training sequence S of m examples, the compression subsequence S i of S is deﬁned by the following vector i of indices def  i = (i1 , i2 , . [sent-110, score-0.368]
</p><p>22 For any sequence S of m examples, we will also use def [ MS = M (Si ) . [sent-126, score-0.147]
</p><p>23 1465  1 2  + ε, then R(BQ ) = 2R(GQ ) − 2ε for  L AVIOLETTE AND M ARCHAND  The perceptron learning rule and the SVM are examples of learning algorithms where the ﬁnal classiﬁer can be reconstructed solely from a compression sequence (Graepel et al. [sent-129, score-0.215]
</p><p>24 In contrast, the reconstruction functions for the set covering machine (Marchand and Shawe-Taylor, 2002) and the decision list machine (Marchand and Sokolova, 2005) need both a compression sequence and a message string. [sent-131, score-0.318]
</p><p>25 Furthermore, Marchand and Sokolova (2005) provide numerous examples where it is advantageous to have a set M (Si ) of possible messages that depend on the compression sequence Si . [sent-132, score-0.264]
</p><p>26 Indeed, we recover this usual setting def / when each classiﬁer is identiﬁed only by a message σ taken from a set M = M (0). [sent-136, score-0.21]
</p><p>27 We consider learning algorithms that output a posterior distribution Q on I × M S after observing some training sequence S. [sent-149, score-0.145]
</p><p>28 Consequently, the set of messages M (S i ) must be deﬁned a priori for any compression sequence Si (we will provide examples in the next section). [sent-153, score-0.279]
</p><p>29 As we will precisely see later, the allowed dependence on Si of the prior comes from the fact that the empirical risk of the classiﬁers will be computed only on the examples of S that are not in the compression sequence S i . [sent-156, score-0.306]
</p><p>30 However, since the risk upper bound will d=0 deteriorate as we put more weight on classiﬁers with large compression sizes |i|, it will be preferable to choose a function ζ(d) that puts more weight on small values of d. [sent-158, score-0.324]
</p><p>31 Recall that Si is the set of training examples which are not in the compression set S i . [sent-161, score-0.179]
</p><p>32 Indeed, it will become obvious that the bound on the risk of classiﬁer R (σ, S i ) depends only on its empirical risk on Si . [sent-162, score-0.208]
</p><p>33 Therefore, given a training sequence S and a distribution Q, the true risk R(G Q ) of the samplecompressed Gibbs classiﬁer GQ is given by R(GQ ) = E  E  R(σ, Si ) . [sent-164, score-0.178]
</p><p>34 i∼QI σ∼QM (Si )  Furthermore, its empirical risk RS (GQ ) is given by RS (GQ ) = E  i∼QI σ∼QM (Si )  Note that these expectations are deﬁned only within the context of a training sequence S. [sent-166, score-0.143]
</p><p>35 Deﬁnition 2 Given a distribution Q on I × MS , we will denote by QI the distribution deﬁned as def  QI (i) =  QI (i) 1 |i| E |i|  ∀i ∈ I ,  (3)  i∼QI  def  where |i| = m − |i|. [sent-168, score-0.22]
</p><p>36 Theorem 3 For any δ ∈ (0, 1], for any reconstruction function mapping compression sequences and messages to classiﬁers, for any T ∈ (X × Y )m and for any prior PT on I × MT , we have Pr  S∼Dm  ∀Q on I × MS : kl(RS (GQ ) R(GQ )) ≤  1 m+1 KL(Q PS ) + ln m − dQ δ  ≥ 1−δ. [sent-182, score-0.433]
</p><p>37 Similarly as Theorem 1, Theorem 3 provides both an upper bound and a lower bound on the true risk R(GQ ) based on the empirical risk RS (GQ ). [sent-183, score-0.261]
</p><p>38 Note that KL(Q PS ) = =  E  E  i∼QI σ∼QM (Si )  ln  QI (i)QM (Si ) (σ) PI (i)PM (Si ) (σ)  Q (i) + E E ln I PI (i) i∼QI i∼QI  E  σ∼QM (S ) i  ln  QM (Si ) (σ) PM (Si )  = KL(QI PI ) + E KL(QM (Si ) PM (Si ) ) . [sent-184, score-0.351]
</p><p>39 i∼QI  Although we must deﬁne a priori a continuous family of priors (one prior PT on I × MT per possible sequence T ∈ (X × Y )m ), only the prior on the observed training sequence S will contribute to the bounds. [sent-185, score-0.181]
</p><p>40 1468  S AMPLE -C OMPRESSED PAC-BAYES B OUNDS  Note also that, when QI is non-zero only for one compression size |i| = d, we have Q I = QI and dQ = d. [sent-188, score-0.157]
</p><p>41 Hence, for a stochastic average of sample-compressed classiﬁers of ﬁxed compression size d, the risk bounds depend only on the “original” posterior Q I . [sent-189, score-0.358]
</p><p>42 More generally, note that QI (i) is smaller than QI (i) for classiﬁers having a compression size |i| smaller than the Q-average. [sent-190, score-0.157]
</p><p>43 This, combined with the fact that KL(Q PS ) favors Q’s close to PS , implies that there will be a specialization performed by Q on classiﬁers having small compression sizes. [sent-191, score-0.157]
</p><p>44 Note also that, since R(BQ ) ≤ 2R(GQ ), Theorem 3 provides an upper bound for the true risk of the (deterministic) majority vote BQ . [sent-195, score-0.157]
</p><p>45 Consider, for example, a majority vote of m classiﬁers, each having a compression size |i| = 1. [sent-196, score-0.177]
</p><p>46 Lemma 6 For any δ ∈ (0, 1], for any reconstruction function mapping compression sequences and messages to classiﬁers, for any T ∈ (X × Y )m and for any prior PT on I × MT , we have ∀Q on I × MS :  Pr  S∼Dm  E  1 1 ln BS (i, σ) |i|  E  i∼QI σ∼QM (Si )  1 m+1 KL(Q PS ) + ln m − dQ δ  ≤  ≥ 1−δ. [sent-211, score-0.55]
</p><p>47 Proof Lemma 5 gives us Pr m  S∼D  ln  E  E  i∼PI σ∼PM (Si )  1 BS (i, σ)  ≤ ln  m+1 δ  ≥ 1−δ. [sent-212, score-0.234]
</p><p>48 BS (i, σ)  Consequently, we have Pr m  S∼D  ∀Q on I × MS : E  E  i∼QI σ∼QM (Si )  ln  1 m+1 ≤ KL(Q PS ) + ln BS (i, σ) δ  ≥ 1−δ. [sent-215, score-0.234]
</p><p>49 Consequently, for any i, σ, and S, we have 1 1 ln BS (i, σ) |i|  ≥ RSi (σ, Si ) ln  RSi (σ, Si ) 1 − RSi (σ, Si ) + (1 − RSi (σ, Si )) ln R(σ, Si ) 1 − R(σ, Si )  def  = kl RSi (σ, Si ) R(σ, Si ) . [sent-220, score-0.607]
</p><p>50 Indeed, from the log-sum inequality (Cover and Thomas, 1991), we can show that for any (q, p) ∈ [0, 1] × [0, 1], any (r, s) ∈ [0, 1] × [0, 1], and any α ∈ [0, 1], we have kl αq + (1 − α)r αp + (1 − α)s ≤ αkl(q p) + (1 − α)kl(r s) . [sent-222, score-0.159]
</p><p>51 Hence, from Equation 6 and Jensen’s inequality applied to kl(q p), we have E  E  i∼QI σ∼QM (Si )  1 1 ln |i| BS (i, σ)  ≥  E  E  i∼QI σ∼QM (Si )  kl RSi (σ, Si ) R(σ, Si )  ≥ kl (RS (GQ ) R(GQ )) . [sent-223, score-0.422]
</p><p>52 This subsequence of examples then constitutes the compression sequence of the sample-compressed classiﬁer and the reconstruction function just 1471  L AVIOLETTE AND M ARCHAND  consists of the perceptron learning rule executed on the compression sequence. [sent-229, score-0.43]
</p><p>53 Here, the compression sequence consists of the set of support vectors and the reconstruction function, again, just consists of running the original learning algorithm on the compression sequence. [sent-232, score-0.391]
</p><p>54 Theorem 3 bounds the risk a stochastic average (and the associated majority-vote) of samplecompressed classiﬁers. [sent-233, score-0.165]
</p><p>55 Theorem 3 conﬁrms this intuition with an upper-bound on the risk that increases with the amount of the KL-divergence between the posterior and the prior. [sent-249, score-0.155]
</p><p>56 1 Stochastic Averages and Majority-Votes of Set Covering Machines The perceptron learning rule and the SVM are examples of learning algorithms that produce samplecompressed classiﬁers R (Si ) that can be reconstructed solely from a compression sequence S i . [sent-255, score-0.25]
</p><p>57 the conjunction makes no error on its compression sequence S i . [sent-274, score-0.225]
</p><p>58 Condition 1 is motivated by the fact that a conjunction of two concentric balls gives the same classiﬁer as the single inner ball (and, symmetrically, a conjunction of two concentric holes can be replaced by the outer hole). [sent-276, score-0.179]
</p><p>59 Even under these three conditions, the compression sequence S i alone, does not give enough information to reconstruct the conjunction of features. [sent-287, score-0.225]
</p><p>60 Thus, Condition 3 implies that the border point of each ball center x c must be the example in P(Si ) which is located furthest from xc and the border point of each hole center xc must be the example in P(Si ) which is located closest from xc . [sent-296, score-0.329]
</p><p>61 Now, to use Theorem 3 as a risk bound for a stochastic average (i. [sent-301, score-0.155]
</p><p>62 In these cases, the message part of the posterior, Q M (Si ) (σ), will be non-zero only for the particular message that is actually used for each compression sequence S i that occurs in the majority-vote of SCMs. [sent-309, score-0.362]
</p><p>63 Marchand and Sokolova (2005) proposed a different data-dependent set of messages and prior which can give a (slightly) tighter risk bound than the solution we present here. [sent-314, score-0.236]
</p><p>64 Hence, each compression sequence S i consists only of the centers. [sent-317, score-0.194]
</p><p>65 Theorem 3 then provides an upper bound for the risk of these stochastic averages (and associated majority-votes) of PAC-Bayes SCMs. [sent-323, score-0.181]
</p><p>66 Let us examine the case when, given a training sequence S, the (stochastic) sample-compressed Gibbs classiﬁer becomes a deterministic classiﬁer with a posterior having all its weight on a single sample-compressed classiﬁer R (Si , σ). [sent-327, score-0.145]
</p><p>67 Pr  S∼Dm  ∀i ∈ I , ∀σ ∈ M (Si ) :  E  E  i∼QI σ∼QM (Si )  ln  1 BS (i, σ) ≤ ln  1 m+1 + ln PI (i)PM (Si ) (σ) δ  Now, let us use the binomial distribution def  Bin (m, k, r) = to express BS (i, σ) as  m k r (1 − r)m−k , k  BS (i, σ) = Bin m, mRSi (σ, Si ), R(σ, Si ) . [sent-330, score-0.476]
</p><p>68 Let us now compare this risk bound with the tightest currently known sample-compression risk bounds. [sent-335, score-0.221]
</p><p>69 Moreover, given a training sequence S of m examples and a vector i selected by the learner, the Gibbs classiﬁer G QM (S ) just chooses i randomly according to QM (Si ) a message σ to classify any new example x with classiﬁer R (σ, S i ). [sent-346, score-0.143]
</p><p>70 Within this methodology, a part of the message string σ is used to indicate which indices in i are pointing to training error examples. [sent-353, score-0.155]
</p><p>71 Theorem 9 For any δ ∈ (0, 1], for any reconstruction function mapping compression sequences and messages to classiﬁers, for any T ∈ (X × Y )m and for any prior PT on I × MT , we have  Pr  S∼Dm  ∀Q on I × MS such that RS (GQ ) = 0 : R(GQ ) ≤ 1 − exp  −1 m − dQ  KL(Q PS ) + ln  1 δ  ≥ 1−δ. [sent-365, score-0.433]
</p><p>72 Since 1 , kl(0 R(GQ )) = ln 1 − R(GQ ) the upper bound of Theorem 3 becomes identical to the bound of Theorem 9 except for the presence of a ln(m + 1) term in the argument of the exponential in Theorem 3. [sent-370, score-0.21]
</p><p>73 Thus Pr m  S∼D  ln  i∼PI σ∼PM (Si )  ≤ ln  1 δ  ≥ 1−δ. [sent-377, score-0.234]
</p><p>74 With this technique, we ﬁnd that, for any prior PS , we have ∀Q on I × MS such that RS (GQ ) = 0 :  Pr  S∼Dm  E  E  i∼QI σ∼QM (Si )  1 1 1 lnCS (i, σ) ≤ KL(Q PS ) + ln m − dQ δ |i|  ≥ 1 − δ . [sent-379, score-0.145]
</p><p>75 When there is a tie, the whole 1478  S AMPLE -C OMPRESSED PAC-BAYES B OUNDS  ensemble may abstain or predict the most frequently encountered class in the training sequence S. [sent-386, score-0.144]
</p><p>76 To bound the risk of majority-votes and stochastic averages of classiﬁers that can abstain, it is “natural” to consider loss functions that may take values ∈ {0, 1}. [sent-388, score-0.168]
</p><p>77 As before, the theorem will also apply to the usual data-independent setting in the limit where each classiﬁer is only described by a data-independent message (and an empty compression sequence). [sent-395, score-0.305]
</p><p>78 First, we are concerned with the probability a(h) that classiﬁer h abstains on a new example, where def  a(h) =  Pr  (x,y)∼D  (h(x) = 0) . [sent-399, score-0.151]
</p><p>79 And we are also concerned with the probability e(h) that classiﬁer h predicts the wrong class label of a new example, where def  e(h) =  Pr  (x,y)∼D  (h(x) = y ∧ h(x) = 0) . [sent-400, score-0.141]
</p><p>80 The empirical estimates (of these probabilities) on a training sequence S of m examples are deﬁned as def  eS (h) =  def  aS (h) =  1 m ∑ I(h(xi ) = 0) · I(h(xi ) = yi ) , m i=1 1 m ∑ I(h(xi ) = 0) . [sent-403, score-0.279]
</p><p>81 m i=1  Similarly as before, each classiﬁer R (σ, Si ) is described by a compression sequence Si and a message σ taken from a data-dependent set M (Si ). [sent-404, score-0.278]
</p><p>82 Given a training sequence S, the prior and the posterior have the same form as before. [sent-405, score-0.158]
</p><p>83 Given a posterior Q, the re-scaled posterior Q is still given by Deﬁnition 2. [sent-416, score-0.142]
</p><p>84 Hence, kl(q1 , q2 p1 , p2 ) = q1 ln  q1 q2 1 − q 1 − q2 + q2 ln + (1 − q1 − q2 ) ln . [sent-418, score-0.351]
</p><p>85 Hence, Pr m  S∼D  ln  i∼PI σ∼PM (Si )  ≤ ln  (m + 1)(m + 2) 2δ  ≥ 1−δ. [sent-424, score-0.234]
</p><p>86 Since kl(q1 , q2 p1 , p2 ) is a convex function of (q1 , q2 , p1 , p2 ), we can use Jensen’s inequality to obtain 1 1 ln ≥ E E kl aSi (σ, Si ), eSi (σ, Si ) a(σ, Si ), e(σ, Si ) E E i∼QI σ∼QM (Si ) i∼QI σ∼QM (Si ) |i| BS (i, σ) ≥ kl (aS (GQ ), eS (GQ ) a(GQ ), e(GQ )) . [sent-429, score-0.422]
</p><p>87 S AMPLE -C OMPRESSED PAC-BAYES B OUNDS  To upper-bound the risk of the majority-vote BQ with Theorem 10, we need to redeﬁne the risk R(BQ ) (in terms of the loss function deﬁned above) and related it to e(G Q ) and a(GQ ). [sent-438, score-0.168]
</p><p>88 For this task, let us adopt the convention that BQ (x) abstains of predicting the label of x whenever the Q-weight of classiﬁers predicting +1 is equal to the Q-weight of classiﬁers predicting −1 (this includes the case when all the classiﬁers having non-zero posterior weight abstain). [sent-439, score-0.201]
</p><p>89 Furthermore, let e (x,y) (GQ ) denote the probability that GQ predicts incorrectly the label of (x, y) and let a(x,y) (GQ ) denote the probability that GQ abstains on (x, y), that is, def  e(x,y) (GQ ) =  E I(h(x) = y ∧ h(x) = 0)  h∼Q  def  a(x,y) (GQ ) =  E I(h(x) = 0) . [sent-441, score-0.292]
</p><p>90 1 Reduced Coulomb Energy Networks Corollaries 8 and 11 can be used to bound the risk of stochastic averages and majority-votes of sample-compressed classiﬁers that can abstain. [sent-457, score-0.168]
</p><p>91 Given any metric d, the output h(x) on any input example x of a ball is given by y c if d(x, xc ) ≤ d(x, xb ), otherwise (if d(x, xc ) > d(x, xb )) it abstains of predicting a class label. [sent-463, score-0.282]
</p><p>92 Hence, each sample-compressed classiﬁer has here a compression sequence S i of only two examples. [sent-464, score-0.194]
</p><p>93 The message-part of the prior, PM (Si ) (σ), assigns equal probability to the two possible single-bit messages and the message-part of the posterior, QM (Si ) (σ), assigns a weight of one to the single-bit message that is actually used with the two-example compression sequence Si . [sent-470, score-0.363]
</p><p>94 With this form for the prior and the posterior, Corollary 8 provides a tight bound for the risk of the stochastic average G Q . [sent-471, score-0.207]
</p><p>95 Conclusion We have derived a PAC-Bayes theorem for the sample-compression setting where each classiﬁer is described by a compression subset of the training data and a message string of additional information. [sent-475, score-0.336]
</p><p>96 For posteriors having all their weights on a single sample-compressed classiﬁer, the general risk bound reduces to a bound similar to the tight sample-compression bound of Laviolette et al. [sent-478, score-0.258]
</p><p>97 Moreover, we have seen that the risk bound supports the strategy of randomizing the predictions over several sample-compressed classiﬁers instead of predicting with a single sample-compressed classiﬁer. [sent-481, score-0.144]
</p><p>98 Indeed, a stochastic Gibbs classiﬁer deﬁned on a posterior over several sample-compressed classiﬁers can have a smaller risk bound than any such single (deterministic) sample-compressed classiﬁer. [sent-482, score-0.226]
</p><p>99 Finally, to obtain a performance guarantee for many “rule-based systems” and RCE networks, we have generalized the PAC-Bayes theorem to the case where each sample-compressed classiﬁer in the ensemble can abstain of predicting a class label. [sent-483, score-0.153]
</p><p>100 PAC-Bayesian compression bounds on the prediction error of learning algorithms for classiﬁcation. [sent-524, score-0.172]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gq', 0.534), ('si', 0.52), ('qm', 0.235), ('bq', 0.19), ('qi', 0.188), ('compression', 0.157), ('kl', 0.146), ('pm', 0.142), ('bs', 0.137), ('ps', 0.118), ('ln', 0.117), ('def', 0.11), ('laviolette', 0.11), ('rsi', 0.106), ('marchand', 0.089), ('rs', 0.089), ('pi', 0.084), ('message', 0.084), ('risk', 0.084), ('asi', 0.08), ('ompressed', 0.077), ('posterior', 0.071), ('archand', 0.07), ('esi', 0.07), ('messages', 0.07), ('er', 0.069), ('pr', 0.068), ('xc', 0.067), ('abstain', 0.065), ('aviolette', 0.065), ('gibbs', 0.064), ('ers', 0.062), ('dq', 0.058), ('ms', 0.057), ('ounds', 0.053), ('theorem', 0.048), ('classi', 0.046), ('mario', 0.045), ('ample', 0.041), ('abstains', 0.041), ('mt', 0.041), ('bound', 0.04), ('francois', 0.04), ('reconstruction', 0.04), ('langford', 0.038), ('sequence', 0.037), ('holes', 0.035), ('samplecompressed', 0.035), ('hole', 0.035), ('scm', 0.035), ('border', 0.034), ('dm', 0.033), ('pt', 0.032), ('xb', 0.031), ('stochastic', 0.031), ('conjunction', 0.031), ('posteriors', 0.03), ('sokolova', 0.03), ('abstaining', 0.029), ('rce', 0.029), ('prior', 0.028), ('balls', 0.027), ('string', 0.025), ('ball', 0.025), ('graepel', 0.025), ('tight', 0.024), ('indices', 0.024), ('consequently', 0.024), ('germain', 0.024), ('lacasse', 0.024), ('mcallester', 0.022), ('training', 0.022), ('perceptron', 0.021), ('sequences', 0.021), ('ensemble', 0.02), ('vote', 0.02), ('bin', 0.02), ('samplecompression', 0.02), ('predicting', 0.02), ('seeger', 0.019), ('subsequence', 0.018), ('bint', 0.018), ('mrsi', 0.018), ('predicts', 0.017), ('usual', 0.016), ('bounds', 0.015), ('weight', 0.015), ('expectation', 0.015), ('binomial', 0.015), ('concentric', 0.015), ('reilly', 0.015), ('observing', 0.015), ('priori', 0.015), ('priors', 0.014), ('tighter', 0.014), ('label', 0.014), ('averages', 0.013), ('tightest', 0.013), ('upper', 0.013), ('inequality', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="65-tfidf-1" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>Author: François Laviolette, Mario Marchand</p><p>Abstract: We propose a PAC-Bayes theorem for the sample-compression setting where each classiﬁer is described by a compression subset of the training data and a message string of additional information. This setting, which is the appropriate one to describe many learning algorithms, strictly generalizes the usual data-independent setting where classiﬁers are represented only by data-independent message strings (or parameters taken from a continuous set). The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classiﬁer vanishes. For posteriors having all their weights on a single sample-compressed classiﬁer, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. (2005). Finally, we extend our results to the case where each sample-compressed classiﬁer of a data-dependent ensemble may abstain of predicting a class label. Keywords: PAC-Bayes, risk bounds, sample-compression, set covering machines, decision list machines</p><p>2 0.24110654 <a title="65-tfidf-2" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>Author: Sanjoy Dasgupta, Leonard Schulman</p><p>Abstract: We show that, given data from a mixture of k well-separated spherical Gaussians in R d , a simple two-round variant of EM will, with high probability, learn the parameters of the Gaussians to nearoptimal precision, if the dimension is high (d ln k). We relate this to previous theoretical and empirical work on the EM algorithm. Keywords: expectation maximization, mixtures of Gaussians, clustering, unsupervised learning, probabilistic analysis</p><p>3 0.21948202 <a title="65-tfidf-3" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>Author: Zakria Hussain, François Laviolette, Mario Marchand, John Shawe-Taylor, Spencer Charles Brubaker, Matthew D. Mullin</p><p>Abstract: Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classiﬁer achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classiﬁer achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassiﬁcations. Keywords: set covering machines, sample-compression, loss bounds</p><p>4 0.074914292 <a title="65-tfidf-4" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>Author: András György, Tamás Linder, Gábor Lugosi, György Ottucsák</p><p>Abstract: The on-line shortest path problem is considered under various models of partial monitoring. Given a weighted directed acyclic graph whose edge weights can change in an arbitrary (adversarial) way, a decision maker has to choose in each round of a game a path between two distinguished vertices such that the loss of the chosen path (deﬁned as the sum of the weights of its composing edges) be as small as possible. In a setting generalizing the multi-armed bandit problem, after choosing a path, the decision maker learns only the weights of those edges that belong to the chosen path. For this problem, an algorithm is given whose average cumulative loss in n rounds exceeds that of the best path, matched off-line to the entire sequence of the edge weights, by a quantity that is √ proportional to 1/ n and depends only polynomially on the number of edges of the graph. The algorithm can be implemented with complexity that is linear in the number of rounds n (i.e., the average complexity per round is constant) and in the number of edges. An extension to the so-called label efﬁcient setting is also given, in which the decision maker is informed about the weights of the edges corresponding to the chosen path at a total of m n time instances. Another extension is shown where the decision maker competes against a time-varying path, a generalization of the problem of tracking the best expert. A version of the multi-armed bandit setting for shortest path is also discussed where the decision maker learns only the total weight of the chosen path but not the weights of the individual edges on the path. Applications to routing in packet switched networks along with simulation results are also presented. Keywords: on-line learning, shortest path problem, multi-armed bandit problem c 2007 Andr´ s Gy¨ rgy, Tam´ s Linder, G´ bor Lugosi and Gy¨ rgy Ottucs´ k. a o a a o a ¨ ´ G Y ORGY, L INDER , L UGOSI AND OTTUCS AK</p><p>5 0.068442322 <a title="65-tfidf-5" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>6 0.058869801 <a title="65-tfidf-6" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>7 0.054927208 <a title="65-tfidf-7" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>8 0.046977088 <a title="65-tfidf-8" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>9 0.043462679 <a title="65-tfidf-9" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>10 0.040387139 <a title="65-tfidf-10" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>11 0.033610661 <a title="65-tfidf-11" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>12 0.032584164 <a title="65-tfidf-12" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>13 0.031087233 <a title="65-tfidf-13" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>14 0.029578328 <a title="65-tfidf-14" href="./jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</a></p>
<p>15 0.02948634 <a title="65-tfidf-15" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>16 0.029288985 <a title="65-tfidf-16" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>17 0.028832598 <a title="65-tfidf-17" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>18 0.028640931 <a title="65-tfidf-18" href="./jmlr-2007-A_Complete_Characterization_of_a_Family_of_Solutions_to_a_Generalized_Fisher_Criterion.html">2 jmlr-2007-A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion</a></p>
<p>19 0.028323567 <a title="65-tfidf-19" href="./jmlr-2007-Hierarchical_Average_Reward_Reinforcement_Learning.html">41 jmlr-2007-Hierarchical Average Reward Reinforcement Learning</a></p>
<p>20 0.027740326 <a title="65-tfidf-20" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.205), (1, -0.085), (2, -0.308), (3, -0.3), (4, 0.066), (5, -0.215), (6, 0.2), (7, -0.295), (8, -0.18), (9, 0.006), (10, 0.017), (11, 0.084), (12, -0.171), (13, 0.021), (14, 0.167), (15, -0.025), (16, -0.152), (17, -0.019), (18, 0.089), (19, 0.045), (20, -0.034), (21, -0.063), (22, 0.12), (23, 0.005), (24, -0.037), (25, -0.044), (26, -0.025), (27, 0.022), (28, -0.049), (29, 0.039), (30, 0.096), (31, -0.026), (32, 0.072), (33, -0.009), (34, -0.052), (35, -0.052), (36, 0.087), (37, 0.031), (38, -0.059), (39, -0.047), (40, -0.039), (41, -0.019), (42, -0.042), (43, 0.048), (44, 0.034), (45, -0.011), (46, 0.009), (47, 0.061), (48, 0.029), (49, -0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97827876 <a title="65-lsi-1" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>Author: François Laviolette, Mario Marchand</p><p>Abstract: We propose a PAC-Bayes theorem for the sample-compression setting where each classiﬁer is described by a compression subset of the training data and a message string of additional information. This setting, which is the appropriate one to describe many learning algorithms, strictly generalizes the usual data-independent setting where classiﬁers are represented only by data-independent message strings (or parameters taken from a continuous set). The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classiﬁer vanishes. For posteriors having all their weights on a single sample-compressed classiﬁer, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. (2005). Finally, we extend our results to the case where each sample-compressed classiﬁer of a data-dependent ensemble may abstain of predicting a class label. Keywords: PAC-Bayes, risk bounds, sample-compression, set covering machines, decision list machines</p><p>2 0.79123229 <a title="65-lsi-2" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>Author: Sanjoy Dasgupta, Leonard Schulman</p><p>Abstract: We show that, given data from a mixture of k well-separated spherical Gaussians in R d , a simple two-round variant of EM will, with high probability, learn the parameters of the Gaussians to nearoptimal precision, if the dimension is high (d ln k). We relate this to previous theoretical and empirical work on the EM algorithm. Keywords: expectation maximization, mixtures of Gaussians, clustering, unsupervised learning, probabilistic analysis</p><p>3 0.77150893 <a title="65-lsi-3" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>Author: Zakria Hussain, François Laviolette, Mario Marchand, John Shawe-Taylor, Spencer Charles Brubaker, Matthew D. Mullin</p><p>Abstract: Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classiﬁer achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classiﬁer achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassiﬁcations. Keywords: set covering machines, sample-compression, loss bounds</p><p>4 0.27335057 <a title="65-lsi-4" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>Author: Rocío Alaiz-Rodríguez, Alicia Guerrero-Curieses, Jesús Cid-Sueiro</p><p>Abstract: The design of a minimum risk classiﬁer based on data usually stems from the stationarity assumption that the conditions during training and test are the same: the misclassiﬁcation costs assumed during training must be in agreement with real costs, and the same statistical process must have generated both training and test data. Unfortunately, in real world applications, these assumptions may not hold. This paper deals with the problem of training a classiﬁer when prior probabilities cannot be reliably induced from training data. Some strategies based on optimizing the worst possible case (conventional minimax) have been proposed previously in the literature, but they may achieve a robust classiﬁcation at the expense of a severe performance degradation. In this paper we propose a minimax regret (minimax deviation) approach, that seeks to minimize the maximum deviation from the performance of the optimal risk classiﬁer. A neural-based minimax regret classiﬁer for general multi-class decision problems is presented. Experimental results show its robustness and the advantages in relation to other approaches. Keywords: classiﬁcation, imprecise class distribution, minimax regret, minimax deviation, neural networks 1. Introduction - Problem Motivation In the general framework of learning from examples and speciﬁcally when dealing with uncertainty, the robustness of the decision machine becomes a key issue. Most machine learning algorithms are based on the assumption that the classiﬁer will use data drawn from the same distribution as the training data set. Unfortunately, for most practical applications (such as remote sensing, direct marketing, fraud detection, information ﬁltering, medical diagnosis or intrusion detection) the target class distribution may not be accurately known during learning: for example, because the cost of labelling data may be class-dependent or the prior probabilities are non-stationary. Therefore, the data used to design the classiﬁer (within the Bayesian context (see VanTrees, 1968), the c 2007 Roc´o Alaiz-Rodr´guez, Alicia Guerrero-Curieses and Jesus Cid-Sueiro. ´ ı ı A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I prior probabilities and the misclassiﬁcation costs) may be non representative of the underlying real distributions. If the ratio of training data corresponding to each class is not in agreement with real class distributions, designing Bayes decision rules based on prior probabilities estimated from these data will be suboptimal and can seriously affect the reliability and performance of the classiﬁer. A similar problem may arise if real misclassiﬁcation costs are unknown during training. However, they are usually known by the end user, who can adapt the classiﬁer decision rules to cost changes without re-training the classiﬁer. For this reason, our attention in this paper is mainly focused on the problem of uncertainty in prior probabilities. Furthermore, being aware that class distribution is seldom known (at least totally) in real world applications, a robust approach (as opposite to adaptive) that prevents severe performance degradation appears to be convenient for these situations. Besides other adaptive and robust approaches that address this problem (discussed in more detail in Section 2.2) it is important to highlight those that handle the problem of uncertainty in priors by following a robust minimax principle: minimize the maximum possible risk. Analytic foundations of minimax classiﬁcation are widely considered in the literature (see VanTrees, 1968; Moon and Stirling, 2000; Duda et al., 2001, for instance) and a few algorithms to carry out minimax decisions have been proposed. From computationally expensive ones such as estimating probability density functions (Takimoto and Warmuth, 2000; Kim, 1996) or using methods from optimization (Polak, 1997) to simpler ones like neural network training algorithms (Guerrero-Curieses et al., 2004; AlaizRodriguez et al., 2005). Minimax classiﬁers may, however, be seen as over-conservative since its goal is to optimize the performance under the least favorable conditions. Consider, for instance, a direct marketing campaign application carried out in order to maximize proﬁts. Since optimal decisions rely on the proportion of potential buyers and it is usually unknown in advance, our classiﬁcation system should take into account this uncertainty. Nevertheless, following a pure minimax strategy can lead to solutions where minimizing the maximum loss implies considering there are no potential clients. If it is the case, this minimax approach does not seem to be suitable for this kind of situation. In this imprecise class distribution scenario, it can be noticed that the classiﬁer performance may be highly deviated from the optimal, that is, that of the classiﬁer knowing actual priors. Minimizing this gap (that is, the maximum possible deviation with respect to the optimal classiﬁer) is the focus of this paper. We seek for a system as robust as the conventional minimax approach but less pessimistic at the same time. We will refer to it as a minimax deviation (or minimax regret) classiﬁer. In contrast to other robust and adaptive approaches, it can be used in general multiclass problems. Furthermore, as shown in Guerrero-Curieses et al. (2004), minimax approaches can be used in combination with the adaptive proposal by Saerens et al. (2002) to exploit its advantages. This minimax regret approach has recently been applied in the context of parameter estimation (Eldar et al., 2004; Eldar and Merhav, 2004) and a similar competitive strategy has been used in the context of hypothesis testing (Feder and Merhav, 2002). Under prior uncertainty, our solution provides an upper bound of the performance divergence from the optimal classiﬁer. We propose a simple learning rate scaling algorithm in order to train a neural-based minimax deviation classiﬁer. Although training can be based on minimizing any objective function, we have chosen objective functions that provide estimates of the posterior probabilities (see Cid-Sueiro and Figueiras-Vidal, 2001, for more details). 104 M INIMAX R EGRET C LASSIFIER This paper is organized as follows: the next section provides an overview of the problem as well as some previous approaches to cope with it. Next, Section 3 states the fundamentals of minimax classiﬁcation together with a deeper analysis of the minimax regret approach proposed in this paper. Section 4 presents a neural training algorithm to get a neural-based minimax regret classiﬁer under complete uncertainty. Moreover, practical situations with partial uncertainty in priors are also discussed. A learning algorithm to solve them is provided in Section 5. In Section 6, some experimental results show that minimax regret classiﬁers outperform (in terms of maximum risk deviation) classiﬁers trained on re-balanced data sets and those with the originally assumed priors. Finally, the main conclusions are summarized in Section 7. 2. Problem Overview Traditionally, supervised learning lies in the fact that training data and real data come from the same (although unknown) statistical model. In order to carefully analyze to what extend classiﬁer performance depends on conditions such as class distribution or decision costs, learning and decision theory principles are brieﬂy revisited. Next, some previous approaches to deal with environment imprecision are reviewed. 2.1 Learning and Making Optimal Decisions Let S = {(xk , dk ), k = 1, . . . , K} denote a set of labelled samples where xk ∈ RN is an observation feature vector and dk ∈ UL = {u0 , . . . , uL−1 } is the label vector. Class-i label ui is a unit L-dimensional vector with components ui, j = δi j , with every component equal to 0, except the i-th component which is equal to 1. We assume a learning process that estimates parameters w of a non-linear mapping f w : RN → P from the input space into probability space P = {p ∈ [0, 1]L | ∑L−1 pi = 1}. The soft decision is given i=0 by yk = fw (xk ) ∈ P and the hard output of the classiﬁer is denoted by d. Note that d and d will be used to distinguish the actual class from the predicted one, respectively. Several costs (or beneﬁts) associated with each possible decision are also deﬁned: c i j denotes the cost of deciding in favor of class i when the true class is j. Negative values represent beneﬁts (for instance, cii , which is the cost of correctly classifying a sample from class i could be negative in some practical cases). In general cost-sensitive classiﬁcation problems, either misclassiﬁcation costs c i j or cii costs can take different values for each class. Thus, there are many applications where classiﬁcation errors lead to very different consequences (medical diagnosis, fault detection, credit risk analysis), what implies misclassiﬁcation costs ci j that may largely vary between them. In the same way, there are also many domains where correct decision costs (or beneﬁts) c ii do not take the same value. For instance, in targeted marketing applications (Zadrozny and Elkan, 2001), correctly identifying a buyer implies some beneﬁt while correctly classifying a non buyer means no income. The same ¨ applies to medical diagnosis domains such as the gastric carcinoma problem studied in G uvenir et al. (2004). In this case, the beneﬁt of correct classiﬁcation also depends on the class: the beneﬁt of correctly classifying an early stage tumor is higher than that of a later stage. The expected risk (or loss) R is given by L−1 L−1 R = ∑ ∑ ci j P{d = ui |d = u j }Pj j=0 i=0 105 , (1) A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I where P{d = ui |d = u j } with i = j represent conditional error probabilities, and P j = P{d = u j } is the prior probability of class u j . Deﬁning the conditional risk of misclassifying samples from class u j as L−1 Rj = ∑ ci j P{d = ui |d = u j } , i=0 we can express risk (1) as L−1 R= ∑ Ri Pi . (2) i=0 It is well-known that the Bayes decision rule for the minimum risk is given by L−1 d = arg min { ∑ ci j P{d = u j |x}} , ui (3) j=0 where P{d = ui |x} is the a posteriori probability of class i given sample x. The optimal decision rule depends on posterior probabilities and therefore, on the prior probabilities and the likelihood. In theory, as long as posterior probabilities (or likelihood and prior probabilities) are known, the optimal decision in Eq. (3) can be expressed after a trivial manipulation as a function of the cost differences between the costs (ci j − c j j ) (Duda et al., 2001). This is the reason why c j j is usually assumed to be zero and the value of the cost difference is directly assigned to c i j . When dealing ¨ with practical applications, however, some authors (Zadrozny and Elkan, 2001; G uvenir et al., 2004) have urged to use meaningful decision costs measured over a common baseline (and not necessarily taking c j j = 0) in order to avoid mistakes that otherwise could be overlooked. For this reason and, what is more important, the uncertainty class distribution problem addressed in this paper, decision costs measured over a common baseline are considered. Furthermore, absolute values of decision costs are relevant to the design of classiﬁers under the minimax regret approach. 2.2 Related Work: Dealing with Cost and Prior Uncertainty Most proposals to address uncertainty in priors fall into the categories of adaptive and robust solutions. While the aim of a robust solution is to avoid a classiﬁer with very poor performance under any conditions, an adaptive system pursues to ﬁt the classiﬁer parameters using more incoming data or more precise information. With an adaptive-oriented principle, Provost (2000) states that, once the classiﬁer is trained under speciﬁc class distributions and cost assumptions (not necessarily the operating conditions), the selection of the optimal classiﬁer for speciﬁc conditions is carried out by a correct placement of the decision thresholds. In the same way, the approaches in Kelly et al. (1999) and Kubat et al. (1998) consider that tuning the classiﬁer parameters should be left to the end user, expecting that class distributions and misclassiﬁcation costs will be precisely known then. Some graphical methods based on the ROC curve have been proposed in Adams and Hand (1998) and Provost and Fawcett (2001) in order to compare the classiﬁer performance under imprecise class distributions and/or misclassiﬁcation costs. The ROC convex hull method presented in Provost and Fawcett (2001) (or the alternative representation proposed in Drummond and Holte (2000)) allows the user to select potentially optimal classiﬁers, providing a ﬂexible way to select 106 M INIMAX R EGRET C LASSIFIER them when precise information about priors or costs is available. Under imprecision, some classiﬁers can be discarded but this does not necessarily provide a method to select the optimal classiﬁer between the possible ones and ﬁt its parameters. Furthermore, due to its graphical character, these methods are limited to binary classiﬁcation problems. Changes in prior probabilities have also been discussed by Saerens et al. (2002), who proposes a method based on re-estimating the prior probabilities of real data in an unsupervised way and subsequently adjusting the outputs of the classiﬁer according to the new a priori probabilities. Obviously, the method requires enough unlabelled data being available for re-estimation. As an alternative to adaptive schemes, several robust solutions have been proposed, as the resampling methods, especially in domains where imbalanced classes come out (Kubat and Matwin, 1997; Lawrence et al., 1998; Chawla et al., 2002; Barandela et al., 2003). Either by undersampling or oversampling, the common purpose is to balance artiﬁcially the training data set in order to get a uniform class distribution, which is supposed to be the least biased towards any class and, thus, the most robust against changes in class distributions. The same approach is followed in cost sensitive domains, but with some subtle differences in practice. It is well known that class priors and decision costs are intrinsically related. For instance, different decision costs can be simulated by altering the priors and vice versa (see Ting, 2002, for instance). Thus, when a uniform distribution is desired in a cost sensitive domain, but working with cost insensitive decision machines, class priors are altered according to decision costs, what is commonly referred as rebalancing. The manipulation of the training data distribution has been applied to cost-sensitive learning in two-class problems (Breiman et al., 1984) in the following way: basically, the class with higher misclassiﬁcation cost (suppose n times the lowest misclassiﬁcation cost) is represented with n times more examples than the other class. Besides random sampling strategies, other sampling-based rebalancing schemes have been proposed to accomplish this task, like those considering closeness to the boundaries between classes (Japkowicz and Stephen, 2002; Zhou and LiuJ, 2006) or the costproportionate rejection sampling presented in Zadrozny et al. (2003). Extending the formulation of this type of procedures to general multiclass problems with multiple (and possibly asymmetric) inter-class misclassiﬁcation costs appears to be a nontrivial task (Zadrozny et al., 2003; Zhou and LiuJ, 2006), but some progress has been made recently with regard to this latter point (Abe et al., 2004). Note, also, that many (although not all) of these rebalancing strategies are usually implemented by oversampling and/or subsampling, that is, replicating examples (without adding any extra information) and/or deleting them (which implies information loss). 3. Robust Classiﬁers Under Prior Uncertainty: Minimax Classiﬁers Prior probability uncertainty can be coped from a robust point of view following a minimax derived strategy. Minimax regret criterion is discussed in this section after presenting the conventional minimax criterion. Although our approach extends to general multi-class problems and the discussion is carried out in that way, we will ﬁrst illustrate, for the sake of clarity and simplicity, a binary situation. 3.1 Minimax Classiﬁers As Eq. (3) shows, the minimum risk decisions depend on the misclassiﬁcation costs, c i j , and the posterior class probabilities and, thus, they depend on the prior probabilities, Pi . Different prior 107 A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I PSfrag replacements distributions (frequency for each class) give rise to different Bayes classiﬁers. Fig. 1 shows the Bayes risk curve, RB (P1 ) versus class-1 prior probability for a binary classiﬁcation problem. Standard RF (Q1 , P1 ) R0 Minimax RF (Q1mM , P1 ) Risk c00 Minimax Deviation RF (Q1mMd , P1 ) Rbasis R1 RB (P1 ) c11 0 Q1 Q1mM 1 Q1mMd P1 Figure 1: Risk vs. P1 . Minimum risk curve and performance under prior changes for the standard, minimax and minimax deviation classiﬁer. RB (P1 ) stands for the optimal Bayes Risk against P1 . RF (Q1 , P1 ) denotes the Risk of a standard classiﬁer (Fixed decision rule optimized for prior probabilities Q1 estimated in the training phase) against P1 . RF (Q1mM , P1 ) denotes the Risk of a minimax classiﬁer (Fixed decision rule optimized for the minimax probabilities Q1mM ) against P1 . RF (Q1mMd , P1 ) denotes the Risk of a minimax deviation classiﬁer (Fixed decision rule optimized for the minimax deviation probabilities Q 1mMd ) against P1 . If the prior probability distribution is unknown when the classiﬁer is designed, or this distribution changes with time or from one environment to other, the mismatch between training and test conditions can degrade signiﬁcantly the classiﬁer performance. For instance, assume that Q = (Q0 , Q1 ) is the vector with class-0 and class-1 prior probabilities estimated in the training phase, respectively, and let RB (Q1 ) represent the minimum (Bayes) risk attainable by any decision rule for these priors. Note, that, according to Eq. (2), for a given classiﬁer, the risk is a linear function of priors. Thus, risk RF (Q1 , P1 ) associated to the (ﬁxed) classiﬁer optimized for Q changes linearly with actual prior probabilities P1 and P0 = 1 − P1 , going from (0, R0 ) to (1, R1 ) (the continuous line in Fig. 1), where R0 and R1 refer to the class conditional risks for classes 0 and 1, respectively. Fig. 1 shows the impact of this change in priors and how performance deviates from optimal. Also, it can be shown (see VanTrees, 1968, for instance) that the minimum risk curve obtained for each prior is convex and the risk function of a given classiﬁer veriﬁes R F (Q1 , P1 ) ≥ RB (P1 ) with a tangent point at P1 = Q1 . 108 M INIMAX R EGRET C LASSIFIER The dashed line in Fig. 1 shows the performance of the minimax classiﬁer, which minimizes the maximum possible risk under the least favorable priors, thus providing the most robust solution, in the sense that performance becomes independent from priors. From Fig. 1, it becomes clear that the minimax classiﬁer is optimal for prior probabilities P = QmM = (Q0mM , Q1mM ) maximizing RB . Thus, this strategy is equivalent to maximizing the minimum risk (Moon and Stirling, 2000; Duda et al., 2001). We will refer to them as the minimax probabilities. Fig. 1 also makes clear that although a minimax classiﬁer is a robust solution to address the imprecision in priors, it may become a somewhat pessimistic approach. 3.2 Minimax Deviation Classiﬁers We propose an alternative classiﬁer that, instead of minimizing the maximum risk, minimizes the maximum deviation (regret) from the optimal Bayes classiﬁer. In the following, we will refer to it as the minimax deviation or minimax regret classiﬁer. A comparison between minimax and minimax deviation approaches is also shown in Fig. 1. This latter case corresponds to a classiﬁer trained on prior probabilities P = Q mMd with performance as a function of priors given by a line (a plane or hyperplane for three or more classes, respectively) parallel to what we name, in the following, basis risk (Rbasis = c00 (1 − P1 ) + c11 P1 ). Note that the maximum deviation (with respect to priors) of the classiﬁer optimized for Q is given by D(Q) = max {RF (Q1 , P1 ) − RB (P1 )} = max {R0 − c00 , R1 − c11 } . P1 The inspection of Fig. 1 shows that the minimum of D (with respect to Q) is achieved when R0 − c00 = R1 − c11 , which means that line RF (Q1 , P1 ) is parallel to arc named Rbasis in the ﬁgure and tangent to RB at Q1mMd . Therefore, the minimax regret classiﬁer is also the Bayes solution with respect to the least favorable priors (Q0mMd , Q1mMd ) (see Berger, 1985, for instance), which will be denoted as minimax deviation probabilities. Now, we extend the formulation to a general L-class problem. Deﬁnition 1 Consider a L-class decision problem with costs ci j , 0 ≤ i, j < L and c j j ≤ ci j , and let Rw (P) be the risk of a decision machine with parameter vector w when prior class probabilities are given by P = (P0 , . . . , PL−1 ). The deviation function is deﬁned as Dw (P) = Rw (P) − RB (P) and the minimax deviation is deﬁned as DmMd = inf max{Dw (P)} . w P (4) Note that the above deﬁnition assumes that the maximum exists. This is actually the case, since Dw (P) is a linear function over a compact set, P . Note, also, that our deﬁnition includes the natural assumption that c j j is never higher than ci j , meaning that making a decision error is always less costly than taking the correct decision. This assumption is used in part of our theoretical analysis. The algorithms proposed in this paper are based on the fact that the minimax deviation can be computed without knowing RB 109 A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I Theorem 2 The minimax deviation is given by DmMd = inf max{Dw (P)} , w P where Dw (P) = Rw (P) − Rbasis (P) and (5) L−1 ∑ c j j Pj Rbasis (P) = . (6) j=0 Proof Note that, according to Eqs. (1) and (2), for any decision machine and any u i ∈ UL , L−1 R(u j ) = R j = ∑ ci j P{d = ui |d = u j } ≥ c j j . i=0 Since the bound is reached by the classiﬁer deciding d = u j for any observation x, we have RB (u j ) = c j j . Therefore, using Eq. (6), we ﬁnd that, for any u ∈ UL , RB (u) = Rbasis (u) and, thus, Dw (u) = Dw (u) . Since Bayes minimum risk RB (P) is a convex function of priors and Rw (P) is linear, Dw (P) is concave and, thus, it is maximum at some of the vertices in P (i.e., at some P = u ∈ U L ). Thus, max{Dw (P)} = max {Dw (u)} . u∈UL P (7) Since the maximum difference between two hyperplanes deﬁned over P is always at some vertex, we can conclude that max{Dw (P)} = max {Dw (u)} = max {Dw (u)} . P u∈UL u∈UL (8) Combining Eqs. (4), (7) and (8), we get DmMd = inf max{Dw (P)} . w P Note that Rbasis represents the risk baseline of the ideal classiﬁer with zero errors. Th. 2 shows that the minimax regret can be computed as the minimax deviation to this ideal classiﬁer. Note, also, that if costs cii do not depend on i, Eq. (5) becomes equivalent (up to a constant) to the Bayes risk and the minimax regret classiﬁer becomes equivalent to the minimax classiﬁer . Another important result for the algorithms proposed in this paper is that, under some conditions on the minimum risk, the minimum and maximum operators can be permuted. Although general results on the permutability of minimum and maximum operators can be found in the literature (see Polak, 1997, for instance), we provide here the proof for the speciﬁc case interesting to this paper. 110 M INIMAX R EGRET C LASSIFIER Theorem 3 Consider the minimum deviation function given by Dmin (P) = inf{Dw (P)} , (9) w where Dw (P) is the normalized deviation function given by Eq. (5), and let P ∗ be the prior probability vector providing the maximum deviation, P∗ = arg max Dmin (P) P . (10) If Dmin (P) is continuously differentiable at P = P∗ , then the minimax deviation, DmMd , deﬁned by Eq. (4), is DmMd = Dmin (P∗ ) = max inf Dw (P) . (11) P w Proof For any classiﬁer with parameter vector w, we can write, max Dw (P) ≥ Dw (P∗ ) ≥ Dmin (P∗ ) P and, thus, inf max Dw (P) ≥ Dmin (P∗ ) . w P (12) Therefore, Dmin (P∗ ) is a lower bound of the minimax regret. Now we prove that Dmin (P∗ ) is also an upper bound. According to Eq. (9), for any ε > 0, there exists a parameter vector wε such that Dwε (P∗ ) ≤ Dmin (P∗ ) + ε . (13) By deﬁnition, for any P, Dmin (P) ≤ Dwε (P). Therefore, using Eq. (13), we can write Dwε (P∗ ) − Dwε (P) ≤ Dmin (P∗ ) − Dmin (P) + ε . (14) Since Dmin (P) is continuously differentiable and (according to Eq. (10)) maximum at P ∗ , for any ε > 0 there exists δ > 0 such that, for any P ∈ P with P∗ − P ≤ δ we have Dmin (P∗ ) − Dmin (P) ≤ ε P∗ − P ≤ ε δ . (15) Let Pδ a prior such that P∗ − Pδ = δ. Taking ε = ε δ and combining Eqs. (14) and (15) we can write Dwε (P∗ ) − Dwε (Pδ ) ≤ 2ε δ . Since the above condition is veriﬁed for any ε > 0 and any prior Pδ at distance δ from P, and taking into account that Dwε (P) is a linear function of P, we conclude that the maximum slope of D wε (P) is bounded by 2ε and, thus, for any P ∈ P , we have √ Dwε (P) − Dwε (P∗ ) ≤ 2ε P − P∗ ≤ 2 2ε , √ (where we have used the fact that the maximum distance between two probability vectors is 2). Therefore, we can write √ max Dwε (P) ≤ Dwε (P∗ ) + 2 2ε P 111 A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I and, thus, √ inf max Dw (P) ≤ Dwε (P∗ ) + 2 2ε . w P √ Finally, using Eq. (13) and taking into account that ε = ε δ ≤ 2ε we get √ inf max Dw (P) ≤ Dmin (P∗ ) + 3 2ε . w P (16) Since the above is true for any ε > 0 we conclude that Dmin (P∗ ) is also an upper bound of Dw . Therefore, combining Eqs. (12) and (16), we conclude that inf max Dw (P) = Dmin (P∗ ) , w P which completes the proof. Note that the deviation function needs to be neither differentiable nor a continuous function of w parameters. If the minimum deviation function is not continuously differentiable at the minimax deviation probability, P∗ , the theorem cannot be applied. The reason is that, although there should exist at least one classiﬁer providing the minimum deviation at P = P∗ , it or they could not provide a constant deviation with respect to the prior probability. The situation can be illustrated with an example. Let x ∈ R be given by p(x|d = 0) = 0.8N(x, σ) + 0.2N(x − 2, σ) and p(x|d = 1) = 0.2N(x − 1, σ) + 0.8N(x − 3, σ), where σ = 0.5 and N(x, σ) = (2πσ)−1/2 exp(−x2 /(2σ2 )), and consider the set Φλ of classiﬁers given by a single threshold over x and decision dˆ = 1 if x ≥ λ 0 if x < λ. Fig. 2 shows the distribution of both classes over x, and Fig. 3 shows, as a function of priors, the minimum error probability (continuous line) that can be obtained using classiﬁers in Φ λ . Note that decision costs c00 = c11 = 0 and c01 = c10 = 1 have been considered for this illustrative problem. An abrupt slope change is observed at the minimax deviation probability, for P{d = 1} = 1/2. For this prior, there are two single threshold classiﬁers providing the minimum error probability, which are given by thresholds λ1 and λ2 in Fig. 2. However, as shown in Fig. 3 neither of them provides a risk that is constant in the prior. The minimax deviation classiﬁer in Φ λ , which has a threshold λ0 , does not attain minimum risk at the minimax deviation probability and, thus, cannot be obtained by using Eq. (11). For this example, the desired robust classiﬁer should have a deviation function given by the horizontal dotted line in Fig. 3. Fortunately, it can be obtained by combining the outputs of several classiﬁers. For instance, let dˆ1 and dˆ2 the decisions of classiﬁers given by thresholds λ1 and λ2 , respectively. It is not difﬁcult to see that the classiﬁer selecting dˆ1 and dˆ2 at random (for each input sample x) provides a robust classiﬁer. This procedure can be extended to the multiclass-case: consider a set of L classiﬁers with parameters wk , k = 0, . . . , L − 1, and consider the classiﬁer such that, for any input sample x, makes a decision equal to dk (i.e., the decision of classiﬁer with parameters wk ), with probability qk . It is not difﬁcult to show that the deviation function of this classiﬁer is given by L−1 D(P) = L−1 j=0 k=0 ∑ Pj ∑ qk D j (wk ) 112 , M INIMAX R EGRET C LASSIFIER 0.7 0.6 Likelihoods 0.5 0.4 0.3 0.2 0.1 λ 0 −2 λ −1 0 λ 0 1 1 2 2 3 4 5 x Figure 2: The conditional data distributions for the one-dimensional example discussed in the text. λ1 and λ2 are the thresholds providing the minimum risk at the minimax deviation probability. λ0 provides the minimax deviation classiﬁer. where D j (wk ) = R j (wk ) − c j j . In order to get a constant deviation function, probabilities q k should be chosen in such a way that L−1 ∑ qk D j (wk ) = D , k=0 where D is a constant. Solving these linear equations for q k , k = 0, . . . , L − 1 (with the constraint ∑k qk = 1), the required probabilities can be found. Note that, in order to build the non-deterministic classiﬁer providing a constant deviation, a set of L independent classiﬁers that are optimal at the minimax deviation prior should be found. However, we go no further on the investigation of this special case for two main reasons: • The situation does not seem to be common in practice. In our simulations, we have found that the maximum of the minimum risk deviation always provided a response which is approximately parallel to Rbasis . • In general, the abrupt change in the derivative may be a symptom that the classiﬁer structure is not optimal for the data distribution. Instead of building a nondeterministic classiﬁer, increasing the classiﬁer complexity should be more efﬁcient. Although the least favorable prior providing the minimax deviation can be computed in closed form for some simple distributions, in general, it must be computed numerically. Moreover, we assume here that the data distribution is not known, and must be learned from examples. Thus, 113 A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I 0.25 0.2 λ Error probability 0 0.15 λ λ 1 2 0.1 0.05 0 0 0.2 0.4 0.6 0.8 1 P{ d=1} Figure 3: Error probabilities as a function of prior probability of class 1 for the example in Fig. 2. Thresholds λ1 and λ2 do not provide the minimax deviation classiﬁer, which is obtained for threshold λ0 . However, the random combination of classiﬁers with thresholds λ 1 and λ2 (dotted line) provides a robust classiﬁer with deviation lower than that of λ 0 . we must incorporate the estimation of the least favorable prior in the learning process. Next, we propose a training algorithm in order to get a minimax regret classiﬁer based on neural networks. 4. Neural Robust Classiﬁers Under Complete Uncertainty Note that, if QmMd is the probability vector providing the maximum in Eq. (11), that is, QmMd = arg max inf{Dw (P)} w P , then we can write DmMd = inf{Dw (QmMd )} . w Therefore, the minimax deviation classiﬁer can be estimated by training a classiﬁer using prior in QmMd . For this reason, QmMd will be called the minimax deviation prior (or least favorable prior). Our proposed algorithms are based on an iterative process of estimating parameters w based on an estimate of the minimax deviation prior, and re-estimating prior based on an estimate of network weights. This is shown in the following. 114 M INIMAX R EGRET C LASSIFIER 4.1 Updating Network Weights Learning is based on minimizing some empirical estimate of the overall error function L−1 L−1 i=0 E{C(y, d)} = i=0 ∑ P{d = ui }E{C(y, d)|d = ui } = ∑ PiCi , where C(y, d) may be any error function and Ci is the expected conditional error for class-i. Selecting the appropriate error function (see Cid-Sueiro and Figueiras-Vidal, 2001, for instance), learning rules can be designed providing a posteriori probability estimates (y i ≈ P{d = ui |x}, where yi is the soft decision) and, thus, according to Eq. (3), the hard decision minimizing the risk can be approximated by L−1 d = arg min { ∑ ci j y j } . i j=0 The overall empirical error function (cost function) used in learning for priors P = (P0 , . . . , PL−1 ) may be written as L−1 C = ∑ PiCi = L−1 i=0 = = 1 K L−1 i=0 1 K k ∑ d C(yk , dk ), Ki k=1 i Pi K k ∑ d C(yk , dk ) Ki /K k=1 i ∑ i=0 1 K ∑ K k=1 ∑ Pi L−1 ∑ Pi d kC(yk , dk ) (0) i i=0 Pi , , (17) (0) where Pi = Ki /K is an initial estimate of class-i prior based on class frequencies in the training set and Pi is the current prior estimate. Minimizing error function (17) by means of a stochastic gradient descent learning rule leads to update the network weights at k-th iteration as w (k+1) = w (k) (n) L−1 −µ = w(k) − Pi i=0 Pi ∑ L−1 d k ∇ C(yk , dk ) (0) i w ∑ µi (n) k di , ∇wC(yk , dk ) , (18) i=0 where (n) (n) µi = µ Pi (19) (0) Pi (n) is a learning step scaled by the prior ratio. Note that di selects the appropriate µi according to the pattern class membership. The classiﬁer is trained without altering the original training data set (0) class distribution Pi and therefore, without missing or duplicating information. 115 A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I 4.2 Updating Prior Probabilities Eq. (11) shows that the learning process should maximize (5) with respect to the prior probabilities. The estimate of (5) can be computed as ¯ Dw (P) = Rw (P) − Rbasis (P) , where (20) L−1 ∑ R j Pj (21) 1 L−1 ∑ ci j Ni j N j i=0 (22) Rw (P) = j=0 is the overall Bayes risk estimate and Rj = is the class- j conditional risk estimate where N j is the number of class u j patterns in the training phase and Ni j is the number of samples from class u j assigned to ui . L−1 In order to derive a learning rule to ﬁnd an estimate Pi satisfying constraints ∑i=0 Pi = 1 and 0 ≤ Pi ≤ 1, we will use auxiliary variables Bi such that Pi = exp(Bi ) L−1 ∑ j=0 exp(B j ) . (23) ¯ We maximize Dw with respect to Bi . Applying the chain rule, ¯ ¯ ∂Dw L−1 ∂Dw ∂Pj =∑ , ∂Bi j=0 ∂Pj ∂Bi and using Eqs. (20), (21) and (23), we get ¯ ∂D w ∂Bi L−1 = ∑ (R j − c j j )Pi (δi j − Pj ), j=0 L−1 L−1 j=0 j=0 = Pi Ri − cii − ∑ (R j Pj ) + ∑ (c j j Pj ) , = Pi Ri − cii − Rw − Rbasis , = Pi Rdi , where Rdi = (Ri − cii ) − (Rw − Rbasis ) . The learning rule for auxiliary variable Bi is (n) Bi (n+1) = Bi + ρ (n) ∂D w , ∂Bi (n) (n) = Bi + ρPi Rdi , 116 (24) M INIMAX R EGRET C LASSIFIER where parameter ρ > 0 controls the rate of convergence. Using Eq. (23) and Eq. (24), the updated learning rule for Pi is (n) (n+1) Pi = (n) (n) (n) exp ρPj Rd j ∑L−1 exp B j j=0 (n) = (n) (n) exp(Bi ) exp ρPi Rdi , (n) (n) Pi exp ρPi Rdi (n) (n) (n) ∑L−1 Pj exp ρPj Rd j j=0 . (25) 4.3 Training Algorithm for a Minimax Deviation Classiﬁer In the previous section, both the network weights updating rule (18) and the prior probability update rule (25) have been derived. The algorithm resulting from the combination is shown as follows: for n = 0 to Niterations − 1 do for k = 1 to K do w(k+1) = w(k) − L−1 ∑ µi (n) k di ∇wC(yk , dk ) i=0 end for (n) Estimate R(n) , Ri , i = 0, . . . , L − 1, according to (21) and (22) (n+1) (n+1) Update minimax probability Pi , i = 0, . . . , L − 1 according to (25) and compute µi with (19) end for 5. Robust Classiﬁers Under Partial Uncertainty Although in many practical situations prior probabilities may not be speciﬁed with precision, they can be partially known. In this section we discuss how partial information about priors can be used to improve the classiﬁer performance in relation to a complete uncertainty situation. From now on, let us consider that lower (or upper) bounds of the priors are known based on previous experience. We will denote the lower and upper bounds of class-i prior probability as Pil and Piu , respectively. In order to illustrate this situation consider a binary classiﬁcation problem where probability lower bounds P0l and P1l are known. That is, P1 ∈ [P1l , 1 − P0l ] where this interval represents the uncertainty region. Let us denote by Γ = {P : 0 ≤ Pi ≤ 1, ∑L−1 Pi = 1, Pi ≥ Pil } the probability region i=0 satisfying the imposed constraints. In the following, we will refer to Γ as the uncertainty region. Now, the aim is to design a classiﬁer that minimizes the maximum regret from the minimum risk only inside the uncertainty region. This is depicted in Fig. 4(a), which shows that reducing the uncertainty in priors allows to reduce deviation from the optimal classiﬁer. This minimax regret approach for the uncertainty region Γ is often called Γ-minimax regret. As discussed before, the minimax deviation solution gives a Bayes solution with respect some priors denoted in the partial uncertainty case as QΓ mMd in Fig. 4(a), which is the least favorable distribution according to the regret criterion. 117 A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I cΓ 00 RΓ basis RΓ 1 cΓ 11 PSfrag replacements Risk 0 0.5 1 1.5 2 2.5 3 3.5 0P1l RB (P) + ψ(P) Minimax Deviation with Restriction Risk RΓ 0 QΓ 1mMd 1 − P0l 1 P1 0P1l 1 − P0l 1 P1 (b) (a) Figure 4: Minimax deviation classiﬁer under partial uncertainty of prior probabilities: (a)Γ-minMaxDev Classiﬁer. (b) Modiﬁed cost function deﬁned as R B (P) + ψ(P). In contrast to the minimax regret criterion, note that a classical minimax classiﬁer for the considered uncertainty region would minimize the worst-case risk. It would be a Bayes solution for the prior where the minimum risk reaches its maximum and it could be denoted as Q Γ . mM Notice, also, that these solutions will be the same if the risk for the vertex of Γ take the same value (cΓ = k). ii 5.1 Neural Robust Classiﬁers Under Partial Uncertainty Minimax search can be formulated as maximizing (with respect to priors) the minimum (with respect to network parameters) of deviation function (5), as described in previous section, but subject to some constraints arg max inf {DΓ (P)} , w w P Pi ≥ Pil , i = 0, . . . , L − 1 s.t. where DΓ = RΓ − RΓ . When uncertainty is global, this hyperplane is deﬁned by the risk in the L w w basis extreme cases with Pi = δik , that is, by the corresponding cii . However, with partial knowledge of the prior probabilities, this hyperplane becomes deﬁned by the risk in L points which are the vertex given by the restrictions and with associated risk denoted by c Γj . j Deﬁning 1 l(Pi ) = , (26) 1 + exp−τ(Pi −Pil ) where τ controls the hardness of this restriction, the minimax problem can be re-formulated as arg max inf {DΓ (P)} w P s.t. w l(Pi ) ≥ 1/2, i = 0, . . . , L − 1. Thus, this constrained optimization problem can be solved as a non-constrained problem by considering an auxiliary function that incorporates the restriction as a barrier function 118 M INIMAX R EGRET C LASSIFIER arg max inf {DΓ (P) + Aψ(P)} , w w P where ψ(Pi ) = log(l(Pi )) and the constant A determines the contribution of the barrier function. Fig. 4(b) shows the new risk function corresponding to the binary case previously depicted in Fig. 4(a). Note that, it is the sum of the original RB (P) and the barrier function ψ(P). As in Section 4.1, in order to derive the network weight learning rule, we need to compute ∂ψ ∂Bi L−1 = ∂ψ ∂P j , j ∂Bi ∑ ∂P j=0 = τPi L−1 ∑ 1 − l(Pk ) (δik − Pk ), k=0 = τPi ψdi , where ψdi = ∑L−1 (1 − l(Pk ))(δik − Pk ) k=0 As τ increases, the constraints become harder around the speciﬁed bound. The update learning rule for the auxiliary variable Bi at cycle n is (n+1) Bi (n) Γ(n) (n) (n) = Bi (n) + ρPi Rdi + ρAτPi ψdi . And therefore, using (23), the update learning rule for Pi is (n) (n+1) Pi = (n) Γ(n) Pi exp ρPi Rdi L−1 ∑ (n) Pj exp (n) (n) exp ρAτPi ψdi (n) Γ(n) ρ P j Rd j . (n) (n) ρAτPj ψd j exp j=0 Note that if the upper bound is known instead of the lower bound, l(Pi ) deﬁned by (26) should be replaced by u(Pi ) = (1 + exp(τ(Pi − Piu )))−1 at the previous formulation. The minimax constrained optimization problem has been tackled by considering a new objective function deﬁned by the sum of the original cost function and a barrier function. Studying the convexity of this new function becomes important from the fact that a stationary point of this risk curve is a global maximum. Since the minimum risk curve (RB (P)) is a convex function of the priors (see VanTrees, 1968, for details), if we verify the convexity of the barrier function, we can conclude that the function deﬁned by the sum of both of them is also convex. This barrier function is convex in P if the Hessian matrix HR veriﬁes PT HR P ≤ 0 The Hessian matrix of the barrier function equals to a diagonal matrix D r = diag(r) with all negative diagonal entries ri = Aτ2 (−l(Pi )(1 − l(Pi ))). As l(Pi ) ∈ [0, 1] and therefore, ri ≤ 0, it is straightforward to see that PT HR P = PT Dr P, L−1 = ∑ Pi2 ri ≤ 0 . i=0 Since the barrier function is convex, the new objective function (deﬁned by the sum of two convex functions) is also convex. 119 A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I 5.2 Extension to Other Learning Algorithms The learning algorithm proposed in this paper is intended to train a minimax deviation classiﬁer based on neural networks with feedforward architecture. Actually, the learning algorithm we propose becomes a feasible solution for any learning process based on minimizing some empirical estimate of an overall cost (error) function. However, it is also applicable to a general classiﬁer provided it is trained (in an iterative process) for the estimated minimax deviation probabilities and the assumed decision costs. Speciﬁcally, in this paper, scaling the learning rate allows to simulate different class distributions and the hard decisions are made based on posterior probability estimates and decision costs. Furthermore, the neural learning phase carried out in one iteration can be re-used for the next one, what allows to reduce computational cost with respect to a complete optimization process on each iteration. Apart from the general approach of completely training a classiﬁer on each iteration and in order to reduce its computational cost, speciﬁc solutions may be studied for different learning machines. Nonetheless, it seems not feasible to readily achieve this improvement for classiﬁers like SVMs, where support vectors for one solution may have nothing in common with the ones obtained in next iteration and thus, making necessary to re-train the classiﬁer in each iteration. Another possible solution for any classiﬁer that provides a posteriori probabilities estimates or any score that can be converted into probabilities (for details on calibration methods see Wei et al., 1999; Zadrozny and Elkan, 2002; Niculescu-Mizil and Caruana, 2005) is outlined here. In this case, an iterative procedure able to estimate the minimax deviation probabilities and consequently to adjust (without re-training) the outputs of the classiﬁer could be studied. The general idea for this approach is as follows: ﬁrst, the new minimax deviation prior probabilities are estimated according to (25) and then, posterior probabilities provided by the model are adjusted as follows (see Saerens et al., 2002, for more details) (k) Pi P(k) {d = ui |x} = P(k−1) {d = ui |x} (k−1) Pi L−1 P(k) j P(k−1) {d = u j |x} (k−1) j=0 Pj . (27) ∑ The algorithm’s main structure is summarized as for k = 1 to K do (k) Estimate R(k) , Ri , i = 0, . . . , L − 1, according to (21), (22) and decision costs c i j (k+1) Update minimax probability Pi according to (25) Adjust classiﬁer outputs according to (27) end for The effectiveness of this method relies on the accuracy of the initial a posteriori probability estimates. Studying in depth this approach and comparing different minimax deviation classiﬁers (decision trees, SVMs, RBF networks, feedforward networks and committee machines) together with different probability calibration methods appears as a challenging issue to be explored in future work. 120 M INIMAX R EGRET C LASSIFIER 6. Experimental Results In this section, we ﬁrst present the neural network architecture used in the experiments and illustrate the proposed minimax deviation strategy on an artiﬁcial data set. Then, we apply it to several realworld classiﬁcation problems. Moreover, a comparison with other proposals such as the traditional minimax and the common re-balancing approach is carried out. 6.1 Softmax-based Network Although our algorithms can be applied to any classiﬁer architecture, we have chosen a neural network based on the softmax non-linearity with soft decisions given by Mi yi = ∑ yi j , j=1 with yi j = exp(wTj x + wi j0 ) i , Mk ∑L−1 ∑l=1 exp(wT x + wkl0 ) k=0 kl where L stands for the number of classes, M j the number of softmax outputs used to compute y j and wi j are weight vectors. We will refer to this network as a Generalized Softmax Perceptron(GSP). 1 A simple network with M j = 2 is used in the experiments. x1 wj,k y1,1 y1,... x2 x3 y1 y1,M1 Class i ... SOFTMAX ... HARD DECISION n inputs / outputs ... yL,1 xd yL,ML yL,... yL Figure 5: GSP(Generalized Softmax Perceptron) Network Fig. 5 corresponds to the neural network architecture used to classify the samples represented by feature vector x. Learning consists of estimating network parameters w by means of the stochastic gradient minimization of certain objective functions. In the experiments, we have considered the Cross Entropy objective function given by L CE(y, d) = − ∑ di log yi . i=1 The stochastic gradient learning rule for the GSP network is given by Eq. (18). Learning step µ(0) decreases according to µ(k) = 1+k/η , where k is the iteration number, µ(0) the initial learning rate and η a decay factor. µ(k) 1. Note that the GSP is similar to a two layer MLP with a single layer of weights and with coupled saturation function (softmax), instead of sigmoidal units. 121 A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I The reason to illustrate this approach with a feedforward architecture is that, as mentioned in Section 5.2, it allows to exploit (in the iterative learning process) the partially optimized solution in current iteration for the next one. On the other hand, posterior probability estimation makes it possible to apply the adaptive strategy based on prior re-estimation proposed by Saerens to the minimax deviation classiﬁer, as long as a data set representative of the operation conditions is available. Finally, the fact that intermediate outputs yi j of the GSP can be interpreted as subclass probabilities may provide quite a natural way to cope with the unexplored problem of uncertainty in subclass distributions as already pointed out by Webb and Ting (2005). Nonetheless, both architecture and cost function issues are not the goal of this paper, but merely illustrative tools. 6.2 Artiﬁcial Data Set To illustrate the minimax regret approach proposed in this paper both under complete and partial uncertainty, an artiﬁcial data set with two classes (class u0 and class u1 ) has been created. Data examples are drawn from the normal distribution p(x|d = ui ) = N(mi , σ2 ) with mean mi and standard i √ deviation σi . Mean values were set to m0 = 0, m1 = 2 and standard deviation to σ0 = σ1 = 2. A total of 4000 instances were generated with prior probabilities of class membership P{d = u 0 } = 0.93 c00 c01 2 5 and P{d = u1 } = 0.07. The cost-beneﬁt matrix is given by . c10 c11 4 0 Initial learning rate was set to µ(0) = 0.3, decay factor to η = 2000 and training was ended after 80 cycles. Classiﬁer assessment was carried out by following 10-fold cross-validation. Two classiﬁers were trained, to be called a standard classiﬁer and a minMaxDev classiﬁer. The former is built by considering that the estimated class prior information is precise and stationary and the latter is the approach proposed in this paper to cope with uncertainty in priors. Thus, for the standard classiﬁer, its performance may deviate from the optimal risk in 3.39 when priors change from training to test conditions. However, a minimax deviation classiﬁer reduces this worst-case difference from the optimal classiﬁer to 0.77. Now, we suppose that some information about priors is available (partial uncertainty). For instance, we consider that the lower bound for prior probabilities P0 and P1 are known and set to P0l = 0.55 and P1l = 0.05, respectively, so that the uncertainty region is Γ = {(P0 , P1 )|P0 ∈ [0.55, 0.95], P1 ∈ [0.05, 0.45]}. A minimax deviation classiﬁer can be derived for Γ (it will be called Γ-minMaxDev classiﬁer).The narrower Γ is, the closer the minimax deviation classiﬁer performance is to the optimal. For this particular case, under partially imprecise priors, the standard classiﬁer may differ from optimal (in Γ) in 0.83, while the use of the simple minMaxDev classiﬁer designed under total prior uncertainty conditions attains a maximum deviation of 0.53. However, the Γ-minMaxDev classiﬁer only differs from optimal in 0.24. These data are reported in Table 1 where both, experimental and also theoretical results, are shown. 6.3 Real Databases In this section we report experimental results obtained with several publicly available data sets. From the UCI repository (Blake and Merz, 1998) the following benchmarks: German Credits, Australian Credits, Insurance Company, DNA slice-junction, Page-blocks, Dermatology and Pen-digits. 122 M INIMAX R EGRET C LASSIFIER Standard Th/Exp Maximum deviation from optimal (complete uncertainty) Maximum deviation from optimal in Γ (partial uncertainty) Classiﬁer minMaxDev Γ-minMaxDev Th/Exp Th/Exp 3.41/3.39 0.72/0.77 – 0.85/0.83 0.50/0.53 0.19/0.24 Table 1: A comparison between the standard classiﬁer (build under stationary prior assumptions), the minimax deviation classiﬁer (minMaxDev) and the minimax deviation classiﬁer under partial uncertainty (Γ-minMaxDev) for an artiﬁcial data set Database German Credits (GCRE) Australian Credits (AUS) Munich Credits (MCRE) Insurance Company (COIL) DNA Slice-junction (DNA) Page-blocks (PAG) Dermatology (DER) Pen-digits (PEN) # Classes 2 2 2 2 3 5 6 10 Class distribution [0.70 0.30] [0.32 0.68] [0.30 0.70] [0.94 0.06] [0.24 0.24 0.52] [0.90 0.06 0.01 0.01 0.02] [0.31 0.16 0.20 0.13 0.14 0.06] [0.104 0.104 0.104 0.096 0.104 0.096 0.096 0.104 0.096 0.096] # Attributes 8 14 20 85 180 10 34 16 # Instances 1000 690 1000 9822 3186 5473 366 10992 Table 2: Experimental Data sets Other public data set used is Munich Credits from the Dept. of Statistics at the University of Munich.2 Data set description is summarized in Table 2, and cost-beneﬁt matrices are shown in Table 3. We have used the cost values that appear in Ikizler (2002) for those data sets in common. Otherwise, for lack of an expert analyst, the cost values have been chosen by hand. 2. Data sets available at http://www.stat.uni-muenchen.de/service/datenarchiv/welcome e.html. Insurance Company 0 1 German, Australian, Munich Credits −1 0 0 −17 Page-Blocks  −1  2   2   2 2 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0               −4 2 3 4 3 4 3 −3 3 5 1 5  5 0 Dermatology 3 2 3 2 −8 4 5 −10 4 3 5 4 2 1 4 5 −6 5 2 3 5 2 3 −10         −1  2 2 DNA 2 −1 2 Pendigits ci j = 0 1 Table 3: Cost-Beneﬁt matrices for the experimental Data sets 123  3 3  0 if i = j Otherwise A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I Standard Maximum Risk Deviation from the optimal classiﬁer Re-balanced Minimax Deviation Minimax minMaxDev minMax GCRE 0.70 0.80 (0.55 0.60) 0.99 ACRE 1.00 1.00 (0.76 0.86) 1.00 MCRE 0.91 0.77 (0.54 0.59) 0.99 COIL 2.78 0.99 (0.87 0.92) 16.32 DNA 0.34 0.53 (0.30 0.27 0.25) PAG 0.62 0.26 (0.13 0.13 0.20 0.16 0.16) DER 1.03 1.28 (0.67 0.78 0.51 0.48 0.54 PEN 0.061 0.059 (0.024 0.028 0.025 0.019 1.14 0.023 0.021 0.026 0.022 0.86 0.60) 0.023 0.029) 7.62 0.029 Table 4: Classiﬁer Performance evaluated as Maximum Risk Deviation from the optimal classiﬁer for several real-world applications. Class-conditional risk deviations (R i − cii ) reported for the minMaxDev classiﬁer. Experimental results for these data sets are shown in the following sections. The robustness of different decision machines under complete uncertainty of prior probabilities is analyzed in Section 6.3.1. If uncertainty is only partial, a similar study and comparison with the previous approach (complete uncertainty) is carried out in Section 6.3.2. 6.3.1 C LASSIFIER ROBUSTNESS U NDER C OMPLETE U NCERTAINTY We now study how different neural-based classiﬁers cope with worst-case situations in prior probabilities. The maximum deviation from the optimal classiﬁer (see Table 4) is reported for the proposed minMaxDev strategy as well as for other alternative approaches: the one based on the assumption of stationary priors (standard) and the common alternative of deriving the classiﬁer from an equally distributed data set (re-balanced). A comparison with the traditional minimax strategy is also provided. Together with the previously mentioned value (maximum deviation or regret), deviation for the L class-conditional extreme cases (Ri − cii ) is also reported for the minMaxDev classiﬁer in Table 4. Results allow to verify that this solution is fairly close to the optimal one where deviation is not dependent on priors and thus, class-conditional deviations take the same value. Although the balanced class distribution to train the classiﬁer can be obtained by means of undersampling and/or oversampling, it is simulated by altering the learning rate used in the training 1/L phase according to (19) as µi = µ (0) , where 1/L represents the simulated probability, equal for Pi all classes. Results evidence that the assumption of stationary priors may lead to signiﬁcant deviations from the optimal decision rule under “unexpected”, but rather realistic, prior changes. This deviation may reach up to three times more than the robust minimax deviation strategy. Thus, for classiﬁcation problems like Page-blocks the maximum deviation from the optimal classiﬁer is 0.62 for the 124 M INIMAX R EGRET C LASSIFIER Standard Maximum Risk Re-balanced Minimax Deviation minMaxDev Minimax minMax GCRE 0.70 0.15 0.60 0.00 ACRE 0.01 0.02 0.86 -0.00 MCRE 0.05 0.20 0.59 0.00 COIL 0.76 0.99 0.86 0.02 DNA 0.34 0.53 0.25 0.13 PAG 0.62 0.26 0.20 0.10 DER -2.10 -1.68 -2.21 -2.38 PEN 0.061 0.059 0.029 0.029 Table 5: Classiﬁer Performance measured as Maximum Risk for several real-world applications. standard classiﬁer while this reduces to 0.20 for the minMaxDev one. Likewise, for the Insurance company(COIL) application the maximum deviation for the standard classiﬁer is 2.78 compared with 0.92 for the minMaxDev model. The remaining databases also show the same behavior as it is presented in Table 4. On the other hand, the use of a classiﬁer inferred from a re-balanced data set does not necessarily involve a decrease in the maximum deviation with respect to the standard classiﬁer. In the same way, the traditional minimax classiﬁer does not protect against prior changes in terms of maximum relative deviation from the minimum risk classiﬁer. However, if our criterion is more conservative and our aim is the minimization of the maximum possible risk (not the minimization of the deviation), the traditional minimax classiﬁer represents the best option. It is shown in Table 5 where the maximum risk for the different classiﬁers is reported. Positive values in this table indicate a cost while negative values represent a beneﬁt. For instance, for the Page-blocks application the minimax classiﬁer assures a maximum risk of 0.10 while the standard, re-balanced and minMaxDev classiﬁers reach values of 0.62, 0.26 and 0.20, respectively. It can be noticed that for the Pen-digits data set, the minimax deviation and minimax approaches attain the same results. The reason is that, for this problem, the R basis plane takes the same value (in this case, zero) in the probability space. 6.3.2 C LASSIFIER ROBUSTNESS UNDER PARTIAL U NCERTAINTY Unlike the previous section, we consider now that partial information about the class priors is available. The aim is to ﬁnd a classiﬁer that behaves well for a delimited and realistic range of priors what constitutes an aid in reducing the maximum deviation from the optimal classiﬁer. This situation can be treated as a constrained minimax regret strategy where the constraints represent any extra information about prior probability value. Experimental results for several situations of partial prior uncertainty are presented in this section. We consider that lower bounds for the prior probabilities are available (see Table 6). In order to get the Γ-minMaxDev classiﬁer, the risk for the different vertex of the uncertainty domain needs to be calculated. With them, the basis risk RΓ over which deviations are measured is derived. basis 125 A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I Lower bound for prior probabilities Data Set P0l P1l GCRE 0.40 0.25 ACRE 0.20 0.25 MCRE 0.20 0.25 COIL 0.15 P2l P3l P4l P5l 0.03 DNA 0.10 0.10 0.22 0.02 0.00 0.01 0.1 0.20 0.10 0.10 0.10 0.10 0.06 0.06 0.10 0.10 0.06 P9l 0.06 0.10 0.05 0.05 0.02 PEN P8l 0.02 DER P7l 0.25 PAG P6l Table 6: Lower bounds for prior probabilities deﬁning the uncertainty region, Γ region for the experimental data sets. Maximum Risk Deviation in the uncertainty region Standard Minimax Deviation Minimax Deviation with restriction minMaxDev Γ-minMaxDev GCRE 0.24 0.19 (0.10 0.09) ACRE 0.03 0.64 (0.03 0.03) MCRE 0.22 0.38 (0.13 0.10) COIL 2.33 0.77 (0.17 0.11) DNA 0.14 0.08 (0.07 0.07 0.06) PAG 0.37 0.15 (0.10 0.08 0.08 0.05 0.04) DER 0.08 0.05 (0.03 0.03 0.04 0.02 0.05 PEN 0.013 0.007 (0.003 0.001 0.003 0.000 0.001 0.001 0.000 0.003 0.05) 0.001 0.001) Table 7: Classiﬁer Performance under partial knowledge of prior probabilities measured as Maximum Risk Deviation for several real-world applications. Class-conditional risk deviations (RΓ − cΓ ) are reported for the Γ-minMaxDev classiﬁer. i ii Maximum deviation from the optimal in Γ is reported for the Γ-minMaxDev classiﬁer together with the standard and the minMaxDev ones. For instance, the standard classiﬁer for the Pageblocks data set deviates from the optimal classiﬁer, in the deﬁned uncertainty region, up to 0.37, while when complete uncertainty is assumed the maximum deviation is equal to 0.62. In the same way, reducing the uncertainty also means a reduction in the maximum deviation for minMaxDev classiﬁer (trained without considering this partial knowledge). Thus, for Γ, this classiﬁer assures a deviation bound of 0.15. However, taking into account this partial information to train a Γ-minMaxDev classiﬁer allows to reduce the deviation for the worst-case conditions to 0.10. It can be seen the same behavior for the other databases in Table 7. 126 M INIMAX R EGRET C LASSIFIER 7. Conclusions This work concerns the design of robust neural-based classiﬁers when the prior probabilities of the classes are partially or completely unknown, even by the end user. This problem of uncertainty in the class priors is often ignored in supervised classiﬁcation, even though it is a widespread situation in real world applications. As a result, the reliability of the inducted classiﬁer can be greatly affected as previously shown by the experiments. To tackle this problem, we have proposed a novel minimax deviation strategy with the goal to minimize the maximum deviation with respect to the optimal classiﬁer. A neural network training algorithm based on learning rate scaling has been developed. The experimental results show that this minimax deviation (minMaxDev) classiﬁer protects against prior changes while other approaches like ignoring this uncertainty or use a balanced learning data set may result in large differences in performance with respect to the minimum risk classiﬁer. Also, it has been shown that the conventional minimax classiﬁer reduces the maximum possible risk following a conservative attitude but at the expense of large worst-case differences from the optimal classiﬁer. Furthermore, a constrained minimax deviation approach (Γ-minMaxDev) has been derived for those situations where uncertainty is only partial. This may be seen as a general approach with some particular cases: a) precise knowledge of prior probabilities and b) complete uncertainty about the priors. In a) the region of uncertainty collapses to a point and we have the Bayes’ rule of minimum risk and in b) the pure minimax deviation strategy comes up. While the ﬁrst one may be criticized for being quite unrealistic, the other may be seen rather pessimistic. The experimental results for this proposed intermediate situation show that the Γ-minMaxDev classiﬁer allows to reduce the maximum deviation from the optimal and performs well over a range of prior probabilities. Acknowledgments The authors thank the four referees and the associate editor for their helpful comments. This work was partially supported by the project TEC2005-06766-C03-02 from the Spanish Ministry of Education and Science. References N. Abe, B. Zadrozny, and J. Langford. An iterative method for multi-class cost-sensitive learning. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 3–11, 2004. N. M. Adams and D. J. Hand. Comparing classiﬁers when the misallocation costs are uncertain. Pattern Recognition, 32(7):1139–1147, March 1998. R. Alaiz-Rodriguez, A. Guerrero-Curieses, and J. Cid-Sueiro. Minimax classiﬁers based on neural networks. Pattern Recognition, 38(1):29–39, January 2005. R. Barandela, J. S. Sanchez, V. Garc´a, and E. Rangel. Strategies for learning in class imbalance ı problems. Pattern Recognition, 36(3):849–851, March 2003. J. O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer, second edition, 1985. 127 A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I C. L. Blake and C. J. Merz. UCI repository of machine learning databases, 1998. http://www.ics.uci.edu/ mlearn/MLRepository.html. URL L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and Regression Trees. Chapman & Hall, NY, 1984. N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: Synthetic minority oversampling technique. Journal of Artiﬁcial Intelligence Research, 16:321–357, 2002. J. Cid-Sueiro and A. R. Figueiras-Vidal. On the structure of strict sense Bayesian cost functions and its applications. IEEE Transactions on Neural Networks, 12(3):445–455, May 2001. C. Drummond and R. C. Holte. Explicitly representing expected cost: An alternative to ROC representation. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 198–207. ACM Press, 2000. R. O. Duda, P. E. Hart, and D. G. Stork. Pattern Classiﬁcation. John Wiley and Sons, 2001. Y. C. Eldar and N. Merhav. Minimax approach to robust estimation of random parameters. IEEE Trans. on Signal Processing, 52(7):1931–1946, July 2004. Y. C. Eldar, A. Ben-Tal, and A. Nemirovski. Linear minimax regret estimation of deterministic parameters with bounded data uncertainties. IEEE Trans. on Signal Processing, 52(8):2177– 2188, August 2004. M. Feder and N. Merhav. Universal composite hypothesis testing: A competitive minimax approach. IEEE Trans. on Information Theory, 48(6):1504–1517, June 2002. A. Guerrero-Curieses, R. Alaiz-Rodriguez, and J. Cid-Sueiro. A ﬁxed-point algorithm to minimax learning with neural networks. IEEE Transactions on Systems, Man and Cybernetics Part C, 34 (4):383–392, November 2004. ¨ H. A. G¨ venir, N. Emeksiz, N. Ikizler, and N. Ormeci. Diagnosis of gastric carcinoma by classiﬁu cation on feature projections. Artiﬁcial Intelligence in Medicine, 31(3), 2004. N. Ikizler. Beneﬁt maximizing classiﬁcation using feature intervals. Technical Report BU-CE-0208, Bilkent University, Ankara, Turkey, 2002. N. Japkowicz and S. Stephen. The class imbalance problem: A systematic study. Intelligent Data Analysis Journal, 6(5):429–450, November 2002. M. G. Kelly, D. J. Hand, and N. M. Adams. The impact of changing populations on classiﬁer performance. In Proceedings of Fifth International Conference on SIG Knowledge Discovery and Data Mining (SIGKDD), pages 367–371, San Diego, CA, 1999. H. J. Kim. On a constrained optimal rule for classiﬁcation with unknown prior individual group membership. Journal of Multivariate Analysis, 59(2):166–186, November 1996. M. Kubat and S. Matwin. Addressing the curse of imbalanced training sets: One-sided selection. In Proceedings 14th International Conference on Machine Learning, pages 179–186. Morgan Kaufmann, 1997. 128 M INIMAX R EGRET C LASSIFIER M. Kubat, R. Holte, and S. Matwin. Machine learning for the detection of oil spills in satellite radar images. Machine Learning, 30(2/3):195–215, 1998. S. Lawrence, I. Burns, A. D. Back, A. C. Tsoi, and C. L. Giles. Neural network classiﬁcation and ¨ unequal prior class probabilities. In G. Orr, K.-R. Muller, and R. Caruana, editors, Tricks of the Trade, Lecture Notes in Computer Science State-of-the-Art Surveys, pages 299–314. Springer Verlag, 1998. T. K. Moon and W. C. Stirling. Mathematical Methods and Algorithms for Signal Processing. Prentice Hall, 2000. A. Niculescu-Mizil and R. Caruana. Predicting good probabilities with supervised learning. In ICML ’05: Proceedings of the 22nd International Conference on Machine learning, pages 625– 632, New York, NY, USA, 2005. ACM Press. ISBN 1-59593-180-5. E. Polak. Optimization: Algorithms and Consistent Approximations. Springer, 1997. F. Provost. Learning with imbalanced data sets 101. In Invited paper for the AAAI 2000 Workshop on Imbalanced Data Sets. AAAI Press. Technical Report WS-00-05, 2000. F. Provost and T. Fawcett. Robust classiﬁcation systems for imprecise environments. Machine Learning, 42(3):203–231, March 2001. M. Saerens, P. Latinne, and C. Decaestecker. Adjusting a classiﬁer for new a priori probabilities: A simple procedure. Neural Computation, 14:21–41, January 2002. E. Takimoto and M. Warmuth. The minimax strategy for Gaussian density estimation. In Proceedings 13th Annual Conference on Computational Learning Theory, pages 100–106. Morgan Kaufmann, San Francisco, 2000. K. M. Ting. A study of the effect of class distribution using cost-sensitive learning. In Proceedings of the Fifth International Conference on Discovery Science, pages 98–112. Berlin: Springer-Verlag, 2002. H. L. VanTrees. Detection, Estimation and Modulation Theory. John Wiley and Sons, 1968. G. I. Webb and K. M. Ting. On the application of ROC analysis to predict classiﬁcation performance under varying class distributions. Machine Learning, 58(1):25–32, 2005. W. Wei, T. K. Leen, and E. Barnard. A fast histogram-based postprocessor that improves posterior probability estimates. Neural Computation, 11(5):1235 – 1248, July 1999. B. Zadrozny and C. Elkan. Learning and making decisions when costs and probabilities are both unknown. In Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 204–213. ACM Press, 2001. B. Zadrozny and C. Elkan. Transforming classiﬁer scores into accurate multiclass probability estimates. In Eighth International Conference on Knowledge Discovery and Data Mining, 2002. 129 A LAIZ -RODR´GUEZ , G UERRERO -C URIESES , AND C ID -S UEIRO I B. Zadrozny, J. Langford, and N. Abe. Cost-sensitive learning by cost-proportionate example weighting. In Proceedings of the third IEEE International Conference on Data Mining, pages 435–442, 2003. Z. H. Zhou and X. Y. LiuJ. Training cost-sensitive neural networks with methods addressing the class imbalance problem. IEEE Transactions on Knowledge and Data Engineering, 18(1):63–77, January 2006. 130</p><p>5 0.25236499 <a title="65-lsi-5" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>6 0.20341435 <a title="65-lsi-6" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>7 0.18020904 <a title="65-lsi-7" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>8 0.16942239 <a title="65-lsi-8" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>9 0.16907889 <a title="65-lsi-9" href="./jmlr-2007-Hierarchical_Average_Reward_Reinforcement_Learning.html">41 jmlr-2007-Hierarchical Average Reward Reinforcement Learning</a></p>
<p>10 0.15736887 <a title="65-lsi-10" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>11 0.14818512 <a title="65-lsi-11" href="./jmlr-2007-Nonlinear_Boosting_Projections_for_Ensemble_Construction.html">59 jmlr-2007-Nonlinear Boosting Projections for Ensemble Construction</a></p>
<p>12 0.14197387 <a title="65-lsi-12" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>13 0.13280129 <a title="65-lsi-13" href="./jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</a></p>
<p>14 0.12915675 <a title="65-lsi-14" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>15 0.11840717 <a title="65-lsi-15" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>16 0.11771267 <a title="65-lsi-16" href="./jmlr-2007-Multi-class_Protein_Classification_Using_Adaptive_Codes.html">57 jmlr-2007-Multi-class Protein Classification Using Adaptive Codes</a></p>
<p>17 0.11758637 <a title="65-lsi-17" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>18 0.11166081 <a title="65-lsi-18" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>19 0.10910228 <a title="65-lsi-19" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>20 0.1070511 <a title="65-lsi-20" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.557), (8, 0.019), (10, 0.015), (12, 0.025), (13, 0.011), (15, 0.014), (28, 0.022), (40, 0.027), (48, 0.014), (60, 0.071), (80, 0.014), (85, 0.041), (98, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98288846 <a title="65-lda-1" href="./jmlr-2007-A_Complete_Characterization_of_a_Family_of_Solutions_to_a_Generalized_Fisher_Criterion.html">2 jmlr-2007-A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion</a></p>
<p>Author: Marco Loog</p><p>Abstract: Recently, Ye (2005) suggested yet another optimization criterion for discriminant analysis and proposed a characterization of the family of solutions to this objective. The characterization, however, merely describes a part of the full solution set, that is, it is not complete and therefore not at all a characterization. This correspondence ﬁrst gives the correct characterization and afterwards compares it to Ye’s. Keywords: linear discriminant analysis, Fisher criterion, small sample, characterization 1. Classical and New Criteria Given N feature vectors of dimensionality n, a linear reduction of dimensionality, based on classical Fisher LDA, determines an n × d transformation matrix L that, for a given d < K, K the number of classes, maximizes the so-called Fisher criterion: F(A) = tr((A t SW A)−1 (At SB A)) or, equivalently, F0 (A) = tr((At ST A)−1 (At SB A)). Here, SB := ∑K pi (mi − m)(mi − m)t , SW := ∑K pi Si , and i=1 i=1 ST = SB + SW . The matrices SB , SW , and ST are the so-called between-class, pooled within-class, and total covariance matrices. In addition, mi is the mean vector of class i, pi is the prior of class i, and the overall mean m equals ∑k pi mi . Finally, Si is the covariance matrix of class i. i=1 A solution to these optimization problems can be obtained by means of a generalized eigenvalue decomposition, which Fukunaga (1990) relates to a simultaneous diagonalization of the two matrices involved (see also Campbell and Atchley, 1981). More common is it to apply a standard −1 eigenvalue decomposition to S−1 SB (or SW SB ), resulting in an equivalent set of eigenvectors. The d T columns of the optimal solution L are simply taken to equal the d eigenvectors corresponding to the d largest eigenvalues. It is known that this solution is not unique and the full class can be obtained by multiplying L to the right with nonsingular d × d matrices (see Fukunaga, 1990). Clearly, if the total covariance ST is singular, neither the generalized nor the standard eigenvalue decomposition can be readily employed. Directly or indirectly, the problem is that the matrix inverse S−1 does not exist, which is the typical situation when dealing with small samples. In an attempt to T overcome this problem, Ye (2005) introduced a different criterion that is deﬁned as F1 (A) = tr((At ST A)+ (At SB A)) , ∗. Also at Nordic Bioscience Imaging, Hovegade 207, DK-2730 Herlev, Denmark. c 2007 Marco Loog. (1) L OOG where + denotes taking the Moore-Penrose generalized inverse of a matrix. Like for F0 , an optimal transform L is one that maximizes the objective F1 . Again, this solution is not unique. 2. Correct Characterization For the full characterization of the set of solutions to Equation (1), initially the problem is looked at from a geometrical point of view (cf., Campbell and Atchley, 1981). It is assumed that the number of samples N is smaller than or equal to the feature dimensionality n. In the undersampled case, it is clear that all data variation occurs in an N − 1-dimensional subspace of the original space. To start with, a PCA of the data is carried out and the ﬁrst N − 1 principal components are rotated to the ﬁrst N − 1 axes of the n-dimensional space by means of a rotation matrix R. This matrix consists of all (normalized) eigenvectors of ST taken as its columns. After this rotation, new total and between-class covariance matrices, ST = Rt ST R and SB = Rt SB R, are obtained. These 0 0 matrices can be partitioned as follows: ST = Σ0T 0 and SB = ΣB 0 , where ΣT and ΣB are N − 1 × 0 N − 1 covariance matrices and ΣT is nonsingular and diagonal by construction. The between-class variation is obviously restricted to the N − 1-dimensional subspace in which the total data variation takes place, therefore a same partitioning of SB is possible. However, the covariance submatrix ΣB is not necessarily diagonal, neither does it have to be nonsingular. Basically, the PCA-based rotation R converts the initial problem into a more convenient one, splitting up the original space in an N − 1-dimensional one in which “everything interesting” takes place and a remaining n − N + 1dimensional subspace in which “nothing happens at all”. Now consider F1 in this transformed space and take a general n × d transformation matrix A, which is partitioned in a way similar to the covariance matrices, that is, X . Y A= (2) Here, X is an N − 1 × d-matrix and Y is of size n − N + 1 × d. Taking this deﬁnition, the following holds (cf., Ye, 2005): t + t F1 (A) = tr((A ST A) (A SB A)) = tr =tr X t ΣT X 0 0 0 + X Y X t ΣB X 0 0 0 t ΣT 0 = tr 0 0 X Y + (Xt ΣT X)−1 0 0 0 X Y t ΣB 0 0 0 X Y X t ΣB X 0 0 0 = tr((Xt ΣT X)−1 (Xt ΣB X)) = F0 (X) . From this it is immediate that a matrix A maximizes F1 if and only if the submatrix X maximizes the original Fisher criterion in the lower-dimensional subspace. Moreover, if L is such a matrix maximizing F1 in the PCA-transformed space, it is easy to check that R−1 L = Rt L provides a solution to the original, general problem that has not been preprocessed by means of a PCA (see also Fukunaga, 1990). A characterization of the complete family of solutions can now be given. Let Λ ∈ RN−1×d be an optimal solution of F0 (X) = tr((Xt ΣT X)−1 (Xt ΣB X)). As already noted in Section 1, the full set of solutions is given by F = {ΛZ ∈ RN−1×d | Z ∈ GLd (R)}, where GLd (R) denotes the general linear group of d × d invertible matrices. The previous paragraph essentially demonstrates that if X ∈ F , A in Equation (2) maximizes F1 . The matrix Y can be chosen ad 2122 C OMPLETE C HARACTERIZATION OF A FAMILY OF S OLUTIONS libitum. Now, the latter provides the solution in the PCA-transformed space and to solve the general problem we need to take the rotation back to the original space into account. All in all, this leads to the following complete family of solutions L , maximizing F1 in the original space: L = Rt ΛZ ∈ Rn×d Z ∈ GLd (R), Y ∈ Rn−N+1×d Y , (3) where Λ = argmaxX tr((Xt ΣT X)−1 (Xt ΣB X)) and Rt takes care of the rotation back. 3. Original Characterization Though not noted by Ye (2005), his attempt to characterize the full set of solutions of Equation (1) is based on a simultaneous diagonalization of the three covariance matrices S B , SW , and ST that is similar to the ideas described by Campbell and Atchley (1981) and Fukunaga (1990). Moreover, Golub and Van Loan (Theorem 8.7.1. 1996) can be readily applied to demonstrate that such simultaneous diagonalization is possible in the small sample setting. After the diagonalization step, partitioned between-class, pooled within-class, and total covariance matrices are considered. This partitioning is similar to the one employed in the previous section, which does not enforce matrices to be diagonal however. In the subsequent optimization step, the classical Fisher criterion is maximized basically in the appropriate subspace, comparable to the approach described above, but in a mildly more involved and concealed way. For this, matrices of the form Rt X are considered, consider Equations (2) and Y (3). However, Y is simply the null matrix and the family of solutions L provided is limited to L = Rt ΛZ ∈ Rn×d Z ∈ GLd (R) . 0 Obviously, this is far from a complete characterization, especially when N − 1 n which is, for instance, typically the case for the data sets considered by Ye (2005). Generally, the utility of a dimensionality reduction criterion, without additional constrains, depends on the efﬁciency over the full set of solutions. As Ye (2005) only considers two very speciﬁc instances from the large class of possibilities, it is unclear to what extent the new criterion really provides an efﬁcient way of performing a reduction of dimensionality. References N. A. Campbell and W. R. Atchley. The geometry of canonical variate analysis. Systematic Zoology, 30(3):268–280, 1981. K. Fukunaga. Introduction to Statistical Pattern Recognition. Academic Press, New York, 1990. G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, third edition, 1996. J. Ye. Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems. Journal of Machine Learning Research, 6:483–502, 2005. 2123</p><p>same-paper 2 0.8240819 <a title="65-lda-2" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>Author: François Laviolette, Mario Marchand</p><p>Abstract: We propose a PAC-Bayes theorem for the sample-compression setting where each classiﬁer is described by a compression subset of the training data and a message string of additional information. This setting, which is the appropriate one to describe many learning algorithms, strictly generalizes the usual data-independent setting where classiﬁers are represented only by data-independent message strings (or parameters taken from a continuous set). The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classiﬁer vanishes. For posteriors having all their weights on a single sample-compressed classiﬁer, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. (2005). Finally, we extend our results to the case where each sample-compressed classiﬁer of a data-dependent ensemble may abstain of predicting a class label. Keywords: PAC-Bayes, risk bounds, sample-compression, set covering machines, decision list machines</p><p>3 0.38165316 <a title="65-lda-3" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>Author: Zakria Hussain, François Laviolette, Mario Marchand, John Shawe-Taylor, Spencer Charles Brubaker, Matthew D. Mullin</p><p>Abstract: Marchand and Shawe-Taylor (2002) have proposed a loss bound for the set covering machine that has the property to depend on the observed fraction of positive examples and on what the classiﬁer achieves on the positive training examples. We show that this loss bound is incorrect. We then propose a loss bound, valid for any sample-compression learning algorithm (including the set covering machine), that depends on the observed fraction of positive examples and on what the classiﬁer achieves on them. We also compare numerically the loss bound proposed in this paper with the incorrect bound, the original SCM bound and a recently proposed loss bound of Marchand and Sokolova (2005) (which does not depend on the observed fraction of positive examples) and show that the latter loss bounds can be substantially larger than the new bound in the presence of imbalanced misclassiﬁcations. Keywords: set covering machines, sample-compression, loss bounds</p><p>4 0.36406958 <a title="65-lda-4" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>Author: Onur C. Hamsici, Aleix M. Martinez</p><p>Abstract: Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere S p−1 . Such representations should then be modeled using spherical distributions. However, the difﬁculty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead—as if the data were represented in R p rather than S p−1 . This opens the question to whether the classiﬁcation results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases (which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to deﬁne optimal classiﬁers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classiﬁers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classiﬁcation of images of objects, gene expression sequences, and text data. Keywords: directional data, spherical distributions, normal distributions, norm normalization, linear and non-linear classiﬁers, computer vision</p><p>5 0.32541418 <a title="65-lda-5" href="./jmlr-2007-Bayesian_Quadratic_Discriminant_Analysis.html">13 jmlr-2007-Bayesian Quadratic Discriminant Analysis</a></p>
<p>Author: Santosh Srivastava, Maya R. Gupta, Béla A. Frigyik</p><p>Abstract: Quadratic discriminant analysis is a common tool for classiﬁcation, but estimation of the Gaussian parameters can be ill-posed. This paper contains theoretical and algorithmic contributions to Bayesian estimation for quadratic discriminant analysis. A distribution-based Bayesian classiﬁer is derived using information geometry. Using a calculus of variations approach to deﬁne a functional Bregman divergence for distributions, it is shown that the Bayesian distribution-based classiﬁer that minimizes the expected Bregman divergence of each class conditional distribution also minimizes the expected misclassiﬁcation cost. A series approximation is used to relate regularized discriminant analysis to Bayesian discriminant analysis. A new Bayesian quadratic discriminant analysis classiﬁer is proposed where the prior is deﬁned using a coarse estimate of the covariance based on the training data; this classiﬁer is termed BDA7. Results on benchmark data sets and simulations show that BDA7 performance is competitive with, and in some cases signiﬁcantly better than, regularized quadratic discriminant analysis and the cross-validated Bayesian quadratic discriminant analysis classiﬁer Quadratic Bayes. Keywords: quadratic discriminant analysis, regularized quadratic discriminant analysis, Bregman divergence, data-dependent prior, eigenvalue decomposition, Wishart, functional analysis</p><p>6 0.31295365 <a title="65-lda-6" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>7 0.31010881 <a title="65-lda-7" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>8 0.30000919 <a title="65-lda-8" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>9 0.29564711 <a title="65-lda-9" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>10 0.26281923 <a title="65-lda-10" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>11 0.25962743 <a title="65-lda-11" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>12 0.25933653 <a title="65-lda-12" href="./jmlr-2007-Local_Discriminant_Wavelet_Packet_Coordinates_for_Face_Recognition.html">50 jmlr-2007-Local Discriminant Wavelet Packet Coordinates for Face Recognition</a></p>
<p>13 0.25403535 <a title="65-lda-13" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>14 0.24809854 <a title="65-lda-14" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>15 0.24747372 <a title="65-lda-15" href="./jmlr-2007-Undercomplete_Blind_Subspace_Deconvolution.html">87 jmlr-2007-Undercomplete Blind Subspace Deconvolution</a></p>
<p>16 0.24738543 <a title="65-lda-16" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>17 0.24191356 <a title="65-lda-17" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>18 0.24042389 <a title="65-lda-18" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>19 0.24018955 <a title="65-lda-19" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>20 0.23763508 <a title="65-lda-20" href="./jmlr-2007-Truncating_the_Loop_Series_Expansion_for_Belief_Propagation.html">86 jmlr-2007-Truncating the Loop Series Expansion for Belief Propagation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
