<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>11 jmlr-2007-Anytime Learning of Decision Trees</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-11" href="#">jmlr2007-11</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>11 jmlr-2007-Anytime Learning of Decision Trees</h1>
<br/><p>Source: <a title="jmlr-2007-11-pdf" href="http://jmlr.org/papers/volume8/esmeir07a/esmeir07a.pdf">pdf</a></p><p>Author: Saher Esmeir, Shaul Markovitch</p><p>Abstract: The majority of existing algorithms for learning decision trees are greedy—a tree is induced topdown, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Even the few non-greedy learners cannot learn good trees when the concept is difﬁcult. Furthermore, they require a ﬁxed amount of time and are not able to generate a better tree if additional time is available. We introduce a framework for anytime induction of decision trees that overcomes these problems by trading computation speed for better tree quality. Our proposed family of algorithms employs a novel strategy for evaluating candidate splits. A biased sampling of the space of consistent trees rooted at an attribute is used to estimate the size of the minimal tree under that attribute, and an attribute with the smallest expected tree is selected. We present two types of anytime induction algorithms: a contract algorithm that determines the sample size on the basis of a pre-given allocation of time, and an interruptible algorithm that starts with a greedy tree and continuously improves subtrees by additional sampling. Experimental results indicate that, for several hard concepts, our proposed approach exhibits good anytime behavior and yields signiﬁcantly better decision trees when more time is available. Keywords: anytime algorithms, decision tree induction, lookahead, hard concepts, resource-bounded reasoning</p><p>Reference: <a title="jmlr-2007-11-reference" href="../jmlr2007_reference/jmlr-2007-Anytime_Learning_of_Decision_Trees_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 IL  Department of Computer Science Technion—Israel Institute of Technology Haifa 32000, Israel  Editor: Claude Sammut  Abstract The majority of existing algorithms for learning decision trees are greedy—a tree is induced topdown, making locally optimal decisions at each node. [sent-7, score-0.556]
</p><p>2 We introduce a framework for anytime induction of decision trees that overcomes these problems by trading computation speed for better tree quality. [sent-11, score-1.056]
</p><p>3 A biased sampling of the space of consistent trees rooted at an attribute is used to estimate the size of the minimal tree under that attribute, and an attribute with the smallest expected tree is selected. [sent-13, score-1.094]
</p><p>4 We present two types of anytime induction algorithms: a contract algorithm that determines the sample size on the basis of a pre-given allocation of time, and an interruptible algorithm that starts with a greedy tree and continuously improves subtrees by additional sampling. [sent-14, score-1.211]
</p><p>5 Experimental results indicate that, for several hard concepts, our proposed approach exhibits good anytime behavior and yields signiﬁcantly better decision trees when more time is available. [sent-15, score-0.762]
</p><p>6 Keywords: anytime algorithms, decision tree induction, lookahead, hard concepts, resource-bounded reasoning  1. [sent-16, score-0.789]
</p><p>7 The majority of existing methods for decision tree induction build a tree top-down and use local measures in an attempt to produce small trees, which, by Occam’s Razor (Blumer et al. [sent-39, score-0.682]
</p><p>8 11) recognized the need for this type of anytime algorithm for decision tree learning: “What is wanted is a resource constrained algorithm that will do the best it can within a speciﬁed computational budget and can pick up threads and continue if this budget is increased. [sent-65, score-0.872]
</p><p>9 ” There are two main classes of anytime algorithms, namely contract and interruptible (Russell and Zilberstein, 1996). [sent-67, score-0.707]
</p><p>10 Therefore, in our work, we are interested both in contract and interruptible decision tree learners. [sent-72, score-0.62]
</p><p>11 When applied to decision tree induction, lookahead attempts to predict the proﬁtability of a split at a node by estimating its effect on deeper descendants of the node. [sent-76, score-0.654]
</p><p>12 The reported results vary from lookahead produces better trees (Norton, 1989; Ragavan and Rendell, 1993; Dong and Kothari, 2001) to lookahead does not help and can hurt (Murthy and Salzberg, 1995). [sent-82, score-0.559]
</p><p>13 A consistent decision tree is a tree that correctly classiﬁes all training examples. [sent-88, score-0.636]
</p><p>14 These works, however, are not capable to handle high-dimensional difﬁcult concepts and are not designed to offer anytime behavior. [sent-96, score-0.459]
</p><p>15 We ﬁrst discuss tree size as a desired property of the tree to be learned and then we describe an anytime algorithm that uses sampling methods to obtain smaller trees. [sent-106, score-0.96]
</p><p>16 Murphy and Pazzani (1994) reported a set of experiments in which all the possible consistent decision trees were produced and showed that, for several tasks, the smallest consistent decision tree had higher predictive error than slightly larger trees. [sent-132, score-0.665]
</p><p>17 The small number of training examples relative to the size of the tree that perfectly describes the concept might explain why, in these cases, the smallest tree did not generalize best. [sent-135, score-0.532]
</p><p>18 RTG builds a tree top-down and chooses the splitting attribute at random. [sent-139, score-0.45]
</p><p>19 For each attribute a, let Tmin (a, A, E) be the smallest tree rooted at a that is consistent with E and uses attributes from A − {a} for the internal splits. [sent-147, score-0.583]
</p><p>20 3 ID3-k can be viewed as a contract anytime algorithm parameterized by k. [sent-169, score-0.528]
</p><p>21 However, despite its ability to exploit additional resources when available, the anytime behavior of ID3-k is problematic. [sent-170, score-0.578]
</p><p>22 If two attributes yield the same decrease in entropy, we prefer the one whose associated lookahead tree is shallower. [sent-180, score-0.556]
</p><p>23 3 Estimating Tree Size by Sampling Motivated by the advantages of smaller decision trees, we introduce a novel algorithm that, given an attribute a, evaluates it by estimating the size of the minimal tree under it. [sent-184, score-0.481]
</p><p>24 Summing up the minimal tree size for each subset gives an estimation of the minimal total tree size under a. [sent-238, score-0.532]
</p><p>25 The minimal size for trees rooted at a4 is 6 and for trees rooted at a3 is 7. [sent-265, score-0.466]
</p><p>26 For more complicated problems such as 3-XOR, the space of SID3 trees under the relevant attributes includes trees other than the smallest. [sent-268, score-0.488]
</p><p>27 4 Evaluating Continuous Attributes by Sampling the Candidate Cuts When attributes have a continuous domain, the decision tree learner needs to discretize their values in order to deﬁne splitting tests. [sent-271, score-0.491]
</p><p>28 In our proposed anytime approach, this problem is tackled by allotting additional time for reaching trees unexplored by greedy algorithms with pruning. [sent-335, score-0.699]
</p><p>29 For this purpose, a post-pruning phase will be applied to the trees that were generated by SID3 to form the lookahead sample, and the size of the pruned trees will be considered. [sent-341, score-0.566]
</p><p>30 5, namely error-based pruning (EPB), and examine post-pruning of the ﬁnal trees as well as pruning of the lookahead trees. [sent-346, score-0.499]
</p><p>31 The space of decision trees over A can be partitioned into two: the space of trees consistent with E and the space of trees inconsistent with E. [sent-349, score-0.677]
</p><p>32 To handle such cases, we need to devise an interruptible anytime algorithm. [sent-372, score-0.607]
</p><p>33 1 Interruptible Induction by Sequencing Contract Algorithms By deﬁnition, every interruptible algorithm can serve as a contract algorithm because one can stop the interruptible algorithm when all the resources have been consumed. [sent-377, score-0.587]
</p><p>34 Our interruptible anytime framework, called IIDT, overcomes these problems by iteratively improving the tree rather than trying to rebuild it. [sent-392, score-0.894]
</p><p>35 It can serve as an interruptible anytime algorithm because, when interrupted, it can immediately return the currently best solution available. [sent-399, score-0.633]
</p><p>36 The principal difference between the algorithms is that LSID3 uses the available resources to induce a decision tree top-down, where each decision made at a node is ﬁnal and does not change. [sent-401, score-0.62]
</p><p>37 Then it iteratively attempts to improve the current tree by choosing a node and rebuilding its subtree with more resources than those used previously. [sent-405, score-0.595]
</p><p>38 In the ﬁrst iteration, the subtree rooted at the bolded node is selected for improvement and replaced by a smaller tree (surrounded by a dashed line). [sent-411, score-0.503]
</p><p>39 Next, the root is selected for improvement and the whole tree is replaced by a tree that perfectly describes the concept. [sent-412, score-0.583]
</p><p>40 In the ﬁrst iteration the subtree rooted at the bolded node is selected for improvement and replaced by a smaller tree (surrounded by a dashed line). [sent-416, score-0.503]
</p><p>41 Next, the root is selected for improvement and the whole tree is replaced by a tree that perfectly describes the concept. [sent-417, score-0.583]
</p><p>42 Given a tree node y, we can view the task of rebuilding the subtree below y as an independent task. [sent-432, score-0.466]
</p><p>43 The whole framework of decision tree induction rests on the assumption that smaller consistent trees are better than larger ones. [sent-443, score-0.636]
</p><p>44 When no such large subtrees exist, our algorithm may repeatedly attempt to improve smaller trees rooted at deep nodes because these trees have low associated costs. [sent-457, score-0.483]
</p><p>45 To control the tradeoff between efﬁcient resource use and anytime performance ﬂexibility, we add a granularity parameter 0 ≤ g ≤ 1. [sent-461, score-0.477]
</p><p>46 Thus, to avoid obtaining an induced tree of lower quality, we replace an existing subtree with a newly induced alternative only if the alternative is expected to improve the quality of the complete decision tree. [sent-475, score-0.502]
</p><p>47 Empirical Evaluation A variety of experiments were conducted to test the performance and behavior of the proposed anytime algorithms. [sent-487, score-0.449]
</p><p>48 1 Experimental Methodology We start our experimental evaluation by comparing our contract algorithm, given a ﬁxed resource allocation, with the basic decision tree induction algorithms. [sent-491, score-0.565]
</p><p>49 We then compare the anytime behavior of our contract algorithm to that of ﬁxed lookahead. [sent-492, score-0.549]
</p><p>50 Next we examine the anytime behavior of our interruptible algorithm. [sent-493, score-0.628]
</p><p>51 When comparing the algorithms that produce consistent trees, namely ID3, ID3-k and LSID3, the average tree size is the smallest for most data sets when the trees are induced with LSID3. [sent-526, score-0.51]
</p><p>52 However, in order to serve as good anytime algorithms, the quality of their output should improve with the increase in their allocated resources. [sent-670, score-0.498]
</p><p>53 For a typical anytime algorithm, this improvement is greater at the beginning and diminishes over time. [sent-671, score-0.455]
</p><p>54 To test the anytime behavior of LSID3 and ID3-k, we invoked them with successive values of r and k respectively. [sent-672, score-0.449]
</p><p>55 In the ﬁrst set of experiments we focused on domains with nominal attributes, while in the second set we tested the anytime behavior for domains with continuous attributes. [sent-673, score-0.477]
</p><p>56 The graphs indicate that the anytime behavior of LSID3 is better than that of ID3-k. [sent-703, score-0.449]
</p><p>57 16 We can easily increase the granularity of the anytime graph by smaller gaps between the p values. [sent-758, score-0.46]
</p><p>58 4 Anytime Behavior of IIDT IIDT was presented as an interruptible decision tree learner that does not require advanced knowledge of its resource allocation: it can be stopped at any moment and return a valid decision tree. [sent-778, score-0.67]
</p><p>59 Figures 23, 24, and 25 show the anytime performance of IIDT in terms of tree size and accuracy for the Glass, XOR-10, and Tic-tac-toe data sets. [sent-781, score-0.739]
</p><p>60 Unlike the graphs given in the previous section, these are interruptible anytime graphs, that is, for each point, the y coordinate reﬂects the performance if the algorithm was interrupted at the associated x coordinate. [sent-783, score-0.639]
</p><p>61 In all cases, the two anytime versions indeed exploit the additional resources and produce both smaller and more accurate trees. [sent-785, score-0.557]
</p><p>62 Although these learners were not presented and studied as anytime algorithms, some of them can be viewed as such. [sent-824, score-0.456]
</p><p>63 In what follows we compare our proposed anytime framework to three such algorithms: bagging, skewing and GATree. [sent-825, score-0.72]
</p><p>64 1 OVERVIEW  OF THE  C OMPARED M ETHODS  Page and Ray (2003) introduced skewing as an alternative to lookahead for addressing problematic concepts such as parity functions. [sent-829, score-0.507]
</p><p>65 In order to convert skewing into an interruptible algorithm, we apply the general conversion method described in Section 3. [sent-834, score-0.471]
</p><p>66 Two improvements to the original skewing algorithm were presented, namely sequential skewing (Page and Ray, 2004), which skews one variable at a time instead of all of them simultaneously, and generalized skewing (Ray and Page, 2005), which can handle nominal and continuous attributes. [sent-836, score-0.931]
</p><p>67 To convert it into an anytime algorithm, we added a parameter k that controls the number of skewing iterations. [sent-841, score-0.72]
</p><p>68 For the Tic-tac-toe data set, where the attributes are ternary, we used only the generalized skewing algorithm, parameterized by the number of random orderings by which the nominal attributes are reweighed. [sent-843, score-0.532]
</p><p>69 GATree can be viewed as an interruptible anytime algorithm that uses additional time to produce more and more generations. [sent-853, score-0.634]
</p><p>70 Bagging is an ensemble-based method, and as such, it is naturally an interruptible anytime learner. [sent-856, score-0.607]
</p><p>71 Generating a set of trees rather than a single good tree eliminates one of the greatest advantages of decision trees—their comprehensibility. [sent-878, score-0.532]
</p><p>72 17 The skewing and sequential skewing versions were run with linearly increasing parameters. [sent-882, score-0.584]
</p><p>73 Still, IIDT performed signiﬁcantly better than bagging-LSID3, indicating that for difﬁcult concepts, it is better to invest more resources for improving a single tree than for adding more trees of lower quality to the committee. [sent-904, score-0.586]
</p><p>74 The inferior results of the skewing algorithms are more difﬁcult to interpret, since skewing was shown to handle difﬁcult concepts well. [sent-905, score-0.615]
</p><p>75 We believe that the general question of tradeoff between the resources allocated for each tree and the number of trees forming the ensemble should be addressed by further research with extensive experiments on various data sets. [sent-928, score-0.656]
</p><p>76 The performance of generalized skewing and IIDT was similar in this case, with a slight advantage for skewing in terms of accuracy and an advantage for IIDT in terms of tree size. [sent-929, score-0.895]
</p><p>77 Related Work While to the best of our knowledge no other work has tried to design an anytime algorithm for decision tree induction in particular, there are several related works that warrant discussion here. [sent-934, score-0.844]
</p><p>78 1 Single Tree Inducers The goal of our research was to develop anytime algorithms for inducing a single decision tree. [sent-938, score-0.503]
</p><p>79 Several other algorithms for single decision-tree induction can either be considered anytime algorithms or can be converted into them with relative ease. [sent-939, score-0.503]
</p><p>80 Therefore, we focus ﬁrst on the differences between skewing and our anytime framework, and then we consider other decision tree learners. [sent-941, score-1.061]
</p><p>81 1 S KEWING The skewing approach was presented as an efﬁcient alternative to lookahead (Page and Ray, 2003). [sent-944, score-0.476]
</p><p>82 Skewing relies on the hypothesis that, when learning a hard concept, it may be easier for a greedy decision tree inducer to pick a relevant attribute if the distribution over the data is signiﬁcantly different from the uniform distribution. [sent-948, score-0.554]
</p><p>83 Hence, skewing iterations can be seen as sampling the space of lookahead sub-paths and considering a few of them at a time. [sent-961, score-0.476]
</p><p>84 Unlike our approach, GATree is not designed to generate consistent decision trees and searches the space of all possible trees over a given set of attributes. [sent-978, score-0.486]
</p><p>85 5) shows that, especially for hard concepts, it is much better to invest the resources in careful tuning of a single tree than to perform genetic search over the large population of decision trees. [sent-982, score-0.49]
</p><p>86 (1997) presented DMTI, an induction algorithm that chooses an attribute by building a single decision tree under each candidate attribute and evaluates it using various measures. [sent-984, score-0.728]
</p><p>87 DMTI is similar to LSID3(r = 1) but, unlike LSID3, it can only use a ﬁxed amount of additional resources and hence cannot serve as an anytime algorithm. [sent-986, score-0.557]
</p><p>88 Furthermore, DMTI uses a single greedy lookahead tree for attribute evaluation, while we use a biased sample of the possible lookahead trees. [sent-988, score-0.827]
</p><p>89 Kim and Loh (2001) introduced CRUISE, a bias-free decision tree learner that attempts to produce more compact trees by (1) using multiway splits—one subnode for each class, and (2) examining pair-wise interactions among the variables. [sent-991, score-0.59]
</p><p>90 The algorithm retains the structure of the tree but attempts to simultaneously improve all the multivariate decisions of the tree using iterative linear programming. [sent-996, score-0.532]
</p><p>91 2 Induction of Other Models Ensemble-based methods, such as bagging and boosting (Schapire, 1999), can also be viewed as anytime algorithms. [sent-1001, score-0.487]
</p><p>92 5 in that the latter produces an ensemble of trees that is obviously not as comprehensible as a single decision trees is. [sent-1016, score-0.486]
</p><p>93 In order to allow induction of better decision trees for hard-to-learn concepts, we presented a framework for anytime induction of decision trees that makes use of additional resources for producing better hypotheses. [sent-1039, score-1.239]
</p><p>94 We showed that for several real-world and synthetic data sets, our proposed anytime algorithms can exploit larger time budgets. [sent-1041, score-0.455]
</p><p>95 The major contributions of this paper are: • A better understanding of the space of consistent decision trees: Using sampling techniques, we conducted experiments that support the application of Occam’s Razor for decision trees and showed that smaller trees are clearly preferable. [sent-1043, score-0.561]
</p><p>96 • LSID3: We presented and empirically evaluated LSID3, a contract anytime algorithm for learning decision trees. [sent-1044, score-0.603]
</p><p>97 On the data sets studied in this paper, LSID3 was shown to exhibit good anytime behavior and to produce better decision trees. [sent-1045, score-0.524]
</p><p>98 • IIDT: Motivated by the need for interruptible learners, we introduced IIDT, an interruptible anytime algorithm for inducing decision trees. [sent-1046, score-0.861]
</p><p>99 To the best of our knowledge, this is the ﬁrst work that proposes anytime algorithms for learning decision trees. [sent-1051, score-0.503]
</p><p>100 Skewing: An efﬁcient alternative to lookahead for decision tree induction. [sent-2139, score-0.525]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('iidt', 0.454), ('anytime', 0.428), ('skewing', 0.292), ('tree', 0.266), ('trees', 0.191), ('lookahead', 0.184), ('interruptible', 0.179), ('attribute', 0.14), ('nytime', 0.134), ('smeir', 0.134), ('resources', 0.129), ('arkovitch', 0.114), ('ecision', 0.114), ('rees', 0.111), ('gatree', 0.109), ('attributes', 0.106), ('rtg', 0.102), ('contract', 0.1), ('subtree', 0.093), ('splits', 0.086), ('decision', 0.075), ('node', 0.075), ('induction', 0.075), ('sec', 0.073), ('allocation', 0.073), ('allocated', 0.07), ('xor', 0.068), ('occam', 0.066), ('tdidt', 0.065), ('pruning', 0.062), ('bagging', 0.059), ('multiway', 0.058), ('onks', 0.058), ('greedy', 0.053), ('resource', 0.049), ('runtime', 0.047), ('hoose', 0.045), ('umeric', 0.045), ('accuracy', 0.045), ('splitting', 0.044), ('rooted', 0.042), ('earning', 0.042), ('foreach', 0.039), ('razor', 0.039), ('dmti', 0.038), ('uci', 0.038), ('subtrees', 0.037), ('gain', 0.037), ('russell', 0.035), ('split', 0.033), ('gaps', 0.032), ('candidate', 0.032), ('anode', 0.032), ('interrupted', 0.032), ('papagelis', 0.032), ('rebuilding', 0.032), ('concepts', 0.031), ('repository', 0.029), ('comprehensible', 0.029), ('sequencing', 0.029), ('consistent', 0.029), ('seconds', 0.029), ('quinlan', 0.028), ('learners', 0.028), ('nominal', 0.028), ('autos', 0.027), ('budget', 0.027), ('markovitch', 0.027), ('zilberstein', 0.027), ('ray', 0.027), ('improvement', 0.027), ('time', 0.027), ('return', 0.026), ('automobile', 0.026), ('esmeir', 0.026), ('gto', 0.026), ('kalles', 0.026), ('multiplexer', 0.026), ('tmin', 0.026), ('zoo', 0.024), ('root', 0.024), ('induced', 0.024), ('ig', 0.023), ('repeatedly', 0.022), ('utgoff', 0.022), ('irrelevant', 0.022), ('afford', 0.022), ('enode', 0.022), ('depth', 0.021), ('behavior', 0.021), ('loh', 0.021), ('glass', 0.021), ('deeper', 0.021), ('committee', 0.021), ('overcomes', 0.021), ('expected', 0.02), ('hard', 0.02), ('obtainable', 0.019), ('ake', 0.019), ('committees', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="11-tfidf-1" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>Author: Saher Esmeir, Shaul Markovitch</p><p>Abstract: The majority of existing algorithms for learning decision trees are greedy—a tree is induced topdown, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Even the few non-greedy learners cannot learn good trees when the concept is difﬁcult. Furthermore, they require a ﬁxed amount of time and are not able to generate a better tree if additional time is available. We introduce a framework for anytime induction of decision trees that overcomes these problems by trading computation speed for better tree quality. Our proposed family of algorithms employs a novel strategy for evaluating candidate splits. A biased sampling of the space of consistent trees rooted at an attribute is used to estimate the size of the minimal tree under that attribute, and an attribute with the smallest expected tree is selected. We present two types of anytime induction algorithms: a contract algorithm that determines the sample size on the basis of a pre-given allocation of time, and an interruptible algorithm that starts with a greedy tree and continuously improves subtrees by additional sampling. Experimental results indicate that, for several hard concepts, our proposed approach exhibits good anytime behavior and yields signiﬁcantly better decision trees when more time is available. Keywords: anytime algorithms, decision tree induction, lookahead, hard concepts, resource-bounded reasoning</p><p>2 0.14632705 <a title="11-tfidf-2" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>Author: Robert Tibshirani, Trevor Hastie</p><p>Abstract: We propose a method for the classiﬁcation of more than two classes, from high-dimensional features. Our approach is to build a binary decision tree in a top-down manner, using the optimal margin classiﬁer at each split. We implement an exact greedy algorithm for this task, and compare its performance to less greedy procedures based on clustering of the matrix of pairwise margins. We compare the performance of the “margin tree” to the closely related “all-pairs” (one versus one) support vector machine, and nearest centroids on a number of cancer microarray data sets. We also develop a simple method for feature selection. We ﬁnd that the margin tree has accuracy that is competitive with other methods and offers additional interpretability in its putative grouping of the classes. Keywords: maximum margin classiﬁer, support vector machine, decision tree, CART</p><p>3 0.12213793 <a title="11-tfidf-3" href="./jmlr-2007-Structure_and_Majority_Classes_in_Decision_Tree_Learning.html">79 jmlr-2007-Structure and Majority Classes in Decision Tree Learning</a></p>
<p>Author: Ray J. Hickey</p><p>Abstract: To provide good classification accuracy on unseen examples, a decision tree, learned by an algorithm such as ID3, must have sufficient structure and also identify the correct majority class in each of its leaves. If there are inadequacies in respect of either of these, the tree will have a percentage classification rate below that of the maximum possible for the domain, namely (100 Bayes error rate). An error decomposition is introduced which enables the relative contributions of deficiencies in structure and in incorrect determination of majority class to be isolated and quantified. A sub-decomposition of majority class error permits separation of the sampling error at the leaves from the possible bias introduced by the attribute selection method of the induction algorithm. It is shown that sampling error can extend to 25% when there are more than two classes. Decompositions are obtained from experiments on several data sets. For ID3, the effect of selection bias is shown to vary from being statistically non-significant to being quite substantial, with the latter appearing to be associated with a simple underlying model. Keywords: decision tree learning, error decomposition, majority classes, sampling error, attribute selection bias 1</p><p>4 0.088700451 <a title="11-tfidf-4" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>Author: Marlon Núñez, Raúl Fidalgo, Rafael Morales</p><p>Abstract: In the process of concept learning, target concepts may have portions with short-term changes, other portions may support long-term changes, and yet others may not change at all. For this reason several local windows need to be handled. We suggest facing this problem, which naturally exists in the ﬁeld of concept learning, by allocating windows which can adapt their size to portions of the target concept. We propose an incremental decision tree that is updated with incoming examples. Each leaf of the decision tree holds a time window and a local performance measure as the main parameter to be controlled. When the performance of a leaf decreases, the size of its local window is reduced. This learning algorithm, called OnlineTree2, automatically adjusts its internal parameters in order to face the current dynamics of the data stream. Results show that it is comparable to other batch algorithms when facing problems with no concept change, and it is better than evaluated methods in its ability to deal with concept drift when dealing with problems in which: concept change occurs at different speeds, noise may be present and, examples may arrive from different areas of the problem domain (virtual drift). Keywords: incremental algorithms, online learning, concept drift, decision trees, robust learners</p><p>5 0.05588989 <a title="11-tfidf-5" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>Author: Evgeniy Gabrilovich, Shaul Markovitch</p><p>Abstract: Most existing methods for text categorization employ induction algorithms that use the words appearing in the training documents as features. While they perform well in many categorization tasks, these methods are inherently limited when faced with more complicated tasks where external knowledge is essential. Recently, there have been efforts to augment these basic features with external knowledge, including semi-supervised learning and transfer learning. In this work, we present a new framework for automatic acquisition of world knowledge and methods for incorporating it into the text categorization process. Our approach enhances machine learning algorithms with features generated from domain-speciﬁc and common-sense knowledge. This knowledge is represented by ontologies that contain hundreds of thousands of concepts, further enriched through controlled Web crawling. Prior to text categorization, a feature generator analyzes the documents and maps them onto appropriate ontology concepts that augment the bag of words used in simple supervised learning. Feature generation is accomplished through contextual analysis of document text, thus implicitly performing word sense disambiguation. Coupled with the ability to generalize concepts using the ontology, this approach addresses two signiﬁcant problems in natural language processing—synonymy and polysemy. Categorizing documents with the aid of knowledge-based features leverages information that cannot be deduced from the training documents alone. We applied our methodology using the Open Directory Project, the largest existing Web directory built by over 70,000 human editors. Experimental results over a range of data sets conﬁrm improved performance compared to the bag of words document representation. Keywords: feature generation, text classiﬁcation, background knowledge</p><p>6 0.047474172 <a title="11-tfidf-6" href="./jmlr-2007-Nonlinear_Boosting_Projections_for_Ensemble_Construction.html">59 jmlr-2007-Nonlinear Boosting Projections for Ensemble Construction</a></p>
<p>7 0.045024756 <a title="11-tfidf-7" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>8 0.044240009 <a title="11-tfidf-8" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>9 0.043586999 <a title="11-tfidf-9" href="./jmlr-2007-Handling_Missing_Values_when_Applying_Classification_Models.html">39 jmlr-2007-Handling Missing Values when Applying Classification Models</a></p>
<p>10 0.042457614 <a title="11-tfidf-10" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>11 0.038558785 <a title="11-tfidf-11" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>12 0.036677092 <a title="11-tfidf-12" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>13 0.036146563 <a title="11-tfidf-13" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>14 0.03207849 <a title="11-tfidf-14" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>15 0.031657264 <a title="11-tfidf-15" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>16 0.030178282 <a title="11-tfidf-16" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>17 0.028311588 <a title="11-tfidf-17" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>18 0.028086558 <a title="11-tfidf-18" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>19 0.026848985 <a title="11-tfidf-19" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>20 0.024128744 <a title="11-tfidf-20" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.165), (1, 0.242), (2, -0.054), (3, 0.11), (4, 0.159), (5, 0.139), (6, 0.147), (7, 0.16), (8, -0.155), (9, 0.191), (10, 0.085), (11, 0.062), (12, -0.001), (13, -0.042), (14, -0.017), (15, -0.002), (16, 0.076), (17, -0.071), (18, 0.066), (19, -0.08), (20, 0.005), (21, -0.155), (22, 0.094), (23, -0.104), (24, 0.037), (25, -0.049), (26, 0.116), (27, -0.114), (28, -0.156), (29, -0.148), (30, 0.102), (31, -0.025), (32, -0.031), (33, 0.064), (34, 0.013), (35, 0.047), (36, 0.069), (37, -0.055), (38, -0.078), (39, 0.029), (40, -0.033), (41, -0.024), (42, -0.178), (43, 0.012), (44, 0.026), (45, -0.001), (46, -0.012), (47, 0.003), (48, -0.065), (49, 0.082)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95993543 <a title="11-lsi-1" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>Author: Saher Esmeir, Shaul Markovitch</p><p>Abstract: The majority of existing algorithms for learning decision trees are greedy—a tree is induced topdown, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Even the few non-greedy learners cannot learn good trees when the concept is difﬁcult. Furthermore, they require a ﬁxed amount of time and are not able to generate a better tree if additional time is available. We introduce a framework for anytime induction of decision trees that overcomes these problems by trading computation speed for better tree quality. Our proposed family of algorithms employs a novel strategy for evaluating candidate splits. A biased sampling of the space of consistent trees rooted at an attribute is used to estimate the size of the minimal tree under that attribute, and an attribute with the smallest expected tree is selected. We present two types of anytime induction algorithms: a contract algorithm that determines the sample size on the basis of a pre-given allocation of time, and an interruptible algorithm that starts with a greedy tree and continuously improves subtrees by additional sampling. Experimental results indicate that, for several hard concepts, our proposed approach exhibits good anytime behavior and yields signiﬁcantly better decision trees when more time is available. Keywords: anytime algorithms, decision tree induction, lookahead, hard concepts, resource-bounded reasoning</p><p>2 0.87232566 <a title="11-lsi-2" href="./jmlr-2007-Structure_and_Majority_Classes_in_Decision_Tree_Learning.html">79 jmlr-2007-Structure and Majority Classes in Decision Tree Learning</a></p>
<p>Author: Ray J. Hickey</p><p>Abstract: To provide good classification accuracy on unseen examples, a decision tree, learned by an algorithm such as ID3, must have sufficient structure and also identify the correct majority class in each of its leaves. If there are inadequacies in respect of either of these, the tree will have a percentage classification rate below that of the maximum possible for the domain, namely (100 Bayes error rate). An error decomposition is introduced which enables the relative contributions of deficiencies in structure and in incorrect determination of majority class to be isolated and quantified. A sub-decomposition of majority class error permits separation of the sampling error at the leaves from the possible bias introduced by the attribute selection method of the induction algorithm. It is shown that sampling error can extend to 25% when there are more than two classes. Decompositions are obtained from experiments on several data sets. For ID3, the effect of selection bias is shown to vary from being statistically non-significant to being quite substantial, with the latter appearing to be associated with a simple underlying model. Keywords: decision tree learning, error decomposition, majority classes, sampling error, attribute selection bias 1</p><p>3 0.6705361 <a title="11-lsi-3" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>Author: Robert Tibshirani, Trevor Hastie</p><p>Abstract: We propose a method for the classiﬁcation of more than two classes, from high-dimensional features. Our approach is to build a binary decision tree in a top-down manner, using the optimal margin classiﬁer at each split. We implement an exact greedy algorithm for this task, and compare its performance to less greedy procedures based on clustering of the matrix of pairwise margins. We compare the performance of the “margin tree” to the closely related “all-pairs” (one versus one) support vector machine, and nearest centroids on a number of cancer microarray data sets. We also develop a simple method for feature selection. We ﬁnd that the margin tree has accuracy that is competitive with other methods and offers additional interpretability in its putative grouping of the classes. Keywords: maximum margin classiﬁer, support vector machine, decision tree, CART</p><p>4 0.39685044 <a title="11-lsi-4" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>Author: Marlon Núñez, Raúl Fidalgo, Rafael Morales</p><p>Abstract: In the process of concept learning, target concepts may have portions with short-term changes, other portions may support long-term changes, and yet others may not change at all. For this reason several local windows need to be handled. We suggest facing this problem, which naturally exists in the ﬁeld of concept learning, by allocating windows which can adapt their size to portions of the target concept. We propose an incremental decision tree that is updated with incoming examples. Each leaf of the decision tree holds a time window and a local performance measure as the main parameter to be controlled. When the performance of a leaf decreases, the size of its local window is reduced. This learning algorithm, called OnlineTree2, automatically adjusts its internal parameters in order to face the current dynamics of the data stream. Results show that it is comparable to other batch algorithms when facing problems with no concept change, and it is better than evaluated methods in its ability to deal with concept drift when dealing with problems in which: concept change occurs at different speeds, noise may be present and, examples may arrive from different areas of the problem domain (virtual drift). Keywords: incremental algorithms, online learning, concept drift, decision trees, robust learners</p><p>5 0.29978269 <a title="11-lsi-5" href="./jmlr-2007-Handling_Missing_Values_when_Applying_Classification_Models.html">39 jmlr-2007-Handling Missing Values when Applying Classification Models</a></p>
<p>Author: Maytal Saar-Tsechansky, Foster Provost</p><p>Abstract: Much work has studied the effect of different treatments of missing values on model induction, but little work has analyzed treatments for the common case of missing values at prediction time. This paper ﬁrst compares several different methods—predictive value imputation, the distributionbased imputation used by C4.5, and using reduced models—for applying classiﬁcation trees to instances with missing values (and also shows evidence that the results generalize to bagged trees and to logistic regression). The results show that for the two most popular treatments, each is preferable under different conditions. Strikingly the reduced-models approach, seldom mentioned or used, consistently outperforms the other two methods, sometimes by a large margin. The lack of attention to reduced modeling may be due in part to its (perceived) expense in terms of computation or storage. Therefore, we then introduce and evaluate alternative, hybrid approaches that allow users to balance between more accurate but computationally expensive reduced modeling and the other, less accurate but less computationally expensive treatments. The results show that the hybrid methods can scale gracefully to the amount of investment in computation/storage, and that they outperform imputation even for small investments. Keywords: missing data, classiﬁcation, classiﬁcation trees, decision trees, imputation</p><p>6 0.28266865 <a title="11-lsi-6" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>7 0.25853634 <a title="11-lsi-7" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>8 0.19451781 <a title="11-lsi-8" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>9 0.19402558 <a title="11-lsi-9" href="./jmlr-2007-Nonlinear_Boosting_Projections_for_Ensemble_Construction.html">59 jmlr-2007-Nonlinear Boosting Projections for Ensemble Construction</a></p>
<p>10 0.17926276 <a title="11-lsi-10" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>11 0.17921653 <a title="11-lsi-11" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>12 0.16979448 <a title="11-lsi-12" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>13 0.16603006 <a title="11-lsi-13" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>14 0.15221874 <a title="11-lsi-14" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>15 0.15081351 <a title="11-lsi-15" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>16 0.14917432 <a title="11-lsi-16" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>17 0.14882101 <a title="11-lsi-17" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>18 0.14832468 <a title="11-lsi-18" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>19 0.14719526 <a title="11-lsi-19" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>20 0.14557576 <a title="11-lsi-20" href="./jmlr-2007-Concave_Learners_for_Rankboost.html">23 jmlr-2007-Concave Learners for Rankboost</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(10, 0.034), (12, 0.026), (15, 0.035), (28, 0.045), (40, 0.027), (45, 0.022), (48, 0.025), (60, 0.022), (63, 0.013), (80, 0.52), (85, 0.046), (98, 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.95719141 <a title="11-lda-1" href="./jmlr-2007-Measuring_Differentiability%3A__Unmasking_Pseudonymous_Authors.html">54 jmlr-2007-Measuring Differentiability:  Unmasking Pseudonymous Authors</a></p>
<p>Author: Moshe Koppel, Jonathan Schler, Elisheva Bonchek-Dokow</p><p>Abstract: In the authorship verification problem, we are given examples of the writing of a single author and are asked to determine if given long texts were or were not written by this author. We present a new learning-based method for adducing the “depth of difference” between two example sets and offer evidence that this method solves the authorship verification problem with very high accuracy. The underlying idea is to test the rate of degradation of the accuracy of learned models as the best features are iteratively dropped from the learning process. Keywords: authorship attribution, one-class learning, unmasking 1</p><p>same-paper 2 0.83692175 <a title="11-lda-2" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>Author: Saher Esmeir, Shaul Markovitch</p><p>Abstract: The majority of existing algorithms for learning decision trees are greedy—a tree is induced topdown, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Even the few non-greedy learners cannot learn good trees when the concept is difﬁcult. Furthermore, they require a ﬁxed amount of time and are not able to generate a better tree if additional time is available. We introduce a framework for anytime induction of decision trees that overcomes these problems by trading computation speed for better tree quality. Our proposed family of algorithms employs a novel strategy for evaluating candidate splits. A biased sampling of the space of consistent trees rooted at an attribute is used to estimate the size of the minimal tree under that attribute, and an attribute with the smallest expected tree is selected. We present two types of anytime induction algorithms: a contract algorithm that determines the sample size on the basis of a pre-given allocation of time, and an interruptible algorithm that starts with a greedy tree and continuously improves subtrees by additional sampling. Experimental results indicate that, for several hard concepts, our proposed approach exhibits good anytime behavior and yields signiﬁcantly better decision trees when more time is available. Keywords: anytime algorithms, decision tree induction, lookahead, hard concepts, resource-bounded reasoning</p><p>3 0.42658401 <a title="11-lda-3" href="./jmlr-2007-Handling_Missing_Values_when_Applying_Classification_Models.html">39 jmlr-2007-Handling Missing Values when Applying Classification Models</a></p>
<p>Author: Maytal Saar-Tsechansky, Foster Provost</p><p>Abstract: Much work has studied the effect of different treatments of missing values on model induction, but little work has analyzed treatments for the common case of missing values at prediction time. This paper ﬁrst compares several different methods—predictive value imputation, the distributionbased imputation used by C4.5, and using reduced models—for applying classiﬁcation trees to instances with missing values (and also shows evidence that the results generalize to bagged trees and to logistic regression). The results show that for the two most popular treatments, each is preferable under different conditions. Strikingly the reduced-models approach, seldom mentioned or used, consistently outperforms the other two methods, sometimes by a large margin. The lack of attention to reduced modeling may be due in part to its (perceived) expense in terms of computation or storage. Therefore, we then introduce and evaluate alternative, hybrid approaches that allow users to balance between more accurate but computationally expensive reduced modeling and the other, less accurate but less computationally expensive treatments. The results show that the hybrid methods can scale gracefully to the amount of investment in computation/storage, and that they outperform imputation even for small investments. Keywords: missing data, classiﬁcation, classiﬁcation trees, decision trees, imputation</p><p>4 0.364025 <a title="11-lda-4" href="./jmlr-2007-Structure_and_Majority_Classes_in_Decision_Tree_Learning.html">79 jmlr-2007-Structure and Majority Classes in Decision Tree Learning</a></p>
<p>Author: Ray J. Hickey</p><p>Abstract: To provide good classification accuracy on unseen examples, a decision tree, learned by an algorithm such as ID3, must have sufficient structure and also identify the correct majority class in each of its leaves. If there are inadequacies in respect of either of these, the tree will have a percentage classification rate below that of the maximum possible for the domain, namely (100 Bayes error rate). An error decomposition is introduced which enables the relative contributions of deficiencies in structure and in incorrect determination of majority class to be isolated and quantified. A sub-decomposition of majority class error permits separation of the sampling error at the leaves from the possible bias introduced by the attribute selection method of the induction algorithm. It is shown that sampling error can extend to 25% when there are more than two classes. Decompositions are obtained from experiments on several data sets. For ID3, the effect of selection bias is shown to vary from being statistically non-significant to being quite substantial, with the latter appearing to be associated with a simple underlying model. Keywords: decision tree learning, error decomposition, majority classes, sampling error, attribute selection bias 1</p><p>5 0.3561365 <a title="11-lda-5" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>Author: Wei Pan, Xiaotong Shen</p><p>Abstract: Variable selection in clustering analysis is both challenging and important. In the context of modelbased clustering analysis with a common diagonal covariance matrix, which is especially suitable for “high dimension, low sample size” settings, we propose a penalized likelihood approach with an L1 penalty function, automatically realizing variable selection via thresholding and delivering a sparse solution. We derive an EM algorithm to ﬁt our proposed model, and propose a modiﬁed BIC as a model selection criterion to choose the number of components and the penalization parameter. A simulation study and an application to gene function prediction with gene expression proﬁles demonstrate the utility of our method. Keywords: BIC, EM, mixture model, penalized likelihood, soft-thresholding, shrinkage</p><p>6 0.33246332 <a title="11-lda-6" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>7 0.33071256 <a title="11-lda-7" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>8 0.32482213 <a title="11-lda-8" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>9 0.31753197 <a title="11-lda-9" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>10 0.31303212 <a title="11-lda-10" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>11 0.30538958 <a title="11-lda-11" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>12 0.30400392 <a title="11-lda-12" href="./jmlr-2007-Nonlinear_Boosting_Projections_for_Ensemble_Construction.html">59 jmlr-2007-Nonlinear Boosting Projections for Ensemble Construction</a></p>
<p>13 0.30200404 <a title="11-lda-13" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>14 0.29744989 <a title="11-lda-14" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>15 0.29471359 <a title="11-lda-15" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>16 0.29336208 <a title="11-lda-16" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>17 0.28377038 <a title="11-lda-17" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>18 0.28240591 <a title="11-lda-18" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>19 0.28125298 <a title="11-lda-19" href="./jmlr-2007-Truncating_the_Loop_Series_Expansion_for_Belief_Propagation.html">86 jmlr-2007-Truncating the Loop Series Expansion for Belief Propagation</a></p>
<p>20 0.2793391 <a title="11-lda-20" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
