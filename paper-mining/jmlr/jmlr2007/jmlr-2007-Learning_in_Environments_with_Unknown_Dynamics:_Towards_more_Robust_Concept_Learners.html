<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-48" href="#">jmlr2007-48</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</h1>
<br/><p>Source: <a title="jmlr-2007-48-pdf" href="http://jmlr.org/papers/volume8/nunez07a/nunez07a.pdf">pdf</a></p><p>Author: Marlon Núñez, Raúl Fidalgo, Rafael Morales</p><p>Abstract: In the process of concept learning, target concepts may have portions with short-term changes, other portions may support long-term changes, and yet others may not change at all. For this reason several local windows need to be handled. We suggest facing this problem, which naturally exists in the ﬁeld of concept learning, by allocating windows which can adapt their size to portions of the target concept. We propose an incremental decision tree that is updated with incoming examples. Each leaf of the decision tree holds a time window and a local performance measure as the main parameter to be controlled. When the performance of a leaf decreases, the size of its local window is reduced. This learning algorithm, called OnlineTree2, automatically adjusts its internal parameters in order to face the current dynamics of the data stream. Results show that it is comparable to other batch algorithms when facing problems with no concept change, and it is better than evaluated methods in its ability to deal with concept drift when dealing with problems in which: concept change occurs at different speeds, noise may be present and, examples may arrive from different areas of the problem domain (virtual drift). Keywords: incremental algorithms, online learning, concept drift, decision trees, robust learners</p><p>Reference: <a title="jmlr-2007-48-reference" href="../jmlr2007_reference/jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Each leaf of the decision tree holds a time window and a local performance measure as the main parameter to be controlled. [sent-13, score-0.821]
</p><p>2 When the performance of a leaf decreases, the size of its local window is reduced. [sent-14, score-0.749]
</p><p>3 The basic idea of adaptive window management is to adjust the window size to the current rate of concept drift. [sent-21, score-0.822]
</p><p>4 This method incrementally learns a decision tree in which each leaf maintains a local window, used to forget examples when a concept change has been detected. [sent-31, score-0.897]
</p><p>5 Virtual drift occurs when the distribution of the observed examples changes over time but the concept remains the same. [sent-34, score-0.592]
</p><p>6 , 2005), was able to detect concept drift from small data sets (less than 200 examples) and manage noise level in data, but it does not work when the data set has numerical features, a data stream is present, change in noise levels appears and/or the problem contains virtual drift. [sent-61, score-0.885]
</p><p>7 OnlineTree2 corrects these deﬁciencies, being able to deal with data streams containing unknown dynamics (that is, possible concept drifts, changes in noise level, virtual drift, continuous or symbolic features and different distribution of examples). [sent-62, score-0.64]
</p><p>8 Figure 1: Illustration of gradual and abrupt concept drift for concepts described by two attributes (x1 and x2 ) In order to learn concepts that change over time, current learners usually use a global window of examples. [sent-74, score-1.029]
</p><p>9 Description of the OnlineTree2 Algorithm OnlineTree2 is an algorithm for incremental induction of binary decision trees, which also supports adaptability to gradual and abrupt concept drift, virtual drift, robustness to noise in data, and the handling of symbolic and numeric attributes. [sent-89, score-0.565]
</p><p>10 Actions that are carried out may be summarized in three stages: downward revision of statistics, treatment of a leaf or a non coherent node, and upward updating of statistics. [sent-99, score-0.692]
</p><p>11 This stage ﬁnishes when the algorithm reaches a non coherent node or a leaf. [sent-104, score-0.529]
</p><p>12 Treatment of a leaf or a non coherent node: (a) Non coherent node treatment: Once OnlineTree2 stops at a non coherent node, the algorithm drops the example down to its corresponding leaf. [sent-106, score-1.212]
</p><p>13 After that, the node is converted into a leaf with the remaining examples in the set of leaves. [sent-108, score-0.745]
</p><p>14 If the leaf improves its performance measure, the algorithm tries to create a new decision node in order to better adapt to the subconcept. [sent-111, score-0.717]
</p><p>15 , no more improvements may be made in this leaf with its examples) the algorithm checks the leaf for stability, discarding old local examples in that case. [sent-114, score-0.946]
</p><p>16 If the leaf is not improving its performance measure, then an attempt to reduce its window is performed. [sent-115, score-0.704]
</p><p>17 The stage ﬁnish when a non coherent node is found or a leaf is reached. [sent-122, score-0.951]
</p><p>18 We say a node is coherent when it brings something useful when inducing the subjacent subconcept of that node. [sent-123, score-0.493]
</p><p>19 Speciﬁcally, in order to know if a node is coherent with a concept we use a χ2 hypothesis test (with a signiﬁcance level of 0. [sent-126, score-0.641]
</p><p>20 Therefore, at each node the class distribution of the examples that are in the leaves below that node is updated. [sent-128, score-0.648]
</p><p>21 2 Stage 2: Treatment of a Leaf or a Non Coherent Node This section explains the actions that are carried out to modify, if necessary, a leaf or a non- coherent node. [sent-133, score-0.568]
</p><p>22 Performance As said before, each leaf of the tree stores a quality measure. [sent-138, score-0.53]
</p><p>23 States Using the above mentioned performance measure, OnlineTree2 employs a state diagram in each leaf to ﬁnd out if the leaf is in one of the following states: • Degradation State: A leaf passes into this state when the performance worsens. [sent-155, score-1.346]
</p><p>24 This suggests that a change in subconcept has occurred and the leaf must react accordingly. [sent-156, score-0.581]
</p><p>25 A window of previous examples is generated, which is called local window in degradation state (explained in Section 4. [sent-157, score-0.767]
</p><p>26 In this state a local window is generated, referred to as local window in improvement state (explained in Section 4. [sent-163, score-0.734]
</p><p>27 2 L OCAL PARAMETERS TO BE A DJUSTED The OnlineTree2 algorithm adjusts three local parameters in each leaf with the aim of achieving more efﬁcient learning in terms of: improving local performance, optimising the number of stored examples and reducing processing time. [sent-168, score-0.642]
</p><p>28 These local parameters are: local window in degradation state, local window in improvement state and local majority/expansion factor. [sent-169, score-0.845]
</p><p>29 Local Window Size in Degradation State When a leaf is in degradation state, it needs to adjust its window of examples to deal with a possible concept change and so improve its performance. [sent-170, score-1.097]
</p><p>30 This section explains in detail the mechanism to carry out the management of the local window size when the leaf is in this state. [sent-171, score-0.778]
</p><p>31 Figure 2: Illustration of the detection of a hypothesis anomaly and the delayed window at a leaf Deterioration in the performance of a leaf reduces the size of the delayed window by a fraction. [sent-176, score-1.565]
</p><p>32 If the deterioration persists, concept drift is more probable, and a greater fraction of window needs to be discarded. [sent-177, score-0.753]
</p><p>33 In order to calculate the fraction of the delayed window to be forgotten we use the following equation: w f = pers · d p where: wf is the window fraction of the delayed window to be forgotten; pers is the anomaly persistence; and, dp is the drop of performance from anomaly time. [sent-178, score-1.143]
</p><p>34 Thus, the new size of the delayed window will be: df =  dw(1 − w f ) if w f < 1 0 otherwise  where: dw is the size of the delayed window; and, wf is the window fraction to be forgotten. [sent-179, score-0.654]
</p><p>35 A reduction in size of the delayed window provokes the forgetting of older examples occurred before its anomaly time. [sent-180, score-0.547]
</p><p>36 Suppose that a burst of examples reached the leaf some time ago and few examples of a new concept have reached the leaf recently. [sent-186, score-1.157]
</p><p>37 This is due to the fact that a wellconstructed leaf with a high performance discards older examples on receiving new ones to avoid excessive accumulation of examples. [sent-193, score-0.527]
</p><p>38 Therefore we make use of a heuristic, that can be describe as: the number of examples in the leaf should not be greater than the number of examples in its brother subtree, especially when the performance of the leaf is high. [sent-194, score-0.998]
</p><p>39 As can be seen, when performance is low the boundary permits the leaf to accumulate examples so that it can induct the concept with more data. [sent-196, score-0.678]
</p><p>40 , every node has high performance), the number of examples beneath the two branches of any node, whether leaf or subtree, should be balanced. [sent-200, score-0.745]
</p><p>41 Therefore we make use of a heuristic which can be resumed as follow: a leaf is labelled when its number of majority class examples is greater than the number of examples of non-majority class in exponential factor; otherwise, OnlineTree2 may try to expand that leaf. [sent-211, score-0.614]
</p><p>42 This heuristic can be formalized as: m < er , try to expand the leaf otherwise , label the leaf where: m is the number of examples with majority class in the leaf, and r is the rest of the examples in that leaf, that is: the total number of examples in the leaf minus m. [sent-213, score-1.437]
</p><p>43 At this point, OnlineTree2 will try to adjust the leaf to adapt better its related subconcept. [sent-219, score-0.481]
</p><p>44 In order to binarize the continuous attributes, we use the clustering algorithm k-means (MacQueen, 1967) using the attribute values of the examples of the leaf we want to expand (Dougherty et al. [sent-233, score-0.479]
</p><p>45 If not, the leaf is labelled with the majority class, and OnlineTree2 adjusts the local window size in improvement state, as presented previously. [sent-240, score-0.858]
</p><p>46 4 S TAGE 2 A : N ON C OHERENT N ODE T REATMENT If, after the ﬁrst stage, the example has found an incoherent node with the current concept, that node may be adjusted by either; pruning and labelling, or grafting a new subtree that replaces the incoherent one. [sent-243, score-0.571]
</p><p>47 Then, the local window size of each leaf in degradation state below the non coherent node is adjusted as described in Section 4. [sent-245, score-1.305]
</p><p>48 In the case of an established concept and once a tree has been constructed that does not change over time (no concept drifts arrive), the complexity in time of each example is calculated as the cost of the example moving down the tree (i. [sent-255, score-0.656]
</p><p>49 As before, the example moves down the tree until it meets the leaf or the badly adjusted node. [sent-264, score-0.494]
</p><p>50 When using OnlineTree2, it is difﬁcult to know when a false alarm regarding concept drift is produced (i. [sent-289, score-0.504]
</p><p>51 , the detection of a concept change when there is none), because information about concept drift is distributed into the leaves of the tree. [sent-291, score-0.775]
</p><p>52 , we use 10−4 ), we suppose that a concept drift is detected. [sent-295, score-0.471]
</p><p>53 Throughout this section, low error levels and fast reaction to concept drift will show the relevance of using a strategy based on local windows when dealing with problems with unknown dynamics. [sent-297, score-0.621]
</p><p>54 To do so, we will present problems in which: a hyperplane changes its position in the attribute space gradually and/or abruptly, noise in examples can be increased/decreased, and the speed of the concept drift also changes. [sent-304, score-0.656]
</p><p>55 We are also interested in evaluating the incidence of virtual drift in which the concept remains the same but the distribution of the observed examples changes over time. [sent-305, score-0.704]
</p><p>56 Algorithms able to track concept drifts used to make comparisons are: CVFDT, IB1 with a global ﬁxed window and DDM with Na¨ve Bayes as base ı algorithm (abbreviated as DDM+NB). [sent-306, score-0.549]
</p><p>57 The incidence of noise and virtual drift within the concept change is evaluated in Section 5. [sent-311, score-0.693]
</p><p>58 2 presents the performance of the algorithms when facing a synthetic data stream involving unknown conditions, such as different degrees of concept change, virtual drift and noise. [sent-315, score-0.754]
</p><p>59 1 T HE I NCIDENCE OF N OISE  AND  V IRTUAL D RIFT WITHIN  THE  C ONCEPT C HANGE  In this section, we use a well-known data set in the concept drift community that is useful to evaluate and illustrate how an algorithm able to deal with concept drift should work. [sent-321, score-0.942]
</p><p>60 Figure 3: Misclassiﬁcation errors on SEA data set with 10% noise When facing concept drift problems, error rate curves show a common behaviour: just after a concept change, a sudden rise in the error rate occurs. [sent-343, score-0.807]
</p><p>61 It is desirable that once a concept change occurs, the classiﬁer detects it and quickly forgets a subset of examples and thus ﬁt the new concept accordingly. [sent-346, score-0.544]
</p><p>62 Results of IB1 using a global window, with ﬁxed size of 12500 examples, show that reactions to concept change are slow, as the algorithm must wait until the window has forgotten all examples from a previous concept. [sent-347, score-0.657]
</p><p>63 With respect to algorithms using a global adaptive window size (CVFDT and DDM+NB algorithms), reactions to concept drift are faster than the previous one, but insufﬁcient in third and fourth concepts. [sent-350, score-0.753]
</p><p>64 This is due to the local window strategy: OnlineTree2 detects persistent changes in subconcepts, reacting by forgetting examples from them. [sent-353, score-0.539]
</p><p>65 During the third concept it also detects some concept drifts when there are none, because of the massive noise level within this concept (noise level at 40%). [sent-372, score-0.832]
</p><p>66 This might be because DDM has difﬁculties in distinguishing concept changes when a previous concept has more noise level than the current concept. [sent-376, score-0.556]
</p><p>67 As in the previous experiment, IB1 with global ﬁxed window does not work well because it is sensitive to noise and it must wait until every example of the previous concept has been forgotten. [sent-378, score-0.545]
</p><p>68 In the following paragraphs we extend the original SEA data set deﬁnition to contain both concept drift and virtual drift. [sent-383, score-0.583]
</p><p>69 CVFDT and DDM+NB are affected by this phenomenon, detecting virtual drift as concept change, which implies degradation in its models. [sent-388, score-0.644]
</p><p>70 Contrary to these algorithms, IB1 with global ﬁxed window and OnlineTree2 show a stable behaviour along this data set, producing models similar to that obtained in the experiment without virtual drift (see Figure 3). [sent-389, score-0.666]
</p><p>71 Also, with the concept change the level of noise (nl) is increased by f rac5015, thus the data stream starts without noise and ﬁnishes this phase with 50% noise. [sent-414, score-0.501]
</p><p>72 This time virtual drift is produced each 1000 examples by changing the distribution of examples using a Normal distribution (N(a, 0. [sent-418, score-0.498]
</p><p>73 This time there are ﬁve concept changes, each of them ﬂip the labelled zones 2610  L EARNING IN E NVIRONMENTS WITH U NKNOWN DYNAMICS  from its previous concept and reduces the noise by 10%. [sent-425, score-0.54]
</p><p>74 The reason for this result is that OnlineTree2 adapts local windows and parameters of those leaves involved in gradual changes, which indicates robustness to noise, virtual drift and different levels of gradual changes. [sent-441, score-0.651]
</p><p>75 Regarding false alarms, OnlineTree2 detects concept drift when there is not due to massive noise in data a few times at the end of this phase. [sent-442, score-0.611]
</p><p>76 Analysis of Abrupt Change Phase Faced with these abrupt concept changes (beyond time step 550000), the best policy is to quickly discard all examples from previous concepts and induce the new concept with fresh examples. [sent-445, score-0.641]
</p><p>77 This is another advantage of the local drift detection, OnlineTree2 reacts better and faster to abrupt concept drifts than those global drift detection methods evaluated, and can control the selective forgetting of examples to reach high performance, independently of noise in data. [sent-447, score-1.086]
</p><p>78 2 Million examples, 10 continuous attributes) shows that it is the best algorithm of those evaluated when dealing with different and unknown conditions such as different speed of change, virtual drift and different noise levels. [sent-458, score-0.484]
</p><p>79 This data stream is presumed to have concept drift, noise and virtual drift. [sent-473, score-0.473]
</p><p>80 Also, from time step 20000, error rates from CVFDT and DDM+NB suggest that they do not detect concept drift because MLP (an incremental method not able to deal with concept drifting dynamics) error curve is below them. [sent-489, score-0.712]
</p><p>81 As this problem is suspected of having concept drift, OnlineTree2 is able to adapt to possibly different rates of changes (it detects 15 concept drifts) and/or noise in data. [sent-490, score-0.598]
</p><p>82 Conclusions An incremental decision tree learning method has been presented which is able to learn changing concepts with the presence of noise and virtual drift in examples for problems with unknown conditions. [sent-851, score-0.68]
</p><p>83 Contrary to most of the current methods, OnlineTree2 uses local adaptive windows using a new strategy which forgets examples as a result of the leaf reducing its window size when the local performance decreases. [sent-852, score-0.92]
</p><p>84 The proposed algorithm has less error rate at the end of each context than other methods studied in problems with gradual and abrupt concept drift and noise. [sent-856, score-0.579]
</p><p>85 1, every tree node has a state used to detect concept drifts. [sent-893, score-0.577]
</p><p>86 Once the information in a node has been deﬁned, we treat tree nodes as decision nodes (d-node) and leaf nodes (l-node). [sent-897, score-0.856]
</p><p>87 Deﬁnition 4 (l-node) l-node = (in f o, E) where: info contains leaf information as deﬁned above, and E is a FIFO structure containing the examples stored in the leaf. [sent-899, score-0.521]
</p><p>88 Deﬁnition 5 (node) A node can be a decision node (d-node) or a leaf node (l-node). [sent-908, score-1.22]
</p><p>89 As in Section 4, the pseudo-algorithm has been described with three stages: downwards revision of statistics, treatment of a non coherent node or leaf in the second stage, and ﬁnally, updating the information stored in visited nodes. [sent-913, score-1.029]
</p><p>90 1, the ﬁrst stage of the algorithm searches either a leaf or a node that does not ﬁt with current concept (non coherent node) in the path of current example from root to a leaf. [sent-918, score-1.107]
</p><p>91 It checks the node to be a leaf or for coherence with current concept by doing a χ 2 test with a signiﬁcance level of 0. [sent-920, score-0.917]
</p><p>92 If a leaf or a non coherent node is reached, the stage ends and the next one starts. [sent-923, score-0.951]
</p><p>93 2 T REATMENT  OF A  L EAF OR A N ON C OHERENT N ODE  As stated above, this stage tries to adjust tree structure to the dynamics of current concept in the data stream. [sent-926, score-0.521]
</p><p>94 It is divided into two parts, depending on the type of node returned by the ﬁrst stage: non coherent node treatment or leaf treatment. [sent-927, score-1.176]
</p><p>95 After that, the node is converted into a leaf with the unforgotten examples (see Pseudocode 10) and its metrics for forgetting examples are updated (see Pseudocode 8). [sent-931, score-0.85]
</p><p>96 Then a reconstruction of the pruned node is attempted (see Pseudocode 11) in order to make a coherent node for the current concept. [sent-932, score-0.678]
</p><p>97 Leaf Treatment When the ﬁrst stage ends in a leaf node, the example updates that leaf (see Pseudocode 6). [sent-933, score-0.918]
</p><p>98 If there is no reconstruction, the algorithm tries to ﬁnd out if the leaf has become stable, deciding to forget the oldest example of the leaf in that case, by calling the AdjustLocalWindowInImprovedLeaf procedure (see Pseudocode 12). [sent-937, score-0.9]
</p><p>99 If the leaf is not improving its performance, then an attempt to reduce its local window is performed (see Pseudocode 9). [sent-938, score-0.749]
</p><p>100 Learning in the presence of concept drift and hidden contexts. [sent-1073, score-0.471]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('leaf', 0.422), ('window', 0.282), ('drift', 0.272), ('node', 0.266), ('concept', 0.199), ('ddm', 0.185), ('cvfdt', 0.177), ('pseudocode', 0.167), ('coherent', 0.146), ('dynamics', 0.146), ('idalgo', 0.137), ('orales', 0.137), ('req', 0.137), ('nez', 0.13), ('nknown', 0.129), ('nvironments', 0.129), ('endif', 0.121), ('virtual', 0.112), ('stream', 0.098), ('subconcept', 0.081), ('labelled', 0.078), ('stage', 0.074), ('nb', 0.073), ('gama', 0.073), ('facing', 0.073), ('forgotten', 0.073), ('tree', 0.072), ('windows', 0.069), ('drifts', 0.068), ('sea', 0.068), ('klinkenberg', 0.067), ('anomaly', 0.067), ('changes', 0.064), ('noise', 0.064), ('abrupt', 0.061), ('degradation', 0.061), ('concepts', 0.061), ('leaves', 0.059), ('examples', 0.057), ('forget', 0.056), ('pedro', 0.055), ('streams', 0.055), ('cdnode', 0.048), ('fnode', 0.048), ('older', 0.048), ('revision', 0.048), ('forgetting', 0.048), ('hulten', 0.048), ('gradual', 0.047), ('change', 0.046), ('na', 0.045), ('local', 0.045), ('delayed', 0.045), ('non', 0.043), ('detects', 0.043), ('kubat', 0.043), ('incremental', 0.042), ('stored', 0.042), ('cd', 0.041), ('alarms', 0.041), ('electricity', 0.041), ('maloof', 0.041), ('earning', 0.041), ('adaptability', 0.04), ('adjustlocalwindowindegradedleaf', 0.04), ('brother', 0.04), ('elea', 0.04), ('isrevisable', 0.04), ('nextnode', 0.04), ('updateleaf', 0.04), ('updateperformance', 0.04), ('state', 0.04), ('subtree', 0.039), ('el', 0.038), ('widmer', 0.037), ('memory', 0.036), ('instantaneous', 0.036), ('speeds', 0.036), ('stores', 0.036), ('dealing', 0.036), ('market', 0.034), ('treatment', 0.033), ('false', 0.033), ('adjustlocalwindowinimprovedleaf', 0.032), ('react', 0.032), ('statenode', 0.032), ('trytoreconstruct', 0.032), ('updatenode', 0.032), ('updatestatistics', 0.032), ('widyantoro', 0.032), ('nodes', 0.032), ('experimentation', 0.031), ('mlp', 0.031), ('adjusts', 0.031), ('adjust', 0.03), ('store', 0.03), ('level', 0.03), ('visited', 0.029), ('adapt', 0.029), ('management', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="48-tfidf-1" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>Author: Marlon Núñez, Raúl Fidalgo, Rafael Morales</p><p>Abstract: In the process of concept learning, target concepts may have portions with short-term changes, other portions may support long-term changes, and yet others may not change at all. For this reason several local windows need to be handled. We suggest facing this problem, which naturally exists in the ﬁeld of concept learning, by allocating windows which can adapt their size to portions of the target concept. We propose an incremental decision tree that is updated with incoming examples. Each leaf of the decision tree holds a time window and a local performance measure as the main parameter to be controlled. When the performance of a leaf decreases, the size of its local window is reduced. This learning algorithm, called OnlineTree2, automatically adjusts its internal parameters in order to face the current dynamics of the data stream. Results show that it is comparable to other batch algorithms when facing problems with no concept change, and it is better than evaluated methods in its ability to deal with concept drift when dealing with problems in which: concept change occurs at different speeds, noise may be present and, examples may arrive from different areas of the problem domain (virtual drift). Keywords: incremental algorithms, online learning, concept drift, decision trees, robust learners</p><p>2 0.18050787 <a title="48-tfidf-2" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>Author: J. Zico Kolter, Marcus A. Maloof</p><p>Abstract: We present an ensemble method for concept drift that dynamically creates and removes weighted experts in response to changes in performance. The method, dynamic weighted majority (DWM), uses four mechanisms to cope with concept drift: It trains online learners of the ensemble, it weights those learners based on their performance, it removes them, also based on their performance, and it adds new experts based on the global performance of the ensemble. After an extensive evaluation— consisting of ﬁve experiments, eight learners, and thirty data sets that varied in type of target concept, size, presence of noise, and the like—we concluded that DWM outperformed other learners that only incrementally learn concept descriptions, that maintain and use previously encountered examples, and that employ an unweighted, ﬁxed-size ensemble of experts. Keywords: concept learning, online learning, ensemble methods, concept drift</p><p>3 0.14561334 <a title="48-tfidf-3" href="./jmlr-2007-Structure_and_Majority_Classes_in_Decision_Tree_Learning.html">79 jmlr-2007-Structure and Majority Classes in Decision Tree Learning</a></p>
<p>Author: Ray J. Hickey</p><p>Abstract: To provide good classification accuracy on unseen examples, a decision tree, learned by an algorithm such as ID3, must have sufficient structure and also identify the correct majority class in each of its leaves. If there are inadequacies in respect of either of these, the tree will have a percentage classification rate below that of the maximum possible for the domain, namely (100 Bayes error rate). An error decomposition is introduced which enables the relative contributions of deficiencies in structure and in incorrect determination of majority class to be isolated and quantified. A sub-decomposition of majority class error permits separation of the sampling error at the leaves from the possible bias introduced by the attribute selection method of the induction algorithm. It is shown that sampling error can extend to 25% when there are more than two classes. Decompositions are obtained from experiments on several data sets. For ID3, the effect of selection bias is shown to vary from being statistically non-significant to being quite substantial, with the latter appearing to be associated with a simple underlying model. Keywords: decision tree learning, error decomposition, majority classes, sampling error, attribute selection bias 1</p><p>4 0.088700451 <a title="48-tfidf-4" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>Author: Saher Esmeir, Shaul Markovitch</p><p>Abstract: The majority of existing algorithms for learning decision trees are greedy—a tree is induced topdown, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Even the few non-greedy learners cannot learn good trees when the concept is difﬁcult. Furthermore, they require a ﬁxed amount of time and are not able to generate a better tree if additional time is available. We introduce a framework for anytime induction of decision trees that overcomes these problems by trading computation speed for better tree quality. Our proposed family of algorithms employs a novel strategy for evaluating candidate splits. A biased sampling of the space of consistent trees rooted at an attribute is used to estimate the size of the minimal tree under that attribute, and an attribute with the smallest expected tree is selected. We present two types of anytime induction algorithms: a contract algorithm that determines the sample size on the basis of a pre-given allocation of time, and an interruptible algorithm that starts with a greedy tree and continuously improves subtrees by additional sampling. Experimental results indicate that, for several hard concepts, our proposed approach exhibits good anytime behavior and yields signiﬁcantly better decision trees when more time is available. Keywords: anytime algorithms, decision tree induction, lookahead, hard concepts, resource-bounded reasoning</p><p>5 0.067223489 <a title="48-tfidf-5" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>6 0.053702172 <a title="48-tfidf-6" href="./jmlr-2007-Dynamics_and_Generalization_Ability_of_LVQ_Algorithms.html">30 jmlr-2007-Dynamics and Generalization Ability of LVQ Algorithms</a></p>
<p>7 0.050299682 <a title="48-tfidf-7" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>8 0.047667917 <a title="48-tfidf-8" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>9 0.047265302 <a title="48-tfidf-9" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>10 0.039592978 <a title="48-tfidf-10" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>11 0.031762797 <a title="48-tfidf-11" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>12 0.031657856 <a title="48-tfidf-12" href="./jmlr-2007-Very_Fast_Online_Learning_of_Highly_Non_Linear_Problems.html">91 jmlr-2007-Very Fast Online Learning of Highly Non Linear Problems</a></p>
<p>13 0.030268226 <a title="48-tfidf-13" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>14 0.028732726 <a title="48-tfidf-14" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>15 0.028505331 <a title="48-tfidf-15" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>16 0.027670037 <a title="48-tfidf-16" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>17 0.026216526 <a title="48-tfidf-17" href="./jmlr-2007-Measuring_Differentiability%3A__Unmasking_Pseudonymous_Authors.html">54 jmlr-2007-Measuring Differentiability:  Unmasking Pseudonymous Authors</a></p>
<p>18 0.02516265 <a title="48-tfidf-18" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>19 0.025081214 <a title="48-tfidf-19" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>20 0.024430458 <a title="48-tfidf-20" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.166), (1, 0.236), (2, -0.083), (3, 0.099), (4, 0.099), (5, 0.257), (6, 0.227), (7, 0.138), (8, -0.212), (9, 0.166), (10, -0.047), (11, 0.087), (12, -0.187), (13, -0.127), (14, -0.072), (15, 0.172), (16, -0.003), (17, 0.089), (18, -0.152), (19, 0.134), (20, 0.007), (21, 0.155), (22, -0.02), (23, 0.058), (24, -0.033), (25, 0.003), (26, 0.028), (27, 0.15), (28, 0.035), (29, 0.037), (30, 0.006), (31, -0.049), (32, -0.092), (33, -0.094), (34, -0.044), (35, 0.063), (36, 0.002), (37, 0.042), (38, 0.027), (39, -0.007), (40, 0.021), (41, 0.032), (42, 0.12), (43, -0.007), (44, 0.04), (45, 0.048), (46, -0.021), (47, 0.017), (48, 0.017), (49, -0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97305638 <a title="48-lsi-1" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>Author: Marlon Núñez, Raúl Fidalgo, Rafael Morales</p><p>Abstract: In the process of concept learning, target concepts may have portions with short-term changes, other portions may support long-term changes, and yet others may not change at all. For this reason several local windows need to be handled. We suggest facing this problem, which naturally exists in the ﬁeld of concept learning, by allocating windows which can adapt their size to portions of the target concept. We propose an incremental decision tree that is updated with incoming examples. Each leaf of the decision tree holds a time window and a local performance measure as the main parameter to be controlled. When the performance of a leaf decreases, the size of its local window is reduced. This learning algorithm, called OnlineTree2, automatically adjusts its internal parameters in order to face the current dynamics of the data stream. Results show that it is comparable to other batch algorithms when facing problems with no concept change, and it is better than evaluated methods in its ability to deal with concept drift when dealing with problems in which: concept change occurs at different speeds, noise may be present and, examples may arrive from different areas of the problem domain (virtual drift). Keywords: incremental algorithms, online learning, concept drift, decision trees, robust learners</p><p>2 0.80127889 <a title="48-lsi-2" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>Author: J. Zico Kolter, Marcus A. Maloof</p><p>Abstract: We present an ensemble method for concept drift that dynamically creates and removes weighted experts in response to changes in performance. The method, dynamic weighted majority (DWM), uses four mechanisms to cope with concept drift: It trains online learners of the ensemble, it weights those learners based on their performance, it removes them, also based on their performance, and it adds new experts based on the global performance of the ensemble. After an extensive evaluation— consisting of ﬁve experiments, eight learners, and thirty data sets that varied in type of target concept, size, presence of noise, and the like—we concluded that DWM outperformed other learners that only incrementally learn concept descriptions, that maintain and use previously encountered examples, and that employ an unweighted, ﬁxed-size ensemble of experts. Keywords: concept learning, online learning, ensemble methods, concept drift</p><p>3 0.58661318 <a title="48-lsi-3" href="./jmlr-2007-Structure_and_Majority_Classes_in_Decision_Tree_Learning.html">79 jmlr-2007-Structure and Majority Classes in Decision Tree Learning</a></p>
<p>Author: Ray J. Hickey</p><p>Abstract: To provide good classification accuracy on unseen examples, a decision tree, learned by an algorithm such as ID3, must have sufficient structure and also identify the correct majority class in each of its leaves. If there are inadequacies in respect of either of these, the tree will have a percentage classification rate below that of the maximum possible for the domain, namely (100 Bayes error rate). An error decomposition is introduced which enables the relative contributions of deficiencies in structure and in incorrect determination of majority class to be isolated and quantified. A sub-decomposition of majority class error permits separation of the sampling error at the leaves from the possible bias introduced by the attribute selection method of the induction algorithm. It is shown that sampling error can extend to 25% when there are more than two classes. Decompositions are obtained from experiments on several data sets. For ID3, the effect of selection bias is shown to vary from being statistically non-significant to being quite substantial, with the latter appearing to be associated with a simple underlying model. Keywords: decision tree learning, error decomposition, majority classes, sampling error, attribute selection bias 1</p><p>4 0.4245393 <a title="48-lsi-4" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>Author: Saher Esmeir, Shaul Markovitch</p><p>Abstract: The majority of existing algorithms for learning decision trees are greedy—a tree is induced topdown, making locally optimal decisions at each node. In most cases, however, the constructed tree is not globally optimal. Even the few non-greedy learners cannot learn good trees when the concept is difﬁcult. Furthermore, they require a ﬁxed amount of time and are not able to generate a better tree if additional time is available. We introduce a framework for anytime induction of decision trees that overcomes these problems by trading computation speed for better tree quality. Our proposed family of algorithms employs a novel strategy for evaluating candidate splits. A biased sampling of the space of consistent trees rooted at an attribute is used to estimate the size of the minimal tree under that attribute, and an attribute with the smallest expected tree is selected. We present two types of anytime induction algorithms: a contract algorithm that determines the sample size on the basis of a pre-given allocation of time, and an interruptible algorithm that starts with a greedy tree and continuously improves subtrees by additional sampling. Experimental results indicate that, for several hard concepts, our proposed approach exhibits good anytime behavior and yields signiﬁcantly better decision trees when more time is available. Keywords: anytime algorithms, decision tree induction, lookahead, hard concepts, resource-bounded reasoning</p><p>5 0.2909081 <a title="48-lsi-5" href="./jmlr-2007-Dynamics_and_Generalization_Ability_of_LVQ_Algorithms.html">30 jmlr-2007-Dynamics and Generalization Ability of LVQ Algorithms</a></p>
<p>Author: Michael Biehl, Anarta Ghosh, Barbara Hammer</p><p>Abstract: Learning vector quantization (LVQ) schemes constitute intuitive, powerful classiﬁcation heuristics with numerous successful applications but, so far, limited theoretical background. We study LVQ rigorously within a simplifying model situation: two competing prototypes are trained from a sequence of examples drawn from a mixture of Gaussians. Concepts from statistical physics and the theory of on-line learning allow for an exact description of the training dynamics in highdimensional feature space. The analysis yields typical learning curves, convergence properties, and achievable generalization abilities. This is also possible for heuristic training schemes which do not relate to a cost function. We compare the performance of several algorithms, including Kohonen’s LVQ1 and LVQ+/-, a limiting case of LVQ2.1. The former shows close to optimal performance, while LVQ+/- displays divergent behavior. We investigate how early stopping can overcome this difﬁculty. Furthermore, we study a crisp version of robust soft LVQ, which was recently derived from a statistical formulation. Surprisingly, it exhibits relatively poor generalization. Performance improves if a window for the selection of data is introduced; the resulting algorithm corresponds to cost function based LVQ2. The dependence of these results on the model parameters, for example, prior class probabilities, is investigated systematically, simulations conﬁrm our analytical ﬁndings. Keywords: prototype based classiﬁcation, learning vector quantization, Winner-Takes-All algorithms, on-line learning, competitive learning</p><p>6 0.27305055 <a title="48-lsi-6" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>7 0.22328404 <a title="48-lsi-7" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>8 0.19800758 <a title="48-lsi-8" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<p>9 0.17295143 <a title="48-lsi-9" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>10 0.15736978 <a title="48-lsi-10" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>11 0.15348555 <a title="48-lsi-11" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>12 0.14902775 <a title="48-lsi-12" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>13 0.14837523 <a title="48-lsi-13" href="./jmlr-2007-Nonlinear_Boosting_Projections_for_Ensemble_Construction.html">59 jmlr-2007-Nonlinear Boosting Projections for Ensemble Construction</a></p>
<p>14 0.14677018 <a title="48-lsi-14" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>15 0.14034757 <a title="48-lsi-15" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>16 0.13821538 <a title="48-lsi-16" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>17 0.12936361 <a title="48-lsi-17" href="./jmlr-2007-Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning.html">85 jmlr-2007-Transfer Learning via Inter-Task Mappings for Temporal Difference Learning</a></p>
<p>18 0.1253235 <a title="48-lsi-18" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>19 0.12445814 <a title="48-lsi-19" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>20 0.12179949 <a title="48-lsi-20" href="./jmlr-2007-Measuring_Differentiability%3A__Unmasking_Pseudonymous_Authors.html">54 jmlr-2007-Measuring Differentiability:  Unmasking Pseudonymous Authors</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.03), (8, 0.011), (10, 0.587), (12, 0.021), (15, 0.019), (28, 0.032), (40, 0.012), (45, 0.013), (48, 0.026), (60, 0.021), (80, 0.038), (85, 0.045), (98, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90136635 <a title="48-lda-1" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>Author: Marlon Núñez, Raúl Fidalgo, Rafael Morales</p><p>Abstract: In the process of concept learning, target concepts may have portions with short-term changes, other portions may support long-term changes, and yet others may not change at all. For this reason several local windows need to be handled. We suggest facing this problem, which naturally exists in the ﬁeld of concept learning, by allocating windows which can adapt their size to portions of the target concept. We propose an incremental decision tree that is updated with incoming examples. Each leaf of the decision tree holds a time window and a local performance measure as the main parameter to be controlled. When the performance of a leaf decreases, the size of its local window is reduced. This learning algorithm, called OnlineTree2, automatically adjusts its internal parameters in order to face the current dynamics of the data stream. Results show that it is comparable to other batch algorithms when facing problems with no concept change, and it is better than evaluated methods in its ability to deal with concept drift when dealing with problems in which: concept change occurs at different speeds, noise may be present and, examples may arrive from different areas of the problem domain (virtual drift). Keywords: incremental algorithms, online learning, concept drift, decision trees, robust learners</p><p>2 0.81868899 <a title="48-lda-2" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>Author: Marc Teboulle</p><p>Abstract: Center-based partitioning clustering algorithms rely on minimizing an appropriately formulated objective function, and different formulations suggest different possible algorithms. In this paper, we start with the standard nonconvex and nonsmooth formulation of the partitioning clustering problem. We demonstrate that within this elementary formulation, convex analysis tools and optimization theory provide a unifying language and framework to design, analyze and extend hard and soft center-based clustering algorithms, through a generic algorithm which retains the computational simplicity of the popular k-means scheme. We show that several well known and more recent center-based clustering algorithms, which have been derived either heuristically, or/and have emerged from intuitive analogies in physics, statistical techniques and information theoretic perspectives can be recovered as special cases of the proposed analysis and we streamline their relationships. Keywords: clustering, k-means algorithm, convex analysis, support and asymptotic functions, distance-like functions, Bregman and Csiszar divergences, nonlinear means, nonsmooth optimization, smoothing algorithms, ﬁxed point methods, deterministic annealing, expectation maximization, information theory and entropy methods</p><p>3 0.47028524 <a title="48-lda-3" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>Author: J. Zico Kolter, Marcus A. Maloof</p><p>Abstract: We present an ensemble method for concept drift that dynamically creates and removes weighted experts in response to changes in performance. The method, dynamic weighted majority (DWM), uses four mechanisms to cope with concept drift: It trains online learners of the ensemble, it weights those learners based on their performance, it removes them, also based on their performance, and it adds new experts based on the global performance of the ensemble. After an extensive evaluation— consisting of ﬁve experiments, eight learners, and thirty data sets that varied in type of target concept, size, presence of noise, and the like—we concluded that DWM outperformed other learners that only incrementally learn concept descriptions, that maintain and use previously encountered examples, and that employ an unweighted, ﬁxed-size ensemble of experts. Keywords: concept learning, online learning, ensemble methods, concept drift</p><p>4 0.322393 <a title="48-lda-4" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>5 0.31494004 <a title="48-lda-5" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>Author: Wei Pan, Xiaotong Shen</p><p>Abstract: Variable selection in clustering analysis is both challenging and important. In the context of modelbased clustering analysis with a common diagonal covariance matrix, which is especially suitable for “high dimension, low sample size” settings, we propose a penalized likelihood approach with an L1 penalty function, automatically realizing variable selection via thresholding and delivering a sparse solution. We derive an EM algorithm to ﬁt our proposed model, and propose a modiﬁed BIC as a model selection criterion to choose the number of components and the penalization parameter. A simulation study and an application to gene function prediction with gene expression proﬁles demonstrate the utility of our method. Keywords: BIC, EM, mixture model, penalized likelihood, soft-thresholding, shrinkage</p><p>6 0.30126503 <a title="48-lda-6" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>7 0.29861742 <a title="48-lda-7" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>8 0.29568505 <a title="48-lda-8" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>9 0.29559448 <a title="48-lda-9" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>10 0.27973199 <a title="48-lda-10" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>11 0.27807713 <a title="48-lda-11" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>12 0.27290776 <a title="48-lda-12" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>13 0.26932967 <a title="48-lda-13" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>14 0.26758566 <a title="48-lda-14" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>15 0.26723677 <a title="48-lda-15" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>16 0.26482308 <a title="48-lda-16" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>17 0.26309252 <a title="48-lda-17" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>18 0.26160493 <a title="48-lda-18" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>19 0.25839388 <a title="48-lda-19" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>20 0.256506 <a title="48-lda-20" href="./jmlr-2007-Loop_Corrections_for_Approximate_Inference_on_Factor_Graphs.html">51 jmlr-2007-Loop Corrections for Approximate Inference on Factor Graphs</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
