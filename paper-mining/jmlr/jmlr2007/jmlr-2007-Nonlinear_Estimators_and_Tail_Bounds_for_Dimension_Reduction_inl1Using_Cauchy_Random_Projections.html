<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-60" href="#">jmlr2007-60</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</h1>
<br/><p>Source: <a title="jmlr-2007-60-pdf" href="http://jmlr.org/papers/volume8/li07b/li07b.pdf">pdf</a></p><p>Author: Ping Li, Trevor J. Hastie, Kenneth W. Church</p><p>Abstract: For1 dimension reduction in the l1 norm, the method of Cauchy random projections multiplies the original data matrix A ∈ Rn×D with a random matrix R ∈ RD×k (k D) whose entries are i.i.d. samples of the standard Cauchy C(0, 1). Because of the impossibility result, one can not hope to recover the pairwise l1 distances in A from B = A × R ∈ Rn×k , using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O log n sufﬁces with the constants explicitly ε2 given. Asymptotically (as k → ∞), both the sample median and the geometric mean estimators are about 80% efﬁcient compared to the MLE. We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian. Keywords: dimension reduction, l1 norm, Johnson-Lindenstrauss (JL) lemma, Cauchy random projections</p><p>Reference: <a title="jmlr-2007-60-reference" href="../jmlr2007_reference/jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 COM  Microsoft Research Microsoft Corporation Redmond, WA 98052, USA  Editor: Sam Roweis  Abstract For1  dimension reduction in the l1 norm, the method of Cauchy random projections multiplies the original data matrix A ∈ Rn×D with a random matrix R ∈ RD×k (k D) whose entries are i. [sent-6, score-0.244]
</p><p>2 Because of the impossibility result, one can not hope to recover the pairwise l1 distances in A from B = A × R ∈ Rn×k , using linear estimators without incurring large errors. [sent-10, score-0.273]
</p><p>3 However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. [sent-11, score-0.251]
</p><p>4 We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). [sent-12, score-0.461]
</p><p>5 We derive tail bounds for the geometric mean estimators and establish that k = O log n sufﬁces with the constants explicitly ε2 given. [sent-13, score-0.696]
</p><p>6 Asymptotically (as k → ∞), both the sample median and the geometric mean estimators are about 80% efﬁcient compared to the MLE. [sent-14, score-0.389]
</p><p>7 We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian. [sent-15, score-0.224]
</p><p>8 This paper focuses on dimension reduction in the l1 norm, in particular, on the method based on Cauchy random projections, which is a special case of linear (stable) random projections (Johnson and Schechtman, 1982; Indyk, 2000, 2006; Li, 2008). [sent-30, score-0.244]
</p><p>9 Thus, we will call random projections for l2 and l1 , normal random projections and Cauchy random projections, respectively. [sent-43, score-0.36]
</p><p>10 In normal random projections (Vempala, 2004), we can estimate the original pairwise l 2 distances in A directly using the corresponding l2 distances in B (up to a normalizing constant). [sent-44, score-0.278]
</p><p>11 The impossibility results (Brinkman and Charikar, 2003; Lee and Naor, 2004; Brinkman and Charikar, 2005) have proved that one can not hope to recover the l 1 distance using linear projections and linear estimators (e. [sent-48, score-0.343]
</p><p>12 In this paper, we study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). [sent-52, score-0.461]
</p><p>13 The sample median and the geometric mean estimators are asymptotically (as k → ∞) equivalent (i. [sent-53, score-0.431]
</p><p>14 Furthermore, we derive explicit tail bounds for the geometric mean estimators and establish an analog of the JL Lemma for dimension reduction in l1 . [sent-56, score-0.68]
</p><p>15 In data stream computations, Cauchy random projections can be used for (A): approximating the l1 frequency moments for individual streams; (B): approximating the l1 differences between a pair of streams. [sent-81, score-0.393]
</p><p>16 2 We can similarly prove the other tail bound for Pr(dˆl2 − dl2 ≤ −εdl2 ). [sent-133, score-0.297]
</p><p>17 For convenience, sometimes we would like to write the tail bounds in a symmetric form ε Pr dˆl2 − dl2 ≥ εdl2 ≤ 2 exp −k G  2  and we know that it sufﬁces to let G = max{GR , GL } ≤ 2  ,  0 < ε < 1,  4 2 . [sent-134, score-0.364]
</p><p>18 1− 3 ε  Since there are in total n(n−1) < n pairs among n data points, we would like to bound the tail 2 2 probabilities simultaneously for all pairs. [sent-135, score-0.297]
</p><p>19 By the Bonferroni union bound, it sufﬁces if n2 Pr dˆl2 − dl2 ≥ εdl2 ≤ δ, 2 that is, it sufﬁces if n2 ε2 2 exp −k 2 G  ≤ δ =⇒ k ≥ G  2 log n − log δ . [sent-136, score-0.29]
</p><p>20 Main Results Although the impossibility results (Lee and Naor, 2004; Brinkman and Charikar, 2005) have ruled out accurate estimators that are also metrics, there is enough information to recover d from k samples {x j }k , with high accuracy. [sent-176, score-0.252]
</p><p>21 j=1 We analyze three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators. [sent-177, score-0.289]
</p><p>22 We recommend the bias-corrected version: dˆme dˆme,c = , bme where bme =  Z 1 (2m + 1)! [sent-183, score-0.315]
</p><p>23 R 1 π 2 m 0 tan 2 t (t − t ) dt  =  π2 2 1 d +O 2 4k k  m t − t 2 dt  R1  . [sent-192, score-0.482]
</p><p>24 2     − 1 ,  k≥5  • bme ≥ 1 and bme → 1 monotonically with increasing k. [sent-193, score-0.292]
</p><p>25 To remove the bias and also reduce the variance, we recommend the bias-corrected geometric mean estimator: π 2k  dˆgm,c = cosk  k  ∏ |x j |1/k , j=1  which is unbiased and has variance Var dˆgm,c = d 2  cos2k cosk  π 2k π k  −1  =  1 π2 d 2 π4 d 2 + +O 3 2 4 k 32 k k  . [sent-205, score-0.85]
</p><p>26 Lemma 3 The sample median estimator, dˆme , is asymptotically unbiased and normal √  π D k dˆme − d =⇒ N 0, d 2 . [sent-226, score-0.259]
</p><p>27 Proof Let f (z; d) and F(z; d) be the probability density and cumulative density respectively for |C(0, d)|:  f (z; d) =  2d 1 , π z2 + d 2  F(z; d) =  2505  2 −1 z , tan π d  z ≥ 0. [sent-233, score-0.378]
</p><p>28 L I , H ASTIE AND C HURCH  The inverse of F(z; d) is F −1 (q; d) = d tan π q . [sent-234, score-0.426]
</p><p>29 2), we know that √ D k dˆme − d =⇒ N 0,  11 22  f d tan  π1 22  ; d × tan  π1 22  2  =  π2 2 d , 4  2 that is, dˆme is asymptotically unbiased and normal with the variance Var dˆme = π d 2 + O k12 . [sent-238, score-0.894]
</p><p>30 Lemma 4 The estimator, dˆme , dˆme,c = bme is unbiased, that is, E dˆme,c = d, where the bias-correction factor bme is Z 1 E dˆme (2m + 1)! [sent-245, score-0.292]
</p><p>31 ) 2 0  The variance of dˆme,c is   2 πt 0 tan 2  R1   (m! [sent-247, score-0.378]
</p><p>32 R 1 0  tan  π 2t  t − t2  m  m t − t 2 dt  (t − t 2 )m dt  (k = 2m + 1). [sent-249, score-0.482]
</p><p>33 4 The bias-correction factor bme is monotonically decreasing with increasing m, and bme ≥ 1,  lim bme = 1. [sent-252, score-0.438]
</p><p>34 1 1 0  5 10 15 20 25 30 35 40 45 50 Sample size k  Figure 1: The bias correction factor, bme in (3), as a function of k = 2m + 1. [sent-293, score-0.216]
</p><p>35 The Geometric Mean Estimators This section derives estimators based on the geometric mean, which are more accurate than the sample median estimators. [sent-297, score-0.386]
</p><p>36 The geometric mean estimators allow us to derive tail bounds in explicit forms and (consequently) establish an analog of the Johnson-Lindenstrauss (JL) Lemma for dimension reduction in the l1 norm. [sent-298, score-0.68]
</p><p>37 Lemma 6 dˆgm,c = cosk  π 2k  k  ∏ |x j |1/k ,  k>1  (4)  j=1  is unbiased, with the variance (valid when k > 2) Var dˆgm,c = d 2  cos2k cosk  π 2k π k  −1  =  1 d 2 π2 π4 d 2 + +O 3 2 k 4 32 k k  . [sent-305, score-0.65]
</p><p>38 In Section 6, we will show how to approximate the distribution of the maximum likelihood estimator by matching the ﬁrst four moments (in the leading terms). [sent-308, score-0.304]
</p><p>39 Fortunately, we do not have to do so because we are able to derive the exact tail bounds for dˆgm,c in Lemma 9. [sent-310, score-0.31]
</p><p>40 As shown in Figure 2, the ratios of the mean square errors (MSE) MSE dˆgm = MSE dˆgm,c  1 cosk ( π ) k  − cosk2 π + 1 ( 2k ) π cos2k ( 2k ) −1 cosk ( π ) k  (5)  demonstrate that the two geometric mean estimators are similar when k > 50, in terms of the MSE. [sent-324, score-0.996]
</p><p>41 One advantage of dˆgm is the convenience for deriving tail bounds. [sent-333, score-0.267]
</p><p>42 Thus, before presenting Lemma 9 for dˆgm,c , we prove tail bounds for dˆgm in Lemma 7 (proved in Appendix B). [sent-334, score-0.31]
</p><p>43 Similarly, we derive tail bounds for the unbiased geometric mean estimator dˆgm,c , in Lemma 9, which is proved in Appendix C. [sent-373, score-0.541]
</p><p>44 2510  C AUCHY R ANDOM P ROJECTIONS  Lemma 9 π 2k  ∗  coskt1  Pr dˆgm,c ≥ (1 + ε)d ≤ UR,gm,c =  cosk  ∗ πt1 2k  (1 + ε)t1  ∗  ε>0  ,  where ∗ t1 =  2k −1 tan π  log(1 + ε) − k log cos  Pr dˆgm,c ≤ (1 − ε)d ≤ UL,gm,c =  π 2k  2 . [sent-374, score-0.915]
</p><p>45 π  ∗  cosk  (1 − ε)t2  ∗ πt2 2k  ∗  coskt2  π 2k  0 < ε < 1, k ≥  ,  π2 8ε  where ∗ t2 =  2k −1 tan π  − log(1 − ε) + k log cos  π 2k  2 . [sent-375, score-0.915]
</p><p>46 Figure 4 plots the tail bound ratios ρR,k and ρL,k as deﬁned in Lemma 9, indicating that the asymptotic expressions ρR,∞ and ρL,∞ are in fact very accurate even for small k (e. [sent-377, score-0.475]
</p><p>47 Figure 4 illustrates that introducing the bias-correction term in dˆgm,c reduces the right tail bound but ampliﬁes the left tail bound. [sent-380, score-0.564]
</p><p>48 = ε2 ε2 UR,gm +UL,gm exp −k GR,gm + exp −k GL,gm  (12)  Finally, Figure 6 compares dˆgm,c with the sample median estimators dˆme and dˆme,c , in terms of the mean square errors. [sent-382, score-0.427]
</p><p>49 9 1 ε  Figure 5: The overall ratios of tail bounds, ρk as deﬁned in (12) are almost always below one, demonstrating that the bias-corrected estimator dˆgm,c may exhibit better overall tail behavior than the biased estimator dˆgm . [sent-432, score-0.76]
</p><p>50 The Maximum Likelihood Estimators This section analyzes the maximum likelihood estimators (MLE), which are asymptotically optimum (in terms of the variance). [sent-434, score-0.256]
</p><p>51 In comparisons, the sample median and geometric mean estimators are not optimum. [sent-435, score-0.389]
</p><p>52 (2006a) applied the maximum likelihood method to normal random projections and provided an improved estimator of the l2 distance by taking advantage of the marginal information. [sent-439, score-0.319]
</p><p>53 We will propose an inverse Gaussian distribution to approximate the distribution of dˆMLE,c , by matching the ﬁrst four moments (at least in the leading terms). [sent-478, score-0.223]
</p><p>54 1 A Numerical Example The maximum likelihood estimators are tested on some Microsoft Web crawl data, a term-bydocument matrix with D = 216 Web pages. [sent-480, score-0.214]
</p><p>55 Figure 7 illustrates that the bias correction is effective and these (asymptotic) formulas for the ﬁrst four moments of dˆMLE,c in Lemma 10 are accurate, especially when k ≥ 20. [sent-483, score-0.22]
</p><p>56 We conduct Cauchy random projections and estimate the l 1 distance between one pair of words using the maximum likelihood estimator dˆMLE and the biascorrected version dˆMLE,c . [sent-513, score-0.271]
</p><p>57 We will ﬁrst consider a gamma distribution with the same ﬁrst two (asymptotic) moments of dˆMLE,c . [sent-525, score-0.273]
</p><p>58 We will furthermore consider a generalized gamma distribution, which allows us to match the ﬁrst three (asymptotic) moments of dˆMLE,c . [sent-530, score-0.273]
</p><p>59 , 10 −10 ) tail probability range, O k−3/2 is not too meaningful. [sent-536, score-0.267]
</p><p>60 (2006c) applied gamma and generalized gamma approximations to model the performance measure distribution in some wireless communication channels using random matrix theory and produced accurate results in evaluating the error probabilities. [sent-541, score-0.297]
</p><p>61 3 A gamma distribution, G(α, β), has two parameters, α and β, which can be determined by matching the ﬁrst two (asymptotic) moments of dˆMLE,c . [sent-545, score-0.273]
</p><p>62 Figure 8(a) shows that both the gamma and normal approximations are fairly accurate when the tail probability ≥ 10−2 ∼ 10−3 ; and the gamma approximation is obviously better. [sent-551, score-0.584]
</p><p>63 Figure 8(b) compares the empirical tail probabilities with the gamma Chernoff upper bound (14)+(15), indicating that these bounds are reliable, when the tail probability ≥ 10 −5 ∼ 10−6 . [sent-552, score-0.753]
</p><p>64 Recall that, in normal random projections for dimension reduction in l 2 (see Lemma 1), the resultant estimator of the squared l2 distance has a Chi-squared distribution, which is a special case of gamma. [sent-556, score-0.351]
</p><p>65 For each k, we simulate standard Cauchy samples, from which we estimate the Cauchy parameter by the MLE dˆMLE,c and compute the tail probabilities. [sent-566, score-0.267]
</p><p>66 Panel (a) compares the empirical tail probabilities (thick solid) with the gamma tail probabilities (thin solid), indicating that the gamma distribution is better than the normal (dashed) for approximating the distribution of dˆMLE,c . [sent-567, score-0.877]
</p><p>67 Panel (b) compares the empirical tail probabilities with the gamma upper bound (14)+(15). [sent-568, score-0.42]
</p><p>68 , 3 k + k2 k  (17)  In general, a generalized gamma distribution does not have a closed-form density function although it always has a closed-from moment generating function. [sent-575, score-0.218]
</p><p>69 the higher order term, k3 , of the true asymptotic fourth moment of d Assuming dˆMLE,c ∼ IG(α, β), the tail probability of dˆMLE,c can be expressed as ∼ Pr dˆMLE,c ≥ (1 + ε)d = Φ −ε ∼ Pr dˆMLE,c ≤ (1 − ε)d = Φ −ε  α − e2α Φ −(2 + ε) 1+ε α + e2α Φ −(2 − ε) 1−ε  α , ε>0 1+ε α , 0 < ε < 1. [sent-583, score-0.441]
</p><p>70 When the tail probability ≥ 10−4 ∼ 10−6 , we can treat the inverse Gaussian as the exact distribution of dˆMLE,c . [sent-598, score-0.315]
</p><p>71 The Chernoff upper bounds for the inverse Gaussian are always reliable in our simulation range (the tail probability ≥ 10−10 ). [sent-599, score-0.358]
</p><p>72 Panel (a) compares the empirical tail probabilities with the inverse Gaussian tail probabilities, indicating that the approximation is highly accurate. [sent-609, score-0.605]
</p><p>73 Panel (b) compares the empirical tail probabilities with the inverse Gaussian upper bound (18)+(19). [sent-610, score-0.345]
</p><p>74 In this study, we analyze three types of 2519  L I , H ASTIE AND C HURCH  nonlinear estimators for Cauchy random projections: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators. [sent-615, score-0.489]
</p><p>75 The sample median estimators and the geometric mean estimators are asymptotically equivalent but the latter are more accurate at small sample size. [sent-617, score-0.649]
</p><p>76 We have derived explicit tail bounds for the geometric mean estimators in exponential forms. [sent-618, score-0.578]
</p><p>77 Using these tail bounds, we have established an analog of the Johnson-Lindenstrauss (JL) Lemma for dimension reduction in l 1 , which is weaker than the classical JL Lemma for dimension reduction in l2 . [sent-619, score-0.443]
</p><p>78 We conduct theoretic analysis on the maximum likelihood estimators (MLE), which are “asymptotically optimum. [sent-620, score-0.214]
</p><p>79 ” Both the sample median and geometric mean estimators are about 80% efﬁcient as the MLE. [sent-621, score-0.389]
</p><p>80 We propose approximating the distribution of the MLE by an inverse Gaussian, which has the same support and matches the leading terms of the ﬁrst four moments of the MLE. [sent-622, score-0.224]
</p><p>81 Approximate tail bounds have been provided based on the inverse Gaussian approximation. [sent-623, score-0.358]
</p><p>82 Veriﬁed by simulations, these approximate tail bounds hold at least in the ≥ 10 −10 tail probability range. [sent-624, score-0.602]
</p><p>83 Li (2008) generalized the geometric mean estimators to the stable distribution family, for dimension reduction in the l p norm (0 < p ≤ 2). [sent-626, score-0.424]
</p><p>84 Li (2008) also proposed the harmonic mean estimator for p → 0+, which is far more accurate than the geometric mean estimator. [sent-627, score-0.232]
</p><p>85 The estimator, dˆgm,c , expressed as π 2k  dˆgm,c = cosk  k  ∏ |x j |1/k , j=1  is unbiased, because, from Lemma 5, E dˆgm,c = cosk  π 2k  k  ∏E j=1  |x j |1/k = cosk  π 2k  k  ∏  j=1  d 1/k π cos 2k  = d. [sent-648, score-1.069]
</p><p>86 By the Markov moment bound, for any ε > 0 and 0 < t < k, Pr dˆgm ≥ (1 + ε)d ≤  t E dˆgm = ((1 + ε)d)t cosk  2521  1 πt 2k  (1 + ε)t  ,  L I , H ASTIE AND C HURCH  2 whose minimum is attained at t = k π tan−1  Pr dˆgm ≥ (1 + ε)d ≤ exp −k log cos tan−1  2 π  log(1 + ε) . [sent-657, score-0.686]
</p><p>87 Thus  2 log(1 + ε) π 2  1 2 = exp −k − log 1 + log(1 + ε) 2 π = exp −k  ε2 GR,gm  + +  2 −1 tan π  2 −1 tan π  2 log(1 + ε) log(1 + ε) π  2 log(1 + ε) log(1 + ε) π  ,  where GR,gm =  ε2 1 − 2 log 1 +  2 π  2  log(1 + ε)  2 + π tan−1  2 π  log(1 + ε) log(1 + ε)  . [sent-658, score-1.1]
</p><p>88 Again, by the Markov moment bound, for any 0 < ε < 1, Pr dˆgm ≤ (1 − ε)d = Pr  1 1 ≥ ˆgm (1 − ε)d d  2 whose minimum is attained at t = −k π tan−1  Pr dˆgm ≤ (1 − ε)d ≤ exp −k log cos tan−1  = exp −k  ε2 GL,gm  −t E dˆgm (1 − ε)t = πt , ((1 − ε)d)−t cosk 2k  log(1 − ε) . [sent-659, score-0.74]
</p><p>89 Thus  2 π  2 log(1 − ε) π  1 2 = exp −k − log 1 + log(1 − ε) 2 π  ≤  +  2  +  2 −1 tan π  2 −1 tan π  2 log(1 − ε) log(1 − ε) π  2 log(1 − ε) log(1 − ε) π  ,  where GL,gm =  ε2 1 − 2 log 1 +  2 π  log(1 − ε)  2  2 + π tan−1  2 π  log(1 − ε) log(1 − ε)  . [sent-660, score-1.046]
</p><p>90 2 Now we show the other tail bound Pr dˆgm,c ≤ (1 − ε)d . [sent-744, score-0.297]
</p><p>91 8k 8ε Now we prove the asymptotic (as k → ∞) expressions for the ratios of tail bounds, another way to compare dˆgm and dˆgm,c . [sent-751, score-0.399]
</p><p>92 ∗ Note that, in the asymptotic decomposition, t1 ∼ kA1 + C1 , the term kA1 is the optimal “t” in proving the right tail bound for dˆgm . [sent-754, score-0.35]
</p><p>93 Thus to study the asymptotic ratio of the right tail bounds, we only need to keep track of the additional terms in ∗  coskt1 ∗ πt1 2k  cosk  π 2k  (1 + ε)t1  ∗  . [sent-755, score-0.645]
</p><p>94 L I , H ASTIE AND C HURCH  ∗ Again, in the above asymptotic decomposition, t1 ∼ kA2 − C2 , the term kA2 is the optimal “t” in proving the left tail bound for dˆgm . [sent-759, score-0.35]
</p><p>95 Thus to study the asymptotic ratio of the left tail bounds, we only need to keep track of the additional terms in ∗  (1 − ε)t2 1 . [sent-760, score-0.32]
</p><p>96 bias, we recommend the bias-corrected estimator 1 , dˆMLE,c = dˆMLE 1 − k  whose ﬁrst four moments are, after some algebra, 1 k2 1 2d 2 3d 2 + 2 +O 3 Var dˆMLE,c = k k k 3 12d 1 3 E dˆMLE,c − E dˆMLE,c = 2 +O 3 k k 4 1 186d 4 12d 4 E dˆMLE,c − E dˆMLE,c +O 4 = 2 + 3 k k k E dˆMLE,c = d + O  . [sent-783, score-0.26]
</p><p>97 Very sparse stable random projections for dimension reduction in l α (0 < α ≤ 2) norm. [sent-968, score-0.3]
</p><p>98 Estimators and tail bounds for dimension reduction in l α (0 < α ≤ 2) using stable random projections. [sent-971, score-0.471]
</p><p>99 Nonlinear estimators and tail bounds for dimensional reduction in l1 using Cauchy random projections. [sent-1004, score-0.546]
</p><p>100 The moment bound is tighter than Chernoff’s bound for positive tail probabilities. [sent-1015, score-0.422]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('tan', 0.378), ('mle', 0.376), ('cosk', 0.325), ('tail', 0.267), ('cauchy', 0.212), ('hurch', 0.172), ('estimators', 0.172), ('auchy', 0.163), ('moments', 0.15), ('astie', 0.146), ('bme', 0.146), ('gm', 0.139), ('rojections', 0.138), ('pr', 0.124), ('gamma', 0.123), ('log', 0.118), ('projections', 0.114), ('andom', 0.105), ('median', 0.098), ('ping', 0.097), ('moment', 0.095), ('mse', 0.094), ('cos', 0.094), ('indyk', 0.089), ('estimator', 0.087), ('li', 0.083), ('jl', 0.08), ('var', 0.074), ('lemma', 0.073), ('geometric', 0.07), ('piotr', 0.065), ('stable', 0.059), ('kenneth', 0.058), ('cormode', 0.057), ('impossibility', 0.057), ('exp', 0.054), ('asymptotic', 0.053), ('ratios', 0.052), ('dt', 0.052), ('stream', 0.049), ('normal', 0.048), ('brinkman', 0.048), ('ggm', 0.048), ('inverse', 0.048), ('unbiased', 0.048), ('chernoff', 0.047), ('trevor', 0.047), ('ig', 0.046), ('distances', 0.044), ('bounds', 0.043), ('likelihood', 0.042), ('ri', 0.042), ('asymptotically', 0.042), ('church', 0.041), ('arriaga', 0.038), ('edgeworth', 0.038), ('shenton', 0.038), ('dimension', 0.038), ('correction', 0.037), ('streams', 0.036), ('reduction', 0.036), ('bias', 0.033), ('focs', 0.033), ('gradshteyn', 0.032), ('johnson', 0.03), ('bound', 0.03), ('nonlinear', 0.03), ('bowman', 0.029), ('charikar', 0.029), ('datar', 0.029), ('fama', 0.029), ('gg', 0.029), ('haas', 0.029), ('maurice', 0.029), ('mayur', 0.029), ('ryzhik', 0.029), ('zolotarev', 0.029), ('simulations', 0.028), ('analog', 0.028), ('random', 0.028), ('gaussian', 0.028), ('expressions', 0.027), ('mean', 0.026), ('approximating', 0.026), ('fourth', 0.026), ('hastie', 0.025), ('sparse', 0.025), ('approximate', 0.025), ('biometrika', 0.025), ('naor', 0.024), ('projected', 0.024), ('taylor', 0.024), ('recommend', 0.023), ('microsoft', 0.023), ('expansions', 0.023), ('norm', 0.023), ('accurate', 0.023), ('sample', 0.023), ('indicating', 0.023), ('panel', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="60-tfidf-1" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>Author: Ping Li, Trevor J. Hastie, Kenneth W. Church</p><p>Abstract: For1 dimension reduction in the l1 norm, the method of Cauchy random projections multiplies the original data matrix A ∈ Rn×D with a random matrix R ∈ RD×k (k D) whose entries are i.i.d. samples of the standard Cauchy C(0, 1). Because of the impossibility result, one can not hope to recover the pairwise l1 distances in A from B = A × R ∈ Rn×k , using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O log n sufﬁces with the constants explicitly ε2 given. Asymptotically (as k → ∞), both the sample median and the geometric mean estimators are about 80% efﬁcient compared to the MLE. We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian. Keywords: dimension reduction, l1 norm, Johnson-Lindenstrauss (JL) lemma, Cauchy random projections</p><p>2 0.13924295 <a title="60-tfidf-2" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>Author: Art B. Owen</p><p>Abstract: In binary classiﬁcation problems it is common for the two classes to be imbalanced: one case is very rare compared to the other. In this paper we consider the inﬁnitely imbalanced case where one class has a ﬁnite sample size and the other class’s sample size grows without bound. For logistic regression, the inﬁnitely imbalanced case often has a useful solution. Under mild conditions, the intercept diverges as expected, but the rest of the coefﬁcient vector approaches a non trivial and useful limit. That limit can be expressed in terms of exponential tilting and is the minimum of a convex objective function. The limiting form of logistic regression suggests a computational shortcut for fraud detection problems. Keywords: classiﬁcation, drug discovery, fraud detection, rare events, unbalanced data</p><p>3 0.09437985 <a title="60-tfidf-3" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>Author: Philippe Rigollet</p><p>Abstract: We consider semi-supervised classiﬁcation when part of the available data is unlabeled. These unlabeled data can be useful for the classiﬁcation problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples. Keywords: semi-supervised learning, statistical learning theory, classiﬁcation, cluster assumption, generalization bounds</p><p>4 0.046508003 <a title="60-tfidf-4" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>Author: Dima Kuzmin, Manfred K. Warmuth</p><p>Abstract: n Maximum concept classes of VC dimension d over n domain points have size ≤d , and this is an upper bound on the size of any concept class of VC dimension d over n points. We give a compression scheme for any maximum class that represents each concept by a subset of up to d unlabeled domain points and has the property that for any sample of a concept in the class, the representative of exactly one of the concepts consistent with the sample is a subset of the domain of the sample. This allows us to compress any sample of a concept in the class to a subset of up to d unlabeled sample points such that this subset represents a concept consistent with the entire original sample. Unlike the previously known compression scheme for maximum classes (Floyd and Warmuth, 1995) which compresses to labeled subsets of the sample of size equal d, our new scheme is tight in the sense that the number of possible unlabeled compression sets of size at most d equals the number of concepts in the class. Keywords: compression schemes, VC dimension, maximum classes, one-inclusion graph</p><p>5 0.043855857 <a title="60-tfidf-5" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>Author: Jean-Yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, ﬁrst, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester (1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand (see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds. Keywords: statistical learning theory, PAC-Bayes theorems, generalization error bounds</p><p>6 0.040649764 <a title="60-tfidf-6" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>7 0.039819825 <a title="60-tfidf-7" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>8 0.038967248 <a title="60-tfidf-8" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>9 0.03623813 <a title="60-tfidf-9" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>10 0.035001263 <a title="60-tfidf-10" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>11 0.032487333 <a title="60-tfidf-11" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>12 0.031939592 <a title="60-tfidf-12" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>13 0.031449534 <a title="60-tfidf-13" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>14 0.031341933 <a title="60-tfidf-14" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>15 0.031159583 <a title="60-tfidf-15" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>16 0.030965835 <a title="60-tfidf-16" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>17 0.030551942 <a title="60-tfidf-17" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>18 0.030356176 <a title="60-tfidf-18" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>19 0.029831132 <a title="60-tfidf-19" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>20 0.028709352 <a title="60-tfidf-20" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.183), (1, -0.005), (2, 0.031), (3, 0.039), (4, 0.048), (5, -0.064), (6, 0.039), (7, 0.004), (8, 0.115), (9, -0.105), (10, -0.071), (11, -0.093), (12, -0.016), (13, -0.116), (14, 0.354), (15, 0.231), (16, 0.301), (17, 0.054), (18, 0.001), (19, -0.028), (20, -0.056), (21, 0.171), (22, -0.081), (23, -0.062), (24, -0.123), (25, 0.163), (26, 0.073), (27, -0.018), (28, -0.183), (29, -0.025), (30, -0.076), (31, -0.109), (32, 0.023), (33, 0.154), (34, 0.028), (35, 0.067), (36, 0.029), (37, 0.065), (38, -0.034), (39, 0.005), (40, -0.041), (41, -0.006), (42, -0.044), (43, 0.032), (44, 0.069), (45, 0.204), (46, 0.042), (47, 0.141), (48, -0.049), (49, -0.077)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96543843 <a title="60-lsi-1" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>Author: Ping Li, Trevor J. Hastie, Kenneth W. Church</p><p>Abstract: For1 dimension reduction in the l1 norm, the method of Cauchy random projections multiplies the original data matrix A ∈ Rn×D with a random matrix R ∈ RD×k (k D) whose entries are i.i.d. samples of the standard Cauchy C(0, 1). Because of the impossibility result, one can not hope to recover the pairwise l1 distances in A from B = A × R ∈ Rn×k , using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O log n sufﬁces with the constants explicitly ε2 given. Asymptotically (as k → ∞), both the sample median and the geometric mean estimators are about 80% efﬁcient compared to the MLE. We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian. Keywords: dimension reduction, l1 norm, Johnson-Lindenstrauss (JL) lemma, Cauchy random projections</p><p>2 0.62434059 <a title="60-lsi-2" href="./jmlr-2007-Infinitely_Imbalanced_Logistic_Regression.html">42 jmlr-2007-Infinitely Imbalanced Logistic Regression</a></p>
<p>Author: Art B. Owen</p><p>Abstract: In binary classiﬁcation problems it is common for the two classes to be imbalanced: one case is very rare compared to the other. In this paper we consider the inﬁnitely imbalanced case where one class has a ﬁnite sample size and the other class’s sample size grows without bound. For logistic regression, the inﬁnitely imbalanced case often has a useful solution. Under mild conditions, the intercept diverges as expected, but the rest of the coefﬁcient vector approaches a non trivial and useful limit. That limit can be expressed in terms of exponential tilting and is the minimum of a convex objective function. The limiting form of logistic regression suggests a computational shortcut for fraud detection problems. Keywords: classiﬁcation, drug discovery, fraud detection, rare events, unbalanced data</p><p>3 0.46118611 <a title="60-lsi-3" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>Author: Philippe Rigollet</p><p>Abstract: We consider semi-supervised classiﬁcation when part of the available data is unlabeled. These unlabeled data can be useful for the classiﬁcation problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples. Keywords: semi-supervised learning, statistical learning theory, classiﬁcation, cluster assumption, generalization bounds</p><p>4 0.22228284 <a title="60-lsi-4" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>Author: Dima Kuzmin, Manfred K. Warmuth</p><p>Abstract: n Maximum concept classes of VC dimension d over n domain points have size ≤d , and this is an upper bound on the size of any concept class of VC dimension d over n points. We give a compression scheme for any maximum class that represents each concept by a subset of up to d unlabeled domain points and has the property that for any sample of a concept in the class, the representative of exactly one of the concepts consistent with the sample is a subset of the domain of the sample. This allows us to compress any sample of a concept in the class to a subset of up to d unlabeled sample points such that this subset represents a concept consistent with the entire original sample. Unlike the previously known compression scheme for maximum classes (Floyd and Warmuth, 1995) which compresses to labeled subsets of the sample of size equal d, our new scheme is tight in the sense that the number of possible unlabeled compression sets of size at most d equals the number of concepts in the class. Keywords: compression schemes, VC dimension, maximum classes, one-inclusion graph</p><p>5 0.21171208 <a title="60-lsi-5" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>Author: Jean-Yves Audibert, Olivier Bousquet</p><p>Abstract: There exist many different generalization error bounds in statistical learning theory. Each of these bounds contains an improvement over the others for certain situations or algorithms. Our goal is, ﬁrst, to underline the links between these bounds, and second, to combine the different improvements into a single bound. In particular we combine the PAC-Bayes approach introduced by McAllester (1998), which is interesting for randomized predictions, with the optimal union bound provided by the generic chaining technique developed by Fernique and Talagrand (see Talagrand, 1996), in a way that also takes into account the variance of the combined functions. We also show how this connects to Rademacher based bounds. Keywords: statistical learning theory, PAC-Bayes theorems, generalization error bounds</p><p>6 0.19596148 <a title="60-lsi-6" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>7 0.1897151 <a title="60-lsi-7" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>8 0.1840044 <a title="60-lsi-8" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>9 0.17681704 <a title="60-lsi-9" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>10 0.17172989 <a title="60-lsi-10" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>11 0.15821819 <a title="60-lsi-11" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>12 0.15442421 <a title="60-lsi-12" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>13 0.15384239 <a title="60-lsi-13" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>14 0.14877707 <a title="60-lsi-14" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>15 0.14422005 <a title="60-lsi-15" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>16 0.14420758 <a title="60-lsi-16" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>17 0.13130863 <a title="60-lsi-17" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>18 0.12476507 <a title="60-lsi-18" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>19 0.1242055 <a title="60-lsi-19" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>20 0.1215861 <a title="60-lsi-20" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.015), (8, 0.051), (10, 0.025), (12, 0.056), (15, 0.015), (21, 0.438), (28, 0.059), (40, 0.043), (45, 0.012), (48, 0.04), (60, 0.049), (80, 0.01), (85, 0.032), (98, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73007303 <a title="60-lda-1" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>Author: Ping Li, Trevor J. Hastie, Kenneth W. Church</p><p>Abstract: For1 dimension reduction in the l1 norm, the method of Cauchy random projections multiplies the original data matrix A ∈ Rn×D with a random matrix R ∈ RD×k (k D) whose entries are i.i.d. samples of the standard Cauchy C(0, 1). Because of the impossibility result, one can not hope to recover the pairwise l1 distances in A from B = A × R ∈ Rn×k , using linear estimators without incurring large errors. However, nonlinear estimators are still useful for certain applications in data stream computations, information retrieval, learning, and data mining. We study three types of nonlinear estimators: the sample median estimators, the geometric mean estimators, and the maximum likelihood estimators (MLE). We derive tail bounds for the geometric mean estimators and establish that k = O log n sufﬁces with the constants explicitly ε2 given. Asymptotically (as k → ∞), both the sample median and the geometric mean estimators are about 80% efﬁcient compared to the MLE. We analyze the moments of the MLE and propose approximating its distribution of by an inverse Gaussian. Keywords: dimension reduction, l1 norm, Johnson-Lindenstrauss (JL) lemma, Cauchy random projections</p><p>2 0.27623671 <a title="60-lda-2" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>3 0.27348402 <a title="60-lda-3" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>4 0.26670218 <a title="60-lda-4" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>Author: Onur C. Hamsici, Aleix M. Martinez</p><p>Abstract: Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere S p−1 . Such representations should then be modeled using spherical distributions. However, the difﬁculty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead—as if the data were represented in R p rather than S p−1 . This opens the question to whether the classiﬁcation results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases (which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to deﬁne optimal classiﬁers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classiﬁers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classiﬁcation of images of objects, gene expression sequences, and text data. Keywords: directional data, spherical distributions, normal distributions, norm normalization, linear and non-linear classiﬁers, computer vision</p><p>5 0.26450896 <a title="60-lda-5" href="./jmlr-2007-Dynamics_and_Generalization_Ability_of_LVQ_Algorithms.html">30 jmlr-2007-Dynamics and Generalization Ability of LVQ Algorithms</a></p>
<p>Author: Michael Biehl, Anarta Ghosh, Barbara Hammer</p><p>Abstract: Learning vector quantization (LVQ) schemes constitute intuitive, powerful classiﬁcation heuristics with numerous successful applications but, so far, limited theoretical background. We study LVQ rigorously within a simplifying model situation: two competing prototypes are trained from a sequence of examples drawn from a mixture of Gaussians. Concepts from statistical physics and the theory of on-line learning allow for an exact description of the training dynamics in highdimensional feature space. The analysis yields typical learning curves, convergence properties, and achievable generalization abilities. This is also possible for heuristic training schemes which do not relate to a cost function. We compare the performance of several algorithms, including Kohonen’s LVQ1 and LVQ+/-, a limiting case of LVQ2.1. The former shows close to optimal performance, while LVQ+/- displays divergent behavior. We investigate how early stopping can overcome this difﬁculty. Furthermore, we study a crisp version of robust soft LVQ, which was recently derived from a statistical formulation. Surprisingly, it exhibits relatively poor generalization. Performance improves if a window for the selection of data is introduced; the resulting algorithm corresponds to cost function based LVQ2. The dependence of these results on the model parameters, for example, prior class probabilities, is investigated systematically, simulations conﬁrm our analytical ﬁndings. Keywords: prototype based classiﬁcation, learning vector quantization, Winner-Takes-All algorithms, on-line learning, competitive learning</p><p>6 0.26443937 <a title="60-lda-6" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>7 0.26440871 <a title="60-lda-7" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>8 0.26387468 <a title="60-lda-8" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>9 0.26122454 <a title="60-lda-9" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>10 0.26083627 <a title="60-lda-10" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>11 0.26023132 <a title="60-lda-11" href="./jmlr-2007-Refinable_Kernels.html">71 jmlr-2007-Refinable Kernels</a></p>
<p>12 0.26020342 <a title="60-lda-12" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>13 0.25912055 <a title="60-lda-13" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>14 0.25873142 <a title="60-lda-14" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>15 0.25867191 <a title="60-lda-15" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>16 0.25772935 <a title="60-lda-16" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>17 0.25767449 <a title="60-lda-17" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>18 0.257227 <a title="60-lda-18" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>19 0.25650832 <a title="60-lda-19" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>20 0.25625786 <a title="60-lda-20" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
