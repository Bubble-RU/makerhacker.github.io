<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>87 jmlr-2007-Undercomplete Blind Subspace Deconvolution</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-87" href="#">jmlr2007-87</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>87 jmlr-2007-Undercomplete Blind Subspace Deconvolution</h1>
<br/><p>Source: <a title="jmlr-2007-87-pdf" href="http://jmlr.org/papers/volume8/szabo07a/szabo07a.pdf">pdf</a></p><p>Author: ZoltĂĄn SzabĂł, BarnabĂĄs PĂłczos, AndrĂĄs LĹ&lsquo;rincz</p><p>Abstract: We introduce the blind subspace deconvolution (BSSD) problem, which is the extension of both the blind source deconvolution (BSD) and the independent subspace analysis (ISA) tasks. We examine the case of the undercomplete BSSD (uBSSD). Applying temporal concatenation we reduce this problem to ISA. The associated Ă˘&euro;&tilde;high dimensionalĂ˘&euro;&trade; ISA problem can be handled by a recent technique called joint f-decorrelation (JFD). Similar decorrelation methods have been used previously for kernel independent component analysis (kernel-ICA). More precisely, the kernel canonical correlation (KCCA) technique is a member of this family, and, as is shown in this paper, the kernel generalized variance (KGV) method can also be seen as a decorrelation method in the feature space. These kernel based algorithms will be adapted to the ISA task. In the numerical examples, we (i) examine how efÄ?Ĺš ciently the emerging higher dimensional ISA tasks can be tackled, and (ii) explore the working and advantages of the derived kernel-ISA methods. Keywords: undercomplete blind subspace deconvolution, independent subspace analysis, joint decorrelation, kernel methods</p><p>Reference: <a title="jmlr-2007-87-reference" href="../jmlr2007_reference/jmlr-2007-Undercomplete_Blind_Subspace_Deconvolution_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('is', 0.701), ('jfd', 0.296), ('kgv', 0.218), ('kcca', 0.213), ('ubssd', 0.179), ('ym', 0.135), ('abc', 0.134), ('oczo', 0.112), ('ds', 0.105), ('sweep', 0.101), ('rincz', 0.095), ('ic', 0.091), ('sm', 0.09), ('econvolv', 0.089), ('ndercomplet', 0.089), ('orincz', 0.089), ('szab', 0.089), ('ubspac', 0.089), ('zab', 0.089), ('dis', 0.089)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="87-tfidf-1" href="./jmlr-2007-Undercomplete_Blind_Subspace_Deconvolution.html">87 jmlr-2007-Undercomplete Blind Subspace Deconvolution</a></p>
<p>Author: ZoltĂĄn SzabĂł, BarnabĂĄs PĂłczos, AndrĂĄs LĹ&lsquo;rincz</p><p>Abstract: We introduce the blind subspace deconvolution (BSSD) problem, which is the extension of both the blind source deconvolution (BSD) and the independent subspace analysis (ISA) tasks. We examine the case of the undercomplete BSSD (uBSSD). Applying temporal concatenation we reduce this problem to ISA. The associated Ă˘&euro;&tilde;high dimensionalĂ˘&euro;&trade; ISA problem can be handled by a recent technique called joint f-decorrelation (JFD). Similar decorrelation methods have been used previously for kernel independent component analysis (kernel-ICA). More precisely, the kernel canonical correlation (KCCA) technique is a member of this family, and, as is shown in this paper, the kernel generalized variance (KGV) method can also be seen as a decorrelation method in the feature space. These kernel based algorithms will be adapted to the ISA task. In the numerical examples, we (i) examine how efÄ?Ĺš ciently the emerging higher dimensional ISA tasks can be tackled, and (ii) explore the working and advantages of the derived kernel-ISA methods. Keywords: undercomplete blind subspace deconvolution, independent subspace analysis, joint decorrelation, kernel methods</p><p>2 0.052795552 <a title="87-tfidf-2" href="./jmlr-2007-Truncating_the_Loop_Series_Expansion_for_Belief_Propagation.html">86 jmlr-2007-Truncating the Loop Series Expansion for Belief Propagation</a></p>
<p>Author: Vicenç Gómez, Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: Recently, Chertkov and Chernyak (2006b) derived an exact expression for the partition sum (normalization constant) corresponding to a graphical model, which is an expansion around the belief propagation (BP) solution. By adding correction terms to the BP free energy, one for each “generalized loop” in the factor graph, the exact partition sum is obtained. However, the usually enormous number of generalized loops generally prohibits summation over all correction terms. In this article we introduce truncated loop series BP (TLSBP), a particular way of truncating the loop series of Chertkov & Chernyak by considering generalized loops as compositions of simple loops. We analyze the performance of TLSBP in different scenarios, including the Ising model on square grids and regular random graphs, and on PROMEDAS, a large probabilistic medical diagnostic system. We show that TLSBP often improves upon the accuracy of the BP solution, at the expense of increased computation time. We also show that the performance of TLSBP strongly depends on the degree of interaction between the variables. For weak interactions, truncating the series leads to signiﬁcant improvements, whereas for strong interactions it can be ineffective, even if a high number of terms is considered. Keywords: belief propagation, loop calculus, approximate inference, partition function, Ising grid, random regular graphs, medical diagnosis</p><p>3 0.047953337 <a title="87-tfidf-3" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>4 0.040783186 <a title="87-tfidf-4" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>Author: Philippe Rigollet</p><p>Abstract: We consider semi-supervised classiﬁcation when part of the available data is unlabeled. These unlabeled data can be useful for the classiﬁcation problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples. Keywords: semi-supervised learning, statistical learning theory, classiﬁcation, cluster assumption, generalization bounds</p><p>5 0.038572051 <a title="87-tfidf-5" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>Author: Mads Dyrholm, Christoforos Christoforou, Lucas C. Parra</p><p>Abstract: Factor analysis and discriminant analysis are often used as complementary approaches to identify linear components in two dimensional data arrays. For three dimensional arrays, which may organize data in dimensions such as space, time, and trials, the opportunity arises to combine these two approaches. A new method, Bilinear Discriminant Component Analysis (BDCA), is derived and demonstrated in the context of functional brain imaging data for which it seems ideally suited. The work suggests to identify a subspace projection which optimally separates classes while ensuring that each dimension in this space captures an independent contribution to the discrimination. Keywords: bilinear, decomposition, component, classiﬁcation, regularization</p><p>6 0.031289957 <a title="87-tfidf-6" href="./jmlr-2007-Very_Fast_Online_Learning_of_Highly_Non_Linear_Problems.html">91 jmlr-2007-Very Fast Online Learning of Highly Non Linear Problems</a></p>
<p>7 0.030633871 <a title="87-tfidf-7" href="./jmlr-2007-Multi-Task_Learning_for_Classification_with_Dirichlet_Process_Priors.html">56 jmlr-2007-Multi-Task Learning for Classification with Dirichlet Process Priors</a></p>
<p>8 0.02969189 <a title="87-tfidf-8" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>9 0.028428555 <a title="87-tfidf-9" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>10 0.026428256 <a title="87-tfidf-10" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>11 0.024736188 <a title="87-tfidf-11" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>12 0.024139065 <a title="87-tfidf-12" href="./jmlr-2007-The_Need_for_Open_Source_Software_in_Machine_Learning.html">82 jmlr-2007-The Need for Open Source Software in Machine Learning</a></p>
<p>13 0.024064349 <a title="87-tfidf-13" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>14 0.023795804 <a title="87-tfidf-14" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>15 0.022018818 <a title="87-tfidf-15" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>16 0.02197472 <a title="87-tfidf-16" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>17 0.021720618 <a title="87-tfidf-17" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>18 0.021562206 <a title="87-tfidf-18" href="./jmlr-2007-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">80 jmlr-2007-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>19 0.021065872 <a title="87-tfidf-19" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>20 0.019254798 <a title="87-tfidf-20" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.112), (1, 0.021), (2, 0.039), (3, -0.029), (4, -0.019), (5, 0.051), (6, 0.025), (7, -0.006), (8, 0.006), (9, 0.001), (10, -0.066), (11, 0.045), (12, 0.078), (13, -0.118), (14, 0.057), (15, 0.001), (16, 0.128), (17, 0.092), (18, -0.024), (19, 0.123), (20, 0.058), (21, -0.179), (22, 0.18), (23, 0.069), (24, 0.127), (25, 0.024), (26, -0.044), (27, 0.208), (28, 0.116), (29, -0.167), (30, -0.104), (31, 0.022), (32, 0.083), (33, -0.101), (34, -0.175), (35, 0.056), (36, 0.405), (37, 0.125), (38, -0.147), (39, -0.046), (40, -0.02), (41, -0.334), (42, 0.088), (43, 0.301), (44, -0.238), (45, -0.166), (46, 0.014), (47, 0.097), (48, 0.094), (49, -0.047)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96365666 <a title="87-lsi-1" href="./jmlr-2007-Undercomplete_Blind_Subspace_Deconvolution.html">87 jmlr-2007-Undercomplete Blind Subspace Deconvolution</a></p>
<p>Author: ZoltĂĄn SzabĂł, BarnabĂĄs PĂłczos, AndrĂĄs LĹ&lsquo;rincz</p><p>Abstract: We introduce the blind subspace deconvolution (BSSD) problem, which is the extension of both the blind source deconvolution (BSD) and the independent subspace analysis (ISA) tasks. We examine the case of the undercomplete BSSD (uBSSD). Applying temporal concatenation we reduce this problem to ISA. The associated Ă˘&euro;&tilde;high dimensionalĂ˘&euro;&trade; ISA problem can be handled by a recent technique called joint f-decorrelation (JFD). Similar decorrelation methods have been used previously for kernel independent component analysis (kernel-ICA). More precisely, the kernel canonical correlation (KCCA) technique is a member of this family, and, as is shown in this paper, the kernel generalized variance (KGV) method can also be seen as a decorrelation method in the feature space. These kernel based algorithms will be adapted to the ISA task. In the numerical examples, we (i) examine how efÄ?Ĺš ciently the emerging higher dimensional ISA tasks can be tackled, and (ii) explore the working and advantages of the derived kernel-ISA methods. Keywords: undercomplete blind subspace deconvolution, independent subspace analysis, joint decorrelation, kernel methods</p><p>2 0.20595396 <a title="87-lsi-2" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>Author: Philippe Rigollet</p><p>Abstract: We consider semi-supervised classiﬁcation when part of the available data is unlabeled. These unlabeled data can be useful for the classiﬁcation problem when we make an assumption relating the behavior of the regression function to that of the marginal distribution. Seeger (2000) proposed the well-known cluster assumption as a reasonable one. We propose a mathematical formulation of this assumption and a method based on density level sets estimation that takes advantage of it to achieve fast rates of convergence both in the number of unlabeled examples and the number of labeled examples. Keywords: semi-supervised learning, statistical learning theory, classiﬁcation, cluster assumption, generalization bounds</p><p>3 0.20211603 <a title="87-lsi-3" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>Author: Mads Dyrholm, Christoforos Christoforou, Lucas C. Parra</p><p>Abstract: Factor analysis and discriminant analysis are often used as complementary approaches to identify linear components in two dimensional data arrays. For three dimensional arrays, which may organize data in dimensions such as space, time, and trials, the opportunity arises to combine these two approaches. A new method, Bilinear Discriminant Component Analysis (BDCA), is derived and demonstrated in the context of functional brain imaging data for which it seems ideally suited. The work suggests to identify a subspace projection which optimally separates classes while ensuring that each dimension in this space captures an independent contribution to the discrimination. Keywords: bilinear, decomposition, component, classiﬁcation, regularization</p><p>4 0.17323089 <a title="87-lsi-4" href="./jmlr-2007-Multi-Task_Learning_for_Classification_with_Dirichlet_Process_Priors.html">56 jmlr-2007-Multi-Task Learning for Classification with Dirichlet Process Priors</a></p>
<p>Author: Ya Xue, Xuejun Liao, Lawrence Carin, Balaji Krishnapuram</p><p>Abstract: Consider the problem of learning logistic-regression models for multiple classiﬁcation tasks, where the training data set for each task is not drawn from the same statistical distribution. In such a multi-task learning (MTL) scenario, it is necessary to identify groups of similar tasks that should be learned jointly. Relying on a Dirichlet process (DP) based statistical model to learn the extent of similarity between classiﬁcation tasks, we develop computationally efﬁcient algorithms for two different forms of the MTL problem. First, we consider a symmetric multi-task learning (SMTL) situation in which classiﬁers for multiple tasks are learned jointly using a variational Bayesian (VB) algorithm. Second, we consider an asymmetric multi-task learning (AMTL) formulation in which the posterior density function from the SMTL model parameters (from previous tasks) is used as a prior for a new task: this approach has the signiﬁcant advantage of not requiring storage and use of all previous data from prior tasks. The AMTL formulation is solved with a simple Markov Chain Monte Carlo (MCMC) construction. Experimental results on two real life MTL problems indicate that the proposed algorithms: (a) automatically identify subgroups of related tasks whose training data appear to be drawn from similar distributions; and (b) are more accurate than simpler approaches such as single-task learning, pooling of data across all tasks, and simpliﬁed approximations to DP. Keywords: classiﬁcation, hierarchical Bayesian models, Dirichlet process</p><p>5 0.14936994 <a title="87-lsi-5" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>6 0.1337536 <a title="87-lsi-6" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>7 0.11258166 <a title="87-lsi-7" href="./jmlr-2007-Truncating_the_Loop_Series_Expansion_for_Belief_Propagation.html">86 jmlr-2007-Truncating the Loop Series Expansion for Belief Propagation</a></p>
<p>8 0.10814759 <a title="87-lsi-8" href="./jmlr-2007-Statistical_Consistency_of_Kernel_Canonical_Correlation_Analysis.html">78 jmlr-2007-Statistical Consistency of Kernel Canonical Correlation Analysis</a></p>
<p>9 0.091434084 <a title="87-lsi-9" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>10 0.09123119 <a title="87-lsi-10" href="./jmlr-2007-Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning.html">85 jmlr-2007-Transfer Learning via Inter-Task Mappings for Temporal Difference Learning</a></p>
<p>11 0.088718638 <a title="87-lsi-11" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>12 0.086379394 <a title="87-lsi-12" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>13 0.085180476 <a title="87-lsi-13" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<p>14 0.082975283 <a title="87-lsi-14" href="./jmlr-2007-The_Need_for_Open_Source_Software_in_Machine_Learning.html">82 jmlr-2007-The Need for Open Source Software in Machine Learning</a></p>
<p>15 0.077416025 <a title="87-lsi-15" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>16 0.073372453 <a title="87-lsi-16" href="./jmlr-2007-A_Complete_Characterization_of_a_Family_of_Solutions_to_a_Generalized_Fisher_Criterion.html">2 jmlr-2007-A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion</a></p>
<p>17 0.072092853 <a title="87-lsi-17" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>18 0.071441121 <a title="87-lsi-18" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>19 0.069721192 <a title="87-lsi-19" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>20 0.068244122 <a title="87-lsi-20" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.517), (19, 0.017), (21, 0.029), (23, 0.042), (25, 0.025), (31, 0.026), (32, 0.035), (33, 0.02), (69, 0.042), (70, 0.04), (82, 0.013), (94, 0.047), (95, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73335707 <a title="87-lda-1" href="./jmlr-2007-Undercomplete_Blind_Subspace_Deconvolution.html">87 jmlr-2007-Undercomplete Blind Subspace Deconvolution</a></p>
<p>Author: ZoltĂĄn SzabĂł, BarnabĂĄs PĂłczos, AndrĂĄs LĹ&lsquo;rincz</p><p>Abstract: We introduce the blind subspace deconvolution (BSSD) problem, which is the extension of both the blind source deconvolution (BSD) and the independent subspace analysis (ISA) tasks. We examine the case of the undercomplete BSSD (uBSSD). Applying temporal concatenation we reduce this problem to ISA. The associated Ă˘&euro;&tilde;high dimensionalĂ˘&euro;&trade; ISA problem can be handled by a recent technique called joint f-decorrelation (JFD). Similar decorrelation methods have been used previously for kernel independent component analysis (kernel-ICA). More precisely, the kernel canonical correlation (KCCA) technique is a member of this family, and, as is shown in this paper, the kernel generalized variance (KGV) method can also be seen as a decorrelation method in the feature space. These kernel based algorithms will be adapted to the ISA task. In the numerical examples, we (i) examine how efÄ?Ĺš ciently the emerging higher dimensional ISA tasks can be tackled, and (ii) explore the working and advantages of the derived kernel-ISA methods. Keywords: undercomplete blind subspace deconvolution, independent subspace analysis, joint decorrelation, kernel methods</p><p>2 0.49031004 <a title="87-lda-2" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>Author: Stéphan Clémençon, Nicolas Vayatis</p><p>Abstract: We formulate a local form of the bipartite ranking problem where the goal is to focus on the best instances. We propose a methodology based on the construction of real-valued scoring functions. We study empirical risk minimization of dedicated statistics which involve empirical quantiles of the scores. We ﬁrst state the problem of ﬁnding the best instances which can be cast as a classiﬁcation problem with mass constraint. Next, we develop special performance measures for the local ranking problem which extend the Area Under an ROC Curve (AUC) criterion and describe the optimal elements of these new criteria. We also highlight the fact that the goal of ranking the best instances cannot be achieved in a stage-wise manner where ﬁrst, the best instances would be tentatively identiﬁed and then a standard AUC criterion could be applied. Eventually, we state preliminary statistical results for the local ranking problem. Keywords: ranking, ROC curve and AUC, empirical risk minimization, fast rates</p><p>3 0.20038733 <a title="87-lda-3" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>4 0.19019248 <a title="87-lda-4" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>Author: Ofer Dekel, Philip M. Long, Yoram Singer</p><p>Abstract: We study the problem of learning multiple tasks in parallel within the online learning framework. On each online round, the algorithm receives an instance for each of the parallel tasks and responds by predicting the label of each instance. We consider the case where the predictions made on each round all contribute toward a common goal. The relationship between the various tasks is deﬁned by a global loss function, which evaluates the overall quality of the multiple predictions made on each round. Speciﬁcally, each individual prediction is associated with its own loss value, and then these multiple loss values are combined into a single number using the global loss function. We focus on the case where the global loss function belongs to the family of absolute norms, and present several online learning algorithms for the induced problem. We prove worst-case relative loss bounds for all of our algorithms, and demonstrate the effectiveness of our approach on a largescale multiclass-multilabel text categorization problem. Keywords: online learning, multitask learning, multiclass multilabel classiifcation, perceptron</p><p>5 0.18914278 <a title="87-lda-5" href="./jmlr-2007-Truncating_the_Loop_Series_Expansion_for_Belief_Propagation.html">86 jmlr-2007-Truncating the Loop Series Expansion for Belief Propagation</a></p>
<p>Author: Vicenç Gómez, Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: Recently, Chertkov and Chernyak (2006b) derived an exact expression for the partition sum (normalization constant) corresponding to a graphical model, which is an expansion around the belief propagation (BP) solution. By adding correction terms to the BP free energy, one for each “generalized loop” in the factor graph, the exact partition sum is obtained. However, the usually enormous number of generalized loops generally prohibits summation over all correction terms. In this article we introduce truncated loop series BP (TLSBP), a particular way of truncating the loop series of Chertkov & Chernyak by considering generalized loops as compositions of simple loops. We analyze the performance of TLSBP in different scenarios, including the Ising model on square grids and regular random graphs, and on PROMEDAS, a large probabilistic medical diagnostic system. We show that TLSBP often improves upon the accuracy of the BP solution, at the expense of increased computation time. We also show that the performance of TLSBP strongly depends on the degree of interaction between the variables. For weak interactions, truncating the series leads to signiﬁcant improvements, whereas for strong interactions it can be ineffective, even if a high number of terms is considered. Keywords: belief propagation, loop calculus, approximate inference, partition function, Ising grid, random regular graphs, medical diagnosis</p><p>6 0.18807524 <a title="87-lda-6" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>7 0.18700199 <a title="87-lda-7" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>8 0.18628345 <a title="87-lda-8" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>9 0.18477446 <a title="87-lda-9" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>10 0.18391563 <a title="87-lda-10" href="./jmlr-2007-Concave_Learners_for_Rankboost.html">23 jmlr-2007-Concave Learners for Rankboost</a></p>
<p>11 0.18360481 <a title="87-lda-11" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>12 0.18204795 <a title="87-lda-12" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>13 0.18171674 <a title="87-lda-13" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>14 0.18066439 <a title="87-lda-14" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>15 0.18046373 <a title="87-lda-15" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>16 0.18026604 <a title="87-lda-16" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>17 0.17987046 <a title="87-lda-17" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>18 0.17939508 <a title="87-lda-18" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>19 0.17890443 <a title="87-lda-19" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>20 0.17863972 <a title="87-lda-20" href="./jmlr-2007-Distances_between_Data_Sets_Based_on_Summary_Statistics.html">27 jmlr-2007-Distances between Data Sets Based on Summary Statistics</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
