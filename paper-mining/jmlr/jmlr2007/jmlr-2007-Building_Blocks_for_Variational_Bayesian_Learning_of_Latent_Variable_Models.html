<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-17" href="#">jmlr2007-17</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</h1>
<br/><p>Source: <a title="jmlr-2007-17-pdf" href="http://jmlr.org/papers/volume8/raiko07a/raiko07a.pdf">pdf</a></p><p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>Reference: <a title="jmlr-2007-17-reference" href="../jmlr2007_reference/jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('valpol', 0.527), ('nod', 0.29), ('honkel', 0.229), ('aiko', 0.183), ('arhun', 0.183), ('arv', 0.183), ('hnfa', 0.176), ('sourc', 0.173), ('postery', 0.163), ('nonlinear', 0.149), ('uild', 0.149), ('lock', 0.122), ('var', 0.118), ('raiko', 0.115), ('vopt', 0.115), ('sub', 0.109), ('lappalain', 0.107), ('block', 0.104), ('bayes', 0.102), ('nfa', 0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000007 <a title="17-tfidf-1" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>2 0.10695633 <a title="17-tfidf-2" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>Author: Marlon Núñez, Raúl Fidalgo, Rafael Morales</p><p>Abstract: In the process of concept learning, target concepts may have portions with short-term changes, other portions may support long-term changes, and yet others may not change at all. For this reason several local windows need to be handled. We suggest facing this problem, which naturally exists in the ﬁeld of concept learning, by allocating windows which can adapt their size to portions of the target concept. We propose an incremental decision tree that is updated with incoming examples. Each leaf of the decision tree holds a time window and a local performance measure as the main parameter to be controlled. When the performance of a leaf decreases, the size of its local window is reduced. This learning algorithm, called OnlineTree2, automatically adjusts its internal parameters in order to face the current dynamics of the data stream. Results show that it is comparable to other batch algorithms when facing problems with no concept change, and it is better than evaluated methods in its ability to deal with concept drift when dealing with problems in which: concept change occurs at different speeds, noise may be present and, examples may arrive from different areas of the problem domain (virtual drift). Keywords: incremental algorithms, online learning, concept drift, decision trees, robust learners</p><p>3 0.092325956 <a title="17-tfidf-3" href="./jmlr-2007-The_Need_for_Open_Source_Software_in_Machine_Learning.html">82 jmlr-2007-The Need for Open Source Software in Machine Learning</a></p>
<p>Author: Sören Sonnenburg, Mikio L. Braun, Cheng Soon Ong, Samy Bengio, Leon Bottou, Geoffrey Holmes, Yann LeCun, Klaus-Robert Müller, Fernando Pereira, Carl Edward Rasmussen, Gunnar Rätsch, Bernhard Schölkopf, Alexander Smola, Pascal Vincent, Jason Weston, Robert Williamson</p><p>Abstract: Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the ﬁeld of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be signiﬁcantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientiﬁc community. Keywords: machine learning, open source, reproducibility, creditability, algorithms, software 2444 M ACHINE L EARNING O PEN S OURCE S OFTWARE</p><p>4 0.084682539 <a title="17-tfidf-4" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>Author: Sanjoy Dasgupta, Leonard Schulman</p><p>Abstract: We show that, given data from a mixture of k well-separated spherical Gaussians in R d , a simple two-round variant of EM will, with high probability, learn the parameters of the Gaussians to nearoptimal precision, if the dimension is high (d ln k). We relate this to previous theoretical and empirical work on the EM algorithm. Keywords: expectation maximization, mixtures of Gaussians, clustering, unsupervised learning, probabilistic analysis</p><p>5 0.081159346 <a title="17-tfidf-5" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>Author: Sofus A. Macskassy, Foster Provost</p><p>Abstract: paper1 This is about classifying entities that are interlinked with entities for which the class is known. After surveying prior work, we present NetKit, a modular toolkit for classiﬁcation in networked data, and a case-study of its application to networked data used in prior machine learning research. NetKit is based on a node-centric framework in which classiﬁers comprise a local classiﬁer, a relational classiﬁer, and a collective inference procedure. Various existing node-centric relational learning algorithms can be instantiated with appropriate choices for these components, and new combinations of components realize new algorithms. The case study focuses on univariate network classiﬁcation, for which the only information used is the structure of class linkage in the network (i.e., only links and some class labels). To our knowledge, no work previously has evaluated systematically the power of class-linkage alone for classiﬁcation in machine learning benchmark data sets. The results demonstrate that very simple network-classiﬁcation models perform quite well—well enough that they should be used regularly as baseline classiﬁers for studies of learning with networked data. The simplest method (which performs remarkably well) highlights the close correspondence between several existing methods introduced for different purposes—that is, Gaussian-ﬁeld classiﬁers, Hopﬁeld networks, and relational-neighbor classiﬁers. The case study also shows that there are two sets of techniques that are preferable in different situations, namely when few versus many labels are known initially. We also demonstrate that link selection plays an important role similar to traditional feature selection. Keywords: relational learning, network learning, collective inference, collective classiﬁcation, networked data, probabilistic relational models, network analysis, network data</p><p>6 0.077322111 <a title="17-tfidf-6" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>7 0.065702714 <a title="17-tfidf-7" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>8 0.06415271 <a title="17-tfidf-8" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>9 0.063643649 <a title="17-tfidf-9" href="./jmlr-2007-Very_Fast_Online_Learning_of_Highly_Non_Linear_Problems.html">91 jmlr-2007-Very Fast Online Learning of Highly Non Linear Problems</a></p>
<p>10 0.062054198 <a title="17-tfidf-10" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>11 0.0514176 <a title="17-tfidf-11" href="./jmlr-2007-Multi-Task_Learning_for_Classification_with_Dirichlet_Process_Priors.html">56 jmlr-2007-Multi-Task Learning for Classification with Dirichlet Process Priors</a></p>
<p>12 0.04940277 <a title="17-tfidf-12" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>13 0.047953337 <a title="17-tfidf-13" href="./jmlr-2007-Undercomplete_Blind_Subspace_Deconvolution.html">87 jmlr-2007-Undercomplete Blind Subspace Deconvolution</a></p>
<p>14 0.047260381 <a title="17-tfidf-14" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>15 0.047220036 <a title="17-tfidf-15" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>16 0.046910137 <a title="17-tfidf-16" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>17 0.0437878 <a title="17-tfidf-17" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>18 0.042695519 <a title="17-tfidf-18" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>19 0.041601293 <a title="17-tfidf-19" href="./jmlr-2007-Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning.html">85 jmlr-2007-Transfer Learning via Inter-Task Mappings for Temporal Difference Learning</a></p>
<p>20 0.040493067 <a title="17-tfidf-20" href="./jmlr-2007-Harnessing_the_Expertise_of_70%2C000_Human_Editors%3A_Knowledge-Based_Feature_Generation_for_Text_Categorization.html">40 jmlr-2007-Harnessing the Expertise of 70,000 Human Editors: Knowledge-Based Feature Generation for Text Categorization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.237), (1, 0.14), (2, -0.044), (3, -0.009), (4, 0.043), (5, 0.014), (6, 0.187), (7, 0.114), (8, 0.177), (9, 0.014), (10, -0.093), (11, 0.141), (12, 0.058), (13, -0.049), (14, 0.067), (15, -0.065), (16, 0.08), (17, 0.211), (18, -0.006), (19, 0.169), (20, 0.096), (21, 0.027), (22, 0.2), (23, -0.009), (24, -0.064), (25, 0.278), (26, 0.064), (27, 0.174), (28, 0.055), (29, 0.078), (30, 0.018), (31, -0.011), (32, -0.062), (33, -0.08), (34, -0.012), (35, -0.04), (36, 0.032), (37, -0.095), (38, 0.107), (39, 0.072), (40, -0.023), (41, 0.068), (42, 0.088), (43, -0.046), (44, 0.043), (45, -0.014), (46, -0.021), (47, -0.03), (48, 0.005), (49, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93809837 <a title="17-lsi-1" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>2 0.56905806 <a title="17-lsi-2" href="./jmlr-2007-The_Need_for_Open_Source_Software_in_Machine_Learning.html">82 jmlr-2007-The Need for Open Source Software in Machine Learning</a></p>
<p>Author: Sören Sonnenburg, Mikio L. Braun, Cheng Soon Ong, Samy Bengio, Leon Bottou, Geoffrey Holmes, Yann LeCun, Klaus-Robert Müller, Fernando Pereira, Carl Edward Rasmussen, Gunnar Rätsch, Bernhard Schölkopf, Alexander Smola, Pascal Vincent, Jason Weston, Robert Williamson</p><p>Abstract: Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the ﬁeld of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be signiﬁcantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientiﬁc community. Keywords: machine learning, open source, reproducibility, creditability, algorithms, software 2444 M ACHINE L EARNING O PEN S OURCE S OFTWARE</p><p>3 0.42182779 <a title="17-lsi-3" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>Author: Marlon Núñez, Raúl Fidalgo, Rafael Morales</p><p>Abstract: In the process of concept learning, target concepts may have portions with short-term changes, other portions may support long-term changes, and yet others may not change at all. For this reason several local windows need to be handled. We suggest facing this problem, which naturally exists in the ﬁeld of concept learning, by allocating windows which can adapt their size to portions of the target concept. We propose an incremental decision tree that is updated with incoming examples. Each leaf of the decision tree holds a time window and a local performance measure as the main parameter to be controlled. When the performance of a leaf decreases, the size of its local window is reduced. This learning algorithm, called OnlineTree2, automatically adjusts its internal parameters in order to face the current dynamics of the data stream. Results show that it is comparable to other batch algorithms when facing problems with no concept change, and it is better than evaluated methods in its ability to deal with concept drift when dealing with problems in which: concept change occurs at different speeds, noise may be present and, examples may arrive from different areas of the problem domain (virtual drift). Keywords: incremental algorithms, online learning, concept drift, decision trees, robust learners</p><p>4 0.38004071 <a title="17-lsi-4" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>Author: Sofus A. Macskassy, Foster Provost</p><p>Abstract: paper1 This is about classifying entities that are interlinked with entities for which the class is known. After surveying prior work, we present NetKit, a modular toolkit for classiﬁcation in networked data, and a case-study of its application to networked data used in prior machine learning research. NetKit is based on a node-centric framework in which classiﬁers comprise a local classiﬁer, a relational classiﬁer, and a collective inference procedure. Various existing node-centric relational learning algorithms can be instantiated with appropriate choices for these components, and new combinations of components realize new algorithms. The case study focuses on univariate network classiﬁcation, for which the only information used is the structure of class linkage in the network (i.e., only links and some class labels). To our knowledge, no work previously has evaluated systematically the power of class-linkage alone for classiﬁcation in machine learning benchmark data sets. The results demonstrate that very simple network-classiﬁcation models perform quite well—well enough that they should be used regularly as baseline classiﬁers for studies of learning with networked data. The simplest method (which performs remarkably well) highlights the close correspondence between several existing methods introduced for different purposes—that is, Gaussian-ﬁeld classiﬁers, Hopﬁeld networks, and relational-neighbor classiﬁers. The case study also shows that there are two sets of techniques that are preferable in different situations, namely when few versus many labels are known initially. We also demonstrate that link selection plays an important role similar to traditional feature selection. Keywords: relational learning, network learning, collective inference, collective classiﬁcation, networked data, probabilistic relational models, network analysis, network data</p><p>5 0.34631369 <a title="17-lsi-5" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>Author: Sanjoy Dasgupta, Leonard Schulman</p><p>Abstract: We show that, given data from a mixture of k well-separated spherical Gaussians in R d , a simple two-round variant of EM will, with high probability, learn the parameters of the Gaussians to nearoptimal precision, if the dimension is high (d ln k). We relate this to previous theoretical and empirical work on the EM algorithm. Keywords: expectation maximization, mixtures of Gaussians, clustering, unsupervised learning, probabilistic analysis</p><p>6 0.34619814 <a title="17-lsi-6" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>7 0.34133825 <a title="17-lsi-7" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>8 0.32614878 <a title="17-lsi-8" href="./jmlr-2007-Transfer_Learning_via_Inter-Task_Mappings_for_Temporal_Difference_Learning.html">85 jmlr-2007-Transfer Learning via Inter-Task Mappings for Temporal Difference Learning</a></p>
<p>9 0.31960565 <a title="17-lsi-9" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>10 0.30817574 <a title="17-lsi-10" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>11 0.28337592 <a title="17-lsi-11" href="./jmlr-2007-Very_Fast_Online_Learning_of_Highly_Non_Linear_Problems.html">91 jmlr-2007-Very Fast Online Learning of Highly Non Linear Problems</a></p>
<p>12 0.27442589 <a title="17-lsi-12" href="./jmlr-2007-Multi-Task_Learning_for_Classification_with_Dirichlet_Process_Priors.html">56 jmlr-2007-Multi-Task Learning for Classification with Dirichlet Process Priors</a></p>
<p>13 0.27300972 <a title="17-lsi-13" href="./jmlr-2007-Undercomplete_Blind_Subspace_Deconvolution.html">87 jmlr-2007-Undercomplete Blind Subspace Deconvolution</a></p>
<p>14 0.26994398 <a title="17-lsi-14" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>15 0.2650421 <a title="17-lsi-15" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>16 0.26175541 <a title="17-lsi-16" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>17 0.25326028 <a title="17-lsi-17" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>18 0.24402086 <a title="17-lsi-18" href="./jmlr-2007-Anytime_Learning_of_Decision_Trees.html">11 jmlr-2007-Anytime Learning of Decision Trees</a></p>
<p>19 0.24302016 <a title="17-lsi-19" href="./jmlr-2007-Margin_Trees_for_High-dimensional_Classification.html">52 jmlr-2007-Margin Trees for High-dimensional Classification</a></p>
<p>20 0.23477329 <a title="17-lsi-20" href="./jmlr-2007-Dynamics_and_Generalization_Ability_of_LVQ_Algorithms.html">30 jmlr-2007-Dynamics and Generalization Ability of LVQ Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(17, 0.016), (19, 0.026), (21, 0.043), (23, 0.081), (25, 0.387), (31, 0.018), (32, 0.051), (33, 0.012), (69, 0.055), (70, 0.07), (75, 0.01), (82, 0.021), (94, 0.054), (95, 0.062)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.677706 <a title="17-lda-1" href="./jmlr-2007-Truncating_the_Loop_Series_Expansion_for_Belief_Propagation.html">86 jmlr-2007-Truncating the Loop Series Expansion for Belief Propagation</a></p>
<p>Author: Vicenç Gómez, Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: Recently, Chertkov and Chernyak (2006b) derived an exact expression for the partition sum (normalization constant) corresponding to a graphical model, which is an expansion around the belief propagation (BP) solution. By adding correction terms to the BP free energy, one for each “generalized loop” in the factor graph, the exact partition sum is obtained. However, the usually enormous number of generalized loops generally prohibits summation over all correction terms. In this article we introduce truncated loop series BP (TLSBP), a particular way of truncating the loop series of Chertkov & Chernyak by considering generalized loops as compositions of simple loops. We analyze the performance of TLSBP in different scenarios, including the Ising model on square grids and regular random graphs, and on PROMEDAS, a large probabilistic medical diagnostic system. We show that TLSBP often improves upon the accuracy of the BP solution, at the expense of increased computation time. We also show that the performance of TLSBP strongly depends on the degree of interaction between the variables. For weak interactions, truncating the series leads to signiﬁcant improvements, whereas for strong interactions it can be ineffective, even if a high number of terms is considered. Keywords: belief propagation, loop calculus, approximate inference, partition function, Ising grid, random regular graphs, medical diagnosis</p><p>same-paper 2 0.64625072 <a title="17-lda-2" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>Author: Tapani Raiko, Harri Valpola, Markus Harva, Juha Karhunen</p><p>Abstract: We introduce standardised building blocks designed to be used with variational Bayesian learning. The blocks include Gaussian variables, summation, multiplication, nonlinearity, and delay. A large variety of latent variable models can be constructed from these blocks, including nonlinear and variance models, which are lacking from most existing variational systems. The introduced blocks are designed to ﬁt together and to yield efﬁcient update rules. Practical implementation of various models is easy thanks to an associated software package which derives the learning formulas automatically once a speciﬁc model structure has been ﬁxed. Variational Bayesian learning provides a cost function which is used both for updating the variables of the model and for optimising the model structure. All the computations can be carried out locally, resulting in linear computational complexity. We present experimental results on several structures, including a new hierarchical nonlinear model for variances and means. The test results demonstrate the good performance and usefulness of the introduced method. Keywords: latent variable models, variational Bayesian learning, graphical models, building blocks, Bayesian modelling, local computation</p><p>3 0.39267299 <a title="17-lda-3" href="./jmlr-2007-Loop_Corrections_for_Approximate_Inference_on_Factor_Graphs.html">51 jmlr-2007-Loop Corrections for Approximate Inference on Factor Graphs</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a method to improve approximate inference methods by correcting for the inﬂuence of loops in the graphical model. The method is a generalization and alternative implementation of a recent idea from Montanari and Rizzo (2005). It is applicable to arbitrary factor graphs, provided that the size of the Markov blankets is not too large. It consists of two steps: (i) an approximate inference method, for example, belief propagation, is used to approximate cavity distributions for each variable (i.e., probability distributions on the Markov blanket of a variable for a modiﬁed graphical model in which the factors involving that variable have been removed); (ii) all cavity distributions are improved by a message-passing algorithm that cancels out approximation errors by imposing certain consistency constraints. This loop correction (LC) method usually gives signiﬁcantly better results than the original, uncorrected, approximate inference algorithm that is used to estimate the effect of loops. Indeed, we often observe that the loop-corrected error is approximately the square of the error of the uncorrected approximate inference method. In this article, we compare different variants of the loop correction method with other approximate inference methods on a variety of graphical models, including “real world” networks, and conclude that the LC method generally obtains the most accurate results. Keywords: loop corrections, approximate inference, graphical models, factor graphs, belief propagation</p><p>4 0.361936 <a title="17-lda-4" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>Author: Ofer Dekel, Philip M. Long, Yoram Singer</p><p>Abstract: We study the problem of learning multiple tasks in parallel within the online learning framework. On each online round, the algorithm receives an instance for each of the parallel tasks and responds by predicting the label of each instance. We consider the case where the predictions made on each round all contribute toward a common goal. The relationship between the various tasks is deﬁned by a global loss function, which evaluates the overall quality of the multiple predictions made on each round. Speciﬁcally, each individual prediction is associated with its own loss value, and then these multiple loss values are combined into a single number using the global loss function. We focus on the case where the global loss function belongs to the family of absolute norms, and present several online learning algorithms for the induced problem. We prove worst-case relative loss bounds for all of our algorithms, and demonstrate the effectiveness of our approach on a largescale multiclass-multilabel text categorization problem. Keywords: online learning, multitask learning, multiclass multilabel classiifcation, perceptron</p><p>5 0.35765556 <a title="17-lda-5" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>6 0.35724816 <a title="17-lda-6" href="./jmlr-2007-Concave_Learners_for_Rankboost.html">23 jmlr-2007-Concave Learners for Rankboost</a></p>
<p>7 0.35209623 <a title="17-lda-7" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>8 0.35069886 <a title="17-lda-8" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>9 0.35045558 <a title="17-lda-9" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>10 0.34936616 <a title="17-lda-10" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>11 0.34894246 <a title="17-lda-11" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>12 0.34747049 <a title="17-lda-12" href="./jmlr-2007-Estimating_High-Dimensional_Directed_Acyclic_Graphs_with_the_PC-Algorithm.html">31 jmlr-2007-Estimating High-Dimensional Directed Acyclic Graphs with the PC-Algorithm</a></p>
<p>13 0.34735718 <a title="17-lda-13" href="./jmlr-2007-Stagewise_Lasso.html">77 jmlr-2007-Stagewise Lasso</a></p>
<p>14 0.34722981 <a title="17-lda-14" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>15 0.3461917 <a title="17-lda-15" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>16 0.34488371 <a title="17-lda-16" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>17 0.34457842 <a title="17-lda-17" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>18 0.34432334 <a title="17-lda-18" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>19 0.34417987 <a title="17-lda-19" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>20 0.34319526 <a title="17-lda-20" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
