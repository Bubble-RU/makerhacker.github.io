<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-76" href="#">jmlr2007-76</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</h1>
<br/><p>Source: <a title="jmlr-2007-76-pdf" href="http://jmlr.org/papers/volume8/hamsici07a/hamsici07a.pdf">pdf</a></p><p>Author: Onur C. Hamsici, Aleix M. Martinez</p><p>Abstract: Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere S p−1 . Such representations should then be modeled using spherical distributions. However, the difﬁculty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead—as if the data were represented in R p rather than S p−1 . This opens the question to whether the classiﬁcation results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases (which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to deﬁne optimal classiﬁers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classiﬁers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classiﬁcation of images of objects, gene expression sequences, and text data. Keywords: directional data, spherical distributions, normal distributions, norm normalization, linear and non-linear classiﬁers, computer vision</p><p>Reference: <a title="jmlr-2007-76-reference" href="../jmlr2007_reference/jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, the difﬁculty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead—as if the data were represented in R p rather than S p−1 . [sent-11, score-0.464]
</p><p>2 Another area where spherical representations are common is in computer vision, where spherical representations emerge after the common normc 2007 Onur C. [sent-29, score-0.394]
</p><p>3 The question arises: how accurate are the classiﬁcation results obtained when approximating spherical distributions with Gaussian distributions? [sent-70, score-0.267]
</p><p>4 Section 2 presents several of the commonly used spherical distributions and describes some of the difﬁculties associated to their parameter estimation. [sent-81, score-0.267]
</p><p>5 In Section 3, we introduce the concept of spherical-homoscedasticity and show that whenever two spherical distributions comply with this model, the Gaussian approximation works well. [sent-82, score-0.267]
</p><p>6 Spherical Data In this section, we introduce some of the most commonly used spherical distributions and discuss their parameter estimation and associated problems. [sent-90, score-0.267]
</p><p>7 For example, the von Mises-Fisher (vMF) distribution is the spherical counterpart of those Gaussian distributions that can be represented with a covariance matrix of the form τ 2 I; where I is the p × p identity matrix and τ > 0. [sent-94, score-0.45]
</p><p>8 As summarized above, the actual values of the parameters of spherical distributions can rarely be computed, and approximations are needed. [sent-166, score-0.267]
</p><p>9 2 Similarly, we can deﬁne the distance of a test sample to each of the spherical distributions deﬁned above (i. [sent-176, score-0.267]
</p><p>10 The question remains: are the classiﬁcation errors obtained using the spherical distributions deﬁned above lower than those obtained when using the Gaussian approximation? [sent-184, score-0.267]
</p><p>11 For example, if we use Gaussian pdfs to model some spherical data that is ellipsoidally symmetric about its mean, then only those distributions that have the same mean up to a sign can be homoscedastic. [sent-208, score-0.292]
</p><p>12 Theorem 2 Let two Gaussian distributions N1 (m, Σ) and N2 (RT m, RT ΣR) model the spherical data of two classes on S p−1 ; where m is the mean, Σ is the covariance matrix (which is assumed to be full ranked), and R ∈ SO(p) is a rotation matrix. [sent-229, score-0.496]
</p><p>13 Let R be spanned by two of the eigenvectors of Σ, v1 and v2 , and let one of these eigenvectors deﬁne the same direction as m (i. [sent-230, score-0.3]
</p><p>14 1 1  Recall that the rotation is constrained to be in the subspace spanned by two of the eigenvectors of Σ. [sent-249, score-0.31]
</p><p>15 2 2 2 2  In addition, we know that the rotation matrix R deﬁnes a rotation in the (v 1 , v2 )-plane. [sent-260, score-0.338]
</p><p>16 From Figure 3 we see that π θ − , v2 + RT v2 = 2w cos 2 2 π θ − = 2w cos , RT v1 + v1 = 2u cos 2 2  v2 − RT v2 = 2u cos RT v1 − v 1  θ , 2 θ , 2  where u and w are the unit vectors as shown in Figure 3. [sent-263, score-0.258]
</p><p>17 The result above, shows that when the rotation matrix is spanned by two of the eigenvectors of Σ, then N1 and N2 are spherical-homoscedastic. [sent-269, score-0.352]
</p><p>18 (13)  We see that (13) deﬁnes the decision boundary between M 1 (µ, κ) and M2 (RT µ, κ) and that this is a hyperplane4 with normal RT µ − µ , 2 cos (ω/2) where ω is the magnitude of the rotation angle. [sent-278, score-0.241]
</p><p>19 Note that since the variance about every direction orthogonal to µ is equal to τ 2 , all rotations can be expressed as a planar rotation spanned by µ and any µ⊥ (where µ⊥ is a vector orthogonal to µ) . [sent-288, score-0.326]
</p><p>20 Theorem 7 Two Bingham distributions, B1 (A) and B2 (RT AR), are spherical-homoscedastic if R ∈ SO(p) deﬁnes a planar rotation in the subspace spanned by any two of the eigenvectors of A, say q1 and q2 . [sent-323, score-0.371]
</p><p>21 (18)  Since the rotation is deﬁned in the subspace spanned by q 1 and q2 and A = QΛQT , then the p p above equation can be expressed (in open form) as ∑i=1 λi (xT qi )2 = ∑i=1 λi (xT RT qi )2 . [sent-325, score-0.261]
</p><p>22 Since antipodally symmetric distributions, such as Bingham distributions, have zero mean, the Gaussian distributions ﬁtted to the data sampled from these distributions will also have zero mean. [sent-330, score-0.258]
</p><p>23 Lemma 8 Two zero-mean Gaussian distributions, N1 (0, Σ) and N2 (0, RT ΣR), are sphericalhomoscedastic if R ∈ SO(p) deﬁnes a planar rotation in the subspace spanned by any two of the eigenvectors of Σ, say v1 and v2 . [sent-332, score-0.417]
</p><p>24 Since the eigenvectors of A and S are the same (v i = qi for all i), these two Gaussian distributions representing the two spherical-homoscedastic Bingham distributions will also be spherical-homoscedastic. [sent-349, score-0.342]
</p><p>25 Furthermore, the hyperplanes of the spherical-homoscedastic Bingham distributions B1 (A) and B2 (RT AR), Equations (20 - 21), and the hyperplanes of the spherical-homoscedastic Gaussian distributions N1 (0, S) and N2 (0, RT SR), Equations (22 - 23), will be identical. [sent-350, score-0.252]
</p><p>26 Theorem 10 Two Kent distributions K1 (µ, κ, A) and K2 (RT µ, κ, RT AR) are spherical-homoscedastic, if the rotation matrix R is deﬁned on the plane spanned by the mean direction µ and one of the eigenvectors of A. [sent-353, score-0.472]
</p><p>27 Thus far, we have shown where the results obtained by modeling the true underlying spherical distributions with Gaussians do not pose a problem. [sent-374, score-0.289]
</p><p>28 Following the classical notation in Bayesian theory, we will refer to this as the reducible error. [sent-380, score-0.416]
</p><p>29 And, in Figure 4(c), we illustrate the reducible error added to the original Bayes error when one employs the new classiﬁer in lieu of the original one. [sent-384, score-0.472]
</p><p>30 In theory, we could calculate the reducible error by means of the posterior probabilities of the two spherical distributions, PS (w1 |x) and PS (w2 |x), and the posteriors of the Gaussians modeling them, PG (w1 |x) and PG (w2 |x). [sent-385, score-0.636]
</p><p>31 Note that, since we are exclusively interested in knowing how the reducible error increases as the data distributions deviate from spherical-homoscedasticity, the use of error bounds would not help us solve this problem either. [sent-388, score-0.636]
</p><p>32 We are therefore left to empirically study how the reducible error increases as the original data distributions deviate from spherical-homoscedastic. [sent-389, score-0.608]
</p><p>33 The dashed area shown in (c) corresponds to the error added to the original Bayes error; that is, the reducible error. [sent-429, score-0.444]
</p><p>34 1 Modeling Spherical-Heteroscedastic vMFs We start our analysis with the case where the two original spherical distributions are given by the vMF model. [sent-431, score-0.267]
</p><p>35 Deﬁning the parameters of the Gaussian distributions in terms of κi and µi (i = {1, 2}) allows us to estimate the reducible error with respect to different rotation angles θ, concentration parameters κ1 and κ2 , and dimensionality p. [sent-436, score-0.778]
</p><p>36 1600  S PHERICAL -H OMOSCEDASTIC D ISTRIBUTIONS  Since our goal is to test how the reducible error increases as the original data distributions deviate from spherical-homoscedasticity, we will plot the results for distinct values of κ 2 /κ1 . [sent-437, score-0.608]
</p><p>37 The average of the reducible error over all possible values of κ 1 and over all values of θ from 10o to 90o is shown in Figure 5(a). [sent-444, score-0.444]
</p><p>38 As anticipated by our theory, the reducible error is zero when κ 2 /κ1 = 1 (i. [sent-445, score-0.444]
</p><p>39 We see that as the distributions start to deviate from spherical-homoscedastic, the probability of reducible error increases really fast. [sent-448, score-0.608]
</p><p>40 This means that the area of possible overlap between M1 and M2 decreases and, hence, the reducible error will generally become smaller. [sent-452, score-0.444]
</p><p>41 In summary, the probability of reducible error increases as the data deviates from spherical-homoscedasticity and decreases as the data becomes more concentrated. [sent-453, score-0.478]
</p><p>42 This means that, in general, the more two nonhighly concentrated distributions deviate from spherical-homoscedastic, the more sense it makes to take the extra effort to model the spherical data using one of the spherical models introduced in Section 2. [sent-454, score-0.539]
</p><p>43 Nevertheless, it is important to note that the reducible error remains relatively low (∼ 3%). [sent-455, score-0.444]
</p><p>44 We also observe, in Figure 5(a), that the probability of reducible error decreases with the dimensionality (which would be something unexpected had the original pdf been deﬁned in the Cartesian space). [sent-456, score-0.527]
</p><p>45 Therefore, as the dimensionality increase, the volume of the reducible error area (i. [sent-459, score-0.49]
</p><p>46 In Figure 5(b) we show the average probability of the reducible error over κ 1 and p, for different values of κ2 /κ1 and θ. [sent-462, score-0.444]
</p><p>47 , κ2 /κ1 = 1), the average of the reducible error is zero. [sent-465, score-0.444]
</p><p>48 As we deviate from spherical-homoscedasticity, the probability of reducible error increases. [sent-466, score-0.511]
</p><p>49 Furthermore, it is interesting to note that as θ increases (in the spherical-heteroscedastic case), the probability of reducible error decreases. [sent-467, score-0.444]
</p><p>50 005  5  κ /κ  0 0  2  1  10  0  5  150  θ  0  (a)  10  100  5  50  κ /κ 2  0 0  1  0  (b)  Figure 5: In (a) we show the average of the probability of reducible error over κ 1 and θ = {10o , 20o , . [sent-485, score-0.444]
</p><p>51 In (b) we show the average probability of reducible error over p and κ 1 for different values of κ2 /κ1 and θ = {10o , 20o , . [sent-489, score-0.444]
</p><p>52 Moreover, if we want to calculate the reducible error on S p−1 , we will need to simulate each of the p variance parameters of B1 and B2 and the p − 1 possible rotations between their means; that is, 3p − 1. [sent-495, score-0.491]
</p><p>53 As seen in the ﬁgure, the probability of reducible error increases as the 6. [sent-516, score-0.444]
</p><p>54 02 0  (c) Figure 6: We show the average of the probability of reducible error over all the possible set of variance parameters when ς = 1 in (a) and when ς = 1 in (b). [sent-538, score-0.466]
</p><p>55 In (c) we show the increase of reducible error as the data deviates from spherical-homoscedastic (i. [sent-539, score-0.478]
</p><p>56 The more the data deviates from spherical-homoscedastic, the larger the reducible error is. [sent-542, score-0.478]
</p><p>57 Nonetheless, the probability of reducible error is still very small even for large values of φ and θ—approximately 0. [sent-547, score-0.444]
</p><p>58 In this case, the probability of reducible error is generally expected to be larger. [sent-550, score-0.444]
</p><p>59 This is shown in Figure 6(b) where the probability of reducible error has been averaged over all possible combinations of variance parameters (λ1 , λ2 , λ3 ) and scales ς = 1. [sent-551, score-0.466]
</p><p>60 Here, it is important to note that as the two original distributions get closer to each other the probability of reducible error increases quite rapidly. [sent-552, score-0.541]
</p><p>61 To further illustrate this point, we can plot the probability of reducible error over ς and θ, Figure 6(c). [sent-558, score-0.444]
</p><p>62 Hence, this plot shows the increase in reducible error as the two distributions deviate from spherical-homoscedastic. [sent-561, score-0.608]
</p><p>63 Here, we want to estimate the probability of reducible error when two Gaussian distributions N1 (m1 , Σ1 ) and N2 (m2 , Σ2 ) are used to model the data sampled from two Kent distributions K1 (µ1 , κ1 , A1 ) and K2 (µ2 , κ2 , A2 ). [sent-565, score-0.638]
</p><p>64 Now, however, R1 deﬁnes a planar rotation in the space spanned by µ1 and the ﬁrst eigenvector of A1 , and R2 is a planar rotation deﬁned in the space of µ1 and the second eigenvector of A1 . [sent-589, score-0.607]
</p><p>65 As anticipated in Theorem 11, in these cases the probability of reducible error is zero. [sent-598, score-0.444]
</p><p>66 Then the more these two distributions deviate from sphericalhomoscedastic, the larger the probability of reducible error will become. [sent-599, score-0.608]
</p><p>67 It is worth mentioning, however, that the probability of reducible error is small over all possible values for θ and φ (i. [sent-600, score-0.444]
</p><p>68 The average of the probability of reducible error for all possible values for κ i and βi (including those where κ1 = κ2 and β1 = β2 ) is shown in Figure 7(b). [sent-606, score-0.444]
</p><p>69 In this case, we see that the probability of reducible error is bounded by 0. [sent-607, score-0.444]
</p><p>70 02  100 100  (c)  (d)  Figure 7: In (a) we show the average of the probability of reducible error when κ 1 = κ2 and β1 = β2 and the data is not concentrated. [sent-650, score-0.444]
</p><p>71 (b) Shows the probability of reducible error when the parameters of the pdf are different in each distribution and the values of κ i are small. [sent-651, score-0.481]
</p><p>72 As expected, when the data is more concentrated in a small area, the probability of reducible error decreases fast as the two distributions fall far apart from each other. [sent-657, score-0.576]
</p><p>73 Similarly, since the data is concentrated, the maximum of the probability of reducible error shown in Figure 7(d) is smaller than that observed in (b). [sent-658, score-0.444]
</p><p>74 This will again require the use of spherical distributions or their Gaussian equivalences described in this paper. [sent-686, score-0.267]
</p><p>75 In this new space F , the sample mean direction of class a is given by µφ ˆa  =  =  1 na  a ∑i=1 φ(xi )  n  1 na T 1 na ∑i=1 φ(xi ) na 1 na na ∑i=1 φ(xi ) √ , 1T K1  a ∑ j=1 φ(x j )  n  8. [sent-687, score-0.299]
</p><p>76 By ﬁnding a kernel which transforms the original distributions to spherical-homoscedastic vMF, we can use the classiﬁer deﬁned in (28), which states that the class label of any test feature vector x is  arg max φ(x)T µφ = ˆa a  =  a ∑i=1 φ(x)T φ(xi ) √ 1T K1 1 na na ∑i=1 k(x, xi ) √ . [sent-690, score-0.277]
</p><p>77 We already know that the decision boundary for two spherical-homoscedastic Bingham distributions deﬁned as B 1 (A) and B2 (RT AR), with R representing a planar rotation given by any two eigenvectors of A, is given by the two hyperplane Equations (20) and (21). [sent-693, score-0.504]
</p><p>78 However, even when R2 and R3 are deﬁned by two eigenvectors of the parameter matrix, the rotation between B 2 and B3 may not be planar. [sent-714, score-0.263]
</p><p>79 Nonetheless, we have shown in Section 4 that if the variances of the two distributions are the same up to an arbitrary rotation, the reducible error is negligible. [sent-715, score-0.561]
</p><p>80 This constraint comes from (30), where the eigenvector of the second distribution must be the ﬁrst eigenvector rotated by the rotation matrix relating the two distributions. [sent-718, score-0.356]
</p><p>81 Also, since we know the data is spherical-homoscedastic, the eigenvectors of the correlation matrix will be the same as those of the covariance matrix Σa of the (zero-mean) Gaussian distribution as seen in Theorem 9. [sent-725, score-0.238]
</p><p>82 This assumes, however, that the parameters of the spherical distributions are known. [sent-777, score-0.267]
</p><p>83 Following the notation used above, we are interested in ﬁnding the reducible error that will (on average) be added to the classiﬁcation error when using the Gaussian model. [sent-780, score-0.472]
</p><p>84 Our experiment was repeated 100 times for each of the possible parameters, and the average was computed to obtain the probability of reducible error. [sent-800, score-0.416]
</p><p>85 In (a) the probability of reducible error is shown over the dimensionality of the data p and the scalar γ, which is deﬁned as the number of samples over their dimensionality, γ = n/p. [sent-803, score-0.513]
</p><p>86 As already noticed in Section 4, the probability of reducible error decreases with the dimensionality. [sent-804, score-0.444]
</p><p>87 In those cases, if p is small, the probability of the reducible error decreases. [sent-807, score-0.444]
</p><p>88 When the dimension to sample ratio is adequate, the probability of reducible error decreases as expected. [sent-810, score-0.469]
</p><p>89 02  80  (d)  Figure 8: (a-b) Shown here are the average reducible errors obtained when ﬁtting two Gaussian distributions to the data sampled from two spherical-homoscedastic vMFs with parameters κ = {1, 2, . [sent-842, score-0.513]
</p><p>90 (c) Shows the average reducible error when data sampled from spherical-homoscedastic Bingham distributions is ﬁtted with the Gaussians estimates. [sent-856, score-0.541]
</p><p>91 We also note that in the worse case scenario, the probability of reducible error is very low and shall not have a dramatic effect in the classiﬁcation results when working with vMF. [sent-859, score-0.444]
</p><p>92 8(b), we see that the closer two vMF distributions get, the larger the reducible error can be. [sent-861, score-0.541]
</p><p>93 1611  H AMSICI AND M ARTINEZ  To simulate the probability of reducible error in Bingham and Kent, several of the parameters of the distributions were considered. [sent-866, score-0.541]
</p><p>94 8(c-d) we show the probability of reducible error as a function of γ and the rotation θ. [sent-885, score-0.592]
</p><p>95 To justify this solution, note that the direct minimization of (33) would result in n  arg min ∑ xT xi − 2 i W  i=1  xT WWT xi i + xT xi i W T xi  n  = arg min ∑ 2 − W  i=1 n  2xT WWT xi i W T xi  = arg min ∑ 1 − WT xi W  i=1 n  = arg max ∑ WT xi . [sent-908, score-0.328]
</p><p>96 Once again, we see that the sample-to-dimension ratio is too small to allow good results in the estimate of the parameters of the spherical distributions or the Gaussian ﬁt. [sent-1125, score-0.292]
</p><p>97 We ﬁrst introduced the concept of spherical-homoscedasticity and showed that if two spherical distributions comply with this model, their Gaussian approximations are enough to obtain optimal classiﬁcation performance in the Bayes sense. [sent-1217, score-0.267]
</p><p>98 We have referred to the additional error caused by this approximation as the reducible error. [sent-1222, score-0.444]
</p><p>99 We have then empirically evaluated this and showed that as the two distributions start to deviate from spherical-homoscedastic, the probability of reducible error increases. [sent-1223, score-0.608]
</p><p>100 These classiﬁers are linear, since the Bayes decision boundary for two spherical-homoscedastic distributions in S p−1 is a hyperplane (same as for homoscedastic distributions in R p ). [sent-1227, score-0.316]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reducible', 0.416), ('rt', 0.384), ('bingham', 0.374), ('vmf', 0.308), ('xt', 0.275), ('kent', 0.236), ('spherical', 0.17), ('rotation', 0.148), ('amsici', 0.131), ('artinez', 0.131), ('istributions', 0.131), ('omoscedastic', 0.131), ('eigenvectors', 0.115), ('pherical', 0.111), ('distributions', 0.097), ('rx', 0.074), ('gaussian', 0.074), ('eigenvector', 0.071), ('xa', 0.068), ('deviate', 0.067), ('planar', 0.061), ('von', 0.06), ('cos', 0.058), ('lda', 0.052), ('cb', 0.05), ('hyperplane', 0.048), ('spanned', 0.047), ('na', 0.046), ('sphericalhomoscedastic', 0.046), ('vmfs', 0.046), ('dimensionality', 0.046), ('sphere', 0.044), ('normalizing', 0.044), ('concentration', 0.043), ('bayes', 0.043), ('matrix', 0.042), ('mahalanobis', 0.04), ('antipodally', 0.039), ('cmf', 0.039), ('homoscedastic', 0.039), ('loot', 0.039), ('covariance', 0.039), ('pdf', 0.037), ('boundary', 0.035), ('concentrated', 0.035), ('wwt', 0.035), ('autocorrelation', 0.035), ('gene', 0.034), ('deviates', 0.034), ('gaussians', 0.034), ('kernel', 0.033), ('bessel', 0.033), ('pg', 0.033), ('maglap', 0.033), ('mardia', 0.033), ('subsphere', 0.033), ('qi', 0.033), ('classi', 0.032), ('qt', 0.03), ('hyperplanes', 0.029), ('ers', 0.029), ('arg', 0.028), ('error', 0.028), ('representations', 0.027), ('ps', 0.027), ('wt', 0.027), ('xi', 0.027), ('audit', 0.026), ('cfb', 0.026), ('cot', 0.026), ('kume', 0.026), ('vectors', 0.026), ('eigenvalues', 0.026), ('ax', 0.026), ('ar', 0.026), ('rotations', 0.025), ('ratio', 0.025), ('symmetric', 0.025), ('ln', 0.024), ('object', 0.024), ('rotated', 0.024), ('cmu', 0.024), ('samples', 0.023), ('gauss', 0.023), ('tan', 0.023), ('direction', 0.023), ('genomics', 0.022), ('hypersphere', 0.022), ('pomeroy', 0.022), ('equivalency', 0.022), ('shape', 0.022), ('variance', 0.022), ('modeling', 0.022), ('er', 0.021), ('saddlepoint', 0.02), ('variances', 0.02), ('banerjee', 0.02), ('equations', 0.02), ('aleix', 0.02), ('circularly', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="76-tfidf-1" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>Author: Onur C. Hamsici, Aleix M. Martinez</p><p>Abstract: Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere S p−1 . Such representations should then be modeled using spherical distributions. However, the difﬁculty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead—as if the data were represented in R p rather than S p−1 . This opens the question to whether the classiﬁcation results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases (which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to deﬁne optimal classiﬁers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classiﬁers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classiﬁcation of images of objects, gene expression sequences, and text data. Keywords: directional data, spherical distributions, normal distributions, norm normalization, linear and non-linear classiﬁers, computer vision</p><p>2 0.17103186 <a title="76-tfidf-2" href="./jmlr-2007-A_Complete_Characterization_of_a_Family_of_Solutions_to_a_Generalized_Fisher_Criterion.html">2 jmlr-2007-A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion</a></p>
<p>Author: Marco Loog</p><p>Abstract: Recently, Ye (2005) suggested yet another optimization criterion for discriminant analysis and proposed a characterization of the family of solutions to this objective. The characterization, however, merely describes a part of the full solution set, that is, it is not complete and therefore not at all a characterization. This correspondence ﬁrst gives the correct characterization and afterwards compares it to Ye’s. Keywords: linear discriminant analysis, Fisher criterion, small sample, characterization 1. Classical and New Criteria Given N feature vectors of dimensionality n, a linear reduction of dimensionality, based on classical Fisher LDA, determines an n × d transformation matrix L that, for a given d < K, K the number of classes, maximizes the so-called Fisher criterion: F(A) = tr((A t SW A)−1 (At SB A)) or, equivalently, F0 (A) = tr((At ST A)−1 (At SB A)). Here, SB := ∑K pi (mi − m)(mi − m)t , SW := ∑K pi Si , and i=1 i=1 ST = SB + SW . The matrices SB , SW , and ST are the so-called between-class, pooled within-class, and total covariance matrices. In addition, mi is the mean vector of class i, pi is the prior of class i, and the overall mean m equals ∑k pi mi . Finally, Si is the covariance matrix of class i. i=1 A solution to these optimization problems can be obtained by means of a generalized eigenvalue decomposition, which Fukunaga (1990) relates to a simultaneous diagonalization of the two matrices involved (see also Campbell and Atchley, 1981). More common is it to apply a standard −1 eigenvalue decomposition to S−1 SB (or SW SB ), resulting in an equivalent set of eigenvectors. The d T columns of the optimal solution L are simply taken to equal the d eigenvectors corresponding to the d largest eigenvalues. It is known that this solution is not unique and the full class can be obtained by multiplying L to the right with nonsingular d × d matrices (see Fukunaga, 1990). Clearly, if the total covariance ST is singular, neither the generalized nor the standard eigenvalue decomposition can be readily employed. Directly or indirectly, the problem is that the matrix inverse S−1 does not exist, which is the typical situation when dealing with small samples. In an attempt to T overcome this problem, Ye (2005) introduced a different criterion that is deﬁned as F1 (A) = tr((At ST A)+ (At SB A)) , ∗. Also at Nordic Bioscience Imaging, Hovegade 207, DK-2730 Herlev, Denmark. c 2007 Marco Loog. (1) L OOG where + denotes taking the Moore-Penrose generalized inverse of a matrix. Like for F0 , an optimal transform L is one that maximizes the objective F1 . Again, this solution is not unique. 2. Correct Characterization For the full characterization of the set of solutions to Equation (1), initially the problem is looked at from a geometrical point of view (cf., Campbell and Atchley, 1981). It is assumed that the number of samples N is smaller than or equal to the feature dimensionality n. In the undersampled case, it is clear that all data variation occurs in an N − 1-dimensional subspace of the original space. To start with, a PCA of the data is carried out and the ﬁrst N − 1 principal components are rotated to the ﬁrst N − 1 axes of the n-dimensional space by means of a rotation matrix R. This matrix consists of all (normalized) eigenvectors of ST taken as its columns. After this rotation, new total and between-class covariance matrices, ST = Rt ST R and SB = Rt SB R, are obtained. These 0 0 matrices can be partitioned as follows: ST = Σ0T 0 and SB = ΣB 0 , where ΣT and ΣB are N − 1 × 0 N − 1 covariance matrices and ΣT is nonsingular and diagonal by construction. The between-class variation is obviously restricted to the N − 1-dimensional subspace in which the total data variation takes place, therefore a same partitioning of SB is possible. However, the covariance submatrix ΣB is not necessarily diagonal, neither does it have to be nonsingular. Basically, the PCA-based rotation R converts the initial problem into a more convenient one, splitting up the original space in an N − 1-dimensional one in which “everything interesting” takes place and a remaining n − N + 1dimensional subspace in which “nothing happens at all”. Now consider F1 in this transformed space and take a general n × d transformation matrix A, which is partitioned in a way similar to the covariance matrices, that is, X . Y A= (2) Here, X is an N − 1 × d-matrix and Y is of size n − N + 1 × d. Taking this deﬁnition, the following holds (cf., Ye, 2005): t + t F1 (A) = tr((A ST A) (A SB A)) = tr =tr X t ΣT X 0 0 0 + X Y X t ΣB X 0 0 0 t ΣT 0 = tr 0 0 X Y + (Xt ΣT X)−1 0 0 0 X Y t ΣB 0 0 0 X Y X t ΣB X 0 0 0 = tr((Xt ΣT X)−1 (Xt ΣB X)) = F0 (X) . From this it is immediate that a matrix A maximizes F1 if and only if the submatrix X maximizes the original Fisher criterion in the lower-dimensional subspace. Moreover, if L is such a matrix maximizing F1 in the PCA-transformed space, it is easy to check that R−1 L = Rt L provides a solution to the original, general problem that has not been preprocessed by means of a PCA (see also Fukunaga, 1990). A characterization of the complete family of solutions can now be given. Let Λ ∈ RN−1×d be an optimal solution of F0 (X) = tr((Xt ΣT X)−1 (Xt ΣB X)). As already noted in Section 1, the full set of solutions is given by F = {ΛZ ∈ RN−1×d | Z ∈ GLd (R)}, where GLd (R) denotes the general linear group of d × d invertible matrices. The previous paragraph essentially demonstrates that if X ∈ F , A in Equation (2) maximizes F1 . The matrix Y can be chosen ad 2122 C OMPLETE C HARACTERIZATION OF A FAMILY OF S OLUTIONS libitum. Now, the latter provides the solution in the PCA-transformed space and to solve the general problem we need to take the rotation back to the original space into account. All in all, this leads to the following complete family of solutions L , maximizing F1 in the original space: L = Rt ΛZ ∈ Rn×d Z ∈ GLd (R), Y ∈ Rn−N+1×d Y , (3) where Λ = argmaxX tr((Xt ΣT X)−1 (Xt ΣB X)) and Rt takes care of the rotation back. 3. Original Characterization Though not noted by Ye (2005), his attempt to characterize the full set of solutions of Equation (1) is based on a simultaneous diagonalization of the three covariance matrices S B , SW , and ST that is similar to the ideas described by Campbell and Atchley (1981) and Fukunaga (1990). Moreover, Golub and Van Loan (Theorem 8.7.1. 1996) can be readily applied to demonstrate that such simultaneous diagonalization is possible in the small sample setting. After the diagonalization step, partitioned between-class, pooled within-class, and total covariance matrices are considered. This partitioning is similar to the one employed in the previous section, which does not enforce matrices to be diagonal however. In the subsequent optimization step, the classical Fisher criterion is maximized basically in the appropriate subspace, comparable to the approach described above, but in a mildly more involved and concealed way. For this, matrices of the form Rt X are considered, consider Equations (2) and Y (3). However, Y is simply the null matrix and the family of solutions L provided is limited to L = Rt ΛZ ∈ Rn×d Z ∈ GLd (R) . 0 Obviously, this is far from a complete characterization, especially when N − 1 n which is, for instance, typically the case for the data sets considered by Ye (2005). Generally, the utility of a dimensionality reduction criterion, without additional constrains, depends on the efﬁciency over the full set of solutions. As Ye (2005) only considers two very speciﬁc instances from the large class of possibilities, it is unclear to what extent the new criterion really provides an efﬁcient way of performing a reduction of dimensionality. References N. A. Campbell and W. R. Atchley. The geometry of canonical variate analysis. Systematic Zoology, 30(3):268–280, 1981. K. Fukunaga. Introduction to Statistical Pattern Recognition. Academic Press, New York, 1990. G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, third edition, 1996. J. Ye. Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems. Journal of Machine Learning Research, 6:483–502, 2005. 2123</p><p>3 0.15130723 <a title="76-tfidf-3" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>Author: Ofer Dekel, Philip M. Long, Yoram Singer</p><p>Abstract: We study the problem of learning multiple tasks in parallel within the online learning framework. On each online round, the algorithm receives an instance for each of the parallel tasks and responds by predicting the label of each instance. We consider the case where the predictions made on each round all contribute toward a common goal. The relationship between the various tasks is deﬁned by a global loss function, which evaluates the overall quality of the multiple predictions made on each round. Speciﬁcally, each individual prediction is associated with its own loss value, and then these multiple loss values are combined into a single number using the global loss function. We focus on the case where the global loss function belongs to the family of absolute norms, and present several online learning algorithms for the induced problem. We prove worst-case relative loss bounds for all of our algorithms, and demonstrate the effectiveness of our approach on a largescale multiclass-multilabel text categorization problem. Keywords: online learning, multitask learning, multiclass multilabel classiifcation, perceptron</p><p>4 0.058098771 <a title="76-tfidf-4" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>5 0.049705792 <a title="76-tfidf-5" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A speciﬁc instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a ﬁnal parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A speciﬁc instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystr¨ m extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker o sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are brieﬂy summarized at the end. Keywords: Markov decision processes, reinforcement learning, value function approximation, manifold learning, spectral graph theory</p><p>6 0.048744842 <a title="76-tfidf-6" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>7 0.040897682 <a title="76-tfidf-7" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>8 0.037342452 <a title="76-tfidf-8" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>9 0.035233635 <a title="76-tfidf-9" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>10 0.033775441 <a title="76-tfidf-10" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>11 0.03355848 <a title="76-tfidf-11" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>12 0.031635638 <a title="76-tfidf-12" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>13 0.031510804 <a title="76-tfidf-13" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>14 0.030965835 <a title="76-tfidf-14" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>15 0.030555885 <a title="76-tfidf-15" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>16 0.02988573 <a title="76-tfidf-16" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>17 0.029437702 <a title="76-tfidf-17" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>18 0.029193476 <a title="76-tfidf-18" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>19 0.028843116 <a title="76-tfidf-19" href="./jmlr-2007-Synergistic_Face_Detection_and_Pose_Estimation_with_Energy-Based_Models.html">80 jmlr-2007-Synergistic Face Detection and Pose Estimation with Energy-Based Models</a></p>
<p>20 0.028155779 <a title="76-tfidf-20" href="./jmlr-2007-Very_Fast_Online_Learning_of_Highly_Non_Linear_Problems.html">91 jmlr-2007-Very Fast Online Learning of Highly Non Linear Problems</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.195), (1, 0.022), (2, -0.028), (3, 0.049), (4, -0.271), (5, -0.141), (6, -0.184), (7, -0.049), (8, -0.263), (9, 0.059), (10, 0.005), (11, 0.215), (12, 0.183), (13, -0.181), (14, -0.002), (15, 0.019), (16, 0.127), (17, 0.184), (18, 0.167), (19, 0.012), (20, 0.017), (21, 0.053), (22, -0.069), (23, -0.086), (24, 0.102), (25, 0.154), (26, -0.019), (27, 0.101), (28, -0.03), (29, -0.04), (30, -0.058), (31, -0.024), (32, -0.002), (33, 0.069), (34, -0.011), (35, -0.085), (36, 0.058), (37, 0.17), (38, 0.08), (39, 0.013), (40, -0.049), (41, -0.052), (42, -0.032), (43, 0.007), (44, -0.08), (45, -0.005), (46, -0.098), (47, -0.005), (48, 0.036), (49, -0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94288099 <a title="76-lsi-1" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>Author: Onur C. Hamsici, Aleix M. Martinez</p><p>Abstract: Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere S p−1 . Such representations should then be modeled using spherical distributions. However, the difﬁculty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead—as if the data were represented in R p rather than S p−1 . This opens the question to whether the classiﬁcation results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases (which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to deﬁne optimal classiﬁers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classiﬁers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classiﬁcation of images of objects, gene expression sequences, and text data. Keywords: directional data, spherical distributions, normal distributions, norm normalization, linear and non-linear classiﬁers, computer vision</p><p>2 0.79125321 <a title="76-lsi-2" href="./jmlr-2007-A_Complete_Characterization_of_a_Family_of_Solutions_to_a_Generalized_Fisher_Criterion.html">2 jmlr-2007-A Complete Characterization of a Family of Solutions to a Generalized Fisher Criterion</a></p>
<p>Author: Marco Loog</p><p>Abstract: Recently, Ye (2005) suggested yet another optimization criterion for discriminant analysis and proposed a characterization of the family of solutions to this objective. The characterization, however, merely describes a part of the full solution set, that is, it is not complete and therefore not at all a characterization. This correspondence ﬁrst gives the correct characterization and afterwards compares it to Ye’s. Keywords: linear discriminant analysis, Fisher criterion, small sample, characterization 1. Classical and New Criteria Given N feature vectors of dimensionality n, a linear reduction of dimensionality, based on classical Fisher LDA, determines an n × d transformation matrix L that, for a given d < K, K the number of classes, maximizes the so-called Fisher criterion: F(A) = tr((A t SW A)−1 (At SB A)) or, equivalently, F0 (A) = tr((At ST A)−1 (At SB A)). Here, SB := ∑K pi (mi − m)(mi − m)t , SW := ∑K pi Si , and i=1 i=1 ST = SB + SW . The matrices SB , SW , and ST are the so-called between-class, pooled within-class, and total covariance matrices. In addition, mi is the mean vector of class i, pi is the prior of class i, and the overall mean m equals ∑k pi mi . Finally, Si is the covariance matrix of class i. i=1 A solution to these optimization problems can be obtained by means of a generalized eigenvalue decomposition, which Fukunaga (1990) relates to a simultaneous diagonalization of the two matrices involved (see also Campbell and Atchley, 1981). More common is it to apply a standard −1 eigenvalue decomposition to S−1 SB (or SW SB ), resulting in an equivalent set of eigenvectors. The d T columns of the optimal solution L are simply taken to equal the d eigenvectors corresponding to the d largest eigenvalues. It is known that this solution is not unique and the full class can be obtained by multiplying L to the right with nonsingular d × d matrices (see Fukunaga, 1990). Clearly, if the total covariance ST is singular, neither the generalized nor the standard eigenvalue decomposition can be readily employed. Directly or indirectly, the problem is that the matrix inverse S−1 does not exist, which is the typical situation when dealing with small samples. In an attempt to T overcome this problem, Ye (2005) introduced a different criterion that is deﬁned as F1 (A) = tr((At ST A)+ (At SB A)) , ∗. Also at Nordic Bioscience Imaging, Hovegade 207, DK-2730 Herlev, Denmark. c 2007 Marco Loog. (1) L OOG where + denotes taking the Moore-Penrose generalized inverse of a matrix. Like for F0 , an optimal transform L is one that maximizes the objective F1 . Again, this solution is not unique. 2. Correct Characterization For the full characterization of the set of solutions to Equation (1), initially the problem is looked at from a geometrical point of view (cf., Campbell and Atchley, 1981). It is assumed that the number of samples N is smaller than or equal to the feature dimensionality n. In the undersampled case, it is clear that all data variation occurs in an N − 1-dimensional subspace of the original space. To start with, a PCA of the data is carried out and the ﬁrst N − 1 principal components are rotated to the ﬁrst N − 1 axes of the n-dimensional space by means of a rotation matrix R. This matrix consists of all (normalized) eigenvectors of ST taken as its columns. After this rotation, new total and between-class covariance matrices, ST = Rt ST R and SB = Rt SB R, are obtained. These 0 0 matrices can be partitioned as follows: ST = Σ0T 0 and SB = ΣB 0 , where ΣT and ΣB are N − 1 × 0 N − 1 covariance matrices and ΣT is nonsingular and diagonal by construction. The between-class variation is obviously restricted to the N − 1-dimensional subspace in which the total data variation takes place, therefore a same partitioning of SB is possible. However, the covariance submatrix ΣB is not necessarily diagonal, neither does it have to be nonsingular. Basically, the PCA-based rotation R converts the initial problem into a more convenient one, splitting up the original space in an N − 1-dimensional one in which “everything interesting” takes place and a remaining n − N + 1dimensional subspace in which “nothing happens at all”. Now consider F1 in this transformed space and take a general n × d transformation matrix A, which is partitioned in a way similar to the covariance matrices, that is, X . Y A= (2) Here, X is an N − 1 × d-matrix and Y is of size n − N + 1 × d. Taking this deﬁnition, the following holds (cf., Ye, 2005): t + t F1 (A) = tr((A ST A) (A SB A)) = tr =tr X t ΣT X 0 0 0 + X Y X t ΣB X 0 0 0 t ΣT 0 = tr 0 0 X Y + (Xt ΣT X)−1 0 0 0 X Y t ΣB 0 0 0 X Y X t ΣB X 0 0 0 = tr((Xt ΣT X)−1 (Xt ΣB X)) = F0 (X) . From this it is immediate that a matrix A maximizes F1 if and only if the submatrix X maximizes the original Fisher criterion in the lower-dimensional subspace. Moreover, if L is such a matrix maximizing F1 in the PCA-transformed space, it is easy to check that R−1 L = Rt L provides a solution to the original, general problem that has not been preprocessed by means of a PCA (see also Fukunaga, 1990). A characterization of the complete family of solutions can now be given. Let Λ ∈ RN−1×d be an optimal solution of F0 (X) = tr((Xt ΣT X)−1 (Xt ΣB X)). As already noted in Section 1, the full set of solutions is given by F = {ΛZ ∈ RN−1×d | Z ∈ GLd (R)}, where GLd (R) denotes the general linear group of d × d invertible matrices. The previous paragraph essentially demonstrates that if X ∈ F , A in Equation (2) maximizes F1 . The matrix Y can be chosen ad 2122 C OMPLETE C HARACTERIZATION OF A FAMILY OF S OLUTIONS libitum. Now, the latter provides the solution in the PCA-transformed space and to solve the general problem we need to take the rotation back to the original space into account. All in all, this leads to the following complete family of solutions L , maximizing F1 in the original space: L = Rt ΛZ ∈ Rn×d Z ∈ GLd (R), Y ∈ Rn−N+1×d Y , (3) where Λ = argmaxX tr((Xt ΣT X)−1 (Xt ΣB X)) and Rt takes care of the rotation back. 3. Original Characterization Though not noted by Ye (2005), his attempt to characterize the full set of solutions of Equation (1) is based on a simultaneous diagonalization of the three covariance matrices S B , SW , and ST that is similar to the ideas described by Campbell and Atchley (1981) and Fukunaga (1990). Moreover, Golub and Van Loan (Theorem 8.7.1. 1996) can be readily applied to demonstrate that such simultaneous diagonalization is possible in the small sample setting. After the diagonalization step, partitioned between-class, pooled within-class, and total covariance matrices are considered. This partitioning is similar to the one employed in the previous section, which does not enforce matrices to be diagonal however. In the subsequent optimization step, the classical Fisher criterion is maximized basically in the appropriate subspace, comparable to the approach described above, but in a mildly more involved and concealed way. For this, matrices of the form Rt X are considered, consider Equations (2) and Y (3). However, Y is simply the null matrix and the family of solutions L provided is limited to L = Rt ΛZ ∈ Rn×d Z ∈ GLd (R) . 0 Obviously, this is far from a complete characterization, especially when N − 1 n which is, for instance, typically the case for the data sets considered by Ye (2005). Generally, the utility of a dimensionality reduction criterion, without additional constrains, depends on the efﬁciency over the full set of solutions. As Ye (2005) only considers two very speciﬁc instances from the large class of possibilities, it is unclear to what extent the new criterion really provides an efﬁcient way of performing a reduction of dimensionality. References N. A. Campbell and W. R. Atchley. The geometry of canonical variate analysis. Systematic Zoology, 30(3):268–280, 1981. K. Fukunaga. Introduction to Statistical Pattern Recognition. Academic Press, New York, 1990. G. H. Golub and C. F. Van Loan. Matrix Computations. The Johns Hopkins University Press, third edition, 1996. J. Ye. Characterization of a family of algorithms for generalized discriminant analysis on undersampled problems. Journal of Machine Learning Research, 6:483–502, 2005. 2123</p><p>3 0.61582351 <a title="76-lsi-3" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>Author: Ofer Dekel, Philip M. Long, Yoram Singer</p><p>Abstract: We study the problem of learning multiple tasks in parallel within the online learning framework. On each online round, the algorithm receives an instance for each of the parallel tasks and responds by predicting the label of each instance. We consider the case where the predictions made on each round all contribute toward a common goal. The relationship between the various tasks is deﬁned by a global loss function, which evaluates the overall quality of the multiple predictions made on each round. Speciﬁcally, each individual prediction is associated with its own loss value, and then these multiple loss values are combined into a single number using the global loss function. We focus on the case where the global loss function belongs to the family of absolute norms, and present several online learning algorithms for the induced problem. We prove worst-case relative loss bounds for all of our algorithms, and demonstrate the effectiveness of our approach on a largescale multiclass-multilabel text categorization problem. Keywords: online learning, multitask learning, multiclass multilabel classiifcation, perceptron</p><p>4 0.29691091 <a title="76-lsi-4" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>Author: Marco Reisert, Hans Burkhardt</p><p>Abstract: This paper presents a new class of matrix valued kernels that are ideally suited to learn vector valued equivariant functions. Matrix valued kernels are a natural generalization of the common notion of a kernel. We set the theoretical foundations of so called equivariant matrix valued kernels. We work out several properties of equivariant kernels, we give an interpretation of their behavior and show relations to scalar kernels. The notion of (ir)reducibility of group representations is transferred into the framework of matrix valued kernels. At the end to two exemplary applications are demonstrated. We design a non-linear rotation and translation equivariant ﬁlter for 2D-images and propose an invariant object detector based on the generalized Hough transform. Keywords: kernel methods, matrix kernels, equivariance, group integration, representation theory, Hough transform, signal processing, Volterra series</p><p>5 0.21021535 <a title="76-lsi-5" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>Author: Jaime S. Cardoso, Joaquim F. Pinto da Costa</p><p>Abstract: Classiﬁcation of ordinal data is one of the most important tasks of relation learning. This paper introduces a new machine learning paradigm speciﬁcally intended for classiﬁcation problems where the classes have a natural order. The technique reduces the problem of classifying ordered classes to the standard two-class problem. The introduced method is then mapped into support vector machines and neural networks. Generalization bounds of the proposed ordinal classiﬁer are also provided. An experimental study with artiﬁcial and real data sets, including an application to gene expression analysis, veriﬁes the usefulness of the proposed approach. Keywords: classiﬁcation, ordinal data, support vector machines, neural networks</p><p>6 0.20258793 <a title="76-lsi-6" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>7 0.18515538 <a title="76-lsi-7" href="./jmlr-2007-The_Pyramid_Match_Kernel%3A_Efficient_Learning_with_Sets_of_Features.html">84 jmlr-2007-The Pyramid Match Kernel: Efficient Learning with Sets of Features</a></p>
<p>8 0.18368131 <a title="76-lsi-8" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>9 0.18244465 <a title="76-lsi-9" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>10 0.1638035 <a title="76-lsi-10" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>11 0.15328085 <a title="76-lsi-11" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>12 0.14010075 <a title="76-lsi-12" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>13 0.13953847 <a title="76-lsi-13" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>14 0.13726614 <a title="76-lsi-14" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>15 0.1372398 <a title="76-lsi-15" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>16 0.13713869 <a title="76-lsi-16" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>17 0.13417545 <a title="76-lsi-17" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>18 0.13272941 <a title="76-lsi-18" href="./jmlr-2007-Learnability_of_Gaussians_with_Flexible_Variances.html">45 jmlr-2007-Learnability of Gaussians with Flexible Variances</a></p>
<p>19 0.12829536 <a title="76-lsi-19" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>20 0.12623842 <a title="76-lsi-20" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.066), (8, 0.043), (10, 0.02), (12, 0.041), (15, 0.037), (28, 0.05), (40, 0.032), (48, 0.051), (60, 0.034), (80, 0.011), (82, 0.393), (85, 0.049), (98, 0.068)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.62319833 <a title="76-lda-1" href="./jmlr-2007-Spherical-Homoscedastic_Distributions%3A_The_Equivalency_of_Spherical_and_Normal_Distributions_in_Classification.html">76 jmlr-2007-Spherical-Homoscedastic Distributions: The Equivalency of Spherical and Normal Distributions in Classification</a></p>
<p>Author: Onur C. Hamsici, Aleix M. Martinez</p><p>Abstract: Many feature representations, as in genomics, describe directional data where all feature vectors share a common norm. In other cases, as in computer vision, a norm or variance normalization step, where all feature vectors are normalized to a common length, is generally used. These representations and pre-processing step map the original data from R p to the surface of a hypersphere S p−1 . Such representations should then be modeled using spherical distributions. However, the difﬁculty associated with such spherical representations has prompted researchers to model their spherical data using Gaussian distributions instead—as if the data were represented in R p rather than S p−1 . This opens the question to whether the classiﬁcation results calculated with the Gaussian approximation are the same as those obtained when using the original spherical distributions. In this paper, we show that in some particular cases (which we named spherical-homoscedastic) the answer to this question is positive. In the more general case however, the answer is negative. For this reason, we further investigate the additional error added by the Gaussian modeling. We conclude that the more the data deviates from spherical-homoscedastic, the less advisable it is to employ the Gaussian approximation. We then show how our derivations can be used to deﬁne optimal classiﬁers for spherical-homoscedastic distributions. By using a kernel which maps the original space into one where the data adapts to the spherical-homoscedastic model, we can derive non-linear classiﬁers with potential applications in a large number of problems. We conclude this paper by demonstrating the uses of spherical-homoscedasticity in the classiﬁcation of images of objects, gene expression sequences, and text data. Keywords: directional data, spherical distributions, normal distributions, norm normalization, linear and non-linear classiﬁers, computer vision</p><p>2 0.30719557 <a title="76-lda-2" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>Author: François Laviolette, Mario Marchand</p><p>Abstract: We propose a PAC-Bayes theorem for the sample-compression setting where each classiﬁer is described by a compression subset of the training data and a message string of additional information. This setting, which is the appropriate one to describe many learning algorithms, strictly generalizes the usual data-independent setting where classiﬁers are represented only by data-independent message strings (or parameters taken from a continuous set). The proposed PAC-Bayes theorem for the sample-compression setting reduces to the PAC-Bayes theorem of Seeger (2002) and Langford (2005) when the compression subset of each classiﬁer vanishes. For posteriors having all their weights on a single sample-compressed classiﬁer, the general risk bound reduces to a bound similar to the tight sample-compression bound proposed in Laviolette et al. (2005). Finally, we extend our results to the case where each sample-compressed classiﬁer of a data-dependent ensemble may abstain of predicting a class label. Keywords: PAC-Bayes, risk bounds, sample-compression, set covering machines, decision list machines</p><p>3 0.30676097 <a title="76-lda-3" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>Author: Masashi Sugiyama</p><p>Abstract: Reducing the dimensionality of data without losing intrinsic information is an important preprocessing step in high-dimensional data analysis. Fisher discriminant analysis (FDA) is a traditional technique for supervised dimensionality reduction, but it tends to give undesired results if samples in a class are multimodal. An unsupervised dimensionality reduction method called localitypreserving projection (LPP) can work well with multimodal data due to its locality preserving property. However, since LPP does not take the label information into account, it is not necessarily useful in supervised learning scenarios. In this paper, we propose a new linear supervised dimensionality reduction method called local Fisher discriminant analysis (LFDA), which effectively combines the ideas of FDA and LPP. LFDA has an analytic form of the embedding transformation and the solution can be easily computed just by solving a generalized eigenvalue problem. We demonstrate the practical usefulness and high scalability of the LFDA method in data visualization and classiﬁcation tasks through extensive simulation studies. We also show that LFDA can be extended to non-linear dimensionality reduction scenarios by applying the kernel trick. Keywords: dimensionality reduction, supervised learning, Fisher discriminant analysis, locality preserving projection, afﬁnity matrix</p><p>4 0.29977605 <a title="76-lda-4" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>5 0.29562598 <a title="76-lda-5" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>6 0.29323199 <a title="76-lda-6" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>7 0.29309976 <a title="76-lda-7" href="./jmlr-2007-Learning_Equivariant_Functions_with_Matrix_Valued_Kernels.html">46 jmlr-2007-Learning Equivariant Functions with Matrix Valued Kernels</a></p>
<p>8 0.29210174 <a title="76-lda-8" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>9 0.28694299 <a title="76-lda-9" href="./jmlr-2007-Bayesian_Quadratic_Discriminant_Analysis.html">13 jmlr-2007-Bayesian Quadratic Discriminant Analysis</a></p>
<p>10 0.28672811 <a title="76-lda-10" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>11 0.28294221 <a title="76-lda-11" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>12 0.28194857 <a title="76-lda-12" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>13 0.28107664 <a title="76-lda-13" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>14 0.28072178 <a title="76-lda-14" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>15 0.28036988 <a title="76-lda-15" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>16 0.27970237 <a title="76-lda-16" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>17 0.27894902 <a title="76-lda-17" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>18 0.27852225 <a title="76-lda-18" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>19 0.27723664 <a title="76-lda-19" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>20 0.27665535 <a title="76-lda-20" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
