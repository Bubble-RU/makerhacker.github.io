<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-22" href="#">jmlr2007-22</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</h1>
<br/><p>Source: <a title="jmlr-2007-22-pdf" href="http://jmlr.org/papers/volume8/boulle07a/boulle07a.pdf">pdf</a></p><p>Author: Marc Boullé</p><p>Abstract: The naive Bayes classiﬁer has proved to be very effective on many real data applications. Its performance usually beneﬁts from an accurate estimation of univariate conditional probabilities and from variable selection. However, although variable selection is a desirable feature, it is prone to overﬁtting. In this paper, we introduce a Bayesian regularization technique to select the most probable subset of variables compliant with the naive Bayes assumption. We also study the limits of Bayesian model averaging in the case of the naive Bayes assumption and introduce a new weighting scheme based on the ability of the models to conditionally compress the class labels. The weighting scheme on the models reduces to a weighting scheme on the variables, and ﬁnally results in a naive Bayes classiﬁer with “soft variable selection”. Extensive experiments show that the compressionbased averaged classiﬁer outperforms the Bayesian model averaging scheme. Keywords: naive Bayes, Bayesian, model selection, model averaging</p><p>Reference: <a title="jmlr-2007-22-reference" href="../jmlr2007_reference/jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 COM  France Telecom R&D; 2, avenue Pierre Marzin 22300 Lannion, France  Editors: Isabelle Guyon and Amir Saffari  Abstract The naive Bayes classiﬁer has proved to be very effective on many real data applications. [sent-3, score-0.225]
</p><p>2 In this paper, we introduce a Bayesian regularization technique to select the most probable subset of variables compliant with the naive Bayes assumption. [sent-6, score-0.414]
</p><p>3 We also study the limits of Bayesian model averaging in the case of the naive Bayes assumption and introduce a new weighting scheme based on the ability of the models to conditionally compress the class labels. [sent-7, score-0.508]
</p><p>4 The weighting scheme on the models reduces to a weighting scheme on the variables, and ﬁnally results in a naive Bayes classiﬁer with “soft variable selection”. [sent-8, score-0.394]
</p><p>5 Keywords: naive Bayes, Bayesian, model selection, model averaging  1. [sent-10, score-0.46]
</p><p>6 Introduction The naive Bayes classiﬁcation approach (see Langley et al. [sent-11, score-0.225]
</p><p>7 The naive independence assumption can harm the performance when violated. [sent-17, score-0.225]
</p><p>8 In order to better deal with highly correlated variables, the selective naive Bayes approach of Langley and Sage (1994) exploits a wrapper approach (see Kohavi and John, 1997) to select the subset of variables which optimizes the classiﬁcation accuracy. [sent-18, score-0.453]
</p><p>9 Although the selective naive Bayes approach performs quite well on data sets with a reasonable number of variables, it does not scale on very large data sets with hundreds of thousands of instances and thousands of variables, such as in marketing applications. [sent-20, score-0.368]
</p><p>10 The resulting variable selection criterion is optimized owing to an efﬁcient search heuristic whose computational complexity is O(KN log(KN)), where N is the number of instances and K the number of variables. [sent-24, score-0.244]
</p><p>11 We show that averaging the models turns into averaging the contribution of the variables in the case of the selective naive Bayes classiﬁer. [sent-27, score-0.782]
</p><p>12 Section 2 introduces the assumptions and recalls the principles of the naive Bayes and selective naive Bayes classiﬁers. [sent-30, score-0.573]
</p><p>13 Section 3 presents the regularization technique for variable selection based on Bayesian model selection and Section 4 applies the Bayesian model averaging method to selective naive Bayes classiﬁers. [sent-31, score-0.722]
</p><p>14 In Section 5, the new selective naive Bayes classiﬁers are evaluated on an illustrative example. [sent-32, score-0.369]
</p><p>15 Section 6 analyzes the limits of Bayesian model averaging and proposes a new model averaging technique based on model compression coefﬁcients. [sent-33, score-0.5]
</p><p>16 Selective Naive Bayes Classiﬁer This section formally states the assumptions and notations and recalls the naive Bayes and selective naive Bayes approaches. [sent-38, score-0.573]
</p><p>17 Let M = {Mm } be the set of all the potential selective naive Bayes models. [sent-52, score-0.348]
</p><p>18 Each model M m is described by K parameter values amk , where amk is 1 if variable k is selected in model Mm and 0 otherwise. [sent-53, score-0.346]
</p><p>19 2 Naive Bayes Classiﬁer The naive Bayes classiﬁer assigns to each instance the class value having the highest conditional probability P(λ j )P(X|λ j ) P(λ j |X) = . [sent-67, score-0.27]
</p><p>20 ∑J P(λi ) ∏K P(Xk |λi ) i=1 k=1  (2)  The naive Bayes classiﬁer is poor at predicting the true class conditional probabilities, since the independence assumption is usually violated in real data applications. [sent-71, score-0.27]
</p><p>21 3 Selective Naive Bayes Classiﬁer The selective naive Bayes classiﬁer reduces the strong bias of the naive independence assumption, owing to variable selection. [sent-74, score-0.64]
</p><p>22 The objective is to search among all the subsets of variables, in order to ﬁnd the best possible classiﬁer, compliant with the naive Bayes assumption. [sent-75, score-0.321]
</p><p>23 This ROC criterion, estimated on the train data set (as in Langley and Sage, 1994), is used by Boull´ (2006a) to assess the quality of variable selection for e naive Bayes classiﬁer. [sent-84, score-0.338]
</p><p>24 The forward selection process requires O(K 2 N log N) time, owing to the decomposability of the naive Bayes formula on the variables. [sent-90, score-0.37]
</p><p>25 1 Introduction The naive Bayes classiﬁer is a very robust algorithm. [sent-95, score-0.225]
</p><p>26 On the opposite, the selective naive Bayes classiﬁer explores the space of all subsets of variables to reduce the strong bias of the naive independence assumption. [sent-97, score-0.631]
</p><p>27 3 Prior for Variable Selection The parameters of a variable selection model Mm are the Boolean values amk . [sent-110, score-0.23]
</p><p>28 The fast forward selection heuristic evaluates each variable one at a time, and adds it to the selection as soon as this improves the criterion. [sent-136, score-0.244]
</p><p>29 Evaluating a selective naive Bayes model requires O(KN) computation time, mainly to evaluate all the class conditional probabilities. [sent-147, score-0.421]
</p><p>30 Each fast forward selection or fast backward selection step considers O(K) additions or removals of variables and requires O(KN) computation time. [sent-149, score-0.231]
</p><p>31 Bayesian Model Averaging of Selective Naive Bayes Classiﬁers Model averaging consists in combining the prediction of an ensemble of classiﬁers in order to reduce the prediction error. [sent-156, score-0.231]
</p><p>32 This section reminds the principles of Bayesian model averaging and applies this averaging scheme to the selective naive Bayes classiﬁer. [sent-157, score-0.756]
</p><p>33 Whereas the MAP approach retrieves the most probable model given the data, the BMA approach exploits every model in the model space, weighted by their posterior probability. [sent-161, score-0.257]
</p><p>34 This approach relies on the deﬁnition of a prior distribution on the models, on an efﬁcient computation technique to estimate the model posterior probabilities and on an effective method to sample the posterior distribution. [sent-162, score-0.281]
</p><p>35 The BMA approach has been applied to the naive Bayes classiﬁer by Dash and Cooper (2002). [sent-164, score-0.225]
</p><p>36 3 Expectation of the Class Conditional Information The selective naive Bayes classiﬁer provides an estimation of the class conditional probabilities. [sent-180, score-0.393]
</p><p>37 k=1  We are looking for the expectation of this conditional information ∑m I(Mn , D)P(Mm |D) ∑m P(Mm |D) ∑ P(Mm |D) ∑K amk I(Xk |Y ) k=1 = I(Y ) − I(X) + m P(Mm |D) ∑m  E(I) =  K  = I(Y ) − I(X) + ∑ I(Xk |Y ) k=1  1666  ∑m amk P(Mm |D) . [sent-187, score-0.275]
</p><p>38 When the above averaging scheme is applied, each variable has a [0, 1] weight, which can be interpreted as a “soft selection”. [sent-196, score-0.236]
</p><p>39 4 An Efﬁcient Algorithm for Model Averaging We have previously introduced a model averaging method which relies on the expectation of the class conditional information. [sent-198, score-0.252]
</p><p>40 The overhead in the time complexity of the learning algorithm is negligible, since the only need is to collect the posterior probability of the models and to compute the weights in the averaging formula. [sent-206, score-0.287]
</p><p>41 Concerning the deployment of the averaged model, the overhead is also negligible, since the initial naive Bayes estimation of the class conditional probabilities is just extended with variable weights. [sent-207, score-0.359]
</p><p>42 Evaluation on an Illustrative Example This section describes the waveform data set, introduces three evaluation criteria and illustrates the behavior of each variant of the selective naive Bayes classiﬁer. [sent-209, score-0.516]
</p><p>43 W a v e fo rm  1  W a v e fo rm  2  W a v e fo rm  3  1 0  5  0 1  2  3  4  5  6  7  8  9  1 0 1 1  1 2  1 3  1 4  1 5  1 6  1 7  1 8  1 9  2 0 2 1  Figure 1: Basic waveforms used to generated the 21 input variables of the waveform data set. [sent-215, score-0.279]
</p><p>44 The input variables are correlated, which violates the naive Bayes assumption. [sent-219, score-0.283]
</p><p>45 Selecting the best subset of variables compliant with the naive Bayes assumption is a challenging problem. [sent-220, score-0.36]
</p><p>46 The ILF criterion (see Witten and Frank, 2000) evaluates the probabilistic prediction owing to the negative log of the predicted class conditional probabilities − log Pm (Y = y(n) |X = x(n) ). [sent-230, score-0.274]
</p><p>47 N n=1  The predicted class conditional probabilities in the ILF criterion are given by Equation (2) for the naive Bayes classiﬁer and by Equation (3) for the selective naive Bayes classiﬁer. [sent-232, score-0.714]
</p><p>48 The null model estimates the class / conditional probabilities by their prior probabilities, ignoring all the explanatory variables. [sent-234, score-0.224]
</p><p>49 In the case of the waveform data set, the MODL preprocessing method determines that 2 variables (1st and 21st ) are irrelevant, and the naive Bayes classiﬁer uses all the 19 remaining variables. [sent-242, score-0.428]
</p><p>50 We evaluate four variants of selective naive Bayes methods. [sent-243, score-0.348]
</p><p>51 The SNB(MAP) method introduced in Section 3 selects the most probable subset of variables compliant with the naive Bayes assumption, and the SNB(BMA) method described in Section 4 averages the selective naive Bayes classiﬁers weighted by their posterior probability. [sent-245, score-0.87]
</p><p>52 The SNB(MAP) which focuses on a subset of variables compliant with the naive Bayes assumption selects only 8 variables, which turns out to be a subset of the variables selected by the alternative methods. [sent-250, score-0.461]
</p><p>53 5 0  SNB(BMA)  Figure 3: Variables selected by the selective naive Bayes classiﬁers for the waveform data set. [sent-255, score-0.487]
</p><p>54 6  Figure 4: Evaluation of the predictive performance of selective naive Bayes classiﬁers on the waveform data set. [sent-274, score-0.462]
</p><p>55 Compared to the other variable selection methods, the SNB(MAP) truly focuses on complying with the naive independence assumption. [sent-281, score-0.312]
</p><p>56 1670  C OMPRESSION -BASED AVERAGING OF S ELECTIVE NAIVE BAYES C LASSIFIERS  The SNB(BMA) method exploits a model averaging approach which results in soft variable selection. [sent-283, score-0.271]
</p><p>57 Compared to the hard variable selection scheme, the soft variable selection exhibits mainly one minor change: a new variable (V20) is selected with a small weight of 0. [sent-286, score-0.234]
</p><p>58 Since the variable selection is almost the same as in the MAP approach, the model averaging approach does not bring any signiﬁcant improvement in the evaluation results, as shown in Figure 4. [sent-292, score-0.324]
</p><p>59 Compression Model Averaging of Selective Naive Bayes Classiﬁers This section analyzes the limits of Bayesian model averaging and proposes a new weighting scheme that better exploits the posterior distributions of the models. [sent-294, score-0.384]
</p><p>60 1 Understanding the Limits of Bayesian Model Averaging We use again the waveform data set to explain why the Bayesian model averaging method fails to outperform the MAP method. [sent-296, score-0.321]
</p><p>61 Figure 5: Index of the selected variables in the 200 most probable selective naive Bayes models for the waveform data set. [sent-297, score-0.617]
</p><p>62 The variable selection problem consists in ﬁnding the most probable subset of variables compliant with the naive Bayes assumption among about half a million (2 19 ) potential subsets. [sent-299, score-0.501]
</p><p>63 The potential beneﬁt of model averaging is to account for all these models, with higher weights for the most probable models. [sent-306, score-0.261]
</p><p>64 In the waveform example, averaging using the posterior probabilities to weight the models is almost the same as selecting the MAP model (which itself is hard to ﬁnd with a heuristic search) and model averaging is almost useless. [sent-315, score-0.734]
</p><p>65 In the case of the selective naive Bayes classiﬁer, this assumption is violated on most real data sets and BMA fails to build effective model averaging. [sent-317, score-0.376]
</p><p>66 2 Model Averaging with Compression Coefﬁcients When the posterior distribution is sharply peaked around the MAP, averaging is almost the same as selecting the MAP model. [sent-319, score-0.307]
</p><p>67 Small improvements in the estimation of the conditional entropy brings very large differences in the posterior probability of the models, which explains why Bayesian model averaging is asymptotically equivalent to selecting the MAP model. [sent-328, score-0.342]
</p><p>68 In our heuristic attempt to better account for all the models, we replace the posterior probabilities by their related compression coefﬁcient in the weighting scheme. [sent-338, score-0.282]
</p><p>69 Let us focus again on the variable weights bk introduced in Section 4 in our ﬁrst model averaging method. [sent-339, score-0.264]
</p><p>70 Dividing the posterior probabilities by those of the null model, we get P(M |D)  bk =  ∑m amk P(Mm|D) / 0 ∑m  P(Mm |D) P(M0 |D) /  . [sent-340, score-0.314]
</p><p>71 ∑m c(Mm , D)  Mainly, the principle of this new heuristic weighting scheme consists in smoothing the exponentially peaked posterior probability distribution of Equation (8) with the log function. [sent-343, score-0.255]
</p><p>72 We evaluate the compression based model averaging (CMA) model using the model averaging algorithm introduced in Section 4. [sent-345, score-0.5]
</p><p>73 3 Evaluation on the Waveform Data Set We use the protocol introduced in Section 5 to evaluate the SNB(CMA) compression model averaging method on the waveform data set, with an exhaustive evaluation of all the models to avoid the potential bias of the optimization algorithms. [sent-348, score-0.427]
</p><p>74 Figure 7 shows the weights of each variable resulting from the soft variable selection of the SNB(CMA) compression model averaging method. [sent-349, score-0.387]
</p><p>75 4  Figure 7: Variables selected by the SNB(CMA) method and the alternative selective naive Bayes classiﬁers for the waveform data set. [sent-358, score-0.487]
</p><p>76 6  Figure 8: Evaluation of the predictive performance of selective naive Bayes classiﬁers on the waveform data set. [sent-377, score-0.462]
</p><p>77 1674  C OMPRESSION -BASED AVERAGING OF S ELECTIVE NAIVE BAYES C LASSIFIERS  In the waveform data set, all the variables are informative, but the most probable subsets of variables compliant with the naive Bayes assumption select only half of the variables. [sent-378, score-0.586]
</p><p>78 Experiments This section presents an experimental evaluation of the performance of the selective naive Bayes methods described in the previous sections. [sent-385, score-0.378]
</p><p>79 1 Experimental Setup The experiments aim at comparing the performance of model averaging methods versus the MAP method, the standard selective naive Bayes (SNB) and naive Bayes (NB) methods. [sent-387, score-0.78]
</p><p>80 The evaluated criteria are the same as for the waveform data set: accuracy (ACC), area under the ROC curve (AUC) and informational loss function (ILF) (with its compression rate (CR) normalization). [sent-393, score-0.277]
</p><p>81 result is less than 0 in the case of the challenge data sets, which means that their estimation of the class conditional probabilities is worse than that of the null model (which selects no variable). [sent-554, score-0.258]
</p><p>82 The domination of the SNB(CMA) method increases with the complexity of the criteria: it is noteworthy for accuracy (ACC), important for the ordering of the class conditional probabilities (AUC) and very large for the prediction of the class conditional probabilities (CR). [sent-580, score-0.265]
</p><p>83 This shows that the regularized and averaged naive Bayes classiﬁer becomes effective for conditional probability estimation, whereas the standard naive Bayes classiﬁer is usually considered to be poor at estimating these probabilities. [sent-581, score-0.495]
</p><p>84 To summarize, this experiment demonstrates that variable selection is useful to improve the classiﬁcation accuracy of the naive Bayes classiﬁer. [sent-582, score-0.33]
</p><p>85 The MAP selection approach presented in Section 3 allows to ﬁnd a selective naive Bayes classiﬁer which is as compliant as possible with the naive Bayes assumption. [sent-583, score-0.702]
</p><p>86 On the opposite, our compression model averaging scheme introduced in Section 6 takes beneﬁt from the full posterior distribution of the models and obtains superior results for all the evaluated criteria. [sent-587, score-0.416]
</p><p>87 Evaluation on the Performance Prediction Challenge This section reports the results obtained by our compression-based averaging method on the performance prediction challenge of Guyon et al. [sent-589, score-0.285]
</p><p>88 The ada data set comes from the marketing domain, the gina data set from handwriting recognition, the hiva data set from drug discovery, the nova data set from text classiﬁcation and the sylva data set from ecology. [sent-598, score-0.255]
</p><p>89 2 Details of the Submissions All our entries are based on the compression-based averaging of the selective naive Bayes classiﬁer SNB(CMA). [sent-601, score-0.547]
</p><p>90 The method computes the posterior probabilities of the classes, which is convenient when the accuracy criterion or the area under the ROC curve is evaluated. [sent-602, score-0.223]
</p><p>91 The main limitation of our SNB(CMA) method comes from the naive Bayes assumption. [sent-678, score-0.225]
</p><p>92 Our method fails to correctly approximate the true class conditional distribution when the representation space of the data set does not contain any competitive subset of variables compliant with the naive Bayes assumption. [sent-679, score-0.405]
</p><p>93 However, when the constructed features allow to “partially” circumvent the naive Bayes assumption, the method succeeds in signiﬁcantly improving its performance, from 13% down to 7%. [sent-682, score-0.225]
</p><p>94 Conclusion The naive Bayes classiﬁer is a popular method that is often highly effective on real data sets and is competitive with or even sometimes outperforms much more sophisticated classiﬁers. [sent-703, score-0.225]
</p><p>95 We have proposed a MAP approach to select the best subset of variables compliant with the naive Bayes assumption and introduced an efﬁcient search algorithm which time complexity is O(KN log(KN)), where K is the number of variables and N the number of instances. [sent-705, score-0.437]
</p><p>96 We have empirically demonstrated that the resulting compression-based model averaging scheme clearly outperforms the Bayesian model averaging scheme. [sent-708, score-0.436]
</p><p>97 Our method consistently improves the performance of the naive Bayes classiﬁer, but is outperformed by more sophisticated methods when the naive Bayes assumption is too harmful. [sent-710, score-0.45]
</p><p>98 In future work, we plan to exploit multivariate preprocessing methods in order to circumvent the naive Bayes assumption. [sent-711, score-0.256]
</p><p>99 In this setting, we think that compression-based model averaging might still be superior to Bayesian model averaging to account for the whole posterior distribution of the models. [sent-713, score-0.504]
</p><p>100 Regularization and averaging of the selective naive Bayes classiﬁer. [sent-732, score-0.527]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('snb', 0.662), ('mm', 0.241), ('naive', 0.225), ('bma', 0.208), ('cma', 0.2), ('averaging', 0.179), ('acc', 0.177), ('ilf', 0.139), ('auc', 0.135), ('selective', 0.123), ('bayes', 0.119), ('amk', 0.115), ('waveform', 0.114), ('ber', 0.107), ('elective', 0.1), ('boull', 0.091), ('posterior', 0.09), ('nb', 0.088), ('oull', 0.085), ('lassifiers', 0.085), ('challenge', 0.08), ('compliant', 0.077), ('ompression', 0.076), ('cr', 0.07), ('modl', 0.065), ('xk', 0.064), ('gina', 0.062), ('nova', 0.062), ('map', 0.061), ('variables', 0.058), ('compression', 0.058), ('roc', 0.057), ('probable', 0.054), ('ffwbw', 0.054), ('probabilities', 0.054), ('selection', 0.052), ('km', 0.049), ('langley', 0.047), ('hiva', 0.046), ('sylva', 0.046), ('conditional', 0.045), ('explanatory', 0.045), ('heuristic', 0.044), ('criterion', 0.042), ('kn', 0.041), ('ada', 0.039), ('peaked', 0.038), ('waveforms', 0.038), ('guess', 0.037), ('bayesian', 0.037), ('forward', 0.036), ('weighting', 0.036), ('discretization', 0.036), ('variable', 0.035), ('guyon', 0.035), ('backward', 0.033), ('null', 0.033), ('er', 0.033), ('pareto', 0.033), ('sage', 0.033), ('owing', 0.032), ('preprocessing', 0.031), ('evaluation', 0.03), ('exploits', 0.029), ('categorical', 0.029), ('pm', 0.028), ('model', 0.028), ('tes', 0.026), ('prediction', 0.026), ('train', 0.026), ('ms', 0.025), ('log', 0.025), ('selected', 0.025), ('evaluates', 0.025), ('criteria', 0.024), ('dc', 0.024), ('raftery', 0.023), ('noteworthy', 0.023), ('iter', 0.023), ('rank', 0.023), ('eror', 0.023), ('fo', 0.023), ('hoeting', 0.023), ('informational', 0.023), ('ues', 0.023), ('classi', 0.023), ('scheme', 0.022), ('bk', 0.022), ('evaluated', 0.021), ('insigni', 0.021), ('entries', 0.02), ('instances', 0.02), ('prior', 0.019), ('search', 0.019), ('area', 0.019), ('accuracy', 0.018), ('selects', 0.018), ('ers', 0.018), ('models', 0.018), ('optimizes', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999982 <a title="22-tfidf-1" href="./jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</a></p>
<p>Author: Marc Boullé</p><p>Abstract: The naive Bayes classiﬁer has proved to be very effective on many real data applications. Its performance usually beneﬁts from an accurate estimation of univariate conditional probabilities and from variable selection. However, although variable selection is a desirable feature, it is prone to overﬁtting. In this paper, we introduce a Bayesian regularization technique to select the most probable subset of variables compliant with the naive Bayes assumption. We also study the limits of Bayesian model averaging in the case of the naive Bayes assumption and introduce a new weighting scheme based on the ability of the models to conditionally compress the class labels. The weighting scheme on the models reduces to a weighting scheme on the variables, and ﬁnally results in a naive Bayes classiﬁer with “soft variable selection”. Extensive experiments show that the compressionbased averaged classiﬁer outperforms the Bayesian model averaging scheme. Keywords: naive Bayes, Bayesian, model selection, model averaging</p><p>2 0.096477605 <a title="22-tfidf-2" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>Author: Carine Hue, Marc Boullé</p><p>Abstract: In this paper, we consider the supervised learning task which consists in predicting the normalized rank of a numerical variable. We introduce a novel probabilistic approach to estimate the posterior distribution of the target rank conditionally to the predictors. We turn this learning task into a model selection problem. For that, we deﬁne a 2D partitioning family obtained by discretizing numerical variables and grouping categorical ones and we derive an analytical criterion to select the partition with the highest posterior probability. We show how these partitions can be used to build univariate predictors and multivariate ones under a naive Bayes assumption. We also propose a new evaluation criterion for probabilistic rank estimators. Based on the logarithmic score, we show that such criterion presents the advantage to be minored, which is not the case of the logarithmic score computed for probabilistic value estimator. A ﬁrst set of experimentations on synthetic data shows the good properties of the proposed criterion and of our partitioning approach. A second set of experimentations on real data shows competitive performance of the univariate and selective naive Bayes rank estimators projected on the value range compared to methods submitted to a recent challenge on probabilistic metric regression tasks. Our approach is applicable for all regression problems with categorical or numerical predictors. It is particularly interesting for those with a high number of predictors as it automatically detects the variables which contain predictive information. It builds pertinent predictors of the normalized rank of the numerical target from one or several predictors. As the criteria selection is regularized by the presence of a prior and a posterior term, it does not suffer from overﬁtting. Keywords: rank regression, probabilistic approach, 2D partitioning, non parametric estimation, Bayesian model selection</p><p>3 0.083508782 <a title="22-tfidf-3" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>Author: Stéphan Clémençon, Nicolas Vayatis</p><p>Abstract: We formulate a local form of the bipartite ranking problem where the goal is to focus on the best instances. We propose a methodology based on the construction of real-valued scoring functions. We study empirical risk minimization of dedicated statistics which involve empirical quantiles of the scores. We ﬁrst state the problem of ﬁnding the best instances which can be cast as a classiﬁcation problem with mass constraint. Next, we develop special performance measures for the local ranking problem which extend the Area Under an ROC Curve (AUC) criterion and describe the optimal elements of these new criteria. We also highlight the fact that the goal of ranking the best instances cannot be achieved in a stage-wise manner where ﬁrst, the best instances would be tentatively identiﬁed and then a standard AUC criterion could be applied. Eventually, we state preliminary statistical results for the local ranking problem. Keywords: ranking, ROC curve and AUC, empirical risk minimization, fast rates</p><p>4 0.064376481 <a title="22-tfidf-4" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>Author: J. Zico Kolter, Marcus A. Maloof</p><p>Abstract: We present an ensemble method for concept drift that dynamically creates and removes weighted experts in response to changes in performance. The method, dynamic weighted majority (DWM), uses four mechanisms to cope with concept drift: It trains online learners of the ensemble, it weights those learners based on their performance, it removes them, also based on their performance, and it adds new experts based on the global performance of the ensemble. After an extensive evaluation— consisting of ﬁve experiments, eight learners, and thirty data sets that varied in type of target concept, size, presence of noise, and the like—we concluded that DWM outperformed other learners that only incrementally learn concept descriptions, that maintain and use previously encountered examples, and that employ an unweighted, ﬁxed-size ensemble of experts. Keywords: concept learning, online learning, ensemble methods, concept drift</p><p>5 0.064359136 <a title="22-tfidf-5" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>Author: Sébastien Gadat, Laurent Younes</p><p>Abstract: We introduce a new model addressing feature selection from a large dictionary of variables that can be computed from a signal or an image. Features are extracted according to an efﬁciency criterion, on the basis of speciﬁed classiﬁcation or recognition tasks. This is done by estimating a probability distribution P on the complete dictionary, which distributes its mass over the more efﬁcient, or informative, components. We implement a stochastic gradient descent algorithm, using the probability as a state variable and optimizing a multi-task goodness of ﬁt criterion for classiﬁers based on variable randomly chosen according to P. We then generate classiﬁers from the optimal distribution of weights learned on the training set. The method is ﬁrst tested on several pattern recognition problems including face detection, handwritten digit recognition, spam classiﬁcation and micro-array analysis. We then compare our approach with other step-wise algorithms like random forests or recursive feature elimination. Keywords: stochastic learning algorithms, Robbins-Monro application, pattern recognition, classiﬁcation algorithm, feature selection</p><p>6 0.034977566 <a title="22-tfidf-6" href="./jmlr-2007-Multi-Task_Learning_for_Classification_with_Dirichlet_Process_Priors.html">56 jmlr-2007-Multi-Task Learning for Classification with Dirichlet Process Priors</a></p>
<p>7 0.034904476 <a title="22-tfidf-7" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>8 0.034538426 <a title="22-tfidf-8" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>9 0.031112768 <a title="22-tfidf-9" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>10 0.030976931 <a title="22-tfidf-10" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>11 0.03076221 <a title="22-tfidf-11" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>12 0.029649271 <a title="22-tfidf-12" href="./jmlr-2007-Structure_and_Majority_Classes_in_Decision_Tree_Learning.html">79 jmlr-2007-Structure and Majority Classes in Decision Tree Learning</a></p>
<p>13 0.029578328 <a title="22-tfidf-13" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<p>14 0.029007131 <a title="22-tfidf-14" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>15 0.028847177 <a title="22-tfidf-15" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>16 0.028338429 <a title="22-tfidf-16" href="./jmlr-2007-Nonlinear_Boosting_Projections_for_Ensemble_Construction.html">59 jmlr-2007-Nonlinear Boosting Projections for Ensemble Construction</a></p>
<p>17 0.027885297 <a title="22-tfidf-17" href="./jmlr-2007-Revised_Loss_Bounds_for_the_Set_Covering_Machine_and_Sample-Compression_Loss_Bounds_for_Imbalanced_Data.html">73 jmlr-2007-Revised Loss Bounds for the Set Covering Machine and Sample-Compression Loss Bounds for Imbalanced Data</a></p>
<p>18 0.027392339 <a title="22-tfidf-18" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>19 0.026386166 <a title="22-tfidf-19" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>20 0.025384445 <a title="22-tfidf-20" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.153), (1, 0.087), (2, -0.054), (3, 0.022), (4, 0.093), (5, 0.104), (6, -0.021), (7, -0.138), (8, 0.063), (9, -0.005), (10, -0.177), (11, -0.042), (12, 0.019), (13, -0.081), (14, -0.106), (15, 0.21), (16, -0.264), (17, 0.178), (18, 0.067), (19, -0.057), (20, -0.013), (21, 0.007), (22, -0.05), (23, -0.127), (24, -0.033), (25, -0.124), (26, -0.099), (27, -0.014), (28, 0.022), (29, 0.123), (30, -0.099), (31, 0.138), (32, 0.236), (33, 0.03), (34, -0.02), (35, 0.002), (36, -0.119), (37, 0.075), (38, 0.127), (39, -0.033), (40, -0.065), (41, -0.135), (42, -0.214), (43, -0.009), (44, -0.076), (45, -0.286), (46, 0.168), (47, -0.029), (48, -0.023), (49, -0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94627476 <a title="22-lsi-1" href="./jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</a></p>
<p>Author: Marc Boullé</p><p>Abstract: The naive Bayes classiﬁer has proved to be very effective on many real data applications. Its performance usually beneﬁts from an accurate estimation of univariate conditional probabilities and from variable selection. However, although variable selection is a desirable feature, it is prone to overﬁtting. In this paper, we introduce a Bayesian regularization technique to select the most probable subset of variables compliant with the naive Bayes assumption. We also study the limits of Bayesian model averaging in the case of the naive Bayes assumption and introduce a new weighting scheme based on the ability of the models to conditionally compress the class labels. The weighting scheme on the models reduces to a weighting scheme on the variables, and ﬁnally results in a naive Bayes classiﬁer with “soft variable selection”. Extensive experiments show that the compressionbased averaged classiﬁer outperforms the Bayesian model averaging scheme. Keywords: naive Bayes, Bayesian, model selection, model averaging</p><p>2 0.55869615 <a title="22-lsi-2" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>Author: Carine Hue, Marc Boullé</p><p>Abstract: In this paper, we consider the supervised learning task which consists in predicting the normalized rank of a numerical variable. We introduce a novel probabilistic approach to estimate the posterior distribution of the target rank conditionally to the predictors. We turn this learning task into a model selection problem. For that, we deﬁne a 2D partitioning family obtained by discretizing numerical variables and grouping categorical ones and we derive an analytical criterion to select the partition with the highest posterior probability. We show how these partitions can be used to build univariate predictors and multivariate ones under a naive Bayes assumption. We also propose a new evaluation criterion for probabilistic rank estimators. Based on the logarithmic score, we show that such criterion presents the advantage to be minored, which is not the case of the logarithmic score computed for probabilistic value estimator. A ﬁrst set of experimentations on synthetic data shows the good properties of the proposed criterion and of our partitioning approach. A second set of experimentations on real data shows competitive performance of the univariate and selective naive Bayes rank estimators projected on the value range compared to methods submitted to a recent challenge on probabilistic metric regression tasks. Our approach is applicable for all regression problems with categorical or numerical predictors. It is particularly interesting for those with a high number of predictors as it automatically detects the variables which contain predictive information. It builds pertinent predictors of the normalized rank of the numerical target from one or several predictors. As the criteria selection is regularized by the presence of a prior and a posterior term, it does not suffer from overﬁtting. Keywords: rank regression, probabilistic approach, 2D partitioning, non parametric estimation, Bayesian model selection</p><p>3 0.35023165 <a title="22-lsi-3" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>Author: Stéphan Clémençon, Nicolas Vayatis</p><p>Abstract: We formulate a local form of the bipartite ranking problem where the goal is to focus on the best instances. We propose a methodology based on the construction of real-valued scoring functions. We study empirical risk minimization of dedicated statistics which involve empirical quantiles of the scores. We ﬁrst state the problem of ﬁnding the best instances which can be cast as a classiﬁcation problem with mass constraint. Next, we develop special performance measures for the local ranking problem which extend the Area Under an ROC Curve (AUC) criterion and describe the optimal elements of these new criteria. We also highlight the fact that the goal of ranking the best instances cannot be achieved in a stage-wise manner where ﬁrst, the best instances would be tentatively identiﬁed and then a standard AUC criterion could be applied. Eventually, we state preliminary statistical results for the local ranking problem. Keywords: ranking, ROC curve and AUC, empirical risk minimization, fast rates</p><p>4 0.30690867 <a title="22-lsi-4" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>Author: J. Zico Kolter, Marcus A. Maloof</p><p>Abstract: We present an ensemble method for concept drift that dynamically creates and removes weighted experts in response to changes in performance. The method, dynamic weighted majority (DWM), uses four mechanisms to cope with concept drift: It trains online learners of the ensemble, it weights those learners based on their performance, it removes them, also based on their performance, and it adds new experts based on the global performance of the ensemble. After an extensive evaluation— consisting of ﬁve experiments, eight learners, and thirty data sets that varied in type of target concept, size, presence of noise, and the like—we concluded that DWM outperformed other learners that only incrementally learn concept descriptions, that maintain and use previously encountered examples, and that employ an unweighted, ﬁxed-size ensemble of experts. Keywords: concept learning, online learning, ensemble methods, concept drift</p><p>5 0.24405971 <a title="22-lsi-5" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>Author: Sébastien Gadat, Laurent Younes</p><p>Abstract: We introduce a new model addressing feature selection from a large dictionary of variables that can be computed from a signal or an image. Features are extracted according to an efﬁciency criterion, on the basis of speciﬁed classiﬁcation or recognition tasks. This is done by estimating a probability distribution P on the complete dictionary, which distributes its mass over the more efﬁcient, or informative, components. We implement a stochastic gradient descent algorithm, using the probability as a state variable and optimizing a multi-task goodness of ﬁt criterion for classiﬁers based on variable randomly chosen according to P. We then generate classiﬁers from the optimal distribution of weights learned on the training set. The method is ﬁrst tested on several pattern recognition problems including face detection, handwritten digit recognition, spam classiﬁcation and micro-array analysis. We then compare our approach with other step-wise algorithms like random forests or recursive feature elimination. Keywords: stochastic learning algorithms, Robbins-Monro application, pattern recognition, classiﬁcation algorithm, feature selection</p><p>6 0.2155228 <a title="22-lsi-6" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>7 0.21482915 <a title="22-lsi-7" href="./jmlr-2007-Boosted_Classification_Trees_and_Class_Probability_Quantile_Estimation.html">16 jmlr-2007-Boosted Classification Trees and Class Probability Quantile Estimation</a></p>
<p>8 0.17842942 <a title="22-lsi-8" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>9 0.17579179 <a title="22-lsi-9" href="./jmlr-2007-Bilinear_Discriminant_Component_Analysis.html">15 jmlr-2007-Bilinear Discriminant Component Analysis</a></p>
<p>10 0.16206944 <a title="22-lsi-10" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>11 0.16132338 <a title="22-lsi-11" href="./jmlr-2007-Integrating_Na%C3%AFve_Bayes_and_FOIL.html">43 jmlr-2007-Integrating Naïve Bayes and FOIL</a></p>
<p>12 0.15663058 <a title="22-lsi-12" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>13 0.15462162 <a title="22-lsi-13" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>14 0.15082581 <a title="22-lsi-14" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>15 0.14301077 <a title="22-lsi-15" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>16 0.13045198 <a title="22-lsi-16" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>17 0.13019535 <a title="22-lsi-17" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>18 0.12999155 <a title="22-lsi-18" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<p>19 0.12561351 <a title="22-lsi-19" href="./jmlr-2007-Multi-Task_Learning_for_Classification_with_Dirichlet_Process_Priors.html">56 jmlr-2007-Multi-Task Learning for Classification with Dirichlet Process Priors</a></p>
<p>20 0.11927269 <a title="22-lsi-20" href="./jmlr-2007-PAC-Bayes_Risk_Bounds_for_Stochastic_Averages_and_Majority_Votes_of_Sample-Compressed_Classifiers.html">65 jmlr-2007-PAC-Bayes Risk Bounds for Stochastic Averages and Majority Votes of Sample-Compressed Classifiers</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.015), (8, 0.018), (10, 0.02), (12, 0.026), (15, 0.017), (28, 0.042), (34, 0.02), (40, 0.026), (45, 0.471), (48, 0.067), (60, 0.025), (80, 0.024), (85, 0.043), (98, 0.09)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89004272 <a title="22-lda-1" href="./jmlr-2007-Compression-Based_Averaging_of_Selective_Naive_Bayes_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">22 jmlr-2007-Compression-Based Averaging of Selective Naive Bayes Classifiers     (Special Topic on Model Selection)</a></p>
<p>Author: Marc Boullé</p><p>Abstract: The naive Bayes classiﬁer has proved to be very effective on many real data applications. Its performance usually beneﬁts from an accurate estimation of univariate conditional probabilities and from variable selection. However, although variable selection is a desirable feature, it is prone to overﬁtting. In this paper, we introduce a Bayesian regularization technique to select the most probable subset of variables compliant with the naive Bayes assumption. We also study the limits of Bayesian model averaging in the case of the naive Bayes assumption and introduce a new weighting scheme based on the ability of the models to conditionally compress the class labels. The weighting scheme on the models reduces to a weighting scheme on the variables, and ﬁnally results in a naive Bayes classiﬁer with “soft variable selection”. Extensive experiments show that the compressionbased averaged classiﬁer outperforms the Bayesian model averaging scheme. Keywords: naive Bayes, Bayesian, model selection, model averaging</p><p>2 0.87958306 <a title="22-lda-2" href="./jmlr-2007-%22Ideal_Parent%22_Structure_Learning_for_Continuous_Variable_Bayesian_Networks.html">1 jmlr-2007-"Ideal Parent" Structure Learning for Continuous Variable Bayesian Networks</a></p>
<p>Author: Gal Elidan, Iftach Nachman, Nir Friedman</p><p>Abstract: Bayesian networks in general, and continuous variable networks in particular, have become increasingly popular in recent years, largely due to advances in methods that facilitate automatic learning from data. Yet, despite these advances, the key task of learning the structure of such models remains a computationally intensive procedure, which limits most applications to parameter learning. This problem is even more acute when learning networks in the presence of missing values or hidden variables, a scenario that is part of many real-life problems. In this work we present a general method for speeding structure search for continuous variable networks with common parametric distributions. We efﬁciently evaluate the approximate merit of candidate structure modiﬁcations and apply time consuming (exact) computations only to the most promising ones, thereby achieving signiﬁcant improvement in the running time of the search algorithm. Our method also naturally and efﬁciently facilitates the addition of useful new hidden variables into the network structure, a task that is typically considered both conceptually difﬁcult and computationally prohibitive. We demonstrate our method on synthetic and real-life data sets, both for learning structure on fully and partially observable data, and for introducing new hidden variables during structure search. Keywords: Bayesian networks, structure learning, continuous variables, hidden variables</p><p>3 0.46084428 <a title="22-lda-3" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>Author: Wei Pan, Xiaotong Shen</p><p>Abstract: Variable selection in clustering analysis is both challenging and important. In the context of modelbased clustering analysis with a common diagonal covariance matrix, which is especially suitable for “high dimension, low sample size” settings, we propose a penalized likelihood approach with an L1 penalty function, automatically realizing variable selection via thresholding and delivering a sparse solution. We derive an EM algorithm to ﬁt our proposed model, and propose a modiﬁed BIC as a model selection criterion to choose the number of components and the penalization parameter. A simulation study and an application to gene function prediction with gene expression proﬁles demonstrate the utility of our method. Keywords: BIC, EM, mixture model, penalized likelihood, soft-thresholding, shrinkage</p><p>4 0.39135602 <a title="22-lda-4" href="./jmlr-2007-Relational_Dependency_Networks.html">72 jmlr-2007-Relational Dependency Networks</a></p>
<p>Author: Jennifer Neville, David Jensen</p><p>Abstract: Recent work on graphical models for relational data has demonstrated signiﬁcant improvements in classiﬁcation and inference when models represent the dependencies among instances. Despite its use in conventional statistical models, the assumption of instance independence is contradicted by most relational data sets. For example, in citation data there are dependencies among the topics of a paper’s references, and in genomic data there are dependencies among the functions of interacting proteins. In this paper, we present relational dependency networks (RDNs), graphical models that are capable of expressing and reasoning with such dependencies in a relational setting. We discuss RDNs in the context of relational Bayes networks and relational Markov networks and outline the relative strengths of RDNs—namely, the ability to represent cyclic dependencies, simple methods for parameter estimation, and efﬁcient structure learning techniques. The strengths of RDNs are due to the use of pseudolikelihood learning techniques, which estimate an efﬁcient approximation of the full joint distribution. We present learned RDNs for a number of real-world data sets and evaluate the models in a prediction context, showing that RDNs identify and exploit cyclic relational dependencies to achieve signiﬁcant performance gains over conventional conditional models. In addition, we use synthetic data to explore model performance under various relational data characteristics, showing that RDN learning and inference techniques are accurate over a wide range of conditions. Keywords: relational learning, probabilistic relational models, knowledge discovery, graphical models, dependency networks, pseudolikelihood estimation</p><p>5 0.38379413 <a title="22-lda-5" href="./jmlr-2007-Multi-class_Protein_Classification_Using_Adaptive_Codes.html">57 jmlr-2007-Multi-class Protein Classification Using Adaptive Codes</a></p>
<p>Author: Iain Melvin, Eugene Ie, Jason Weston, William Stafford Noble, Christina Leslie</p><p>Abstract: Predicting a protein’s structural class from its amino acid sequence is a fundamental problem in computational biology. Recent machine learning work in this domain has focused on developing new input space representations for protein sequences, that is, string kernels, some of which give state-of-the-art performance for the binary prediction task of discriminating between one class and all the others. However, the underlying protein classiﬁcation problem is in fact a huge multiclass problem, with over 1000 protein folds and even more structural subcategories organized into a hierarchy. To handle this challenging many-class problem while taking advantage of progress on the binary problem, we introduce an adaptive code approach in the output space of one-vsthe-rest prediction scores. Speciﬁcally, we use a ranking perceptron algorithm to learn a weighting of binary classiﬁers that improves multi-class prediction with respect to a ﬁxed set of output codes. We use a cross-validation set-up to generate output vectors for training, and we deﬁne codes that capture information about the protein structural hierarchy. Our code weighting approach signiﬁcantly improves on the standard one-vs-all method for two difﬁcult multi-class protein classiﬁcation problems: remote homology detection and fold recognition. Our algorithm also outperforms a previous code learning approach due to Crammer and Singer, trained here using a perceptron, when the dimension of the code vectors is high and the number of classes is large. Finally, we compare against PSI-BLAST, one of the most widely used methods in protein sequence analysis, and ﬁnd that our method strongly outperforms it on every structure clas∗. The ﬁrst two authors contributed equally to this work. c 2007 Iain Melvin, Eugene Ie, Jason Weston, William Stafford Noble and Christina Leslie. M ELVIN , I E , W ESTON , N OBLE AND L ESLIE siﬁcation problem that we consider. Supplementary data and source code are available at http: //www.cs</p><p>6 0.38037986 <a title="22-lda-6" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>7 0.36497074 <a title="22-lda-7" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>8 0.36022234 <a title="22-lda-8" href="./jmlr-2007-Handling_Missing_Values_when_Applying_Classification_Models.html">39 jmlr-2007-Handling Missing Values when Applying Classification Models</a></p>
<p>9 0.35481572 <a title="22-lda-9" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>10 0.35381383 <a title="22-lda-10" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>11 0.35285038 <a title="22-lda-11" href="./jmlr-2007-Loop_Corrections_for_Approximate_Inference_on_Factor_Graphs.html">51 jmlr-2007-Loop Corrections for Approximate Inference on Factor Graphs</a></p>
<p>12 0.34676042 <a title="22-lda-12" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>13 0.33588409 <a title="22-lda-13" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>14 0.33566135 <a title="22-lda-14" href="./jmlr-2007-An_Interior-Point_Method_for_Large-Scalel1-Regularized_Logistic_Regression.html">10 jmlr-2007-An Interior-Point Method for Large-Scalel1-Regularized Logistic Regression</a></p>
<p>15 0.33249995 <a title="22-lda-15" href="./jmlr-2007-Dimensionality_Reduction_of_Multimodal_Labeled_Data_by_Local_Fisher_Discriminant_Analysis.html">26 jmlr-2007-Dimensionality Reduction of Multimodal Labeled Data by Local Fisher Discriminant Analysis</a></p>
<p>16 0.33103076 <a title="22-lda-16" href="./jmlr-2007-Maximum_Entropy_Density_Estimation_with_Generalized_Regularization_and_an_Application_to_Species_Distribution_Modeling.html">53 jmlr-2007-Maximum Entropy Density Estimation with Generalized Regularization and an Application to Species Distribution Modeling</a></p>
<p>17 0.32832009 <a title="22-lda-17" href="./jmlr-2007-A_New_Probabilistic_Approach_in_Rank_Regression_with_Optimal_Bayesian_Partitioning_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">4 jmlr-2007-A New Probabilistic Approach in Rank Regression with Optimal Bayesian Partitioning     (Special Topic on Model Selection)</a></p>
<p>18 0.32704961 <a title="22-lda-18" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>19 0.32620272 <a title="22-lda-19" href="./jmlr-2007-Truncating_the_Loop_Series_Expansion_for_Belief_Propagation.html">86 jmlr-2007-Truncating the Loop Series Expansion for Belief Propagation</a></p>
<p>20 0.32541591 <a title="22-lda-20" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
