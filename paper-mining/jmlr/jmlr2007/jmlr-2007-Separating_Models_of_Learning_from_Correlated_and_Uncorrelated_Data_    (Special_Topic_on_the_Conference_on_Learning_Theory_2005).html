<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</title>
</head>

<body>
<p><a title="jmlr" href="../jmlr_home.html">jmlr</a> <a title="jmlr-2007" href="../home/jmlr2007_home.html">jmlr2007</a> <a title="jmlr-2007-74" href="#">jmlr2007-74</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</h1>
<br/><p>Source: <a title="jmlr-2007-74-pdf" href="http://jmlr.org/papers/volume8/elbaz07a/elbaz07a.pdf">pdf</a></p><p>Author: Ariel Elbaz, Homin K. Lee, Rocco A. Servedio, Andrew Wan</p><p>Abstract: We consider a natural framework of learning from correlated data, in which successive examples used for learning are generated according to a random walk over the space of possible examples. A recent paper by Bshouty et al. (2003) shows that the class of polynomial-size DNF formulas is efﬁciently learnable in this random walk model; this result suggests that the Random Walk model is more powerful than comparable standard models of learning from independent examples, in which similarly efﬁcient DNF learning algorithms are not known. We give strong evidence that the Random Walk model is indeed more powerful than the standard model, by showing that if any cryptographic one-way function exists (a universally held belief in cryptography), then there is a class of functions that can be learned efﬁciently in the Random Walk setting but not in the standard setting where all examples are independent. Keywords: random walks, uniform distribution learning, cryptographic hardness, correlated data, PAC learning</p><p>Reference: <a title="jmlr-2007-74-reference" href="../jmlr2007_reference/jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Keywords: random walks, uniform distribution learning, cryptographic hardness, correlated data, PAC learning  1. [sent-15, score-0.246]
</p><p>2 Introduction It is a commonly held belief in machine learning that having access to correlated data—for example, having random data points that differ only slightly from each other—is advantageous for learning. [sent-16, score-0.245]
</p><p>3 We study a natural model of learning from correlated data, by considering a framework in which the learning algorithm has access to successive examples that are generated by a random walk. [sent-18, score-0.322]
</p><p>4 Aldous and Vazirani (1990) introduced and studied a variant of the PAC learning model in which successive examples are generated according to a Markov process, that is, by taking a random walk on an (exponentially large) graph. [sent-28, score-0.56]
</p><p>5 In this framework (described in detail in Section 2), successive examples for the learning algorithm are produced sequentially according to an unbiased random walk on the Boolean hypercube {0, 1} n . [sent-35, score-0.59]
</p><p>6 The PAC goal of constructing a highaccuracy hypothesis for the target concept with high probability (where accuracy is measured with respect to the stationary distribution of the random walk, that is, the uniform distribution on {0, 1} n ) is unchanged. [sent-36, score-0.283]
</p><p>7 Since proper learning algorithms were not known for these concept classes in the standard uniform distribution model, this gave the ﬁrst evidence that having access to random walk examples rather than uniform independent examples might bestow a computational advantage. [sent-48, score-0.934]
</p><p>8 Thus, it is natural to now ask whether the superiority of random walk learning over uniform distribution learning can be established under some widely accepted hypothesis about efﬁcient computation. [sent-52, score-0.584]
</p><p>9 2 Our Results In this work we give a separation, under a generic cryptographic hardness assumption, between the Random Walk model and the uniform distribution model. [sent-57, score-0.202]
</p><p>10 The rest of the paper is structured as follows: Section 2 gives necessary deﬁnitions and background from cryptography and the basics of our random walk model. [sent-64, score-0.566]
</p><p>11 For an n-bit string r ∈ {0, 1} n and an index i ∈ [n], the i-th bit of r is denoted r[i]. [sent-71, score-0.217]
</p><p>12 A uniform example oracle for f is an oracle EXU ( f ) which takes no inputs and, when invoked, outputs a pair x, f (x) where x is drawn uniformly and independently from {0, 1} n at each invocation. [sent-75, score-0.842]
</p><p>13 (2003), a random walk oracle is an oracle EXRW ( f ) which, at its ﬁrst invocation, outputs an example x, f (x) where x is drawn uniformly at random from {0, 1}n . [sent-79, score-1.274]
</p><p>14 Subsequent calls to EXRW ( f ) yield examples generated according to a uniform random walk on the hypercube {0, 1} n . [sent-80, score-0.653]
</p><p>15 (2003), it is convenient for us to work with a slight variant of the Random Walk oracle which is of equivalent power; we call this the Updating Random Walk oracle and denote it by EXURW ( f ). [sent-84, score-0.636]
</p><p>16 If the last example generated by EXURW ( f ) was x ∈ {0, 1}n , the Updating Random Walk oracle chooses a uniform index i ∈ [n], but instead of ﬂipping the bit x[i] it replaces x[i] with a uniform random bit from {0, 1} (i. [sent-85, score-0.818]
</p><p>17 , it ﬂips the bit with probability 1/2 and leaves x unchanged with probability 1/2) to obtain the new example x . [sent-87, score-0.183]
</p><p>18 We say that such a step updates the i-th bit position. [sent-88, score-0.143]
</p><p>19 It is easy to simulate an Updating Random Walk oracle using a Random Walk oracle by, at each time step, ﬂipping a fair coin and with probability 1/2 returning the previous example and with probability 1/2 requesting a new example. [sent-89, score-0.701]
</p><p>20 Similarly, it is easy to simulate a Random Walk oracle using an Updating Random Walk oracle by invoking the Updating Random Walk oracle until it ﬂips a bit (the probability that this takes more than k invocations is at most 2 −k ). [sent-90, score-1.117]
</p><p>21 Thus any concept class that is efﬁciently learnable from one oracle is also efﬁciently learnable from the other. [sent-91, score-0.53]
</p><p>22 We introduce the Updating Random Walk oracle because it is easy to see (and well known) that the updating random walk on the hypercube mixes rapidly. [sent-92, score-0.913]
</p><p>23 This follows since y will be independent of x if and only if all n bit positions are updated in the n ln n draws between x and y. [sent-96, score-0.201]
</p><p>24 For each draw, the probability that a particular bit is δ not updated is (1 − 1 ). [sent-97, score-0.163]
</p><p>25 Thus after n ln n draws, the probability that any bit of r has not been updated n n δ 1 is at most n(1 − n )n ln δ ≤ δ. [sent-98, score-0.209]
</p><p>26 Note that Fact 4 implies that any concept class C that is uniform distribution PAC-learnable is also PAC-learnable in the Random Walk model, since we can obtain independent uniform random examples in the Random Walk model with essentially just a Θ(n log n) slowdown. [sent-100, score-0.373]
</p><p>27 We refer to a function f chosen uniformly at random from Rn as a truly random function. [sent-103, score-0.316]
</p><p>28 ) algorithm D with black-box oracle access to the function f . [sent-107, score-0.457]
</p><p>29 oracle algorithms D, and all sufﬁciently large n, we have that Pr [D f (1n ) outputs 1] −  f ∈Rn  Pr n [D fs (1n ) outputs 1] <  s∈{0,1}  1 . [sent-118, score-0.512]
</p><p>30 Intuitively, condition (2) above states that a pseudorandom function cannot be distinguished from a truly random function by any polynomial-time algorithm that has black-box access to the pseudorandom function with an inverse polynomial advantage over random guessing. [sent-120, score-0.7]
</p><p>31 , s[k] of the seed s (by waiting for pairs of successive examples (x, i, b), (x, i, 1 − b) in which the ﬁnal bit b ﬂips for all k possible values of i), and will thus be able to exactly identify the target concept. [sent-130, score-0.342]
</p><p>32 However, even though a standard uniform distribution learner will not obtain any pair of inputs that differ only in the ﬁnal bit b, it is not clear how to show that no algorithm in the standard uniform distribution model can learn the concept class to high accuracy. [sent-131, score-0.504]
</p><p>33 Such a proof would require one to show that any polynomial-time uniform distribution learning algorithm could be used to “break” the pseudorandom function family { f s }, and this seems difﬁcult to do. [sent-132, score-0.206]
</p><p>34 (Intuitively, this difﬁculty arises because the b = 1 case of the deﬁnition of f s “mixes” bits of the seed with the output of the pseudorandom function, and because of this it is not clear how to simulate f s given black-box access to fs for an unknown seed s. [sent-133, score-0.579]
</p><p>35 1; as in the construction proposed there, the concepts in our class will reveal information about the seed of a pseudorandom function to learning algorithms that can obtain pairs of points with only the last bit ﬂipped. [sent-139, score-0.401]
</p><p>36   fs (x)  gr,s (x, i, b, y) = fs (x) ⊕ r[i]   fr (x)  if y = 0, b = 0, if y = 0, b = 1, if y = 1. [sent-144, score-0.204]
</p><p>37 Here b and y are one bit and i is log k bits to indicate which bit of the seed r is exposed. [sent-145, score-0.464]
</p><p>38 (A standard argument shows that an efﬁcient PAC learning algorithm can be used to obtain an efﬁcient distinguisher simply by running the learning algorithm and using its hypothesis to predict a fresh random example. [sent-148, score-0.232]
</p><p>39 Such an approach must succeed with high probability for any function from the concept class by virtue of the PAC criterion, but no algorithm that has seen only poly(n) many examples of a truly random function can predict its outputs on fresh examples with probability nonnegligibly greater than 1 . [sent-149, score-0.503]
</p><p>40 Then for any ε = Ω( poly(n) ), algorithm A cannot distinguish between having oracle access to EXU (gr,s ) versus oracle access to EXU ( f ) with success probability greater than 1 2 + ε. [sent-156, score-0.984]
</p><p>41 We will construct two intermediate functions, h r and hr . [sent-158, score-0.249]
</p><p>42 Consider the function   f (x) if y = 0, b = 0,  hr (x, i, b, y) = f (x) ⊕ r[i] if y = 0, b = 1, (1)   fr (x) if y = 1. [sent-161, score-0.281]
</p><p>43 Here we have simply replaced f s with a truly random function. [sent-162, score-0.215]
</p><p>44 algorithm can distinguish oracle access to EXU (gr,s ) from oracle access to EXU (hr ); for if such a distinguisher D existed, we could use it to obtain an algorithm D to distinguish a randomly chosen f s ∈ F from a truly random function in the following way. [sent-166, score-1.27]
</p><p>45 D picks r at random from {0, 1}k and runs D, answering D’s queries to its oracle by choosing i, b and y at random, querying its own oracle to receive a bit q, and outputting q when both y and b are 0, q⊕r[i] when y = 0 and b = 1, and f r (x) when y = 1. [sent-167, score-0.853]
</p><p>46 It is easy to see that if D ’s oracle is for a truly random function f ∈ R then this process perfectly simulates access to EXU (hr ), and if D ’s oracle is for a randomly chosen f s ∈ F then this process perfectly simulates access to EXU (gr,s ) for r, s chosen uniformly at random. [sent-168, score-1.27]
</p><p>47 We now consider the intermediate function hr (x, i, b, y) =  f (x) fr (x)  if y = 0, if y = 1,  and argue that no algorithm that makes only poly(n) many oracle calls can distinguish oracle access to EXU (hr ) from access to EXU (hr ) with non-negligible success probability. [sent-169, score-1.284]
</p><p>48 When y = 1 or both y = 0 and b = 0, both hr and hr will have the same output. [sent-170, score-0.498]
</p><p>49 Otherwise, if y = 0 and b = 1 we have that hr (x, i, b, y) = f (x) ⊕ ri whereas hr (x, i, b, y) = f (x). [sent-171, score-0.498]
</p><p>50 Now, it is easy to see that an algorithm with black-box query access to hr can easily distinguish hr from hr (simply because ﬂipping the penultimate bit b will always cause the value of hr to ﬂip but will only cause the value of hr to ﬂip half of the time). [sent-172, score-1.556]
</p><p>51 (This is simply because a random function f can be viewed as tossing a coin to determine its output on each new input value, so no matter what r[i] is, XORing it with f (x) yields a fresh independent uniform random bit. [sent-174, score-0.287]
</p><p>52 algorithm can distinguish oracle access to EXU (hr ) from access to EXU ( f ). [sent-178, score-0.625]
</p><p>53 Proof As described in Section 2, for convenience in this proof we will assume that we have an Updating Random Walk oracle EXURW (gr,s ). [sent-184, score-0.318]
</p><p>54 Once the learner has obtained r she outputs the following (randomized) hypothesis h: h(x, i, b, y) =  $ fr (x)  if y = 0, if y = 1,  where $ denotes a random coin toss at each invocation. [sent-186, score-0.271]
</p><p>55 We now show that with probability 1−δ (over the random examples received from EXURW (gr,s )) the learner can obtain all of r after receiving T = O(n2 k ·log2 (n/δ)) many examples from EXURW (gr,s ). [sent-189, score-0.235]
</p><p>56 The learner does this by looking at pairs of successive examples; we show (Fact 10 below) that after seeing t = O(nk · log(k/δ)) pairs, each of which is independent from all other pairs, we obtain all δ of r with probability at least 1 − 2 . [sent-190, score-0.199]
</p><p>57 To get t independent pairs of successive examples, we look at blocks of t = O(n log(tn/δ)) many consecutive examples, and use only the ﬁrst two examples from each such block. [sent-191, score-0.188]
</p><p>58 By Fact 4 we have that for a given pair of consecutive blocks, with probability δ at least 1 − 2t the ﬁrst example from the second block is random even given the pair of examples δ from the ﬁrst block. [sent-192, score-0.173]
</p><p>59 We have the following simple facts: Fact 8 If the learner receives two consecutive examples w = (x, i, 0, 0), w = (x, i, 1, 0) and the corresponding labels gr,s (w), gr,s (w ), then the learner can obtain the bit r[i]. [sent-194, score-0.332]
</p><p>60 Fact 10 After receiving t = 4kn · log(k/δ ) independent pairs of consecutive examples as described above, the learner can obtain all k bits of r with probability at least 1 − δ . [sent-199, score-0.278]
</p><p>61 Thus after seeing t independent pairs of consecutive examples, the 1 probability that any bit of r is not obtained is at most k(1 − 4kn )t . [sent-201, score-0.289]
</p><p>62 A Full Separation We would like to have a concept class for which a Random Walk learner can output an ε-accurate hypothesis for any ε > 0. [sent-206, score-0.174]
</p><p>63 Intuitively, a Random Walk 4 learner cannot achieve accuracy better than 3 because on half of the inputs the concept’s value is 4 essentially determined by a pseudorandom function whose seed the Random Walk learner cannot 1 discover. [sent-209, score-0.381]
</p><p>64 It is not difﬁcult to see that for any given ε = poly(n) , by altering the parameters of the construction we could obtain a concept class that a Random Walk algorithm can learn to accuracy 1−ε (and which would still be unlearnable for a standard uniform distribution algorithm). [sent-210, score-0.181]
</p><p>65 However, this would give us a different concept class for each ε, whereas what we require is a single concept class that can be learned to accuracy ε for each ε > 0. [sent-211, score-0.176]
</p><p>66 Instead of depending on two seeds r, s, a concept in G is deﬁned using k seeds r1 , . [sent-215, score-0.164]
</p><p>67 By this design, the subfunction gr j ,r j+1 will be used on a 1/2 j fraction of the inputs to g . [sent-244, score-0.149]
</p><p>68 , rk uniformly at random from {0, 1}k , where k satisﬁes n = 2k + log k + 1. [sent-258, score-0.195]
</p><p>69 Then for any ε = Ω( poly(n) ), algorithm A cannot distinguish between  1 having access to EXU (gr1 ,. [sent-260, score-0.168]
</p><p>70 ,rk ) versus access to EXU ( f ) with success probability greater than 2 + ε. [sent-263, score-0.18]
</p><p>71 , r are chosen randomly from {0, 1}k and f is a truly random function from Rk . [sent-290, score-0.215]
</p><p>72 Using Lemma 6, it is easy to see that for a distinguisher that is given only oracle access to EXU (·), a random function from H (2) is indistinguishable from a truly random function from R n . [sent-291, score-0.913]
</p><p>73 We will now show that, for 2 ≤ < k, if a random function from H ( ) is indistinguishable from a truly random function then the same is true for H ( + 1). [sent-292, score-0.373]
</p><p>74 This will then imply that a random function from H (k) is indistinguishable from a truly random function. [sent-293, score-0.373]
</p><p>75 ,r +1 ; f be taken randomly from H ( + 1) and f be a truly random function from R n . [sent-297, score-0.215]
</p><p>76 Suppose we had a distinguisher D that distinguishes between a random function from H ( + 1) and 1 a truly random function from Rn with success probability 1 + ε, where ε = Ω( poly(n) ). [sent-298, score-0.413]
</p><p>77 D then runs D, simulating its oracle in the following way. [sent-304, score-0.318]
</p><p>78 At each invocation, D draws a random (x, i, b, y, z) and behaves as follows: 286  S EPARATING M ODELS OF L EARNING FROM C ORRELATED AND U NCORRELATED DATA  • If α(z) < , then D outputs (x, i, b, y, z), grα(z) ,rα(z)+1 (x, i, b, y) . [sent-305, score-0.163]
</p><p>79 • If α(z) = , then D calls its oracle to obtain x , β . [sent-306, score-0.357]
</p><p>80 • If α(z) > , D outputs the labeled example (x, i, b, y, z), r(x) where r(x) is a fresh random bit for each x. [sent-310, score-0.315]
</p><p>81 (The pairs (x, r(x)) are stored, and if any k-bit string x is drawn twice—which is exponentially unlikely in a sequence of poly(n) many draws—D uses the same bit r(x) as before. [sent-311, score-0.259]
</p><p>82 ) It is straightforward to check that if D ’s oracle is EXU ( fs ) for a random f s ∈ F , then D simulates an oracle EXU (hr1 ,. [sent-312, score-0.853]
</p><p>83 On the other hand, we claim that if D ’s oracle is EXU ( f ) for a random f ∈ Rk , then D simulates an oracle that is indistinguishable from EXU (hr1 ,. [sent-319, score-0.851]
</p><p>84 Clearly the oracle D simulates is identical to EXU (hr1 ,. [sent-326, score-0.375]
</p><p>85 For α(z) = , D simulates the function hr as in Equation 1 in the proof of Lemma 6, which is indistinguishable from a truly random function as proved in the lemma. [sent-330, score-0.605]
</p><p>86 Thus the success probability of the distinguisher D is the same as the probability that D succeeds in distinguishing H ( + 1) from H ( ). [sent-331, score-0.211]
</p><p>87 Recall that H ( ) is indistinguishable from a truly random function, and that D succeeds in distinguishing H ( + 1) from a truly random function with probability at least 1 + ε by assumption. [sent-332, score-0.601]
</p><p>88 This implies that D succeeds in distinguishing a randomly 2 1 chosen fs ∈ F from a randomly chosen function f ∈ Rk with probability at least 1 + ε − ω(poly(n)) , 2 but this contradicts the pseudorandomness of F . [sent-333, score-0.214]
</p><p>89 algorithm, having oracle access to a random function from H (k) is indistinguishable from having oracle access to a random function from G . [sent-337, score-1.146]
</p><p>90 ,rk ∈ G , if B is given access to a Random Walk oracle EXRW (gr1 ,. [sent-360, score-0.457]
</p><p>91 Again, for convenience we will assume that we have an Updating Random Walk oracle EXURW (gr1 ,. [sent-368, score-0.318]
</p><p>92 Recall from Lemma 7 that there is an algorithm A that can obtain the string r j with probability at least 1 − δ given t = O(nk · log(n/δ )) independent pairs of successive random walk examples w, gr j ,r j+1 (w) , w , gr j ,r j+1 (w ) . [sent-372, score-0.884]
</p><p>93 In stage j, the algorithm simply tries to obtain t independent example pairs for gr j ,r j+1 and then uses Algorithm A. [sent-383, score-0.175]
</p><p>94 Algorithm B fails to acquire t pairs of examples for gr j ,r j+1 in some stage j; or 3. [sent-398, score-0.206]
</p><p>95 Algorithm B acquires t pairs of examples for gr j ,r j+1 but Algorithm A fails to obtain r j in some stage j. [sent-399, score-0.206]
</p><p>96 Observe that each pair of examples has both examples from gr j ,r j+1 with probability at least 2−( j+1) . [sent-406, score-0.176]
</p><p>97 By a Chernoff bound, the probability that less than t of the t  example pairs in stage j are from gr j ,r j+1 is at most e− 8 . [sent-407, score-0.195]
</p><p>98 In stage j, we know by Fact 10 that after seeing t = O(nk log(3vk/δ)) pairs of examples for gr j ,r j+1 , the probability of failing to obtain r j is at most δ/3v. [sent-410, score-0.262]
</p><p>99 It follows that the overall number of examples required from the Updating Random Walk oracle is poly(2v , n, log 1 ) = poly(n, 1 , log 1 ), which is what we required. [sent-416, score-0.429]
</p><p>100 On learning thresholds of parities and unions of rectangles in random walk models. [sent-510, score-0.506]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('exu', 0.467), ('walk', 0.409), ('oracle', 0.318), ('hr', 0.249), ('poly', 0.249), ('exurw', 0.166), ('bit', 0.143), ('truly', 0.141), ('access', 0.139), ('pseudorandom', 0.136), ('wan', 0.11), ('ervedio', 0.097), ('lbaz', 0.097), ('bshouty', 0.096), ('gr', 0.094), ('dnf', 0.094), ('concept', 0.088), ('pac', 0.087), ('fs', 0.086), ('indistinguishable', 0.084), ('cryptography', 0.083), ('distinguisher', 0.083), ('eparating', 0.083), ('exrw', 0.083), ('ncorrelated', 0.083), ('orrelated', 0.083), ('seed', 0.08), ('string', 0.074), ('random', 0.074), ('uniform', 0.07), ('cryptographic', 0.07), ('hardness', 0.062), ('learnable', 0.062), ('updating', 0.061), ('bits', 0.058), ('simulates', 0.057), ('learner', 0.055), ('ee', 0.055), ('inputs', 0.055), ('outputs', 0.054), ('rk', 0.054), ('columbia', 0.048), ('consecutive', 0.048), ('odels', 0.047), ('ipping', 0.047), ('successive', 0.046), ('jackson', 0.044), ('fresh', 0.044), ('pairs', 0.042), ('aldous', 0.041), ('homin', 0.041), ('pseudorandomness', 0.041), ('failure', 0.04), ('log', 0.04), ('calls', 0.039), ('stage', 0.039), ('rv', 0.038), ('seeds', 0.038), ('boolean', 0.037), ('ips', 0.036), ('succeeds', 0.036), ('seeing', 0.036), ('draws', 0.035), ('goldreich', 0.035), ('rocco', 0.035), ('nk', 0.034), ('correlated', 0.032), ('fr', 0.032), ('distinguishing', 0.031), ('hypothesis', 0.031), ('examples', 0.031), ('separation', 0.031), ('lemma', 0.03), ('hypercube', 0.03), ('distinguish', 0.029), ('ariel', 0.028), ('elbaz', 0.028), ('frk', 0.028), ('gamarnik', 0.028), ('pru', 0.028), ('stad', 0.028), ('uniformly', 0.027), ('np', 0.027), ('earning', 0.026), ('coin', 0.025), ('receiving', 0.024), ('invocation', 0.023), ('parities', 0.023), ('remind', 0.023), ('subfunctions', 0.023), ('learn', 0.023), ('independence', 0.023), ('ln', 0.023), ('gave', 0.022), ('blocks', 0.021), ('fourier', 0.021), ('success', 0.021), ('linial', 0.021), ('mixes', 0.021), ('probability', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="74-tfidf-1" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ariel Elbaz, Homin K. Lee, Rocco A. Servedio, Andrew Wan</p><p>Abstract: We consider a natural framework of learning from correlated data, in which successive examples used for learning are generated according to a random walk over the space of possible examples. A recent paper by Bshouty et al. (2003) shows that the class of polynomial-size DNF formulas is efﬁciently learnable in this random walk model; this result suggests that the Random Walk model is more powerful than comparable standard models of learning from independent examples, in which similarly efﬁcient DNF learning algorithms are not known. We give strong evidence that the Random Walk model is indeed more powerful than the standard model, by showing that if any cryptographic one-way function exists (a universally held belief in cryptography), then there is a class of functions that can be learned efﬁciently in the Random Walk setting but not in the standard setting where all examples are independent. Keywords: random walks, uniform distribution learning, cryptographic hardness, correlated data, PAC learning</p><p>2 0.20298149 <a title="74-tfidf-2" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Vitaly Feldman</p><p>Abstract: We consider the problems of attribute-efﬁcient PAC learning of two well-studied concept classes: parity functions and DNF expressions over {0, 1}n . We show that attribute-efﬁcient learning of parities with respect to the uniform distribution is equivalent to decoding high-rate random linear codes from low number of errors, a long-standing open problem in coding theory. This is the ﬁrst evidence that attribute-efﬁcient learning of a natural PAC learnable concept class can be computationally hard. An algorithm is said to use membership queries (MQs) non-adaptively if the points at which the algorithm asks MQs do not depend on the target concept. Using a simple non-adaptive parity learning algorithm and a modiﬁcation of Levin’s algorithm for locating a weakly-correlated parity due to Bshouty et al. (1999), we give the ﬁrst non-adaptive and attribute-efﬁcient algorithm for ˜ learning DNF with respect to the uniform distribution. Our algorithm runs in time O(ns4 /ε) and ˜ uses O(s4 · log2 n/ε) non-adaptive MQs, where s is the number of terms in the shortest DNF representation of the target concept. The algorithm improves on the best previous algorithm for learning DNF (of Bshouty et al., 1999) and can also be easily modiﬁed to tolerate random persistent classiﬁcation noise in MQs. Keywords: attribute-efﬁcient, non-adaptive, membership query, DNF, parity function, random linear code</p><p>3 0.11474141 <a title="74-tfidf-3" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>Author: Manu Chhabra, Robert A. Jacobs, Daniel Štefankovič</p><p>Abstract: In a search problem, an agent uses the membership oracle of a target concept to ﬁnd a positive example of the concept. In a shaped search problem the agent is aided by a sequence of increasingly restrictive concepts leading to the target concept (analogous to behavioral shaping). The concepts are given by membership oracles, and the agent has to ﬁnd a positive example of the target concept while minimizing the total number of oracle queries. We show that for the concept class of intervals on the real line an agent using a bounded number of queries per oracle exists. In contrast, for the concept class of unions of two intervals on the real line no agent with a bounded number of queries per oracle exists. We then investigate the (amortized) number of queries per oracle needed for the shaped search problem over other concept classes. We explore the following methods to obtain efﬁcient agents. For axis-parallel rectangles we use a bootstrapping technique to obtain gradually better approximations of the target concept. We show that given rectangles R ⊆ A ⊆ R d one can obtain a rectangle A ⊇ R with vol(A )/vol(R) ≤ 2, using only O(d · vol(A)/vol(R)) random samples from A. For ellipsoids of bounded eccentricity in Rd we analyze a deterministic ray-shooting process which uses a sequence of rays to get close to the centroid. Finally, we use algorithms for generating random points in convex bodies (Dyer et al., 1991; Kannan et al., 1997) to give a randomized agent for the concept class of convex bodies. In the ﬁnal section, we explore connections between our bootstrapping method and active learning. Speciﬁcally, we use the bootstrapping technique for axis-parallel rectangles to active learn axis-parallel rectangles under the uniform distribution in O(d ln(1/ε)) labeled samples. Keywords: computational learning theory, behavioral shaping, active learning</p><p>4 0.054182533 <a title="74-tfidf-4" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>Author: Stéphan Clémençon, Nicolas Vayatis</p><p>Abstract: We formulate a local form of the bipartite ranking problem where the goal is to focus on the best instances. We propose a methodology based on the construction of real-valued scoring functions. We study empirical risk minimization of dedicated statistics which involve empirical quantiles of the scores. We ﬁrst state the problem of ﬁnding the best instances which can be cast as a classiﬁcation problem with mass constraint. Next, we develop special performance measures for the local ranking problem which extend the Area Under an ROC Curve (AUC) criterion and describe the optimal elements of these new criteria. We also highlight the fact that the goal of ranking the best instances cannot be achieved in a stage-wise manner where ﬁrst, the best instances would be tentatively identiﬁed and then a standard AUC criterion could be applied. Eventually, we state preliminary statistical results for the local ranking problem. Keywords: ranking, ROC curve and AUC, empirical risk minimization, fast rates</p><p>5 0.05112756 <a title="74-tfidf-5" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Matthias Hein, Jean-Yves Audibert, Ulrike von Luxburg</p><p>Abstract: Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. Keywords: graphs, graph Laplacians, semi-supervised learning, spectral clustering, dimensionality reduction</p><p>6 0.04755548 <a title="74-tfidf-6" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>7 0.032404069 <a title="74-tfidf-7" href="./jmlr-2007-The_On-Line_Shortest_Path_Problem_Under_Partial_Monitoring.html">83 jmlr-2007-The On-Line Shortest Path Problem Under Partial Monitoring</a></p>
<p>8 0.031762797 <a title="74-tfidf-8" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>9 0.031389829 <a title="74-tfidf-9" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>10 0.02975476 <a title="74-tfidf-10" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>11 0.026764043 <a title="74-tfidf-11" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>12 0.025366666 <a title="74-tfidf-12" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>13 0.024190653 <a title="74-tfidf-13" href="./jmlr-2007-A_Generalized_Maximum_Entropy_Approach_to_Bregman_Co-clustering_and_Matrix_Approximation.html">3 jmlr-2007-A Generalized Maximum Entropy Approach to Bregman Co-clustering and Matrix Approximation</a></p>
<p>14 0.024136735 <a title="74-tfidf-14" href="./jmlr-2007-Polynomial_Identification_in_the_Limit_of_Substitutable_Context-free_Languages.html">67 jmlr-2007-Polynomial Identification in the Limit of Substitutable Context-free Languages</a></p>
<p>15 0.022543419 <a title="74-tfidf-15" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>16 0.022482377 <a title="74-tfidf-16" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>17 0.022240635 <a title="74-tfidf-17" href="./jmlr-2007-A_Probabilistic_Analysis_of_EM_for_Mixtures_of_Separated%2C_Spherical_Gaussians.html">6 jmlr-2007-A Probabilistic Analysis of EM for Mixtures of Separated, Spherical Gaussians</a></p>
<p>18 0.022074251 <a title="74-tfidf-18" href="./jmlr-2007-AdaBoost_is_Consistent.html">9 jmlr-2007-AdaBoost is Consistent</a></p>
<p>19 0.02168717 <a title="74-tfidf-19" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>20 0.020714575 <a title="74-tfidf-20" href="./jmlr-2007-VC_Theory_of_Large_Margin_Multi-Category_Classifiers_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">89 jmlr-2007-VC Theory of Large Margin Multi-Category Classifiers     (Special Topic on Model Selection)</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/jmlr2007_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.148), (1, -0.027), (2, -0.112), (3, -0.072), (4, -0.136), (5, 0.145), (6, 0.15), (7, 0.349), (8, 0.069), (9, -0.207), (10, -0.317), (11, 0.079), (12, 0.122), (13, 0.054), (14, 0.172), (15, -0.121), (16, -0.046), (17, 0.059), (18, -0.154), (19, 0.041), (20, 0.031), (21, -0.117), (22, 0.102), (23, 0.158), (24, 0.02), (25, -0.07), (26, -0.011), (27, 0.033), (28, 0.02), (29, 0.009), (30, -0.006), (31, -0.101), (32, 0.055), (33, 0.094), (34, -0.067), (35, -0.013), (36, 0.051), (37, 0.014), (38, -0.021), (39, -0.023), (40, 0.025), (41, -0.097), (42, -0.047), (43, -0.03), (44, -0.07), (45, -0.075), (46, -0.093), (47, -0.026), (48, 0.031), (49, -0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97572201 <a title="74-lsi-1" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ariel Elbaz, Homin K. Lee, Rocco A. Servedio, Andrew Wan</p><p>Abstract: We consider a natural framework of learning from correlated data, in which successive examples used for learning are generated according to a random walk over the space of possible examples. A recent paper by Bshouty et al. (2003) shows that the class of polynomial-size DNF formulas is efﬁciently learnable in this random walk model; this result suggests that the Random Walk model is more powerful than comparable standard models of learning from independent examples, in which similarly efﬁcient DNF learning algorithms are not known. We give strong evidence that the Random Walk model is indeed more powerful than the standard model, by showing that if any cryptographic one-way function exists (a universally held belief in cryptography), then there is a class of functions that can be learned efﬁciently in the Random Walk setting but not in the standard setting where all examples are independent. Keywords: random walks, uniform distribution learning, cryptographic hardness, correlated data, PAC learning</p><p>2 0.8334111 <a title="74-lsi-2" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Vitaly Feldman</p><p>Abstract: We consider the problems of attribute-efﬁcient PAC learning of two well-studied concept classes: parity functions and DNF expressions over {0, 1}n . We show that attribute-efﬁcient learning of parities with respect to the uniform distribution is equivalent to decoding high-rate random linear codes from low number of errors, a long-standing open problem in coding theory. This is the ﬁrst evidence that attribute-efﬁcient learning of a natural PAC learnable concept class can be computationally hard. An algorithm is said to use membership queries (MQs) non-adaptively if the points at which the algorithm asks MQs do not depend on the target concept. Using a simple non-adaptive parity learning algorithm and a modiﬁcation of Levin’s algorithm for locating a weakly-correlated parity due to Bshouty et al. (1999), we give the ﬁrst non-adaptive and attribute-efﬁcient algorithm for ˜ learning DNF with respect to the uniform distribution. Our algorithm runs in time O(ns4 /ε) and ˜ uses O(s4 · log2 n/ε) non-adaptive MQs, where s is the number of terms in the shortest DNF representation of the target concept. The algorithm improves on the best previous algorithm for learning DNF (of Bshouty et al., 1999) and can also be easily modiﬁed to tolerate random persistent classiﬁcation noise in MQs. Keywords: attribute-efﬁcient, non-adaptive, membership query, DNF, parity function, random linear code</p><p>3 0.50748897 <a title="74-lsi-3" href="./jmlr-2007-Behavioral_Shaping_for_Geometric_Concepts.html">14 jmlr-2007-Behavioral Shaping for Geometric Concepts</a></p>
<p>Author: Manu Chhabra, Robert A. Jacobs, Daniel Štefankovič</p><p>Abstract: In a search problem, an agent uses the membership oracle of a target concept to ﬁnd a positive example of the concept. In a shaped search problem the agent is aided by a sequence of increasingly restrictive concepts leading to the target concept (analogous to behavioral shaping). The concepts are given by membership oracles, and the agent has to ﬁnd a positive example of the target concept while minimizing the total number of oracle queries. We show that for the concept class of intervals on the real line an agent using a bounded number of queries per oracle exists. In contrast, for the concept class of unions of two intervals on the real line no agent with a bounded number of queries per oracle exists. We then investigate the (amortized) number of queries per oracle needed for the shaped search problem over other concept classes. We explore the following methods to obtain efﬁcient agents. For axis-parallel rectangles we use a bootstrapping technique to obtain gradually better approximations of the target concept. We show that given rectangles R ⊆ A ⊆ R d one can obtain a rectangle A ⊇ R with vol(A )/vol(R) ≤ 2, using only O(d · vol(A)/vol(R)) random samples from A. For ellipsoids of bounded eccentricity in Rd we analyze a deterministic ray-shooting process which uses a sequence of rays to get close to the centroid. Finally, we use algorithms for generating random points in convex bodies (Dyer et al., 1991; Kannan et al., 1997) to give a randomized agent for the concept class of convex bodies. In the ﬁnal section, we explore connections between our bootstrapping method and active learning. Speciﬁcally, we use the bootstrapping technique for axis-parallel rectangles to active learn axis-parallel rectangles under the uniform distribution in O(d ln(1/ε)) labeled samples. Keywords: computational learning theory, behavioral shaping, active learning</p><p>4 0.18325853 <a title="74-lsi-4" href="./jmlr-2007-Graph_Laplacians_and_their_Convergence_on_Random_Neighborhood_Graphs_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">38 jmlr-2007-Graph Laplacians and their Convergence on Random Neighborhood Graphs     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Matthias Hein, Jean-Yves Audibert, Ulrike von Luxburg</p><p>Abstract: Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator. Keywords: graphs, graph Laplacians, semi-supervised learning, spectral clustering, dimensionality reduction</p><p>5 0.16326465 <a title="74-lsi-5" href="./jmlr-2007-Proto-value_Functions%3A_A_Laplacian_Framework_for_Learning_Representation_and_Control_in_Markov_Decision_Processes.html">69 jmlr-2007-Proto-value Functions: A Laplacian Framework for Learning Representation and Control in Markov Decision Processes</a></p>
<p>Author: Sridhar Mahadevan, Mauro Maggioni</p><p>Abstract: This paper introduces a novel spectral framework for solving Markov decision processes (MDPs) by jointly learning representations and optimal policies. The major components of the framework described in this paper include: (i) A general scheme for constructing representations or basis functions by diagonalizing symmetric diffusion operators (ii) A speciﬁc instantiation of this approach where global basis functions called proto-value functions (PVFs) are formed using the eigenvectors of the graph Laplacian on an undirected graph formed from state transitions induced by the MDP (iii) A three-phased procedure called representation policy iteration comprising of a sample collection phase, a representation learning phase that constructs basis functions from samples, and a ﬁnal parameter estimation phase that determines an (approximately) optimal policy within the (linear) subspace spanned by the (current) basis functions. (iv) A speciﬁc instantiation of the RPI framework using least-squares policy iteration (LSPI) as the parameter estimation method (v) Several strategies for scaling the proposed approach to large discrete and continuous state spaces, including the Nystr¨ m extension for out-of-sample interpolation of eigenfunctions, and the use of Kronecker o sum factorization to construct compact eigenfunctions in product spaces such as factored MDPs (vi) Finally, a series of illustrative discrete and continuous control tasks, which both illustrate the concepts and provide a benchmark for evaluating the proposed approach. Many challenges remain to be addressed in scaling the proposed framework to large MDPs, and several elaboration of the proposed framework are brieﬂy summarized at the end. Keywords: Markov decision processes, reinforcement learning, value function approximation, manifold learning, spectral graph theory</p><p>6 0.15605201 <a title="74-lsi-6" href="./jmlr-2007-Learning_in_Environments_with_Unknown_Dynamics%3A_Towards_more_Robust_Concept_Learners.html">48 jmlr-2007-Learning in Environments with Unknown Dynamics: Towards more Robust Concept Learners</a></p>
<p>7 0.15417412 <a title="74-lsi-7" href="./jmlr-2007-Ranking_the_Best_Instances.html">70 jmlr-2007-Ranking the Best Instances</a></p>
<p>8 0.15145466 <a title="74-lsi-8" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>9 0.14019965 <a title="74-lsi-9" href="./jmlr-2007-Unlabeled_Compression_Schemes_for_Maximum_Classes.html">88 jmlr-2007-Unlabeled Compression Schemes for Maximum Classes</a></p>
<p>10 0.1336818 <a title="74-lsi-10" href="./jmlr-2007-Characterizing_the_Function_Space_for_Bayesian_Kernel_Models.html">18 jmlr-2007-Characterizing the Function Space for Bayesian Kernel Models</a></p>
<p>11 0.13323615 <a title="74-lsi-11" href="./jmlr-2007-Dynamic_Conditional_Random_Fields%3A_Factorized_Probabilistic_Models_for_Labeling_and_Segmenting_Sequence_Data.html">28 jmlr-2007-Dynamic Conditional Random Fields: Factorized Probabilistic Models for Labeling and Segmenting Sequence Data</a></p>
<p>12 0.1170802 <a title="74-lsi-12" href="./jmlr-2007-Learning_Horn_Expressions_with_LOGAN-H.html">47 jmlr-2007-Learning Horn Expressions with LOGAN-H</a></p>
<p>13 0.11601274 <a title="74-lsi-13" href="./jmlr-2007-Minimax_Regret_Classifier_for_Imprecise_Class_Distributions.html">55 jmlr-2007-Minimax Regret Classifier for Imprecise Class Distributions</a></p>
<p>14 0.11354592 <a title="74-lsi-14" href="./jmlr-2007-Nonlinear_Estimators_and_Tail_Bounds_for_Dimension_Reduction_inl1Using_Cauchy_Random_Projections.html">60 jmlr-2007-Nonlinear Estimators and Tail Bounds for Dimension Reduction inl1Using Cauchy Random Projections</a></p>
<p>15 0.1129365 <a title="74-lsi-15" href="./jmlr-2007-Consistent_Feature_Selection_for_Pattern_Recognition_in_Polynomial_Time.html">24 jmlr-2007-Consistent Feature Selection for Pattern Recognition in Polynomial Time</a></p>
<p>16 0.10563233 <a title="74-lsi-16" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>17 0.10320219 <a title="74-lsi-17" href="./jmlr-2007-Dynamic_Weighted_Majority%3A_An_Ensemble_Method_for_Drifting_Concepts.html">29 jmlr-2007-Dynamic Weighted Majority: An Ensemble Method for Drifting Concepts</a></p>
<p>18 0.097512208 <a title="74-lsi-18" href="./jmlr-2007-On_the_Consistency_of_Multiclass_Classification_Methods_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">61 jmlr-2007-On the Consistency of Multiclass Classification Methods     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>19 0.093939371 <a title="74-lsi-19" href="./jmlr-2007-A_Unified_Continuous_Optimization_Framework_for_Center-Based_Clustering_Methods.html">8 jmlr-2007-A Unified Continuous Optimization Framework for Center-Based Clustering Methods</a></p>
<p>20 0.093846269 <a title="74-lsi-20" href="./jmlr-2007-Polynomial_Identification_in_the_Limit_of_Substitutable_Context-free_Languages.html">67 jmlr-2007-Polynomial Identification in the Limit of Substitutable Context-free Languages</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/jmlr2007_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(8, 0.025), (10, 0.034), (12, 0.027), (16, 0.051), (28, 0.045), (40, 0.023), (48, 0.012), (60, 0.033), (66, 0.476), (80, 0.019), (85, 0.065), (98, 0.083)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74542964 <a title="74-lda-1" href="./jmlr-2007-Separating_Models_of_Learning_from_Correlated_and_Uncorrelated_Data_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">74 jmlr-2007-Separating Models of Learning from Correlated and Uncorrelated Data     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Ariel Elbaz, Homin K. Lee, Rocco A. Servedio, Andrew Wan</p><p>Abstract: We consider a natural framework of learning from correlated data, in which successive examples used for learning are generated according to a random walk over the space of possible examples. A recent paper by Bshouty et al. (2003) shows that the class of polynomial-size DNF formulas is efﬁciently learnable in this random walk model; this result suggests that the Random Walk model is more powerful than comparable standard models of learning from independent examples, in which similarly efﬁcient DNF learning algorithms are not known. We give strong evidence that the Random Walk model is indeed more powerful than the standard model, by showing that if any cryptographic one-way function exists (a universally held belief in cryptography), then there is a class of functions that can be learned efﬁciently in the Random Walk setting but not in the standard setting where all examples are independent. Keywords: random walks, uniform distribution learning, cryptographic hardness, correlated data, PAC learning</p><p>2 0.323753 <a title="74-lda-2" href="./jmlr-2007-Attribute-Efficient_and_Non-adaptive_Learning_of_Parities_and_DNF_Expressions_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_the_Conference_on_Learning_Theory_2005%29.html">12 jmlr-2007-Attribute-Efficient and Non-adaptive Learning of Parities and DNF Expressions     (Special Topic on the Conference on Learning Theory 2005)</a></p>
<p>Author: Vitaly Feldman</p><p>Abstract: We consider the problems of attribute-efﬁcient PAC learning of two well-studied concept classes: parity functions and DNF expressions over {0, 1}n . We show that attribute-efﬁcient learning of parities with respect to the uniform distribution is equivalent to decoding high-rate random linear codes from low number of errors, a long-standing open problem in coding theory. This is the ﬁrst evidence that attribute-efﬁcient learning of a natural PAC learnable concept class can be computationally hard. An algorithm is said to use membership queries (MQs) non-adaptively if the points at which the algorithm asks MQs do not depend on the target concept. Using a simple non-adaptive parity learning algorithm and a modiﬁcation of Levin’s algorithm for locating a weakly-correlated parity due to Bshouty et al. (1999), we give the ﬁrst non-adaptive and attribute-efﬁcient algorithm for ˜ learning DNF with respect to the uniform distribution. Our algorithm runs in time O(ns4 /ε) and ˜ uses O(s4 · log2 n/ε) non-adaptive MQs, where s is the number of terms in the shortest DNF representation of the target concept. The algorithm improves on the best previous algorithm for learning DNF (of Bshouty et al., 1999) and can also be easily modiﬁed to tolerate random persistent classiﬁcation noise in MQs. Keywords: attribute-efﬁcient, non-adaptive, membership query, DNF, parity function, random linear code</p><p>3 0.24050418 <a title="74-lda-3" href="./jmlr-2007-A_Nonparametric_Statistical_Approach_to_Clustering_via_Mode_Identification.html">5 jmlr-2007-A Nonparametric Statistical Approach to Clustering via Mode Identification</a></p>
<p>Author: Jia Li, Surajit Ray, Bruce G. Lindsay</p><p>Abstract: A new clustering approach based on mode identiﬁcation is developed by applying new optimization techniques to a nonparametric density estimator. A cluster is formed by those sample points that ascend to the same local maximum (mode) of the density function. The path from a point to its associated mode is efﬁciently solved by an EM-style algorithm, namely, the Modal EM (MEM). This method is then extended for hierarchical clustering by recursively locating modes of kernel density estimators with increasing bandwidths. Without model ﬁtting, the mode-based clustering yields a density description for every cluster, a major advantage of mixture-model-based clustering. Moreover, it ensures that every cluster corresponds to a bump of the density. The issue of diagnosing clustering results is also investigated. Speciﬁcally, a pairwise separability measure for clusters is deﬁned using the ridgeline between the density bumps of two clusters. The ridgeline is solved for by the Ridgeline EM (REM) algorithm, an extension of MEM. Based upon this new measure, a cluster merging procedure is created to enforce strong separation. Experiments on simulated and real data demonstrate that the mode-based clustering approach tends to combine the strengths of linkage and mixture-model-based clustering. In addition, the approach is robust in high dimensions and when clusters deviate substantially from Gaussian distributions. Both of these cases pose difﬁculty for parametric mixture modeling. A C package on the new algorithms is developed for public access at http://www.stat.psu.edu/∼jiali/hmac. Keywords: modal clustering, mode-based clustering, mixture modeling, modal EM, ridgeline EM, nonparametric density</p><p>4 0.2387799 <a title="74-lda-4" href="./jmlr-2007-The_Locally_Weighted_Bag_of_Words_Framework_for_Document_Representation.html">81 jmlr-2007-The Locally Weighted Bag of Words Framework for Document Representation</a></p>
<p>Author: Guy Lebanon, Yi Mao, Joshua Dillon</p><p>Abstract: The popular bag of words assumption represents a document as a histogram of word occurrences. While computationally efﬁcient, such a representation is unable to maintain any sequential information. We present an effective sequential document representation that goes beyond the bag of words representation and its n-gram extensions. This representation uses local smoothing to embed documents as smooth curves in the multinomial simplex thereby preserving valuable sequential information. In contrast to bag of words or n-grams, the new representation is able to robustly capture medium and long range sequential trends in the document. We discuss the representation and its geometric properties and demonstrate its applicability for various text processing tasks. Keywords: text processing, local smoothing</p><p>5 0.23754929 <a title="74-lda-5" href="./jmlr-2007-On_the_Effectiveness_of_Laplacian_Normalization_for_Graph_Semi-supervised_Learning.html">62 jmlr-2007-On the Effectiveness of Laplacian Normalization for Graph Semi-supervised Learning</a></p>
<p>Author: Rie Johnson, Tong Zhang</p><p>Abstract: This paper investigates the effect of Laplacian normalization in graph-based semi-supervised learning. To this end, we consider multi-class transductive learning on graphs with Laplacian regularization. Generalization bounds are derived using geometric properties of the graph. Speciﬁcally, by introducing a deﬁnition of graph cut from learning theory, we obtain generalization bounds that depend on the Laplacian regularizer. We then use this analysis to better understand the role of graph Laplacian matrix normalization. Under assumptions that the cut is small, we derive near-optimal normalization factors by approximately minimizing the generalization bounds. The analysis reveals the limitations of the standard degree-based normalization method in that the resulting normalization factors can vary signiﬁcantly within each connected component with the same class label, which may cause inferior generalization performance. Our theory also suggests a remedy that does not suffer from this problem. Experiments conﬁrm the superiority of the normalization scheme motivated by learning theory on artiﬁcial and real-world data sets. Keywords: transductive learning, graph learning, Laplacian regularization, normalization of graph Laplacian</p><p>6 0.23703246 <a title="74-lda-6" href="./jmlr-2007-Building_Blocks_for_Variational_Bayesian_Learning_of_Latent_Variable_Models.html">17 jmlr-2007-Building Blocks for Variational Bayesian Learning of Latent Variable Models</a></p>
<p>7 0.23650123 <a title="74-lda-7" href="./jmlr-2007-Learning_to_Classify_Ordinal_Data%3A_The_Data_Replication_Method.html">49 jmlr-2007-Learning to Classify Ordinal Data: The Data Replication Method</a></p>
<p>8 0.23345485 <a title="74-lda-8" href="./jmlr-2007-A_Stochastic_Algorithm_for_Feature_Selection_in_Pattern_Recognition.html">7 jmlr-2007-A Stochastic Algorithm for Feature Selection in Pattern Recognition</a></p>
<p>9 0.23344105 <a title="74-lda-9" href="./jmlr-2007-Online_Learning_of_Multiple_Tasks_with_a_Shared_Loss.html">64 jmlr-2007-Online Learning of Multiple Tasks with a Shared Loss</a></p>
<p>10 0.23325071 <a title="74-lda-10" href="./jmlr-2007-Combining_PAC-Bayesian_and_Generic_Chaining_Bounds.html">20 jmlr-2007-Combining PAC-Bayesian and Generic Chaining Bounds</a></p>
<p>11 0.2327943 <a title="74-lda-11" href="./jmlr-2007-Fast_Iterative_Kernel_Principal_Component_Analysis.html">33 jmlr-2007-Fast Iterative Kernel Principal Component Analysis</a></p>
<p>12 0.23225899 <a title="74-lda-12" href="./jmlr-2007-GiniSupport_Vector_Machine%3A_Quadratic_Entropy_Based_Robust_Multi-Class_Probability_Regression.html">37 jmlr-2007-GiniSupport Vector Machine: Quadratic Entropy Based Robust Multi-Class Probability Regression</a></p>
<p>13 0.23179939 <a title="74-lda-13" href="./jmlr-2007-Stagewise_Lasso.html">77 jmlr-2007-Stagewise Lasso</a></p>
<p>14 0.23159257 <a title="74-lda-14" href="./jmlr-2007-Euclidean_Embedding_of_Co-occurrence_Data.html">32 jmlr-2007-Euclidean Embedding of Co-occurrence Data</a></p>
<p>15 0.23116064 <a title="74-lda-15" href="./jmlr-2007-Noise_Tolerant_Variants_of_the_Perceptron_Algorithm.html">58 jmlr-2007-Noise Tolerant Variants of the Perceptron Algorithm</a></p>
<p>16 0.23113549 <a title="74-lda-16" href="./jmlr-2007-Preventing_Over-Fitting_during_Model_Selection_via_Bayesian_Regularisation_of_the_Hyper-Parameters_%C2%A0%C2%A0%C2%A0%C2%A0%28Special_Topic_on_Model_Selection%29.html">68 jmlr-2007-Preventing Over-Fitting during Model Selection via Bayesian Regularisation of the Hyper-Parameters     (Special Topic on Model Selection)</a></p>
<p>17 0.23058262 <a title="74-lda-17" href="./jmlr-2007-Penalized_Model-Based_Clustering_with_Application_to_Variable_Selection.html">66 jmlr-2007-Penalized Model-Based Clustering with Application to Variable Selection</a></p>
<p>18 0.22997752 <a title="74-lda-18" href="./jmlr-2007-Generalization_Error_Bounds_in_Semi-supervised_Classification_Under_the_Cluster_Assumption.html">36 jmlr-2007-Generalization Error Bounds in Semi-supervised Classification Under the Cluster Assumption</a></p>
<p>19 0.22996703 <a title="74-lda-19" href="./jmlr-2007-On_the_Representer_Theorem_and_Equivalent_Degrees_of_Freedom_of_SVR.html">63 jmlr-2007-On the Representer Theorem and Equivalent Degrees of Freedom of SVR</a></p>
<p>20 0.22956952 <a title="74-lda-20" href="./jmlr-2007-Classification_in_Networked_Data%3A_A_Toolkit_and_a_Univariate_Case_Study.html">19 jmlr-2007-Classification in Networked Data: A Toolkit and a Univariate Case Study</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
