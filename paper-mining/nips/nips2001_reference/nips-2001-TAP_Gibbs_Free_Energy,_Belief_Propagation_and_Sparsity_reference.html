<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-178" href="../nips2001/nips-2001-TAP_Gibbs_Free_Energy%2C_Belief_Propagation_and_Sparsity.html">nips2001-178</a> <a title="nips-2001-178-reference" href="#">nips2001-178-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>178 nips-2001-TAP Gibbs Free Energy, Belief Propagation and Sparsity</h1>
<br/><p>Source: <a title="nips-2001-178-pdf" href="http://papers.nips.cc/paper/2027-tap-gibbs-free-energy-belief-propagation-and-sparsity.pdf">pdf</a></p><p>Author: Lehel Csató, Manfred Opper, Ole Winther</p><p>Abstract: The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived. We show how a speciﬁc sequential minimization of the free energy leads to a generalization of Minka’s expectation propagation. Lastly, we derive a sparse representation version of the sequential algorithm. The usefulness of the approach is demonstrated on classiﬁcation and density estimation with Gaussian processes and on an independent component analysis problem.</p><br/>
<h2>reference text</h2><p>[1] T.P. Minka. Expectation propagation for approximate Bayesian inference. PhD thesis, Dep. of Electrical Eng. and Comp. Sci.; MIT, 2000.</p>
<p>[2] J. S. Yedidia, W. T. Freeman and Y. Weiss, Generalized Belief Propagation, to appear in Advances in Neural Information Processing Systems (NIPS’2000), MIT Press (2001).</p>
<p>[3] M. Opper and O. Winther, Tractable approximations for probabilistic models: The adaptive TAP approach, Phys. Rev. Lett. 86, 3695 (2001).</p>
<p>[4] L. Csat´ and M. Opper. Sparse Gaussian Processes. Neural Computation accepted (2001). o</p>
<p>[5] P.A.d.F.R. Højen-Sørensen, O. Winther, and L. K. Hansen, Mean Field Approaches to Independent Component Analysis, Neural Computation accepted (2001). Available from http://www.cbs.dtu.dk/winther/</p>
<p>[6] T. Plefka, Convergence condition of the TAP equations for the inﬁnite-ranged Ising spin glass model, J. Phys. A 15, 1971 (1982).</p>
<p>[7] T. Tanaka, Mean-Field Theory of Boltzmann Machine Learning, Phys. Rev. E 58, 2302(1998).</p>
<p>[8] G. Parisi and M. Potters, Mean-Field Equations for Spin Models with Orthogonal Interaction Matrices, J. Phys. A (Math. Gen.) 28, 5267 (1995).</p>
<p>[9] L. Csat´ , E. Fokou´ , M. Opper, B. Schottky, and O. Winther. Efﬁcient approaches to Gaussian o e process classiﬁcation. In Advances in Neural Information Processing Systems, volume 12, (2000).</p>
<p>[10] D.M. Schmidt. Continuous probability distributions from ﬁnite data. arXiv:physics/9808005 (1998)</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
