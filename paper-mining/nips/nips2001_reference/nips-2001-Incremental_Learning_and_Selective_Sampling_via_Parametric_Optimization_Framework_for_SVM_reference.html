<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-94" href="../nips2001/nips-2001-Incremental_Learning_and_Selective_Sampling_via_Parametric_Optimization_Framework_for_SVM.html">nips2001-94</a> <a title="nips-2001-94-reference" href="#">nips2001-94-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>94 nips-2001-Incremental Learning and Selective Sampling via Parametric Optimization Framework for SVM</h1>
<br/><p>Source: <a title="nips-2001-94-pdf" href="http://papers.nips.cc/paper/1978-incremental-learning-and-selective-sampling-via-parametric-optimization-framework-for-svm.pdf">pdf</a></p><p>Author: Shai Fine, Katya Scheinberg</p><p>Abstract: We propose a framework based on a parametric quadratic programming (QP) technique to solve the support vector machine (SVM) training problem. This framework, can be specialized to obtain two SVM optimization methods. The first solves the fixed bias problem, while the second starts with an optimal solution for a fixed bias problem and adjusts the bias until the optimal value is found. The later method can be applied in conjunction with any other existing technique which obtains a fixed bias solution. Moreover, the second method can also be used independently to solve the complete SVM training problem. A combination of these two methods is more flexible than each individual method and, among other things, produces an incremental algorithm which exactly solve the 1-Norm Soft Margin SVM optimization problem. Applying Selective Sampling techniques may further boost convergence. 1</p><br/>
<h2>reference text</h2><p>[1] A. B. Berkelaar, B. Jansen, K. Roos, and T. Terlaky. Sensitivity analysis in (degenerate) quadratic programming. Technical Report 96-26 , Delft University, 1996.</p>
<p>[2] C. L. Blake and C. J Merz. UCI repository of machine learning databases, 1998.</p>
<p>[3] G . Cauwenberghs and T . Poggio. Incremental and decremental support vector machine learning. In Adv. in N eural Information Processing Systems 13, pages 409- 415, 2001.</p>
<p>[4] N. Cristianini and J. Shawe-Taylor. An Introductin to Support Vector Macines and Other Kernel-Based Learning Methods. Cambridge University Press, 2000.</p>
<p>[5] S. Fine and K. Scheinberg. Poker: Parametric optimization framework for kernel methods. Technical report , IBM T. J. Watson Research Center, 2001. Submitted.</p>
<p>[6] T. T. Friess, N. Cristianini, and C. Campbell. The kernel-adaraton algorithm: A fast simple learning procedure for SVM. In Pmc. of 15th ICML , pages 188- 196, 1998.</p>
<p>[7] S. S. Keerthi, S. K. Shevade, C. Bhattacharyya, and K. R. K. Murthy. A fast iterative nearest point algorithm for SVM classifier design . IEEE Trnas . NN, 11:124- 36, 2000.</p>
<p>[8] A. Kowalczyk. Maximal margin perceptron. In Advances in Large Margin Classifiers , pages 75-113. MIT Press, 2000.</p>
<p>[9] R. T. Rockafellar. Conjugate Duality and Optimization. SIAM, Philadelphia, 1974.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
