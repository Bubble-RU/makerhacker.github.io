<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>134 nips-2001-On Kernel-Target Alignment</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-134" href="../nips2001/nips-2001-On_Kernel-Target_Alignment.html">nips2001-134</a> <a title="nips-2001-134-reference" href="#">nips2001-134-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>134 nips-2001-On Kernel-Target Alignment</h1>
<br/><p>Source: <a title="nips-2001-134-pdf" href="http://papers.nips.cc/paper/1946-on-kernel-target-alignment.pdf">pdf</a></p><p>Author: Nello Cristianini, John Shawe-Taylor, Andr√© Elisseeff, Jaz S. Kandola</p><p>Abstract: We introduce the notion of kernel-alignment, a measure of similarity between two kernel functions or between a kernel and a target function. This quantity captures the degree of agreement between a kernel and a given learning task, and has very natural interpretations in machine learning, leading also to simple algorithms for model selection and learning. We analyse its theoretical properties, proving that it is sharply concentrated around its expected value, and we discuss its relation with other standard measures of performance. Finally we describe some of the algorithms that can be obtained within this framework, giving experimental results showing that adapting the kernel to improve alignment on the labelled data significantly increases the alignment on the test set, giving improved classification accuracy. Hence, the approach provides a principled method of performing transduction. Keywords: Kernels, alignment, eigenvectors, eigenvalues, transduction 1</p><br/>
<h2>reference text</h2><p>[1] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University Press, 2000. See also the web site www.supportvector.net.</p>
<p>[2] Nello Cristianini, Huma Lodhi, and John Shawe-Taylor. Latent semantic kernels for feature selection. Technical Report NC-TR-00-080, NeuroCOLT Working Group, http://www.neurocolt.org, 2000.</p>
<p>[3] L. Devroye, L. Gyorfi, and G. Lugosi. A Probabilistic Th eory of Pattern Recognition. Number 31 in Applications of mathematics. Springer, 1996.</p>
<p>[4] C. McDiarmid. On the method of bounded differences. In Surveys in Combinatorics 1989, pages 148-188. Cambridge University Press, 1989.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
