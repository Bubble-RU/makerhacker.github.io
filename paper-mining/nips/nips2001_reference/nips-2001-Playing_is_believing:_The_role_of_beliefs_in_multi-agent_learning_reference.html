<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-146" href="../nips2001/nips-2001-Playing_is_believing%3A_The_role_of_beliefs_in_multi-agent_learning.html">nips2001-146</a> <a title="nips-2001-146-reference" href="#">nips2001-146-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>146 nips-2001-Playing is believing: The role of beliefs in multi-agent learning</h1>
<br/><p>Source: <a title="nips-2001-146-pdf" href="http://papers.nips.cc/paper/1984-playing-is-believing-the-role-of-beliefs-in-multi-agent-learning.pdf">pdf</a></p><p>Author: Yu-Han Chang, Leslie Pack Kaelbling</p><p>Abstract: We propose a new classiﬁcation for multi-agent learning algorithms, with each league of players characterized by both their possible strategies and possible beliefs. Using this classiﬁcation, we review the optimality of existing algorithms, including the case of interleague play. We propose an incremental improvement to the existing algorithms that seems to achieve average payoffs that are at least the Nash equilibrium payoffs in the longrun against fair opponents.</p><br/>
<h2>reference text</h2><p>[1] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Proceedings of the 11th International Conference on Machine Learning (ICML-94), 1994.</p>
<p>[2] Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiaent systems. In Proceedings of the 15th Natl. Conf. on Artiﬁcial Intelligence, 1998.</p>
<p>[3] Junling Hu and Michael P. Wellman. Multiagent reinforcement learning: Theoretical framework and an algorithm. In Proceedings of the 15th Int. Conf. on Machine Learning (ICML-98), 1998.</p>
<p>[4] Michael L. Littman. Friend-or-foe q-learning in general-sum games. In Proceedings of the 18th Int. Conf. on Machine Learning (ICML-01), 2001.</p>
<p>[5] Keith Hall and Amy Greenwald. Correlated q-learning. In DIMACS Workshop on Computational Issues in Game Theory and Mechanism Design, 2001.</p>
<p>[6] Michael Bowling and Manuela Veloso. Multiagent learning using a variable learning rate. Under submission.</p>
<p>[7] Yasuo Nagayuki, Shin Ishii, and Kenji Doya. Multi-agent reinforcement learning: An approach based on the other agent’s internal model. In Proceedings of the International Conference on Multi-Agent Systems (ICMAS-00), 2000.</p>
<p>[8] Drew Fudenburg and David K. Levine. Consistency and cautious ﬁctitious play. Journal of Economic Dynamics and Control, 19:1065–1089, 1995.</p>
<p>[9] J.H. Nachbar and W.R. Zame. Non-computable strategies and discounted repeated games. Economic Theory, 1996.</p>
<p>[10] Yoav Freund and Robert E. Schapire. Adaptive game playing using multiplicative weights. Games and Economic Behavior, 29:79–103, 1999.</p>
<p>[11] Michael Littman and Peter Stone. Leading best-response stratgies in repeated games. In 17th Int. Joint Conf. on Artiﬁcial Intelligence (IJCAI-2001) workshop on Economic Agents, Models, and Mechanisms, 2001.</p>
<p>[12] S. Singh, M. Kearns, and Y. Mansour. Nash convergence of gradient dynamics in general-sum games. In Proceedings of the 16th Conference on Uncertainty in Artiﬁcial Intelligence, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
