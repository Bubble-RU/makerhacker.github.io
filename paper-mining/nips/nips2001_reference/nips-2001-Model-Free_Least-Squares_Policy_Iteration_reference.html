<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>121 nips-2001-Model-Free Least-Squares Policy Iteration</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-121" href="../nips2001/nips-2001-Model-Free_Least-Squares_Policy_Iteration.html">nips2001-121</a> <a title="nips-2001-121-reference" href="#">nips2001-121-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>121 nips-2001-Model-Free Least-Squares Policy Iteration</h1>
<br/><p>Source: <a title="nips-2001-121-pdf" href="http://papers.nips.cc/paper/2134-model-free-least-squares-policy-iteration.pdf">pdf</a></p><p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We propose a new approach to reinforcement learning which combines least squares function approximation with policy iteration. Our method is model-free and completely off policy. We are motivated by the least squares temporal difference learning algorithm (LSTD), which is known for its efﬁcient use of sample experiences compared to pure temporal difference algorithms. LSTD is ideal for prediction problems, however it heretofore has not had a straightforward application to control problems. Moreover, approximations learned by LSTD are strongly inﬂuenced by the visitation distribution over states. Our new algorithm, Least Squares Policy Iteration (LSPI) addresses these issues. The result is an off-policy method which can use (or reuse) data collected from any source. We have tested LSPI on several problems, including a bicycle simulator in which it learns to guide the bicycle to a goal efﬁciently by merely observing a relatively small number of completely random trials.</p><br/>
<h2>reference text</h2><p>[1] J. Baxter and P.Bartlett. Reinforcement learning in POMDP’s via direct gradient ascent. In Proc. 17th International Conf. on Machine Learning, pages 41–48. Morgan Kaufmann, San Francisco, CA, 2000.</p>
<p>[2] D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, Belmont, Massachusetts, 1996.</p>
<p>[3] Justin A. Boyan. Least-squares temporal difference learning. In I. Bratko and S. Dzeroski, editors, Machine Learning: Proceedings of the Sixteenth International Conference, pages 49– 56. Morgan Kaufmann, San Francisco, CA, 1999.</p>
<p>[4] S. Bradtke and A. Barto. Linear least-squares algorithms for temporal difference learning. Machine Learning, 22(1/2/3):33–57, 1996.</p>
<p>[5] D. Koller and R. Parr. Policy iteration for factored mdps. In Proceedings of the Sixteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI-00). Morgan Kaufmann, 2000.</p>
<p>[6] V. Konda and J. Tsitsiklis. Actor-critic algorithms. In NIPS 2000 editors, editor, Advances in Neural Information Processing Systems 12: Proceedings of the 1999 Conference. MIT Press, 2000.</p>
<p>[7] M. G. Lagoudakis and R. Parr. Model-Free Least-Squares policy iteration. Technical Report CS-2001-05, Department of Computer Science, Duke University, December 2001.</p>
<p>[8] A. Ng and M. Jordan. PEGASUS: A policy search method for large MDPs and POMDPs. In Proceedings of the Sixteenth Conference on Uncertainty in Artiﬁcial Intelligence (UAI-00). Morgan Kaufmann, 2000.</p>
<p>[9] A. Ng, R. Parr, and D. Koller. Policy search via density estimation. In Advances in Neural Information Processing Systems 12: Proceedings of the 1999 Conference. MIT Press, 2000.</p>
<p>[10] Andrew Y. Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: theory and application to reward shaping. In Proc. 16th International Conf. on Machine Learning, pages 278–287. Morgan Kaufmann, San Francisco, CA, 1999.</p>
<p>[11] D. Ormoneit and S. Sen. Kernel-based reinforcement learning. To appear, Machine Learning, 2001.</p>
<p>[12] J. Randløv and P. Alstrøm. Learning to drive a bicycle using reinforcement learning and shaping. In The Fifteenth International Conference on Machine Learning, 1998. Morgan Kaufmann.</p>
<p>[13] R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems 12: Proceedings of the 1999 Conference, 2000. MIT Press.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
