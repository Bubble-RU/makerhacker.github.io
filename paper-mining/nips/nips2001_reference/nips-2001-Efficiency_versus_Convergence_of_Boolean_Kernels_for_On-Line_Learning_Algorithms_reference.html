<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-66" href="../nips2001/nips-2001-Efficiency_versus_Convergence_of_Boolean_Kernels_for_On-Line_Learning_Algorithms.html">nips2001-66</a> <a title="nips-2001-66-reference" href="#">nips2001-66-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>66 nips-2001-Efficiency versus Convergence of Boolean Kernels for On-Line Learning Algorithms</h1>
<br/><p>Source: <a title="nips-2001-66-pdf" href="http://papers.nips.cc/paper/2100-efficiency-versus-convergence-of-boolean-kernels-for-on-line-learning-algorithms.pdf">pdf</a></p><p>Author: Roni Khardon, Dan Roth, Rocco A. Servedio</p><p>Abstract: We study online learning in Boolean domains using kernels which capture feature expansions equivalent to using conjunctions over basic features. We demonstrate a tradeoff between the computational efﬁciency with which these kernels can be computed and the generalization ability of the resulting classiﬁer. We ﬁrst describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efﬁciently run the Perceptron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple functions. We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. While known upper bounds imply that Winnow can learn DNF formulae with a polynomial mistake bound in this setting, we prove that it is computationally hard to simulate Winnow’s behavior for learning DNF over such a feature set, and thus that such kernel functions for Winnow are not efﬁciently computable.</p><br/>
<h2>reference text</h2><p>[1] D. Angluin. Negative results for equivalence queries. Machine Learning, 2:121–150, 1990.</p>
<p>[2] A. Carlson, C. Cumby, J. Rosen, and D. Roth. The SNoW learning architecture. Technical Report UIUCDCS-R-99-2101, UIUC Computer Science Department, May 1999.</p>
<p>[3] N. Cristianini and J. Shaw-Taylor. An Introduction to Support Vector Machines. Cambridge Press, 2000.</p>
<p>[4] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. Machine Learning, 2:285–318, 1988.</p>
<p>[5] W. Maass and M. K. Warmuth. Efﬁcient learning with virtual threshold gates. Information and Computation, 141(1):378–386, 1998.</p>
<p>[6] A. Novikoff. On convergence proofs for perceptrons. In Proceeding of the Symposium on the Mathematical Theory of Automata, volume 12, pages 615–622, 1963.</p>
<p>[7] D. Roth. Learning to resolve natural language ambiguities: A uniﬁed approach. In Proc. of the American Association of Artiﬁcial Intelligence, pages 806–813, 1998.</p>
<p>[8] K. Sadohara. Learning of boolean functions using support vector machines. In Proc. of the Conference on Algorithmic Learning Theory, pages 106–118. Springer, 2001. LNAI 2225.</p>
<p>[9] L. G. Valiant. The complexity of enumeration and reliability problems. SIAM Journal of Computing, 8:410–421, 1979.</p>
<p>[10] C. Watkins. Kernels from matching operations. Technical Report CSD-TR-98-07, Computer Science Department, Royal Holloway, University of London, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
