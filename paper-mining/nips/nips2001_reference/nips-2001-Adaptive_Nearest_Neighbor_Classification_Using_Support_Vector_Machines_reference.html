<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-28" href="../nips2001/nips-2001-Adaptive_Nearest_Neighbor_Classification_Using_Support_Vector_Machines.html">nips2001-28</a> <a title="nips-2001-28-reference" href="#">nips2001-28-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>28 nips-2001-Adaptive Nearest Neighbor Classification Using Support Vector Machines</h1>
<br/><p>Source: <a title="nips-2001-28-pdf" href="http://papers.nips.cc/paper/2054-adaptive-nearest-neighbor-classification-using-support-vector-machines.pdf">pdf</a></p><p>Author: Carlotta Domeniconi, Dimitrios Gunopulos</p><p>Abstract: The nearest neighbor technique is a simple and appealing method to address classification problems. It relies on t he assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of examples due to the curse of dimensionality. We propose a technique that computes a locally flexible metric by means of Support Vector Machines (SVMs). The maximum margin boundary found by the SVM is used to determine the most discriminant direction over the query's neighborhood. Such direction provides a local weighting scheme for input features. We present experimental evidence of classification performance improvement over the SVM algorithm alone and over a variety of adaptive learning schemes, by using both simulated and real data sets. 1</p><br/>
<h2>reference text</h2><p>[1] S. Amari and S. Wu, </p>
<p>[2] R.E. Bellman, Adaptive Control Processes. Princeton Univ. Press, 1961.</p>
<p>[3] M. Brown, W. Grundy, D. Lin, N. Cristianini, C. Sugnet, T. Furey, M. Ares, and D. Haussler, </p>
<p>[4] W.S. Cleveland and S.J. Devlin, </p>
<p>[5] T.M. Cover and P.E. Hart, </p>
<p>[6] C. Domeniconi and D. Gunopulos, </p>
<p>[7] C. Domeniconi, J. Peng, and D. Gunopulos, </p>
<p>[8] R.O. Duda and P.E. Hart, Pattern Classification and Scene Analysis. John Wiley & Sons, Inc., 1973.</p>
<p>[9] J.H. Friedman </p>
<p>[10] T. Hastie and R. Tibshirani, </p>
<p>[11] T. Joachims, </p>
<p>[12] T. Joachims, </p>
<p>[13] D.G. Lowe, </p>
<p>[14] E. Osuna, R. Freund, and F. Girosi, </p>
<p>[15] J.R. Quinlan, C4.5: Programs for Machine Learning. Morgan-Kaufmann Publishers, Inc., 1993.</p>
<p>[16] C.J. Stone, Nonparametric regression and its applications (with discussion). Ann. Statist. 5, 595, 1977.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
