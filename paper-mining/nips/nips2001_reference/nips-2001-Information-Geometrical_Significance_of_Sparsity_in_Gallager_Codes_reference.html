<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>97 nips-2001-Information-Geometrical Significance of Sparsity in Gallager Codes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-97" href="../nips2001/nips-2001-Information-Geometrical_Significance_of_Sparsity_in_Gallager_Codes.html">nips2001-97</a> <a title="nips-2001-97-reference" href="#">nips2001-97-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>97 nips-2001-Information-Geometrical Significance of Sparsity in Gallager Codes</h1>
<br/><p>Source: <a title="nips-2001-97-pdf" href="http://papers.nips.cc/paper/1990-information-geometrical-significance-of-sparsity-in-gallager-codes.pdf">pdf</a></p><p>Author: Toshiyuki Tanaka, Shiro Ikeda, Shun-ichi Amari</p><p>Abstract: We report a result of perturbation analysis on decoding error of the belief propagation decoder for Gallager codes. The analysis is based on information geometry, and it shows that the principal term of decoding error at equilibrium comes from the m-embedding curvature of the log-linear submanifold spanned by the estimated pseudoposteriors, one for the full marginal, and K for partial posteriors, each of which takes a single check into account, where K is the number of checks in the Gallager code. It is then shown that the principal error term vanishes when the parity-check matrix of the code is so sparse that there are no two columns with overlap greater than 1. 1</p><br/>
<h2>reference text</h2><p>[1] R. G. Gallager, Low Density Parity Check Codes, Ph. D. Thesis, Mass. Inst. Tech., 1960.</p>
<p>[2] R. J. McEliece, D. J. C. MacKay, and J. Cheng, “Turbo decoding as an instance of Pearl's `belief propagation' algorithm,” IEEE J. Select. A. Commun., vol. 16, no. 2, pp. 140–152, 1998.</p>
<p>[3] D. J. C. MacKay, “Good error-correcting codes based on very sparse matrices,” IEEE Trans. Inform. Theory, vol. 45, no. 2, pp. 399–431, 1999.</p>
<p>[4] D. J. Thouless, P. W. Anderson, and R. G. Palmer, “Solution of `Solvable model of a spin glass',” Phil. Mag., vol. 35, no. 3, pp. 593–601, 1977.</p>
<p>[5] T. Murayama, Y. Kabashima, D. Saad, and R. Vicente, “Statistical physics of regular low-density parity-check error-correcting codes,” Phys. Rev. E, vol. 62, no. 2, pp. 1577–1591, 2000.</p>
<p>[6] S. Amari and H. Nagaoka (Transl. by D. Harada), Methods of Information Geometry, Translations of Mathematical Monographs, vol. 191, American Math. Soc., 2000.</p>
<p>[7] Y. Kabashima and D. Saad, “The TAP approach to intensive and extensive connectivity systems,” in M. Opper and D. Saad (eds.), Advanced Mean Field Methods — Theory and Practice, The MIT Press, 2001, pp. 65–84.</p>
<p>[8] S. Ikeda, T. Tanaka, and S. Amari, “Information geometrical framework for analyzing belief propagation decoder,” in T. G. Dietterich et al. (eds.), Advances in Neural Information Processing Systems, vol. 14 (this volume), The MIT Press, 2002.</p>
<p>[9] S. Ikeda, T. Tanaka, and S. Amari, “Information geometry of turbo codes and low-density paritycheck codes,” submitted to IEEE Trans. Inform. Theory, 2001.</p>
<p>[10] H. J. Kappen and F. B. Rodriguez, “Efﬁcient learning in Boltzmann machines using linear response theory,” Neural Computation, vol. 10, no. 5, pp. 1137–1156, 1998.</p>
<p>[11] H. J. Kappen and F. B. Rodriguez, “Boltzmann machine learning using mean ﬁeld theory and linear response correction,” in M. I. Jordan et al. (eds.), Advances in Neural Information Processing Systems, vol. 10, The MIT Press, 1998, pp. 280–286.</p>
<p>[12] T. Tanaka, “A theory of mean ﬁeld approximation,” in M. S. Kearns et al. (eds.), Advances in Neural Information Processing Systems, vol. 11, The MIT Press, 1999, pp. 351–357.</p>
<p>[13] T. Tanaka, “Information geometry of mean-ﬁeld approximation,” Neural Computation, vol. 12, no. 8, pp. 1951–1968, 2000.</p>
<p>[14] J. S. Yedidia, “An idiosyncratic journey beyond mean ﬁeld theory,” in M. Opper and D. Saad (eds.), Advanced Mean Field Methods — Theory and Practice, The MIT Press, 2001, pp. 21–35.</p>
<p>[15] H. J. Kappen and W. J. Wiegerinck, “Mean ﬁeld theory for graphical models,” in M. Opper and D. Saad (eds.), Advanced Mean Field Methods — Theory and Practice, The MIT Press, 2001, pp. 37–49.</p>
<p>[16] S. Amari, S. Ikeda, and H. Shimokawa, “Information geometry of α-projection in mean ﬁeld approximation,” in M. Opper and D. Saad (eds.), Advanced Mean Field Methods — Theory and Practice, The MIT Press, 2001, pp. 241–257.</p>
<p>[17] T. Tanaka, “Information geometry of mean-ﬁeld approximation,” in M. Opper and D. Saad (eds.), Advanced Mean Field Methods — Theory and Practice, The MIT Press, 2001, pp. 259– 273.</p>
<p>[18] T. J. Richardson and R. L. Urbanke, “The capacity of low-density parity-check codes under message-passing decodeing,” IEEE Trans. Inform. Theory, vol. 47, no. 2, pp. 599–618, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
