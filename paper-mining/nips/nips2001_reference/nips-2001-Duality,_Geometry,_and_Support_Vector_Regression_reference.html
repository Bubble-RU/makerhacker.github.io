<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 nips-2001-Duality, Geometry, and Support Vector Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-62" href="../nips2001/nips-2001-Duality%2C_Geometry%2C_and_Support_Vector_Regression.html">nips2001-62</a> <a title="nips-2001-62-reference" href="#">nips2001-62-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>62 nips-2001-Duality, Geometry, and Support Vector Regression</h1>
<br/><p>Source: <a title="nips-2001-62-pdf" href="http://papers.nips.cc/paper/2132-duality-geometry-and-support-vector-regression.pdf">pdf</a></p><p>Author: J. Bi, Kristin P. Bennett</p><p>Abstract: We develop an intuitive geometric framework for support vector regression (SVR). By examining when -tubes exist, we show that SVR can be regarded as a classiﬁcation problem in the dual space. Hard and soft -tubes are constructed by separating the convex or reduced convex hulls respectively of the training data with the response variable shifted up and down by . A novel SVR model is proposed based on choosing the max-margin plane between the two shifted datasets. Maximizing the margin corresponds to shrinking the eﬀective -tube. In the proposed approach the eﬀects of the choices of all parameters become clear geometrically. 1</p><br/>
<h2>reference text</h2><p>[1] K. Bennett and E. Bredensteiner. Duality and Geometry in SVM Classiﬁers. In P. Langley, eds., Proc. of Seventeenth Intl. Conf. on Machine Learning, p 57–64, Morgan Kaufmann, San Francisco, 2000.</p>
<p>[2] D. Crisp and C. Burges. A Geometric Interpretation of ν-SVM Classiﬁers. In S. Solla, T. Leen, and K. Muller, eds., Advances in Neural Info. Proc. Sys., Vol 12. p 244–251, MIT Press, Cambridge, MA, 1999.</p>
<p>[3] S.S. Keerthi, S.K. Shevade, C. Bhattacharyya and K.R.K. Murthy, A Fast Iterative Nearest Point Algorithm for Support Vector Machine Classiﬁer Design, IEEE Transactions on Neural Networks, Vol. 11, pp.124-136, 2000.</p>
<p>[4] O. Mangasarian. Nonlinear Programming. SIAM, Philadelphia, 1994.</p>
<p>[5] B. Sch¨lkopf, P. Bartlett, A. Smola and R. Williamson. Shrinking the Tube: o A New Support Vector Regression Algorithm. In M. Kearns, S. Solla, and D. Cohn eds., Advances in Neural Info. Proc. Sys., Vol 12, MIT Press, Cambridge, MA, 1999.</p>
<p>[6] V. Vapnik. The Nature of Statistical Learning Theory. Wiley, New York, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
