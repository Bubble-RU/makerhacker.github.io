<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>13 nips-2001-A Natural Policy Gradient</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-13" href="../nips2001/nips-2001-A_Natural_Policy_Gradient.html">nips2001-13</a> <a title="nips-2001-13-reference" href="#">nips2001-13-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>13 nips-2001-A Natural Policy Gradient</h1>
<br/><p>Source: <a title="nips-2001-13-pdf" href="http://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf">pdf</a></p><p>Author: Sham M. Kakade</p><p>Abstract: We provide a natural gradient method that represents the steepest descent direction based on the underlying structure of the parameter space. Although gradient methods cannot make large changes in the values of the parameters, we show that the natural gradient is moving toward choosing a greedy optimal action rather than just a better action. These greedy optimal actions are those that would be chosen under one improvement step of policy iteration with approximate, compatible value functions, as defined by Sutton et al. [9]. We then show drastic performance improvements in simple MDPs and in the more challenging MDP of Tetris. 1</p><br/>
<h2>reference text</h2><p>[I] S. Amari. Natural gradient works efficiently in learning. 10(2):251- 276, 1998.  Neural Computation,</p>
<p>[2] J. Baxter and P. Bartlett. Direct gradient-based reinforcement learning. Technical report, Australian National University, Research School of Information Sciences and Engineering, July 1999.</p>
<p>[3] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996.</p>
<p>[4] P. Dayan and G. Hinton. Using em for reinforcement learning. Neural Computation, 9:271- 278 , 1997.</p>
<p>[5] S. Kakade. Optimizing average reward using discounted reward. COLT. in press., 200l.</p>
<p>[6] V. Konda and J. Tsitsiklis. Actor-critic algorithms. Advances in N eural Information Processing Systems, 12, 2000.</p>
<p>[7] D . MacKay. Maximum likelihood and covariant algorithms for independent component analysis. Technical report , University of Cambridge, 1996.</p>
<p>[8] P. Marbach and J . Tsitsiklis. Simulation-based optimization of markov reward processes. Technical report, Massachusetts Institute of Technology, 1998.</p>
<p>[9] R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. Neural Information Processing Systems, 13, 2000.</p>
<p>[10] L. Xu and M. 1. Jordan. On convergence properties of the EM algorithm for gaussian mixtures. Neural Computation, 8(1):129- 151, 1996.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
