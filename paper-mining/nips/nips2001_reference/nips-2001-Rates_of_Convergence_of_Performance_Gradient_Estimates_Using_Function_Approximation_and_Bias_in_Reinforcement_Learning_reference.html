<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-157" href="../nips2001/nips-2001-Rates_of_Convergence_of_Performance_Gradient_Estimates_Using_Function_Approximation_and_Bias_in_Reinforcement_Learning.html">nips2001-157</a> <a title="nips-2001-157-reference" href="#">nips2001-157-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>157 nips-2001-Rates of Convergence of Performance Gradient Estimates Using Function Approximation and Bias in Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2001-157-pdf" href="http://papers.nips.cc/paper/2053-rates-of-convergence-of-performance-gradient-estimates-using-function-approximation-and-bias-in-reinforcement-learning.pdf">pdf</a></p><p>Author: Gregory Z. Grudic, Lyle H. Ungar</p><p>Abstract: We address two open theoretical questions in Policy Gradient Reinforcement Learning. The ﬁrst concerns the efﬁcacy of using function approximation to represent the state action value function, . Theory is presented showing that linear function approximation representations of can degrade the rate of convergence of performance gradient estimates by a factor of relative to when no function approximation of is used, where is the number of possible actions and is the number of basis functions in the function approximation representation. The second concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by , where is the number of possible actions. Experimental evidence is presented showing that these theoretical results lead to signiﬁcant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms.       ¤ ¨ ¦ ¢ ©§¥¤£¡ ¦ ¤ ¨ £¡ ¨ ¤¢  ¢</p><br/>
<h2>reference text</h2><p>[1] Jonathan Baxter and Peter L. Bartlett, Reinforcement learning in pomdp’s via direct gradient ascent, Proceedings of the Seventeenth International Conference on Machine Learning (ICML’2000) (Stanford University, CA), June 2000, pp. 41–48.</p>
<p>[2] G. Z. Grudic and L. H. Ungar, Localizing policy gradient estimates to action transitions, Proceedings of the Seventeenth International Conference on Machine Learning, vol. 17, Morgan Kaufmann, June 29 - July 2 2000, pp. 343–350.</p>
<p>[3]  , Localizing search in reinforcement learning, Proceedings of the Seventeenth National Conference on Artiﬁcial Intelligence, vol. 17, Menlo Park, CA: AAAI Press / Cambridge, MA: MIT Press, July 30 - August 3 2000, pp. 590–595.</p>
<p>[4] V. R. Konda and J. N. Tsitsiklis, Actor-critic algorithms, Advances in Neural Information Processing Systems (Cambridge, MA) (S. A. Solla, T. K. Leen, and K.-R. Mller, eds.), vol. 12, MIT Press, 2000.</p>
<p>[5] R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour, Policy gradient methods for reinforcement learning with function approximation, Advances in Neural Information Processing Systems (Cambridge, MA) (S. A. Solla, T. K. Leen, and K.-R. Mller, eds.), vol. 12, MIT Press, 2000.</p>
<p>[6] R. J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement learning, Machine Learning 8 (1992), no. 3, 229–256. Appendix: Proofs of Theorems 1 and 2  @ ¡B    9 @&¥3 d B B ¤ 03 £ ! ¢ ¨ @ B 397d 6  ¨ 3 0! 3  © ¦ ¦ ) 7 2! 58C#¨¢ q3 a $ 7 ¨©¨¢ ¡ ( ¢ $ §¤ A 4   C §¤ A 0  3 ¢ C 3 ¨   £ ¦§¤ ¦ @ ¨9    9 AB ©@¥  §¤ !    $ A  H  33  ) a 0     ¡ © 9 @&¥3 d d ©© 03 C ¢ ! ¨ @ B 397d 6  0! 3  © ) 7 ¨2! 3 8C2¨¢ §3 a $ 7 ¨ 2¨¢ ¡ 1(¢ $ A  12 © 03  ¤A 4   C ¢  A 4 ¡s   d ! ¦¨ §¨#¢ @9 B ©¢ ( 3 ¨ ¨ ¨ 03  $ A 3    ¡ 2! G©¨¢ q3  ¥ 3 £ 9  45D 3 9 AB @ ¥ @  9 w¥@ 3 ¢4 D   ¨©¨¢ 9 B ¨ ©¨¢ ¡ (     d @ ©  9 £ B @ ¥ @  @w¥@ 3  A   @ 9 9a  £¤ @ @  a  @ B 39d f¡  45D  2¨¢ @ @ 9  ¥ @     9 w¥@ 3 #¨¢ ( d @  A ¨ ¨  9 AB  9 AB  Y @ B 39 d f¡ 9 w¥a 3 ¡ d 45D  ©¨¢ @ ¨ ¡AB   Y @ B  39 d f¡  a @ #¨¢ ¡ ( @  A  9 w¥3 ¨ d 4 D  ¨ 3 8C©¨¢ ¡   Y  B  39d f¡  a @ #¨¢ ¡ ( @  A © @  9 ¨ 4 D ¡  ¡¥¨35G¨#¢ d 231¡   3 @ B x9 d f¡  @&¥@ 3 #¨¢ ¡ ( d @ A d 321 ©© ¡      a¨ d 321 ¡  ¡  9 @w¥3 9 @&¥3 d ¨ d ! 03 ! 03  A £ ¨ 3 8C2¨¢   ¨  ¨ ¡   ¨2! 3 8C©¨¢ §3 $ ¨©¨¢ ( $  ¨2¢ @ ¡ B   2! 3 8C2¨¢ §3 $ 2¨¢ ¡ ( $ s a ¡ ¢ @ a @  9 AB   9 B  Proof of Theorem 1: Consider the deﬁnition of there exist and such that:  ¨¨ ©¢  Let be the observation of following:  where the basis functions  and  Denoting  given in (7). In [5] it is shown that  (13) (3) after a single episode. Using (13), we get the  have the form  , with variance  as the least squares (LS) estimate of (3), its form is given by:  (14)  where are LS estimates of the weights and correspond to the basis functions . Then, it can be shown that any linear system of the type given in (14) has a rate of convergence given by:  Substituting (5) and (6) into the above equation completes the proof.  </p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
