<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-138" href="../nips2001/nips-2001-On_the_Generalization_Ability_of_On-Line_Learning_Algorithms.html">nips2001-138</a> <a title="nips-2001-138-reference" href="#">nips2001-138-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>138 nips-2001-On the Generalization Ability of On-Line Learning Algorithms</h1>
<br/><p>Source: <a title="nips-2001-138-pdf" href="http://papers.nips.cc/paper/2113-on-the-generalization-ability-of-on-line-learning-algorithms.pdf">pdf</a></p><p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this paper we show that on-line algorithms for classiﬁcation and regression can be naturally used to obtain hypotheses with good datadependent tail bounds on their risk. Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds.</p><br/>
<h2>reference text</h2><p>[1] Angluin, D. Queries and concept learning, Machine Learning, 2(4), 319-342, 1988.</p>
<p>[2] Azoury, K., and Warmuth, M. K. Relative loss bounds for on-line density estimation with the exponential family of distributions, Machine Learning, 43:211–246, 2001.</p>
<p>[3] K. Azuma. Weighted sum of certain dependend random variables. Tohoku Mathematical Journal, 68, 357–367, 1967. 2 Using a slightly different linear regression algorithm, Forster and Warmuth [7] have proven a sharper bound on the expected relative loss. In particular, they have exhibited an algorithm computing hypothesis such that in expectation (over ) the relative risk is bounded by .  ` X aY0  W US¡IGECA4 V T R Q P H FD B @  &0 7 986  D  2  s q i rrpe  4 5D h  20 & ' 31)(&  g e c 54 fdb</p>
<p>[4] A. Blum, A. Kalai, and J. Langford. Beating the hold-out: bounds for k-fold and progressive cross-validation. In 12th COLT, pages 203–208, 1999.</p>
<p>[5] S. Boucheron, G. Lugosi, and P. Massart. A sharp concentration inequality with applications. Random Structures and Algorithms, 16, 277–292, 2000.</p>
<p>[6] N. Cesa-Bianchi, Y. Freund, D. Haussler, D. P. Helmbold, R. E. Schapire, and M. K. Warmuth. How to use expert advice. Journal of the ACM, 44(3), 427–485, 1997.</p>
<p>[7] J. Forster, and M. K. Warmuth. Relative expected instantaneous loss bounds. 13th COLT, 90–99, 2000.</p>
<p>[8] Y. Freund and R. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine Learning, 37(3), 277–296, 1999.</p>
<p>[9] C. Gentile The robustness of the -norm algorithms. Manuscript, 2001. An extended abstract (co-authored with N. Littlestone) appeared in 12th COLT, 1–11, 1999.</p>
<p>[10] A. J. Grove, N. Littlestone, and D. Schuurmans. General convergence results for linear discriminant updates, Machine Learning, 43(3), 173–210, 2001.  ¥</p>
<p>[11] D. Helmbold and M. K. Warmuth. On weak learning. JCSS, 50(3), 551–573, June 1995.</p>
<p>[12] A. Hoerl, and R. Kennard, Ridge regression: biased estimation for nonorthogonal problems. Technometrics, 12, 55–67, 1970.</p>
<p>[13] J. Kivinen and M. K. Warmuth. Additive versus exponentiated gradient updates for linear prediction. Information and Computation, 132(1), 1–64, 1997.</p>
<p>[14] N. Littlestone. Learning quickly when irrelevant attributes abound: A new linearthreshold algorithm. Machine Learning, 2, 285–318, 1988.</p>
<p>[15] N. Littlestone. From on-line to batch learning. In 2nd COLT, 269–284, 1989.</p>
<p>[16] N. Littlestone and M. K. Warmuth. The weighted majority algorithm. Information and Computation, 108(2), 212–261, 1994.</p>
<p>[17] F. Rosenblatt. Principles of neurodynamics: Perceptrons and the theory of brain mechanisms. Spartan Books, Washington, D.C., 1962.</p>
<p>[18] C. Saunders, A. Gammerman, and V. Vovk. Ridge Regression Learning Algorithm in Dual Variables, In 15th ICML, 1998.</p>
<p>[19] J. Shawe-Taylor, P. Bartlett, R. Williamson, and M. Anthony, Structural Risk Minimization over Data-dependent Hierarchies. IEEE Trans. IT, 44, 1926–1940, 1998.</p>
<p>[20] J. Shawe-Taylor and N. Cristianini, On the generalization of soft margin algorithms, 2000. NeuroCOLT2 Tech. Rep. 2000-082, http://www.neurocolt.org.</p>
<p>[21] V.N. Vapnik, Statistical learning theory. J. Wiley and Sons, NY, 1998.</p>
<p>[22] V. Vovk, Competitive on-line linear regression. In NIPS*10, 1998. Also: Tech. Rep. Department of Computer Science, Royal Holloway, University of London, CSD-TR97-13, 1997.</p>
<p>[23] R. C. Williamson, J. Shawe-Taylor, B. Sch¨ lkopf and A. J. Smola, Sample Based o Generalization Bounds, IEEE Trans. IT, to appear.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
