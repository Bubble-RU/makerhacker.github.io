<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>128 nips-2001-Multiagent Planning with Factored MDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-128" href="../nips2001/nips-2001-Multiagent_Planning_with_Factored_MDPs.html">nips2001-128</a> <a title="nips-2001-128-reference" href="#">nips2001-128-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>128 nips-2001-Multiagent Planning with Factored MDPs</h1>
<br/><p>Source: <a title="nips-2001-128-pdf" href="http://papers.nips.cc/paper/1941-multiagent-planning-with-factored-mdps.pdf">pdf</a></p><p>Author: Carlos Guestrin, Daphne Koller, Ronald Parr</p><p>Abstract: We present a principled and efﬁcient planning algorithm for cooperative multiagent dynamic systems. A striking feature of our method is that the coordination and communication between the agents is not imposed, but derived directly from the system dynamics and function approximation architecture. We view the entire multiagent system as a single, large Markov decision process (MDP), which we assume can be represented in a factored way using a dynamic Bayesian network (DBN). The action space of the resulting MDP is the joint action space of the entire set of agents. Our approach is based on the use of factored linear value functions as an approximation to the joint value function. This factorization of the value function allows the agents to coordinate their actions at runtime using a natural message passing scheme. We provide a simple and efﬁcient method for computing such an approximate value function by solving a single linear program, whose size is determined by the interaction between the value function structure and the DBN. We thereby avoid the exponential blowup in the state and action space. We show that our approach compares favorably with approaches based on reward sharing. We also show that our algorithm is an efﬁcient alternative to more complicated algorithms even in the single agent case.</p><br/>
<h2>reference text</h2><p>[1] U. Bertele and F. Brioschi. Nonserial Dynamic Programming. Academic Press, 1972.</p>
<p>[2] C. Boutilier, T. Dean, and S. Hanks. Decision theoretic planning: Structural assumptions and computational leverage. Journal of Artiﬁcial Intelligence Research, 11:1 – 94, 1999.</p>
<p>[3] D.P. de Farias and B. Van Roy. The linear programming approach to approximate dynamic programming. submitted to the IEEE Transactions on Automatic Control, January 2001.</p>
<p>[4] T. Dean and K. Kanazawa. A model for reasoning about persistence and causation. Computational Intelligence, 5(3):142–150, 1989.</p>
<p>[5] R. Dechter. Bucket elimination: A unifying framework for reasoning. Artiﬁcial Intelligence, 113(1–2):41–85, 1999.</p>
<p>[6] C. Guestrin, D. Koller, and R. Parr. Max-norm projections for factored MDPs. In Proc. 17th IJCAI, 2001.</p>
<p>[7] F. Jensen, F. Jensen, and S. Dittmer. From inﬂuence diagrams to junction trees. In Uncertainty in Artiﬁcial Intelligence: Proceedings of the Tenth Conference, pages 367–373, Seattle, Washington, July 1994. Morgan Kaufmann.</p>
<p>[8] D. Koller and R. Parr. Computing factored value functions for policies in structured MDPs. In Proceedings of the Sixteenth International Joint Conference on Artiﬁcial Intelligence (IJCAI99). Morgan Kaufmann, 1999.</p>
<p>[9] D. Koller and R. Parr. Policy iteration for factored MDPs. In Proc. 16th UAI, 2000.</p>
<p>[10] L. Peshkin, N. Meuleau, K. Kim, and L. Kaelbling. Learning to cooperate via policy search. In Proc. 16th UAI, 2000.</p>
<p>[11] J. Schneider, W. Wong, A. Moore, and M. Riedmiller. Distributed value functions. In Proc. 16th ICML, 1999.</p>
<p>[12] P. Schweitzer and A. Seidmann. Generalized polynomial approximations in Markovian decision processes. Journal of Mathematical Analysis and Applications, 110:568 – 582, 1985.</p>
<p>[13] D. Wolpert, K. Wheller, and K. Tumer. General principles of learning-based multi-agent systems. In Proc. 3rd Agents Conference, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
