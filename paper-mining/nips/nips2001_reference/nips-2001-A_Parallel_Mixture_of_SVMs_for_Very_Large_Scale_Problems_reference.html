<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-16" href="../nips2001/nips-2001-A_Parallel_Mixture_of_SVMs_for_Very_Large_Scale_Problems.html">nips2001-16</a> <a title="nips-2001-16-reference" href="#">nips2001-16-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>16 nips-2001-A Parallel Mixture of SVMs for Very Large Scale Problems</h1>
<br/><p>Source: <a title="nips-2001-16-pdf" href="http://papers.nips.cc/paper/1949-a-parallel-mixture-of-svms-for-very-large-scale-problems.pdf">pdf</a></p><p>Author: Ronan Collobert, Samy Bengio, Yoshua Bengio</p><p>Abstract: Support Vector Machines (SVMs) are currently the state-of-the-art models for many classification problems but they suffer from the complexity of their training algorithm which is at least quadratic with respect to the number of examples. Hence, it is hopeless to try to solve real-life problems having more than a few hundreds of thousands examples with SVMs. The present paper proposes a new mixture of SVMs that can be easily implemented in parallel and where each SVM is trained on a small subset of the whole dataset. Experiments on a large benchmark dataset (Forest) as well as a difficult speech database , yielded significant time improvement (time complexity appears empirically to locally grow linearly with the number of examples) . In addition, and that is a surprise, a significant improvement in generalization was observed on Forest. 1</p><br/>
<h2>reference text</h2><p>[1] RA. Cole, M. Noel, T. Lander, and T. Durham. New telephone speech corpora at CSLU. Proceedings of the European Conference on Speech Communication and Technology, EUROSPEECH, 1:821- 824, 1995.</p>
<p>[2] R Collobert and S. Bengio. SVMTorch: Support vector machines for large-scale regression problems. Journal of Machine Learning Research, 1:143- 160, 200l.</p>
<p>[3] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273- 297, 1995.</p>
<p>[4] Robert A. Jacobs, Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. Adaptive mixtures of local experts. Neural Computation, 3(1):79- 87, 1991.</p>
<p>[5] J. T. Kwok. Support vector mixture for classification and regression problems. In Proceedings of the International Conference on Pattern Recognition (ICPR) , pages 255-258, Brisbane, Queensland, Australia, 1998.</p>
<p>[6] E. Osuna, R Freund, and F. Girosi. Training support vector machines: an application to face detection. In IEEE conference on Computer Vision and Pattern Recognition, pages 130- 136, San Juan, Puerto Rico, 1997.</p>
<p>[7] A. Rida, A. Labbi, and C. Pellegrini. Local experts combination trough density decomposition. In International Workshop on AI and Statistics (Uncertainty'99). Morgan Kaufmann, 1999.</p>
<p>[8] V. Tresp. A bayesian committee machine. Neural Computation, 12:2719-2741,2000.</p>
<p>[9] V. N. Vapnik. The nature of statistical learning theory. Springer, second edition, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
