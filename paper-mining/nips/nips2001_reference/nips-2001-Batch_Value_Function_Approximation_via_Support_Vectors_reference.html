<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 nips-2001-Batch Value Function Approximation via Support Vectors</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-40" href="../nips2001/nips-2001-Batch_Value_Function_Approximation_via_Support_Vectors.html">nips2001-40</a> <a title="nips-2001-40-reference" href="#">nips2001-40-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>40 nips-2001-Batch Value Function Approximation via Support Vectors</h1>
<br/><p>Source: <a title="nips-2001-40-pdf" href="http://papers.nips.cc/paper/2116-batch-value-function-approximation-via-support-vectors.pdf">pdf</a></p><p>Author: Thomas G. Dietterich, Xin Wang</p><p>Abstract: We present three ways of combining linear programming with the kernel trick to find value function approximations for reinforcement learning. One formulation is based on SVM regression; the second is based on the Bellman equation; and the third seeks only to ensure that good moves have an advantage over bad moves. All formulations attempt to minimize the number of support vectors while fitting the data. Experiments in a difficult, synthetic maze problem show that all three formulations give excellent performance, but the advantage formulation is much easier to train. Unlike policy gradient methods, the kernel methods described here can easily 'adjust the complexity of the function approximator to fit the complexity of the value function. 1</p><br/>
<h2>reference text</h2><p>Baird, L. C. (1993). Advantage updating. Tech. rep. 93-1146, Wright-Patterson AFB. Moll, R., Barto, A. G., Perkins, T. J., & Sutton, R. S. (1999). Learning instanceindependent value functions to enhance local search. NIPS-II, 1017-1023. Tesauro, G. (1995). Temporal difference learning and TD-Gammon. CACM, 28(3), 58-68. Utgoff, P. E., & Saxena, S. (1987). Learning a preference predicate. In ICML-87, 115-121. Vapnik, V. (2000). The Nature of Statistical Learning Theory, 2nd Ed. Springer. Zhang, W., & Dietterich, T. G. (1995). A reinforcement learning approach to jobshop scheduling. In IJCAI95, 1114-1120.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
