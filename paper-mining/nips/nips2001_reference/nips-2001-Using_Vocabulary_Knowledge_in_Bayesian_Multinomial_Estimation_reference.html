<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-194" href="../nips2001/nips-2001-Using_Vocabulary_Knowledge_in_Bayesian_Multinomial_Estimation.html">nips2001-194</a> <a title="nips-2001-194-reference" href="#">nips2001-194-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>194 nips-2001-Using Vocabulary Knowledge in Bayesian Multinomial Estimation</h1>
<br/><p>Source: <a title="nips-2001-194-pdf" href="http://papers.nips.cc/paper/2063-using-vocabulary-knowledge-in-bayesian-multinomial-estimation.pdf">pdf</a></p><p>Author: Thomas L. Griffiths, Joshua B. Tenenbaum</p><p>Abstract: Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks. Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity. We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates. We demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data. 1</p><br/>
<h2>reference text</h2><p>[1] T. C. Bell, J. G. Cleary, and 1. H. Witten. Text compression. Prentice Hall, 1990.</p>
<p>[2] G. E. P. Box and G. C. Tiao. Bayesian Inference in Statistical Analysis. AddisonWesley, 1973.</p>
<p>[3] S. F. Chen and J. Goodman. An empirical study of smoothing techniques for language modeling. Technical Report TR-10-98, Center for Research in Computing Technology, Harvard University, 1998.</p>
<p>[4] N. Friedman and Y. Singer. Efficient Bayesian parameter estimation in large discrete domains. In Neural Information Processing Systems, 1998.</p>
<p>[5] H. Jeffreys. An invariant form for the prior probability in estimation problems. Proceedings of the Royal Society A, 186:453-461, 1946.</p>
<p>[6] P.-S. Laplace. Philosophical Essay on Probabilities. Springer-Verlag, 1995. Originally published 1825.</p>
<p>[7] G. Lidstone. Note on the general case of the Bayes-Laplace formula for inductive or a posteriori probabilities. Transactions of the Faculty of Actuaries, 8:182-192, 1920.</p>
<p>[8] K. Nigam, A. K. Mccallum, S. Thrun, and T. Mitchell. Text classification fro'in labeled and unlabeled documents using EM. Machine Learning, 39:103-134, 2000.</p>
<p>[9] W. Perks. Some observations on inverse probability, including a new indifference rule. Journal of the Institute of Actuaries, 73:285-312, 1947.</p>
<p>[10] E. S. Ristad. A natural law Â·of succession. Technical Report CS-TR-895-95, Department of Computer Science, Princeton University, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
