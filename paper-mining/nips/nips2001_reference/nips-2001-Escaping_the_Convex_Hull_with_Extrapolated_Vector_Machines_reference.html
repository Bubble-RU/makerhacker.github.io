<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-69" href="../nips2001/nips-2001-Escaping_the_Convex_Hull_with_Extrapolated_Vector_Machines.html">nips2001-69</a> <a title="nips-2001-69-reference" href="#">nips2001-69-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>69 nips-2001-Escaping the Convex Hull with Extrapolated Vector Machines</h1>
<br/><p>Source: <a title="nips-2001-69-pdf" href="http://papers.nips.cc/paper/2037-escaping-the-convex-hull-with-extrapolated-vector-machines.pdf">pdf</a></p><p>Author: Patrick Haffner</p><p>Abstract: Maximum margin classifiers such as Support Vector Machines (SVMs) critically depends upon the convex hulls of the training samples of each class, as they implicitly search for the minimum distance between the convex hulls. We propose Extrapolated Vector Machines (XVMs) which rely on extrapolations outside these convex hulls. XVMs improve SVM generalization very significantly on the MNIST [7] OCR data. They share similarities with the Fisher discriminant: maximize the inter-class margin while minimizing the intra-class disparity. 1</p><br/>
<h2>reference text</h2><p>[1] R. Collobert and S. Bengio. Support vector machines for large-scale regression problems. Technical Report IDIAP-RR-00-17, IDIAP, 2000.</p>
<p>[2] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:1- 25 , 1995.</p>
<p>[3] D. Crisp and C.J.C. Burges. A geometric interpretation of v-SVM classifiers. In Advances in Neural Information Processing Systems 12, S. A. Solla, T. K. Leen, K.-R. Mller, eds, Cambridge, MA, 2000. MIT Press.</p>
<p>[4] D. DeCoste and B. Schoelkopf. Training invariant support vector machines. Machine Learning, special issue on Support Vector Machines and Methods, 200l.</p>
<p>[5] S.S. Keerthi, S.K. Shevade, C. Bhattacharyya, and K.R.K. Murthy. A fast iterative nearest point algorithm for support vector machine classifier design. IEEE transactions on neural networks, 11(1):124 - 136, jan 2000.</p>
<p>[6] A. Kowalczyk. Maximal margin perceptron. In Advances in Large Margin Classifiers, Smola, Bartlett, Schlkopf, and Schuurmans, editors, Cambridge, MA, 2000. MIT Press.</p>
<p>[7] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. proceedings of the IEEE, 86(11), 1998.</p>
<p>[8] J. Platt, N. Christianini, and J. Shawe-Taylor. Large margin dags for multiclass classification. In Advances in Neural Information Processing Systems 12, S. A. Solla, T. K. Leen, K.-R. Mller, eds, Cambridge, MA, 2000. MIT Press.</p>
<p>[9] V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons, New-York, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
