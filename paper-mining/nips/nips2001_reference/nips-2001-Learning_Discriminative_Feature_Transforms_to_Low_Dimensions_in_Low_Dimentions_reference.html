<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-109" href="../nips2001/nips-2001-Learning_Discriminative_Feature_Transforms_to_Low_Dimensions_in_Low_Dimentions.html">nips2001-109</a> <a title="nips-2001-109-reference" href="#">nips2001-109-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>109 nips-2001-Learning Discriminative Feature Transforms to Low Dimensions in Low Dimentions</h1>
<br/><p>Source: <a title="nips-2001-109-pdf" href="http://papers.nips.cc/paper/2028-learning-discriminative-feature-transforms-to-low-dimensions-in-low-dimentions.pdf">pdf</a></p><p>Author: Kari Torkkola</p><p>Abstract: The marriage of Renyi entropy with Parzen density estimation has been shown to be a viable tool in learning discriminative feature transforms. However, it suffers from computational complexity proportional to the square of the number of samples in the training data. This sets a practical limit to using large databases. We suggest immediate divorce of the two methods and remarriage of Renyi entropy with a semi-parametric density estimation method, such as a Gaussian Mixture Models (GMM). This allows all of the computation to take place in the low dimensional target space, and it reduces computational complexity proportional to square of the number of components in the mixtures. Furthermore, a convenient extension to Hidden Markov Models as commonly used in speech recognition becomes possible.</p><br/>
<h2>reference text</h2><p>[1] R. Battiti. Using mutual information for selecting features in supervised neural net learning. Neural Networks, 5(4):537–550, July 1994.</p>
<p>[2] Sanjoy Dasgupta. Experiments with random projection. In Proceedings of the 16th Conference on Uncertainty in Artiﬁcial Intelligence, pages 143–151, Stanford, CA, June30 - July 3 2000.</p>
<p>[3] R.M. Fano. Transmission of Information: A Statistical theory of Communications. Wiley, New York, 1961.</p>
<p>[4] J.W. Fisher III and J.C. Principe. A methodology for information theoretic feature extraction. In Proc. of IEEE World Congress On Computational Intelligence, pages 1712–1716, Anchorage, Alaska, May 4-9 1998.</p>
<p>[5] K. Fukunaga. Introduction to statistical pattern recognition (2nd edition). Academic Press, New York, 1990.</p>
<p>[6] Xuan Guorong, Chai Peiqi, and Wu Minhui. Bhattacharyya distance feature selection. In Proceedings of the 13th International Conference on Pattern Recognition, volume 2, pages 195 – 199. IEEE, 25-29 Aug. 1996.</p>
<p>[7] J.N. Kapur. Measures of information and their applications. Wiley, New Delhi, India, 1994.</p>
<p>[8] J.N. Kapur and H.K. Kesavan. Entropy optimization principles with applications. Academic Press, San Diego, London, 1992.</p>
<p>[9] Nagendra Kumar and Andreas G. Andreou. Heteroscedastic discriminant analysis and reduced rank HMMs for improved speech recognition. Speech Communication, 26(4):283–297, 1998.</p>
<p>[10] J.C. Principe, J.W. Fisher III, and D. Xu. Information theoretic learning. In Simon Haykin, editor, Unsupervised Adaptive Filtering. Wiley, New York, NY, 2000.</p>
<p>[11] J.C. Principe, D. Xu, and J.W. Fisher III. Pose estimation in SAR using an information-theoretic criterion. In Proc. SPIE98, 1998.</p>
<p>[12] George Saon and Mukund Padmanabhan. Minimum bayes error feature selection for continuous speech recognition. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Processing Systems 13, pages 800– 806. MIT Press, 2001.</p>
<p>[13] Janne Sinkkonen and Samuel Kaski. Clustering based on conditional distributions in an auxiliary space. Neural Computation, 14:217–239, 2002.</p>
<p>[14] Kari Torkkola. Nonlinear feature transforms using maximum mutual information. In Proceedings of the IJCNN, pages 2756–2761, Washington DC, USA, July 15-19 2001.</p>
<p>[15] Kari Torkkola and William Campbell. Mutual information in learning feature transformations. In Proceedings of the 17th International Conference on Machine Learning, pages 1015–1022, Stanford, CA, USA, June 29 - July 2 2000.</p>
<p>[16] N. Vlassis, Y. Motomura, and B. Krose. Supervised dimension reduction of intrinsically low-dimensional data. Neural Computation, 14(1), January 2002.</p>
<p>[17] H. Yang and J. Moody. Feature selection based on joint mutual information. In Proceedings of International ICSC Symposium on Advances in Intelligent Data Analysis, Rochester, New York, June 22-25 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
