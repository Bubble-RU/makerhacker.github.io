<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>120 nips-2001-Minimax Probability Machine</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-120" href="../nips2001/nips-2001-Minimax_Probability_Machine.html">nips2001-120</a> <a title="nips-2001-120-reference" href="#">nips2001-120-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>120 nips-2001-Minimax Probability Machine</h1>
<br/><p>Source: <a title="nips-2001-120-pdf" href="http://papers.nips.cc/paper/2036-minimax-probability-machine.pdf">pdf</a></p><p>Author: Gert Lanckriet, Laurent E. Ghaoui, Chiranjib Bhattacharyya, Michael I. Jordan</p><p>Abstract: When constructing a classifier, the probability of correct classification of future data points should be maximized. In the current paper this desideratum is translated in a very direct way into an optimization problem, which is solved using methods from convex optimization. We also show how to exploit Mercer kernels in this setting to obtain nonlinear decision boundaries. A worst-case bound on the probability of misclassification of future data is obtained explicitly. 1</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
