<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-101" href="../nips2001/nips-2001-K-Local_Hyperplane_and_Convex_Distance_Nearest_Neighbor_Algorithms.html">nips2001-101</a> <a title="nips-2001-101-reference" href="#">nips2001-101-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 nips-2001-K-Local Hyperplane and Convex Distance Nearest Neighbor Algorithms</h1>
<br/><p>Source: <a title="nips-2001-101-pdf" href="http://papers.nips.cc/paper/2058-k-local-hyperplane-and-convex-distance-nearest-neighbor-algorithms.pdf">pdf</a></p><p>Author: Pascal Vincent, Yoshua Bengio</p><p>Abstract: Guided by an initial idea of building a complex (non linear) decision surface with maximal local margin in input space, we give a possible geometrical intuition as to why K-Nearest Neighbor (KNN) algorithms often perform more poorly than SVMs on classiﬁcation tasks. We then propose modiﬁed K-Nearest Neighbor algorithms to overcome the perceived problem. The approach is similar in spirit to Tangent Distance, but with invariances inferred from the local neighborhood rather than prior knowledge. Experimental results on real world classiﬁcation tasks suggest that the modiﬁed KNN algorithms often give a dramatic improvement over standard KNN and perform as well or better than SVMs. 1 Motivation The notion of margin for classiﬁcation tasks has been largely popularized by the success of the Support Vector Machine (SVM) [2, 15] approach. The margin of SVMs has a nice geometric interpretation1: it can be deﬁned informally as (twice) the smallest Euclidean distance between the decision surface and the closest training point. The decision surface produced by the original SVM algorithm is the hyperplane that maximizes this distance while still correctly separating the two classes. While the notion of keeping the largest possible safety margin between the decision surface and the data points seems very reasonable and intuitively appealing, questions arise when extending the approach to building more complex, non-linear decision surfaces. Non-linear SVMs usually use the “kernel trick” to achieve their non-linearity. This conceptually corresponds to ﬁrst mapping the input into a higher-dimensional feature space with some non-linear transformation and building a maximum-margin hyperplane (a linear decision surface) there. The “trick” is that this mapping is never computed directly, but implicitly induced by a kernel. In this setting, the margin being maximized is still the smallest Euclidean distance between the decision surface and the training points, but this time measured in some strange, sometimes inﬁnite dimensional, kernel-induced feature space rather than the original input space. It is less clear whether maximizing the margin in this new space, is meaningful in general (see [16]). 1 for the purpose of this discussion, we consider the original hard-margin SVM algorithm for two linearly separable classes. A different approach is to try and build a non-linear decision surface with maximal distance to the closest data point as measured directly in input space (as proposed in [14]). We could for instance restrict ourselves to a certain class of decision functions and try to ﬁnd the function with maximal margin among this class. But let us take this even further. Extending the idea of building a correctly separating non-linear decision surface as far away as possible from the data points, we deﬁne the notion of local margin as the Euclidean distance, in input space, between a given point on the decision surface and the closest training point. Now would it be possible to ﬁnd an algorithm that could produce a decision surface which correctly separates the classes and such that the local margin is everywhere maximal along its surface? Surprisingly, the plain old Nearest Neighbor algorithm (1NN) [5] does precisely this! So why does 1NN in practice often perform worse than SVMs? One typical explanation, is that it has too much capacity, compared to SVM, that the class of function it can produce is too rich. But, considering it has inﬁnite capacity (VC-dimension), 1NN is still performing quite well... This study is an attempt to better understand what is happening, based on geometrical intuition, and to derive an improved Nearest Neighbor algorithm from this understanding. 2 Fixing a broken Nearest Neighbor algorithm 2.1 Setting and deﬁnitions The setting is that of a classical classiﬁcation problem in (the input space).  wx u  r i q ©</p><br/>
<h2>reference text</h2><p>[1] C. G. Atkeson, A. W. Moore, and S. Schaal. Locally weighted learning. Artiﬁcial Intelligence Review, 1996.</p>
<p>[2] B. Boser, I. Guyon, and V. Vapnik. An algorithm for optimal margin classiﬁers. In Fifth Annual Workshop on Computational Learning Theory, pages 144–152, Pittsburgh, 1992.</p>
<p>[3] L. Bottou and V. Vapnik. Local learning algorithms. Neural Computation, 4(6):888–900, 1992.</p>
<p>[4] Olivier Chapelle, Jason Weston, L´ Bottou, and Vladimir Vapnik. Vicinal risk minimization. eon In T.K. Leen, T.G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems, volume 13, pages 416–422, 2001.</p>
<p>[5] T.M. Cover and P.E. Hart. Nearest neighbor pattern classiﬁcation. IEEE Transactions on Information Theory, 13(1):21–27, 1967.</p>
<p>[6] J. Friedman. Flexible metric nearest neighbor classiﬁcation. Technical Report 113, Stanford University Statistics Department, 1994.</p>
<p>[7] Trevor Hastie and Robert Tibshirani. Discriminant adaptive nearest neighbor classiﬁcation and regression. In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, Advances in Neural Information Processing Systems, volume 8, pages 409–415. The MIT Press, 1996.</p>
<p>[8] S.Z. Li and J.W. Lu. Face recognition using the nearest feature line method. IEEE Transactions on Neural Networks, 10(2):439–443, 1999.</p>
<p>[9] J. Myles and D. Hand. The multi-class measure problem in nearest neighbour discrimination rules. Pattern Recognition, 23:1291–1297, 1990.</p>
<p>[10] D. Ormoneit and T. Hastie. Optimal kernel shapes for local linear regression. In S. A. Solla, T. K. Leen, and K-R. Mller, editors, Advances in Neural Information Processing Systems, volume 12. MIT Press, 2000.</p>
<p>[11] Sam Roweis and Lawrence Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, Dec. 2000.</p>
<p>[12] R. D. Short and K. Fukunaga. The optimal distance measure for nearest neighbor classiﬁcation. IEEE Transactions on Information Theory, 27:622–627, 1981.</p>
<p>[13] P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. Transformation invariance in pattern recognition — tangent distance and tangent propagation. Lecture Notes in Computer Science, 1524, 1998.</p>
<p>[14] S. Tong and D. Koller. Restricted bayes optimal classiﬁers. In Proceedings of the 17th National Conference on Artiﬁcial Intelligence (AAAI), pages 658–664, Austin, Texas, 2000.</p>
<p>[15] V.N. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.</p>
<p>[16] Bin Zhang. Is the maximal margin hyperplane special in a feature space? Technical Report HPL-2001-89, Hewlett-Packards Labs, 2001.  Table 1: Test-error obtained on the USPS and MNIST digit classiﬁcation tasks by KNN, SVM (using a Gaussian Kernel), HKNN and CKNN. Hyper parameters were tuned on a separate validation set. Both HKNN and CKNN appear to perform much better than original KNN, and even compare favorably to SVMs. Parameters used  #  Test Error 4.98% 4.33% 3.93% 3.98% 2.95% 1.30% 1.26% 1.46%   l ¥ ¨## l  £  ¢# l   § ¢# © l ¥ # §¨ ¦ #  l ¤£ ¢# l ¡  Algorithm KNN SVM HKNN CKNN KNN SVM HKNN CKNN     9 8 #    8 #       @ #   f 8 8 #  8  Data Set USPS (6291 train, 1000 valid., 2007 test points) MNIST (50000 train, 10000 valid., 10000 test points)  0.032 CKNN basic HKNN HKNN, lambda=1 HKNN, lambda=10  0.03 0.028  error rate  0.026 0.024 0.022 0.02 0.018 0.016 0.014 0.012 0  20  40  60  80  100  120  K  Figure 3: Error rate on MNIST as a function of for CKNN, and HKNN with different values of . As can be seen the basic HKNN algorithm performs poorly for large values of . As expected, CKNN is relatively unaffected by this problem, and HKNN can be made robust through the added “weight decay” penalty controlled by .  l        l  Table 2: Test-error obtained on MNIST with HKNN and CKNN when using a reduced training set made of the 16712 support vectors retained by the best Gaussian Kernel SVM. This corresponds to 28% of the initial 60000 training patterns. Performance is even better than when using the whole dataset. But here, hyper parameters and were chosen with the test set, as we did not have a separate validation set in this setting. It is nevertheless remarkable that comparable performances can be achieved with far fewer points.     l  Parameters used  ¥  #  ¢#  l  l  Test Error 1.23% 1.36%  8 #    Algorithm HKNN CKNN    Data Set MNIST (16712 train s.v., 10000 test points)</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
