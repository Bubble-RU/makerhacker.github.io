<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-104" href="../nips2001/nips-2001-Kernel_Logistic_Regression_and_the_Import_Vector_Machine.html">nips2001-104</a> <a title="nips-2001-104-reference" href="#">nips2001-104-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>104 nips-2001-Kernel Logistic Regression and the Import Vector Machine</h1>
<br/><p>Source: <a title="nips-2001-104-pdf" href="http://papers.nips.cc/paper/2059-kernel-logistic-regression-and-the-import-vector-machine.pdf">pdf</a></p><p>Author: Ji Zhu, Trevor Hastie</p><p>Abstract: The support vector machine (SVM) is known for its good performance in binary classiﬁcation, but its extension to multi-class classiﬁcation is still an on-going research issue. In this paper, we propose a new approach for classiﬁcation, called the import vector machine (IVM), which is built on kernel logistic regression (KLR). We show that the IVM not only performs as well as the SVM in binary classiﬁcation, but also can naturally be generalized to the multi-class case. Furthermore, the IVM provides an estimate of the underlying probability. Similar to the “support points” of the SVM, the IVM model uses only a fraction of the training data to index kernel basis functions, typically a much smaller fraction than the SVM. This gives the IVM a computational advantage over the SVM, especially when the size of the training data set is large.</p><br/>
<h2>reference text</h2><p>[1] Burges, C.J.C. (1998) A tutorial on support vector machines for pattern recognition. In Data Mining and Knowledge Discovery. Kluwer Academic Publishers, Boston. (Volume 2)</p>
<p>[2] Evgeniou, T., Pontil, M., & Poggio., T. (1999) Regularization networks and support vector machines. In A.J. Smola, P. Bartlett, B. Sch¨ lkopf, and C. Schuurmans, editors, Advances in Large o Margin Classiﬁers. MIT Press.</p>
<p>[3] Green, P. & Yandell, B. (1985) Semi-parametric generalized linear models. Proceedings 2nd International GLIM Conference, Lancaster, Lecture notes in Statistics No. 32 44-55 Springer-Verlag, New York.</p>
<p>[4] Hastie, T. & Tibshirani, R. (1990) Generalized Additive Models, Chapman and Hall.</p>
<p>[5] Hastie, T., Tibshirani, R., & Friedman, J.(2001) The elements of statistical learning. In print.</p>
<p>[6] Lin, X., Wahba, G., Xiang, D., Gao, F., Klein, R. & Klein B. (1998), Smoothing spline ANOVA models for large data sets with Bernoulli observations and the randomized GACV. Technical Report 998, Department of Statistics, University of Wisconsin, Madison WI.</p>
<p>[7] Kimeldorf, G. & Wahba, G. (1971) Some results on Tchebychefﬁan spline functions. J. Math. Anal. Applic. 33, 82-95.</p>
<p>[8] Smola, A. & Sch¨ lkopf, B. (2000) Sparse Greedy Matrix Approximation for Machine Learning. o In Proceedings of the Seventeenth International Conference on Machine Learning. Morgan Kaufmann Publishers.</p>
<p>[9] Wahba, G. (1998) Support Vector Machine, Reproducing Kernel Hilbert Spaces and the Randomized GACV. Technical Report 984rr, Department of Statistics, University of Wisconsin, Madison WI.</p>
<p>[10] Wahba, G., Gu, C., Wang, Y., & Chappell, R. (1995) Soft Classiﬁcation, a.k.a. Risk Estimation, via Penalized Log Likelihood and Smoothing Spline Analysis of Variance. In D.H. Wolpert, editor, The Mathematics of Generalization. Santa Fe Institute Studies in the Sciences of Complexity. Addison-Wesley Publisher.</p>
<p>[11] Williams, C. & Seeger, M (2001) Using the Nystrom Method to Speed Up Kernel Machines. In T. K. Leen, T. G. Diettrich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13. MIT Press.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
