<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-38" href="../nips2001/nips-2001-Asymptotic_Universality_for_Learning_Curves_of_Support_Vector_Machines.html">nips2001-38</a> <a title="nips-2001-38-reference" href="#">nips2001-38-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>38 nips-2001-Asymptotic Universality for Learning Curves of Support Vector Machines</h1>
<br/><p>Source: <a title="nips-2001-38-pdf" href="http://papers.nips.cc/paper/2123-asymptotic-universality-for-learning-curves-of-support-vector-machines.pdf">pdf</a></p><p>Author: Manfred Opper, Robert Urbanczik</p><p>Abstract: Using methods of Statistical Physics, we investigate the rOle of model complexity in learning with support vector machines (SVMs). We show the advantages of using SVMs with kernels of infinite complexity on noisy target rules, which, in contrast to common theoretical beliefs, are found to achieve optimal generalization error although the training error does not converge to the generalization error. Moreover, we find a universal asymptotics of the learning curves which only depend on the target rule but not on the SVM kernel. 1</p><br/>
<h2>reference text</h2><p>[1] C. Cortes and V. Vapnik. , Machine Learning, 20:273-297, 1995.</p>
<p>[2] N. Cristianini and J . Shawe-Taylor. Support Vector Machines. Cambridge U niversity Press , 2000.</p>
<p>[3] M. Opper and R. Urbanczik. Phys. Rev. Lett., 86:4410- 4413, 200l.</p>
<p>[4] R. Dietrich, M. Opper, and H. Sompolinsky. Phys. Rev. Lett., 82:2975 - 2978, 1999.</p>
<p>[5] S. Risau-Gusman and M. Gordon. Phys. Rev. E, 62:7092- 7099,2000.</p>
<p>[6] I. Schoenberg. Anal. Math, 39:811-841, 1938.  0.3  ,-----r -- - - - - - - - - - - - - - - - - ,  (A) (8) (C) (D)  0.2  D  6.    0  (E)  tOg 0.1 - trllin -  o  -  -  -  -  -  10  5  20  15  a=P/N Figure 1: Linear target rule corrupted by additive Gaussian noise rJ ((rJ) = 0, \rJ 2 ) = 1/16) and learned using different kernels. The curves are the theoretical prediction; symbols show simulation results for N = 600 with Gaussian inputs and error bars are approximately the size of the symbols. (A) Gaussian kernel,  (z) = e- kz with k = 2/3. (B) Wiener kernel given by the non analytic function  (z) = e - e..jZ. We chose c ~ 0.065 so that the theoretical prediction for this case coincides with (A). (C) Gaussian kernel with k = 1/20, the influence of the parameter change on t he learning curve is minimal. (D) Perceptron, ¢(z) = z . Above a e ~ 7.5 vanishing training error cannot be achieved and the SVM is undefined. (E) Kernel invariant asymptote for (A,B,C).  0.1 -  -E~in-  -  o w-______ o 2  -  -  -  -  -  -  -  -  -  -  -  ~______~_ _ _ _ _~_____ _ w  4  6  8  a = P/N2  Figure 2: A noisy quadratic rule (Tl = 0, T2 = 1) learned using the Gaussian kernel with k = 1/20. The upper curve (simulations.) is for additive Gaussian noise as in Fig. 1. The lower curve (simulations .) is for binary noise, rJ ± s with equal probability. We chose s ~ 0.20 so that the value of fmin is the same for the two noise processes. The inset shows that f9 decays as l/a for Gaussian noise, whereas an exponential decay is found in the binary case. The dashed curves are the kernel invariant asymptotes. The simulations are for N = 90 with Gaussian inputs, and standard errors are approximately the size of the symbols.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
