<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>168 nips-2001-Sequential Noise Compensation by Sequential Monte Carlo Method</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-168" href="../nips2001/nips-2001-Sequential_Noise_Compensation_by_Sequential_Monte_Carlo_Method.html">nips2001-168</a> <a title="nips-2001-168-reference" href="#">nips2001-168-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>168 nips-2001-Sequential Noise Compensation by Sequential Monte Carlo Method</h1>
<br/><p>Source: <a title="nips-2001-168-pdf" href="http://papers.nips.cc/paper/2093-sequential-noise-compensation-by-sequential-monte-carlo-method.pdf">pdf</a></p><p>Author: K. Yao, S. Nakamura</p><p>Abstract: We present a sequential Monte Carlo method applied to additive noise compensation for robust speech recognition in time-varying noise. The method generates a set of samples according to the prior distribution given by clean speech models and noise prior evolved from previous estimation. An explicit model representing noise effects on speech features is used, so that an extended Kalman ﬁlter is constructed for each sample, generating the updated continuous state estimate as the estimation of the noise parameter, and prediction likelihood for weighting each sample. Minimum mean square error (MMSE) inference of the time-varying noise parameter is carried out over these samples by fusion the estimation of samples according to their weights. A residual resampling selection step and a Metropolis-Hastings smoothing step are used to improve calculation eﬃciency. Experiments were conducted on speech recognition in simulated non-stationary noises, where noise power changed artiﬁcially, and highly non-stationary Machinegun noise. In all the experiments carried out, we observed that the method can have signiﬁcant recognition performance improvement, over that achieved by noise compensation with stationary noise assumption. 1</p><br/>
<h2>reference text</h2><p>[1] A. Varga and R.K. Moore, “Hidden markov model decomposition of speech and noise,” in ICASSP, 1990, pp. 845–848.</p>
<p>[2] N.S. Kim, “Nonstationary environment compensation based on sequential estimation,” IEEE Signal Processing Letters, vol. 5, no. 3, March 1998.</p>
<p>[3] K. Yao, K. K. Paliwal, and S. Nakamura, “Sequential noise compensation by a sequential kullback proximal algorithm,” in EUROSPEECH, 2001, pp. 1139–1142, extended paper submitted for publication.</p>
<p>[4] K. Yao, B. E. Shi, S. Nakamura, and Z. Cao, “Residual noise compensation by a sequential em algorithm for robust speech recognition in nonstationary noise,” in ICSLP, 2000, vol. 1, pp. 770–773.</p>
<p>[5] B. Frey, L. Deng, A. Acero, and T. Kristjansson, “Algonquin: Iterating laplace’s method to remove multiple types of acoustic distortion for robust speech recognition,” in EUROSPEECH, 2001, pp. 901–904.</p>
<p>[6] J. S. Liu and R. Chen, “Sequential monte carlo methods for dynamic systems,” J. Am. Stat. Assoc, vol. 93, pp. 1032–1044, 1998.</p>
<p>[7] W. K. Hastings, “Monte carlo sampling methods using markov chains and their applications,” Biometrika, vol. 57, pp. 97–109, 1970.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
