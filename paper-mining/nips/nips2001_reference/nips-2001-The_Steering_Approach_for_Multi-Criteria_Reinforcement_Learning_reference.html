<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-187" href="../nips2001/nips-2001-The_Steering_Approach_for_Multi-Criteria_Reinforcement_Learning.html">nips2001-187</a> <a title="nips-2001-187-reference" href="#">nips2001-187-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>187 nips-2001-The Steering Approach for Multi-Criteria Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2001-187-pdf" href="http://papers.nips.cc/paper/1986-the-steering-approach-for-multi-criteria-reinforcement-learning.pdf">pdf</a></p><p>Author: Shie Mannor, Nahum Shimkin</p><p>Abstract: We consider the problem of learning to attain multiple goals in a dynamic environment, which is initially unknown. In addition, the environment may contain arbitrarily varying elements related to actions of other agents or to non-stationary moves of Nature. This problem is modelled as a stochastic (Markov) game between the learning agent and an arbitrary player, with a vector-valued reward function. The objective of the learning agent is to have its long-term average reward vector belong to a given target set. We devise an algorithm for achieving this task, which is based on the theory of approachability for stochastic games. This algorithm combines, in an appropriate way, a ﬁnite set of standard, scalar-reward learning algorithms. Suﬃcient conditions are given for the convergence of the learning algorithm to a general target set. The specialization of these results to the single-controller Markov decision problem are discussed as well. 1</p><br/>
<h2>reference text</h2><p>[1] J. Abounadi, D. Bertsekas, and V. Borkar. Learning algorithms for markov decision processes with average cost. LIDS-P 2434, Lab. for Info. and Decision Systems, MIT, October 1998.</p>
<p>[2] A.G. Barto and R.S. Sutton. Reinforcement Learning. MIT Press, 1998.</p>
<p>[3] D. Blackwell. An analog of the minimax theorem for vector payoﬀs. Paciﬁc J. Math., 6(1):1–8, 1956.  2</p>
<p>[4] R.I. Brafman and M. Tennenholtz. A near optimal polynomial time algorithm for learning in certain classes of stochastic games. Artiﬁcial Intelligence, 121(1-2):31–47, April 2000.</p>
<p>[5] C. Derman. Finite state Markovian decision processes. Academic Press, 1970.</p>
<p>[6] J. Filar and K. Vrieze. Competitive Markov Decision Processes. Springer Verlag, 1996.</p>
<p>[7] L.P. Kaelbling, M. Littman, and A.W. Moore. Reinforcement learning - a survey. Journal of Artiﬁcial Intelligence Research, (4):237–285, May 1996.</p>
<p>[8] M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. In Proc. of the 15th Int. Conf. on Machine Learning, pages 260–268. Morgan Kaufmann, 1998.</p>
<p>[9] M.L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Morgan Kaufman, editor, Eleventh International Conference on Machine Learning, pages 157–163, 1994.</p>
<p>[10] S. Mahadevan. Average reward reinforcement learning: Foundations, algorithms, and empirical results. Machine Learning, 22(1):159–196, 1996.</p>
<p>[11] S. Mannor and N. Shimkin. The empirical bayes envelope approach to regret minimization in stochastic games. Technical report EE- 1262, Faculty of Electrical Engineering, Technion, Israel, October 2000.</p>
<p>[12] J.F. Mertens and A. Neyman. Stochastic games. International Journal of Game Theory, 10(2):53–66, 1981.</p>
<p>[13] A. Schwartz. A reinforcement learning method for maximizing undiscounted rewards. In Proceedings of the Tenth International Conference on Machine Learning, pages 298–305. Morgan Kaufmann, 1993.</p>
<p>[14] N. Shimkin and A. Shwartz. Guaranteed performance regions in markovian systems with competing decision makers. IEEE Trans. on Automatic Control, 38(1):84–95, January 1993.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
