<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>144 nips-2001-Partially labeled classification with Markov random walks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-144" href="../nips2001/nips-2001-Partially_labeled_classification_with_Markov_random_walks.html">nips2001-144</a> <a title="nips-2001-144-reference" href="#">nips2001-144-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>144 nips-2001-Partially labeled classification with Markov random walks</h1>
<br/><p>Source: <a title="nips-2001-144-pdf" href="http://papers.nips.cc/paper/1967-partially-labeled-classification-with-markov-random-walks.pdf">pdf</a></p><p>Author: Martin Szummer, Tommi Jaakkola</p><p>Abstract: To classify a large number of unlabeled examples we combine a limited number of labeled examples with a Markov random walk representation over the unlabeled examples. The random walk representation exploits any low dimensional structure in the data in a robust, probabilistic manner. We develop and compare several estimation criteria/algorithms suited to this representation. This includes in particular multi-way classiﬁcation with an average margin criterion which permits a closed form solution. The time scale of the random walk regularizes the representation and can be set through a margin-based criterion favoring unambiguous classiﬁcation. We also extend this basic regularization by adapting time scales for individual examples. We demonstrate the approach on synthetic examples and on text classiﬁcation problems.</p><br/>
<h2>reference text</h2><p>[1] Szummer, M; Jaakkola, T. (2000) Kernel expansions with unlabeled examples. NIPS 13.</p>
<p>[2] Jaakkola, T; Meila, M; Jebara, T. (1999) Maximum entropy discrimination. NIPS 12.</p>
<p>[3] Tishby, N; Slonim, N. (2000) Data clustering by Markovian relaxation and the Information Bottleneck Method. NIPS 13.</p>
<p>[4] Blum, A; Chawla, S. (2001) Learning from Labeled and Unlabeled Data using Graph Mincuts. ICML.</p>
<p>[5] Alon, N. et al (1997) Scale-sensitive Dimensions, Uniform Convergence, and Learnability. J. ACM, 44 (4) 615-631</p>
<p>[6] Tenenbaum, J; de Silva, V; Langford J. (2000) A Global Geometric Framework for Nonlinear Dimensionality Reduction. Science 290 (5500): 2319-2323.</p>
<p>[7] Kondor, I; Lafferty J; (2001) Diffusion kernels in continuous spaces. Tech report CMU, to appear.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
