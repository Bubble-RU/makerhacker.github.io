<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-113" href="../nips2001/nips-2001-Learning_a_Gaussian_Process_Prior_for_Automatically_Generating_Music_Playlists.html">nips2001-113</a> <a title="nips-2001-113-reference" href="#">nips2001-113-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>113 nips-2001-Learning a Gaussian Process Prior for Automatically Generating Music Playlists</h1>
<br/><p>Source: <a title="nips-2001-113-pdf" href="http://papers.nips.cc/paper/1996-learning-a-gaussian-process-prior-for-automatically-generating-music-playlists.pdf">pdf</a></p><p>Author: John C. Platt, Christopher J. C. Burges, Steven Swenson, Christopher Weare, Alice Zheng</p><p>Abstract: This paper presents AutoDJ: a system for automatically generating music playlists based on one or more seed songs selected by a user. AutoDJ uses Gaussian Process Regression to learn a user preference function over songs. This function takes music metadata as inputs. This paper further introduces Kernel Meta-Training, which is a method of learning a Gaussian Process kernel from a distribution of functions that generates the learned function. For playlist generation, AutoDJ learns a kernel from a large set of albums. This learned kernel is shown to be more effective at predicting users’ playlists than a reasonable hand-designed kernel.</p><br/>
<h2>reference text</h2><p>[1] D. Barber and C. K. I. Williams. Gaussian processes for Bayesian classiﬁcation via hybrid Monte Carlo. In M. C. Mozer, M. I. Jordan, and T. Petsche, editors, NIPS, volume 9, pages 340–346, 1997.</p>
<p>[2] J. Baxter. A Bayesian/information theoretic model of bias learning. Machine Learning, 28:7–40, 1997.</p>
<p>[3] K. P. Bennett and A. Demiriz. Semi-supervised support vector machines. In M. S. Kearns, S. A. Solla, and D. A. Cohn, editors, NIPS, volume 11, pages 368–374, 1998.</p>
<p>[4] J. S. Breese, D. Heckerman, and C. Kadie. Empirical analysis of predictive algorithms for collaborative ﬁltering. In Uncertainty in Artiﬁcial Intelligence, pages 43–52, 1998.</p>
<p>[5] R. Caruana. Learning many related tasks at the same time with backpropagation. In NIPS, volume 7, pages 657–664, 1995.</p>
<p>[6] V. Castelli and T. M. Cover. The relative value of labeled and unlabled samples in pattern recognition with an unknown mixing parameter. IEEE Trans. Info. Theory, 42(6):75–85, 1996.</p>
<p>[7] N. A. C. Cressie. Statistics for Spatial Data. Wiley, New York, 1993.</p>
<p>[8] N. Cristianini, A. Elisseeff, and J. Shawe-Taylor. On optimizing kernel alignment. Technical Report NC-TR-01-087, NeuroCOLT, 2001.</p>
<p>[9] D. Goldberg, D. Nichols, B. M. Oki, and D. Terry. Using collaborative ﬁltering to weave an information tapestry. CACM, 35(12):61–70, 1992.</p>
<p>[10] T. Minka and R. Picard. Learning how to learn is learning with points sets. http:// wwwwhite.media.mit.edu/ tpminka/papers/learning.html, 1997.</p>
<p>[11] M. Pazzani and D. Billsus. Learning and revising user proﬁles: The identiﬁcation of interesting web sites. Machine Learning, 27:313–331, 1997.</p>
<p>[12] P. S. R. S. Rao. Variance Components Estimation: Mixed models, methodologies and applications. Chapman & Hill, 1997.</p>
<p>[13] S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst? In NIPS, volume 8, pages 640–646, 1996.</p>
<p>[14] C. K. I. Williams and C. E. Rasmussen. Gaussian processes for regression. In NIPS, volume 8, pages 514–520, 1996.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
