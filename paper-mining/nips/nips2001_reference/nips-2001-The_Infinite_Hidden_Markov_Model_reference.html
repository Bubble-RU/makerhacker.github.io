<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>183 nips-2001-The Infinite Hidden Markov Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-183" href="../nips2001/nips-2001-The_Infinite_Hidden_Markov_Model.html">nips2001-183</a> <a title="nips-2001-183-reference" href="#">nips2001-183-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>183 nips-2001-The Infinite Hidden Markov Model</h1>
<br/><p>Source: <a title="nips-2001-183-pdf" href="http://papers.nips.cc/paper/1956-the-infinite-hidden-markov-model.pdf">pdf</a></p><p>Author: Matthew J. Beal, Zoubin Ghahramani, Carl E. Rasmussen</p><p>Abstract: We show that it is possible to extend hidden Markov models to have a countably inﬁnite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the inﬁnitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters deﬁne a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a ﬁnite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be inﬁnite— consider, for example, symbols being possible words appearing in English text.</p><br/>
<h2>reference text</h2><p>[1] C. E. Antoniak. Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems. Annals of Statistics, 2(6):1152–1174, 1974.</p>
<p>[2] T. S. Ferguson. A Bayesian analysis of some nonparametric problems. Annals of Statistics, 1(2):209–230, March 1973.</p>
<p>[3] D. J. C. MacKay. Ensemble learning for hidden Markov models. Technical report, Cavendish Laboratory, University of Cambridge, 1997.</p>
<p>[4] D. J. C. MacKay and L. C. Peto. A hierarchical Dirichlet language model. Natural Language Engineering, 1(3):1–19, 1995.</p>
<p>[5] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture models. Technical Report 9815, Dept. of Statistics, University of Toronto, 1998.</p>
<p>[6] L. R. Rabiner and B. H. Juang. An introduction to hidden Markov models. IEEE Acoustics, Speech & Signal Processing Magazine, 3:4–16, 1986.</p>
<p>[7] C. E. Rasmussen. The inﬁnite Gaussian mixture model. In Advances in Neural Information Processing Systems 12, Cambridge, MA, 2000. MIT Press.</p>
<p>[8] A. Stolcke and S. Omohundro. Hidden Markov model induction by Bayesian model merging. In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, Advances in Neural Information Processing Systems 5, pages 11–18, San Francisco, CA, 1993. Morgan Kaufmann.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
