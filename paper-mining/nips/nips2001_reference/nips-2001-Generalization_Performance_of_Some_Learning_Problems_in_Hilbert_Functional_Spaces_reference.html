<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-81" href="../nips2001/nips-2001-Generalization_Performance_of_Some_Learning_Problems_in_Hilbert_Functional_Spaces.html">nips2001-81</a> <a title="nips-2001-81-reference" href="#">nips2001-81-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>81 nips-2001-Generalization Performance of Some Learning Problems in Hilbert Functional Spaces</h1>
<br/><p>Source: <a title="nips-2001-81-pdf" href="http://papers.nips.cc/paper/2136-generalization-performance-of-some-learning-problems-in-hilbert-functional-spaces.pdf">pdf</a></p><p>Author: T. Zhang</p><p>Abstract: We investigate the generalization performance of some learning problems in Hilbert functional Spaces. We introduce a notion of convergence of the estimated functional predictor to the best underlying predictor, and obtain an estimate on the rate of the convergence. This estimate allows us to derive generalization bounds on some learning formulations.</p><br/>
<h2>reference text</h2><p>[1] Olivier Bousquet and Andr´ Elisseeff. Algorithmic stability and generalization perfore mance. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Processing Systems 13, pages 196–202. MIT Press, 2001.</p>
<p>[2] Nello Cristianini and John Shawe-Taylor. An Introduction to Support Vector Machines and other Kernel-based Learning Methods. Cambridge University Press, 2000.</p>
<p>[3] Wee Sun Lee, Peter L. Bartlett, and Robert C. Williamson. The importance of convexity in learning with squared loss. IEEE Trans. Inform. Theory, 44(5):1974–1980, 1998.</p>
<p>[4] R. Tyrrell Rockafellar. Convex analysis. Princeton University Press, Princeton, NJ, 1970.</p>
<p>[5] Vadim Yurinsky. Sums and Gaussian vectors. Springer-Verlag, Berlin, 1995.</p>
<p>[6] Tong Zhang. Convergence of large margin separable linear classiﬁcation. In Advances in Neural Information Processing Systems 13, pages 357–363, 2001.</p>
<p>[7] Tong Zhang. A leave-one-out cross validation bound for kernel methods with applications in learning. In 14th Annual Conference on Computational Learning Theory, pages 427–443, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
