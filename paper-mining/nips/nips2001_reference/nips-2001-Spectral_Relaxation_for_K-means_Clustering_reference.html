<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>171 nips-2001-Spectral Relaxation for K-means Clustering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-171" href="../nips2001/nips-2001-Spectral_Relaxation_for_K-means_Clustering.html">nips2001-171</a> <a title="nips-2001-171-reference" href="#">nips2001-171-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>171 nips-2001-Spectral Relaxation for K-means Clustering</h1>
<br/><p>Source: <a title="nips-2001-171-pdf" href="http://papers.nips.cc/paper/1992-spectral-relaxation-for-k-means-clustering.pdf">pdf</a></p><p>Author: Hongyuan Zha, Xiaofeng He, Chris Ding, Ming Gu, Horst D. Simon</p><p>Abstract: The popular K-means clustering partitions a data set by minimizing a sum-of-squares cost function. A coordinate descend method is then used to find local minima. In this paper we show that the minimization can be reformulated as a trace maximization problem associated with the Gram matrix of the data vectors. Furthermore, we show that a relaxed version of the trace maximization problem possesses global optimal solutions which can be obtained by computing a partial eigendecomposition of the Gram matrix, and the cluster assignment for each data vectors can be found by computing a pivoted QR decomposition of the eigenvector matrix. As a by-product we also derive a lower bound for the minimum of the sum-of-squares cost function. 1</p><br/>
<h2>reference text</h2><p>[1] P. S. Bradley and Usama M. Fayyad. (1998). R efining Initial Points for K-Means Clustering. Proc. 15th International Conf. on Machine Learning, 91- 99.</p>
<p>[2] P. S. Bradley, K. Bennett and A. Demiritz. Constrained K-means Clustering. Microsoft Research, MSR-TR-2000-65, 2000.</p>
<p>[3] M. Girolani. (2001). Mercer Kernel Based Clustering in Feature Space. To appear in IEEE Transactions on Neural Networks.</p>
<p>[4] G. Golub and C. Van Loan . (1996) . Matrix Computations. Johns Hopkins University Press, 3rd Edition.</p>
<p>[5] Ming Gu, Hongyuan Zha, Chris Ding, Xiaofeng He and Horst Simon. (2001) . Spectral Embedding for K- Way Graph Clustering. Technical Report, Department of Computer Science and Engineering, CSE-OI-007, Pennsylvania State University.</p>
<p>[6] J.A. Hartigan and M.A. Wong. (1979). A K-means Clustering Algorithm. Applied Statistics, 28:100- 108.</p>
<p>[7] L. Lovasz and M.D. Plummer. (1986) Matching Theory. Amsterdam: North Holland.</p>
<p>[8] A. McCallum. Bow: A toolkit for statistical language modeling, text retrieval, classification and clustering. http : //www . CS. cmu. edu/ mccallum/bow.</p>
<p>[9] B. Schi:ilkopf, A. Smola and K.R. Miiller. (1998). Nonlinear Component Analysis as a Kernel Eigenvalue Problem. N eural Computation, 10: 1299- 1219.</p>
<p>[10] N . Slonim and N. Tishby. (2000). Document clustering using word clusters via the information bottleneck method. Proceedings of SIGIR-2000.</p>
<p>[11] G.W. Stewart and J.G. Sun. (1990). Matrix Perturbation Theory. Academic Press, San Diego , CA.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
