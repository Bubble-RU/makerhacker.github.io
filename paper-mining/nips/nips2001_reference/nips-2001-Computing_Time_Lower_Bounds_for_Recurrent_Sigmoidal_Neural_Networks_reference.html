<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-52" href="../nips2001/nips-2001-Computing_Time_Lower_Bounds_for_Recurrent_Sigmoidal_Neural_Networks.html">nips2001-52</a> <a title="nips-2001-52-reference" href="#">nips2001-52-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>52 nips-2001-Computing Time Lower Bounds for Recurrent Sigmoidal Neural Networks</h1>
<br/><p>Source: <a title="nips-2001-52-pdf" href="http://papers.nips.cc/paper/2111-computing-time-lower-bounds-for-recurrent-sigmoidal-neural-networks.pdf">pdf</a></p><p>Author: M. Schmitt</p><p>Abstract: Recurrent neural networks of analog units are computers for realvalued functions. We study the time complexity of real computation in general recurrent neural networks. These have sigmoidal, linear, and product units of unlimited order as nodes and no restrictions on the weights. For networks operating in discrete time, we exhibit a family of functions with arbitrarily high complexity, and we derive almost tight bounds on the time required to compute these functions. Thus, evidence is given of the computational limitations that time-bounded analog recurrent neural networks are subject to. 1</p><br/>
<h2>reference text</h2><p>Anthony, M. and Bartlett, P. L. (1999). Neural Network Learning: Theoretical Foundations. Cambridge University Press, Cambridge. Balcazar, J. , Gavalda, R., and Siegelmann, H. T. (1997). Computational power of neural networks: A characterization in terms of Kolmogorov complexity. IEEE Transcations on Information Theory, 43: 1175- 1183. Blum, L., Cucker, F. , Shub, M. , and Smale, S. (1998) . Complexity and Real Computation. Springer-Verlag, New York. Carrasco, R. C., Forcada, M. L., Valdes-Munoz, M. A. , and Neco, R. P. (2000). Stable encoding of finite state machines in discrete-time recurrent neural nets with sigmoid units. Neural Computation, 12:2129- 2174. Durbin, R. and Rumelhart, D. (1989). Product units: A computationally powerful and biologically plausible extension to backpropagation networks. Neural Computation, 1:133- 142. Gavalda, R. and Siegelmann, H. T . (1999) . Discontinuities in recurrent neural networks. Neural Computation, 11:715- 745. Haykin, S. (1999). Neural Networks : A Comprehensive Foundation. Prentice Hall, Upper Saddle River, NJ , second edition. Kilian, J. and Siegelmann, H. T. (1996). The dynamic universality of sigmoidal neural networks. Information and Computation, 128:48- 56. Koiran, P. and Sontag, E . D. (1998). Vapnik-Chervonenkis dimension of recurrent neural networks. Discrete Applied Mathematics, 86:63- 79. Maass, W., NatschUiger, T., and Markram, H. (2001). Real-time computing without stable states: A new framework for neural computation based on perturbations. Preprint. Maass, W. and Orponen, P. (1998). On the effect of analog noise in discrete-time analog computations. Neural Computation, 10:1071- 1095. Maass, W. and Sontag, E . D. (1999). Analog neural nets with Gaussian or other common noise distributions cannot recognize arbitrary regular languages. Neural Computation, 11:771- 782. amlin, C. W. and Giles, C. L. (1996). Constructing deterministic finite-state automata in recurrent neural networks. Journal of the Association for Computing Machinery, 43:937- 972. Schmitt, M. (2002). On the complexity of computing and learning with multiplicative neural networks. Neural Computation, 14. In press. Siegelmann, H. T . (1999). Neural Networks and Analog Computation: Beyond the Turing Limit. Progress in Theoretical Computer Science. Birkhiiuser, Boston. Siegelmann, H. T. and Sontag, E. D. (1995). On the computational power of neural nets. Journal of Computer and System Sciences, 50:132- 150. Sima, J. and Orponen, P. (2001). Exponential transients in continuous-time symmetric Hopfield nets. In Dorffner, G., Bischof, H. , and Hornik, K. , editors , Proceedings of the International Conference on Artificial Neural Networks ICANN 2001, volume 2130 of Lecture Notes in Computer Science, pages 806- 813, Springer, Berlin.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
