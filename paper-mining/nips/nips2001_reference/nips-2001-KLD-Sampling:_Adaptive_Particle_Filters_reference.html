<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 nips-2001-KLD-Sampling: Adaptive Particle Filters</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-102" href="../nips2001/nips-2001-KLD-Sampling%3A_Adaptive_Particle_Filters.html">nips2001-102</a> <a title="nips-2001-102-reference" href="#">nips2001-102-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>102 nips-2001-KLD-Sampling: Adaptive Particle Filters</h1>
<br/><p>Source: <a title="nips-2001-102-pdf" href="http://papers.nips.cc/paper/1998-kld-sampling-adaptive-particle-filters.pdf">pdf</a></p><p>Author: Dieter Fox</p><p>Abstract: Over the last years, particle ﬁlters have been applied with great success to a variety of state estimation problems. We present a statistical approach to increasing the efﬁciency of particle ﬁlters by adapting the size of sample sets on-the-ﬂy. The key idea of the KLD-sampling method is to bound the approximation error introduced by the sample-based representation of the particle ﬁlter. The name KLD-sampling is due to the fact that we measure the approximation error by the Kullback-Leibler distance. Our adaptation approach chooses a small number of samples if the density is focused on a small part of the state space, and it chooses a large number of samples if the state uncertainty is high. Both the implementation and computation overhead of this approach are small. Extensive experiments using mobile robot localization as a test application show that our approach yields drastic improvements over particle ﬁlters with ﬁxed sample set sizes and over a previously introduced adaptation technique.</p><br/>
<h2>reference text</h2><p>[1] I. J. Cox and G. T. Wilfong, editors. Autonomous Robot Vehicles. Springer Verlag, 1990.</p>
<p>[2] P. Del Moral and L. Miclo. Branching and interacting particle systems approximations of feynamkac formulae with applications to non linear ﬁltering. In Seminaire de Probabilites XXXIV, number 1729 in Lecture Notes in Mathematics. Springer-Verlag, 2000.</p>
<p>[3] A. Doucet, N. de Freitas, and N. Gordon, editors. Sequential Monte Carlo in Practice. SpringerVerlag, New York, 2001.</p>
<p>[4] A. Doucet, S.J. Godsill, and C. Andrieu. On sequential monte carlo sampling methods for Bayesian ﬁltering. Statistics and Computing, 10(3), 2000.</p>
<p>[5] D. Fox, W. Burgard, F. Dellaert, and S. Thrun. Monte Carlo Localization: Efﬁcient position estimation for mobile robots. In Proc. of the National Conference on Artiﬁcial Intelligence (AAAI), 1999.</p>
<p>[6] D. Fox, S. Thrun, F. Dellaert, and W. Burgard. Particle ﬁlters for mobile robot localization. In Doucet et al. [3].</p>
<p>[7] N. Johnson, S. Kotz, and N. Balakrishnan. Continuous univariate distributions, volume 1. John Wiley & Sons, New York, 1994.</p>
<p>[8] D. Koller and R. Fratkina. Using learning for approximation in stochastic processes. In Proc. of the International Conference on Machine Learning (ICML), 1998.</p>
<p>[9] A. W. Moore, J. Schneider, and K. Deng. Efﬁcient locally weighted polynomial regression predictions. In Proc. of the International Conference on Machine Learning (ICML), 1997.</p>
<p>[10] M. Pelikan, D.E. Goldberg, and E. Cant-Paz. Bayesian optimization algorithm, population size, and time to convergence. In Proc. of the Genetic and Evolutionary Computation Conference (GECCO), 2000.</p>
<p>[11] M. K. Pitt and N. Shephard. Filtering via simulation: auxiliary particle ﬁlters. Journal of the American Statistical Association, 94(446), 1999.</p>
<p>[12] J.A. Rice. Mathematical Statistics and Data Analysis. Duxbury Press, second edition, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
