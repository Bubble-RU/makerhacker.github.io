<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-55" href="../nips2001/nips-2001-Convergence_of_Optimistic_and_Incremental_Q-Learning.html">nips2001-55</a> <a title="nips-2001-55-reference" href="#">nips2001-55-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>55 nips-2001-Convergence of Optimistic and Incremental Q-Learning</h1>
<br/><p>Source: <a title="nips-2001-55-pdf" href="http://papers.nips.cc/paper/1944-convergence-of-optimistic-and-incremental-q-learning.pdf">pdf</a></p><p>Author: Eyal Even-dar, Yishay Mansour</p><p>Abstract: Vie sho,v the convergence of tV/O deterministic variants of Qlearning. The first is the widely used optimistic Q-learning, which initializes the Q-values to large initial values and then follows a greedy policy with respect to the Q-values. We show that setting the initial value sufficiently large guarantees the converges to an Eoptimal policy. The second is a new and novel algorithm incremental Q-learning, which gradually promotes the values of actions that are not taken. We show that incremental Q-learning converges, in the limit, to the optimal policy. Our incremental Q-learning algorithm can be viewed as derandomization of the E-greedy Q-learning. 1</p><br/>
<h2>reference text</h2><p>[1] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]  Scientific, Belmont, MA, 1996. V.S. Borkar and S.P. Meyn. The O.D.E. method for convergence of stochastic approximation and reinforcement learning. Siam J. control, 38 (2):447-69, 2000. R. I. Brafman and M. Tennenholtz. R-max - a general polynomial time algorithm for near-optimal reinforcement learning. m IJCAI, 2001. E. Even-Dar and Y. Mansour. Learning rates for Q-Iearning. m COLT, 2001. J. C. Gittins and D. M. Jones. A dynamic allocation index for the sequential design of experiments. Progress in Statistics, pages 241 -266, 1974. T. Jaakkola, M. I. Jordan, and S. P. Singh. On the convergence of stochastic iterative dynamic programming algorithms. Neural Computation, 6, 1994. M. Kearns and S. Singh. Efficient reinforcement learning: theoretical framework and algorithms. In fCML, 1998. M. Littman and Cs. Szepesvari. A generalized reinforcement learning model: convergence and applications. m ICML, 1996. M.L Puterman. Markov Decision Processes - Discrete Stochastic Dynamic Programming. John Wiley & Sons. mc., New York, NY, 1994.</p>
<p>[10] R. S. Sutton and A. G. Bato. Reinforcement Learning. MIT press, 1998.</p>
<p>[11] J. N. Tsitsiklis. Asynchronous stochastic approximation and Q-Iearning. Machine Learning, 16:185-202, 1994.</p>
<p>[12] C. Watkins and P. Dyan. Q-Iearning. Machine Learning, 8(3/4):279 -292, 1992.</p>
<p>[13] C. Watkins. Learning from Delayed Rewards. PhD thesis, Cambridge University, 1989.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
