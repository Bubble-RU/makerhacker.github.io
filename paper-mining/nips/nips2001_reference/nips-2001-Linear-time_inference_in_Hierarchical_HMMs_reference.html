<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>115 nips-2001-Linear-time inference in Hierarchical HMMs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-115" href="../nips2001/nips-2001-Linear-time_inference_in_Hierarchical_HMMs.html">nips2001-115</a> <a title="nips-2001-115-reference" href="#">nips2001-115-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>115 nips-2001-Linear-time inference in Hierarchical HMMs</h1>
<br/><p>Source: <a title="nips-2001-115-pdf" href="http://papers.nips.cc/paper/2050-linear-time-inference-in-hierarchical-hmms.pdf">pdf</a></p><p>Author: Kevin P. Murphy, Mark A. Paskin</p><p>Abstract: The hierarchical hidden Markov model (HHMM) is a generalization of the hidden Markov model (HMM) that models sequences with structure at many length/time scales [FST98]. Unfortunately, the original infertime, where is ence algorithm is rather complicated, and takes the length of the sequence, making it impractical for many domains. In this paper, we show how HHMMs are a special kind of dynamic Bayesian network (DBN), and thereby derive a much simpler inference algorithm, which only takes time. Furthermore, by drawing the connection between HHMMs and DBNs, we enable the application of many standard approximation techniques to further speed up inference. ¥ ©§ £ ¨¦¥¤¢ © £ ¦¥¤¢</p><br/>
<h2>reference text</h2><p>[BFGK96] C. Boutilier, N. Friedman, M. Goldszmidt, and D. Koller. Context-Speciﬁc Independence in Bayesian Networks. In UAI, 1996. [BVW00]  H. Bui, S. Venkatesh, and G. West. On the recognition of abstract Markov policies. In AAAI, 2000.  [BVW01]  H. Bui, S. Venkatesh, and G. West. Tracking and surveillance in wide-area spatial environments using the Abstract Hidden Markov Model. Intl. J. of Pattern Rec. and AI, 2001.  [FST98]  Shai Fine, Yoram Singer, and Naftali Tishby. The hierarchical Hidden Markov Model: Analysis and applications. Machine Learning, 32:41, 1998.  [GJ97]  Z. Ghahramani and M. Jordan. Factorial hidden Markov models. Machine Learning, 29:245–273, 1997.  [HIM 00] M. Hu, C. Ingram, M.Sirski, C. Pal, S. Swamy, and C. Patten. A Hierarchical HMM Implementation for Vertebrate Gene Splice Site Prediction. Technical report, Dept. Computer Science, Univ. Waterloo, 2000.    [Hoe01]  J. Hoey. Hierarchical unsupervised learning of facial expression categories. In ICCV Workshop on Detection and Recognition of Events in Video, 2001.  [IB00]  Y. Ivanov and A. Bobick. Recognition of visual activities and interactions by stochastic parsing. IEEE Trans. on Pattern Analysis and Machine Intelligence, 22(8):852–872, 2000.  [JGS96]  M. I. Jordan, Z. Ghahramani, and L. K. Saul. Hidden Markov decision trees. In NIPS, 1996.  [Kja90]  U. Kjaerulff. Triangulation of graphs – algorithms giving small total state space. Technical Report R-90-09, Dept. of Math. and Comp. Sci., Aalborg Univ., Denmark, 1990.  [LY90]  K. Lari and S. J. Young. The estimation of stochastic context-free grammars using the Inside-Outside algorithm. Computer Speech and Language, 4:35–56, 1990.  [ME01]  D. Moore and I. Essa. Recognizing multitasked activities using stochastic context-free grammar. In CVPR Workshop on Models vs Exemplars in Computer Vision, 2001.  [Mur99]  K. Murphy. Pearl’s algorithm and multiplexer nodes. Technical report, U.C. Berkeley, Dept. Comp. Sci., 1999.  [Mur01]  K. Murphy. Applying the junction tree algorithm to variable-length DBNs. Technical report, Comp. Sci. Div., UC Berkeley, 2001.  [MW01]  K. Murphy and Y. Weiss. The Factored Frontier Algorithm for Approximate Inference in DBNs. In UAI, 2001.  [NI00]  A. Neﬁan and M. Hayes III. Maximum likelihood training of the embedded HMM for face detection and recognition. In IEEE Intl. Conf. on Image Processing, 2000.  [PR97]  R. Parr and S. Russell. Reinforcement learning with hierarchies of machines. In NIPS, 1997.  [Rab89]  L. R. Rabiner. A tutorial on Hidden Markov Models and selected applications in speech recognition. Proc. of the IEEE, 77(2):257–286, 1989.  [SJ99]  L. Saul and M. Jordan. Mixed memory markov models: Decomposing complex stochastic processes as mixture of simpler ones. Machine Learning, 37(1):75–87, 1999.  [SPS99]  R.S. Sutton, D. Precup, and S. Singh. Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artiﬁcial Intelligence, 112:181–211, 1999.  [TRM01]  G. Theocharous, K. Rohanimanesh, and S. Mahadevan. Learning Hierarchical Partially Observed Markov Decision Process Models for Robot Navigation. In ICRA, 2001.  [Zwe97]  G. Zweig. Speech Recognition with Dynamic Bayesian Networks. PhD thesis, U.C. Berkeley, Dept. Comp. Sci., 1997.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
