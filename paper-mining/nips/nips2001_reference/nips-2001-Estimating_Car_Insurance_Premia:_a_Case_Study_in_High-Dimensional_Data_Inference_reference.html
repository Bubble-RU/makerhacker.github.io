<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-70" href="../nips2001/nips-2001-Estimating_Car_Insurance_Premia%3A_a_Case_Study_in_High-Dimensional_Data_Inference.html">nips2001-70</a> <a title="nips-2001-70-reference" href="#">nips2001-70-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>70 nips-2001-Estimating Car Insurance Premia: a Case Study in High-Dimensional Data Inference</h1>
<br/><p>Source: <a title="nips-2001-70-pdf" href="http://papers.nips.cc/paper/2062-estimating-car-insurance-premia-a-case-study-in-high-dimensional-data-inference.pdf">pdf</a></p><p>Author: Nicolas Chapados, Yoshua Bengio, Pascal Vincent, Joumana Ghosn, Charles Dugas, Ichiro Takeuchi, Linyan Meng</p><p>Abstract: Estimating insurance premia from data is a difficult regression problem for several reasons: the large number of variables, many of which are .discrete, and the very peculiar shape of the noise distribution, asymmetric with fat tails, with a large majority zeros and a few unreliable and very large values. We compare several machine learning methods for estimating insurance premia, and test them on a large data base of car insurance policies. We find that function approximation methods that do not optimize a squared loss, like Support Vector Machines regression, do not work well in this context. Compared methods include decision trees and generalized linear models. The best results are obtained with a mixture of experts, which better identifies the least and most risky contracts, and allows to reduce the median premium by charging more to the most risky customers. 1</p><br/>
<h2>reference text</h2><p>Bailey, R. A. and Simon, L. (1960). Two studies in automobile insurance ratemaking. ASTIN Bulletin, 1(4):192-217. Biggs, D., Ville, B., and Suen, E. (1991). A method of choosing multiway partitions for classification and decision trees. Journal of Applied Statistics, 18(1):49-62. Dugas, C., Bengio, Y., Belisle, F., and Nadeau, C. (2001). Incorporating second order functionalÂ· knowledge into learning algorithms. In Leen, T., Dietterich, T., and Tresp, V., editors, Advances in Neural Information Processing Systems, volume 13, pages 472-478. F.R.Hampel, E.M.Ronchetti, P.J.Rousseeuw, and W.A.Stahel (1986). Robust Statistics, The Approach based on Influence Functions. John Wiley & Sons. Huber, P. (1982). Robust Statistics. John Wiley & Sons Inc. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. (1991). Adaptive mixture of local experts. Neural Computation, 3:79-87. Kass, G. (1980). An exploratory technique for investigating large quantities of categorical data. Applied Statistics, 29(2):119-127. McCullagh, P. and NeIder, J. (1989). Generalized Linear Models. Chapman and Hall, London. ' Rousseeuw, P. and Leroy, A. (1987). Robust Regression and Outlier Detection. John Wiley & Sons Inc. Vapnik, V. (1998). Statistical Learning Theory. Wiley, Lecture Notes in Economics and Mathematical Systems, volume 454.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
