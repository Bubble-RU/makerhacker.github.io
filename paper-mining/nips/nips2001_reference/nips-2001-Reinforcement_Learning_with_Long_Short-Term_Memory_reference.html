<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>161 nips-2001-Reinforcement Learning with Long Short-Term Memory</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-161" href="../nips2001/nips-2001-Reinforcement_Learning_with_Long_Short-Term_Memory.html">nips2001-161</a> <a title="nips-2001-161-reference" href="#">nips2001-161-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>161 nips-2001-Reinforcement Learning with Long Short-Term Memory</h1>
<br/><p>Source: <a title="nips-2001-161-pdf" href="http://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory.pdf">pdf</a></p><p>Author: Bram Bakker</p><p>Abstract: This paper presents reinforcement learning with a Long ShortTerm Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage(,x) learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevant events. This is demonstrated in a T-maze task, as well as in a difficult variation of the pole balancing task. 1</p><br/>
<h2>reference text</h2><p>[1] B. Bakker. Reinforcement learning with LSTM in non-Markovian tasks with longterm dependencies. Technical report, Dept. of Psychology, Leiden University, 2001.</p>
<p>[2] L. Chrisman. Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. In Proc. of the 10th National Conf. on AI AAAI Press, 1992.</p>
<p>[3] F. Gers, J. Schmidhuber, and F. Cummins. Learning to forget: Continual prediction with LSTM. Neural Computation, 12 (10):2451-2471, 2000.</p>
<p>[4] M. E. Harmon and L. C. Baird. Multi-player residual advantage learning with general function approximation. Technical report, Wright-Patterson Air Force Base, 1996.</p>
<p>[5] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9 (8):1735-1780, 1997.</p>
<p>[6] L.-J. Lin and T. Mitchell. Reinforcement learning with hidden states. In Proc. of the 2nd Int. Conf. on Simulation of Adaptive Behavior. MIT Press, 1993.</p>
<p>[7] J. Loch and S. Singh. Using eligibility traces to find the best memoryless policy in Partially Observable Markov Decision Processes. In Proc. of ICML'98, 1998.</p>
<p>[8] R. A. McCallum. Learning to use selective attention and short-term memory in sequential tasks. In Proc. 4th Int. Conf. on Simulation of Adaptive Behavior, 1996.</p>
<p>[9] L. Peshkin, N. Meuleau, and L. P. Kaelbling. Learning policies with external memory. In Proc. of the 16th Int. Conf. on Machine Learning, 1999.</p>
<p>[10] J. Schmidhuber. Networks adjusting networks. In Proc. of Distributed Adaptive Neural Information Processing, St. Augustin, 1990.</p>
<p>[11] J. Schmidhuber. Curious model-building control systems. In Proc. of IJCNN'91, volume 2, pages 1458-1463, Singapore, 1991.</p>
<p>[12] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT Press, Cambridge; MA, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
