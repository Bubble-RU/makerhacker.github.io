<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 nips-2001-Discriminative Direction for Kernel Classifiers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-60" href="../nips2001/nips-2001-Discriminative_Direction_for_Kernel_Classifiers.html">nips2001-60</a> <a title="nips-2001-60-reference" href="#">nips2001-60-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>60 nips-2001-Discriminative Direction for Kernel Classifiers</h1>
<br/><p>Source: <a title="nips-2001-60-pdf" href="http://papers.nips.cc/paper/1985-discriminative-direction-for-kernel-classifiers.pdf">pdf</a></p><p>Author: Polina Golland</p><p>Abstract: In many scientiﬁc and engineering applications, detecting and understanding differences between two groups of examples can be reduced to a classical problem of training a classiﬁer for labeling new examples while making as few mistakes as possible. In the traditional classiﬁcation setting, the resulting classiﬁer is rarely analyzed in terms of the properties of the input data captured by the discriminative model. However, such analysis is crucial if we want to understand and visualize the detected differences. We propose an approach to interpretation of the statistical model in the original feature space that allows us to argue about the model in terms of the relevant changes to the input vectors. For each point in the input space, we deﬁne a discriminative direction to be the direction that moves the point towards the other class while introducing as little irrelevant change as possible with respect to the classiﬁer function. We derive the discriminative direction for kernel-based classiﬁers, demonstrate the technique on several examples and brieﬂy discuss its use in the statistical shape analysis, an application that originally motivated this work.</p><br/>
<h2>reference text</h2><p>[1] S. Amari and S. Wu. Improving Support Vector Machines by Modifying Kernel Functions. Neural Networks, 783-789, 1999.</p>
<p>[2] S. Amari. Natural Gradient Works Efﬁciently in Learning. Neural Comp., 10:251-276, 1998.</p>
<p>[3] C. J. C. Burges. A Tutorial on Support Vector Machines for Pattern Recognition. Data Mining and Knowledge Discovery, 2(2):121-167, 1998.</p>
<p>[4] C. J. C. Burges. Geometry and Invariance in Kernel Based Methods. In Adv. in Kernel Methods: Support Vector Learning, Eds. Sch¨ lkopf, Burges and Smola, MIT Press, o 89-116, 1999.</p>
<p>[5] P. Golland et al. Small Sample Size Learning for Shape Analysis of Anatomical Structures. In Proc. of MICCAI’2000, LNCS 1935:72-82, 2000.</p>
<p>[6] B. Sch¨ lkopf et al. Input Space vs. Feature Space in Kernel-Based Methods. IEEE o Trans. on Neural Networks, 10(5):1000-1017, 1999.</p>
<p>[7] B. Sch¨ lkopf, A. Smola, and K.-R. M¨ ller. Nonlinear Component Analysis as a Kero u nel Eigenvalue Problem. Neural Comp., 10:1299-1319, 1998.</p>
<p>[8] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.</p>
<p>[9] V. N. Vapnik. Statistical Learning Theory. John Wiley & Sons, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
