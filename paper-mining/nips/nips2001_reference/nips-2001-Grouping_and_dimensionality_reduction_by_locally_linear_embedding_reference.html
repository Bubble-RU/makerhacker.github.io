<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-88" href="../nips2001/nips-2001-Grouping_and_dimensionality_reduction_by_locally_linear_embedding.html">nips2001-88</a> <a title="nips-2001-88-reference" href="#">nips2001-88-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>88 nips-2001-Grouping and dimensionality reduction by locally linear embedding</h1>
<br/><p>Source: <a title="nips-2001-88-pdf" href="http://papers.nips.cc/paper/2033-grouping-and-dimensionality-reduction-by-locally-linear-embedding.pdf">pdf</a></p><p>Author: Marzia Polito, Pietro Perona</p><p>Abstract: Locally Linear Embedding (LLE) is an elegant nonlinear dimensionality-reduction technique recently introduced by Roweis and Saul [2]. It fails when the data is divided into separate groups. We study a variant of LLE that can simultaneously group the data and calculate local embedding of each group. An estimate for the upper bound on the intrinsic dimension of the data set is obtained automatically. 1</p><br/>
<h2>reference text</h2><p>[1] C. Bishop, Neural Networks for Pattern Recognition, Oxford Univ. Press, (1995).</p>
<p>[2] S. T. Roweis, L.K.Saul, Science, 290, p. 2323-2326, (2000).</p>
<p>[3] J. Tenenbaum , V. de Silva, J. Langford, Science, 290, p. 2319-2323, (2000).  A  Appendix  In Proposition 2 of Section 4 we proved that during the LLE procedure we can automatically detect the number of K -connected components, in case there is no noise. Similarly, in Proposition 1 of Section 3 we proved that under ideal conditions (no noise, locally flat data), we can determine an estimate for the intrinsic dimension of the data. Our next goal is to establish a certain robustness of these results in the case there is numerical noise, or the components are not completely separated, or the data is not exactly locally flat . In general, suppose we have a non degenerate matrix A, and an orthonormal basis of eigenvectors VI, ... , V m , with eigenvalues AI , ... Am. As a consequence of a small perturbation of the matrix into A + dA, we will have eigenvectors Vi + dVi with eigenvalues Ai + dAi' The unitary norm constraint makes sure that dVi is orthogonal to Vi and could be therefore written as dVi = L:k#i O'.ikVk. Using again the orthonormality, one can derive expressions for the perturbations of Ai and Vi : dAi O'.ij (Ai - Aj)  < vi,dAvi >  < Vj,dAVi > .  This shows that if the perturbation dA has order E, then the perturbations dA and are also of order E. Notice that we are not interested in perturbations O'.ij within the eigenspace of eigenvalue 0, but rather those orthogonal to it, and therefore Ai =j:. Aj. O'.ij</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
