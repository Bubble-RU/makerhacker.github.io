<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>167 nips-2001-Semi-supervised MarginBoost</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-167" href="../nips2001/nips-2001-Semi-supervised_MarginBoost.html">nips2001-167</a> <a title="nips-2001-167-reference" href="#">nips2001-167-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>167 nips-2001-Semi-supervised MarginBoost</h1>
<br/><p>Source: <a title="nips-2001-167-pdf" href="http://papers.nips.cc/paper/2108-semi-supervised-marginboost.pdf">pdf</a></p><p>Author: Florence D'alch√©-buc, Yves Grandvalet, Christophe Ambroise</p><p>Abstract: In many discrimination problems a large amount of data is available but only a few of them are labeled. This provides a strong motivation to improve or develop methods for semi-supervised learning. In this paper, boosting is generalized to this task within the optimization framework of MarginBoost . We extend the margin definition to unlabeled data and develop the gradient descent algorithm that corresponds to the resulting margin cost function. This meta-learning scheme can be applied to any base classifier able to benefit from unlabeled data. We propose here to apply it to mixture models trained with an Expectation-Maximization algorithm. Promising results are presented on benchmarks with different rates of labeled data. 1</p><br/>
<h2>reference text</h2><p>[1] C. Ambroise and G. Govaert. EM algorithm for partially known labels. In IFCS 2000, july 2000.</p>
<p>[2] J.-P. Aubin. L 'analyse non lineaire et ses applications d l'economie. Masson , 1984.</p>
<p>[3] K P. Bennett and A. Demiriz. Semi-supervised support vector machines. In D. Cohn, M. Kearns, and S. Solla, editors, Advances in Neural Information Processing Systems, pages 368-374. MIT Press, 1999.</p>
<p>[4] C.M. Bishop and M.E. Tipping. A hierarchical latent variable model for data vizualization. IEEE PAMI, 20:281- 293, 1998.</p>
<p>[5] A. Blum and Tom Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of th e 1998 Conference on Computational Learning Th eory, July 1998.</p>
<p>[6] L. Breiman. Prediction games and arcing algorithms. Technical Report 504, Statistics Department , University of California at Berkeley, 1997.</p>
<p>[7] Y . Freund and R. E. Schapire. Experiments with a new boosting algorithm. In Machin e Learning: Proceedings of th e Thirteenth International Conference, pages 148- 156. Morgan Kauffman, 1996.</p>
<p>[8] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. The Annals of Statistics, 28(2):337- 407, 2000.</p>
<p>[9] Y. Grandvalet, F. d'Alche Buc, and C. Ambroise. Boosting mixture models for semisupervised learning. In ICANN 2001 , august 200l.</p>
<p>[10] L. Mason , J. Baxter, P. L. Bartlett, and M. Frean. Functional gradient techniques for combining hypotheses. In Advances in Large Margin Classifiers. MIT, 2000.</p>
<p>[11] G.J. McLachlan and T. Krishnan. Th e EM algorithm and extensions. Wiley, 1997.</p>
<p>[12] K Nigam, A. K McCallum, S. Thrun, and T. Mitchell. Text classification from labeled and unlabeled documents using EM. Machine learning, 39(2/3):135- 167, 2000.</p>
<p>[13] G. Riitsch, T. Onoda, and K-R. Muller. Soft margins for AdaBoost. Technical report, Department of Computer Science, Royal Holloway, London , 1998.</p>
<p>[14] G. Riitsch, T. Onoda, and K-R. Muller. Soft margins for AdaBoost. Machine Learning, 42(3):287- 320, 200l.</p>
<p>[15] R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. Th e Annals of Statistics, 26(5):1651- 1686, 1998.</p>
<p>[16] Matthias Seeger. Learning with data,www.citeseer.nj.nec.com/seegerOllearning.html.  labeled  and  unlabeled</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
