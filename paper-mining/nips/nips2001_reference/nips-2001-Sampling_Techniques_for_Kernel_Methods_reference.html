<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>164 nips-2001-Sampling Techniques for Kernel Methods</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-164" href="../nips2001/nips-2001-Sampling_Techniques_for_Kernel_Methods.html">nips2001-164</a> <a title="nips-2001-164-reference" href="#">nips2001-164-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>164 nips-2001-Sampling Techniques for Kernel Methods</h1>
<br/><p>Source: <a title="nips-2001-164-pdf" href="http://papers.nips.cc/paper/2072-sampling-techniques-for-kernel-methods.pdf">pdf</a></p><p>Author: Dimitris Achlioptas, Frank Mcsherry, Bernhard Schölkopf</p><p>Abstract: We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function by a “randomized kernel” which behaves like in expectation.</p><br/>
<h2>reference text</h2><p>[1] D. Achlioptas, Database-friendly random projections, Proc. of the 20th Symposium on Principle of Database Systems (Santa Barbara, California), 2001, pp. 274–281.</p>
<p>[2] C. J. C. Burges, Simpliﬁed support vector decision rules, Proc. of the 13th International Conference on Machine Learning, Morgan Kaufmann, 1996, pp. 71–77.</p>
<p>[3] N. Cristianini, J. Shawe-Taylor, and H. Lodhi, Latent semantic kernels, Proc. of the 18th International Conference on Machine Learning, Morgan Kaufman, 2001.</p>
<p>[4] C. Davis and W. Kahan, The rotation of eigenvectors by a perturbation 3, SIAM Journal on Numerical Analysis 7 (1970), 1–46.</p>
<p>[5] Z. F¨ redi and J. Koml´ s, The eigenvalues of random symmetric matrices, Combinau o torica 1 (1981), no. 3, 233–241.</p>
<p>[6] N. I. M. Gould, An algorithm for large-scale quadratic programming, IMA Journal of Numerical Analysis 11 (1991), no. 3, 299–324.</p>
<p>[7] W. Hoeffding, Probability inequalities for sums of bounded random variables, Journal of the American Statistical Association 58 (1963), 13–30.</p>
<p>[8] W. B. Johnson and J. Lindenstrauss, Extensions of Lipschitz mappings into a Hilbert space, Conference in modern analysis and probability (New Haven, Conn., 1982), American Mathematical Society, 1984, pp. 189–206.</p>
<p>[9] R. H. Nickel and J. W. Tolle, A sparse sequential quadratic programming algorithm, Journal of Optimization Theory and Applications 60 (1989), no. 3, 453–473.</p>
<p>[10] E. Osuna, R. Freund, and F. Girosi, An improved training algorithm for support vector machines, Neural Networks for Signal Processing VII, 1997, pp. 276–285.</p>
<p>[11] B. Sch¨ lkopf, A. J. Smola, and K.-R. M¨ ller, Nonlinear component analysis as a o u kernel eigenvalue problem, Neural Computation 10 (1998), 1299–1319.</p>
<p>[12] A. J. Smola and B. Sch¨ lkopf, Sparse greedy matrix approximation for machine o learning, Proc. of the 17th International Conference on Machine Learning, Morgan Kaufman, 2000, pp. 911–918.</p>
<p>[13] V. Vapnik, The nature of statistical learning theory, Springer, NY, 1995.</p>
<p>[14] C. K. I. Williams and M. Seeger, Using the Nystrom method to speed up kernel machines, Advances in Neural Information Processing Systems 2000, MIT Press, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
