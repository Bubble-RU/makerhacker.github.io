<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-9" href="../nips2001/nips-2001-A_Generalization_of_Principal_Components_Analysis_to_the_Exponential_Family.html">nips2001-9</a> <a title="nips-2001-9-reference" href="#">nips2001-9-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>9 nips-2001-A Generalization of Principal Components Analysis to the Exponential Family</h1>
<br/><p>Source: <a title="nips-2001-9-pdf" href="http://papers.nips.cc/paper/2078-a-generalization-of-principal-components-analysis-to-the-exponential-family.pdf">pdf</a></p><p>Author: Michael Collins, S. Dasgupta, Robert E. Schapire</p><p>Abstract: Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss functions, and give examples on simulated data.</p><br/>
<h2>reference text</h2><p>[1] Katy S. Azoury and M. K. Warmuth. Relative loss bounds for on-line density estimation with the exponential family of distributions. Machine Learning, 43:211–246, 2001.</p>
<p>[2] I. Csisz´ and G. Tusn´ ar ady. Information geometry and alternating minimization procedures. Statistics and Decisions, Supplement Issue, 1:205–237, 1984.</p>
<p>[3] J¨ rgen Forster and Manfred Warmuth. Relative expected instantaneous loss bounds. Journal of u Computer and System Sciences, to appear.</p>
<p>[4] Thomas Hofmann. Probabilistic latent semantic indexing. In Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999.</p>
<p>[5] I. T. Jolliffe. Principal Component Analysis. Springer-Verlag, 1986.</p>
<p>[6] D. D. Lee and H. S. Seung. Learning the parts of objects with nonnegative matrix factorization. Nature, 401:788, 1999.</p>
<p>[7] Daniel D. Lee and H. Sebastian Seung. Algorithms for non-negative matrix factorization. In Advances in Neural Information Processing Systems 13, 2001.</p>
<p>[8] P. McCullagh and J. A. Nelder. Generalized Linear Models. CRC Press, 2nd edition, 1990.</p>
<p>[9] M. E. Tipping and C. M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society, Series B, 61(3):611–622, 1999.</p>
<p>[10] Michael E. Tipping. Probabilistic visualisation of high-dimensional binary data. In Advances in Neural Information Processing Systems 11, pages 592–598, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
