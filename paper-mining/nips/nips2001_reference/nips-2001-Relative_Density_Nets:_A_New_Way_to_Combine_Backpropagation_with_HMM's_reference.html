<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-162" href="../nips2001/nips-2001-Relative_Density_Nets%3A_A_New_Way_to_Combine_Backpropagation_with_HMM%27s.html">nips2001-162</a> <a title="nips-2001-162-reference" href="#">nips2001-162-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>162 nips-2001-Relative Density Nets: A New Way to Combine Backpropagation with HMM's</h1>
<br/><p>Source: <a title="nips-2001-162-pdf" href="http://papers.nips.cc/paper/2137-relative-density-nets-a-new-way-to-combine-backpropagation-with-hmms.pdf">pdf</a></p><p>Author: Andrew D. Brown, Geoffrey E. Hinton</p><p>Abstract: Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's. 1</p><br/>
<h2>reference text</h2><p>[1] L. E. Baum, T. Petrie, G. Soules, and N. Weiss, </p>
<p>[2] 1. R. Bahl, P. F. Brown, P. V. de Souza, and R. 1. Mercer, </p>
<p>[3] J. Bridle, </p>
<p>[4] M. I. Jordan, </p>
<p>[5] A. D. Brown and G. E. Hinton, </p>
<p>[6] C. E. Rasmussen, Evaluation of Gaussian Processes and other Methods for NonLinear Regression. PhD thesis, University of Toronto, 1996. Matlab conjugate gradient code available from http ://www .gatsby.ucl.ac.uk/~edward/code/.  then this might have made a difference.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
