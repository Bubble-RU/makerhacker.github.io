<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2001" href="../home/nips2001_home.html">nips2001</a> <a title="nips-2001-111" href="../nips2001/nips-2001-Learning_Lateral_Interactions_for_Feature_Binding_and_Sensory_Segmentation.html">nips2001-111</a> <a title="nips-2001-111-reference" href="#">nips2001-111-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>111 nips-2001-Learning Lateral Interactions for Feature Binding and Sensory Segmentation</h1>
<br/><p>Source: <a title="nips-2001-111-pdf" href="http://papers.nips.cc/paper/2022-learning-lateral-interactions-for-feature-binding-and-sensory-segmentation.pdf">pdf</a></p><p>Author: Heiko Wersing</p><p>Abstract: We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efﬁcient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of ﬂuorescence microscope cell images.</p><br/>
<h2>reference text</h2><p>[1] R. Hahnloser, R. Sarpeshkar, M. A. Mahowald, R. J. Douglas, and H. S. Seung. Digital selection and analogue ampliﬁcation coexist in a cortex-inspired silicon circuit. Nature, 405:947–951, 2000.</p>
<p>[2] T. Hofmann, J. Puzicha, and J. Buhmann. Unsupervised texture segmentation in a deterministic annealing framework. IEEE Trans. Pattern Analysis and Machine Intelligence, 20(8):803–818, 1998.</p>
<p>[3] Z. Li. A neural model of contour integration in the primary visual cortex. Neural Computation, 10:903–940, 1998.</p>
<p>[4] M. Mozer, R. S. Zemel, M. Behrmann, and C. K. I. Williams. Learning to segment images using dynamic feature binding. Neural Computation, 4(5):650–665, 1992.</p>
<p>[5] T. W. Nattkemper, H. Ritter, and W. Schubert. A neural classiﬁcator enabling high-throughput topological analysis of lymphocytes in tissue sections. IEEE Trans. Inf. Techn. in Biomed., 5(2):138–149, 2001.</p>
<p>[6] J. Park, H. Cho, and D. Park. On the design of BSB associative memories using semideﬁnite programming. Neural Computation, 11:1985–1994, 1999.</p>
<p>[7] M. Pelillo and M Reﬁce. Learning compatibility coefﬁcients for relaxation labeling processes. IEEE Trans. Pattern Analysis and Machine Intelligence, 16(9):933–945, 1994.</p>
<p>[8] Renzo Perfetti. A synthesis procedure for Brain-State-in-a-Box neural networks. IEEE Transactions on Neural Networks, 6(5):1071–1080, September 1995.</p>
<p>[9] H. Ritter. A spatial approach to feature linking. In Proc. International Neural Network Conference Paris Vol.2, pages 898–901, 1990.</p>
<p>[10] V. Vapnik. The nature of statistical learning theory. Springer, New York, 1995.</p>
<p>[11] C. von der Malsburg. The what and why of binding: The modeler’s perspective. Neuron, 24:95–104, 1999.</p>
<p>[12] D. Wang and D. Terman. Image segmentation based on oscillatory correlation. Neural Computation, 9(4):805–836, 1997.</p>
<p>[13] H. Wersing, W.-J. Beyn, and H. Ritter. Dynamical stability conditions for recurrent neural networks with unsaturating piecewise linear transfer functions. Neural Computation, 13(8):1811– 1825, 2001.</p>
<p>[14] H. Wersing, J. J. Steil, and H. Ritter. A competitive layer model for feature binding and sensory segmentation. Neural Computation, 13(2):357–387, 2001.</p>
<p>[15] Heiko Wersing. Spatial Feature Binding and Learning in Competitive Neural Layer Architectures. PhD thesis, University of Bielefeld, 2000. Published by Cuvillier, Goettingen.</p>
<p>[16] X. Xie, R. Hahnloser, and H.S. Seung. Learning winner-take-all competition between groups of neurons in lateral inhibition networks. In Advances in Neural Information Processing Systems, volume 13. The MIT Press, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
