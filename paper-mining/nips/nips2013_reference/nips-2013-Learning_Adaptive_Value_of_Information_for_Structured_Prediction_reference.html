<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-150" href="../nips2013/nips-2013-Learning_Adaptive_Value_of_Information_for_Structured_Prediction.html">nips2013-150</a> <a title="nips-2013-150-reference" href="#">nips2013-150-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>150 nips-2013-Learning Adaptive Value of Information for Structured Prediction</h1>
<br/><p>Source: <a title="nips-2013-150-pdf" href="http://papers.nips.cc/paper/5142-learning-adaptive-value-of-information-for-structured-prediction.pdf">pdf</a></p><p>Author: David J. Weiss, Ben Taskar</p><p>Abstract: Discriminative methods for learning structured models have enabled wide-spread use of very rich feature representations. However, the computational cost of feature extraction is prohibitive for large-scale or time-sensitive applications, often dominating the cost of inference in the models. Signiﬁcant efforts have been devoted to sparsity-based model selection to decrease this cost. Such feature selection methods control computation statically and miss the opportunity to ﬁnetune feature extraction to each input at run-time. We address the key challenge of learning to control ﬁne-grained feature extraction adaptively, exploiting nonhomogeneity of the data. We propose an architecture that uses a rich feedback loop between extraction and prediction. The run-time control policy is learned using efﬁcient value-function approximation, which adaptively determines the value of information of features at the level of individual variables for each input. We demonstrate signiﬁcant speedups over state-of-the-art methods on two challenging datasets. For articulated pose estimation in video, we achieve a more accurate state-of-the-art model that is also faster, with similar results on an OCR task. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Altun, I. Tsochantaridis, and T. Hofmann. Hidden Markov support vector machines. In Proc. ICML, 2003.</p>
<p>[2] M. Chen, Z. Xu, K.Q. Weinberg, O. Chapelle, and D. Kedem. Classiﬁer cascade for minimizing feature evaluation cost. In AISATATS, 2012.</p>
<p>[3] M. Collins. Discriminative training methods for hidden markov models: theory and experiments with perceptron algorithms. In Proc. EMNLP, 2002.</p>
<p>[4] T. Gao and D. Koller. Active classiﬁcation based on value of classiﬁer. In NIPS, 2011.</p>
<p>[5] A. Grubb and D. Bagnell. Speedboost: Anytime prediction with uniform near-optimality. In AISTATS, 2012.</p>
<p>[6] H. He, H. Daum´ III, and J. Eisner. Imitation learning by coaching. In NIPS, 2012. e</p>
<p>[7] H. He, H. Daum´ III, and J. Eisner. Dynamic feature selection for dependency parsing. In EMNLP, 2013. e</p>
<p>[8] R. A Howard. Information value theory. Systems Science and Cybernetics, IEEE Transactions on, 2(1):22–26, 1966.</p>
<p>[9] Andreas Krause and Carlos Guestrin. Optimal value of information in graphical models. Journal of Artiﬁcial Intelligence Research (JAIR), 35:557–591, 2009.</p>
<p>[10] J.D. Lafferty, A. McCallum, and F.C.N. Pereira. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. In Proc. ICML, 2001.</p>
<p>[11] M. Lagoudakis and R. Parr. Least-squares policy iteration. JMLR, 2003.</p>
<p>[12] Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of Mathematical Statistics, pages 986–1005, 1956.</p>
<p>[13] C. Liu. Beyond Pixels: Exploring New Representations and Applications for Motion Analysis. PhD thesis, MIT, 2009.</p>
<p>[14] V.C. Raykar, B. Krishnapuram, and S. Yu. Designing efﬁcient cascaded classiﬁers: tradeoff between accuracy and cost. In SIGKDD, 2010.</p>
<p>[15] B. Sapp and B. Taskar. MODEC: Multimodal decomposable models for human pose estimation. In CVPR, 2013.</p>
<p>[16] B. Sapp, D. Weiss, and B. Taskar. Parsing human motion with stretchable models. In CVPR, 2011.</p>
<p>[17] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In ICML, 2007.</p>
<p>[18] B. Taskar, V. Chatalbashev, D. Koller, and C. Guestrin. Learning structured prediction models: A large margin approach. In ICML, 2005.</p>
<p>[19] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. In NIPS, 2003.</p>
<p>[20] K. Trapeznikov and V. Saligrama. Supervised sequential classiﬁcation under budget constraints. In AISTATS, 2013.</p>
<p>[21] C. Watkins and P. Dayan. Q-learning. Machine learning, 1992.</p>
<p>[22] D. Weiss, B. Sapp, and B. Taskar. Dynamic structured model selection. In ICCV, 2013.</p>
<p>[23] D. Weiss and B. Taskar. Structured prediction cascades. In AISTATS, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
