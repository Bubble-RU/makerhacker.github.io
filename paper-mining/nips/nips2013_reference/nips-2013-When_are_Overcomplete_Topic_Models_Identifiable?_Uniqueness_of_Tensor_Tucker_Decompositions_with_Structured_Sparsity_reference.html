<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-353" href="../nips2013/nips-2013-When_are_Overcomplete_Topic_Models_Identifiable%3F_Uniqueness_of_Tensor_Tucker_Decompositions_with_Structured_Sparsity.html">nips2013-353</a> <a title="nips-2013-353-reference" href="#">nips2013-353-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>353 nips-2013-When are Overcomplete Topic Models Identifiable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity</h1>
<br/><p>Source: <a title="nips-2013-353-pdf" href="http://papers.nips.cc/paper/4982-when-are-overcomplete-topic-models-identifiable-uniqueness-of-tensor-tucker-decompositions-with-structured-sparsity.pdf">pdf</a></p><p>Author: Anima Anandkumar, Daniel Hsu, Majid Janzamin, Sham M. Kakade</p><p>Abstract: Overcomplete latent representations have been very popular for unsupervised feature learning in recent years. In this paper, we specify which overcomplete models can be identiﬁed given observable moments of a certain order. We consider probabilistic admixture or topic models in the overcomplete regime, where the number of latent topics can greatly exceed the size of the observed word vocabulary. While general overcomplete topic models are not identiﬁable, we establish generic identiﬁability under a constraint, referred to as topic persistence. Our sufﬁcient conditions for identiﬁability involve a novel set of “higher order” expansion conditions on the topic-word matrix or the population structure of the model. This set of higher-order expansion conditions allow for overcomplete models, and require the existence of a perfect matching from latent topics to higher order observed words. We establish that random structured topic models are identiﬁable w.h.p. in the overcomplete regime. Our identiﬁability results allow for general (non-degenerate) distributions for modeling the topic proportions, and thus, we can handle arbitrarily correlated topics in our framework. Our identiﬁability results imply uniqueness of a class of tensor decompositions with structured sparsity which is contained in the class of Tucker decompositions, but is more general than the Candecomp/Parafac (CP) decomposition. Keywords: Overcomplete representation, admixture models, generic identiﬁability, tensor decomposition.</p><br/>
<h2>reference text</h2><p>[1] Andr´ Uschmajew. Local convergence of the alternating least squares algorithm for canonical e tensor approximation. SIAM Journal on Matrix Analysis and Applications, 33(2):639–652, 2012.</p>
<p>[2] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent Dirichlet Allocation. Journal of Machine Learning Research, 3:993–1022, 2003.</p>
<p>[3] J. K. Pritchard, M. Stephens, and P. Donnelly. Inference of population structure using multilocus genotype data. Genetics, 155:945–959, 2000.</p>
<p>[4] J.B. Kruskal. More factors than subjects, tests and treatments: an indeterminacy theorem for canonical decomposition and individual differences scaling. Psychometrika, 41(3):281–293, 1976.</p>
<p>[5] Tamara Kolda and Brett Bader. Tensor decompositions and applications. SIREV, 51(3):455– 500, 2009.</p>
<p>[6] G.H. Golub and C.F. Van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, Maryland, 2012.</p>
<p>[7] XuanLong Nguyen. Posterior contraction of the population polytope in ﬁnite admixture models. arXiv preprint arXiv:1206.0068, 2012.</p>
<p>[8] T. Austin et al. On exchangeable random variables and the statistics of large graphs and hypergraphs. Probab. Surv, 5:80–145, 2008.</p>
<p>[9] A. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky. Tensor Methods for Learning Latent Variable Models. Under Review. J. of Machine Learning. Available at arXiv:1210.7559, Oct. 2012.</p>
<p>[10] Elizabeth S. Allman, John A. Rhodes, and Amelia Taylor. A semialgebraic description of the general markov model on phylogenetic trees. Arxiv preprint arXiv:1212.1200, Dec. 2012.</p>
<p>[11] J.B. Kruskal. Three-way arrays: Rank and uniqueness of trilinear decompositions, with application to arithmetic complexity and statistics. Linear algebra and its applications, 18(2):95– 138, 1977.</p>
<p>[12] A. Anandkumar, D. Hsu, M. Janzamin, and S. Kakade. When are Overcomplete Topic Models Identiﬁable? Uniqueness of Tensor Tucker Decompositions with Structured Sparsity. Preprint available on arXiv:1308.2853, Aug. 2013.</p>
<p>[13] A. Anandkumar, D. Hsu, A. Javanmard, and S. M. Kakade. Learning Linear Bayesian Networks with Latent Variables. ArXiv e-prints, September 2012.</p>
<p>[14] Daniel A. Spielman, Huan Wang, and John Wright. Exact recovery of sparsely-used dictionaries. ArxXiv preprint, abs/1206.5882, 2012.</p>
<p>[15] L. De Lathauwer, J. Castaing, and J.-F Cardoso. Fourth-order cumulant-based blind identiﬁcation of underdetermined mixtures. IEEE Tran. on Signal Processing, 55:2965–2973, June 2007.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
