<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-218" href="../nips2013/nips-2013-On_Sampling_from_the_Gibbs_Distribution_with_Random_Maximum_A-Posteriori_Perturbations.html">nips2013-218</a> <a title="nips-2013-218-reference" href="#">nips2013-218-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>218 nips-2013-On Sampling from the Gibbs Distribution with Random Maximum A-Posteriori Perturbations</h1>
<br/><p>Source: <a title="nips-2013-218-pdf" href="http://papers.nips.cc/paper/4962-on-sampling-from-the-gibbs-distribution-with-random-maximum-a-posteriori-perturbations.pdf">pdf</a></p><p>Author: Tamir Hazan, Subhransu Maji, Tommi Jaakkola</p><p>Abstract: In this paper we describe how MAP inference can be used to sample efﬁciently from Gibbs distributions. Speciﬁcally, we provide means for drawing either approximate or unbiased samples from Gibbs’ distributions by introducing low dimensional perturbations and solving the corresponding MAP assignments. Our approach also leads to new ways to derive lower bounds on partition functions. We demonstrate empirically that our method excels in the typical “high signal high coupling” regime. The setting results in ragged energy landscapes that are challenging for alternative approaches to sampling and/or lower bounds. 1</p><br/>
<h2>reference text</h2><p>[1] Alexandre Bouchard-Cˆ t´ and Michael I Jordan. Optimization of structured mean ﬁeld objecoe tives. In AUAI, pages 67–74, 2009.</p>
<p>[2] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. PAMI, 2001.</p>
<p>[3] L.A. Goldberg and M. Jerrum. The complexity of ferromagnetic ising with local ﬁelds. Combinatorics Probability and Computing, 16(1):43, 2007.</p>
<p>[4] E.J. Gumbel and J. Lieblein. Statistical theory of extreme values and some practical applications: a series of lectures, volume 33. US Govt. Print. Ofﬁce, 1954.</p>
<p>[5] T. Hazan and T. Jaakkola. On the partition function and random maximum a-posteriori perturbations. In Proceedings of the 29th International Conference on Machine Learning, 2012.</p>
<p>[6] T. Hazan, S. Maji, Keshet J., and T. Jaakkola. Learning efﬁcient random maximum a-posteriori predictors with non-decomposable loss functions. Advances in Neural Information Processing Systems, 2013.</p>
<p>[7] M. Jerrum and A. Sinclair. Polynomial-time approximation algorithms for the ising model. SIAM Journal on computing, 22(5):1087–1116, 1993.</p>
<p>[8] M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, and L.K. Saul. An introduction to variational methods for graphical models. Machine learning, 37(2):183–233, 1999.</p>
<p>[9] J. Keshet, D. McAllester, and T. Hazan. Pac-bayesian approach for minimization of phoneme error rate. In ICASSP, 2011.</p>
<p>[10] Pushmeet Kohli and Philip HS Torr. Measuring uncertainty in graph cut solutions–efﬁciently computing min-marginal energies using dynamic graph cuts. In ECCV, pages 30–43. 2006.</p>
<p>[11] D. Koller and N. Friedman. Probabilistic graphical models. MIT press, 2009.</p>
<p>[12] S. Kotz and S. Nadarajah. Extreme value distributions: theory and applications. World Scientiﬁc Publishing Company, 2000.</p>
<p>[13] A. Kulesza and B. Taskar. Structured determinantal point processes. In Proc. Neural Information Processing Systems, 2010.</p>
<p>[14] Qiang Liu and Alexander T Ihler. Negative tree reweighted belief propagation. arXiv preprint arXiv:1203.3494, 2012.</p>
<p>[15] Francesco Orabona, Tamir Hazan, Anand D Sarwate, and Tommi. Jaakkola. On measure concentration of random maximum a-posteriori perturbations. arXiv:1310.4227, 2013.</p>
<p>[16] G. Papandreou and A. Yuille. Gaussian sampling by local perturbations. In Proc. Int. Conf. on Neural Information Processing Systems (NIPS), pages 1858–1866, December 2010.</p>
<p>[17] G. Papandreou and A. Yuille. Perturb-and-map random ﬁelds: Using discrete optimization to learn and sample from energy models. In ICCV, Barcelona, Spain, November 2011.</p>
<p>[18] Nicholas Ruozzi. The bethe partition function of log-supermodular graphical models. arXiv preprint arXiv:1202.6035, 2012.</p>
<p>[19] D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations for MAP using message passing. In Conf. Uncertainty in Artiﬁcial Intelligence (UAI), 2008.</p>
<p>[20] E.B. Sudderth, M.J. Wainwright, and A.S. Willsky. Loop series and Bethe variational bounds in attractive graphical models. Advances in neural information processing systems, 20, 2008.</p>
<p>[21] D. Tarlow, R.P. Adams, and R.S. Zemel. Randomized optimum models for structured prediction. In Proceedings of the 15th Conference on Artiﬁcial Intelligence and Statistics, 2012.</p>
<p>[22] M. J. Wainwright, T. S. Jaakkola, and A. S. Willsky. A new class of upper bounds on the log partition function. Trans. on Information Theory, 51(7):2313–2335, 2005.</p>
<p>[23] Max Welling. Herding dynamical weights to learn. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1121–1128. ACM, 2009.</p>
<p>[24] T. Werner. High-arity interactions, polyhedral relaxations, and cutting plane algorithm for soft constraint optimisation (map-mrf). In CVPR, pages 1–8, 2008.</p>
<p>[25] J. Zhang, H. Liang, and F. Bai. Approximating partition functions of the two-state spin system. Information Processing Letters, 111(14):702–710, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
