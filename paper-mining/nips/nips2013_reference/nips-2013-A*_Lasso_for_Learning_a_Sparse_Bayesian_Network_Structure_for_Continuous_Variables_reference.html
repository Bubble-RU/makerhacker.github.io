<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-3" href="../nips2013/nips-2013-A%2A_Lasso_for_Learning_a_Sparse_Bayesian_Network_Structure_for_Continuous_Variables.html">nips2013-3</a> <a title="nips-2013-3-reference" href="#">nips2013-3-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>3 nips-2013-A* Lasso for Learning a Sparse Bayesian Network Structure for Continuous Variables</h1>
<br/><p>Source: <a title="nips-2013-3-pdf" href="http://papers.nips.cc/paper/5174-a-lasso-for-learning-a-sparse-bayesian-network-structure-for-continuous-variables.pdf">pdf</a></p><p>Author: Jing Xiang, Seyoung Kim</p><p>Abstract: We address the problem of learning a sparse Bayesian network structure for continuous variables in a high-dimensional space. The constraint that the estimated Bayesian network structure must be a directed acyclic graph (DAG) makes the problem challenging because of the huge search space of network structures. Most previous methods were based on a two-stage approach that prunes the search space in the ﬁrst stage and then searches for a network structure satisfying the DAG constraint in the second stage. Although this approach is effective in a lowdimensional setting, it is difﬁcult to ensure that the correct network structure is not pruned in the ﬁrst stage in a high-dimensional setting. In this paper, we propose a single-stage method, called A* lasso, that recovers the optimal sparse Bayesian network structure by solving a single optimization problem with A* search algorithm that uses lasso in its scoring system. Our approach substantially improves the computational efﬁciency of the well-known exact methods based on dynamic programming. We also present a heuristic scheme that further improves the efﬁciency of A* lasso without signiﬁcantly compromising the quality of solutions. We demonstrate our approach on data simulated from benchmark Bayesian networks and real data. 1</p><br/>
<h2>reference text</h2><p>[1] David Maxwell Chickering. Learning Bayesian networks is NP-complete. In Learning from data, pages 121–130. Springer, 1996.</p>
<p>[2] Nir Friedman, Iftach Nachman, and Dana Pe´ r. Learning Bayesian network structure from e massive datasets: the “Sparse Candidate” algorithm. In Proceedings of the Fifteenth conference on Uncertainty in Artiﬁcial Intelligence, pages 206–215. Morgan Kaufmann Publishers Inc., 1999.</p>
<p>[3] Wenjiang J Fu. Penalized regressions: the bridge versus the lasso. Journal of Computational and Graphical Statistics, 7(3):397–416, 1998.</p>
<p>[4] David Heckerman, Dan Geiger, and David M Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine learning, 20(3):197–243, 1995.</p>
<p>[5] Shuai Huang, Jing Li, Jieping Ye, Adam Fleisher, Kewei Chen, Teresa Wu, and Eric Reiman. A sparse structure learning algorithm for Gaussian Bayesian network identiﬁcation from high-dimensional data. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(6):1328–1342, 2013.</p>
<p>[6] Tommi Jaakkola, David Sontag, Amir Globerson, and Marina Meila. Learning Bayesian network structure using LP relaxations. In Proceedings of the Thirteenth International Conference on Artiﬁcial intelligence and Statistics (AISTATS), 2010.</p>
<p>[7] Mikko Koivisto and Kismat Sood. Exact Bayesian structure discovery in Bayesian networks. Journal of Machine Learning Research, 5:549–573, 2004.</p>
<p>[8] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.</p>
<p>[9] Wai Lam and Fahiem Bacchus. Learning Bayesian belief networks: An approach based on the MDL principle. Computational intelligence, 10(3):269–293, 1994.</p>
<p>[10] Maxim Likhachev, Geoff Gordon, and Sebastian Thrun. ARA*: Anytime A* with provable bounds on sub-optimality. Advances in Neural Information Processing Systems (NIPS), 16, 2003.</p>
<p>[11] Jean-Philippe Pellet and Andr´ Elisseeff. Using Markov blankets for causal structure learning. e The Journal of Machine Learning Research, 9:1295–1342, 2008.</p>
<p>[12] Stuart Jonathan Russell, Peter Norvig, John F Canny, Jitendra M Malik, and Douglas D Edwards. Artiﬁcial intelligence: a modern approach, volume 74. Prentice hall Englewood Cliffs, 1995.</p>
<p>[13] Mark Schmidt, Alexandru Niculescu-Mizil, and Kevin Murphy. Learning graphical model structure using L1-regularization paths. In Proceedings of the National Conference on Artiﬁcial Intelligence, volume 22, page 1278, 2007.</p>
<p>[14] Gideon Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6(2):461–464, 1978.</p>
<p>[15] Ajit Singh and Andrew Moore. Finding optimal Bayesian networks by dynamic programming. Technical Report 05-106, School of Computer Science, Carnegie Mellon University, 2005.</p>
<p>[16] Marc Teyssier and Daphne Koller. Ordering-based search: A simple and effective algorithm for learning Bayesian networks. In Proceedings of the Twentieth conference on Uncertainty in Artiﬁcial Intelligence, pages 584–590, 2005.</p>
<p>[17] Ioannis Tsamardinos, Laura E Brown, and Constantin F Aliferis. The max-min hill-climbing Bayesian network structure learning algorithm. Machine Learning, 65(1):31–78, 2006.</p>
<p>[18] Ioannis Tsamardinos, Alexander Statnikov, Laura E Brown, and Constantin F Aliferis. Generating realistic large Bayesian networks by tiling. In the Nineteenth International FLAIRS conference, pages 592–597, 2006.</p>
<p>[19] Changhe Yuan, Brandon Malone, and Xiaojian Wu. Learning optimal Bayesian networks using A* search. In Proceedings of the Twenty-Second international joint conference on Artiﬁcial Intelligence, pages 2186–2191. AAAI Press, 2011.  9</p>
<br/>
<br/><br/><br/></body>
</html>
