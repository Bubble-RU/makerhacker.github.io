<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-346" href="../nips2013/nips-2013-Variational_Inference_for_Mahalanobis_Distance_Metrics_in_Gaussian_Process_Regression.html">nips2013-346</a> <a title="nips-2013-346-reference" href="#">nips2013-346-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>346 nips-2013-Variational Inference for Mahalanobis Distance Metrics in Gaussian Process Regression</h1>
<br/><p>Source: <a title="nips-2013-346-pdf" href="http://papers.nips.cc/paper/5088-variational-inference-for-mahalanobis-distance-metrics-in-gaussian-process-regression.pdf">pdf</a></p><p>Author: Michalis Titsias, Miguel Lazaro-Gredilla</p><p>Abstract: We introduce a novel variational method that allows to approximately integrate out kernel hyperparameters, such as length-scales, in Gaussian process regression. This approach consists of a novel variant of the variational framework that has been recently developed for the Gaussian process latent variable model which additionally makes use of a standardised representation of the Gaussian process. We consider this technique for learning Mahalanobis distance metrics in a Gaussian process regression setting and provide experimental evaluations and comparisons with existing methods by considering datasets with high-dimensional inputs. 1</p><br/>
<h2>reference text</h2><p>Bishop, C. M. (1999). Variational principal components. In In Proceedings Ninth International Conference on Artiﬁcial Neural Networks, ICANN?99, pages 509–514. Bishop, C. M. (2006). Pattern Recognition and Machine Learning (Information Science and Statistics). Springer, 1st ed. 2006 edition. MacKay, D. J. (1994). Bayesian non-linear modelling for the energy prediction competition. SHRAE Transactions, 4:448–472. Murray, I. and Adams, R. P. (2010). Slice sampling covariance hyperparameters of latent Gaussian models. In Lafferty, J., Williams, C. K. I., Zemel, R., Shawe-Taylor, J., and Culotta, A., editors, Advances in Neural Information Processing Systems 23, pages 1723–1731. Neal, R. M. (1998). Assessing relevance determination methods using delve. Neural Networksand Machine Learning, pages 97–129. Rasmussen, C. and Williams, C. (2006). Gaussian Processes for Machine Learning. Adaptive Computation and Machine Learning. MIT Press. Snelson, E. and Ghahramani, Z. (2006a). Sparse Gaussian processes using pseudo-inputs. In Advances in Neural Information Processing Systems 18, pages 1259–1266. MIT Press. Snelson, E. and Ghahramani, Z. (2006b). Variable noise and dimensionality reduction for sparse Gaussian processes. In Uncertainty in Artiﬁcial Intelligence. Tipping, M. E. (2001). Sparse bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211–244. Titsias, M. K. (2009). Variational learning of inducing variables in sparse Gaussian processes. In Proc. of the 12th International Workshop on AI Stats. Titsias, M. K. and Lawrence, N. D. (2010). Bayesian Gaussian process latent variable model. Journal of Machine Learning Research - Proceedings Track, 9:844–851. Vivarelli, F. and Williams, C. K. I. (1998). Discovering hidden features with Gaussian processes regression. In Advances in Neural Information Processing Systems, pages 613–619. Weinberger, K. Q. and Saul, L. K. (2009). Distance metric learning for large margin nearest neighbor classiﬁcation. J. Mach. Learn. Res., 10:207–244. Xing, E., Ng, A., Jordan, M., and Russell, S. (2003). Distance metric learning, with application to clustering with side-information.  9</p>
<br/>
<br/><br/><br/></body>
</html>
