<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>279 nips-2013-Robust Bloom Filters for Large MultiLabel Classification Tasks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-279" href="../nips2013/nips-2013-Robust_Bloom_Filters_for_Large_MultiLabel_Classification_Tasks.html">nips2013-279</a> <a title="nips-2013-279-reference" href="#">nips2013-279-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>279 nips-2013-Robust Bloom Filters for Large MultiLabel Classification Tasks</h1>
<br/><p>Source: <a title="nips-2013-279-pdf" href="http://papers.nips.cc/paper/5083-robust-bloom-filters-for-large-multilabel-classification-tasks.pdf">pdf</a></p><p>Author: Moustapha M. Cisse, Nicolas Usunier, Thierry Artières, Patrick Gallinari</p><p>Abstract: This paper presents an approach to multilabel classiﬁcation (MLC) with a large number of labels. Our approach is a reduction to binary classiﬁcation in which label sets are represented by low dimensional binary vectors. This representation follows the principle of Bloom ﬁlters, a space-efﬁcient data structure originally designed for approximate membership testing. We show that a naive application of Bloom ﬁlters in MLC is not robust to individual binary classiﬁers’ errors. We then present an approach that exploits a speciﬁc feature of real-world datasets when the number of labels is large: many labels (almost) never appear together. Our approach is provably robust, has sublinear training and inference complexity with respect to the number of labels, and compares favorably to state-of-the-art algorithms on two large scale multilabel datasets. 1</p><br/>
<h2>reference text</h2><p>[1] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre. Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment., 10, 2008.</p>
<p>[2] B. H. Bloom. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422–426, 1970.</p>
<p>[3] L. Carter, R. Floyd, J. Gill, G. Markowsky, and M. Wegman. Exact and approximate membership testers. In Proceedings of the tenth annual ACM symposium on Theory of computing, STOC ’78, pages 59–65, New York, NY, USA, 1978. ACM.</p>
<p>[4] Y.-N. Chen and H.-T. Lin. Feature-aware label space dimension reduction for multi-label classiﬁcation. In NIPS, pages 1538–1546, 2012.</p>
<p>[5] W. Cheng and E. H¨ llermeier. Combining instance-based learning and logistic regression for u multilabel classiﬁcation. Machine Learning, 76(2-3):211–225, 2009.</p>
<p>[6] K. Christensen, A. Roginsky, and M. Jimeno. A new analysis of the false positive rate of a bloom ﬁlter. Inf. Process. Lett., 110(21):944–949, Oct. 2010.</p>
<p>[7] O. Dekel and O. Shamir. Multiclass-multilabel classiﬁcation with more classes than examples. volume 9, pages 137–144, 2010.</p>
<p>[8] K. Dembczynski, W. Cheng, and E. H¨ llermeier. Bayes optimal multilabel classiﬁcation via u probabilistic classiﬁer chains. In ICML, pages 279–286, 2010.</p>
<p>[9] K. Dembczynski, W. Waegeman, W. Cheng, and E. H¨ llermeier. On label dependence and loss u minimization in multi-label classiﬁcation. Machine Learning, 88(1-2):5–45, 2012.</p>
<p>[10] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. Liblinear: A library for large linear classiﬁcation. J. Mach. Learn. Res., 9:1871–1874, June 2008.</p>
<p>[11] B. Hariharan, S. V. N. Vishwanathan, and M. Varma. Large Scale Max-Margin Multi-Label Classiﬁcation with Prior Knowledge about Densely Correlated Labels. In Proceedings of International Conference on Machine Learning, 2010.</p>
<p>[12] D. Hsu, S. Kakade, J. Langford, and T. Zhang. Multi-label prediction via compressed sensing. In NIPS, pages 772–780, 2009.</p>
<p>[13] RCV1. RCV1 Dataset, http://www.daviddlewis.com/resources/testcollections/rcv1/.</p>
<p>[14] J. Read, B. Pfah ringer, G. Holmes, and E. Frank. Classiﬁer chains for multi-label classiﬁcation. In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases: Part II, ECML PKDD ’09, pages 254–269, Berlin, Heidelberg, 2009. Springer-Verlag.</p>
<p>[15] F. Tai and H.-T. Lin. Multilabel classiﬁcation with principal label space transformation. Neural Computation, 24(9):2508–2542, 2012.</p>
<p>[16] G. Tsoumakas, I. Katakis, and I. Vlahavas. A Review of Multi-Label Classiﬁcation Methods. In Proceedings of the 2nd ADBIS Workshop on Data Mining and Knowledge Discovery (ADMKD 2006), pages 99–109, 2006.</p>
<p>[17] Wikipedia. Wikipedia Dataset, http://lshtc.iit.demokritos.gr/.  9</p>
<br/>
<br/><br/><br/></body>
</html>
