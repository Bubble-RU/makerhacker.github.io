<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-192" href="../nips2013/nips-2013-Minimax_Theory_for_High-dimensional_Gaussian_Mixtures_with_Sparse_Mean_Separation.html">nips2013-192</a> <a title="nips-2013-192-reference" href="#">nips2013-192-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>192 nips-2013-Minimax Theory for High-dimensional Gaussian Mixtures with Sparse Mean Separation</h1>
<br/><p>Source: <a title="nips-2013-192-pdf" href="http://papers.nips.cc/paper/4983-minimax-theory-for-high-dimensional-gaussian-mixtures-with-sparse-mean-separation.pdf">pdf</a></p><p>Author: Martin Azizyan, Aarti Singh, Larry Wasserman</p><p>Abstract: While several papers have investigated computationally and statistically efﬁcient methods for learning Gaussian mixtures, precise minimax bounds for their statistical performance as well as fundamental limits in high-dimensional settings are not well-understood. In this paper, we provide precise information theoretic bounds on the clustering accuracy and sample complexity of learning a mixture of two isotropic Gaussians in high dimensions under small mean separation. If there is a sparse subset of relevant dimensions that determine the mean separation, then the sample complexity only depends on the number of relevant dimensions and mean separation, and can be achieved by a simple computationally efﬁcient procedure. Our results provide the ﬁrst step of a theoretical basis for recent methods that combine feature selection and clustering. 1</p><br/>
<h2>reference text</h2><p>Dimitris Achlioptas and Frank McSherry. On spectral learning of mixtures of distributions. In Learning Theory, pages 458–469. Springer, 2005. Sanjeev Arora and Ravi Kannan. Learning mixtures of arbitrary gaussians. In Proceedings of the thirty-third annual ACM symposium on Theory of computing, pages 247–257. ACM, 2001. Mikhail Belkin and Kaushik Sinha. Polynomial learning of distribution families. In Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 103–112. IEEE, 2010. S Charles Brubaker and Santosh S Vempala. Isotropic pca and afﬁne-invariant clustering. In Building Bridges, pages 241–281. Springer, 2008. Kamalika Chaudhuri and Satish Rao. Learning mixtures of product distributions using correlations and independence. In COLT, pages 9–20, 2008. Kamalika Chaudhuri, Sanjoy Dasgupta, and Andrea Vattani. Learning mixtures of gaussians using the k-means algorithm. arXiv preprint arXiv:0912.0086, 2009. Sanjoy Dasgupta. Learning mixtures of gaussians. In Foundations of Computer Science, 1999. 40th Annual Symposium on, pages 634–644. IEEE, 1999. Jian Guo, Elizaveta Levina, George Michailidis, and Ji Zhu. Pairwise variable selection for highdimensional model-based clustering. Biometrics, 66(3):793–804, 2010. Daniel Hsu and Sham M Kakade. Learning mixtures of spherical gaussians: moment methods and spectral decompositions. In Proceedings of the 4th conference on Innovations in Theoretical Computer Science, pages 11–20. ACM, 2013. Adam Tauman Kalai, Ankur Moitra, and Gregory Valiant. Disentangling gaussians. Communications of the ACM, 55(2):113–120, 2012. Ravindran Kannan, Hadi Salmasian, and Santosh Vempala. The spectral method for general mixture models. In Learning Theory, pages 444–457. Springer, 2005. Pascal Massart. Concentration inequalities and model selection. 2007. Wei Pan and Xiaotong Shen. Penalized model-based clustering with application to variable selection. The Journal of Machine Learning Research, 8:1145–1164, 2007. Adrian E Raftery and Nema Dean. Variable selection for model-based clustering. Journal of the American Statistical Association, 101(473):168–178, 2006. Leonard J. Schulman and Sanjoy Dasgupta. A two-round variant of em for gaussian mixtures. In Proc. 16th UAI (Conference on Uncertainty in Artiﬁcial Intelligence), pages 152–159, 2000. Wei Sun, Junhui Wang, and Yixin Fang. Regularized k-means clustering of high-dimensional data and its asymptotic consistency. Electronic Journal of Statistics, 6:148–167, 2012. Alexandre B. Tsybakov. Introduction to Nonparametric Estimation. Springer Series in Statistics. Springer, 2009. Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of Computer and System Sciences, 68(4):841–860, 2004. Vincent Q Vu and Jing Lei. Minimax sparse principal subspace estimation in high dimensions. arXiv preprint arXiv:1211.0373, 2012. Daniela M Witten and Robert Tibshirani. A framework for feature selection in clustering. Journal of the American Statistical Association, 105(490), 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
