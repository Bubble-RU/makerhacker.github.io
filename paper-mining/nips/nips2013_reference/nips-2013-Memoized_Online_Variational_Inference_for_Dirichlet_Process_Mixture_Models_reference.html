<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-187" href="../nips2013/nips-2013-Memoized_Online_Variational_Inference_for_Dirichlet_Process_Mixture_Models.html">nips2013-187</a> <a title="nips-2013-187-reference" href="#">nips2013-187-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>187 nips-2013-Memoized Online Variational Inference for Dirichlet Process Mixture Models</h1>
<br/><p>Source: <a title="nips-2013-187-pdf" href="http://papers.nips.cc/paper/4969-memoized-online-variational-inference-for-dirichlet-process-mixture-models.pdf">pdf</a></p><p>Author: Michael Hughes, Erik Sudderth</p><p>Abstract: Variational inference algorithms provide the most effective framework for largescale training of Bayesian nonparametric models. Stochastic online approaches are promising, but are sensitive to the chosen learning rate and often converge to poor local optima. We present a new algorithm, memoized online variational inference, which scales to very large (yet ﬁnite) datasets while avoiding the complexities of stochastic gradient. Our algorithm maintains ﬁnite-dimensional sufﬁcient statistics from batches of the full dataset, requiring some additional memory but still scaling to millions of examples. Exploiting nested families of variational bounds for inﬁnite nonparametric models, we develop principled birth and merge moves allowing non-local optimization. Births adaptively add components to the model to escape local optima, while merges remove redundancy and improve speed. Using Dirichlet process mixture models for image clustering and denoising, we demonstrate major improvements in robustness and accuracy.</p><br/>
<h2>reference text</h2><p>[1] M. Hoffman, D. Blei, C. Wang, and J. Paisley. Stochastic variational inference. JMLR, 14:1303–1347, 2013.</p>
<p>[2] R. Ranganath, C. Wang., D. Blei, and E. Xing. An adaptive learning rate for stochastic variational inference. In ICML, 2013.</p>
<p>[3] P. Gopalan, D. M. Mimno, S. Gerrish, M. J. Freedman, and D. M. Blei. Scalable inference of overlapping communities. In NIPS, 2012.</p>
<p>[4] M. Bryant and E. Sudderth. Truly nonparametric online variational inference for hierarchical Dirichlet processes. In NIPS, 2012.</p>
<p>[5] S. Jain and R.M. Neal. A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model. Journal of Computational and Graphical Statistics, 13(1):158–182, 2004.</p>
<p>[6] D. B. Dahl. Sequentially-allocated merge-split sampler for conjugate and nonconjugate Dirichlet process mixture models. Submitted to Journal of Computational and Graphical Statistics, 2005.</p>
<p>[7] D. M. Blei and M. I. Jordan. Variational inference for Dirichlet process mixture models. Bayesian Analysis, 1(1):121–144, 2006.</p>
<p>[8] Y. W. Teh, K. Kurihara, and M. Welling. Collapsed variational inference for HDP. In NIPS, 2008.</p>
<p>[9] K. Kurihara, M. Welling, and N. Vlassis. Accelerated variational Dirichlet process mixtures. In NIPS, 2006.</p>
<p>[10] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. In Learning in graphical models, 1999.</p>
<p>[11] O. Papaspiliopoulos and G. O. Roberts. Retrospective Markov chain Monte Carlo methods for Dirichlet process hierarchical models. Biometrika, 95(1):169–186, 2008.</p>
<p>[12] N. Goodman, V. Mansinghka, D. M. Roy, K. Bonawitz, and J. Tenenbaum. Church: A language for generative models. In Uncertainty in Artiﬁcial Intelligence, 2008.</p>
<p>[13] N. Le Roux, M. Schmidt, and F. Bach. A stochastic gradient method with an exponential convergence rate for ﬁnite training sets. In NIPS, 2012.</p>
<p>[14] N. Ueda and Z. Ghahramani. Bayesian model search for mixture models based on optimizing variational bounds. Neural Networks, 15(1):1223–1241, 2002.</p>
<p>[15] C. Wang and D. Blei. Truncation-free stochastic variational inference for Bayesian nonparametric models. In NIPS, 2012.</p>
<p>[16] N. Ueda, R. Nakano, Z. Ghahramani, and G. Hinton. SMEM algorithm for mixture models. Neural Computation, 12(9):2109–2128, 2000.</p>
<p>[17] D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In ACM-SIAM Symposium on Discrete Algorithms, pages 1027–1035, 2007.</p>
<p>[18] J. Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, 2010.</p>
<p>[19] D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. In ICCV, 2011.</p>
<p>[20] D. Zoran and Y. Weiss. Natural images, Gaussian mixtures and dead leaves. In NIPS, 2012.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
