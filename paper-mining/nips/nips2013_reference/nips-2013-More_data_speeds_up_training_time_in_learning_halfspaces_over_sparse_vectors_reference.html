<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-199" href="../nips2013/nips-2013-More_data_speeds_up_training_time_in_learning_halfspaces_over_sparse_vectors.html">nips2013-199</a> <a title="nips-2013-199-reference" href="#">nips2013-199-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>199 nips-2013-More data speeds up training time in learning halfspaces over sparse vectors</h1>
<br/><p>Source: <a title="nips-2013-199-pdf" href="http://papers.nips.cc/paper/4905-more-data-speeds-up-training-time-in-learning-halfspaces-over-sparse-vectors.pdf">pdf</a></p><p>Author: Amit Daniely, Nati Linial, Shai Shalev-Shwartz</p><p>Abstract: The increased availability of data in recent years has led several authors to ask whether it is possible to use data as a computational resource. That is, if more data is available, beyond the sample complexity limit, is it possible to use the extra examples to speed up the computation time required to perform the learning task? We give the ﬁrst positive answer to this question for a natural supervised learning problem — we consider agnostic PAC learning of halfspaces over 3-sparse vectors in {−1, 1, 0}n . This class is inefﬁciently learnable using O n/ 2 examples. Our main contribution is a novel, non-cryptographic, methodology for establishing computational-statistical gaps, which allows us to show that, under a widely believed assumption that refuting random 3CNF formulas is hard, it is impossible to efﬁciently learn this class using only O n/ 2 examples. We further show that under stronger hardness assumptions, even O n1.499 / 2 examples do not sufﬁce. On the other hand, we show a new algorithm that learns this class efﬁciently ˜ using Ω n2 / 2 examples. This formally establishes the tradeoff between sample and computational complexity for a natural supervised learning problem. 1</p><br/>
<h2>reference text</h2><p>Benny Applebaum, Boaz Barak, and David Xiao. On basing lower-bounds for learning on worstcase assumptions. In Foundations of Computer Science, 2008. FOCS’08. IEEE 49th Annual IEEE Symposium on, pages 211–220. IEEE, 2008. 8  Quentin Berthet and Philippe Rigollet. Complexity theoretic lower bounds for sparse principal component detection. In COLT, 2013. Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile. On the generalization ability of on-line learning algorithms. IEEE Transactions on Information Theory, 50:2050–2057, 2001. Venkat Chandrasekaran and Michael I. Jordan. Computational and statistical tradeoffs via convex relaxation. Proceedings of the National Academy of Sciences, 2013. S. Decatur, O. Goldreich, and D. Ron. Computational sample complexity. SIAM Journal on Computing, 29, 1998. O. Dubios, R. Monasson, B. Selma, and R. Zecchina (Guest Editors). Phase Transitions in Combinatorial Problems. Theoretical Computer Science, Volume 265, Numbers 1-2, 2001. U. Feige. Relations between average case complexity and approximation complexity. In STOC, pages 534–543, 2002. Uriel Feige and Eran Ofek. Easily refutable subformulas of large random 3cnf formulas. Theory of Computing, 3(1):25–43, 2007. E. Hazan, S. Kale, and S. Shalev-Shwartz. Near-optimal algorithms for online matrix prediction. In COLT, 2012. P. Long. and R. Servedio. Low-weight halfspaces for sparse boolean vectors. In ITCS, 2013. R. Servedio. Computational sample complexity and attribute-efﬁcient learning. J. of Comput. Syst. Sci., 60(1):161–178, 2000. Shai Shalev-Shwartz, Ohad Shamir, and Eran Tromer. Using more data to speed-up training time. In AISTATS, 2012. V.N. Vapnik. The Nature of Statistical Learning Theory. Springer, 1995.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
