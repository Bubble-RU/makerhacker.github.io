<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-194" href="../nips2013/nips-2013-Model_Selection_for_High-Dimensional_Regression_under_the_Generalized_Irrepresentability_Condition.html">nips2013-194</a> <a title="nips-2013-194-reference" href="#">nips2013-194-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>194 nips-2013-Model Selection for High-Dimensional Regression under the Generalized Irrepresentability Condition</h1>
<br/><p>Source: <a title="nips-2013-194-pdf" href="http://papers.nips.cc/paper/4930-model-selection-for-high-dimensional-regression-under-the-generalized-irrepresentability-condition.pdf">pdf</a></p><p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: In the high-dimensional regression model a response variable is linearly related to p covariates, but the sample size n is smaller than p. We assume that only a small subset of covariates is ‘active’ (i.e., the corresponding coefﬁcients are non-zero), and consider the model-selection problem of identifying the active covariates. A popular approach is to estimate the regression coefﬁcients through the Lasso ( 1 -regularized least squares). This is known to correctly identify the active set only if the irrelevant covariates are roughly orthogonal to the relevant ones, as quantiﬁed through the so called ‘irrepresentability’ condition. In this paper we study the ‘Gauss-Lasso’ selector, a simple two-stage method that ﬁrst solves the Lasso, and then performs ordinary least squares restricted to the Lasso active set. We formulate ‘generalized irrepresentability condition’ (GIC), an assumption that is substantially weaker than irrepresentability. We prove that, under GIC, the Gauss-Lasso correctly recovers the active set. 1</p><br/>
<h2>reference text</h2><p>[1] F. R. Bach. Bolasso: model consistent lasso estimation through the bootstrap. In Proceedings of the 25th international conference on Machine learning, pages 33–40. ACM, 2008.</p>
<p>[2] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Amer. J. of Mathematics, 37:1705–1732, 2009.</p>
<p>[3] P. B¨ hlmann and S. van de Geer. Statistics for high-dimensional data. Springer-Verlag, 2011. u</p>
<p>[4] E. Cand` s and Y. Plan. Near-ideal model selection by e 37(5A):2145–2177, 2009.  1  minimization. The Annals of Statistics,</p>
<p>[5] E. Candes, J. K. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Trans. on Inform. Theory, 52:489 – 509, 2006.</p>
<p>[6] E. Cand´ s and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n. Annals e of Statistics, 35:2313–2351, 2007.</p>
<p>[7] E. J. Cand´ s and T. Tao. Decoding by linear programming. IEEE Trans. on Inform. Theory, 51:4203– e 4215, 2005.</p>
<p>[8] S. Chen and D. Donoho. Examples of basis pursuit. In Proceedings of Wavelet Applications in Signal and Image Processing III, San Diego, CA, 1995.</p>
<p>[9] D. L. Donoho. Compressed sensing. IEEE Trans. on Inform. Theory, 52:489–509, April 2006.</p>
<p>[10] A. Javanmard and A. Montanari. Hypothesis testing in high-dimensional regression under the gaussian random design model: Asymptotic theory. arXiv preprint arXiv:1301.4240, 2013.</p>
<p>[11] A. Javanmard and A. Montanari. Model selection for high-dimensional regression under the generalized irrepresentability condition. arXiv:1305.0355, 2013.</p>
<p>[12] K. Knight and W. Fu. Asymptotics for lasso-type estimators. Annals of Statistics, pages 1356–1378, 2000.</p>
<p>[13] K. Lounici. Sup-norm convergence rate and sign concentration property of lasso and dantzig estimators. Electronic Journal of statistics, 2:90–102, 2008.</p>
<p>[14] N. Meinshausen and P. B¨ hlmann. High-dimensional graphs and variable selection with the lasso. u Ann. Statist., 34:1436–1462, 2006.</p>
<p>[15] J. Peng, J. Zhu, A. Bergamaschi, W. Han, D.-Y. Noh, J. R. Pollack, and P. Wang. Regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer. The Annals of Applied Statistics, 4(1):53–77, 2010.</p>
<p>[16] S. K. Shevade and S. S. Keerthi. A simple and efﬁcient algorithm for gene selection using sparse logistic regression. Bioinformatics, 19(17):2246–2253, 2003.</p>
<p>[17] R. Tibshirani. Regression shrinkage and selection with the Lasso. J. Royal. Statist. Soc B, 58:267–288, 1996.</p>
<p>[18] R. J. Tibshirani. The lasso problem and uniqueness. Electronic Journal of Statistics, 7:1456–1490, 2013.</p>
<p>[19] S. van de Geer and P. B¨ hlmann. On the conditions used to prove oracle results for the lasso. Electron. J. u Statist., 3:1360–1392, 2009.</p>
<p>[20] S. van de Geer, P. B¨ hlmann, and S. Zhou. The adaptive and the thresholded Lasso for potentially u misspeciﬁed models (and a lower bound for the Lasso). Electron. J. Stat., 5:688–749, 2011.</p>
<p>[21] M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using quadratic programming. IEEE Trans. on Inform. Theory, 55:2183–2202, 2009.  1 -constrained</p>
<p>[22] F. Ye and C.-H. Zhang. Rate minimaxity of the lasso and dantzig selector for the lq loss in lr balls. Journal of Machine Learning Research, 11:3519–3540, 2010.</p>
<p>[23] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research, 7:2541–2563, 2006.</p>
<p>[24] S. Zhou. Thresholded Lasso for high dimensional variable selection and statistical estimation. arXiv:1002.1583v2, 2010.</p>
<p>[25] H. Zou. The adaptive lasso and its oracle properties. Journal of the American Statistical Association, 101(476):1418–1429, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
