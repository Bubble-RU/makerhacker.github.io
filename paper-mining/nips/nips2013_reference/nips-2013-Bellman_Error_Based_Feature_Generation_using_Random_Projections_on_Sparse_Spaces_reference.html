<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-55" href="../nips2013/nips-2013-Bellman_Error_Based_Feature_Generation_using_Random_Projections_on_Sparse_Spaces.html">nips2013-55</a> <a title="nips-2013-55-reference" href="#">nips2013-55-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>55 nips-2013-Bellman Error Based Feature Generation using Random Projections on Sparse Spaces</h1>
<br/><p>Source: <a title="nips-2013-55-pdf" href="http://papers.nips.cc/paper/5182-bellman-error-based-feature-generation-using-random-projections-on-sparse-spaces.pdf">pdf</a></p><p>Author: Mahdi Milani Fard, Yuri Grinberg, Amir massoud Farahmand, Joelle Pineau, Doina Precup</p><p>Abstract: This paper addresses the problem of automatic generation of features for value function approximation in reinforcement learning. Bellman Error Basis Functions (BEBFs) have been shown to improve policy evaluation, with a convergence rate similar to that of value iteration. We propose a simple, fast and robust algorithm based on random projections, which generates BEBFs for sparse feature spaces. We provide a ﬁnite sample analysis of the proposed method, and prove that projections logarithmic in the dimension of the original space guarantee a contraction in the error. Empirical results demonstrate the strength of this method in domains in which choosing a good state representation is challenging. 1</p><br/>
<h2>reference text</h2><p>[1] D. Di Castro and S. Mannor. Adaptive bases for reinforcement learning. Machine Learning and Knowledge Discovery in Databases, pages 312–327, 2010.</p>
<p>[2] J.Z. Kolter and A.Y. Ng. Regularization and feature selection in least-squares temporal difference learning. In International Conference on Machine Learning, 2009.</p>
<p>[3] P.W. Keller, S. Mannor, and D. Precup. Automatic basis function construction for approximate dynamic programming and reinforcement learning. In International Conference on Machine Learning, 2006.</p>
<p>[4] P. Manoonpong, F. W¨ rg¨ tter, and J. Morimoto. Extraction of reward-related feature space using o o correlation-based and reward-based learning methods. Neural Information Processing. Theory and Algorithms, pages 414–421, 2010.</p>
<p>[5] A. Geramifard, F. Doshi, J. Redding, N. Roy, and J.P. How. Online discovery of feature dependencies. In International Conference on Machine Learning, 2011.</p>
<p>[6] R. Parr, C. Painter-Wakeﬁeld, L. Li, and M. Littman. Analyzing feature generation for value-function approximation. In International Conference on Machine Learning, 2007.</p>
<p>[7] J. Boyan and A.W. Moore. Generalization in reinforcement learning: Safely approximating the value function. In Advances in Neural Information Processing Systems, 1995.</p>
<p>[8] E.J. Cand` s and T. Tao. Near-optimal signal recovery from random projections: Universal encoding e strategies. Information Theory, IEEE Transactions on, 52(12):5406–5425, 2006.</p>
<p>[9] E.J. Cand` s and M.B. Wakin. An introduction to compressive sampling. Signal Processing Magazine, e IEEE, 25(2):21–30, 2008.</p>
<p>[10] O.A. Maillard and R. Munos. Linear regression with random projections. Journal of Machine Learning Research, 13:2735–2772, 2012.</p>
<p>[11] M.M. Fard, Y. Grinberg, J. Pineau, and D. Precup. Compressed least-squares regression on sparse spaces. In AAAI, 2012.</p>
<p>[12] O.A. Maillard and R. Munos. Compressed least-squares regression. In Advances in Neural Information Processing Systems, 2009.</p>
<p>[13] S. Zhou, J. Lafferty, and L. Wasserman. Compressed regression. In Proceedings of Advances in neural information processing systems, 2007.</p>
<p>[14] M. Ghavamzadeh, A. Lazaric, O.A. Maillard, and R. Munos. LSTD with random projections. In Advances in Neural Information Processing Systems, 2010.</p>
<p>[15] B.A. Olshausen, P. Sallee, and M.S. Lewicki. Learning sparse image codes using a wavelet pyramid architecture. In Advances in neural information processing systems, 2001.</p>
<p>[16] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.</p>
<p>[17] S.J. Bradtke and A.G. Barto. Linear least-squares algorithms for temporal difference learning. Machine Learning, 22(1):33–57, 1996.</p>
<p>[18] J.A. Boyan. Technical update: Least-squares temporal difference learning. Machine Learning, 49(2): 233–246, 2002.</p>
<p>[19] M.G. Lagoudakis and R. Parr. Least-squares policy iteration. Journal of Machine Learning Research, 4: 1107–1149, 2003. ISSN 1532-4435.</p>
<p>[20] H.R. Maei and R.S. Sutton. GQ (λ): A general gradient algorithm for temporal-difference prediction learning with eligibility traces. In Third Conference on Artiﬁcial General Intelligence, 2010.</p>
<p>[21] I. Menache, S. Mannor, and N. Shimkin. Basis function adaptation in temporal difference reinforcement learning. Annals of Operations Research, 134(1):215–238, 2005.</p>
<p>[22] M.A. Davenport, M.B. Wakin, and R.G. Baraniuk. Detection and estimation with compressive measurements. Dept. of ECE, Rice University, Tech. Rep, 2006.</p>
<p>[23] M.M. Fard, Y. Grinberg, J. Pineau, and D. Precup. Random projections preserve linearity in sparse spaces. School of Computer Science, Mcgill University, Tech. Rep, 2012.</p>
<p>[24] M.A. Davenport, P.T. Boufounos, M.B. Wakin, and R.G. Baraniuk. Signal processing with compressive measurements. Selected Topics in Signal Processing, IEEE Journal of, 4(2):445–460, 2010.</p>
<p>[25] Andrew Y Ng, Adam Coates, Mark Diel, Varun Ganapathi, Jamie Schulte, Ben Tse, Eric Berger, and Eric Liang. Autonomous inverted helicopter ﬂight via reinforcement learning. In Experimental Robotics IX, pages 363–372. Springer, 2006.</p>
<p>[26] Richard Barrett, Michael Berry, Tony F Chan, James Demmel, June Donato, Jack Dongarra, Victor Eijkhout, Roldan Pozo, Charles Romine, and Henk Van der Vorst. Templates for the solution of linear systems: building blocks for iterative methods. Number 43. Society for Industrial and Applied Mathematics, 1987.</p>
<p>[27] A.M. Farahmand, M. Ghavamzadeh, and C. Szepesv´ ri. Regularized policy iteration. In Advances in a Neural Information Processing Systems, 2010.</p>
<p>[28] J. Johns, C. Painter-Wakeﬁeld, and R. Parr. Linear complementarity for regularized policy evaluation and improvement. In Advances in Neural Information Processing Systems, 2010.  9</p>
<br/>
<br/><br/><br/></body>
</html>
