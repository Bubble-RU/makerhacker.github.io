<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-219" href="../nips2013/nips-2013-On_model_selection_consistency_of_penalized_M-estimators%3A_a_geometric_theory.html">nips2013-219</a> <a title="nips-2013-219-reference" href="#">nips2013-219-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>219 nips-2013-On model selection consistency of penalized M-estimators: a geometric theory</h1>
<br/><p>Source: <a title="nips-2013-219-pdf" href="http://papers.nips.cc/paper/5175-on-model-selection-consistency-of-penalized-m-estimators-a-geometric-theory.pdf">pdf</a></p><p>Author: Jason Lee, Yuekai Sun, Jonathan E. Taylor</p><p>Abstract: Penalized M-estimators are used in diverse areas of science and engineering to ﬁt high-dimensional models with some low-dimensional structure. Often, the penalties are geometrically decomposable, i.e. can be expressed as a sum of support functions over convex sets. We generalize the notion of irrepresentable to geometrically decomposable penalties and develop a general framework for establishing consistency and model selection consistency of M-estimators with such penalties. We then use this framework to derive results for some special cases of interest in bioinformatics and statistical learning. 1</p><br/>
<h2>reference text</h2><p>[1] F. Bach. Consistency of the group lasso and multiple kernel learning. J. Mach. Learn. Res., 9:1179–1225, 2008.  8</p>
<p>[2] P.J. Bickel, Y. Ritov, and A.B. Tsybakov. Simultaneous analysis of lasso and dantzig selector. Ann. Statis., 37(4):1705–1732, 2009.</p>
<p>[3] P. B¨ hlmann and S. van de Geer. Statistics for high-dimensional data: Methods, theory and applications. u 2011.</p>
<p>[4] F. Bunea. Honest variable selection in linear and logistic regression models via 1 and 1 + 2 penalization. Electron. J. Stat., 2:1153–1194, 2008.</p>
<p>[5] E. Cand` s and B. Recht. Simple bounds for recovering low-complexity models. Math. Prog. Ser. A, pages e 1–13, 2012.</p>
<p>[6] J. Guo, E. Levina, G. Michailidis, and J. Zhu. Asymptotic properties of the joint neighborhood selection method for estimating categorical markov networks. arXiv preprint.</p>
<p>[7] L. Jacob, G. Obozinski, and J. Vert. Group lasso with overlap and graph lasso. In Int. Conf. Mach. Learn. (ICML), pages 433–440. ACM, 2009.</p>
<p>[8] A. Jalali, P. Ravikumar, V. Vasuki, S. Sanghavi, and UT ECE. On learning discrete graphical models using group-sparse regularization. In Int. Conf. Artif. Intell. Stat. (AISTATS), 2011.</p>
<p>[9] G.M. James, C. Paulson, and P. Rusmevichientong. The constrained lasso. Technical report, University of Southern California, 2012.</p>
<p>[10] S.M. Kakade, O. Shamir, K. Sridharan, and A. Tewari. Learning exponential families in high-dimensions: Strong convexity and sparsity. In Int. Conf. Artif. Intell. Stat. (AISTATS), 2010.</p>
<p>[11] M. Kolar, L. Song, A. Ahmed, and E. Xing. Estimating time-varying networks. Ann. Appl. Stat., 4(1):94– 123, 2010.</p>
<p>[12] C. Lam and J. Fan. Sparsistency and rates of convergence in large covariance matrix estimation. Ann. Statis., 37(6B):4254, 2009.</p>
<p>[13] J.D. Lee and T. Hastie. Learning mixed graphical models. arXiv preprint arXiv:1205.5012, 2012.</p>
<p>[14] P.L. Loh and M.J. Wainwright. Structure estimation for discrete graphical models: Generalized covariance matrices and their inverses. arXiv:1212.0478, 2012.</p>
<p>[15] N. Meinshausen and P. B¨ hlmann. High-dimensional graphs and variable selection with the lasso. Ann. u Statis., 34(3):1436–1462, 2006.</p>
<p>[16] Y. Nardi and A. Rinaldo. On the asymptotic properties of the group lasso estimator for linear models. Electron. J. Stat., 2:605–633, 2008.</p>
<p>[17] S.N. Negahban, P. Ravikumar, M.J. Wainwright, and B. Yu. A uniﬁed framework for high-dimensional analysis of m-estimators with decomposable regularizers. Statist. Sci., 27(4):538–557, 2012.</p>
<p>[18] G. Obozinski, M.J. Wainwright, and M.I. Jordan. Support union recovery in high-dimensional multivariate regression. Ann. Statis., 39(1):1–47, 2011.</p>
<p>[19] P. Ravikumar, M.J. Wainwright, and J.D. Lafferty. High-dimensional ising model selection using 1 regularized logistic regression. Ann. Statis., 38(3):1287–1319, 2010.</p>
<p>[20] P. Ravikumar, M.J. Wainwright, G. Raskutti, and B. Yu. High-dimensional covariance estimation by minimizing 1 -penalized log-determinant divergence. Electron. J. Stat., 5:935–980, 2011.</p>
<p>[21] A.J. Rothman, P.J. Bickel, E. Levina, and J. Zhu. Sparse permutation invariant covariance estimation. Electron. J. Stat., 2:494–515, 2008.</p>
<p>[22] Y. She. Sparse regression with exact clustering. Electron. J. Stat., 4:1055–1096, 2010.</p>
<p>[23] R.J. Tibshirani and J.E. Taylor. The solution path of the generalized lasso. Ann. Statis., 39(3):1335–1371, 2011.</p>
<p>[24] S. Vaiter, G. Peyr´ , C. Dossal, and J. Fadili. Robust sparse analysis regularization. IEEE Trans. Inform. e Theory, 59(4):2001–2016, 2013.</p>
<p>[25] S. van de Geer. Weakly decomposable regularization penalties and structured sparsity. arXiv preprint arXiv:1204.4813, 2012.</p>
<p>[26] M.J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using 1 -constrained quadratic programming (lasso). IEEE Trans. Inform. Theory, 55(5):2183–2202, 2009.</p>
<p>[27] E. Yang and P. Ravikumar. Dirty statistical models. In Adv. Neural Inf. Process. Syst. (NIPS), pages 827–835, 2013.</p>
<p>[28] E. Yang, P. Ravikumar, G.I. Allen, and Z. Liu. On graphical models via univariate exponential family distributions. arXiv:1301.4183, 2013.</p>
<p>[29] M. Yuan and Y. Lin. Model selection and estimation in regression with grouped variables. J. R. Stat. Soc. Ser. B Stat. Methodol., 68(1):49–67, 2006.</p>
<p>[30] P. Zhao and B. Yu. On model selection consistency of lasso. J. Mach. Learn. Res., 7:2541–2563, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
