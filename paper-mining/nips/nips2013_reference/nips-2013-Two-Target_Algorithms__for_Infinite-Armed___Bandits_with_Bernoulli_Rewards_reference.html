<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-338" href="../nips2013/nips-2013-Two-Target_Algorithms__for_Infinite-Armed___Bandits_with_Bernoulli_Rewards.html">nips2013-338</a> <a title="nips-2013-338-reference" href="#">nips2013-338-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>338 nips-2013-Two-Target Algorithms  for Infinite-Armed   Bandits with Bernoulli Rewards</h1>
<br/><p>Source: <a title="nips-2013-338-pdf" href="http://papers.nips.cc/paper/5109-two-target-algorithms-for-infinite-armed-bandits-with-bernoulli-rewards.pdf">pdf</a></p><p>Author: Thomas Bonald, Alexandre Proutiere</p><p>Abstract: We consider an inﬁnite-armed bandit problem with Bernoulli rewards. The mean rewards are independent, uniformly distributed over [0, 1]. Rewards 0 and 1 are referred to as a success and a failure, respectively. We propose a novel algorithm where the decision to exploit any arm is based on two successive targets, namely, the total number of successes until the ﬁrst failure and until the ﬁrst m failures, respectively, where m is a ﬁxed parameter. This two-target algorithm achieves a √ long-term average regret in 2n for a large parameter m and a known time horizon n. This regret is optimal and strictly less than the regret achieved by the best √ known algorithms, which is in 2 n. The results are extended to any mean-reward distribution whose support contains 1 and to unknown time horizons. Numerical experiments show the performance of the algorithm for ﬁnite time horizons. 1</p><br/>
<h2>reference text</h2><p>[1] Peter Auer, Nicol` Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed o bandit problem. Mach. Learn., 47(2-3):235–256, May 2002.</p>
<p>[2] Donald A. Berry, Robert W. Chen, Alan Zame, David C. Heath, and Larry A. Shepp. Bandit problems with inﬁnitely many arms. Annals of Statistics, 25(5):2103–2116, 1997.</p>
<p>[3] Olivier Capp´ , Aur´ lien Garivier, Odalric-Ambrym Maillard, R´ mi Munos, and Gilles Stoltz. e e e Kullback-leibler upper conﬁdence bounds for optimal sequential allocation. To appear in Annals of Statistics, 2013.</p>
<p>[4] Kung-Yu Chen and Chien-Tai Lin. A note on strategies for bandit problems with inﬁnitely many arms. Metrika, 59(2):193–203, 2004.</p>
<p>[5] Kung-Yu Chen and Chien-Tai Lin. A note on inﬁnite-armed bernoulli bandit problems with generalized beta prior distributions. Statistical Papers, 46(1):129–140, 2005.</p>
<p>[6] Stephen J Herschkorn, Erol Pekoez, and Sheldon M Ross. Policies without memory for the inﬁnite-armed bernoulli bandit under the average-reward criterion. Probability in the Engineering and Informational Sciences, 10:21–28, 1996.</p>
<p>[7] Ying-Chao Hung. Optimal bayesian strategies for the inﬁnite-armed bernoulli bandit. Journal of Statistical Planning and Inference, 142(1):86–94, 2012.</p>
<p>[8] Emilie Kaufmann, Nathaniel Korda, and R´ mi Munos. Thompson sampling: An asymptotie cally optimal ﬁnite-time analysis. In Algorithmic Learning Theory, pages 199–213. Springer, 2012.</p>
<p>[9] Tze L. Lai and Herbert Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4–22, 1985.</p>
<p>[10] Tze Leung Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of Statistics, pages 1091–1114, 1987.</p>
<p>[11] Chien-Tai Lin and CJ Shiau. Some optimal strategies for bandit problems with beta prior distributions. Annals of the Institute of Statistical Mathematics, 52(2):397–405, 2000.</p>
<p>[12] C.L Mallows and Herbert Robbins. Some problems of optimal sampling strategy. Journal of Mathematical Analysis and Applications, 8(1):90 – 103, 1964.</p>
<p>[13] Olivier Teytaud, Sylvain Gelly, and Mich` le Sebag. Anytime many-armed bandits. In CAP07, e 2007.</p>
<p>[14] W. R. Thompson. On the Likelihood that one Unknown Probability Exceeds Another in View of the Evidence of Two Samples. Biometrika, 25:285–294, 1933.</p>
<p>[15] Yizao Wang, Jean-Yves Audibert, and R´ mi Munos. Algorithms for inﬁnitely many-armed e bandits. In NIPS, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
