<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>223 nips-2013-On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-223" href="../nips2013/nips-2013-On_the_Relationship_Between_Binary_Classification%2C_Bipartite_Ranking%2C_and_Binary_Class_Probability_Estimation.html">nips2013-223</a> <a title="nips-2013-223-reference" href="#">nips2013-223-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>223 nips-2013-On the Relationship Between Binary Classification, Bipartite Ranking, and Binary Class Probability Estimation</h1>
<br/><p>Source: <a title="nips-2013-223-pdf" href="http://papers.nips.cc/paper/4907-on-the-relationship-between-binary-classification-bipartite-ranking-and-binary-class-probability-estimation.pdf">pdf</a></p><p>Author: Harikrishna Narasimhan, Shivani Agarwal</p><p>Abstract: We investigate the relationship between three fundamental problems in machine learning: binary classiﬁcation, bipartite ranking, and binary class probability estimation (CPE). It is known that a good binary CPE model can be used to obtain a good binary classiﬁcation model (by thresholding at 0.5), and also to obtain a good bipartite ranking model (by using the CPE model directly as a ranking model); it is also known that a binary classiﬁcation model does not necessarily yield a CPE model. However, not much is known about other directions. Formally, these relationships involve regret transfer bounds. In this paper, we introduce the notion of weak regret transfer bounds, where the mapping needed to transform a model from one problem to another depends on the underlying probability distribution (and in practice, must be estimated from data). We then show that, in this weaker sense, a good bipartite ranking model can be used to construct a good classiﬁcation model (by thresholding at a suitable point), and more surprisingly, also to construct a good binary CPE model (by calibrating the scores of the ranking model). 1</p><br/>
<h2>reference text</h2><p>[1] T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. Annals of Statistics, 32(1):56–134, 2004.</p>
<p>[2] P. L. Bartlett, M. I. Jordan, and J. D. McAuliffe. Convexity, classiﬁcation and risk bounds. Journal of the American Statistical Association, 101(473):138–156, 2006.</p>
<p>[3] M. D. Reid and R. C. Williamson. Surrogate regret bounds for proper losses. In ICML, 2009.</p>
<p>[4] C. Scott. Calibrated asymmetric surrogate losses. Electronic Journal of Statistics, 6:958–992, 2012.</p>
<p>[5] Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. An efﬁcient boosting algorithm for combining preferences. Journal of Machine Learning Research, 4:933–969, 2003.</p>
<p>[6] C. Cortes and M. Mohri. AUC optimization vs. error rate minimization. In Advances in Neural Information Processing Systems 16. MIT Press, 2004.</p>
<p>[7] S. Agarwal, T. Graepel, R. Herbrich, S. Har-Peled, and D. Roth. Generalization bounds for the area under the ROC curve. Journal of Machine Learning Research, 6:393–425, 2005.</p>
<p>[8] S. Cl´ mencon, G. Lugosi, and N. Vayatis. Ranking and empirical minimization of U-statistics. Annals of e ¸ Statistics, 36:844–874, 2008.</p>
<p>[9] A. Buja, W. Stuetzle, and Y. Shen. Loss functions for binary class probability estimation: Structure and applications. Technical report, University of Pennsylvania, November 2005.</p>
<p>[10] M. D. Reid and R. C. Williamson. Composite binary losses. Journal of Machine Learning Research, 11:2387–2422, 2010.</p>
<p>[11] L. Devroye, L. Gyorﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, 1996.</p>
<p>[12] S. Cl´ mencon and S. Robbiano. Minimax learning rates for bipartite ranking and plug-in rules. In e ¸ Proceedings of the 28th International Conference on Machine Learning, 2011.</p>
<p>[13] John Langford and Bianca Zadrozny. Estimating class membership probabilities using classiﬁer learners. In AISTATS, 2005.</p>
<p>[14] C. Rudin and R.E. Schapire. Margin-based ranking and an equivalence between adaboost and rankboost. Journal of Machine Learning Research, 10:2193–2232, 2009.</p>
<p>[15] S. Ertekin and C. Rudin. On equivalence relationships between classiﬁcation and ranking algorithms. ¸ Journal of Machine Learning Research, 12:2905–2929, 2011.</p>
<p>[16] M. Ayer, H.D. Brunk, G. M. Ewing, W. T. Reid, and E. Silverman. An empirical distribution function for sampling with incomplete information. The Annals of Mathematical Statistics, 26(4):641–647, 1955.</p>
<p>[17] H. D. Brunk. On the estimation of parameters restricted by inequalities. The Annals of Mathematical Statistics, 29(2):437–454, 1958.</p>
<p>[18] B. Zadrozny and C. Elkan. Transforming classiﬁer scores into accurate multiclass probability estimates. In KDD, 2002.</p>
<p>[19] A.K. Menon, X. Jiang, S. Vembu, C. Elkan, and L. Ohno-Machado. Predicting accurate probabilities with a ranking loss. In ICML, 2012.</p>
<p>[20] J. Hern´ ndez-Orallo, P. Flach, and C. Ferri. A uniﬁed view of performance metrics: Translating threshold a choice into expected classiﬁcation loss. Journal of Machine Learning Research, 13:2813–2869, 2012.</p>
<p>[21] P. Bartlett. CS281B/Stat241B (Spring 2008) Statistical Learning Theory [Lecture 19 notes], University of California, Berkeley. http://www.cs.berkeley.edu/˜bartlett/courses/281b-sp08/ 19.pdf. 2008.</p>
<p>[22] C. Drummond and R.C. Holte. Cost curves: An improved method for visualizing classiﬁer performance. Machine Learning, 65(1):95–130, 2006.</p>
<p>[23] M.A. Maloof. Learning when data sets are imbalanced and when costs are unequal and unknown. In ICML-2003 Workshop on Learning from Imbalanced Data Sets II, volume 2, 2003.</p>
<p>[24] A.T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression. In COLT, 2009.</p>
<p>[25] T. Fawcett and A. Niculescu-Mizil. PAV and the ROC convex hull. Machine Learning, 68(1):97–106, 2007.</p>
<p>[26] S. Agarwal. Surrogate regret bounds for the area under the ROC curve via strongly proper losses. In COLT, 2013.</p>
<p>[27] D. Anevski and P. Soulier. Monotone spectral density estimation. Annals of Statistics, 39(1):418–438, 2011.</p>
<p>[28] P. Groeneboom and G. Jongbloed. Generalized continuous isotonic regression. Statistics & Probability Letters, 80(34):248–253, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
