<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-239" href="../nips2013/nips-2013-Optimistic_policy_iteration_and_natural_actor-critic%3A_A_unifying_view_and_a_non-optimality_result.html">nips2013-239</a> <a title="nips-2013-239-reference" href="#">nips2013-239-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>239 nips-2013-Optimistic policy iteration and natural actor-critic: A unifying view and a non-optimality result</h1>
<br/><p>Source: <a title="nips-2013-239-pdf" href="http://papers.nips.cc/paper/5188-optimistic-policy-iteration-and-natural-actor-critic-a-unifying-view-and-a-non-optimality-result.pdf">pdf</a></p><p>Author: Paul Wagner</p><p>Abstract: Approximate dynamic programming approaches to the reinforcement learning problem are often categorized into greedy value function methods and value-based policy gradient methods. As our ﬁrst main result, we show that an important subset of the latter methodology is, in fact, a limiting special case of a general formulation of the former methodology; optimistic policy iteration encompasses not only most of the greedy value function methods but also natural actor-critic methods, and permits one to directly interpolate between them. The resulting continuum adjusts the strength of the Markov assumption in policy improvement and, as such, can be seen as dual in spirit to the continuum in TD(λ)-style algorithms in policy evaluation. As our second main result, we show for a substantial subset of softgreedy value function approaches that, while having the potential to avoid policy oscillation and policy chattering, this subset can never converge toward an optimal policy, except in a certain pathological case. Consequently, in the context of approximations (either in state estimation or in value function representation), the majority of greedy value function methods seem to be deemed to suffer either from the risk of oscillation/chattering or from the presence of systematic sub-optimality. 1</p><br/>
<h2>reference text</h2><p>[1] Armijo, L. (1966). Minimization of functions having Lipschitz continuous ﬁrst partial derivatives. Paciﬁc Journal of Mathematics, 16(1), 1–3.</p>
<p>[2] Baxter, J., & Bartlett, P. L. (2000). Reinforcement learning in POMDP’s via direct gradient ascent. In Proceedings of the Seventeenth International Conference on Machine Learning, (pp. 41–48).</p>
<p>[3] Bertsekas, D. P. (1997). A new class of incremental gradient methods for least squares problems. SIAM Journal on Optimization, 7(4), 913–926.</p>
<p>[4] Bertsekas, D. P. (2005). Dynamic Programming and Optimal Control. Athena Scientiﬁc.</p>
<p>[5] Bertsekas, D. P. (2010). Pathologies of temporal difference methods in approximate dynamic programming. In 49th IEEE Conference on Decision and Control, (pp. 3034–3039).</p>
<p>[6] Bertsekas, D. P. (2011). Approximate policy iteration: A survey and some new methods. Journal of Control Theory and Applications, 9(3), 310–335.</p>
<p>[7] Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientiﬁc.</p>
<p>[8] Bhatnagar, S., Sutton, R. S., Ghavamzadeh, M., & Lee, M. (2009). Natural actor-critic algorithms. Automatica, 45(11), 2471–2482.</p>
<p>[9] Busoniu, L., Babuˇka, R., De Schutter, B., & Ernst, D. (2010). Reinforcement learning and dynamic ¸ s programming using function approximators. CRC Press.</p>
<p>[10] De Farias, D. P., & Van Roy, B. (2000). On the existence of ﬁxed points for approximate value iteration and temporal-difference learning. Journal of Optimization Theory and Applications, 105(3), 589–608.</p>
<p>[11] Kakade, S. M. (2002). A natural policy gradient. In Advances in Neural Information Processing Systems.</p>
<p>[12] Kakade, S. M. (2003). On the Sample Complexity of Reinforcement Learning. Ph.D. thesis, University College London.</p>
<p>[13] Konda, V. R., & Tsitsiklis, J. N. (2004). On actor-critic algorithms. SIAM Journal on Control and Optimization, 42(4), 1143–1166.</p>
<p>[14] Melo, F. S., Meyn, S. P., & Ribeiro, M. I. (2008). An analysis of reinforcement learning with function approximation. In Proceedings of the 25th International Conference on Machine Learning, (pp. 664–671).</p>
<p>[15] Nedi´ , A., & Bertsekas, D. P. (2003). Least squares policy evaluation algorithms with linear function c approximation. Discrete Event Dynamic Systems: Theory and Applications, 13(1–2), 79–110.</p>
<p>[16] Pendrith, M. D., & McGarity, M. J. (1998). An analysis of direct reinforcement learning in non-Markovian domains. In Proceedings of the Fifteenth International Conference on Machine Learning.</p>
<p>[17] Perkins, T. J., & Pendrith, M. D. (2002). On the existence of ﬁxed points for Q-learning and sarsa in partially observable domains. In Proceedings of the Nineteenth International Conference on Machine Learning, (pp. 490–497).</p>
<p>[18] Perkins, T. J., & Precup, D. (2003). A convergent form of approximate policy iteration. In Advances in Neural Information Processing Systems.</p>
<p>[19] Peters, J., & Schaal, S. (2008). Natural actor-critic. Neurocomputing, 71(7-9), 1180–1190.</p>
<p>[20] Singh, S. P., Jaakkola, T., & Jordan, M. I. (1994). Learning without state-estimation in partially observable Markovian decision processes. In Proceedings of the Eleventh International Conference on Machine Learning.</p>
<p>[21] Sutton, R. S., & Barto, A. G. (1998). Reinforcement Learning: An Introduction. MIT Press.</p>
<p>[22] Sutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (2000). Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems.</p>
<p>[23] Szepesv´ ri, C. (2010). Algorithms for Reinforcement Learning. Morgan & Claypool Publishers. a</p>
<p>[24] Wagner, P. (2011). A reinterpretation of the policy oscillation phenomenon in approximate policy iteration. In Advances in Neural Information Processing Systems 24, (pp. 2573–2581).</p>
<p>[25] Wagner, P. (to appear). Policy oscillation is overshooting. Neural Networks. Author manuscript available at http://users.ics.aalto.fi/pwagner/.  9</p>
<br/>
<br/><br/><br/></body>
</html>
