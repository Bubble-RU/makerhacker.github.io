<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-333" href="../nips2013/nips-2013-Trading_Computation_for_Communication%3A_Distributed_Stochastic_Dual_Coordinate_Ascent.html">nips2013-333</a> <a title="nips-2013-333-reference" href="#">nips2013-333-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>333 nips-2013-Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</h1>
<br/><p>Source: <a title="nips-2013-333-pdf" href="http://papers.nips.cc/paper/5114-trading-computation-for-communication-distributed-stochastic-dual-coordinate-ascent.pdf">pdf</a></p><p>Author: Tianbao Yang</p><p>Abstract: We present and study a distributed optimization algorithm by employing a stochastic dual coordinate ascent method. Stochastic dual coordinate ascent methods enjoy strong theoretical guarantees and often have better performances than stochastic gradient descent methods in optimizing regularized loss minimization problems. It still lacks of efforts in studying them in a distributed framework. We make a progress along the line by presenting a distributed stochastic dual coordinate ascent algorithm in a star network, with an analysis of the tradeoff between computation and communication. We verify our analysis by experiments on real data sets. Moreover, we compare the proposed algorithm with distributed stochastic gradient descent methods and distributed alternating direction methods of multipliers for optimizing SVMs in the same distributed framework, and observe competitive performances. 1</p><br/>
<h2>reference text</h2><p>[1] A. Agarwal and J. C. Duchi. Distributed delayed stochastic optimization. In CDC, pages 5451–5452, 2012.</p>
<p>[2] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Found. Trends Mach. Learn., 3:1– 122, 2011.</p>
<p>[3] J. K. Bradley, A. Kyrola, D. Bickson, and C. Guestrin. Parallel Coordinate Descent for L1Regularized Loss Minimization. In ICML, 2011.</p>
<p>[4] C. T. Chu, S. K. Kim, Y. A. Lin, Y. Yu, G. R. Bradski, A. Y. Ng, and K. Olukotun. Map-Reduce for machine learning on multicore. In NIPS, pages 281–288, 2006.</p>
<p>[5] W. Deng and W. Yin. On the global and linear convergence of the generalized alternating direction method of multipliers. Technical report, 2012.</p>
<p>[6] M. Eberts and I. Steinwart. Optimal learning rates for least squares svms using gaussian kernels. In NIPS, pages 1539–1547, 2011.</p>
<p>[7] D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems via ﬁnite element approximation. Comput. Math. Appl., 2:17–40, 1976.</p>
<p>[8] C.-J. Hsieh, K.-W. Chang, C.-J. Lin, S. S. Keerthi, and S. Sundararajan. A dual coordinate descent method for large-scale linear svm. In ICML, pages 408–415, 2008.</p>
<p>[9] H. D. III, J. M. Phillips, A. Saha, and S. Venkatasubramanian. Protocols for learning classiﬁers on distributed data. JMLR- Proceedings Track, 22:282–290, 2012.</p>
<p>[10] S. Lacoste-Julien, M. Jaggi, M. W. Schmidt, and P. Pletscher. Stochastic block-coordinate frank-wolfe optimization for structural svms. CoRR, abs/1207.4747, 2012.</p>
<p>[11] J. Langford, A. Smola, and M. Zinkevich. Slow learners are fast. In NIPS, pages 2331–2339. 2009.</p>
<p>[12] Z. Q. Luo and P. Tseng. On the convergence of the coordinate descent method for convex differentiable minimization. Journal of Optimization Theory and Applications, pages 7–35, 1992.</p>
<p>[13] G. Mann, R. McDonald, M. Mohri, N. Silberman, and D. Walker. Efﬁcient Large-Scale distributed training of conditional maximum entropy models. In NIPS, pages 1231–1239. 2009.</p>
<p>[14] H. Ouyang, N. He, L. Tran, and A. G. Gray. Stochastic alternating direction method of multipliers. In ICML, pages 80–88, 2013.</p>
<p>[15] N. L. Roux, M. W. Schmidt, and F. Bach. A stochastic gradient method with an exponential convergence rate for ﬁnite training sets. In NIPS, pages 2672–2680, 2012.</p>
<p>[16] S. Shalev-Shwartz and T. Zhang. Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization. JMLR, 2013.</p>
<p>[17] S. Smale and D.-X. Zhou. Estimating the approximation error in learning theory. Anal. Appl. (Singap.), 1(1):17–41, 2003.</p>
<p>[18] K. Sridharan, S. Shalev-Shwartz, and N. Srebro. Fast rates for regularized objectives. In NIPS, pages 1545–1552, 2008.</p>
<p>[19] T. Suzuki. Dual averaging and proximal gradient descent for online alternating direction multiplier method. In ICML, pages 392–400, 2013.</p>
<p>[20] M. Tak´ c, A. S. Bijral, P. Richt´ rik, and N. Srebro. Mini-batch primal and dual methods for a a svms. In ICML, 2013.</p>
<p>[21] C. H. Teo, S. Vishwanthan, A. J. Smola, and Q. V. Le. Bundle methods for regularized risk minimization. JMLR, pages 311–365, 2010.</p>
<p>[22] K. I. Tsianos, S. Lawlor, and M. G. Rabbat. Communication/computation tradeoffs in consensus-based distributed optimization. In NIPS, pages 1952–1960, 2012.</p>
<p>[23] C. Zhang, H. Lee, and K. G. Shin. Efﬁcient distributed linear classiﬁcation algorithms via the alternating direction method of multipliers. In AISTAT, pages 1398–1406, 2012.</p>
<p>[24] M. Zinkevich, M. Weimer, A. Smola, and L. Li. Parallelized stochastic gradient descent. In NIPS, pages 2595–2603, 2010. 9</p>
<br/>
<br/><br/><br/></body>
</html>
