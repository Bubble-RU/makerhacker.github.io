<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>92 nips-2013-Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-92" href="../nips2013/nips-2013-Discovering_Hidden_Variables_in_Noisy-Or_Networks_using_Quartet_Tests.html">nips2013-92</a> <a title="nips-2013-92-reference" href="#">nips2013-92-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>92 nips-2013-Discovering Hidden Variables in Noisy-Or Networks using Quartet Tests</h1>
<br/><p>Source: <a title="nips-2013-92-pdf" href="http://papers.nips.cc/paper/5064-discovering-hidden-variables-in-noisy-or-networks-using-quartet-tests.pdf">pdf</a></p><p>Author: Yacine Jernite, Yonatan Halpern, David Sontag</p><p>Abstract: We give a polynomial-time algorithm for provably learning the structure and parameters of bipartite noisy-or Bayesian networks of binary variables where the top layer is completely hidden. Unsupervised learning of these models is a form of discrete factor analysis, enabling the discovery of hidden variables and their causal relationships with observed data. We obtain an efﬁcient learning algorithm for a family of Bayesian networks that we call quartet-learnable. For each latent variable, the existence of a singly-coupled quartet allows us to uniquely identify and learn all parameters involving that latent variable. We give a proof of the polynomial sample complexity of our learning algorithm, and experimentally compare it to variational EM. 1</p><br/>
<h2>reference text</h2><p>Anandkumar, Anima, Chaudhuri, Kamalika, Hsu, Daniel, Kakade, Sham, Song, Le, & Zhang, Tong. 2011. Spectral Methods for Learning Multivariate Latent Tree Structure. Proceedings of NIPS 24, 2025–2033. Anandkumar, Anima, Foster, Dean, Hsu, Daniel, Kakade, Sham, & Liu, Yi-Kai. 2012a. A spectral algorithm for latent Dirichlet allocation. Proceedings of NIPS 25, 926–934. Anandkumar, Animashree, Hsu, Daniel, & Kakade, Sham M. 2012b. A method of moments for mixture models and hidden Markov models. In: Proceedings of COLT 2012. Anandkumar, Animashree, Javanmard, Adel, Hsu, Daniel J, & Kakade, Sham M. 2013. Learning Linear Bayesian Networks with Latent Variables. Pages 249–257 of: Proceedings of ICML. Chang, Joseph T. 1996. Full reconstruction of Markov models on evolutionary trees: identiﬁability and consistency. Mathematical biosciences, 137(1), 51–73. Cooper, Gregory F. 1987. Probabilistic Inference Using Belief Networks Is NP-Hard. Technical Report BMIR-1987-0195. Medical Computer Science Group, Stanford University. Elidan, Gal, & Friedman, Nir. 2006. Learning hidden variable networks: The information bottleneck approach. Journal of Machine Learning Research, 6(1), 81. Elidan, Gal, Lotner, Noam, Friedman, Nir, & Koller, Daphne. 2001. Discovering hidden variables: A structure-based approach. Advances in Neural Information Processing Systems, 479–485. Elsner, Ludwig. 1985. An optimal bound for the spectral variation of two matrices. Linear algebra and its applications, 71, 77–80. Eriksson, Nicholas. 2005. Tree construction using singular value decomposition. Algebraic Statistics for computational biology, 347–358. Friedman, Nir. 1997. Learning Belief Networks in the Presence of Missing Values and Hidden Variables. Pages 125–133 of: ICML ’97. Halpern, Yoni, & Sontag, David. 2013. Unsupervised Learning of Noisy-Or Bayesian Networks. In: Conference on Uncertainty in Artiﬁcial Intelligence (UAI-13). Ishteva, Mariya, Park, Haesun, & Song, Le. 2013. Unfolding Latent Tree Structures using 4th Order Tensors. In: ICML ’13. Kearns, Michael, & Mansour, Yishay. 1998. Exact inference of hidden structure from sample data in noisy-OR networks. Pages 304–310 of: Proceedings of UAI 14. Lazarsfeld, Paul. 1950. Latent Structure Analysis. In: Stouffer, Samuel, Guttman, Louis, Suchman, Edward, Lazarsfeld, Paul, Star, Shirley, & Clausen, John (eds), Measurement and Prediction. Princeton, New Jersey: Princeton University Press. Lazic, Nevena, Bishop, Christopher M, & Winn, John. 2013. Structural Expectation Propagation: Bayesian structure learning for networks with latent variables. In: Proceedings of AISTATS 16. Martin, J, & VanLehn, Kurt. 1995. Discrete factor analysis: Learning hidden variables in Bayesian networks. Tech. rept. Department of Computer Science, University of Pittsburgh. Mossel, Elchanan, & Roch, S´ bastien. 2005. Learning nonsingular phylogenies and hidden Markov e models. Pages 366–375 of: Proceedings of 37th STOC. ACM. Pearl, Judea, & Tarsi, Michael. 1986. Structuring causal trees. Journal of Complexity, 2(1), 60–77. Saund, Eric. 1995. A multiple cause mixture model for unsupervised learning. Neural Computation, 7(1), 51–71. Shwe, Michael A, Middleton, B, Heckerman, DE, Henrion, M, Horvitz, EJ, Lehmann, HP, & Cooper, GF. 1991. Probabilistic diagnosis using a reformulation of the INTERNIST-1/QMR knowledge base. Meth. Inform. Med, 30, 241–255. Silva, Ricardo, Scheine, Richard, Glymour, Clark, & Spirtes, Peter. 2006. Learning the structure of linear latent variable models. The Journal of Machine Learning Research, 7, 191–246. ˇ Singliar, Tom´ s, & Hauskrecht, Miloˇ. 2006. Noisy-or component analysis and its application to aˇ s link analysis. The Journal of Machine Learning Research, 7, 2189–2213.  9</p>
<br/>
<br/><br/><br/></body>
</html>
