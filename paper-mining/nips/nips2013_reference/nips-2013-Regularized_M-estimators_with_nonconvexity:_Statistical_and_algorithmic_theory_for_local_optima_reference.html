<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-271" href="../nips2013/nips-2013-Regularized_M-estimators_with_nonconvexity%3A_Statistical_and_algorithmic_theory_for_local_optima.html">nips2013-271</a> <a title="nips-2013-271-reference" href="#">nips2013-271-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>271 nips-2013-Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</h1>
<br/><p>Source: <a title="nips-2013-271-pdf" href="http://papers.nips.cc/paper/4904-regularized-m-estimators-with-nonconvexity-statistical-and-algorithmic-theory-for-local-optima.pdf">pdf</a></p><p>Author: Po-Ling Loh, Martin J. Wainwright</p><p>Abstract: We establish theoretical results concerning local optima of regularized M estimators, where both loss and penalty functions are allowed to be nonconvex. Our results show that as long as the loss satisﬁes restricted strong convexity and the penalty satisﬁes suitable regularity conditions, any local optimum of the composite objective lies within statistical precision of the true parameter vector. Our theory covers a broad class of nonconvex objective functions, including corrected versions of the Lasso for errors-in-variables linear models and regression in generalized linear models using nonconvex regularizers such as SCAD and MCP. On the optimization side, we show that a simple adaptation of composite gradient descent may be used to compute a global optimum up to the statistical precision stat in log(1/ stat ) iterations, the fastest possible rate for any ﬁrst-order method. We provide simulations to illustrate the sharpness of our theoretical predictions. 1</p><br/>
<h2>reference text</h2><p>[1] A. Agarwal, S. Negahban, and M. J. Wainwright. Fast global convergence of gradient methods for high-dimensional statistical recovery. Annals of Statistics, 40(5):2452–2482, 2012.</p>
<p>[2] P. Breheny and J. Huang. Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection. Annals of Applied Statistics, 5(1):232–253, 2011.</p>
<p>[3] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96:1348–1360, 2001.</p>
<p>[4] D. R. Hunter and R. Li. Variable selection using MM algorithms. Annals of Statistics, 33(4):1617–1642, 2005.</p>
<p>[5] E.L. Lehmann and G. Casella. Theory of Point Estimation. Springer Verlag, 1998.</p>
<p>[6] P. Loh and M.J. Wainwright. High-dimensional regression with noisy and missing data: Provable guarantees with non-convexity. Annals of Statistics, 40(3):1637–1664, 2012.</p>
<p>[7] P. Loh and M.J. Wainwright. Regularized M -estimators with nonconvexity: Statistical and algorithmic theory for local optima. arXiv e-prints, May 2013. Available at http://arxiv. org/abs/1305.2436.</p>
<p>[8] P. McCullagh and J. A. Nelder. Generalized Linear Models (Second Edition). London: Chapman & Hall, 1989.</p>
<p>[9] S. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A uniﬁed framework for highdimensional analysis of M -estimators with decomposable regularizers. Statistical Science, 27(4):538–557, December 2012. See arXiv version for lemma/propositions cited here.</p>
<p>[10] Y. Nesterov. Gradient methods for minimizing composite objective function. CORE Discussion Papers 2007076, Universit Catholique de Louvain, Center for Operations Research and Econometrics (CORE), 2007.</p>
<p>[11] Y. Nesterov and A. Nemirovskii. Interior Point Polynomial Algorithms in Convex Programming. SIAM studies in applied and numerical mathematics. Society for Industrial and Applied Mathematics, 1987.</p>
<p>[12] S. A. Vavasis. Complexity issues in global optimization: A survey. In Handbook of Global Optimization, pages 27–41. Kluwer, 1995.</p>
<p>[13] C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Annals of Statistics, 38(2):894–942, 2010.</p>
<p>[14] C.-H. Zhang and T. Zhang. A general theory of concave regularization for high-dimensional sparse estimation problems. Statistical Science, 27(4):576–593, 2012.</p>
<p>[15] H. Zou and R. Li. One-step sparse estimates in nonconcave penalized likelihood models. Annals of Statistics, 36(4):1509–1533, 2008.  9</p>
<br/>
<br/><br/><br/></body>
</html>
