<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>10 nips-2013-A Latent Source Model for Nonparametric Time Series Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-10" href="../nips2013/nips-2013-A_Latent_Source_Model_for_Nonparametric_Time_Series_Classification.html">nips2013-10</a> <a title="nips-2013-10-reference" href="#">nips2013-10-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>10 nips-2013-A Latent Source Model for Nonparametric Time Series Classification</h1>
<br/><p>Source: <a title="nips-2013-10-pdf" href="http://papers.nips.cc/paper/5116-a-latent-source-model-for-nonparametric-time-series-classification.pdf">pdf</a></p><p>Author: George H. Chen, Stanislav Nikolov, Devavrat Shah</p><p>Abstract: For classifying time series, a nearest-neighbor approach is widely used in practice with performance often competitive with or better than more elaborate methods such as neural networks, decision trees, and support vector machines. We develop theoretical justiﬁcation for the effectiveness of nearest-neighbor-like classiﬁcation of time series. Our guiding hypothesis is that in many applications, such as forecasting which topics will become trends on Twitter, there aren’t actually that many prototypical time series to begin with, relative to the number of time series we have access to, e.g., topics become trends on Twitter only in a few distinct manners whereas we can collect massive amounts of Twitter data. To operationalize this hypothesis, we propose a latent source model for time series, which naturally leads to a “weighted majority voting” classiﬁcation rule that can be approximated by a nearest-neighbor classiﬁer. We establish nonasymptotic performance guarantees of both weighted majority voting and nearest-neighbor classiﬁcation under our model accounting for how much of the time series we observe and the model complexity. Experimental results on synthetic data show weighted majority voting achieving the same misclassiﬁcation rate as nearest-neighbor classiﬁcation while observing less of the time series. We then use weighted majority to forecast which news topics on Twitter become trends, where we are able to detect such “trending topics” in advance of Twitter 79% of the time, with a mean early advantage of 1 hour and 26 minutes, a true positive rate of 95%, and a false positive rate of 4%. 1</p><br/>
<h2>reference text</h2><p>[1] Anthony Bagnall, Luke Davis, Jon Hills, and Jason Lines. Transformation based ensembles for time series classiﬁcation. In Proceedings of the 12th SIAM International Conference on Data Mining, pages 307–319, 2012.</p>
<p>[2] Gustavo E.A.P.A. Batista, Xiaoyue Wang, and Eamonn J. Keogh. A complexity-invariant distance measure for time series. In Proceedings of the 11th SIAM International Conference on Data Mining, pages 699–710, 2011.</p>
<p>[3] Hila Becker, Mor Naaman, and Luis Gravano. Beyond trending topics: Real-world event identiﬁcation on Twitter. In Proceedings of the Fifth International Conference on Weblogs and Social Media, 2011.</p>
<p>[4] Mario Cataldi, Luigi Di Caro, and Claudio Schifanella. Emerging topic detection on twitter based on temporal and social terms evaluation. In Proceedings of the 10th International Workshop on Multimedia Data Mining, 2010.</p>
<p>[5] Thomas M. Cover and Peter E. Hart. Nearest neighbor pattern classiﬁcation. IEEE Transactions on Information Theory, 13(1):21–27, 1967.</p>
<p>[6] Sanjoy Dasgupta and Leonard Schulman. A probabilistic analysis of EM for mixtures of separated, spherical gaussians. Journal of Machine Learning Research, 8:203–226, 2007.</p>
<p>[7] Hui Ding, Goce Trajcevski, Peter Scheuermann, Xiaoyue Wang, and Eamonn Keogh. Querying and mining of time series data: experimental comparison of representations and distance measures. Proceedings of the VLDB Endowment, 1(2):1542–1552, 2008.</p>
<p>[8] Dan Feldman, Matthew Faulkner, and Andreas Krause. Scalable training of mixture models via coresets. In Advances in Neural Information Processing Systems 24, 2011.</p>
<p>[9] Keinosuke Fukunaga. Introduction to statistical pattern recognition (2nd ed.). Academic Press Professional, Inc., 1990.</p>
<p>[10] Daniel Hsu and Sham M. Kakade. Learning mixtures of spherical gaussians: Moment methods and spectral decompositions, 2013. arXiv:1206.5766.</p>
<p>[11] Shiva Prasad Kasiviswanathan, Prem Melville, Arindam Banerjee, and Vikas Sindhwani. Emerging topic detection using dictionary learning. In Proceedings of the 20th ACM Conference on Information and Knowledge Management, pages 745–754, 2011.</p>
<p>[12] Shiva Prasad Kasiviswanathan, Huahua Wang, Arindam Banerjee, and Prem Melville. Online l1dictionary learning with application to novel document detection. In Advances in Neural Information Processing Systems 25, pages 2267–2275, 2012.</p>
<p>[13] Michael Mathioudakis and Nick Koudas. Twittermonitor: trend detection over the Twitter stream. In Proceedings of the 2010 ACM SIGMOD International Conference on Management of Data, 2010.</p>
<p>[14] Ankur Moitra and Gregory Valiant. Settling the polynomial learnability of mixtures of gaussians. In 51st Annual IEEE Symposium on Foundations of Computer Science, pages 93–102, 2010.</p>
<p>[15] Alex Nanopoulos, Rob Alcock, and Yannis Manolopoulos. Feature-based classiﬁcation of time-series data. International Journal of Computer Research, 10, 2001.</p>
<p>[16] Juan J. Rodr´guez and Carlos J. Alonso. Interval and dynamic time warping-based decision trees. In ı Proceedings of the 2004 ACM Symposium on Applied Computing, 2004.</p>
<p>[17] Santosh Vempala and Grant Wang. A spectral algorithm for learning mixture models. Journal of Computer and System Sciences, 68(4):841–860, 2004.</p>
<p>[18] Kilian Q. Weinberger and Lawrence K. Saul. Distance metric learning for large margin nearest neighbor classiﬁcation. Journal of Machine Learning Research, 10:207–244, 2009.</p>
<p>[19] Yi Wu and Edward Y. Chang. Distance-function design and fusion for sequence data. In Proceedings of the 2004 ACM International Conference on Information and Knowledge Management, 2004.</p>
<p>[20] Xiaopeng Xi, Eamonn J. Keogh, Christian R. Shelton, Li Wei, and Chotirat Ann Ratanamahatana. Fast time series classiﬁcation using numerosity reduction. In Proceedings of the 23rd International Conference on Machine Learning, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
