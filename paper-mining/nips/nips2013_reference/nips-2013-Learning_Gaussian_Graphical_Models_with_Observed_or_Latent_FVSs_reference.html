<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>154 nips-2013-Learning Gaussian Graphical Models with Observed or Latent FVSs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-154" href="../nips2013/nips-2013-Learning_Gaussian_Graphical_Models_with_Observed_or_Latent_FVSs.html">nips2013-154</a> <a title="nips-2013-154-reference" href="#">nips2013-154-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>154 nips-2013-Learning Gaussian Graphical Models with Observed or Latent FVSs</h1>
<br/><p>Source: <a title="nips-2013-154-pdf" href="http://papers.nips.cc/paper/5158-learning-gaussian-graphical-models-with-observed-or-latent-fvss.pdf">pdf</a></p><p>Author: Ying Liu, Alan Willsky</p><p>Abstract: Gaussian Graphical Models (GGMs) or Gauss Markov random ﬁelds are widely used in many applications, and the trade-off between the modeling capacity and the efﬁciency of learning and inference has been an important research problem. In this paper, we study the family of GGMs with small feedback vertex sets (FVSs), where an FVS is a set of nodes whose removal breaks all the cycles. Exact inference such as computing the marginal distributions and the partition function has complexity O(k 2 n) using message-passing algorithms, where k is the size of the FVS, and n is the total number of nodes. We propose efﬁcient structure learning algorithms for two cases: 1) All nodes are observed, which is useful in modeling social or ﬂight networks where the FVS nodes often correspond to a small number of highly inﬂuential nodes, or hubs, while the rest of the networks is modeled by a tree. Regardless of the maximum degree, without knowing the full graph structure, we can exactly compute the maximum likelihood estimate with complexity O(kn2 + n2 log n) if the FVS is known or in polynomial time if the FVS is unknown but has bounded size. 2) The FVS nodes are latent variables, where structure learning is equivalent to decomposing an inverse covariance matrix (exactly or approximately) into the sum of a tree-structured matrix and a low-rank matrix. By incorporating efﬁcient inference into the learning steps, we can obtain a learning algorithm using alternating low-rank corrections with complexity O(kn2 + n2 log n) per iteration. We perform experiments using both synthetic data as well as real data of ﬂight delays to demonstrate the modeling capacity with FVSs of various sizes. 1</p><br/>
<h2>reference text</h2><p>[1] J. Pearl, “A constraint propagation approach to probabilistic reasoning,” Proc. Uncertainty in Artiﬁcial Intell. (UAI), 1986.</p>
<p>[2] C. Chow and C. Liu, “Approximating discrete probability distributions with dependence trees,” IEEE Trans. Inform. Theory, vol. 14, no. 3, pp. 462–467, 1968.</p>
<p>[3] M. Choi, V. Chandrasekaran, and A. Willsky, “Exploiting sparse Markov and covariance structure in multiresolution models,” in Proc. 26th Annu. Int. Conf. on Machine Learning. ACM, 2009, pp. 177–184.</p>
<p>[4] M. Comer and E. Delp, “Segmentation of textured images using a multiresolution Gaussian autoregressive model,” IEEE Trans. Image Process., vol. 8, no. 3, pp. 408–420, 1999.</p>
<p>[5] C. Bouman and M. Shapiro, “A multiscale random ﬁeld model for Bayesian image segmentation,” IEEE Trans. Image Process., vol. 3, no. 2, pp. 162–177, 1994.</p>
<p>[6] D. Karger and N. Srebro, “Learning Markov networks: Maximum bounded tree-width graphs,” in Proc. 12th Annu. ACM-SIAM Symp. on Discrete Algorithms, 2001, pp. 392–401.</p>
<p>[7] M. Jordan, “Graphical models,” Statistical Sci., pp. 140–155, 2004.</p>
<p>[8] P. Abbeel, D. Koller, and A. Ng, “Learning factor graphs in polynomial time and sample complexity,” J. Machine Learning Research, vol. 7, pp. 1743–1788, 2006.</p>
<p>[9] A. Dobra, C. Hans, B. Jones, J. Nevins, G. Yao, and M. West, “Sparse graphical models for exploring gene expression data,” J. Multivariate Anal., vol. 90, no. 1, pp. 196–212, 2004.</p>
<p>[10] M. Tipping, “Sparse Bayesian learning and the relevance vector machine,” J. Machine Learning Research, vol. 1, pp. 211–244, 2001.</p>
<p>[11] J. Friedman, T. Hastie, and R. Tibshirani, “Sparse inverse covariance estimation with the graphical lasso,” Biostatistics, vol. 9, no. 3, pp. 432–441, 2008.</p>
<p>[12] P. Ravikumar, G. Raskutti, M. Wainwright, and B. Yu, “Model selection in Gaussian graphical models: High-dimensional consistency of l1-regularized MLE,” Advances in Neural Information Processing Systems (NIPS), vol. 21, 2008.</p>
<p>[13] V. Vazirani, Approximation Algorithms.  New York: Springer, 2004.</p>
<p>[14] Y. Liu, V. Chandrasekaran, A. Anandkumar, and A. Willsky, “Feedback message passing for inference in Gaussian graphical models,” IEEE Trans. Signal Process., vol. 60, no. 8, pp. 4135–4150, 2012.</p>
<p>[15] N. Friedman, D. Geiger, and M. Goldszmidt, “Bayesian network classiﬁers,” Machine learning, vol. 29, no. 2, pp. 131–163, 1997.</p>
<p>[16] V. Chandrasekaran, P. A. Parrilo, and A. S. Willsky, “Latent variable graphical model selection via convex optimization,” in Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on. IEEE, 2010, pp. 1610–1613.</p>
<p>[17] M. Dinneen, K. Cattell, and M. Fellows, “Forbidden minors to graphs with small feedback sets,” Discrete Mathematics, vol. 230, no. 1, pp. 215–252, 2001.</p>
<p>[18] F. Brandt, “Minimal stable sets in tournaments,” J. Econ. Theory, vol. 146, no. 4, pp. 1481– 1499, 2011.</p>
<p>[19] V. Bafna, P. Berman, and T. Fujito, “A 2-approximation algorithm for the undirected feedback vertex set problem,” SIAM J. Discrete Mathematics, vol. 12, p. 289, 1999.</p>
<p>[20] S. Kirshner, P. Smyth, and A. W. Robertson, “Conditional Chow-Liu tree structures for modeling discrete-valued vector time series,” in Proceedings of the 20th conference on Uncertainty in artiﬁcial intelligence. AUAI Press, 2004, pp. 317–324.</p>
<p>[21] M. J. Choi, V. Y. Tan, A. Anandkumar, and A. S. Willsky, “Learning latent tree graphical models,” Journal of Machine Learning Research, vol. 12, pp. 1729–1770, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
