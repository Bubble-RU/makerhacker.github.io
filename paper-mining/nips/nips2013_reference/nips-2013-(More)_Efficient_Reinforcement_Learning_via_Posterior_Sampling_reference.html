<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-1" href="../nips2013/nips-2013-%28More%29_Efficient_Reinforcement_Learning_via_Posterior_Sampling.html">nips2013-1</a> <a title="nips-2013-1-reference" href="#">nips2013-1-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>1 nips-2013-(More) Efficient Reinforcement Learning via Posterior Sampling</h1>
<br/><p>Source: <a title="nips-2013-1-pdf" href="http://papers.nips.cc/paper/5185-more-efficient-reinforcement-learning-via-posterior-sampling.pdf">pdf</a></p><p>Author: Ian Osband, Dan Russo, Benjamin Van Roy</p><p>Abstract: Most provably-eﬃcient reinforcement learning algorithms introduce optimism about poorly-understood states and actions to encourage exploration. We study an alternative approach for eﬃcient exploration: posterior sampling for reinforcement learning (PSRL). This algorithm proceeds in repeated episodes of known duration. At the start of each episode, PSRL updates a prior distribution over Markov decision processes and takes one sample from this posterior. PSRL then follows the policy that is optimal for this sample during the episode. The algorithm is conceptually simple, computationally eﬃcient and allows an √ agent to encode prior knowledge ˜ in a natural way. We establish an O(τ S AT ) bound on expected regret, where T is time, τ is the episode length and S and A are the cardinalities of the state and action spaces. This bound is one of the ﬁrst for an algorithm not based on optimism, and close to the state of the art for any reinforcement learning algorithm. We show through simulation that PSRL signiﬁcantly outperforms existing algorithms with similar regret bounds. 1</p><br/>
<h2>reference text</h2><p>[1] A. N. Burnetas and M. N. Katehakis. Optimal adaptive policies for markov decision processes. Mathematics of Operations Research, 22(1):222–255, 1997.</p>
<p>[2] P. R. Kumar and P. Varaiya. Stochastic systems: estimation, identiﬁcation and adaptive control. Prentice-Hall, Inc., 1986.</p>
<p>[3] T.L. Lai and H. Robbins. Asymptotically eﬃcient adaptive allocation rules. Advances in applied mathematics, 6(1):4–22, 1985.</p>
<p>[4] T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. The Journal of Machine Learning Research, 99:1563–1600, 2010.</p>
<p>[5] P. L. Bartlett and A. Tewari. Regal: A regularization based algorithm for reinforcement learning in weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, pages 35–42. AUAI Press, 2009.</p>
<p>[6] R. I. Brafman and M. Tennenholtz. R-max-a general polynomial time algorithm for nearoptimal reinforcement learning. The Journal of Machine Learning Research, 3:213–231, 2003.</p>
<p>[7] S. M. Kakade. On the sample complexity of reinforcement learning. PhD thesis, University of London, 2003.</p>
<p>[8] M. Kearns and S. Singh. Near-optimal reinforcement learning in polynomial time. Machine Learning, 49(2-3):209–232, 2002.</p>
<p>[9] W. R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933.</p>
<p>[10] O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Neural Information Processing Systems (NIPS), 2011.</p>
<p>[11] S.L. Scott. A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models in Business and Industry, 26(6):639–658, 2010.</p>
<p>[12] S. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sampling. arXiv preprint arXiv:1209.3353, 2012.</p>
<p>[13] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoﬀs. arXiv preprint arXiv:1209.3352, 2012.</p>
<p>[14] E. Kauﬀmann, N. Korda, and R. Munos. Thompson sampling: an asymptotically optimal ﬁnite time analysis. In International Conference on Algorithmic Learning Theory, 2012.</p>
<p>[15] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. CoRR, abs/1301.2609, 2013.</p>
<p>[16] M. Strens. A Bayesian framework for reinforcement learning. In Proceedings of the 17th International Conference on Machine Learning, pages 943–950, 2000.</p>
<p>[17] J. Z. Kolter and A. Y. Ng. Near-Bayesian exploration in polynomial time. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 513–520. ACM, 2009.</p>
<p>[18] T. Wang, D. Lizotte, M. Bowling, and D. Schuurmans. Bayesian sparse sampling for on-line reward optimization. In Proceedings of the 22nd international conference on Machine learning, pages 956–963. ACM, 2005.</p>
<p>[19] A. Guez, D. Silver, and P. Dayan. Eﬃcient bayes-adaptive reinforcement learning using samplebased search. arXiv preprint arXiv:1205.3109, 2012.</p>
<p>[20] J. Asmuth and M. L. Littman. Approaching bayes-optimalilty using monte-carlo tree search. In Proc. 21st Int. Conf. Automat. Plan. Sched., Freiburg, Germany, 2011.</p>
<p>[21] A. L. Strehl and M. L. Littman. An analysis of model-based interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309–1331, 2008.</p>
<p>[22] S. Filippi, O. Capp´, and A. Garivier. Optimism in reinforcement learning based on kullbacke leibler divergence. CoRR, abs/1004.5229, 2010.  9  A  Relating Bayesian to frequentist regret  Let M be any family of MDPs with non-zero probability under the prior. Then, for any > 0, α > 1: 2 P Regret(T, πτ S ) P > M∗ ∈ M → 0 Tα This provides regret bounds even if M ∗ is not distributed according to f . As long as the true MDP is not impossible under the prior, we will have an asymptotic frequentist regret close to the √ theoretical lower bounds of in T -dependence of O( T ). Proof. We have for any  > 0:  P E[Regret(T, πτ S )] Tα  ≥  P Regret(T, πτ S ) M ∗ ∈ M P (M ∗ ∈ M) Tα  E  ≥  P Regret(T, πτ S ) M ∗ ∈ M P (M ∗ ∈ M) Tα  P  1 Therefore via theorem (1), for any α > 2 : P Regret(T, πτ S ) M∗ ∈ M α T  P  B  1  ≤  P (M ∗  ∈ M)  E[Regret(T, π P Sτ )] →0 Tα  Bounding the sum of conﬁdence set widths  We are interested in bounding min{τ O(τ S  AT log(SAT ) for βk (s, a) :=  m τ min{βk stk +i , atk +i ), 1}, T } k=1 i=1 14S log(2SAmtk ) . max{1,Ntk (s,a)}  which we claim is  Proof. In a manner similar to [4] we can say: m  τ  14S log(2SAmtk ) max{1, Ntk (s, a)}  k=1 i=1  m  τ  m  ≤  τ  14S log(2SAmtk ) max{1, Ntk (s, a)}  1{Ntk >τ }  1{Ntk ≤τ } + k=1 i=1  k=1 i=1  Now, the consider the event (st , at ) = (s, a) and (Ntk (s, a) ≤ τ ). This can happen fewer than m τ 2τ times per state action pair. Therefore, 1(Ntk (s, a) ≤ τ ) ≤ 2τ SA.Now, suppose k=1 i=1 Ntk (s, a) > τ . Then for any t ∈ {tk , .., tk+1 − 1}, Nt (s, a) + 1 ≤ Ntk (s, a) + τ ≤ 2Ntk (s, a). Therefore: m tk+1 −1  k=1  t=tk  1(Ntk (st , at ) > τ ) Ntk (st , at )  m tk+1 −1  √ 2 = 2 Nt (st , at ) + 1  ≤ k=1  ≤  t=tk  NT +1 (s,a)  √ 2  j −1/2 ≤ s,a  ≤  (Nt (st , at ) + 1)−1/2 t=1 NT +1 (s,a)  √ 2  j=1  T  s,a  x−1/2 dx  x=0  √ NT +1 (s, a) =  2SA  2SAT  s,a  Note that since all rewards and transitions are absolutely constrained ∈ [0, 1] our regret m  τ  ≤  min{2τ 2 SA + τ  ≤  min{βk (stk +i , atk +i ), 1}, T }  min{τ  √ 2τ 2 SAT + τ  k=1 i=1  Which is our required result.  10  28S 2 AT log(SAT ), T } 28S 2 AT log(SAT ) ≤ τ S  30AT log(SAT )</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
