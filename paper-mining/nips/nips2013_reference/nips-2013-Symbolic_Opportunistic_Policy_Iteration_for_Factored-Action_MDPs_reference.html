<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-322" href="../nips2013/nips-2013-Symbolic_Opportunistic_Policy_Iteration_for_Factored-Action_MDPs.html">nips2013-322</a> <a title="nips-2013-322-reference" href="#">nips2013-322-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>322 nips-2013-Symbolic Opportunistic Policy Iteration for Factored-Action MDPs</h1>
<br/><p>Source: <a title="nips-2013-322-pdf" href="http://papers.nips.cc/paper/5143-symbolic-opportunistic-policy-iteration-for-factored-action-mdps.pdf">pdf</a></p><p>Author: Aswin Raghavan, Roni Khardon, Alan Fern, Prasad Tadepalli</p><p>Abstract: This paper addresses the scalability of symbolic planning under uncertainty with factored states and actions. Our ﬁrst contribution is a symbolic implementation of Modiﬁed Policy Iteration (MPI) for factored actions that views policy evaluation as policy-constrained value iteration (VI). Unfortunately, a na¨ve approach ı to enforce policy constraints can lead to large memory requirements, sometimes making symbolic MPI worse than VI. We address this through our second and main contribution, symbolic Opportunistic Policy Iteration (OPI), which is a novel convergent algorithm lying between VI and MPI, that applies policy constraints if it does not increase the size of the value function representation, and otherwise performs VI backups. We also give a memory bounded version of this algorithm allowing a space-time tradeoff. Empirical results show signiﬁcantly improved scalability over state-of-the-art symbolic planners. 1</p><br/>
<h2>reference text</h2><p>[1] Jesse Hoey, Robert St-Aubin, Alan Hu, and Craig Boutilier. SPUDD: Stochastic Planning Using Decision Diagrams. In Proceedings of the Fifteenth conference on Uncertainty in Artiﬁcial Intelligence(UAI), 1999.</p>
<p>[2] Robert St-Aubin, Jesse Hoey, and Craig Boutilier. APRICODD: Approximate Policy Construction Using Decision Diagrams. Advances in Neural Information Processing Systems(NIPS), 2001.</p>
<p>[3] Scott Sanner, William Uther, and Karina Valdivia Delgado. Approximate Dynamic Programming with Afﬁne ADDs. In Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems, 2010.</p>
<p>[4] Aswin Raghavan, Saket Joshi, Alan Fern, Prasad Tadepalli, and Roni Khardon. Planning in Factored Action Spaces with Symbolic Dynamic Programming. In Twenty-Sixth AAAI Conference on Artiﬁcial Intelligence(AAAI), 2012.</p>
<p>[5] Martin L Puterman and Moon Chirl Shin. Modiﬁed Policy Iteration Algorithms for Discounted Markov Decision Problems. Management Science, 1978.</p>
<p>[6] Craig Boutilier, Richard Dearden, and Moises Goldszmidt. Exploiting Structure in Policy Construction. In International Joint Conference on Artiﬁcial Intelligence(IJCAI), 1995.</p>
<p>[7] R Iris Bahar, Erica A Frohm, Charles M Gaona, Gary D Hachtel, Enrico Macii, Abelardo Pardo, and Fabio Somenzi. Algebraic Decision Diagrams and their Applications. In Computer-Aided Design, 1993.</p>
<p>[8] Chenggang Wang and Roni Khardon. arXiv:1206.5287, 2012.  Policy Iteration for Relational MDPs.  arXiv preprint</p>
<p>[9] Dimitri P. Bertsekas and John N. Tsitsiklis. Neuro-Dynamic Programming. 1996.</p>
<p>[10] Jason Pazis and Ronald Parr. Generalized Value Functions for Large Action Sets. In Proc. of ICML, 2011.</p>
<p>[11] Scott Sanner. Relational Dynamic Inﬂuence Diagram Language (RDDL): Language Description. Unpublished ms. Australian National University, 2010.</p>
<p>[12] Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent Planning with Factored MDPs. Advances in Neural Information Processing Systems(NIPS), 2001.</p>
<p>[13] Bruno Scherrer, Victor Gabillon, Mohammad Ghavamzadeh, and Matthieu Geist. Approximate Modiﬁed Policy Iteration. In ICML, 2012.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
