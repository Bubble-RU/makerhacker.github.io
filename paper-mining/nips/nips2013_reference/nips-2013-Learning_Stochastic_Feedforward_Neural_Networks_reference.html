<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>160 nips-2013-Learning Stochastic Feedforward Neural Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-160" href="../nips2013/nips-2013-Learning_Stochastic_Feedforward_Neural_Networks.html">nips2013-160</a> <a title="nips-2013-160-reference" href="#">nips2013-160-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>160 nips-2013-Learning Stochastic Feedforward Neural Networks</h1>
<br/><p>Source: <a title="nips-2013-160-pdf" href="http://papers.nips.cc/paper/5026-learning-stochastic-feedforward-neural-networks.pdf">pdf</a></p><p>Author: Yichuan Tang, Ruslan Salakhutdinov</p><p>Abstract: Multilayer perceptrons (MLPs) or neural networks are popular models used for nonlinear regression and classiﬁcation tasks. As regressors, MLPs model the conditional distribution of the predictor variables Y given the input variables X. However, this predictive distribution is assumed to be unimodal (e.g. Gaussian). For tasks involving structured prediction, the conditional distribution should be multi-modal, resulting in one-to-many mappings. By using stochastic hidden variables rather than deterministic ones, Sigmoid Belief Nets (SBNs) can induce a rich multimodal distribution in the output space. However, previously proposed learning algorithms for SBNs are not efﬁcient and unsuitable for modeling real-valued data. In this paper, we propose a stochastic feedforward network with hidden layers composed of both deterministic and stochastic variables. A new Generalized EM training procedure using importance sampling allows us to efﬁciently learn complicated conditional distributions. Our model achieves superior performance on synthetic and facial expressions datasets compared to conditional Restricted Boltzmann Machines and Mixture Density Networks. In addition, the latent features of our model improves classiﬁcation and can learn to generate colorful textures of objects. 1</p><br/>
<h2>reference text</h2><p>[1] C. M. Bishop. Mixture density networks. Technical Report NCRG/94/004, Aston University, 1994.</p>
<p>[2] R. M. Neal. Connectionist learning of belief networks. volume 56, pages 71–113, July 1992.</p>
<p>[3] R. M. Neal. Learning stochastic feedforward networks. Technical report, University of Toronto, 1990.</p>
<p>[4] Lawrence K. Saul, Tommi Jaakkola, and Michael I. Jordan. Mean ﬁeld theory for sigmoid belief networks. Journal of Artiﬁcial Intelligence Research, 4:61–76, 1996.</p>
<p>[5] David Barber and Peter Sollich. Gaussian ﬁelds for approximate inference in layered sigmoid belief networks. In Sara A. Solla, Todd K. Leen, and Klaus-Robert M¨ ller, editors, NIPS, pages 393–399. The u MIT Press, 1999.</p>
<p>[6] G. Taylor, G. E. Hinton, and S. Roweis. Modeling human motion using binary latent variables. In NIPS, 2006.</p>
<p>[7] Carl Edward Rasmussen. Gaussian processes for machine learning. MIT Press, 2006.</p>
<p>[8] H. Rue and L. Held. Gaussian Markov Random Fields: Theory and Applications, volume 104 of Monographs on Statistics and Applied Probability. Chapman & Hall, London, 2005.</p>
<p>[9] John Lafferty. Conditional random ﬁelds: Probabilistic models for segmenting and labeling sequence data. pages 282–289. Morgan Kaufmann, 2001.</p>
<p>[10] Volodymyr Mnih, Hugo Larochelle, and Geoffrey Hinton. Conditional restricted boltzmann machines for structured output prediction. In Proceedings of the International Conference on Uncertainty in Artiﬁcial Intelligence, 2011.</p>
<p>[11] Yujia Li, Daniel Tarlow, and Richard Zemel. Exploring compositional high order pattern potentials for structured output learning. In Proceedings of International Conference on Computer Vision and Pattern Recognition, 2013.</p>
<p>[12] R. M. Neal and G. E. Hinton. A new view of the EM algorithm that justiﬁes incremental, sparse and other variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355–368. 1998.</p>
<p>[13] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14:1771–1800, 2002.</p>
<p>[14] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11:125–139, 2001.</p>
<p>[15] R. Salakhutdinov and I. Murray. On the quantitative analysis of deep belief networks. In Proceedings of the Intl. Conf. on Machine Learning, volume 25, 2008.</p>
<p>[16] J.M. Susskind. The Toronto Face Database. Technical report, 2011. http://aclab.ca/users/josh/TFD.html.</p>
<p>[17] Zoubin Ghahramani and G. E. Hinton. The EM algorithm for mixtures of factor analyzers. Technical Report CRG-TR-96-1, University of Toronto, 1996.</p>
<p>[18] Ian Nabney. NETLAB: algorithms for pattern recognitions. Advances in pattern recognition. SpringerVerlag, 2002.</p>
<p>[19] V. Nair and G. E. Hinton. 3-D object recognition with deep belief nets. In NIPS 22, 2009.</p>
<p>[20] J. M. Geusebroek, G. J. Burghouts, and A. W. M. Smeulders. The amsterdam library of object images. International Journal of Computer Vision, 61(1), January 2005.</p>
<p>[21] Eran Borenstein and Shimon Ullman. Class-speciﬁc, top-down segmentation. In In ECCV, pages 109– 124, 2002.</p>
<p>[22] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal. The wake-sleep algorithm for unsupervised neural networks. Science, 268(5214):1158–1161, 1995.</p>
<p>[23] R. Salakhutdinov and H. Larochelle. Efﬁcient learning of deep boltzmann machines. AISTATS, 2010.  9</p>
<br/>
<br/><br/><br/></body>
</html>
