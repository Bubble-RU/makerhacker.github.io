<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-56" href="../nips2013/nips-2013-Better_Approximation_and_Faster_Algorithm_Using_the_Proximal_Average.html">nips2013-56</a> <a title="nips-2013-56-reference" href="#">nips2013-56-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>56 nips-2013-Better Approximation and Faster Algorithm Using the Proximal Average</h1>
<br/><p>Source: <a title="nips-2013-56-pdf" href="http://papers.nips.cc/paper/4934-better-approximation-and-faster-algorithm-using-the-proximal-average.pdf">pdf</a></p><p>Author: Yao-Liang Yu</p><p>Abstract: It is a common practice to approximate “complicated” functions with more friendly ones. In large-scale machine learning applications, nonsmooth losses/regularizers that entail great computational challenges are usually approximated by smooth functions. We re-examine this powerful methodology and point out a nonsmooth approximation which simply pretends the linearity of the proximal map. The new approximation is justiﬁed using a recent convex analysis tool— proximal average, and yields a novel proximal gradient algorithm that is strictly better than the one based on smoothing, without incurring any extra overhead. Numerical experiments conducted on two important applications, overlapping group lasso and graph-guided fused lasso, corroborate the theoretical claims. 1</p><br/>
<h2>reference text</h2><p>[1] Walter Rudin. Principles of mathematical analysis. McGraw-Hill, 3rd edition, 1976.</p>
<p>[2] Amir Beck and Marc Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.</p>
<p>[3] Yurii Nesterov. Gradient methods for minimizing composite functions. Mathematical Programming, Series B, 140:125–161, 2013.</p>
<p>[4] Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski. Structured sparsity through convex optimization. Statistical Science, 27(4):450–468, 2012.</p>
<p>[5] Peng Zhao, Guilherme Rocha, and Bin Yu. The composite absolute penalties family for grouped and hierarchical variable selection. Annals of Statistics, 37(6A):3468–3497, 2009.</p>
<p>[6] Seyoung Kim and Eric P. Xing. Statistical estimation of correlated genome associations to a quantitative trait network. PLoS Genetics, 5(8):1–18, 2009.</p>
<p>[7] Naum Z. Shor. Minimization Methods for Non-Differentiable Functions. Springer, 1985.</p>
<p>[8] Yurii Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1):127–152, 2005.</p>
<p>[9] Patrick L. Combettes and Jean-Christophe Pesquet. Proximal splitting methods in signal processing. In Fixed-Point Algorithms for Inverse Problems in Science and Engineering, pages 185–212. Springer, 2011.</p>
<p>[10] Silvia Villa, Saverio Salzo, Luca Baldassarre, and Alessandro Verri. Accelerated and inexact forward-backward algorithms. SIAM Journal on Optimization, 23(3):1607–1633, 2013.</p>
<p>[11] Heinz H. Bauschke, Rafal Goebel, Yves Lucet, and Xianfu Wang. The proximal average: Basic theory. SIAM Journal on Optimization, 19(2):766–785, 2008.</p>
<p>[12] Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. Sparsity and smoothness via the fused lasso. Journal of the Royal Statistical Society: Series B, 67:91–108, 2005.</p>
<p>[13] Xi Chen, Qihan Lin, Seyoung Kim, Jaime G. Carbonell, and Eric P. Xing. Smoothing proximal gradient method for general structured sparse regression. The Annals of Applied Statistics, 6 (2):719–752, 2012.</p>
<p>[14] Ralph Tyrell Rockafellar and Roger J-B Wets. Variational Analysis. Springer, 1998.</p>
<p>[15] Jean J. Moreau. Proximit´ et dualtit´ dans un espace Hilbertien. Bulletin de la Soci´ t´ e e ee Math´ matique de France, 93:273–299, 1965. e</p>
<p>[16] Ralph Tyrrell Rockafellar. Monotone operators and the proximal point algorithm. SIAM Journal on Control and Optimization, 14(5):877–898, 1976.</p>
<p>[17] Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer, 1st edition, 2011.</p>
<p>[18] Hua Ouyang, Niao He, Long Q. Tran, and Alexander Gray. Stochastic alternating direction method of multipliers. In International Conference on Machine Learning, 2013.</p>
<p>[19] Taiji Suzuki. Dual averaging and proximal gradient descent for online alternating direction multiplier method. In International Conference on Machine Learning, 2013.</p>
<p>[20] Yaoliang Yu. Fast Gradient Algorithms for Stuctured Sparsity. PhD thesis, University of Alberta, 2013.  9</p>
<br/>
<br/><br/><br/></body>
</html>
