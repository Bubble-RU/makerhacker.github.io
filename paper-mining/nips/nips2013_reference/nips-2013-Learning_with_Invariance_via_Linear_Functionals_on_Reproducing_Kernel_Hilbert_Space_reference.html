<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-170" href="../nips2013/nips-2013-Learning_with_Invariance_via_Linear_Functionals_on_Reproducing_Kernel_Hilbert_Space.html">nips2013-170</a> <a title="nips-2013-170-reference" href="#">nips2013-170-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>170 nips-2013-Learning with Invariance via Linear Functionals on Reproducing Kernel Hilbert Space</h1>
<br/><p>Source: <a title="nips-2013-170-pdf" href="http://papers.nips.cc/paper/4895-learning-with-invariance-via-linear-functionals-on-reproducing-kernel-hilbert-space.pdf">pdf</a></p><p>Author: Xinhua Zhang, Wee Sun Lee, Yee Whye Teh</p><p>Abstract: Incorporating invariance information is important for many learning problems. To exploit invariances, most existing methods resort to approximations that either lead to expensive optimization problems such as semi-deﬁnite programming, or rely on separation oracles to retain tractability. Some methods further limit the space of functions and settle for non-convex models. In this paper, we propose a framework for learning in reproducing kernel Hilbert spaces (RKHS) using local invariances that explicitly characterize the behavior of the target function around data instances. These invariances are compactly encoded as linear functionals whose value are penalized by some loss function. Based on a representer theorem that we establish, our formulation can be efﬁciently optimized via a convex program. For the representer theorem to hold, the linear functionals are required to be bounded in the RKHS, and we show that this is true for a variety of commonly used RKHS and invariances. Experiments on learning with unlabeled data and transform invariances show that the proposed method yields better or similar results compared with the state of the art. 1</p><br/>
<h2>reference text</h2><p>[1] G. E. Hinton. Learning translation invariant recognition in massively parallel networks. In Proceedings Conference on Parallel Architectures and Laguages Europe, pages 1–13. Springer, 1987.</p>
<p>[2] M. Ferraro and T. M. Caelli. Lie transformation groups, integral transforms, and invariant pattern recognition. Spatial Vision, 8:33–44, 1994.</p>
<p>[3] P. Simard, Y. LeCun, J. S. Denker, and B. Victorri. Transformation invariance in pattern recognitiontangent distance and tangent propagation. In Neural Networks: Tricks of the Trade, pages 239–274, 1996.</p>
<p>[4] O. Chapelle, B. Sch¨ lkopf, and A. Zien, editors. Semi-Supervised Learning. MIT Press, 2006. o</p>
<p>[5] S. Ben-David, T. Lu, and D. Pal. Does unlabeled data provably help? Worst-case analysis of the sample complexity of semi-supervised learning. In COLT, 2008.</p>
<p>[6] T. Zhang and F. J. Oles. A probability analysis on the value of unlabeled data for classiﬁcation problems. In ICML, 2000.</p>
<p>[7] O. Bousquet, O. Chapelle, and M. Hein. Measure based regularization. In NIPS, 2003.</p>
<p>[8] T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In ICML, 1999.</p>
<p>[9] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In ICML, 2001.</p>
<p>[10] M. Belkin, P. Niyogi, and V. Sindhwani. On manifold regularization. In AI-Stats, 2005.</p>
<p>[11] X. Zhu, Z. Ghahramani, and J. D. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In ICML, 2003.</p>
<p>[12] D. Zhou and B. Sch¨ lkopf. Discrete regularization. In Semi-Supervised Learning, pages 221–232. MIT o Press, 2006.</p>
<p>[13] H. Xu, C. Caramanis, and S. Mannor. Robustness and regularization of support vector machines. Journal of Machine Learning Research, 10:3589–3646, 2009.</p>
<p>[14] A. Ben-Tal, L. El Ghaoui, and A. Nemirovski. Robust Optimization. Princeton University Press, 2008.</p>
<p>[15] C. Bhattacharyya, K. S. Pannagadatta, and A. J. Smola. A second order cone programming formulation for classifying missing data. In NIPS, 2005.</p>
<p>[16] A. Globerson and S. Roweis. Nightmare at test time: Robust learning by feature deletion. In ICML, 2006.</p>
<p>[17] N. Dalvi, P. Domingos, Mausam, S. Sanghai, and D. Verma. Adversarial classiﬁcation. In KDD, 2004.</p>
<p>[18] T. Graepel and R. Herbrich. Invariant pattern recognition by semideﬁnite programming machines. In NIPS, 2004.</p>
<p>[19] C. H. Teo, A. Globerson, S. Roweis, and A. Smola. Convex learning with invariances. In NIPS, 2007.</p>
<p>[20] D. DeCoste and B. Sch¨ lkopf. Training invariant support vector machines. Machine Learning, 46:161– o 190, 2002.</p>
<p>[21] O. Chapelle and B. Sch¨ lkopf. Incorporating invariances in nonlinear support vector machines. In NIPS, o 2001.</p>
<p>[22] G. Wahba. An introduction to model building with reproducing kernel Hilbert spaces. Technical Report TR 1020, University of Wisconsin-Madison, 2000.</p>
<p>[23] A. J. Smola and B. Sch¨ lkopf. On a kernel-based method for pattern recognition, regression, approximao tion and operator inversion. Algorithmica, 22:211–231, 1998.</p>
<p>[24] C. Walder and O. Chapelle. Learning with transformation invariant kernels. In NIPS, 2007.</p>
<p>[25] C. Burges. Geometry and invariance in kernel based methods. In B. Sch¨ lkopf, C. Burges, and A. Smola, o editors, Advances in Kernel Methods — Support Vector Learning, pages 89–116. MIT Press, 1999.</p>
<p>[26] B. Sch¨ lkopf and A. Smola. Learning with Kernels. MIT Press, 2001. o</p>
<p>[27] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines and Other Kernel-based Learning Methods. Cambridge University Press, Cambridge, UK, 2000.</p>
<p>[28] I. Steinwart and A. Christmann. Support Vector Machines. Information Science and Statistics. Springer, 2008.</p>
<p>[29] E. Kreyszig. Introductory Functional Analysis with Applications. Wiley, 1989.</p>
<p>[30] S. Shalev-Shwartz, Y. Singer, and N. Srebro. Pegasos: Primal estimated sub-gradient solver for SVM. In ICML, 2007.</p>
<p>[31] C. H. Teo, S. V. N. Vishwanthan, A. J. Smola, and Q. V. Le. Bundle methods for regularized risk minimization. Journal of Machine Learning Research, 11:311–365, January 2010.</p>
<p>[32] Y. Nesterov. Efﬁciency of coordinate descent methods on huge-scale optimization problems. SIAM Journal on Optimization, 22(2):341–362, 2012.</p>
<p>[33] O. Chapelle. Training a support vector machine in the primal. Neural Comput., 19(5):1155–1178, 2007.</p>
<p>[34] http://www.cs.ubc.ca/∼pcarbo/lbfgsb-for-matlab.html.</p>
<p>[35] K. Bache and M. Lichman. UCI machine learning repository, 2013. University of California, Irvine.</p>
<p>[36] http://www.dii.unisi.it/∼melacci/lapsvmp.</p>
<p>[37] V. Sindhwani, P. Niyogi, and M. Belkin. Beyond the point cloud: from transductive to semi-supervised learning. In ICML, 2005.</p>
<p>[38] http://www.cs.nyu.edu/∼roweis/data.html.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
