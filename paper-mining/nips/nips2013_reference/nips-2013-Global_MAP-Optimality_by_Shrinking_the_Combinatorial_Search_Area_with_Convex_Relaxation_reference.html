<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>132 nips-2013-Global MAP-Optimality by Shrinking the Combinatorial Search Area with Convex Relaxation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-132" href="../nips2013/nips-2013-Global_MAP-Optimality_by_Shrinking_the_Combinatorial_Search_Area_with_Convex_Relaxation.html">nips2013-132</a> <a title="nips-2013-132-reference" href="#">nips2013-132-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>132 nips-2013-Global MAP-Optimality by Shrinking the Combinatorial Search Area with Convex Relaxation</h1>
<br/><p>Source: <a title="nips-2013-132-pdf" href="http://papers.nips.cc/paper/5159-global-map-optimality-by-shrinking-the-combinatorial-search-area-with-convex-relaxation.pdf">pdf</a></p><p>Author: Bogdan Savchynskyy, Jörg Hendrik Kappes, Paul Swoboda, Christoph Schnörr</p><p>Abstract: We consider energy minimization for undirected graphical models, also known as the MAP-inference problem for Markov random ﬁelds. Although combinatorial methods, which return a provably optimal integral solution of the problem, made a signiﬁcant progress in the past decade, they are still typically unable to cope with large-scale datasets. On the other hand, large scale datasets are often deﬁned on sparse graphs and convex relaxation methods, such as linear programming relaxations then provide good approximations to integral solutions. We propose a novel method of combining combinatorial and convex programming techniques to obtain a global solution of the initial combinatorial problem. Based on the information obtained from the solution of the convex relaxation, our method conﬁnes application of the combinatorial solver to a small fraction of the initial graphical model, which allows to optimally solve much larger problems. We demonstrate the efﬁcacy of our approach on a computer vision energy minimization benchmark. 1</p><br/>
<h2>reference text</h2><p>[1] D. Koller and N. Friedman. Probabilistic Graphical Models:Principles and Techniques. MIT Press, 2009.</p>
<p>[2] T. Werner. A linear programming approach to max-sum problem: A review. IEEE Trans. on PAMI, 29(7), July 2007.</p>
<p>[3] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Found. Trends Mach. Learn., 1(1-2):1–305, 2008.</p>
<p>[4] M. Schlesinger. Syntactic analysis of two-dimensional visual signals in the presence of noise. Kibernetika, (4):113–130, 1976.</p>
<p>[5] M. Wainwright, T. Jaakkola, and A. Willsky. MAP estimation via agreement on (hyper)trees: message passing and linear programming approaches. IEEE Trans. on Inf. Th., 51(11), 2005.</p>
<p>[6] N. Komodakis, N. Paragios, and G. Tziritas. MRF energy minimization and beyond via dual decomposition. IEEE Trans. on PAMI, 33(3):531 –552, march 2011.</p>
<p>[7] B. Savchynskyy, J. H. Kappes, S. Schmidt, and C. Schn¨ rr. A study of Nesterov’s scheme for Lagrangian o decomposition and MAP labeling. In CVPR 2011, 2011.  8</p>
<p>[8] S. Schmidt, B. Savchynskyy, J. H. Kappes, and C. Schn¨ rr. Evaluation of a ﬁrst-order primal-dual algoo rithm for MRF energy minimization. In EMMCVPR, pages 89–103, 2011.</p>
<p>[9] O. Meshi and A. Globerson. An alternating direction method for dual MAP LP relaxation. ECML/PKDD (2), pages 470–483, 2011.  In</p>
<p>[10] A. F. T. Martins, M. A. T. Figueiredo, P. M. Q. Aguiar, N. A. Smith, and E. P. Xing. An augmented Lagrangian approach to constrained MAP inference. In ICML, 2011.</p>
<p>[11] B. Savchynskyy, S. Schmidt, J. H. Kappes, and C. Schn¨ rr. Efﬁcient MRF energy minimization via o adaptive diminishing smoothing. In UAI-2012, pages 746–755.</p>
<p>[12] D. V. N. Luong, P. Parpas, D. Rueckert, and B. Rustem. Solving MRF minimization by mirror descent. In Advances in Visual Computing, volume 7431, pages 587–598. Springer Berlin Heidelberg, 2012.</p>
<p>[13] J. H. Kappes, B. Savchynskyy, and C. Schn¨ rr. A bundle approach to efﬁcient MAP-inference by Lao grangian relaxation. In CVPR 2012, 2012.</p>
<p>[14] B. Savchynskyy and S. Schmidt. Getting feasible variable estimates from infeasible ones: MRF local polytope study. Technical report, arXiv:1210.4081, 2012.</p>
<p>[15] J. H. Kappes, B. Andres, F. A. Hamprecht, C. Schn¨ rr, S. Nowozin, D. Batra, S. Kim, B. X. Kausler, o J. Lellmann, N. Komodakis, and C. Rother. A comparative study of modern inference techniques for discrete energy minimization problems. In CVPR, 2013.</p>
<p>[16] V. Kolmogorov. Convergent tree-reweighted message passing for energy minimization. IEEE Trans. on PAMI, 28(10):1568–1583, 2006.</p>
<p>[17] A. Globerson and T. Jaakkola. Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. In NIPS, 2007.</p>
<p>[18] T. Hazan and A. Shashua. Norm-product belief propagation: Primal-dual message-passing for approximate inference. IEEE Trans. on Inf. Theory,, 56(12):6294 –6316, 2010.</p>
<p>[19] M. I. Schlesinger and K. V. Antoniuk. Diffusion algorithms and structural recognition optimization problems. Cybernetics and Systems Analysis, 47(2):175–192, 2011.</p>
<p>[20] V. Franc, S. Sonnenburg, and T. Werner. Cutting-Plane Methods in Machine Learning, chapter 7, pages 185–218. The MIT Press, Cambridge,USA, 2012.</p>
<p>[21] J. H. Kappes, M. Speth, B. Andres, G. Reinelt, and C. Schn¨ rr. Globally optimal image partitioning by o multicuts. In EMMCVPR, 2011.</p>
<p>[22] M. Bergtholdt, J. H. Kappes, S. Schmidt, and C. Schn¨ rr. A study of parts-based object class detection o using complete graphs. IJCV, 87(1-2):93–117, 2010.</p>
<p>[23] M. Sun, M. Telaprolu, H. Lee, and S. Savarese. Efﬁcient and exact MAP-MRF inference using branch and bound. In AISTATS-2012.</p>
<p>[24] L. Otten and R. Dechter. Anytime AND/OR depth-ﬁrst search for combinatorial optimization. In Proceedings of the Annual Symposium on Combinatorial Search (SOCS), 2011.</p>
<p>[25] M. C. Cooper, S. de Givry, M. Sanchez, T. Schiex, M. Zytnicki, and T. Werner. Soft arc consistency revisited. Artiﬁcial Intelligence, 174(7-8):449–478, May 2010.</p>
<p>[26] R. Szeliski, R. Zabih, D. Scharstein, O. Veksler, V. Kolmogorov, A. Agarwala, M. Tappen, and C. Rother. A comparative study of energy minimization methods for Markov random ﬁelds with smoothness-based priors. IEEE Trans. PAMI., 30:1068–1080, June 2008.</p>
<p>[27] ILOG, Inc. ILOG CPLEX: High-performance software for mathematical programming and optimization. See http://www.ilog.com/products/cplex/.</p>
<p>[28] B. Andres, T. Beier, and J. H. Kappes. OpenGM: A C++ library for discrete graphical models. ArXiv e-prints, 2012. Projectpage: http://hci.iwr.uni-heidelberg.de/opengm2/.</p>
<p>[29] I. Kovtun. Partial optimal labeling search for a NP-hard subclass of (max, +) problems. In Proceedings of the DAGM Symposium, 2003.</p>
<p>[30] J. H. Kappes, M. Speth, G. Reinelt, and C. Schn¨ rr. Towards efﬁcient and exact MAP-inference for large o scale discrete computer vision problems via combinatorial optimization. In CVPR, 2013.</p>
<p>[31] S. Chopra and M. R. Rao. On the multiway cut polyhedron. Networks, 21(1):51–89, 1991.</p>
<p>[32] P. Swoboda, B. Savchynskyy, J. H. Kappes, and C. Schn¨ rr. Partial optimality via iterative pruning for o the Potts model. In SSVM, 2013.</p>
<p>[33] D. Sontag, T. Meltzer, A. Globerson, Y. Weiss, and T. Jaakkola. Tightening LP relaxations for MAP using message-passing. In UAI-2008, pages 503–510.</p>
<p>[34] D. Sontag. C++ code for MAP inference in graphical models. ˜dsontag/code/mplp_ver2.tgz.  9  See http://cs.nyu.edu/</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
