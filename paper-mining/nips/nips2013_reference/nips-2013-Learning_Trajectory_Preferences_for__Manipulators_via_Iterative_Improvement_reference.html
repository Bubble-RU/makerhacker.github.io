<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>162 nips-2013-Learning Trajectory Preferences for  Manipulators via Iterative Improvement</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-162" href="../nips2013/nips-2013-Learning_Trajectory_Preferences_for__Manipulators_via_Iterative_Improvement.html">nips2013-162</a> <a title="nips-2013-162-reference" href="#">nips2013-162-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>162 nips-2013-Learning Trajectory Preferences for  Manipulators via Iterative Improvement</h1>
<br/><p>Source: <a title="nips-2013-162-pdf" href="http://papers.nips.cc/paper/5179-learning-trajectory-preferences-for-manipulators-via-iterative-improvement.pdf">pdf</a></p><p>Author: Ashesh Jain, Brian Wojcik, Thorsten Joachims, Ashutosh Saxena</p><p>Abstract: We consider the problem of learning good trajectories for manipulation tasks. This is challenging because the criterion deﬁning a good trajectory varies with users, tasks and environments. In this paper, we propose a co-active online learning framework for teaching robots the preferences of its users for object manipulation tasks. The key novelty of our approach lies in the type of feedback expected from the user: the human user does not need to demonstrate optimal trajectories as training data, but merely needs to iteratively provide trajectories that slightly improve over the trajectory currently proposed by the system. We argue that this co-active preference feedback can be more easily elicited from the user than demonstrations of optimal trajectories, which are often challenging and non-intuitive to provide on high degrees of freedom manipulators. Nevertheless, theoretical regret bounds of our algorithm match the asymptotic rates of optimal trajectory algorithms. We demonstrate the generalizability of our algorithm on a variety of grocery checkout tasks, for whom, the preferences were not only inﬂuenced by the object being manipulated but also by the surrounding environment.1 1</p><br/>
<h2>reference text</h2><p>[1] P. Abbeel, A. Coates, and A. Y. Ng. Autonomous helicopter aerobatics through apprenticeship learning. IJRR, 29(13), 2010.</p>
<p>[2] B. Akgun, M. Cakmak, K. Jiang, and A. L. Thomaz. Keyframe-based learning from demonstration. IJSR, 4(4):343–355, 2012.</p>
<p>[3] R. Alterovitz, T. Siméon, and K. Goldberg. The stochastic motion roadmap: A sampling framework for planning with markov motion uncertainty. In RSS, 2007.</p>
<p>[4] J. V. D. Berg, P. Abbeel, and K. Goldberg. Lqg-mp: Optimized path planning for robots with motion uncertainty and imperfect state information. In RSS, 2010.</p>
<p>[5] S. Calinon, F. Guenter, and A. Billard. On learning, representing, and generalizing a task in a humanoid robot. IEEE Transactions on Systems, Man, and Cybernetics, 2007.</p>
<p>[6] D. Dey, T. Y. Liu, M. Hebert, and J. A. Bagnell. Contextual sequence prediction with application to control library optimization. In RSS, 2012.</p>
<p>[7] R. Diankov. Automated Construction of Robotic Manipulation Programs. PhD thesis, CMU, RI, 2010.</p>
<p>[8] A. Dragan and S. Srinivasa. Generating legible motion. In RSS, 2013.</p>
<p>[9] C. J. Green and A. Kelly. Toward optimal sampling in the space of paths. In ISRR. 2007.</p>
<p>[10] Y. Jiang, M. Lim, and A. Saxena. Learning object arrangements in 3d scenes using human context. In ICML, 2012.</p>
<p>[11] Y. Jiang, M. Lim, C. Zheng, and A. Saxena. Learning to place new objects in a scene. IJRR, 31(9), 2012.</p>
<p>[12] Y. Jiang, H. Koppula, and A. Saxena. Hallucinated humans as the hidden context for labeling 3d scenes. In CVPR, 2013.</p>
<p>[13] T. Joachims. Training linear svms in linear time. In KDD, 2006.</p>
<p>[14] T. Joachims, T. Finley, and C. Yu. Cutting-plane training of structural svms. Mach Learn, 77(1), 2009.</p>
<p>[15] S. Karaman and E. Frazzoli. Incremental sampling-based algorithms for optimal motion planning. In RSS, 2010.</p>
<p>[16] E. Klingbeil, D. Rao, B. Carpenter, V. Ganapathi, A. Y. Ng, and O. Khatib. Grasping with application to an autonomous checkout robot. In ICRA, 2011.</p>
<p>[17] J. Kober and J. Peters. Policy search for motor primitives in robotics. Machine Learning, 84(1), 2011.</p>
<p>[18] H. S. Koppula and A. Saxena. Anticipating human activities using object affordances for reactive robotic response. In RSS, 2013.</p>
<p>[19] H. S. Koppula, A. Anand, T. Joachims, and A. Saxena. Semantic labeling of 3d point clouds for indoor scenes. In NIPS, 2011.</p>
<p>[20] S. M. LaValle and J. J. Kuffner. Randomized kinodynamic planning. IJRR, 20(5):378–400, 2001.</p>
<p>[21] I. Lenz, H. Lee, and A. Saxena. Deep learning for detecting robotic grasps. In RSS, 2013.</p>
<p>[22] S. Levine and V. Koltun. Continuous inverse optimal control with locally optimal examples. In ICML, 2012.</p>
<p>[23] J. Mainprice, E. A. Sisbot, L. Jaillet, J. Cortés, R. Alami, and T. Siméon. Planning human-aware motions using a sampling-based costmap planner. In ICRA, 2011.</p>
<p>[24] C. D. Manning, P. Raghavan, and H. Schütze. Introduction to information retrieval, volume 1. Cambridge University Press Cambridge, 2008.</p>
<p>[25] N. Ratliff. Learning to Search: Structured Prediction Techniques for Imitation Learning. PhD thesis, CMU, RI, 2009.</p>
<p>[26] N. Ratliff, J. A. Bagnell, and M. Zinkevich. Maximum margin planning. In ICML, 2006.</p>
<p>[27] N. Ratliff, D. Bradley, J. A. Bagnell, and J. Chestnutt. Boosting structured prediction for imitation learning. In NIPS, 2007.</p>
<p>[28] N. Ratliff, D. Silver, and J. A. Bagnell. Learning to search: Functional gradient techniques for imitation learning. Autonomous Robots, 27(1):25–53, 2009.</p>
<p>[29] N. Ratliff, M. Zucker, J. A. Bagnell, and S. Srinivasa. Chomp: Gradient optimization techniques for efﬁcient motion planning. In ICRA, 2009.</p>
<p>[30] A. Saxena, J. Driemeyer, and A.Y. Ng. Robotic grasping of novel objects using vision. IJRR, 27(2), 2008.</p>
<p>[31] P. Shivaswamy and T. Joachims. Online structured prediction via coactive learning. In ICML, 2012.</p>
<p>[32] B. Shneiderman and C. Plaisant. Designing The User Interface: Strategies for Effective Human-Computer Interaction. Addison-Wesley Publication, 2010.</p>
<p>[33] E. A. Sisbot, L. F. Marin, and R. Alami. Spatial reasoning for human robot interaction. In IROS, 2007.</p>
<p>[34] E. A. Sisbot, L. F. Marin-Urias, R. Alami, and T. Simeon. A human aware mobile robot motion planner. IEEE Transactions on Robotics, 2007.</p>
<p>[35] I. A. Sucan, M. Moll, and L. E. Kavraki. The Open Motion Planning Library. IEEE Robotics & Automation Magazine, 19(4):72–82, 2012. http://ompl.kavrakilab.org.</p>
<p>[36] P. Vernaza and J. A. Bagnell. Efﬁcient high dimensional maximum entropy modeling via symmetric partition functions. In NIPS, 2012.</p>
<p>[37] A. Wilson, A. Fern, and P. Tadepalli. A bayesian approach for policy learning from trajectory preference queries. In NIPS, 2012.</p>
<p>[38] F. Zacharias, C. Schlette, F. Schmidt, C. Borst, J. Rossmann, and G. Hirzinger. Making planned paths look more human-like in humanoid robot manipulation planning. In ICRA, 2011.</p>
<p>[39] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In AAAI, 2008.  9</p>
<br/>
<br/><br/><br/></body>
</html>
