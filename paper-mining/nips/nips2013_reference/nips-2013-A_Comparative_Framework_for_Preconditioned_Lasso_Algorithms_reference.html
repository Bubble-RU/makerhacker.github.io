<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-4" href="../nips2013/nips-2013-A_Comparative_Framework_for_Preconditioned_Lasso_Algorithms.html">nips2013-4</a> <a title="nips-2013-4-reference" href="#">nips2013-4-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>4 nips-2013-A Comparative Framework for Preconditioned Lasso Algorithms</h1>
<br/><p>Source: <a title="nips-2013-4-pdf" href="http://papers.nips.cc/paper/5104-a-comparative-framework-for-preconditioned-lasso-algorithms.pdf">pdf</a></p><p>Author: Fabian L. Wauthier, Nebojsa Jojic, Michael Jordan</p><p>Abstract: The Lasso is a cornerstone of modern multivariate data analysis, yet its performance suffers in the common situation in which covariates are correlated. This limitation has led to a growing number of Preconditioned Lasso algorithms that pre-multiply X and y by matrices PX , Py prior to running the standard Lasso. A direct comparison of these and similar Lasso-style algorithms to the original Lasso is difﬁcult because the performance of all of these methods depends critically on an auxiliary penalty parameter λ. In this paper we propose an agnostic framework for comparing Preconditioned Lasso algorithms to the Lasso without having to choose λ. We apply our framework to three Preconditioned Lasso instances and highlight cases when they will outperform the Lasso. Additionally, our theory reveals fragilities of these algorithms to which we provide partial solutions. 1</p><br/>
<h2>reference text</h2><p>[1] D.L. Donoho, M. Elad, and V.N. Temlyakov. Stable recovery of sparse overcomplete representations in the presence of noise. Information Theory, IEEE Transactions on, 52(1):6–18, 2006.</p>
<p>[2] J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. Journal of the American Statistical Association, 96:1348–1360, 2001.</p>
<p>[3] J.J. Fuchs. Recovery of exact sparse representations in the presence of bounded noise. Information Theory, IEEE Transactions on, 51(10):3601–3608, 2005.</p>
<p>[4] H.-C. Huang, N.-J. Hsu, D.M. Theobald, and F.J. Breidt. Spatial Lasso with applications to GIS model selection. Journal of Computational and Graphical Statistics, 19(4):963–983, 2010.</p>
<p>[5] J.C. Huang and N. Jojic. Variable selection through Correlation Sifting. In V. Bafna and S.C. Sahinalp, editors, RECOMB, volume 6577 of Lecture Notes in Computer Science, pages 106–123. Springer, 2011.</p>
<p>[6] J. Jia and K. Rohe. “Preconditioning” to comply with the irrepresentable condition. 2012.</p>
<p>[7] N. Meinshausen. Lasso with relaxation. Technical Report 129, Eidgen¨ ssische Technische o Hochschule, Z¨ rich, 2005. u</p>
<p>[8] N. Meinshausen and P. B¨ hlmann. High-dimensional graphs and variable selection with the u Lasso. Annals of Statistics, 34(3):1436–1462, 2006.</p>
<p>[9] D. Paul, E. Bair, T. Hastie, and R. Tibshirani. “Preconditioning” for feature selection and regression in high-dimensional problems. Annals of Statistics, 36(4):1595–1618, 2008.</p>
<p>[10] R. Tibshirani. Regression shrinkage and selection via the Lasso. Journal of the Royal Statistical Society, Series B, 58(1):267–288, 1994.</p>
<p>[11] R.J. Tibshirani. The solution path of the Generalized Lasso. Stanford University, 2011.</p>
<p>[12] M.J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using 1 -constrained quadratic programming (Lasso). IEEE Transactions on Information Theory, 55(5):2183–2202, 2009.</p>
<p>[13] P. Zhao and B. Yu. On model selection consistency of Lasso. Journal of Machine Learning Research, 7:2541–2563, 2006.</p>
<p>[14] H. Zou. The Adaptive Lasso and its oracle properties. Journal of the American Statistical Association, 101:1418–1429, 2006.</p>
<p>[15] H. Zou and T. Hastie. Regularization and variable selection via the Elastic Net. Journal of the Royal Statistical Society, Series B, 67:301–320, 2005.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
