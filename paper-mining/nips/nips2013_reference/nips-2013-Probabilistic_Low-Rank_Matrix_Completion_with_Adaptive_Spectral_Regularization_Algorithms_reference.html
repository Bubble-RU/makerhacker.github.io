<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-254" href="../nips2013/nips-2013-Probabilistic_Low-Rank_Matrix_Completion_with_Adaptive_Spectral_Regularization_Algorithms.html">nips2013-254</a> <a title="nips-2013-254-reference" href="#">nips2013-254-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>254 nips-2013-Probabilistic Low-Rank Matrix Completion with Adaptive Spectral Regularization Algorithms</h1>
<br/><p>Source: <a title="nips-2013-254-pdf" href="http://papers.nips.cc/paper/5005-probabilistic-low-rank-matrix-completion-with-adaptive-spectral-regularization-algorithms.pdf">pdf</a></p><p>Author: Adrien Todeschini, François Caron, Marie Chavent</p><p>Abstract: We propose a novel class of algorithms for low rank matrix completion. Our approach builds on novel penalty functions on the singular values of the low rank matrix. By exploiting a mixture model representation of this penalty, we show that a suitably chosen set of latent variables enables to derive an ExpectationMaximization algorithm to obtain a Maximum A Posteriori estimate of the completed low rank matrix. The resulting algorithm is an iterative soft-thresholded algorithm which iteratively adapts the shrinkage coefﬁcients associated to the singular values. The algorithm is simple to implement and can scale to large matrices. We provide numerical comparisons between our approach and recent alternatives showing the interest of the proposed approach for low rank matrix completion. 1</p><br/>
<h2>reference text</h2><p>[1] N. Srebro, J.D.M. Rennie, and T. Jaakkola. Maximum-Margin Matrix Factorization. In Advances in neural information processing systems, volume 17, pages 1329–1336. MIT Press, 2005.</p>
<p>[2] E.J. Cand` s and B. Recht. Exact matrix completion via convex optimization. Foundations of e Computational mathematics, 9(6):717–772, 2009.</p>
<p>[3] E.J. Cand` s and Y. Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):925– e 936, 2010.</p>
<p>[4] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. The Journal of Machine Learning Research, 11:2287–2322, 2010.</p>
<p>[5] M. Fazel. Matrix rank minimization with applications. PhD thesis, Stanford University, 2002.</p>
<p>[6] E.J. Cand` s, M.B. Wakin, and S.P. Boyd. Enhancing sparsity by reweighted l1 minimization. e Journal of Fourier Analysis and Applications, 14(5):877–905, 2008.</p>
<p>[7] J.F. Cai, E.J. Cand` s, and Z. Shen. A singular value thresholding algorithm for matrix complee tion. SIAM Journal on Optimization, 20(4):1956–1982, 2010.</p>
<p>[8] Anthony Lee, Francois Caron, Arnaud Doucet, and Chris Holmes. A hierarchical Bayesian framework for constructing sparsity-inducing priors. arXiv preprint arXiv:1009.1914, 2010.</p>
<p>[9] M. Fazel, H. Hindi, and S.P. Boyd. Log-det heuristic for matrix rank minimization with applications to hankel and euclidean distance matrices. In American Control Conference, 2003. Proceedings of the 2003, volume 3, pages 2156–2162. IEEE, 2003.</p>
<p>[10] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society. Series B, pages 1–38, 1977.</p>
<p>[11] S. Ga¨ffas and G. Lecu´ . Weighted algorithms for compressed sensing and matrix completion. ı e arXiv preprint arXiv:1107.1638, 2011.</p>
<p>[12] Kun Chen, Hongbo Dong, and Kung-Sik Chan. Reduced rank regression via adaptive nuclear norm penalization. Biometrika, 100(4):901–920, 2013.</p>
<p>[13] N. Srebro and T. Jaakkola. Weighted low-rank approximations. In NIPS, volume 20, page 720, 2003.</p>
<p>[14] F. Bach. Consistency of trace norm minimization. The Journal of Machine Learning Research, 9:1019–1048, 2008.</p>
<p>[15] R. M. Larsen. Lanczos bidiagonalization with partial reorthogonalization. Technical report, DAIMI PB-357, 1998.</p>
<p>[16] R. M. Larsen. Propack-software for large and sparse svd calculations. Available online. URL http://sun. stanford. edu/rmunk/PROPACK, 2004.</p>
<p>[17] J. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the 22nd international conference on Machine learning, pages 713–719. ACM, 2005.</p>
<p>[18] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. Eigentaste: A constant time collaborative ﬁltering algorithm. Information Retrieval, 4(2):133–151, 2001.</p>
<p>[19] Z. Zhang, S. Wang, D. Liu, and M. I. Jordan. EP-GIG priors and applications in Bayesian sparse learning. The Journal of Machine Learning Research, 98888:2031–2061, 2012.</p>
<p>[20] M. Seeger and G. Bouchard. Fast variational Bayesian inference for non-conjugate matrix factorization models. In Proc. of AISTATS, 2012.</p>
<p>[21] S. Nakajima, M. Sugiyama, S. D. Babacan, and R. Tomioka. Global analytic solution of fullyobserved variational Bayesian matrix factorization. Journal of Machine Learning Research, 14:1–37, 2013.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
