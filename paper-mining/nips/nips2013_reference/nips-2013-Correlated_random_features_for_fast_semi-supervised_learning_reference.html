<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>76 nips-2013-Correlated random features for fast semi-supervised learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-76" href="../nips2013/nips-2013-Correlated_random_features_for_fast_semi-supervised_learning.html">nips2013-76</a> <a title="nips-2013-76-reference" href="#">nips2013-76-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>76 nips-2013-Correlated random features for fast semi-supervised learning</h1>
<br/><p>Source: <a title="nips-2013-76-pdf" href="http://papers.nips.cc/paper/5000-correlated-random-features-for-fast-semi-supervised-learning.pdf">pdf</a></p><p>Author: Brian McWilliams, David Balduzzi, Joachim Buhmann</p><p>Abstract: This paper presents Correlated Nystr¨ m Views (XNV), a fast semi-supervised alo gorithm for regression and classiﬁcation. The algorithm draws on two main ideas. First, it generates two views consisting of computationally inexpensive random features. Second, multiview regression, using Canonical Correlation Analysis (CCA) on unlabeled data, biases the regression towards useful features. It has been shown that CCA regression can substantially reduce variance with a minimal increase in bias if the views contains accurate estimators. Recent theoretical and empirical work shows that regression with random features closely approximates kernel regression, implying that the accuracy requirement holds for random views. We show that XNV consistently outperforms a state-of-the-art algorithm for semi-supervised learning: substantially improving predictive performance and reducing the variability of performance on a wide variety of real-world datasets, whilst also reducing runtime by orders of magnitude. 1</p><br/>
<h2>reference text</h2><p>[1] Williams C, Seeger M: Using the Nystr¨ m method to speed up kernel machines. In NIPS 2001. o</p>
<p>[2] Rahimi A, Recht B: Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning. In Adv in Neural Information Processing Systems (NIPS) 2008.</p>
<p>[3] Yang T, Li YF, Mahdavi M, Jin R, Zhou ZH: Nystr¨ m Method vs Random Fourier Features: A Theoo retical and Empirical Comparison. In NIPS 2012.</p>
<p>[4] Gittens A, Mahoney MW: Revisiting the Nystr¨ m method for improved large-scale machine learning. o In ICML 2013.</p>
<p>[5] Bach F: Sharp analysis of low-rank kernel approximations. In COLT 2013.</p>
<p>[6] Rahimi A, Recht B: Random Features for Large-Scale Kernel Machines. In Adv in Neural Information Processing Systems 2007.</p>
<p>[7] Kakade S, Foster DP: Multi-view Regression Via Canonical Correlation Analysis. In Computational Learning Theory (COLT) 2007.</p>
<p>[8] Hotelling H: Relations between two sets of variates. Biometrika 1936, 28:312–377.</p>
<p>[9] Hardoon DR, Szedmak S, Shawe-Taylor J: Canonical Correlation Analysis: An Overview with Application to Learning Methods. Neural Comp 2004, 16(12):2639–2664.</p>
<p>[10] Ji M, Yang T, Lin B, Jin R, Han J: A Simple Algorithm for Semi-supervised Learning with Improved Generalization Error Bound. In ICML 2012.</p>
<p>[11] Belkin M, Niyogi P, Sindhwani V: Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. JMLR 2006, 7:2399–2434.</p>
<p>[12] Blum A, Mitchell T: Combining labeled and unlabeled data with co-training. In COLT 1998.</p>
<p>[13] Chaudhuri K, Kakade SM, Livescu K, Sridharan K: Multiview clustering via Canonical Correlation Analysis. In ICML 2009.</p>
<p>[14] McWilliams B, Montana G: Multi-view predictive partitioning in high dimensions. Statistical Analysis and Data Mining 2012, 5:304–321.</p>
<p>[15] Drineas P, Mahoney MW: On the Nystr¨ m Method for Approximating a Gram Matrix for Improved o Kernel-Based Learning. JMLR 2005, 6:2153–2175.</p>
<p>[16] Avron H, Boutsidis C, Toledo S, Zouzias A: Efﬁcient Dimensionality Reduction for Canonical Correlation Analysis. In ICML 2013.</p>
<p>[17] Hsu D, Kakade S, Zhang T: An Analysis of Random Design Linear Regression. In COLT 2012.</p>
<p>[18] Dhillon PS, Foster DP, Kakade SM, Ungar LH: A Risk Comparison of Ordinary Least Squares vs Ridge Regression. Journal of Machine Learning Research 2013, 14:1505–1511.</p>
<p>[19] Andrew G, Arora R, Bilmes J, Livescu K: Deep Canonical Correlation Analysis. In ICML 2013.</p>
<p>[20] Kumar S, Mohri M, Talwalkar A: Sampling methods for the Nystr¨ m method. JMLR 2012, 13:981– o 1006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
