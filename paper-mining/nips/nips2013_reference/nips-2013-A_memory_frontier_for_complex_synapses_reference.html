<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>15 nips-2013-A memory frontier for complex synapses</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-15" href="../nips2013/nips-2013-A_memory_frontier_for_complex_synapses.html">nips2013-15</a> <a title="nips-2013-15-reference" href="#">nips2013-15-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>15 nips-2013-A memory frontier for complex synapses</h1>
<br/><p>Source: <a title="nips-2013-15-pdf" href="http://papers.nips.cc/paper/4872-a-memory-frontier-for-complex-synapses.pdf">pdf</a></p><p>Author: Subhaneil Lahiri, Surya Ganguli</p><p>Abstract: An incredible gulf separates theoretical models of synapses, often described solely by a single scalar value denoting the size of a postsynaptic potential, from the immense complexity of molecular signaling pathways underlying real synapses. To understand the functional contribution of such molecular complexity to learning and memory, it is essential to expand our theoretical conception of a synapse from a single scalar to an entire dynamical system with many internal molecular functional states. Moreover, theoretical considerations alone demand such an expansion; network models with scalar synapses assuming ﬁnite numbers of distinguishable synaptic strengths have strikingly limited memory capacity. This raises the fundamental question, how does synaptic complexity give rise to memory? To address this, we develop new mathematical theorems elucidating the relationship between the structural organization and memory properties of complex synapses that are themselves molecular networks. Moreover, in proving such theorems, we uncover a framework, based on ﬁrst passage time theory, to impose an order on the internal states of complex synaptic models, thereby simplifying the relationship between synaptic structure and function. 1</p><br/>
<h2>reference text</h2><p>[1] J. J. Hopﬁeld, “Neural networks and physical systems with emergent collective computational abilities,” Proc. Natl. Acad. Sci. U.S.A. 79 (1982) no. 8, 2554–2558.</p>
<p>[2] D. J. Amit, H. Gutfreund, and H. Sompolinsky, “Spin-glass models of neural networks,” Phys. Rev. A 32 (Aug, 1985) 1007–1018.</p>
<p>[3] E. Gardner, “The space of interactions in neural network models,” Journal of Physics A: Mathematical and General 21 (1988) no. 1, 257.</p>
<p>[4] T. V. P. Bliss and G. L. Collingridge, “A synaptic model of memory: long-term potentiation in the hippocampus,” Nature 361 (Jan, 1993) 31–39.</p>
<p>[5] C. C. H. Petersen, R. C. Malenka, R. A. Nicoll, and J. J. Hopﬁeld, “All-or-none potentiation at CA3-CA1 synapses,” Proc. Natl. Acad. Sci. U.S.A. 95 (1998) no. 8, 4732–4737.</p>
<p>[6] D. H. O’Connor, G. M. Wittenberg, and S. S.-H. Wang, “Graded bidirectional synaptic plasticity is composed of switch-like unitary events,” Proc. Natl. Acad. Sci. U.S.A. 102 (2005) no. 27, 9679–9684.</p>
<p>[7] R. Enoki, Y. ling Hu, D. Hamilton, and A. Fine, “Expression of Long-Term Plasticity at Individual Synapses in Hippocampus Is Graded, Bidirectional, and Mainly Presynaptic: Optical Quantal Analysis,” Neuron 62 (2009) no. 2, 242 – 253.</p>
<p>[8] D. J. Amit and S. Fusi, “Constraints on learning in dynamic synapses,” Network: Computation in Neural Systems 3 (1992) no. 4, 443–464.</p>
<p>[9] D. J. Amit and S. Fusi, “Learning in neural networks with material synapses,” Neural Computation 6 (1994) no. 5, 957–982.</p>
<p>[10] S. Fusi, P. J. Drew, and L. F. Abbott, “Cascade models of synaptically stored memories,” Neuron 45 (Feb, 2005) 599–611.</p>
<p>[11] S. Fusi and L. F. Abbott, “Limits on the memory storage capacity of bounded synapses,” Nat. Neurosci. 10 (Apr, 2007) 485–493.</p>
<p>[12] C. Leibold and R. Kempter, “Sparseness Constrains the Prolongation of Memory Lifetime via Synaptic Metaplasticity,” Cerebral Cortex 18 (2008) no. 1, 67–77.</p>
<p>[13] D. S. Bredt and R. A. Nicoll, “AMPA Receptor Trafﬁcking at Excitatory Synapses,” Neuron 40 (2003) no. 2, 361 – 379.</p>
<p>[14] M. P. Coba, A. J. Pocklington, M. O. Collins, M. V. Kopanitsa, R. T. Uren, S. Swamy, M. D. Croning, J. S. Choudhary, and S. G. Grant, “Neurotransmitters drive combinatorial multistate postsynaptic density networks,” Sci Signal 2 (2009) no. 68, ra19.</p>
<p>[15] W. C. Abraham and M. F. Bear, “Metaplasticity: the plasticity of synaptic plasticity,” Trends in Neurosciences 19 (1996) no. 4, 126 – 130.</p>
<p>[16] J. M. Montgomery and D. V. Madison, “State-Dependent Heterogeneity in Synaptic Depression between Pyramidal Cell Pairs,” Neuron 33 (2002) no. 5, 765 – 777.</p>
<p>[17] R. D. Emes and S. G. Grant, “Evolution of Synapse Complexity and Diversity,” Annual Review of Neuroscience 35 (2012) no. 1, 111–131.</p>
<p>[18] A. B. Barrett and M. C. van Rossum, “Optimal learning rules for discrete synapses,” PLoS Comput. Biol. 4 (Nov, 2008) e1000230.</p>
<p>[19] J. Kemeny and J. Snell, Finite markov chains. Springer, 1960.</p>
<p>[20] C. Burke and M. Rosenblatt, “A Markovian function of a Markov chain,” The Annals of Mathematical Statistics 29 (1958) no. 4, 1112–1122.</p>
<p>[21] F. Ball and G. F. Yeo, “Lumpability and Marginalisability for Continuous-Time Markov Chains,” Journal of Applied Probability 30 (1993) no. 3, 518–528.  9</p>
<br/>
<br/><br/><br/></body>
</html>
