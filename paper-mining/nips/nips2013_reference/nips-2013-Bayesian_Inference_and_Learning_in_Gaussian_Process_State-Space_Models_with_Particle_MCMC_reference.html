<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-48" href="../nips2013/nips-2013-Bayesian_Inference_and_Learning_in_Gaussian_Process_State-Space_Models_with_Particle_MCMC.html">nips2013-48</a> <a title="nips-2013-48-reference" href="#">nips2013-48-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>48 nips-2013-Bayesian Inference and Learning in Gaussian Process State-Space Models with Particle MCMC</h1>
<br/><p>Source: <a title="nips-2013-48-pdf" href="http://papers.nips.cc/paper/5085-bayesian-inference-and-learning-in-gaussian-process-state-space-models-with-particle-mcmc.pdf">pdf</a></p><p>Author: Roger Frigola, Fredrik Lindsten, Thomas B. Schon, Carl Rasmussen</p><p>Abstract: State-space models are successfully used in many areas of science, engineering and economics to model time series and dynamical systems. We present a fully Bayesian approach to inference and learning (i.e. state estimation and system identiﬁcation) in nonlinear nonparametric state-space models. We place a Gaussian process prior over the state transition dynamics, resulting in a ﬂexible model able to capture complex dynamical phenomena. To enable efﬁcient inference, we marginalize over the transition dynamics function and, instead, infer directly the joint smoothing distribution using specially tailored Particle Markov Chain Monte Carlo samplers. Once a sample from the smoothing distribution is computed, the state transition predictive distribution can be formulated analytically. Our approach preserves the full nonparametric expressivity of the model and can make use of sparse Gaussian processes to greatly reduce computational complexity. 1</p><br/>
<h2>reference text</h2><p>[1] C. E. Rasmussen and C. K. I. Williams, Gaussian Processes for Machine Learning.  MIT Press, 2006.</p>
<p>[2] R. Turner, M. P. Deisenroth, and C. E. Rasmussen, “State-space inference and learning with Gaussian processes,” in 13th International Conference on Artiﬁcial Intelligence and Statistics, ser. W&CP;, Y. W. Teh and M. Titterington, Eds., vol. 9, Chia Laguna, Sardinia, Italy, May 13–15 2010, pp. 868–875.</p>
<p>[3] C. Andrieu, A. Doucet, and R. Holenstein, “Particle Markov chain Monte Carlo methods,” Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 72, no. 3, pp. 269–342, 2010.</p>
<p>[4] F. Lindsten, M. Jordan, and T. B. Sch¨ n, “Ancestor sampling for particle Gibbs,” in Advances in Neural o Information Processing Systems 25, P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, Eds., 2012, pp. 2600–2608.</p>
<p>[5] M. Deisenroth, R. Turner, M. Huber, U. Hanebeck, and C. Rasmussen, “Robust ﬁltering and smoothing with Gaussian processes,” IEEE Transactions on Automatic Control, vol. 57, no. 7, pp. 1865 –1871, july 2012.</p>
<p>[6] M. Deisenroth and S. Mohamed, “Expectation Propagation in Gaussian process dynamical systems,” in Advances in Neural Information Processing Systems 25, P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, Eds., 2012, pp. 2618–2626.</p>
<p>[7] Z. Ghahramani and S. Roweis, “Learning nonlinear dynamical systems using an EM algorithm,” in Advances in Neural Information Processing Systems 11, M. J. Kearns, S. A. Solla, and D. A. Cohn, Eds. MIT Press, 1999.</p>
<p>[8] J. Wang, D. Fleet, and A. Hertzmann, “Gaussian process dynamical models,” in Advances in Neural Information Processing Systems 18, Y. Weiss, B. Sch¨ lkopf, and J. Platt, Eds. Cambridge, MA: MIT o Press, 2006, pp. 1441–1448.</p>
<p>[9] J. S. Liu, Monte Carlo Strategies in Scientiﬁc Computing.  Springer, 2001.</p>
<p>[10] A. Doucet and A. Johansen, “A tutorial on particle ﬁltering and smoothing: Fifteen years later,” in The Oxford Handbook of Nonlinear Filtering, D. Crisan and B. Rozovsky, Eds. Oxford University Press, 2011.</p>
<p>[11] F. Gustafsson, “Particle ﬁlter theory and practice with positioning applications,” IEEE Aerospace and Electronic Systems Magazine, vol. 25, no. 7, pp. 53–82, 2010.</p>
<p>[12] M. K. Pitt and N. Shephard, “Filtering via simulation: Auxiliary particle ﬁlters,” Journal of the American Statistical Association, vol. 94, no. 446, pp. 590–599, 1999.</p>
<p>[13] F. Lindsten and T. B. Sch¨ n, “Backward simulation methods for Monte Carlo statistical inference,” Founo dations and Trends in Machine Learning, vol. 6, no. 1, pp. 1–143, 2013.</p>
<p>[14] F. Lindsten and T. B. Sch¨ n, “On the use of backward simulation in the particle Gibbs sampler,” in o Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Kyoto, Japan, Mar. 2012.</p>
<p>[15] D. K. Agarwal and A. E. Gelfand, “Slice sampling for simulation based ﬁtting of spatial data models,” Statistics and Computing, vol. 15, no. 1, pp. 61–69, 2005.</p>
<p>[16] E. Snelson and Z. Ghahramani, “Sparse Gaussian processes using pseudo-inputs,” in Advances in Neural Information Processing Systems (NIPS), Y. Weiss, B. Sch¨ lkopf, and J. Platt, Eds., Cambridge, MA, 2006, o pp. 1257–1264.</p>
<p>[17] J. Qui˜ onero-Candela and C. E. Rasmussen, “A unifying view of sparse approximate Gaussian process n regression,” Journal of Machine Learning Research, vol. 6, pp. 1939–1959, 2005.</p>
<p>[18] M. Seeger, C. Williams, and N. Lawrence, “Fast Forward Selection to Speed Up Sparse Gaussian Process Regression,” in Artiﬁcial Intelligence and Statistics 9, 2003.</p>
<p>[19] Y. Chen, M. Welling, and A. Smola, “Super-samples from kernel herding,” in Proceedings of the 26th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2010), P. Gr¨ nwald and P. Spirtes, Eds. AUAI u Press, 2010.</p>
<p>[20] M. Deisenroth, “Efﬁcient reinforcement learning using Gaussian processes,” Ph.D. dissertation, Karlsruher Institut f¨ r Technologie, 2010. u  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
