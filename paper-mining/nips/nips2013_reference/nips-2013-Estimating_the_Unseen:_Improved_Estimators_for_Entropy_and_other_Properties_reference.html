<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 nips-2013-Estimating the Unseen: Improved Estimators for Entropy and other Properties</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-110" href="../nips2013/nips-2013-Estimating_the_Unseen%3A_Improved_Estimators_for_Entropy_and_other_Properties.html">nips2013-110</a> <a title="nips-2013-110-reference" href="#">nips2013-110-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>110 nips-2013-Estimating the Unseen: Improved Estimators for Entropy and other Properties</h1>
<br/><p>Source: <a title="nips-2013-110-pdf" href="http://papers.nips.cc/paper/5170-estimating-the-unseen-improved-estimators-for-entropy-and-other-properties.pdf">pdf</a></p><p>Author: Paul Valiant, Gregory Valiant</p><p>Abstract: Recently, Valiant and Valiant [1, 2] showed that a class of distributional properties, which includes such practically relevant properties as entropy, the number of distinct elements, and distance metrics between pairs of distributions, can be estimated given a sublinear sized sample. Speciﬁcally, given a sample consisting of independent draws from any distribution over at most n distinct elements, these properties can be estimated accurately using a sample of size O(n/ log n). We propose a novel modiﬁcation of this approach and show: 1) theoretically, this estimator is optimal (to constant factors, over worst-case instances), and 2) in practice, it performs exceptionally well for a variety of estimation tasks, on a variety of natural distributions, for a wide range of parameters. Perhaps unsurprisingly, the key step in our approach is to ﬁrst use the sample to characterize the “unseen” portion of the distribution. This goes beyond such tools as the Good-Turing frequency estimation scheme, which estimates the total probability mass of the unobserved portion of the distribution: we seek to estimate the shape of the unobserved portion of the distribution. This approach is robust, general, and theoretically principled; we expect that it may be fruitfully used as a component within larger machine learning and data analysis systems. 1</p><br/>
<h2>reference text</h2><p>[1] G. Valiant and P. Valiant. Estimating the unseen: an n/ log(n)–sample estimator for entropy and support size, shown optimal via new CLTs. In Symposium on Theory of Computing (STOC), 2011.</p>
<p>[2] G. Valiant and P. Valiant. The power of linear estimators. In IEEE Symposium on Foundations of Computer Science (FOCS), 2011.</p>
<p>[3] M. R. Nelson et al. An abundance of rare functional variants in 202 drug target genes sequenced in 14,002 people. Science, 337(6090):100–104, 2012.</p>
<p>[4] J. A. Tennessen et al. Evolution and functional impact of rare coding variation from deep sequencing of human exomes. Science, 337(6090):64–69, 2012.</p>
<p>[5] A. Keinan and A. G. Clark. Recent explosive human population growth has resulted in an excess of rare genetic variants. Science, 336(6082):740–743, 2012.</p>
<p>[6] F. Olken and D. Rotem. Random sampling from database ﬁles: a survey. In Proceedings of the Fifth International Workshop on Statistical and Scientiﬁc Data Management, 1990.</p>
<p>[7] P. J. Haas, J. F. Naughton, S. Seshadri, and A. N. Swami. Selectivity and cost estimation for joins based on random sampling. Journal of Computer and System Sciences, 52(3):550–569, 1996.</p>
<p>[8] R.A. Fisher, A. Corbet, and C.B. Williams. The relation between the number of species and the number of individuals in a random sample of an animal population. Journal of the British Ecological Society, 12(1):42–58, 1943.</p>
<p>[9] I. J. Good. The population frequencies of species and the estimation of population parameters. Biometrika, 40(16):237–264, 1953.</p>
<p>[10] D. A. McAllester and R.E. Schapire. On the convergence rate of Good-Turing estimators. In Conference on Learning Theory (COLT), 2000.</p>
<p>[11] A. Orlitsky, N.P. Santhanam, and J. Zhang. Always Good Turing: Asymptotically optimal probability estimation. Science, 302(5644):427–431, October 2003.</p>
<p>[12] A. Orlitsky, N. Santhanam, K.Viswanathan, and J. Zhang. On modeling proﬁles instead of values. Uncertainity in Artiﬁcial Intelligence, 2004.</p>
<p>[13] J. Acharya, A. Orlitsky, and S. Pan. The maximum likelihood probability of unique-singleton, ternary, and length-7 patterns. In IEEE Symp. on Information Theory, 2009.</p>
<p>[14] J. Acharya, H. Das, A. Orlitsky, and S. Pan. Competitive closeness testing. In COLT, 2011.</p>
<p>[15] L. Paninski. Estimation of entropy and mutual information. Neural Comp., 15(6):1191–1253, 2003.</p>
<p>[16] J. Bunge and M. Fitzpatrick. Estimating the number of species: A review. Journal of the American Statistical Association, 88(421):364–373, 1993.</p>
<p>[17] J. Bunge. Bibliography of references on the problem of estimating support size, available at http://www.stat.cornell.edu/˜bunge/bibliography.html.</p>
<p>[18] Z. Bar-Yossef, R. Kumar, and D. Sivakumar. Sampling algorithms: lower bounds and applications. In STOC, 2001.</p>
<p>[19] T. Batu Testing Properties of Distributions Ph.D. thesis, Cornell, 2001.</p>
<p>[20] M. Charikar, S. Chaudhuri, R. Motwani, and V.R. Narasayya. Towards estimation error guarantees for distinct values. In SODA, 2000.</p>
<p>[21] T. Batu, L. Fortnow, R. Rubinfeld, W.D. Smith, and P. White. Testing that distributions are close. In IEEE Symposium on Foundations of Computer Science (FOCS), 2000.</p>
<p>[22] V.Q. Vu, B. Yu, and R.E. Kass. Coverage-adjusted entropy estimation. Statistics in Medicine, 26(21):4039–4060, 2007.</p>
<p>[23] G. Miller. Note on the bias of information estimates. Information Theory in Psychology II-B, ed H Quastler (Glencoe, IL: Free Press):pp 95–100, 1955.</p>
<p>[24] S. Panzeri and A Treves. Analytical estimates of limited sampling biases in different information measures. Network: Computation in Neural Systems, 7:87–107, 1996.</p>
<p>[25] S. Zahl. Jackkniﬁng an index of diversity. Ecology, 58:907–913, 1977.</p>
<p>[26] B. Efron and C. Stein. The jacknife estimate of variance. Annals of Statistics, 9:586–596, 1981.</p>
<p>[27] A. Chao and T.J. Shen. Nonparametric estimation of shannons index of diversity when there are unseen species in sample. Environmental and Ecological Statistics, 10:429–443, 2003.</p>
<p>[28] D.G. Horvitz and D.J. Thompson. A generalization of sampling without replacement from a ﬁnite universe. Journal of the American Statistical Association, 47(260):663–685, 1952.</p>
<p>[29] P. Valiant. Testing Symmetric Properties of Distributions. SIAM J. Comput., 40(6):1927–1968,2011.  9</p>
<br/>
<br/><br/><br/></body>
</html>
