<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 nips-2013-Bayesian entropy estimation for binary spike train data using parametric prior knowledge</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-51" href="../nips2013/nips-2013-Bayesian_entropy_estimation_for_binary_spike_train_data_using_parametric_prior_knowledge.html">nips2013-51</a> <a title="nips-2013-51-reference" href="#">nips2013-51-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>51 nips-2013-Bayesian entropy estimation for binary spike train data using parametric prior knowledge</h1>
<br/><p>Source: <a title="nips-2013-51-pdf" href="http://papers.nips.cc/paper/4873-bayesian-entropy-estimation-for-binary-spike-train-data-using-parametric-prior-knowledge.pdf">pdf</a></p><p>Author: Evan W. Archer, Il M. Park, Jonathan W. Pillow</p><p>Abstract: Shannon’s entropy is a basic quantity in information theory, and a fundamental building block for the analysis of neural codes. Estimating the entropy of a discrete distribution from samples is an important and difﬁcult problem that has received considerable attention in statistics and theoretical neuroscience. However, neural responses have characteristic statistical structure that generic entropy estimators fail to exploit. For example, existing Bayesian entropy estimators make the naive assumption that all spike words are equally likely a priori, which makes for an inefﬁcient allocation of prior probability mass in cases where spikes are sparse. Here we develop Bayesian estimators for the entropy of binary spike trains using priors designed to ﬂexibly exploit the statistical structure of simultaneouslyrecorded spike responses. We deﬁne two prior distributions over spike words using mixtures of Dirichlet distributions centered on simple parametric models. The parametric model captures high-level statistical features of the data, such as the average spike count in a spike word, which allows the posterior over entropy to concentrate more rapidly than with standard estimators (e.g., in cases where the probability of spiking differs strongly from 0.5). Conversely, the Dirichlet distributions assign prior mass to distributions far from the parametric model, ensuring consistent estimates for arbitrary distributions. We devise a compact representation of the data and prior that allow for computationally efﬁcient implementations of Bayesian least squares and empirical Bayes entropy estimators with large numbers of neurons. We apply these estimators to simulated and real neural data and show that they substantially outperform traditional methods.</p><br/>
<h2>reference text</h2><p>[1] K. H. Schindler, M. Palus, M. Vejmelka, and J. Bhattacharya. Causality detection based on informationtheoretic approaches in time series analysis. Physics Reports, 441:1–46, 2007.</p>
<p>[2] A. R´ nyi. On measures of dependence. Acta Mathematica Hungarica, 10(3-4):441–451, 9 1959. e</p>
<p>[3] C. Chow and C. Liu. Approximating discrete probability distributions with dependence trees. Information Theory, IEEE Transactions on, 14(3):462–467, 1968.</p>
<p>[4] A. Chao and T. Shen. Nonparametric estimation of Shannon’s index of diversity when there are unseen species in sample. Environmental and Ecological Statistics, 10(4):429–443, 2003.</p>
<p>[5] P. Grassberger. Estimating the information content of symbol sequences and efﬁcient codes. Information Theory, IEEE Transactions on, 35(3):669–675, 1989.</p>
<p>[6] S. Ma. Calculation of entropy from data of motion. Journal of Statistical Physics, 26(2):221–240, 1981.</p>
<p>[7] S. Panzeri, R. Senatore, M. A. Montemurro, and R. S. Petersen. Correcting for the sampling bias problem in spike train information measures. J Neurophysiol, 98(3):1064–1072, Sep 2007.</p>
<p>[8] L. Paninski. Estimation of entropy and mutual information. Neural Computation, 15:1191–1253, 2003.</p>
<p>[9] W. Bialek, F. Rieke, R. R. de Ruyter van Steveninck, R., and D. Warland. Reading a neural code. Science, 252:1854–1857, 1991.</p>
<p>[10] R. Strong, S. Koberle, de Ruyter van Steveninck R., and W. Bialek. Entropy and information in neural spike trains. Physical Review Letters, 80:197–202, 1998.</p>
<p>[11] L. Paninski. Estimation of entropy and mutual information. Neural Computation, 15:1191–1253, 2003.</p>
<p>[12] R. Barbieri, L. Frank, D. Nguyen, M. Quirk, V. Solo, M. Wilson, and E. Brown. Dynamic analyses of information encoding in neural ensembles. Neural Computation, 16:277–307, 2004.</p>
<p>[13] M. Kennel, J. Shlens, H. Abarbanel, and E. Chichilnisky. Estimating entropy rates with Bayesian conﬁdence intervals. Neural Computation, 17:1531–1576, 2005.</p>
<p>[14] J. Victor. Approaches to information-theoretic analysis of neural activity. Biological theory, 1(3):302– 316, 2006.</p>
<p>[15] J. Shlens, M. B. Kennel, H. D. I. Abarbanel, and E. J. Chichilnisky. Estimating information rates with conﬁdence intervals in neural spike trains. Neural Computation, 19(7):1683–1719, Jul 2007.</p>
<p>[16] V. Q. Vu, B. Yu, and R. E. Kass. Coverage-adjusted entropy estimation. Statistics in medicine, 26(21):4039–4060, 2007.</p>
<p>[17] V. Q. Vu, B. Yu, and R. E. Kass. Information in the nonstationary case. Neural Computation, 21(3):688– 703, 2009, http://www.mitpressjournals.org/doi/pdf/10.1162/neco.2008.01-08-700. PMID: 18928371.</p>
<p>[18] E. Archer, I. M. Park, and J. Pillow. Bayesian estimation of discrete entropy with mixtures of stickbreaking priors. In P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2024–2032. MIT Press, Cambridge, MA, 2012.</p>
<p>[19] I. Nemenman, F. Shafee, and W. Bialek. Entropy and inference, revisited. In Advances in Neural Information Processing Systems 14, pages 471–478. MIT Press, Cambridge, MA, 2002.</p>
<p>[20] M. Okun, P. Yger, S. L. Marguet, F. Gerard-Mercier, A. Benucci, S. Katzner, L. Busse, M. Carandini, and K. D. Harris. Population rate dynamics and multineuron ﬁring patterns in sensory cortex. The Journal of Neuroscience, 32(48):17108–17119, 2012, http://www.jneurosci.org/content/32/48/17108.full.pdf+html.</p>
<p>[21] G. Tkaˇ ik, O. Marre, T. Mora, D. Amodei, M. J. Berry II, and W. Bialek. The simplest maximum c entropy model for collective behavior in a neural network. Journal of Statistical Mechanics: Theory and Experiment, 2013(03):P03011, 2013.</p>
<p>[22] D. Wolpert and D. Wolf. Estimating functions of probability distributions from a ﬁnite set of samples. Physical Review E, 52(6):6841–6854, 1995.</p>
<p>[23] I. M. Park, E. Archer, K. Latimer, and J. W. Pillow. Universal models for binary spike patterns using centered Dirichlet processes. In Advances in Neural Information Processing Systems (NIPS), 2013.</p>
<p>[24] A. Panzeri, S. Treves, S. Schultz, and E. Rolls. On decoding the responses of a population of neurons from short time windows. Neural Computation, 11:1553–1577, 1999.  9</p>
<br/>
<br/><br/><br/></body>
</html>
