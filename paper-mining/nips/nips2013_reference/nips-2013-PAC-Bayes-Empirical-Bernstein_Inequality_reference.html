<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-242" href="../nips2013/nips-2013-PAC-Bayes-Empirical-Bernstein_Inequality.html">nips2013-242</a> <a title="nips-2013-242-reference" href="#">nips2013-242-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>242 nips-2013-PAC-Bayes-Empirical-Bernstein Inequality</h1>
<br/><p>Source: <a title="nips-2013-242-pdf" href="http://papers.nips.cc/paper/4903-pac-bayes-empirical-bernstein-inequality.pdf">pdf</a></p><p>Author: Ilya O. Tolstikhin, Yevgeny Seldin</p><p>Abstract: We present a PAC-Bayes-Empirical-Bernstein inequality. The inequality is based on a combination of the PAC-Bayesian bounding technique with an Empirical Bernstein bound. We show that when the empirical variance is signiﬁcantly smaller than the empirical loss the PAC-Bayes-Empirical-Bernstein inequality is signiﬁcantly tighter than the PAC-Bayes-kl inequality of Seeger (2002) and otherwise it is comparable. Our theoretical analysis is conﬁrmed empirically on a synthetic example and several UCI datasets. The PAC-Bayes-Empirical-Bernstein inequality is an interesting example of an application of the PAC-Bayesian bounding technique to self-bounding functions. 1</p><br/>
<h2>reference text</h2><p>[1] John Langford and John Shawe-Taylor. PAC-Bayes & margins. In Advances in Neural Information Processing Systems (NIPS), 2002.</p>
<p>[2] David McAllester. PAC-Bayesian stochastic model selection. Machine Learning, 51(1), 2003.</p>
<p>[3] John Langford. Tutorial on practical prediction theory for classiﬁcation. Journal of Machine Learning Research, 2005.</p>
<p>[4] Yevgeny Seldin and Naftali Tishby. PAC-Bayesian analysis of co-clustering and beyond. Journal of Machine Learning Research, 11, 2010.</p>
<p>[5] Matthew Higgs and John Shawe-Taylor. A PAC-Bayes bound for tailored density estimation. In Proceedings of the International Conference on Algorithmic Learning Theory (ALT), 2010. 8</p>
<p>[6] Yevgeny Seldin, Peter Auer, Francois Laviolette, John Shawe-Taylor, and Ronald Ortner. PAC¸ Bayesian analysis of contextual bandits. In Advances in Neural Information Processing Systems (NIPS), 2011.</p>
<p>[7] Matthias Seeger. PAC-Bayesian generalization error bounds for Gaussian process classiﬁcation. Journal of Machine Learning Research, 2002.</p>
<p>[8] Yevgeny Seldin, Francois Laviolette, Nicol` Cesa-Bianchi, John Shawe-Taylor, and Peter ¸ o Auer. PAC-Bayesian inequalities for martingales. IEEE Transactions on Information Theory, 58, 2012.</p>
<p>[9] Andreas Maurer and Massimiliano Pontil. Empirical Bernstein bounds and sample variance penalization. In Proceedings of the International Conference on Computational Learning Theory (COLT), 2009.</p>
<p>[10] Thomas M. Cover and Joy A. Thomas. Elements of Information Theory. John Wiley & Sons, 1991.</p>
<p>[11] Andreas Maurer. A note on the PAC-Bayesian theorem. www.arxiv.org, 2004.</p>
<p>[12] David McAllester. Some PAC-Bayesian theorems. Machine Learning, 37, 1999.</p>
<p>[13] A.W. Van Der Vaart. Asymptotic statistics. Cambridge University Press, 1998.</p>
<p>[14] St´ phane Boucheron, G´ bor Lugosi, and Olivier Bousquet. Concentration inequalities. In e a O. Bousquet, U.v. Luxburg, and G. R¨ tsch, editors, Advanced Lectures in Machine Learning. a Springer, 2004.</p>
<p>[15] A. Asuncion and D.J. Newman. UCI machine http://www.ics.uci.edu/∼mlearn/MLRepository.html.  9  learning  repository,  2007.</p>
<br/>
<br/><br/><br/></body>
</html>
