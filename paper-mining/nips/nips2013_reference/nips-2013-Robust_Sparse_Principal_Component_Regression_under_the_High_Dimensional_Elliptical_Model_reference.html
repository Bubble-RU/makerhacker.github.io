<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>283 nips-2013-Robust Sparse Principal Component Regression under the High Dimensional Elliptical Model</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-283" href="../nips2013/nips-2013-Robust_Sparse_Principal_Component_Regression_under_the_High_Dimensional_Elliptical_Model.html">nips2013-283</a> <a title="nips-2013-283-reference" href="#">nips2013-283-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>283 nips-2013-Robust Sparse Principal Component Regression under the High Dimensional Elliptical Model</h1>
<br/><p>Source: <a title="nips-2013-283-pdf" href="http://papers.nips.cc/paper/4869-robust-sparse-principal-component-regression-under-the-high-dimensional-elliptical-model.pdf">pdf</a></p><p>Author: Fang Han, Han Liu</p><p>Abstract: In this paper we focus on the principal component regression and its application to high dimension non-Gaussian data. The major contributions are two folds. First, in low dimensions and under the Gaussian model, by borrowing the strength from recent development in minimax optimal principal component estimation, we ﬁrst time sharply characterize the potential advantage of classical principal component regression over least square estimation. Secondly, we propose and analyze a new robust sparse principal component regression on high dimensional elliptically distributed data. The elliptical distribution is a semiparametric generalization of the Gaussian, including many well known distributions such as multivariate Gaussian, rank-deﬁcient Gaussian, t, Cauchy, and logistic. It allows the random vector to be heavy tailed and have tail dependence. These extra ﬂexibilities make it very suitable for modeling ﬁnance and biomedical imaging data. Under the elliptical model, we prove that our method can estimate the regression coefﬁcients in the optimal parametric rate and therefore is a good alternative to the Gaussian based methods. Experiments on synthetic and real world data are conducted to illustrate the empirical usefulness of the proposed method. 1</p><br/>
<h2>reference text</h2><p>Artemiou, A. and Li, B. (2009). On principal components and regression: a statistical explanation of a natural phenomenon. Statistica Sinica, 19(4):1557. Bunea, F. and Xiao, L. (2012). On the sample covariance matrix estimator of reduced effective rank population matrices, with applications to fPCA. arXiv preprint arXiv:1212.5321. Cai, T. T., Ma, Z., and Wu, Y. (2013). Sparse PCA: Optimal rates and adaptive estimation. The Annals of Statistics (to appear). Choi, K. and Marden, J. (1998). A multivariate version of kendall’s τ . Journal of Nonparametric Statistics, 9(3):261–293. Cook, R. D. (2007). Fisher lecture: Dimension reduction in regression. Statistical Science, 22(1):1–26. Croux, C., Ollila, E., and Oja, H. (2002). Sign and rank covariance matrices: statistical properties and application to principal components analysis. In Statistical data analysis based on the L1-norm and related methods, pages 257–269. Springer. d’Aspremont, A., El Ghaoui, L., Jordan, M. I., and Lanckriet, G. R. (2007). A direct formulation for sparse PCA using semideﬁnite programming. SIAM review, 49(3):434–448. Fang, K., Kotz, S., and Ng, K. (1990). Symmetric multivariate and related distributions. Chapman&Hall;, London. Han, F. and Liu, H. (2013a). Optimal sparse principal component analysis in high dimensional elliptical model. arXiv preprint arXiv:1310.3561. Han, F. and Liu, H. (2013b). Scale-invariant sparse PCA on high dimensional meta-elliptical data. Journal of the American Statistical Association (in press). Johnstone, I. M. and Lu, A. Y. (2009). On consistency and sparsity for principal components analysis in high dimensions. Journal of the American Statistical Association, 104(486). Kendall, M. G. (1968). A course in multivariate analysis. Kl¨ ppelberg, C., Kuhn, G., and Peng, L. (2007). Estimating the tail dependence function of an elliptical u distribution. Bernoulli, 13(1):229–251. Lounici, K. (2012). arXiv:1205.7060.  Sparse principal component analysis with missing observations.  arXiv preprint  Ma, Z. (2013). Sparse principal component analysis and iterative thresholding. to appear Annals of Statistics. Massy, W. F. (1965). Principal components regression in exploratory statistical research. Journal of the American Statistical Association, 60(309):234–256. Moghaddam, B., Weiss, Y., and Avidan, S. (2006). Spectral bounds for sparse PCA: Exact and greedy algorithms. Advances in neural information processing systems, 18:915. Oja, H. (2010). Multivariate Nonparametric Methods with R: An approach based on spatial signs and ranks, volume 199. Springer. Ravikumar, P., Raskutti, G., Wainwright, M., and Yu, B. (2008). Model selection in gaussian graphical models: High-dimensional consistency of l1-regularized mle. Advances in Neural Information Processing Systems (NIPS), 21. Tyler, D. E. (1987). A distribution-free m-estimator of multivariate scatter. The Annals of Statistics, 15(1):234– 251. Vershynin, R. (2010). arXiv:1011.3027.  Introduction to the non-asymptotic analysis of random matrices.  arXiv preprint  Vu, V. Q. and Lei, J. (2012). Minimax rates of estimation for sparse pca in high dimensions. Journal of Machine Learning Research (AIStats Track). Yuan, X. and Zhang, T. (2013). Truncated power method for sparse eigenvalue problems. Journal of Machine Learning Research, 14:899–925. Zou, H., Hastie, T., and Tibshirani, R. (2006). Sparse principal component analysis. Journal of computational and graphical statistics, 15(2):265–286.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
