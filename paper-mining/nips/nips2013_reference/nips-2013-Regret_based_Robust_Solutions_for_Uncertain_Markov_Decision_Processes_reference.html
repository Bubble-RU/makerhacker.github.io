<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-270" href="../nips2013/nips-2013-Regret_based_Robust_Solutions_for_Uncertain_Markov_Decision_Processes.html">nips2013-270</a> <a title="nips-2013-270-reference" href="#">nips2013-270-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>270 nips-2013-Regret based Robust Solutions for Uncertain Markov Decision Processes</h1>
<br/><p>Source: <a title="nips-2013-270-pdf" href="http://papers.nips.cc/paper/4970-regret-based-robust-solutions-for-uncertain-markov-decision-processes.pdf">pdf</a></p><p>Author: Asrar Ahmed, Pradeep Varakantham, Yossiri Adulyasak, Patrick Jaillet</p><p>Abstract: In this paper, we seek robust policies for uncertain Markov Decision Processes (MDPs). Most robust optimization approaches for these problems have focussed on the computation of maximin policies which maximize the value corresponding to the worst realization of the uncertainty. Recent work has proposed minimax regret as a suitable alternative to the maximin objective for robust optimization. However, existing algorithms for handling minimax regret are restricted to models with uncertainty over rewards only. We provide algorithms that employ sampling to improve across multiple dimensions: (a) Handle uncertainties over both transition and reward models; (b) Dependence of model uncertainties across state, action pairs and decision epochs; (c) Scalability and quality bounds. Finally, to demonstrate the empirical effectiveness of our sampling approaches, we provide comparisons against benchmark algorithms on two domains from literature. We also provide a Sample Average Approximation (SAA) analysis to compute a posteriori error bounds.</p><br/>
<h2>reference text</h2><p>[1] J. Andrew Bagnell, Andrew Y. Ng, and Jeff G. Schneider. Solving uncertain markov decision processes. Technical report, Carnegie Mellon University, 2001.</p>
<p>[2] S´ bastien Bubeck, R´ mi Munos, and Gilles Stoltz. Pure exploration in multi-armed bandits e e problems. In Proceedings of the 20th international conference on Algorithmic learning theory, Algorithmic Learning Theory, 2009.</p>
<p>[3] Robert Givan, Sonia Leach, and Thomas Dean. Bounded-parameter markov decision processes. Artiﬁcial Intelligence, 122, 2000.</p>
<p>[4] G. Iyengar. Robust dynamic programming. Mathematics of Operations Research, 30, 2004.</p>
<p>[5] Michael Kearns, Yishay Mansour, and Andrew Y. Ng. A sparse sampling algorithm for nearoptimal planning in large markov decision processes. Machine Learning, 49, 2002.</p>
<p>[6] Shie Mannor, Oﬁr Mebel, and Huan Xu. Lightning does not strike twice: Robust MDPs with coupled uncertainty. In International Conference on Machine Learning (ICML), 2012.</p>
<p>[7] Andrew Mastin and Patrick Jaillet. Loss bounds for uncertain transition probabilities in markov decision processes. In IEEE Annual Conference on Decision and Control (CDC), 2012, 2012.</p>
<p>[8] Arnab Nilim and Laurent El Ghaoui. Ghaoui, l.: Robust control of markov decision processes with uncertain transition matrices. Operations Research, 2005.</p>
<p>[9] Joelle Pineau, Geoffrey J. Gordon, and Sebastian Thrun. Point-based value iteration: An anytime algorithm for POMDPs. In International Joint Conference on Artiﬁcial Intelligence, 2003.</p>
<p>[10] Martin Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John Wiley and Sons, 1994.</p>
<p>[11] Kevin Regan and Craig Boutilier. Regret-based reward elicitation for markov decision processes. In Uncertainty in Artiﬁcial Intelligence, 2009.</p>
<p>[12] Kevin Regan and Craig Boutilier. Robust policy computation in reward-uncertain MDPs using nondominated policies. In National Conference on Artiﬁcial Intelligence (AAAI), 2010.</p>
<p>[13] Leonard Savage. The Foundations of Statistics. Wiley, 1954.</p>
<p>[14] A. Shapiro. Monte carlo sampling methods. In Stochastic Programming, volume 10 of Handbooks in Operations Research and Management Science. Elsevier, 2003.</p>
<p>[15] Wolfram Wiesemann, Daniel Kuhn, and Ber Rustem. Robust markov decision processes. Mathematics of Operations Research, 38(1), 2013.</p>
<p>[16] Huan Xu and Shie Mannor. Parametric regret in uncertain markov decision processes. In IEEE Conference on Decision and Control, CDC, 2009.  9</p>
<br/>
<br/><br/><br/></body>
</html>
