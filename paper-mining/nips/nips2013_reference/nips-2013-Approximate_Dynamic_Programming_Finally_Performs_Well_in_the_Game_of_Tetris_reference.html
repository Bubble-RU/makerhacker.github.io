<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-38" href="../nips2013/nips-2013-Approximate_Dynamic_Programming_Finally_Performs_Well_in_the_Game_of_Tetris.html">nips2013-38</a> <a title="nips-2013-38-reference" href="#">nips2013-38-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>38 nips-2013-Approximate Dynamic Programming Finally Performs Well in the Game of Tetris</h1>
<br/><p>Source: <a title="nips-2013-38-pdf" href="http://papers.nips.cc/paper/5190-approximate-dynamic-programming-finally-performs-well-in-the-game-of-tetris.pdf">pdf</a></p><p>Author: Victor Gabillon, Mohammad Ghavamzadeh, Bruno Scherrer</p><p>Abstract: Tetris is a video game that has been widely used as a benchmark for various optimization techniques including approximate dynamic programming (ADP) algorithms. A look at the literature of this game shows that while ADP algorithms that have been (almost) entirely based on approximating the value function (value function based) have performed poorly in Tetris, the methods that search directly in the space of policies by learning the policy parameters using an optimization black box, such as the cross entropy (CE) method, have achieved the best reported results. This makes us conjecture that Tetris is a game in which good policies are easier to represent, and thus, learn than their corresponding value functions. So, in order to obtain a good performance with ADP, we should use ADP algorithms that search in a policy space, instead of the more traditional ones that search in a value function space. In this paper, we put our conjecture to test by applying such an ADP algorithm, called classiﬁcation-based modiﬁed policy iteration (CBMPI), to the game of Tetris. Our experimental results show that for the ﬁrst time an ADP algorithm, namely CBMPI, obtains the best results reported in the literature for Tetris in both small 10 × 10 and large 10 × 20 boards. Although the CBMPI’s results are similar to those of the CE method in the large board, CBMPI uses considerably fewer (almost 1/6) samples (calls to the generative model) than CE. 1</p><br/>
<h2>reference text</h2><p>[1] D. Bertsekas and S. Ioffe. Temporal differences-based policy iteration and applications in neuro-dynamic programming. Technical report, MIT, 1996.</p>
<p>[2] D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.</p>
<p>[3] H. Burgiel. How to Lose at Tetris. Mathematical Gazette, 81:194–200, 1997.</p>
<p>[4] E. Demaine, S. Hohenberger, and D. Liben-Nowell. Tetris is hard, even to approximate. In Proceedings of the Ninth International Computing and Combinatorics Conference, pages 351– 363, 2003.</p>
<p>[5] C. Fahey. Tetris AI, Computer plays Tetris, 2003. http://colinfahey.com/tetris/ tetris.html.</p>
<p>[6] V. Farias and B. van Roy. Tetris: A study of randomized constraint sampling. Springer-Verlag, 2006.</p>
<p>[7] A. Fern, S. Yoon, and R. Givan. Approximate Policy Iteration with a Policy Language Bias: Solving Relational Markov Decision Processes. Journal of Artiﬁcial Intelligence Research, 25:75–118, 2006.</p>
<p>[8] T. Furmston and D. Barber. A unifying perspective of parametric policy search methods for Markov decision processes. In Proceedings of the Advances in Neural Information Processing Systems, pages 2726–2734, 2012.</p>
<p>[9] V. Gabillon, A. Lazaric, M. Ghavamzadeh, and B. Scherrer. Classiﬁcation-based policy iteration with a critic. In Proceedings of ICML, pages 1049–1056, 2011.</p>
<p>[10] N. Hansen and A. Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary Computation, 9:159–195, 2001.</p>
<p>[11] S. Kakade. A natural policy gradient. In Proceedings of the Advances in Neural Information Processing Systems, pages 1531–1538, 2001.</p>
<p>[12] M. Lagoudakis and R. Parr. Reinforcement Learning as Classiﬁcation: Leveraging Modern Classiﬁers. In Proceedings of ICML, pages 424–431, 2003.</p>
<p>[13] A. Lazaric, M. Ghavamzadeh, and R. Munos. Analysis of a Classiﬁcation-based Policy Iteration Algorithm. In Proceedings of ICML, pages 607–614, 2010.</p>
<p>[14] M. Puterman and M. Shin. Modiﬁed policy iteration algorithms for discounted Markov decision problems. Management Science, 24(11), 1978.</p>
<p>[15] R. Rubinstein and D. Kroese. The cross-entropy method: A uniﬁed approach to combinatorial optimization, Monte-Carlo simulation, and machine learning. Springer-Verlag, 2004.</p>
<p>[16] B. Scherrer. Performance Bounds for λ-Policy Iteration and Application to the Game of Tetris. Journal of Machine Learning Research, 14:1175–1221, 2013.</p>
<p>[17] B. Scherrer, M. Ghavamzadeh, V. Gabillon, and M. Geist. Approximate modiﬁed policy iteration. In Proceedings of ICML, pages 1207–1214, 2012.</p>
<p>[18] I. Szita and A. L˝ rincz. Learning Tetris Using the Noisy Cross-Entropy Method. Neural o Computation, 18(12):2936–2941, 2006.</p>
<p>[19] C. Thiery and B. Scherrer. Building Controllers for Tetris. International Computer Games Association Journal, 32:3–11, 2009.</p>
<p>[20] C. Thiery and B. Scherrer. Improvements on Learning Tetris with Cross Entropy. International Computer Games Association Journal, 32, 2009.</p>
<p>[21] C. Thiery and B. Scherrer. MDPTetris features documentation, 2010. http:// mdptetris.gforge.inria.fr/doc/feature_functions_8h.html.</p>
<p>[22] J. Tsitsiklis and B Van Roy. Feature-based methods for large scale dynamic programming. Machine Learning, 22:59–94, 1996.  9</p>
<br/>
<br/><br/><br/></body>
</html>
