<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-145" href="../nips2013/nips-2013-It_is_all_in_the_noise%3A_Efficient_multi-task_Gaussian_process_inference_with_structured_residuals.html">nips2013-145</a> <a title="nips-2013-145-reference" href="#">nips2013-145-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>145 nips-2013-It is all in the noise: Efficient multi-task Gaussian process inference with structured residuals</h1>
<br/><p>Source: <a title="nips-2013-145-pdf" href="http://papers.nips.cc/paper/5089-it-is-all-in-the-noise-efficient-multi-task-gaussian-process-inference-with-structured-residuals.pdf">pdf</a></p><p>Author: Barbara Rakitsch, Christoph Lippert, Karsten Borgwardt, Oliver Stegle</p><p>Abstract: Multi-task prediction methods are widely used to couple regressors or classiﬁcation models by sharing information across related tasks. We propose a multi-task Gaussian process approach for modeling both the relatedness between regressors and the task correlations in the residuals, in order to more accurately identify true sharing between regressors. The resulting Gaussian model has a covariance term in form of a sum of Kronecker products, for which efﬁcient parameter inference and out of sample prediction are feasible. On both synthetic examples and applications to phenotype prediction in genetics, we ﬁnd substantial beneﬁts of modeling structured noise compared to established alternatives. 1</p><br/>
<h2>reference text</h2><p>[1] Edwin V. Bonilla, Kian Ming Adam Chai, and Christopher K. I. Williams. Multi-task gaussian process prediction. In NIPS, 2007. ´</p>
<p>[2] Mauricio A. Alvarez and Neil D. Lawrence. Sparse convolved gaussian processes for multioutput regression. In NIPS, pages 57–64, 2008.</p>
<p>[3] Edwin V. Bonilla, Felix V. Agakov, and Christopher K. I. Williams. Kernel multi-task learning using task-speciﬁc features. In AISTATS, 2007.</p>
<p>[4] Byron M. Yu, John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, and Maneesh Sahani. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity. In NIPS, pages 1881–1888, 2008.</p>
<p>[5] Oliver Stegle, Christoph Lippert, Joris M. Mooij, Neil D. Lawrence, and Karsten M. Borgwardt. Efﬁcient inference in matrix-variate gaussian models with iid observation noise. In NIPS, pages 630–638, 2011.</p>
<p>[6] Karin Meyer. Estimating variances and covariances for multivariate animal models by restricted maximum likelihood. Genetics Selection Evolution, 23(1):67–83, 1991.</p>
<p>[7] V Ducrocq and H Chapuis. Generalizing the use of the canonical transformation for the solution of multivariate mixed model equations. Genetics Selection Evolution, 29(2):205–224, 1997.</p>
<p>[8] Hao Zhang. Maximum-likelihood estimation for multivariate spatial linear coregionalization models. Environmetrics, 18(2):125–139, 2007.</p>
<p>[9] Andrew Gordon Wilson, David A. Knowles, and Zoubin Ghahramani. Gaussian process regression networks. In ICML, 2012.</p>
<p>[10] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning). The MIT Press, 2005.</p>
<p>[11] Alfredo A. Kalaitzis and Neil D. Lawrence. Residual components analysis. In ICML, 2012.</p>
<p>[12] Ciyou Zhu, Richard H. Byrd, Peihuang Lu, and Jorge Nocedal. Algorithm 778: L-bfgs-b: Fortran subroutines for large-scale bound-constrained optimization. ACM Trans. Math. Softw., 23(4):550–560, December 1997.</p>
<p>[13] Ulrike Ober, Julien F. Ayroles, Eric A. Stone, Stephen Richards, and et al. Using WholeGenome Sequence Data to Predict Quantitative Trait Phenotypes in Drosophila melanogaster. PLoS Genetics, 8(5):e1002685+, May 2012.</p>
<p>[14] Erin N Smith and Leonid Kruglyak. Gene–environment interaction in yeast gene expression. PLoS Biology, 6(4):e83, 2008.</p>
<p>[15] S. Atwell, Y. S. Huang, B. J. Vilhjalmsson, Willems, and et al. Genome-wide association study of 107 phenotypes in Arabidopsis thaliana inbred lines. Nature, 465(7298):627–631, Jun 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
