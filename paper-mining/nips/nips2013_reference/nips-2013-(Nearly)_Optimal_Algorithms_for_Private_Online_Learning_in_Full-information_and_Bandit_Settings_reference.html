<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-2" href="../nips2013/nips-2013-%28Nearly%29_Optimal_Algorithms_for_Private_Online_Learning_in_Full-information_and_Bandit_Settings.html">nips2013-2</a> <a title="nips-2013-2-reference" href="#">nips2013-2-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>2 nips-2013-(Nearly) Optimal Algorithms for Private Online Learning in Full-information and Bandit Settings</h1>
<br/><p>Source: <a title="nips-2013-2-pdf" href="http://papers.nips.cc/paper/5012-nearly-optimal-algorithms-for-private-online-learning-in-full-information-and-bandit-settings.pdf">pdf</a></p><p>Author: Abhradeep Guha Thakurta, Adam Smith</p><p>Abstract: We give differentially private algorithms for a large class of online learning algorithms, in both the full information and bandit settings. Our algorithms aim to minimize a convex loss function which is a sum of smaller convex loss terms, one for each data point. To design our algorithms, we modify the popular mirror descent approach, or rather a variant called follow the approximate leader. The technique leads to the ﬁrst nonprivate algorithms for private online learning in the bandit setting. In the full information setting, our algorithms improve over the regret bounds of previous work (due to Dwork, Naor, Pitassi and Rothblum (2010) and Jain, Kothari and Thakurta (2012)). In many cases, our algorithms (in both settings) match the dependence on the input length, T , of the optimal nonprivate regret bounds up to logarithmic factors in T . Our algorithms require logarithmic space and update time. 1</p><br/>
<h2>reference text</h2><p>[ACBF02] Peter Auer, Nicol` Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit o problem. Machine learning, 2002. [ADX10]  Alekh Agarwal, Ofer Dekel, and Lin Xiao. Optimal algorithms for online convex optimization with multi-point bandit feedback. In COLT, 2010.  [AHR08]  Jacob Abernethy, Elad Hazan, and Alexander Rakhlin. Competing in the dark: An efﬁcient algorithm for bandit linear optimization. In COLT, 2008.  [BCB12]  S´ bastien Bubeck and Nicolo Cesa-Bianchi. Regret analysis of stochastic and nonstochastic multie armed bandit problems. arXiv preprint arXiv:1204.5721, 2012.  [BDMN05] Avrim Blum, Cynthia Dwork, Frank McSherry, and Kobbi Nissim. Practical privacy: The SuLQ framework. In PODS, 2005. [CM08]  Kamalika Chaudhuri and Claire Monteleoni. Privacy-preserving logistic regression. In NIPS, 2008.  [CMS11]  Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12:1069–1109, 2011.  [CSS10]  TH Hubert Chan, Elaine Shi, and Dawn Song. Private and continual release of statistics. In ICALP, 2010.  [DJW13]  John C. Duchi, Michael I. Jordan, and Martin J. Wainwright. Local privacy and statistical minimax rates. In IEEE Symp. on Foundations of Computer Science (FOCS), 2013. http://arxiv.org/abs/1302.3203.  [DMNS06] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private data analysis. In TCC, 2006. [DN10]  Cynthia Dwork and Moni Naor. On the difﬁculties of disclosure prevention in statistical databases or the case for differential privacy. J. Privacy and Conﬁdentiality, 2(1), 2010.  [DNPR10] Cynthia Dwork, Moni Naor, Toniann Pitassi, and Guy N Rothblum. Differential privacy under continual observation. In Proceedings of the 42nd ACM symposium on Theory of computing, 2010. [Dwo06]  Cynthia Dwork. Differential privacy. In ICALP, 2006.  [FKM05]  Abraham D Flaxman, Adam Tauman Kalai, and H Brendan McMahan. Online convex optimization in the bandit setting: gradient descent without a gradient. In SODA, 2005.  [HAK07]  Elad Hazan, Amit Agarwal, and Satyen Kale. Logarithmic regret algorithms for online convex optimization. Journal of Machine Learning Research, 2007.  [Han57]  James Hannan. Approximation to bayes risk in repeated play. 1957.  [JKT12]  Prateek Jain, Pravesh Kothari, and Abhradeep Thakurta. Differentially private online learning. In COLT, 2012.  [JT13]  Prateek Jain and Abhradeep Thakurta. Differentially private learning with kernels. In ICML, 2013.  [KLN+ 08] Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim, Sofya Raskhodnikova, and Adam Smith. What can we learn privately? In FOCS, 2008. [KM12]  Daniel Kifer and Ashwin Machanavajjhala. A rigorous and customizable framework for privacy. In PODS, 2012.  [KS08]  Shiva Prasad Kasiviswanathan and Adam Smith. A note on differential privacy: Deﬁning resistance to arbitrary side information. CoRR, arXiv:0803.39461 [cs.CR], 2008.  [KST12]  Daniel Kifer, Adam Smith, and Abhradeep Thakurta. Private convex empirical risk minimization and high-dimensional regression. In COLT, 2012.  [Sha11]  Shai Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends R in Machine Learning, 2011.  [Smi11]  Adam Smith. Privacy-preserving statistical estimators with optimal convergence rates. In STOC, 2011.  [Zin03]  Martin Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML, 2003.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
