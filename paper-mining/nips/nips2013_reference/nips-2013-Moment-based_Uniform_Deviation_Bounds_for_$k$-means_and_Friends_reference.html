<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-197" href="../nips2013/nips-2013-Moment-based_Uniform_Deviation_Bounds_for_%24k%24-means_and_Friends.html">nips2013-197</a> <a title="nips-2013-197-reference" href="#">nips2013-197-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>197 nips-2013-Moment-based Uniform Deviation Bounds for $k$-means and Friends</h1>
<br/><p>Source: <a title="nips-2013-197-pdf" href="http://papers.nips.cc/paper/5100-moment-based-uniform-deviation-bounds-for-k-means-and-friends.pdf">pdf</a></p><p>Author: Matus Telgarsky, Sanjoy Dasgupta</p><p>Abstract: Suppose k centers are ﬁt to m points by heuristically minimizing the k-means cost; what is the corresponding ﬁt over the source distribution? This question is resolved here for distributions with p 4 bounded moments; in particular, the difference between the sample cost and distribution cost decays with m and p as mmin{ 1/4, 1/2+2/p} . The essential technical contribution is a mechanism to uniformly control deviations in the face of unbounded parameter sets, cost functions, and source distributions. To further demonstrate this mechanism, a soft clustering variant of k-means cost is also considered, namely the log likelihood of a Gaussian mixture, subject to the constraint that all covariance matrices have bounded spectrum. Lastly, a rate with reﬁned constants is provided for k-means instances possessing some cluster structure. 1</p><br/>
<h2>reference text</h2><p>[1] David Pollard. Strong consistency of k-means clustering. The Annals of Statistics, 9(1):135– 140, 1981.</p>
<p>[2] Gbor Lugosi and Kenneth Zeger. Rates of convergence in the source coding theorem, in empirical quantizer design, and in universal lossy source coding. IEEE Trans. Inform. Theory, 40: 1728–1740, 1994.</p>
<p>[3] Shai Ben-david. A framework for statistical clustering with a constant time approximation algorithms for k-median clustering. In COLT, pages 415–426. Springer, 2004.</p>
<p>[4] Alexander Rakhlin and Andrea Caponnetto. Stability of k-means clustering. In NIPS, pages 1121–1128, 2006.</p>
<p>[5] Richard O. Duda, Peter E. Hart, and David G. Stork. Pattern Classiﬁcation. Wiley, 2 edition, 2001.</p>
<p>[6] Thomas S. Ferguson. A course in large sample theory. Chapman & Hall, 1996.</p>
<p>[7] David Pollard. A central limit theorem for k-means clustering. The Annals of Probability, 10 (4):919–926, 1982.</p>
<p>[8] Shai Ben-david, Ulrike Von Luxburg, and D´ vid P´ l. A sober look at clustering stability. In In a a COLT, pages 5–19. Springer, 2006.</p>
<p>[9] Ohad Shamir and Naftali Tishby. Cluster stability for ﬁnite samples. In Annals of Probability, 10(4), pages 919–926, 1982.</p>
<p>[10] Ohad Shamir and Naftali Tishby. Model selection and stability in k-means clustering. In COLT, 2008.</p>
<p>[11] St´ phane Boucheron, Olivier Bousquet, and G´ bor Lugosi. Theory of classiﬁcation: a survey e a of recent advances. ESAIM: Probability and Statistics, 9:323–375, 2005.</p>
<p>[12] St´ phane Boucheron, G´ bor Lugosi, and Pascal Massart. Concentration Inequalities: A e a Nonasymptotic Theory of Independence. Oxford, 2013.</p>
<p>[13] Jon Wellner. Consistency and rates of convergence for maximum likelihood estimators via empirical process theory. 2005.</p>
<p>[14] Aad van der Vaart and Jon Wellner. Weak Convergence and Empirical Processes. Springer, 1996.</p>
<p>[15] FuChang Gao and Jon A. Wellner. On the rate of convergence of the maximum likelihood estimator of a k-monotone density. Science in China Series A: Mathematics, 52(7):1525–1538, 2009.</p>
<p>[16] Yair Al Censor and Stavros A. Zenios. Parallel Optimization: Theory, Algorithms and Applications. Oxford University Press, 1997.</p>
<p>[17] Arindam Banerjee, Srujana Merugu, Inderjit S. Dhillon, and Joydeep Ghosh. Clustering with Bregman divergences. Journal of Machine Learning Research, 6:1705–1749, 2005.</p>
<p>[18] Terence Tao. 254a notes 1: Concentration of measure, January 2010. URL http://terrytao.wordpress.com/2010/01/03/ 254a-notes-1-concentration-of-measure/.</p>
<p>[19] I. F. Pinelis and S. A. Utev. Estimates of the moments of sums of independent random variables. Teor. Veroyatnost. i Primenen., 29(3):554–557, 1984. Translation to English by Bernard Seckler.</p>
<p>[20] Shai Shalev-Shwartz. Online Learning: Theory, Algorithms, and Applications. PhD thesis, The Hebrew University of Jerusalem, July 2007.</p>
<p>[21] Jean-Baptiste Hiriart-Urruty and Claude Lemar´ chal. e Springer Publishing Company, Incorporated, 2001.  9  Fundamentals of Convex Analysis.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
