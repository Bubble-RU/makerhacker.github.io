<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>277 nips-2013-Restricting exchangeable nonparametric distributions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-277" href="../nips2013/nips-2013-Restricting_exchangeable_nonparametric_distributions.html">nips2013-277</a> <a title="nips-2013-277-reference" href="#">nips2013-277-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>277 nips-2013-Restricting exchangeable nonparametric distributions</h1>
<br/><p>Source: <a title="nips-2013-277-pdf" href="http://papers.nips.cc/paper/4884-restricting-exchangeable-nonparametric-distributions.pdf">pdf</a></p><p>Author: Sinead A. Williamson, Steve N. MacEachern, Eric Xing</p><p>Abstract: Distributions over matrices with exchangeable rows and inﬁnitely many columns are useful in constructing nonparametric latent variable models. However, the distribution implied by such models over the number of features exhibited by each data point may be poorly-suited for many modeling tasks. In this paper, we propose a class of exchangeable nonparametric priors obtained by restricting the domain of existing models. Such models allow us to specify the distribution over the number of features per data point, and can achieve better performance on data sets where the number of features is not well-modeled by the original distribution. 1</p><br/>
<h2>reference text</h2><p>´ ´e</p>
<p>[1] D. Aldous. Exchangeability and related topics. Ecole d’Et´ de Probabilit´ s de Saint-Flour e XIII, pages 1–198, 1985.</p>
<p>[2] D. J. Aldous. Representations for partially exchangeable arrays of random variables. Journal of Multivariate Analysis, 11(4):581–598, 1981.</p>
<p>[3] R. E. Barlow and K. D. Heidtmann. Computing k-out-of-n system reliability. IEEE Transactions on Reliability, 33:322–323, 1984.</p>
<p>[4] F. Caron. Bayesian nonparametric models for bipartite graphs. In Neural Information Processing Systems, 2012.</p>
<p>[5] S. X Chen, A. P. Dempster, and J. S. Liu. Weighted ﬁnite population sampling to maximize entropy. Biometrika, 81:457–469, 1994.</p>
<p>[6] S. X. Chen and J. S. Liu. Statistical applications of the Poisson-binomial and conditional Bernoulli distributions. Statistica Sinica, 7:875–892, 1997.</p>
<p>[7] F. Doshi-Velez and Z. Ghahramani. Accelerated Gibbs sampling for the Indian buffet process. In International Conference on Machine Learning, 2009.</p>
<p>[8] M. Fern´ ndez and S. Williams. Closed-form expression for the Poisson-binomial probability a density function. IEEE Transactions on Aerospace Electronic Systems, 46:803–817, 2010.</p>
<p>[9] S. Fortini, L. Ladelli, and E. Regazzini. Exchangeability, predictive distributions and parametric models. Sankhy¯ : The Indian Journal of Statistics, Series A, pages 86–109, 2000. a</p>
<p>[10] E. B. Fox, E. B. Sudderth, M. I. Jordan, and A. S. Willsky. Sharing features among dynamical systems with beta processes. In Neural Information Processing Systems, 2010.</p>
<p>[11] T. L. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the Indian buffet process. In Neural Information Processing Systems, 2005.</p>
<p>[12] J. F. C. Kingman. Completely random measures. Paciﬁc Journal of Mathematics, 21(1):59–78, 1967.</p>
<p>[13] K. T. Miller, T. L. Grifﬁths, and M. I. Jordan. Nonparametric latent feature models for link prediction. In Neural Information Processing Systems, 2009.</p>
<p>[14] R. M. Neal. Slice sampling. Annals of Statistics, 31(3):705–767, 2003.</p>
<p>[15] Y. W. Teh and D. G¨ r¨ r. Indian buffet processes with power law behaviour. In Neural Inforou mation Processing Systems, 2009.</p>
<p>[16] Y. W. Teh, D. G¨ r¨ r, and Z. Ghahramani. Stick-breaking construction for the Indian buffet ou process. In Artiﬁcial Intelligence and Statistics, 2007.</p>
<p>[17] R. Thibaux and M.I. Jordan. Hierarchical beta processes and the Indian buffet process. In Artiﬁcial Intelligence and Statistics, 2007.</p>
<p>[18] M. Titsias. The inﬁnite gamma-Poisson feature model. In Neural Information Processing Systems, 2007.</p>
<p>[19] A. Y. Volkova. A reﬁnement of the central limit theorem for sums of independent random indicators. Theory of Probability and its Applications, 40:791–794, 1996.</p>
<p>[20] F. Wood, T. L. Grifﬁths, and Z. Ghahramani. A non-parametric Bayesian method for inferring hidden causes. In Uncertainty in Artiﬁcial Intelligence, 2006.</p>
<p>[21] M. Zhou, L. A. Hannah, D. B. Dunson, and L. Carin. Beta-negative binomial process and Poisson factor analysis. In Artiﬁcial Intelligence and Statistics, 2012.</p>
<p>[22] G. K. Zipf. Selective Studies and the Principle of Relative Frequency in Language. Harvard University Press, 1932.  9</p>
<br/>
<br/><br/><br/></body>
</html>
