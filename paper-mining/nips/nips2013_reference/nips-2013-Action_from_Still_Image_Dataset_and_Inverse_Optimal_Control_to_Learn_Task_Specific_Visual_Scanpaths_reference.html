<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-21" href="../nips2013/nips-2013-Action_from_Still_Image_Dataset_and_Inverse_Optimal_Control_to_Learn_Task_Specific_Visual_Scanpaths.html">nips2013-21</a> <a title="nips-2013-21-reference" href="#">nips2013-21-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>21 nips-2013-Action from Still Image Dataset and Inverse Optimal Control to Learn Task Specific Visual Scanpaths</h1>
<br/><p>Source: <a title="nips-2013-21-pdf" href="http://papers.nips.cc/paper/5196-action-from-still-image-dataset-and-inverse-optimal-control-to-learn-task-specific-visual-scanpaths.pdf">pdf</a></p><p>Author: Stefan Mathe, Cristian Sminchisescu</p><p>Abstract: Human eye movements provide a rich source of information into the human visual information processing. The complex interplay between the task and the visual stimulus is believed to determine human eye movements, yet it is not fully understood, making it difﬁcult to develop reliable eye movement prediction systems. Our work makes three contributions towards addressing this problem. First, we complement one of the largest and most challenging static computer vision datasets, VOC 2012 Actions, with human eye movement recordings collected under the primary task constraint of action recognition, as well as, separately, for context recognition, in order to analyze the impact of different tasks. Our dataset is unique among the eyetracking datasets of still images in terms of large scale (over 1 million ﬁxations recorded in 9157 images) and different task controls. Second, we propose Markov models to automatically discover areas of interest (AOI) and introduce novel sequential consistency metrics based on them. Our methods can automatically determine the number, the spatial support and the transitions between AOIs, in addition to their locations. Based on such encodings, we quantitatively show that given unconstrained read-world stimuli, task instructions have signiﬁcant inﬂuence on the human visual search patterns and are stable across subjects. Finally, we leverage powerful machine learning techniques and computer vision features in order to learn task-sensitive reward functions from eye movement data within models that allow to effectively predict the human visual search patterns based on inverse optimal control. The methodology achieves state of the art scanpath modeling results. 1</p><br/>
<h2>reference text</h2><p>[1] E. Bazavan, F. Li, and C. Sminchisescu. Fourier kernel learning. In European Conference on Computer Vision, 2012.</p>
<p>[2] A. Borji and L. Itti. State-of-the-art in visual attention modelling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35, 2011.</p>
<p>[3] G. T. Buswell. How People Look at Pictures: A Study of the Psychology of Perception in Art. Chicago University Press, 1935.</p>
<p>[4] N. J. Butko and J. R. Movellan. Infomax control of eye movements. IEEE Transactions on Autonomous Mental Development, 2:91–107, 2010.</p>
<p>[5] M. S. Castelhano, M. L. Mack, and J. M. Henderson. Viewing task inﬂuences eye movement control during active scene perception. Journal of Vision, 9, 2008.</p>
<p>[6] M. Cerf, E. P. Frady, and C. Koch. Faces and text attract gaze independent of the task: Experimental data and computer model. Journal of Vision, 9, 2009.</p>
<p>[7] M. Cerf, J. Harel, W. Einhauser, and C. Koch. Predicting human gaze using low-level saliency combined with face detection. In Advances in Neural Information Processing Systems, 2007.</p>
<p>[8] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In IEEE International Conference on Computer Vision and Pattern Recognition, 2005.</p>
<p>[9] A. Dempster, N. Laird, and D. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, 1977.</p>
<p>[10] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html.</p>
<p>[11] L. Itti and C. Koch. A saliency-based search mechanism for overt and covert shifts of visual attention. Vision Research, 40, 2000.</p>
<p>[12] T. Judd, F. Durand, and A. Torralba. Fixations on low resolution images. In IEEE International Conference on Computer Vision, 2009.</p>
<p>[13] T. Judd, K. Ehinger, F. Durand, and A. Torralba. Learning to predict where humans look. In IEEE International Conference on Computer Vision, 2009.</p>
<p>[14] K.A.Ehinger, B.Sotelo, A.Torralba, and A.Oliva. Modeling search for people in 900 scenes: A combined source model of eye guidance. Visual Cognition, 17, 2009.</p>
<p>[15] W. Kienzle, B. Scholkopf, F. Wichmann, and M. Franz. How to ﬁnd interesting locations in video: a spatiotemporal interest point detector learned from human eye movements. In DAGM, 2007.</p>
<p>[16] M. F. Land and B. W. Tatler. Looking and Acting. Oxford University Press, 2009.</p>
<p>[17] F. Li, G. Lebanon, and C. Sminchisescu. Chebyshev approximations to the histogram χ2 kernel. In IEEE International Conference on Computer Vision and Pattern Recognition, 2012.</p>
<p>[18] E. Marinoiu, D. Papava, and C. Sminchisescu. Pictorial human spaces: How well do humans perceive a 3d articulated pose? In IEEE International Conference on Computer Vision, 2013.</p>
<p>[19] S. Mathe and C. Sminchisescu. Dynamic eye movement datasets and learnt saliency models for visual action recognition. In European Conference on Computer Vision, 2012.</p>
<p>[20] L. W. Renninger, J. Coughlan, P. Verghese, and J. Malik. An information maximization model of eye movements. In Advances in Neural Information Processing Systems, pages 1121–1128, 2004.</p>
<p>[21] R. Subramanian, H. Katti, N. Sebe, and T.-S. Kankanhalli, M. Chua. An eye ﬁxation database for saliency detection in images. In European Conference on Computer Vision, 2010.</p>
<p>[22] A. Torralba, A. Oliva, M. Castelhano, and J. Henderson. Contextual guidance of eye movements and attention in real-world scenes: The role of global features in object search. Psychological Review, 113, 2006.</p>
<p>[23] E. Vig, M. Dorr, and D. D. Cox. Space-variant descriptor sampling for action recognition based on saliency and eye movements. In European Conference on Computer Vision, 2012.</p>
<p>[24] S. Winkler and R. Subramanian. Overview of eye tracking datasets. In International Workshop on Quality of Multimedia Experience, 2013.</p>
<p>[25] A. Yarbus. Eye Movements and Vision. New York Plenum Press, 1967.</p>
<p>[26] K. Yun, Y. Pen, D. Samaras, G. J. Zelinsky, and T. L. Berg. Studying relationships between human gaze, description and computer vision. In IEEE International Conference on Computer Vision and Pattern Recognition, 2013.</p>
<p>[27] B. D. Ziebart, A. Maas, J. A. Bagnell, and A. K. Dey. Maximum entropy inverse reinforcement learning. In AAAI Conference on Artiﬁcial Intelligence, 2008.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
