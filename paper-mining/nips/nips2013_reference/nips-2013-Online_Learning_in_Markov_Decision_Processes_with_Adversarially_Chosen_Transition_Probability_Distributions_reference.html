<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-227" href="../nips2013/nips-2013-Online_Learning_in_Markov_Decision_Processes_with_Adversarially_Chosen_Transition_Probability_Distributions.html">nips2013-227</a> <a title="nips-2013-227-reference" href="#">nips2013-227-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>227 nips-2013-Online Learning in Markov Decision Processes with Adversarially Chosen Transition Probability Distributions</h1>
<br/><p>Source: <a title="nips-2013-227-pdf" href="http://papers.nips.cc/paper/4975-online-learning-in-markov-decision-processes-with-adversarially-chosen-transition-probability-distributions.pdf">pdf</a></p><p>Author: Yasin Abbasi, Peter Bartlett, Varun Kanade, Yevgeny Seldin, Csaba Szepesvari</p><p>Abstract: We study the problem of online learning Markov Decision Processes (MDPs) when both the transition distributions and loss functions are chosen by an adversary. We present an algorithm that, under a mixing assumption, achieves p O( T log |⇧| + log |⇧|) regret with respect to a comparison set of policies ⇧. The regret is independent of the size of the state and action spaces. When expectations over sample paths can be computed efﬁciently and the comparison set ⇧ has polynomial size, this algorithm is efﬁcient. We also consider the episodic adversarial online shortest path problem. Here, in each episode an adversary may choose a weighted directed acyclic graph with an identiﬁed start and ﬁnish node. The goal of the learning algorithm is to choose a path that minimizes the loss while traversing from the start to ﬁnish node. At the end of each episode the loss function (given by weights on the edges) is revealed to the learning algorithm. The goal is to minimize regret with respect to a ﬁxed policy for selecting paths. This problem is a special case of the online MDP problem. It was shown that for randomly chosen graphs and adversarial losses, the problem can be efﬁciently solved. We show that it also can be efﬁciently solved for adversarial graphs and randomly chosen losses. When both graphs and losses are adversarially chosen, we show that designing efﬁcient algorithms for the adversarial online shortest path problem (and hence for the adversarial MDP problem) is as hard as learning parity with noise, a notoriously difﬁcult problem that has been used to design efﬁcient cryptographic schemes. Finally, we present an efﬁcient algorithm whose regret scales linearly with the number of distinct graphs. 1</p><br/>
<h2>reference text</h2><p>[1] Nicol` Cesa-Bianchi and G´ bor Lugosi. Prediction, Learning, and Games. Cambridge University Press, o a New York, NY, USA, 2006. P 4 Thus, `t (Gt , ⇡(Gt )) = L ct (n⇡ , ⇡). t,l l=1  8</p>
<p>[2] Apostolos N. Burnetas and Michael N. Katehakis. Optimal adaptive policies for Markov decision processes. Mathematics of Operations Research, 22(1):222–255, 1997.</p>
<p>[3] T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11:1563—1600, 2010.</p>
<p>[4] P. L. Bartlett and A. Tewari. REGAL: A regularization based algorithm for reinforcement learning in weakly communicating MDPs. In UAI, 2009.</p>
<p>[5] Yasin Abbasi-Yadkori and Csaba Szepesv´ ri. Regret bounds for the adaptive control of linear quadratic a systems. In COLT, 2011.</p>
<p>[6] Yasin Abbasi-Yadkori. Online Learning for Linearly Parametrized Control Problems. PhD thesis, University of Alberta, 2012.</p>
<p>[7] Ronald Ortner and Daniil Ryabko. Online regret bounds for undiscounted continuous reinforcement learning. In NIPS, 2012.</p>
<p>[8] Eyal Even-Dar, Sham M. Kakade, and Yishay Mansour. Experts in a Markov decision process. In NIPS, 2004.</p>
<p>[9] Jia Yuan Yu and Shie Mannor. Arbitrarily modulated Markov decision processes. In IEEE Conference on Decision and Control, 2009.</p>
<p>[10] Jia Yuan Yu and Shie Mannor. Online learning in Markov decision processes with arbitrarily changing rewards and transitions. In GameNets, 2009.</p>
<p>[11] Gergely Neu, Andr´ s Gy¨ rgy, and Csaba Szepesv´ ri. The adversarial stochastic shortest path problem a o a with unknown transition probabilities. In AISTATS, 2012.</p>
<p>[12] Eyal Even-Dar, Sham M. Kakade, and Yishay Mansour. Online Markov decision processes. Mathematics of Operations Research, 34(3):726–736, 2009.</p>
<p>[13] Eyal Even-Dar. Personal communication., 2013.</p>
<p>[14] Gergely Neu, Andr´ s Gy¨ rgy, Csaba Szepesv´ ri, and Andr´ s Antos. Online Markov decision processes a o a a under bandit feedback. In NIPS, 2010.</p>
<p>[15] Vladimir Vovk. Aggregating strategies. In COLT, pages 372–383, 1990.</p>
<p>[16] Nick Littlestone and Manfred K. Warmuth. The weighted majority algorithm. Information and Computation, 108(2):212–261, 1994.</p>
<p>[17] Sascha Geulen, Berthold V¨ cking, and Melanie Winkler. Regret minimization for online buffering probo lems using the weighted majority algorithm. In COLT, 2010.</p>
<p>[18] Adam Kalai and Santosh Vempala. Efﬁcient algorithms for online decision problems. Journal of Computer and System Sciences, 71(3):291–307, 2005.</p>
<p>[19] Gergely Neu, Andr´ s Gy¨ rgy, and Csaba Szepesv´ ri. The online loop-free stochastic shortest path proba o a lem. In COLT, 2010.</p>
<p>[20] Adam Tauman Kalai, Yishay Mansour, and Elad Verbin. On agnostic boosting and parity learning. In STOC, pages 629–638, 2008.</p>
<p>[21] Oded Regev. On lattices, learning with errors, random linear codes, and cryptography. In STOC, pages 84–93, 2005.</p>
<p>[22] Gergely Neu, Andr´ s Gy¨ rgy, and Csaba Szepesv´ ri. The adversarial stochastic shortest path problem a o a with unknown transition probabilities. In AISTATS, 2012.</p>
<p>[23] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. Journal of Machine Learning Research, 2002.</p>
<p>[24] V. Dani, T. P. Hayes, and S. M. Kakade. Stochastic linear optimization under bandit feedback. In Rocco Servedio and Tong Zhang, editors, COLT, pages 355–366, 2008.</p>
<p>[25] Yasin Abbasi-Yadkori, D´ vid P´ l, and Csaba Szepesv´ ri. Improved algorithms for linear stochastic bana a a dits. In NIPS, 2011.</p>
<p>[26] Nick Littlestone. From on-line to batch learning. In COLT, pages 269–284, 1989.</p>
<p>[27] Varun Kanade and Thomas Steinke. Learning hurdles for sleeping experts. In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ITCS ’12, pages 11–18, 2012.  9</p>
<br/>
<br/><br/><br/></body>
</html>
