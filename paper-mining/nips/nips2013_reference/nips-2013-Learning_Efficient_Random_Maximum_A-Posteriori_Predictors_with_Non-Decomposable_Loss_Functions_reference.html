<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-152" href="../nips2013/nips-2013-Learning_Efficient_Random_Maximum_A-Posteriori_Predictors_with_Non-Decomposable_Loss_Functions.html">nips2013-152</a> <a title="nips-2013-152-reference" href="#">nips2013-152-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>152 nips-2013-Learning Efficient Random Maximum A-Posteriori Predictors with Non-Decomposable Loss Functions</h1>
<br/><p>Source: <a title="nips-2013-152-pdf" href="http://papers.nips.cc/paper/5066-learning-efficient-random-maximum-a-posteriori-predictors-with-non-decomposable-loss-functions.pdf">pdf</a></p><p>Author: Tamir Hazan, Subhransu Maji, Joseph Keshet, Tommi Jaakkola</p><p>Abstract: In this work we develop efﬁcient methods for learning random MAP predictors for structured label problems. In particular, we construct posterior distributions over perturbations that can be adjusted via stochastic gradient methods. We show that any smooth posterior distribution would sufﬁce to deﬁne a smooth PAC-Bayesian risk bound suitable for gradient methods. In addition, we relate the posterior distributions to computational properties of the MAP predictors. We suggest multiplicative posteriors to learn super-modular potential functions that accompany specialized MAP predictors such as graph-cuts. We also describe label-augmented posterior models that can use efﬁcient MAP approximations, such as those arising from linear program relaxations. 1</p><br/>
<h2>reference text</h2><p>[1] Andrew Blake, Carsten Rother, Matthew Brown, Patrick Perez, and Philip Torr. Interactive image segmentation using an adaptive gmmrf model. In ECCV 2004, pages 428–441. 2004.</p>
<p>[2] Y. Boykov, O. Veksler, and R. Zabih. Fast approximate energy minimization via graph cuts. PAMI, 2001.</p>
<p>[3] O. Catoni. Pac-bayesian supervised classiﬁcation: the thermodynamics of statistical learning. arXiv preprint arXiv:0712.0248, 2007.</p>
<p>[4] G.B. Folland. Real analysis: Modern techniques and their applications, john wiley & sons. New York, 1999.</p>
<p>[5] P. Germain, A. Lacasse, F. Laviolette, and M. Marchand. Pac-bayesian learning of linear classiﬁers. In ICML, pages 353–360. ACM, 2009.</p>
<p>[6] A. Globerson and T. S. Jaakkola. Fixing max-product: Convergent message passing algorithms for MAP LP-relaxations. Advances in Neural Information Processing Systems, 21, 2007.</p>
<p>[7] L.A. Goldberg and M. Jerrum. The complexity of ferromagnetic ising with local ﬁelds. Combinatorics Probability and Computing, 16(1):43, 2007.</p>
<p>[8] T. Hazan and T. Jaakkola. On the partition function and random maximum a-posteriori perturbations. In Proceedings of the 29th International Conference on Machine Learning, 2012.</p>
<p>[9] T. Hazan, S. Maji, and T. Jaakkola. On sampling from the gibbs distribution with random maximum a-posteriori perturbations. Advances in Neural Information Processing Systems, 2013.</p>
<p>[10] J. Keshet, D. McAllester, and T. Hazan. Pac-bayesian approach for minimization of phoneme error rate. In ICASSP, 2011.</p>
<p>[11] John Langford and John Shawe-Taylor. Pac-bayes & margins. Advances in neural information processing systems, 15:423–430, 2002.</p>
<p>[12] Erich Leo Lehmann and George Casella. Theory of point estimation, volume 31. 1998.</p>
<p>[13] Andreas Maurer. A note on the pac bayesian theorem. arXiv preprint cs/0411099, 2004.</p>
<p>[14] D. McAllester. Simpliﬁed pac-bayesian margin bounds. Learning Theory and Kernel Machines, pages 203–215, 2003.</p>
<p>[15] D. McAllester, T. Hazan, and J. Keshet. Direct loss minimization for structured prediction. Advances in Neural Information Processing Systems, 23:1594–1602, 2010.</p>
<p>[16] Francesco Orabona, Tamir Hazan, Anand D Sarwate, and Tommi. Jaakkola. On measure concentration of random maximum a-posteriori perturbations. arXiv:1310.4227, 2013.</p>
<p>[17] G. Papandreou and A. Yuille. Perturb-and-map random ﬁelds: Using discrete optimization to learn and sample from energy models. In ICCV, Barcelona, Spain, November 2011.</p>
<p>[18] A.M. Rush and M. Collins. A tutorial on dual decomposition and lagrangian relaxation for inference in natural language processing.</p>
<p>[19] Matthias Seeger. Pac-bayesian generalisation error bounds for gaussian process classiﬁcation. The Journal of Machine Learning Research, 3:233–269, 2003.</p>
<p>[20] Yevgeny Seldin. A PAC-Bayesian Approach to Structure Learning. PhD thesis, 2009.</p>
<p>[21] D. Sontag, T. Meltzer, A. Globerson, T. Jaakkola, and Y. Weiss. Tightening LP relaxations for MAP using message passing. In Conf. Uncertainty in Artiﬁcial Intelligence (UAI), 2008.</p>
<p>[22] Martin Szummer, Pushmeet Kohli, and Derek Hoiem. Learning crfs using graph cuts. In Computer Vision–ECCV 2008, pages 582–595. Springer, 2008.</p>
<p>[23] D. Tarlow, R.P. Adams, and R.S. Zemel. Randomized optimum models for structured prediction. In AISTATS, pages 21–23, 2012.</p>
<p>[24] Daniel Tarlow and Richard S Zemel. Structured output learning with high order loss functions. In International Conference on Artiﬁcial Intelligence and Statistics, pages 1212–1220, 2012.</p>
<p>[25] B. Taskar, C. Guestrin, and D. Koller. Max-margin Markov networks. Advances in neural information processing systems, 16:51, 2004.</p>
<p>[26] Max Welling. Herding dynamical weights to learn. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 1121–1128. ACM, 2009.  9</p>
<br/>
<br/><br/><br/></body>
</html>
