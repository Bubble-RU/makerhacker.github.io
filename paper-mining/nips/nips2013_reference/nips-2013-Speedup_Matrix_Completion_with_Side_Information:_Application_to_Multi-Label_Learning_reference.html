<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-307" href="../nips2013/nips-2013-Speedup_Matrix_Completion_with_Side_Information%3A_Application_to_Multi-Label_Learning.html">nips2013-307</a> <a title="nips-2013-307-reference" href="#">nips2013-307-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>307 nips-2013-Speedup Matrix Completion with Side Information: Application to Multi-Label Learning</h1>
<br/><p>Source: <a title="nips-2013-307-pdf" href="http://papers.nips.cc/paper/4999-speedup-matrix-completion-with-side-information-application-to-multi-label-learning.pdf">pdf</a></p><p>Author: Miao Xu, Rong Jin, Zhi-Hua Zhou</p><p>Abstract: In standard matrix completion theory, it is required to have at least O(n ln2 n) observed entries to perfectly recover a low-rank matrix M of size n × n, leading to a large number of observations when n is large. In many real tasks, side information in addition to the observed entries is often available. In this work, we develop a novel theory of matrix completion that explicitly explore the side information to reduce the requirement on the number of observed entries. We show that, under appropriate conditions, with the assistance of side information matrices, the number of observed entries needed for a perfect recovery of matrix M can be dramatically reduced to O(ln n). We demonstrate the effectiveness of the proposed approach for matrix completion in transductive incomplete multi-label learning. 1</p><br/>
<h2>reference text</h2><p>[1] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. A new approach to collaborative ﬁltering: Operator estimation with spectral regularization. JMLR, 10:803–826, 2009.</p>
<p>[2] R. Adams, G. Dahl, and I. Murray. Incorporating side information in probabilistic matrix factorization with gaussian processes. In UAI, 2010.</p>
<p>[3] D. Agarwal and B.-C. Chen. Regression-based latent factor models. In KDD, 2009.</p>
<p>[4] A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. MLJ, 73(3):243–272, 2008.</p>
<p>[5] H. Avron, S. Kale, S. Kasiviswanathan, and V. Sindhwani. Efﬁcient and practical stochastic subgradient descent for nuclear norm regularization. In ICML, 2012.</p>
<p>[6] F. Bach. Consistency of trace norm minimization. JMLR, 9:1019–1048, 2008.</p>
<p>[7] M. R. Boutell, J. Luo, X. Shen, and C. M. Brown. Learning multi-label scene classiﬁcation. Pattern Recognition, 37(9):1757–1771, 2004.</p>
<p>[8] S. Bucak, R. Jin, and A. Jain. Multi-label learning with incomplete class assignments. In CVPR, 2011.</p>
<p>[9] R. Cabral, F. Torre, J. Costeira, and A. Bernardino. Matrix completion for multi-label image classiﬁcation. In NIPS, 2011.</p>
<p>[10] J.-F. Cai, E. Cand` s, and Z. Shen. A singular value thresholding algorithm for matrix completion. SIAM e Journal on Optimization, 20(4):1956–1982, 2010.</p>
<p>[11] E. Cand` s and B. Recht. Exact matrix completion via convex optimization. CACM, 55(6):111–119, 2012. e</p>
<p>[12] E. Cand` s and T. Tao. The power of convex relaxation: near-optimal matrix completion. IEEE TIT, e 56(5):2053–2080, 2010.</p>
<p>[13] C.-C. Chang and C.-J. Lin. Libsvm: A library for support vector machines. ACM TIST, 2(3):27, 2011.</p>
<p>[14] T.-S. Chua, J. Tang, R. Hong, H. Li, Z. Luo, and Y.-T. Zheng. Nus-wide: A real-world web image database from national university of singapore. In CIVR, 2009.</p>
<p>[15] B. Eriksson, L. Balzano, and R. Nowak. High-rank matrix completion and subspace clustering with missing data. CoRR, 2011.  7  Table 2: Results on transductive incomplete multi-label learning. Algo. speciﬁes the name of the algorithms. Time is the CPU time measured in seconds. AP is Average Precision measured based on test data; the higher the AP, the better the performance. ω% represents the percentage of training instances with observed label assignment for each label. The best result and its comparable ones (pairwise single-tailed t-tests at 95% conﬁdence level) are bolded. Maxide MC-b MC-1 BR-R BR-1 Maxide MC-b MC-1 BR-R BR-1 Maxide MC-b MC-1 BR-R BR-1 Maxide MC-b MC-1 BR-R BR-1 Maxide MC-b MC-1 BR-R BR-1 Maxide MC-b MC-1 BR-R BR-1 Maxide MC-b MC-1 BR-R BR-1 Maxide MC-b MC-1 BR-R BR-1 Maxide MC-b MC-1 BR-R BR-1 Maxide MC-b MC-1 BR-R BR-1 Maxide MC-b MC-1 BR-R BR-1  ω% = 10% time AP 3.09 × 100 0.548 2.47 × 104 0.428 2.39 × 104 0.430 1 1.63 × 10 0.540 1.77 × 101 0.540 3.24 × 100 0.868 2.94 × 104 0.865 3.25 × 104 0.865 1 1.02 × 10 0.846 1 1.19 × 10 0.846 4.67 × 100 0.635 5.58 × 104 0.597 6.56 × 104 0.600 1 2.34 × 10 0.622 1 2.70 × 10 0.621 4.40 × 100 0.566 3.82 × 104 0.472 4.68 × 104 0.484 1.77 × 101 0.535 1 1.94 × 10 0.535 0 2.77 × 10 0.631 4.86 × 104 0.474 4.40 × 104 0.489 1.89 × 101 0.628 2.04 × 101 0.627 4.31 × 100 0.725 4 4.98 × 10 0.609 5.82 × 104 0.626 2.03 × 101 0.725 2.16 × 101 0.725 2.75 × 100 0.559 4 3.56 × 10 0.381 3.48 × 104 0.381 1.97 × 101 0.548 2.24 × 101 0.547 5.11 × 100 0.635 4 9.38 × 10 0.565 1.11 × 105 0.576 1 2.28 × 10 0.644 2.71 × 101 0.644 6.21 × 100 0.513 6.80 × 104 0.395 8.50 × 104 0.411 1 2.93 × 10 0.506 1 3.60 × 10 0.506 7.18 × 100 0.721 1.71 × 105 0.582 2.22 × 105 0.602 1 3.09 × 10 0.717 1 3.71 × 10 0.717 3.69 × 100 0.580 4.75 × 104 0.550 4.14 × 104 0.550 2.50 × 101 0.571 1 2.84 × 10 0.572  ω% = 20% time AP 3.60 × 100 0.572 1.59 × 104 0.444 2.05 × 104 0.494 1 2.98 × 10 0.563 3.07 × 101 0.563 3.89 × 100 0.860 1.83 × 104 0.851 2.18 × 104 0.855 1 1.78 × 10 0.841 1 1.96 × 10 0.841 5.81 × 100 0.660 3.38 × 104 0.599 4.40 × 104 0.608 1 4.13 × 10 0.649 1 4.50 × 10 0.648 5.41 × 100 0.604 2.40 × 104 0.478 3.02 × 104 0.536 3.16 × 101 0.568 1 3.28 × 10 0.568 0 3.41 × 10 0.650 3.13 × 104 0.467 4.15 × 104 0.492 3.38 × 101 0.638 3.44 × 101 0.640 0 5.36 × 10 0.746 4 2.99 × 10 0.607 3.82 × 104 0.632 3.61 × 101 0.742 3.59 × 101 0.741 3.38 × 100 0.592 4 2.41 × 10 0.381 3.25 × 104 0.430 3.48 × 101 0.574 3.74 × 101 0.573 6.47 × 100 0.666 4 5.38 × 10 0.561 6.53 × 104 0.576 1 3.89 × 10 0.670 4.34 × 101 0.669 7.67 × 100 0.543 3.94 × 104 0.403 4.97 × 104 0.470 1 5.06 × 10 0.535 1 5.91 × 10 0.535 9.09 × 100 0.748 9.65 × 104 0.595 1.17 × 105 0.625 1 5.35 × 10 0.746 1 6.00 × 10 0.746 4.54 × 100 0.594 2.93 × 104 0.545 3.65 × 104 0.561 4.54 × 101 0.590 1 4.92 × 10 0.590  ω% = 40% time AP 4.42 × 100 0.596 9.54 × 103 0.434 1.27 × 104 0.473 1 5.71 × 10 0.574 7.10 × 101 0.575 5.04 × 100 0.872 1.08 × 104 0.858 1.21 × 104 0.862 1 3.32 × 10 0.854 1 4.30 × 10 0.854 7.79 × 100 0.675 1.87 × 104 0.604 2.30 × 104 0.618 1 7.68 × 10 0.662 1 8.25 × 10 0.661 6.73 × 100 0.618 1.32 × 104 0.474 1.55 × 104 0.564 6.01 × 101 0.583 1 6.94 × 10 0.583 0 4.56 × 10 0.679 1.73 × 104 0.468 2.27 × 104 0.578 6.47 × 101 0.668 6.41 × 101 0.667 0 7.11 × 10 0.769 4 1.71 × 10 0.610 2.03 × 104 0.645 6.83 × 101 0.757 7.05 × 101 0.757 0 4.44 × 10 0.614 4 1.30 × 10 0.378 1.90 × 104 0.421 6.53 × 101 0.596 6.86 × 101 0.596 8.49 × 100 0.696 4 2.75 × 10 0.575 3.22 × 104 0.575 1 7.08 × 10 0.693 7.48 × 101 0.692 1.02 × 101 0.568 2.06 × 104 0.394 2.52 × 104 0.414 1 9.30 × 10 0.557 2 1.04 × 10 0.557 1.21 × 101 0.754 4.56 × 104 0.594 5.41 × 104 0.604 1 9.74 × 10 0.751 2 1.02 × 10 0.751 5.80 × 100 0.616 1.62 × 104 0.552 2.04 × 104 0.590 8.59 × 101 0.600 1 9.58 × 10 0.601  Maxide BR-1 Maxide BR-1  1.47 × 103 1.24 × 102 1.33 × 104 2.48 × 104  2.10 × 103 2.38 × 102 1.89 × 104 4.74 × 104  3.53 × 103 4.81 × 102 2.67 × 104 1.11 × 105  Data  Algo.  Arts  Business  Computers  Education  Entertainment  Health  Recreation  Reference  Science  Social  Society  NUS-WIDE Flickr  0.513 0.329 0.124 0.064  0.519 0.398 0.124 0.074  0.522 0.466 0.124 0.077</p>
<p>[16] Y. Fang and L. Si. Matrix co-factorization for recommendation with rich side information and implicit feedback. In Proceedings of the 2nd International Workshop on Information Heterogeneity and Fusion in  8  Recommender Systems, 2011.</p>
<p>[17] A. Goldberg, X. Zhu, B. Recht, J.-M. Xu, and R. Nowak. Transduction with matrix completion: Three birds with one stone. In NIPS, 2010.</p>
<p>[18] Y. Guo and D. Schuurmans. Semi-supervised multi-label classiﬁcation - a simultaneous large-margin, subspace learning approach. In ECML, 2012.</p>
<p>[19] P. Jain, P. Netrapalli, and S. Sanghavi. Provable matrix sensing using alternating minimization. In NIPS Workshop on Optimization for Machine Learning, 2012.</p>
<p>[20] A. Jalali, Y. Chen, S. Sanghavi, and H. Xu. Clustering partially observed graphs via convex optimization. In ICML, 2011.</p>
<p>[21] S. Ji, L. Tang, S. Yu, and J. Ye. Extracting shared subspace for multi-label classiﬁcation. In KDD, 2008.</p>
<p>[22] S. Ji and J. Ye. An accelerated gradient method for trace norm minimization. In ICML, 2009.</p>
<p>[23] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE TIT, 56(6):2980– 2998, 2010.</p>
<p>[24] X. Kong, M. Ng, and Z.-H. Zhou. Transductive multi-label learning via label set propagation. IEEE TKDE, 25(3):704–719, 2013.</p>
<p>[25] Z. Lin, M. Chen, L. Wu, and Y. Ma. The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices. Technical report, UIUC, 2009.</p>
<p>[26] Y. Liu, R. Jin, and L. Yang. Semi-supervised multi-label learning by constrained non-negative matrix factorization. In AAAI, 2006.</p>
<p>[27] S. Ma, D. Goldfarb, and L. Chen. Fixed point and bregman iterative methods for matrix rank minimization. Mathematical Programming, 128(1-2):321–353, 2011.</p>
<p>[28] R. Mazumder, T. Hastie, and R. Tibshirani. Spectral regularization algorithms for learning large incomplete matrices. JMLR, 11:2287–2322, 2010.</p>
<p>[29] A. Menon, K. Chitrapura, S. Garg, D. Agarwal, and N. Kota. Response prediction using collaborative ﬁltering with hierarchies and side-information. In KDD, 2011.</p>
<p>[30] S. Negahban and M. Wainwright. Estimation of (near) low-rank matrices with noise and high dimensional scaling. Annual of Statistics, 39(2):1069–1097, 2011.</p>
<p>[31] G. Obozinski, B. Taskar, and M. Jordan. Joint covariate selection and joint subspace selection for multiple classiﬁcation problems. Statistics and Computing, 20(2):231–252, 2010.</p>
<p>[32] W. Pan, E. Xiang, N. Liu, and Q. Yang. Transfer learning in collaborative ﬁltering for sparsity reduction. In AAAI, 2010.</p>
<p>[33] I. Porteous, A. Asuncion, and M. Welling. Bayesian matrix factorization with side information and dirichlet process mixtures. In AAAI, 2010.</p>
<p>[34] B. Recht. A simpler approach to matrix completion. JMLR, 12:3413–3430, 2011.</p>
<p>[35] J. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction. In ICML, 2005.</p>
<p>[36] A. Rhode and A. Tsybakov. Estimation of high dimensional low rank matrices. Annual of Statistics, 39(2):887–930, 2011.</p>
<p>[37] N. Srebro, Jason D. Rennie, and T. Jaakkola. Maximum-margin matrix factorization. In NIPS. 2005.</p>
<p>[38] Y.-Y. Sun, Y. Zhang, and Z.-H. Zhou. Multi-label learning with weak label. In AAAI, 2010.</p>
<p>[39] K.-C. Toh and Y. Sangwoon. An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems. Paciﬁc Journal of Optimization, 2010.</p>
<p>[40] N. Ueda and K. Saito. Parametric mixture models for multi-labeled text. In NIPS, 2002.</p>
<p>[41] K. Weinberger and L. Saul. Unsupervised learning of image manifolds by semideﬁnite programming. IJCV, 70(1):77–90, 2006.</p>
<p>[42] J. Yi, T. Yang, R. Jin, A. Jain, and M. Mahdavi. Robust ensemble clustering by matrix completion. In ICDM, 2012.</p>
<p>[43] G. Yu, C. Domeniconi, H. Rangwala, G. Zhang, and Z. Yu. Transductive multi-label ensemble classiﬁcation for protein function prediction. In KDD, 2012.</p>
<p>[44] M.-L. Zhang and Z.-H. Zhou. A review on multi-label learning algorithms. IEEE TKDE, in press.</p>
<p>[45] J. Zhuang and S. Hoi. A two-view learning approach for image tag ranking. In WSDM, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
