<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-39" href="../nips2013/nips-2013-Approximate_Gaussian_process_inference_for_the_drift_function_in_stochastic_differential_equations.html">nips2013-39</a> <a title="nips-2013-39-reference" href="#">nips2013-39-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>39 nips-2013-Approximate Gaussian process inference for the drift function in stochastic differential equations</h1>
<br/><p>Source: <a title="nips-2013-39-pdf" href="http://papers.nips.cc/paper/4967-approximate-gaussian-process-inference-for-the-drift-function-in-stochastic-differential-equations.pdf">pdf</a></p><p>Author: Andreas Ruttor, Philipp Batz, Manfred Opper</p><p>Abstract: We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from sparse observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process of the Ornstein-Uhlenbeck type and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression. 1</p><br/>
<h2>reference text</h2><p>[1] Michalis K. Titsias. Variational learning of inducing variables in sparse Gaussian processes. JMLR WC&P;, 5:567–574, 2009.</p>
<p>[2] Marc Deisenroth and Shakir Mohamed. Expectation propagation in Gaussian process dynamical systems. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 2618–2626. 2012.</p>
<p>[3] Jonathan Ko and Dieter Fox. GP-BayesFilters: Bayesian ﬁltering using Gaussian process prediction and observation models. Autonomous Robots, 27(1):75–90, July 2009.</p>
<p>[4] C´ dric Archambeau, Manfred Opper, Yuan Shen, Dan Cornford, and John Shawe-Taylor. Variational e inference for diffusion processes. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 17–24. MIT Press, Cambridge, MA, 2008.</p>
<p>[5] Jos´ Bento Ayres Pereira, Morteza Ibrahimi, and Andrea Montanari. Learning networks of stochastic e differential equations. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 172–180. 2010.</p>
<p>[6] Danilo J. Rezende, Daan Wierstra, and Wulfram Gerstner. Variational learning for recurrent spiking networks. In J. Shawe-Taylor, R.S. Zemel, P. Bartlett, F.C.N. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 24, pages 136–144. 2011.</p>
<p>[7] Simon Lyons, Amos Storkey, and Simo Sarkka. The coloured noise expansion and parameter estimation of diffusion processes. In P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1961–1969. 2012.</p>
<p>[8] Omiros Papaspiliopoulos, Yvo Pokern, Gareth O. Roberts, and Andrew M. Stuart. Nonparametric estimation of diffusions: a differential equations approach. Biometrika, 99(3):511–531, 2012.</p>
<p>[9] Yvo Pokern, Andrew M. Stuart, and J.H. van Zanten. Posterior consistency via precision operators for Bayesian nonparametric drift estimation in SDEs. Stochastic Processes and their Applications, 123(2):603–628, 2013.</p>
<p>[10] P. E. Kloeden and E. Platen. Numerical Solution of Stochastic Differential Equations. Springer, New York, corrected edition, June 2011.</p>
<p>[11] C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT Press, 2006.</p>
<p>[12] Lehel Csat´ , Manfred Opper, and Ole Winther. TAP Gibbs free energy, belief propagation and sparsity. o In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 657–663. MIT Press, 2002.</p>
<p>[13] C. W. Gardiner. Handbook of Stochastic Methods. Springer, Berlin, second edition, 1996.</p>
<p>[14] Manfred Opper, Andreas Ruttor, and Guido Sanguinetti. Approximate inference in continuous time Gaussian-jump processes. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1831–1839. 2010.</p>
<p>[15] Florian Stimberg, Manfred Opper, and Andreas Ruttor. Bayesian inference for change points in dynamical systems with reusable states—a Chinese restaurant process approach. JMLR WC&P;, 22:1117–1124, 2012.</p>
<p>[16] Frank Kwasniok. Analysis and modelling of glacial climate transitions using simple dynamical systems. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 371(1991), 2013.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
