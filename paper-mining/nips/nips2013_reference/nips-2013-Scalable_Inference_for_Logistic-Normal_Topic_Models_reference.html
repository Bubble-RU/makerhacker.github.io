<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>287 nips-2013-Scalable Inference for Logistic-Normal Topic Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-287" href="../nips2013/nips-2013-Scalable_Inference_for_Logistic-Normal_Topic_Models.html">nips2013-287</a> <a title="nips-2013-287-reference" href="#">nips2013-287-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>287 nips-2013-Scalable Inference for Logistic-Normal Topic Models</h1>
<br/><p>Source: <a title="nips-2013-287-pdf" href="http://papers.nips.cc/paper/4981-scalable-inference-for-logistic-normal-topic-models.pdf">pdf</a></p><p>Author: Jianfei Chen, June Zhu, Zi Wang, Xun Zheng, Bo Zhang</p><p>Abstract: Logistic-normal topic models can effectively discover correlation structures among latent topics. However, their inference remains a challenge because of the non-conjugacy between the logistic-normal prior and multinomial topic mixing proportions. Existing algorithms either make restricting mean-ﬁeld assumptions or are not scalable to large-scale applications. This paper presents a partially collapsed Gibbs sampling algorithm that approaches the provably correct distribution by exploring the ideas of data augmentation. To improve time efﬁciency, we further present a parallel implementation that can deal with large-scale applications and learn the correlation structures of thousands of topics from millions of documents. Extensive empirical results demonstrate the promise. 1</p><br/>
<h2>reference text</h2><p>[1] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and A. Smola. Scalable inference in latent variable models. In International Conference on Web Search and Data Mining (WSDM), 2012.</p>
<p>[2] K. Bache and M. Lichman. UCI machine learning repository, 2013.</p>
<p>[3] D. Blei and J. Lafferty. Correlated topic models. In Advances in Neural Information Processing Systems (NIPS), 2006.</p>
<p>[4] D. Blei and J. Lafferty. Dynamic topic models. In International Conference on Machine Learning (ICML), 2006.</p>
<p>[5] D.M. Blei, A.Y. Ng, and M.I. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, 2003.</p>
<p>[6] N. Chen, J. Zhu, F. Xia, and B. Zhang. Generalized relational topic models with data augmentation. In International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2013.</p>
<p>[7] M. Hoffman, D. Blei, and F. Bach. Online learning for latent Dirichlet allocation. In Advances in Neural Information Processing Systems (NIPS), 2010.</p>
<p>[8] C. Holmes and L. Held. Bayesian auxiliary variable models for binary and multinomial regression. Bayesian Analysis, 1(1):145–168, 2006.</p>
<p>[9] D. Mimno, H. Wallach, and A. McCallum. Gibbs sampling for logistic normal topic models with graph-based priors. In NIPS Workshop on Analyzing Graphs, 2008.</p>
<p>[10] D. Newman, A. Asuncion, P. Smyth, and M. Welling. Distributed algorithms for topic models. Journal of Machine Learning Research, (10):1801–1828, 2009.</p>
<p>[11] J. Paisley, C. Wang, and D. Blei. The discrete inﬁnite logistic normal distribution for mixedmembership modeling. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2011.</p>
<p>[12] N. G. Polson and J. G. Scott. Default bayesian analysis for multi-way tables: a dataaugmentation approach. arXiv:1109.4180, 2011.</p>
<p>[13] N. G. Polson, J. G. Scott, and J. Windle. Bayesian inference for logistic models using PolyaGamma latent variables. arXiv:1205.0310v2, 2013.</p>
<p>[14] C. P. Robert. Simulation of truncated normal variables. Statistics and Compuating, 5:121–125, 1995.</p>
<p>[15] W. Rudin. Principles of mathematical analysis. McGraw-Hill Book Co., 1964.</p>
<p>[16] A. Smola and S. Narayanamurthy. An architecture for parallel topic models. Very Large Data Base (VLDB), 2010.</p>
<p>[17] M. A. Tanner and W. H. Wong. The calculation of posterior distributions by data augmentation. Journal of the Americal Statistical Association, 82(398):528–540, 1987.</p>
<p>[18] D. van Dyk and X. Meng. The art of data augmentation. Journal of Computational and Graphical Statistics, 10(1):1–50, 2001.</p>
<p>[19] L. Yao, D. Mimno, and A. McCallum. Efﬁcient methods for topic model inference on streaming document collections. In International Conference on Knowledge Discovery and Data mining (SIGKDD), 2009.</p>
<p>[20] A. Zhang, J. Zhu, and B. Zhang. Sparse online topic models. In International Conference on World Wide Web (WWW), 2013.</p>
<p>[21] J. Zhu, X. Zheng, and B. Zhang. Improved bayesian supervised topic models with data augmentation. In Annual Meeting of the Association for Computational Linguistics (ACL), 2013.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
