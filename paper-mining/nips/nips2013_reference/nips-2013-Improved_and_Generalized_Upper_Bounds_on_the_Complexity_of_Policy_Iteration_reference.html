<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-140" href="../nips2013/nips-2013-Improved_and_Generalized_Upper_Bounds_on_the_Complexity_of_Policy_Iteration.html">nips2013-140</a> <a title="nips-2013-140-reference" href="#">nips2013-140-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>140 nips-2013-Improved and Generalized Upper Bounds on the Complexity of Policy Iteration</h1>
<br/><p>Source: <a title="nips-2013-140-pdf" href="http://papers.nips.cc/paper/4971-improved-and-generalized-upper-bounds-on-the-complexity-of-policy-iteration.pdf">pdf</a></p><p>Author: Bruno Scherrer</p><p>Abstract: Given a Markov Decision Process (MDP) with n states and m actions per state, we study the number of iterations needed by Policy Iteration (PI) algorithms to converge to the optimal “-discounted optimal policy. We consider two variations of PI: Howard’s PI that changes the actions in all states with a positive advantage, and Simplex-PI that only changes the action in the state with maximal Ï advantage. We show that Howard’s PI terminates 1 2Ì 1 1 22 1 1 nm 1 after at most n(m ≠ 1) 1≠“ log 1≠“ = O 1≠“ log 1≠“ iterations, improving by a factor O(log 1 a result by [3], while Simplex-PI terminates n) 1 22 1 2 1 22 2 1 1 2 after at most n (m ≠ 1) 1 + 1≠“ log 1≠“ = O n m log 1≠“ 1≠“ iterations, improving by a factor O(log n) a result by [11]. Under some structural assumptions of the MDP, we then consider bounds that are independent of the discount factor “: given a measure of the maximal transient time ·t and the maximal time ·r to revisit states in recurrent classes under all policies, we show that Simplex-PI terminates after at most n2 (m≠ # $ 1) (Á·r log(n·r )Ë + Á·r log(n·t )Ë) (m ≠ 1)Án·t log(n·t )Ë + Án·t log(n2 ·t )Ë = !</p><br/>
<h2>reference text</h2><p>[1] D.P. Bertsekas and J.N. Tsitsiklis. Neurodynamic Programming. Athena Scientiﬁc, 1996.</p>
<p>[2] J. Fearnley. Exponential lower bounds for policy iteration. In Proceedings of the 37th international colloquium conference on Automata, languages and programming: Part II, ICALP’10, pages 551–562, Berlin, Heidelberg, 2010. Springer-Verlag.</p>
<p>[3] T.D. Hansen, P.B. Miltersen, and U. Zwick. Strategy iteration is strongly polynomial for 2-player turn-based stochastic games with a constant discount factor. J. ACM, 60(1):1:1–1:16, February 2013.</p>
<p>[4] T.D. Hansen and U. Zwick. Lower bounds for howard’s algorithm for ﬁnding minimum mean-cost cycles. In ISAAC (1), pages 415–426, 2010.</p>
<p>[5] R. Hollanders, J.C. Delvenne, and R. Jungers. The complexity of policy iteration is exponential for discounted markov decision processes. In 51st IEEE conference on Decision and control (CDC’12), 2012.</p>
<p>[6] Y. Mansour and S.P. Singh. On the complexity of policy iteration. In UAI, pages 401–408, 1999.</p>
<p>[7] M. Melekopoglou and A. Condon. On the complexity of the policy improvement algorithm for markov decision processes. INFORMS Journal on Computing, 6(2):188–192, 1994.</p>
<p>[8] I. Post and Y. Ye. The simplex method is strongly polynomial for deterministic markov decision processes. Technical report, arXiv:1208.5083v2, 2012.</p>
<p>[9] M. Puterman. Markov Decision Processes. Wiley, New York, 1994.</p>
<p>[10] N. Schmitz. How good is howard’s policy improvement algorithm? Operations Research, 29(7):315–316, 1985.  Zeitschrift f¨r u</p>
<p>[11] Y. Ye. The simplex and policy-iteration methods are strongly polynomial for the markov decision problem with a ﬁxed discount rate. Math. Oper. Res., 36(4):593–603, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
