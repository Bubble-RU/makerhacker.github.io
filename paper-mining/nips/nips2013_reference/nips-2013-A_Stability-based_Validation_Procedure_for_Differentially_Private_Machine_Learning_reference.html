<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>14 nips-2013-A Stability-based Validation Procedure for Differentially Private Machine Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-14" href="../nips2013/nips-2013-A_Stability-based_Validation_Procedure_for_Differentially_Private_Machine_Learning.html">nips2013-14</a> <a title="nips-2013-14-reference" href="#">nips2013-14-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>14 nips-2013-A Stability-based Validation Procedure for Differentially Private Machine Learning</h1>
<br/><p>Source: <a title="nips-2013-14-pdf" href="http://papers.nips.cc/paper/5014-a-stability-based-validation-procedure-for-differentially-private-machine-learning.pdf">pdf</a></p><p>Author: Kamalika Chaudhuri, Staal A. Vinterbo</p><p>Abstract: Differential privacy is a cryptographically motivated deﬁnition of privacy which has gained considerable attention in the algorithms, machine-learning and datamining communities. While there has been an explosion of work on differentially private machine learning algorithms, a major barrier to achieving end-to-end differential privacy in practical machine learning applications is the lack of an effective procedure for differentially private parameter tuning, or, determining the parameter value, such as a bin size in a histogram, or a regularization parameter, that is suitable for a particular application. In this paper, we introduce a generic validation procedure for differentially private machine learning algorithms that apply when a certain stability condition holds on the training algorithm and the validation performance metric. The training data size and the privacy budget used for training in our procedure is independent of the number of parameter values searched over. We apply our generic procedure to two fundamental tasks in statistics and machine-learning – training a regularized linear classiﬁer and building a histogram density estimator that result in end-toend differentially private solutions for these problems. 1</p><br/>
<h2>reference text</h2><p>[1] R Bhaskar, S Laxman, A Smith, and A Thakurta. Discovering frequent patterns in sensitive data. In KDD, 2010.</p>
<p>[2] A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: the SuLQ framework. In PODS, 2005.</p>
<p>[3] K. Chaudhuri and D. Hsu. Convergence rates for differentially private statistical estimation. In ICML, 2012.</p>
<p>[4] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Differentially private empirical risk minimization. Journal of Machine Learning Research, 12:1069–1109, March 2011.</p>
<p>[5] K. Chaudhuri, A.D. Sarwate, and K. Sinha. Near-optimal algorithms for differentially-private principal components. Journal of Machine Learning Research, 2013 (to appear).</p>
<p>[6] L. Devroye and G. Lugosi. Combinatorial methods in density estimation. Springer, 2001.</p>
<p>[7] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In Theory of Cryptography, Berlin, Heidelberg, 2006.</p>
<p>[8] C. Dwork, G. Rothblum, and S. Vadhan. Boosting and differential privacy. In FOCS, 2010.</p>
<p>[9] A. Frank and A. Asuncion. UCI machine learning repository, 2013.</p>
<p>[10] A. Friedman and A. Schuster. Data mining with differential privacy. In KDD, 2010.</p>
<p>[11] S. R. Ganta, S. P. Kasiviswanathan, and A. Smith. Composition attacks and auxiliary information in data privacy. In KDD, 2008.</p>
<p>[12] M. Hardt and A. Roth. Beyond worst-case analysis in private singular vector computation. In STOC, 2013.</p>
<p>[13] M. Hardt and G. Rothblum. A multiplicative weights mechanism for privacy-preserving data analysis. In FOCS, pages 61–70, 2010.</p>
<p>[14] M. Hay, V. Rastogi, G. Miklau, and D. Suciu. Boosting the accuracy of differentially private histograms through consistency. PVLDB, 3(1):1021–1032, 2010.</p>
<p>[15] P. Jain, P. Kothari, and A. Thakurta. Differentially private online learning. In COLT, 2012.</p>
<p>[16] M C Jones, J S Marron, and S J Sheather. A brief survey of bandwidth selection for density estimation. JASA, 91(433):401–407, 1996.</p>
<p>[17] M. Kapralov and K. Talwar. On differentially private low rank approximation. In SODA, 2013.</p>
<p>[18] D. Kifer, A. Smith, and A. Thakurta. Private convex optimization for empirical risk minimization with applications to high-dimensional regression. In COLT, 2012.</p>
<p>[19] J. Lei. Differentially private M-estimators. In NIPS 24, 2011.</p>
<p>[20] A. Machanavajjhala, D. Kifer, J. M. Abowd, J. Gehrke, and L. Vilhuber. Privacy: Theory meets practice on the map. In ICDE, 2008.</p>
<p>[21] F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS, 2007.</p>
<p>[22] R Core Team. R: A Language and Environment for Statistical Computing. R Foundation.</p>
<p>[23] B. Rubinstein, P. Bartlett, L. Huang, and N. Taft. Learning in a large function space: Privacypreserving mechanisms for svm learning. Journal of Privacy and Conﬁdentiality, 2012.</p>
<p>[24] A.D. Sarwate and K. Chaudhuri. Signal processing and machine learning with differential privacy: Algorithms and challenges for continuous data. IEEE Signal Process. Mag., 2013.</p>
<p>[25] J. A. Swets and R. M. Pickett. Evaluation of Diagnostic Systems. Methods from Signal Detection Theory. Academic Press, New York, 1982.</p>
<p>[26] Berwin A Turlach. Bandwidth selection in kernel density estimation: A review. In CORE and Institut de Statistique. Citeseer, 1993.</p>
<p>[27] S. Vinterbo. Differentially private projected histograms: Construction and use for prediction. In ECML, 2012.</p>
<p>[28] L. Wasserman and S. Zhou. A statistical framework for differential privacy. JASA, 105(489):375–389, 2010.</p>
<p>[29] J. Xu, Z. Zhang, X. Xiao, Y. Yang, and G. Yu. Differentially private histogram publication. In ICDE, 2012.  9  6 6.1  Appendix An Example to Show Training Stability is not a Direct Consequence of Differential Privacy  We now present an example to illustrate that training stability is a property of the training algorithm and not a direct consequence of differential privacy. We present a problem and two α-differentially private training algorithms which approximately optimize the same function; the ﬁrst algorithm is based on exponential mechanism, and the second on a maximum of Laplace random variables mechanism. We show that while both provide α-differential privacy guarantees, the ﬁrst algorithm does not satisfy training stability while the second one does. Let i ∈ {1, . . . , l}, and let f : X n × R → [0, 1] be a function such that for all i and all datasets D 1 and D of size n that differ in the value of a single individual, |f (D, i) − f (D , i)| ≤ n . Consider the following training and validation problem. Given a sensitive dataset D, the private training procedure A outputs a tuple (i∗ , t1 , . . . , tl ), where i∗ is the output of the α/2-differentially private exponential mechanism [21] run to approximately maximize f (D, i), and each ti is equal to 2l f (D, i) plus an independent Laplace random variable with standard deviation αn . For any validation ∗ ∗. dataset V , the validation score q((i , t1 , . . . , tl ), V ) = ti It follows from standard results that A is α-differentially private. Moreover, A can be represented by a tuple TA = (GA , FA ), where GA is the following density over sequences of real numbers of length l + 1: 1 GA (r0 , r1 , . . . , rl ) = 10≤r0 ≤1 · l e−(|r1 |+|r2 |+...+|rl |) 2 Thus GA is the product of the uniform density on [0, 1] and l standard Laplace densities. Consider the following map E0 . For r ∈ [0, 1], let E0 (r) = i, if  j <  enα/8 (eα/2 −1) . (enα/8 +1)(enα/8 +eα/2 )  Consider a different algorithm A which computes t1 , . . . , tl ﬁrst, and then outputs the index i∗ that maximizes ti∗ . Then A can be represented by a tuple TA = (GA , FA ), where GA is a density over sequences of real numbers of length l as follows: GA (r1 , . . . , rl ) =  1 −(|r1 |+...+|rl |) e 2l  and FA is the map: FA (D, α, R) =  argmaxi (f (D, i) +  lRi lR1 lR2 lRl ), f (D, 1) + , f (D, 2) + , . . . , f (D, l) + αn αn αn αn 10  For the same value of R1 , . . . , Rl , if i∗ = i on input dataset D and if i∗ = i on input dataset D , 1 then, |f (D, i) − f (D, i )| ≤ n ; this implies that 1 n with probability 1 over GA . Thus the training stability condition holds for β1 = 1 and δ = 0. |q(FA (D, α, R), V ) − q(FA (D , α, R), V )| = |ti − ti | = |f (D, i) − f (D , i )| ≤  6.2  Output Perturbation Algorithm  We present the output perturbation algorithm for regularized linear classiﬁcation. Algorithm 4 Output Perturbation for Differentially Private Linear Classiﬁcation 1: Inputs: Regularization parameter λ, training set T = {(xi , yi ), i = 1, . . . , n}, privacy parame-  ter α.  2: Let G be the following density over Rd : ρG (r) ∝ e− 3: Solve the convex optimization problem:  1 w = argminw∈Rd λ w 2 ∗  4: Output w∗ +  6.3  2  r  . Draw R ∼ G.  1 + n  n  (w, xi , yi )  (4)  i=1  2 λαn R.  Case Study: Histogram Density Estimation  Our second case study is developing an end-to-end differentially private solution for histogrambased density estimation. In density estimation, we are given n samples x1 , . . . , xn drawn from ˆ an unknown density f , and our goal is to build an approximation f to f . In a histogram density estimator, we divide the range of the data into equal-sized bins of width h; if ni out of n of the input 1/h ni ˆ ˆ samples lie in bin i, then f is the density function: f (x) = i=1 hn · 1(x ∈ Bin i). A critical parameter while constructing the histogram density estimator is the bin size h. There is much theoretical literature on how to choose h – see [16, 26] for surveys. However, the choice of h is usually data-dependent, and in practice, the optimal h is often determined by building a histogram density estimator for a few different values of h, and selecting the one which has the best performance on held-out validation data. The most popular measure to evaluate the quality of a density estimator is the L2 -distance or the Integrated Square Error (ISE) between the density estimate and the true density: ˆ f −f  2  ˆ (f (x) − f (x))2 dx =  = x  ˆ f 2 (x)dx − 2  f 2 (x)dx + x  x  ˆ f (x)f (x)dx  (5)  x  f is typically unknown, so the ISE cannot be computed exactly. Fortunately it is still possible to compare multiple density estimates based on this distance. The ﬁrst term in the right hand side of ˆ ˆ Equation 5 depends only on f , and is equal for all f . The second term is a function of f only and can ˆ(x)], and even though it cannot be computed exactly thus be computed. The third term is 2Ex∼f [f without knowledge of f , we can estimate it based on a held out validation dataset. Thus, given a ˆ density estimator f and a validation dataset V = {z1 , . . . , zm }, we will use the following function ˆ to evaluate the quality of f on V : ˆ q(f , V ) = −  2 ˆ f 2 (x)dx + m x  ˆ A higher value of q indicates a smaller distance f − f For other measures, see [6].  2  m  ˆ f (zi )  (6)  i=1  , and thus a higher quality density estimate.  In the sequel, we assume that the data lies in the interval [0, 1] and that this interval is known in 1 advance. For ease of notation, we also assume without loss of generality that h is an integer. For 11  ease of exposition, we conﬁne ourselves to one-dimensional data, although the general techniques can be easily extended to higher dimensions. Given n samples and a bin size h, several works, including [7, 19, 27, 28, 20, 29, 14] have shown different ways of constructing and sampling from differentially private histograms. The most basic approach is to construct a non-private histogram and then add Laplace noise to each cell, followed by some post-processing. Algorithm 5 presents a variant of a differentially private histogram density estimator due to [19] in our framework. Algorithm 5 Differentially Private Histogram Density Estimator 1: Inputs: Bin size h (such that 1/h is an integer), data T = {x1 , . . . , xn }, privacy parameter α. 1 2: for i = 1, . . . , h do 1 3: Draw Ri independently from the standard Laplace density: ρG (r) = 2 e−|r| . n  i ˜ Let Ii = i−1 , h . Deﬁne: ni = j=1 1(xj ∈ Ii ), and let ni = max 0, ni + h 5: end for 1/h ni ˜ ˆ 6: Let n = i ni . Return the density estimator: f (x) = i=1 h˜ · 1(x ∈ Ii ) ˜ ˜ n  4:  2Ri α  .  The following theorem shows stability guarantees on the differentially private histogram density estimator described in Algorithm 5. Theorem 6 (Stability of Private Histogram Density Estimator) Let H = {h1 , . . . , hk } be a set ln(4k/δ) of bin sizes, and let hmin = mini hi . For any ﬁxed δ, if the sample size n ≥ 1 + 2α√h , then, min  δ the validation score q in Equation 6 is (β1 , β2 , k )-Stable with respect to Algorithm 5 and H for: 2 ln(4k/δ) 2 6 β1 = (1−ν)hmin , β2 = hmin , where: ν = nα√h . min  6.4  Proofs of Theorems 1, 2 and 3  We now present the proofs of Theorems 1, 2 and 3. Our proofs involve ideas similar to those in the analysis of the multiplicative weights update method for answering a set of linear queries in a differentially private manner [13]. Let A(D) denote the output of Algorithm 1 when the input is a sensitive dataset D = (T, V ), where T is the training part and V is the validation part. Let D = (T , V ) where T and T differ in the value of a single individual, and let D = (T, V ) where V and V differ in the value of a single individual. The proof of Theorem 1 is a consequence of the following two lemmas. Lemma 1 Suppose that the conditions in Theorem 1 hold. Then, for all D = (T, V ), all D = (T , V ), such that T and T differ in the value of a single individual, and for any set of outcomes S: Pr(A(D) ∈ S) ≤ eα2 Pr(A(D ) ∈ S) + δ  (7)  Lemma 2 Suppose that the conditions in Theorem 1 hold. Then, for all D = (T, V ), all D = (T, V ) such that V and V differ in the value of a single individual, and for any set of outcomes S, Pr(A(D) ∈ S) ≤ eα2 Pr(A(D ) ∈ S) + δ  (8)  P ROOF : (Of Lemma 1) Let S = (I, C), where I ⊆ [k] is a set of indices and C ⊆ C. Let E be the event that all of R1 , . . . , Rk lie in the set Σ. We will ﬁrst show that conditioned on E, for all i, it holds that: Pr(i∗ = i|D, E) ≤ eα2 Pr(i∗ = i|D , E) (9) Since Pr(E) ≥ 1 − δ, from the conditions in Theorem 1, for any subset I of indices, we can write: Pr(i∗ ∈ I|D) ≤ ≤ ≤ ≤  Pr(i∗ ∈ I|D, E) Pr(E) + (1 − Pr(E)) eα2 Pr(i∗ ∈ I|D , E) Pr(E) + δ eα2 Pr(i∗ ∈ I, E|D ) + δ eα2 Pr(i∗ ∈ I|D ) + δ 12  (10)  We will now prove Equation 9. For this purpose, we adopt the following notation. We use the notation Z\i to denote the random variables Z1 , . . . , Zi−1 , Zi+1 , . . . , Zk and z\i to denote the set of values z1 , . . . , zi−1 , zi+1 , . . . , zk . We also use the notation h(·) to represent the density induced on the random variables Z1 , . . . , Zk by Algorithm 1. In addition, we use the notation R to denote the vector (R1 , . . . , Rk ). We ﬁrst ﬁx a value z\i for Z\i , and a value of R such that R1 , . . . , Rk all lie in Σ, and consider the ratio of probabilities: Pr(i∗ = i|Z\i = z\i , D, R) Pr(i∗ = i|Z\i = z\i , D , R) Observe that this ratio of probabilities is equal to: Pr(Zi + q(F (T, θi , α1 , Ri ), V ) ≥ supj=i zj + q(F (T, θj , α1 , Rj ), V )) Pr(Zi + q(F (T , θi , α1 , Ri ), V ) ≥ supj=i zj + q(F (T , θj , α1 , Rj ), V )) which is in turn equal to: Pr(Zi ≥ supj=i zj + q(F (T, θj , α1 , Rj ), V ) − q(F (T, θi , α1 , Ri ), V )) Pr(Zi ≥ supj=i zj + q(F (T , θj , α1 , Rj ), V ) − q(F (T , θi , α1 , Ri ), V )) Observe that from the stability condition, |(q(F (T, θj , α1 , Rj ), V ) − q(F (T, θi , α1 , Ri ), V )) − (q(F (T , θj , α1 , Rj ), V ) − q(F (T , θi , α1 , Ri ), V ))| ≤ |q(F (T, θj , α1 , Rj ), V ) − q(F (T , θj , α1 , Rj ), V )| + |q(F (T, θi , α1 , Ri ), V ) − q(F (T , θi , α1 , Ri ), V )| 2β1 ≤ 2β ≤ n Thus, the ratio of the probabilities is at most the ratio Pr(Zi ≥ γ)/ Pr(Zi ≥ γ + 2β) where γ = supj=i zj +q(F (T, θj , α1 , Rj ), V )−q(F (T, θi , α1 , Ri ), V ), which is at most eα2 by properties of the exponential distribution. Thus, we have established that for all z\i , for all R in Σk , Pr(i∗ = i|Z\i = z\i , D, R) ≤ eα2 · Pr(i∗ = i|Z\i = z\i , D , R) Equation 9 follows by integrating over z\i and R. The lemma follows. P ROOF :(Of Lemma 2) Let S = (I, C), where I ⊆ [k] is a set of indices and C ⊆ C. Let E be the event that all of R1 , . . . , Rk lie in Σ. We will ﬁrst show that conditioned on E, for all i, it holds that: Pr(i∗ = i|D, E) ≤ eα2 Pr(i∗ = i|D , E)  (11)  Since Pr(E) ≥ 1 − δ, from the conditions in Theorem 1, for any subset I of indices, we can write: Pr(i∗ ∈ I|D) ≤ ≤ ≤ ≤  Pr(i∗ ∈ I|D, E) Pr(E) + (1 − Pr(E)) eα2 Pr(i∗ ∈ I|D , E) Pr(E) + δ eα2 Pr(i∗ ∈ I, E|D ) + δ eα2 Pr(i∗ ∈ I|D ) + δ  (12)  We will now focus on showing Equation 11. We ﬁrst consider the case when event E holds, that is, Rj ∈ R, for j = 1, . . . , k. In this case, the stability deﬁnition and the conditions of the theorem imply that for all θj ∈ Θ, |q(F (T, θj , α1 , Rj ), V ) − q(F (T, θj , α1 , Rj ), V )| ≤  β2 ≤β m  (13)  In what follows, we use the notation Z\i to denote the random variables Z1 , . . . , Zi−1 , Zi+1 , . . . , Zk and z\i to denote the set of values z1 , . . . , zi−1 , zi+1 , . . . , zk . We also use the notation h(·) to represent the density induced on the random variables Z1 , . . . , Zk by Algorithm 1. In addition, we use the notation R to denote the vector (R1 , . . . , Rk ). We ﬁrst ﬁx a value z\i for Z\i , and a value of R such that E holds, and consider the ratio of probabilities: Pr(i∗ = i|Z\i = z\i , D, R) Pr(i∗ = i|Z\i = z\i , D , R) 13  Observe that this ratio of probabilities is equal to: Pr(Zi + q(F (T, θi , α1 , Ri ), V ) ≥ supj=i zj + q(F (T, θj , α1 , Rj ), V )) Pr(Zi + q(F (T, θi , α1 , Ri ), V ) ≥ supj=i zj + q(F (T, θj , α1 , Rj ), V )) which is in turn equal to: Pr(Zi ≥ supj=i zj + q(F (T, θj , α1 , Rj ), V ) − q(F (T, θi , α1 , Ri ), V )) Pr(Zi ≥ supj=i zj + q(F (T, θj , α1 , Rj ), V ) − q(F (T, θi , α1 , Ri ), V )) Observe that from Equation 13, |(q(F (T, θj , α1 , Rj ), V )−q(F (T, θi , α1 , Ri ), V ))−(q(F (T, θj , α1 , Rj ), V )−q(F (T, θi , α1 , Ri ), V ))| ≤ Thus, the ratio of the probabilities is at most the ratio Pr(Zi ≥ γ)/ Pr(Zi ≥ γ + 2β) for γ = supj=i zj + q(F (T, θj , α1 , rj ), V ) − q(F (T, θi , α1 , ri ), V ), which is at most eα2 by properties of the exponential distribution. Thus, we have established that when R ∈ Σk , for all j, Pr(i∗ = i|Z\i = z\i , D, R) ≤ eα2 Pr(i∗ = i|Z\i = z\i , D , R) Thus for any such R, we can write: Pr(i∗ = i|D, R) Pr(i∗ = i|D , R)  =  z\i z\i  Pr(i∗ = i|Z\i = z\i , D, R)h(z\i )dz\i Pr(i∗ = i|Z\i = z\i , D , R)h(z\i )dz\i  ≤ eα2  Equation 11 now follows by integrating R over E. P ROOF :(Of Theorem 1) The proof of Theorem 1 follows from a combination of Lemmas 1 and 2. P ROOF :(Of Theorem 2) The proof of Theorem 2 follows from privacy composition; Theorem 1 ensures that Step (2) of Algorithm 2 is (α2 , δ)-differentially private; moreover the training procedure T is α1 -differentially private. The theorem follows by composing these two results. P ROOF :(Of Theorem 3) Observe that: Pr q(hi∗ , V ) < max q(hi , V ) − 1≤i≤k  2β log(k/δ0 ) α2  ≤ Pr ∃j s.t. Zj ≥  By properties of the exponential distribution, for any ﬁxed j, Pr(Zj ≥ theorem follows by an Union Bound. 6.5  log(k/δ0 ) α2  log(k/δ0 ) ) α2  ≤  δ0 k .  Thus the  Proof of Theorem 4  P ROOF : (Of Theorem 4 for Output Perturbation) Let T and T be two training sets which differ in a single labelled example ((xn , yn ) vs. (xn , yn )), and let w∗ (T ) and w∗ (T ) be the solutions to the regularized convex optimization problem in Equation 1 when the inputs are T and T respectively. We observe that for ﬁxed λ, α and R, F (T, λ, α, R) − F (T , λ, α, R) = w∗ (T ) − w∗ (T ) When the training sets are T and T , the objective functions in the regularized convex optimization 1 problems are both λ-strongly convex, and they differ by n ( (w, xn , yn )− (w, xn , yn )). Combining this fact with Lemma 1 of [4], and using the fact that is 1-Lipschitz, we have that for all λ and R, 2 λn Since g is L-Lipschitz, this implies that for any ﬁxed validation set V , and for all λ, α and R, F (T, λ, α, R) − F (T , λ, α, R) ≤  |q(F (T, λ, α, R), V ) − q(F (T , λ, α, R), V )| ≤ 14  2L λn  (14)  2β2 ≤ 2β m  Now let V and V be two validation sets that differ in the value of a single labelled example (¯m , ym ). Since g ≥ 0 for all inputs, for any such V and V , and for a ﬁxed Λ, α and R, x ¯ |q(F (T, λ, α, R), V ) − q(F (T, λ, α, R), V )| ≤ gmax , where m gmax =  sup g(F (T, λ, α, R), x, y) (x,y)∈X  By deﬁnition, gmax ≤ g ∗ . Moreover, as g is L-Lipschitz, gmax ≤ L · F (T, λ, α, R) Now, let E be the event that R ≤ d log(dk/δ). From Lemma 4 of [4], Pr(E) ≥ 1 − δ/k. Thus, provided E holds, we have that: F (T, λ, α, R) ≤ w∗ +  d log(dk/δ) 1 d log(dk/δ) 1 ≤ + = λαn λ λαn λ  1+  d log(dk/δ) nα  where the bound on w∗ follows from an application of Lemma 1 of [4] on the functions 1 λ w 2 2 n 1 1 and 2 λ w 2 + n i=1 (w, xi , yi ). This implies that provided E holds, for all training sets T , and for all λ, |q(F (T, λ, α, R), V ) − q(F (T, λ, α, R), V )| ≤  L λm  1+  d log(dk/δ) nα  (15)  The theorem now follows from a combination of Equations 14 and 15, and the deﬁnition of g ∗ . P ROOF : (Of Theorem 4 for Objective Perturbation) Let T and T be two training sets which differ in a single labelled example (xn , yn ). We observe that for a ﬁxed R and λ, the objective of the regular1 ized convex optimization problem in Equation 2 differs in the term n ( (w, xn , yn ) − (w, xn , yn )). Combining this with Lemma 1 of [4], and using the fact that is 1-Lipschitz, we have that for all λ, α, R, 2 F (T, λ, α, R) − F (T , λ, α, R) ≤ λn Since g is L-Lipschitz, this implies that for any ﬁxed validation set V , and for all λ and r, |q(F (T, λ, α, R), V ) − q(F (T , λ, α, R), V )| ≤  2L λn  (16)  Now let V and V be two validation sets that differ in the value of a single labelled example (¯m , ym ). Since g ≥ 0, for any such V and V , |q(F (T, λ, α, R), V ) − q(F (T, λ, α, R), V )| ≤ x ¯ gmax m , where gmax = sup g(F (T, λ, α, R), x, y) (x,y)∈X ∗  By deﬁnition gmax ≤ g . Moreover, as g is L-Lipschitz, gmax ≤ L · F (T, λ, α, R) Let E be the event that R ≤ d log(dk/δ). From Lemma 4 of [4], Pr(E) ≥ 1 − δ/k. Thus, provided E holds, we have that: F (T, λ, α, R) ≤  1 + R /(αn) 1 ≤ λ λ  1+  d log(dk/δ) nα  This implies that provided E holds, for all training sets T , and for all λ, |q(F (T, λ, α, R), V ) − q(F (T, λ, α, R), V )| ≤  L λm  1+  d log(dk/δ) nα  The theorem now follows from a combination of Equations 16 and 17, and the deﬁnition of g ∗ .  15  (17)  6.6  Proof of Theorem 6  Lemma 3 (Concentration of Sum of Laplace Random Variables) Let Z1 , . . . , Zs be s ≥ 2 iid standard Laplace random variables, and let Z = Z1 + . . . + Zs . Then, for any θ, Pr(Z ≥ θ) ≤  1−  −s  1 s  √  e−θ/  s  √  ≤ 4e−θ/  s  P ROOF : The proof follows from using the method of generating functions. The generating function 1 for the standard Laplace distribution is: ψ(X) = E[etX ] = 1−t2 , for |t| ≤ 1. As Z1 , . . . , Zs are independently distributed, the generating function for Z is E[etZ ] = (1 − t2 )−s . Now, we can write: Pr(Z ≥ θ)  = ≤  Plugging in t =  1 √ , s  Pr(etZ ≥ etθ ) E[etZ ] = e−tθ · (1 − t2 )−s etθ  we get that: Pr(Z ≥ θ) ≤  1−  1 s  −s  √  e−θ/  s  The lemma follows by observing that for s ≥ 2, (1 − 1 )s ≥ 1 . s 4 P ROOF : (Of Theorem 6) Let V = {z1 , . . . , zm } be a validation dataset, and let V be a validation dataset that differs from V in a single sample (zm vs zm ). We use the notation R to denote the sequence of values R = (R1 , R2 , . . . , R1/h ). Given an input sample T , a bin size h, a priˆ vacy parameter α, and a sequence R, we use the notation fT,h,α,R to denote the density estimator F (T, h, α, R). For all such T , all h, all α and all R, we can write: 2 ˆ ˆ |q(F (T, h, α, R), V ) − q(F (T, h, α, R), V )| = (fT,h,α,R (zm ) − fT,h,α,R (zm )) m ˜ 2 2 maxi ni · ≤ (18) ≤ m h˜ n mh For a ﬁxed value of h, we deﬁne the following event E: 1/h  Ri ≥ − i=1  ln(4k/δ) √ h  Using the symmetry of Laplace random variables and Lemma 3, we get that Pr(E) ≥ 1 − δ/k. We observe that provided the event E holds, 1/h  n≥n− ˜  Ri ≥ n − i=1  2 ln(4k/δ) √ ≥ n(1 − ν) α h  (19)  Let T and T be two input datasets that differ in a single sample (xn vs xn ). We ﬁx a bin size h, a value of α, and a sequence R, and for these ﬁxed values, we use the notation ni and ni to denote the ˜ ˜ value of ni in Algorithm 5 when the inputs are T and T respectively. Similarly, we use n = i ni ˜ ˜ ˜ and n = i ni . ˜ ˜ For any V , we can write: q(F (T, h, α, R), V ) − q(F (T , h, α, R), V )  =  2 m  m  ˆ ˆ (fT,h,α,R (zj ) − fT  ,h,α,R (zj ))  j=1  1/h  −  h· i=1  n2 ˜i n2 ˜ − 2i 2 h2 n2 ˜ h n ˜  (20)  We now look at bounding the right hand side of Equation 20 term by term. Suppose T is obtained rom T by moving a single sample xn from bin a to bin b in the histogram. Then, depending on the relative values of na and nb , there are four cases: ˜ ˜ 16  1. 2. 3. 4.  na ˜ na ˜ na ˜ na ˜  = na − 1, nb = nb + 1. Thus n ˜ ˜ ˜ ˜ = na = 0, nb = nb + 1. Thus n ˜ ˜ ˜ ˜ = na − 1, nb = nb = 0. Thus n ˜ ˜ ˜ ˜ = na = 0, nb = nb = 0. Thus n ˜ ˜ ˜ ˜  = n. ˜ = n + 1. ˜ = n − 1. ˜ = n. ˜  ˆ ˆ In the fourth case, fT,h,α,R = fT ,h,α,R , and thus the right hand side of Equation 20 is 0. Moreover, the second and the third cases are symmetric. We thus focus on the ﬁrst two cases. In the ﬁrst case, the ﬁrst term in the right hand side of Equation 20 can be written as: m 1/h  2 · 1(zj ∈ Ii ) · m j=1 i=1  m 1/h  n ˜ ni ˜ − i h˜ n h˜ n  =  2 ni − ni ˜ ˜ · 1(zj ∈ Ii ) · m j=1 i=1 h˜ n  2 1 2 ·m· ≤ m h˜ n h˜ n The second term on the right hand side of Equation 20 can be written as: ≤  1/h  i=1  n2 ˜i n2 ˜ − i2 2 h˜ n h˜ n  =  n2 + n2 − (˜ a − 1)2 − (˜ b + 1)2 ˜a ˜b n n 2 h˜ n  2˜ a − 2˜ b − 2 n n 2 ≤ h˜ 2 n h˜ n where the last step follows from the fact that nb = nb + 1 ≤ n. Thus, for the ﬁrst case, the right ˜ ˜ ˜ 4 hand side of Equation 20 is at most h˜ . n =  We now consider the second case. The ﬁrst term on the right hand side of Equation 20 can be written as: m 1/h  2 · 1(zj ∈ Ii ) · m j=1 i=1 m 1/h  = ≤ ≤  2 · 1(zj ∈ Ii ) · mh j=1 i=1  ni ˜ n ˜ − i h˜ n h˜ n ni ˜ ni ˜ − n ˜ n+1 ˜  2 1 ·m· · max(|˜ i (˜ + 1) − ni n|, |˜ i (˜ + 1) − n(˜ i + 1)|) n n ˜ ˜ n n ˜ n hm n(˜ + 1) ˜ n 1 2 2 · · max(|˜ i |, |˜ − ni |) ≤ n n ˜ h n(˜ + 1) ˜ n h(˜ + 1) n  where the last step follows from the fact that max(|˜ i |, |˜ − ni |) ≤ n. The second term on the right n n ˜ ˜ hand side of Equation 20 can be written as: 1/h  i=1  n2 ˜i n2 ˜ − i2 2 h˜ n h˜ n  = i=b  n2 ˜i n2 ˜i − 2 h˜ n h(˜ + 1)2 n  +  (˜ b + 1)2 n n2 ˜b − 2 h˜ n h(˜ + 1)2 n  (˜ b − n)(2˜ b n + n + nb ) n ˜ n ˜ ˜ ˜ h˜ 2 (˜ + 1)2 n n  =  2˜ + 1 n · h˜ 2 (˜ + 1)2 n n  ≤  2˜ + 1 n n · 2˜ (˜ + 1) ˜ nn 4 + ≤ h(˜ + 1)2 n h˜ 2 (˜ + 1)2 n n h(˜ + 1) n  n2 + ˜i i=b  Thus, in the second case, the right hand side of Equation 20 is at most h(˜6 . We observe that the n+1) third case is symmetric to the second case, and thus we can carry out very similar calculations in 6 the third case to show that the right hand side is at most h˜ . Thus, we have that for any T and T , n provided the event E holds, 6 |q(F (T, h, α, R), V ) − q(F (T , h, α, R), V )| ≤ (21) h˜ n The theorem now follows by combining Equation 21 with Equation 19.  17  6.7  Proof of Theorem 5  Lemma 4 (Parallel construction) Let A = {A1 , A2 , . . . , Ak } be a list of k independently randomized functions, and let Ai be αi -differentially private. Let {D1 , D2 , . . . , Dk } be k subsets of a set D such that i = j =⇒ Di ∩ Dj = ∅. Algorithm B(D, A) = (A1 (D1 ), A2 (D2 ), . . . , Ak (Dk )) is max1≤i≤k αi -differentially private. P ROOF : Let D, D be two datasets such that their symmetric difference contains one element. We have that P (B(D, A) ∈ S) P (B(D, A) ∈ S1 × · · · × Sk ) P (A1 (D1 ) ∈ S1 ) · · · P (Ak (Dk ) ∈ Sk ) = = P (B(D , A) ∈ S) P (B(D , A) ∈ S1 × · · · × Sk ) P (A1 (D1 ) ∈ S1 ) · · · P (Ak (Dk ) ∈ Sk ) (22) by independence of randomness in the Ai . Since i = j =⇒ Di ∩ Dj = ∅, there exists at most one index j such that Dj = Dj . If j does not exist, (22) reduces to e0 ≤ emax1≤i≤k αi . Let j exist, then P (B(D, A) ∈ S) P (Aj (Dj ) ∈ Sj ) ≤ eαj ≤ emax1≤i≤k αi , = P (B(D , A) ∈ S) P (Aj (Dj ) ∈ Sj ) which concludes the proof. P ROOF : (Theorem 5) We begin by separating task (a) of producing the fi in step 1. from the task (b) of computing ei in step 2. and selecting i∗ in step 3. From the parallel construction Lemma 4 it follows that (a) in dataSplit is α-differentially private. From standard composition of privacy it follows that (a) in alphaSplit is α-differentially private. Task (b) is for both alphaSplit and dataSplit an application of the exponential mechanism [21], which for choosing with a probability proportional to (−ei ) yields 2 ∆-differential privacy, where ∆ is the sensitivity of ei . Since a single change in V can change the number of errors any ﬁxed classiﬁer can make by at most 1 = ∆, we get that task (b) is α-differentially private for = α/2. If T and V are disjoint, we get by parallel construction that both alphaSplit and dataSplit yield α-differential privacy. If T and V are not disjoint, by standard composition of privacy we get that both alphaSplit and dataSplit yield 2α-differential privacy. In Random, the results of step 2. in task (b) are never used in step 3. Step 3 is done without looking at the input data and does not incur loss of differential privacy. We can therefore simulate Random by ﬁrst choosing i∗ uniformly at random, and then computing fi at α-differential privacy, which by standard privacy composition is α-differentially private. 6.8  Experimental selection of regularizer index  18  Adult  Magic  6  q q 4  q q  q q  q  2  q q  q 0.3 0.5  1.0  q  q  2.0  3.0  5.0  0.3 0.5  1.0  2.0  3.0  5.0  alpha  q Stability  alphaSplit  dataSplit  Random  Control  Figure 2: A summary of 10 times 10-fold cross-validation selection of regularizer index i into Θ for different privacy levels α. Each point in the ﬁgure represents a summary of 100 data points. The error bars indiciate a boot-strap sample estimate of the 95% conﬁdence interval of the mean. A small amount of jitter was added to positions on the x-axes to avoid over-plotting.  19</p>
<br/>
<br/><br/><br/></body>
</html>
