<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-222" href="../nips2013/nips-2013-On_the_Linear_Convergence_of_the_Proximal_Gradient_Method_for_Trace_Norm_Regularization.html">nips2013-222</a> <a title="nips-2013-222-reference" href="#">nips2013-222-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>222 nips-2013-On the Linear Convergence of the Proximal Gradient Method for Trace Norm Regularization</h1>
<br/><p>Source: <a title="nips-2013-222-pdf" href="http://papers.nips.cc/paper/4936-on-the-linear-convergence-of-the-proximal-gradient-method-for-trace-norm-regularization.pdf">pdf</a></p><p>Author: Ke Hou, Zirui Zhou, Anthony Man-Cho So, Zhi-Quan Luo</p><p>Abstract: Motivated by various applications in machine learning, the problem of minimizing a convex smooth loss function with trace norm regularization has received much attention lately. Currently, a popular method for solving such problem is the proximal gradient method (PGM), which is known to have a sublinear rate of convergence. In this paper, we show that for a large class of loss functions, the convergence rate of the PGM is in fact linear. Our result is established without any strong convexity assumption on the loss function. A key ingredient in our proof is a new Lipschitzian error bound for the aforementioned trace norm–regularized problem, which may be of independent interest.</p><br/>
<h2>reference text</h2><p>[1] Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering Shared Structures in Multiclass Classiﬁcation. In Proc. 24th ICML, pages 17–24, 2007.</p>
<p>[2] A. Argyriou, T. Evgeniou, and M. Pontil. Convex Multi–Task Feature Learning. Mach. Learn., 73(3):243– 272, 2008.</p>
<p>[3] A. Beck and M. Teboulle. A Fast Iterative Shrinkage–Thresholding Algorithm for Linear Inverse Problems. SIAM J. Imaging Sci., 2(1):183–202, 2009.</p>
<p>[4] A. Ben-Tal and A. Nemirovski. Lectures on Modern Convex Optimization: Analysis, Algorithms, and Engineering Applications. MPS–SIAM Series on Optimization. Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania, 2001.</p>
<p>[5] M. Fazel, H. Hindi, and S. P. Boyd. A Rank Minimization Heuristic with Application to Minimum Order System Approximation. In Proc. 2001 ACC, pages 4734–4739, 2001.</p>
<p>[6] D. Gross. Recovering Low–Rank Matrices from Few Coefﬁcients in Any Basis. IEEE Trans. Inf. Theory, 57(3):1548–1566, 2011.</p>
<p>[7] S. Ji, K.-F. Sze, Z. Zhou, A. M.-C. So, and Y. Ye. Beyond Convex Relaxation: A Polynomial–Time Non–Convex Optimization Approach to Network Localization. In Proc. 32nd IEEE INFOCOM, pages 2499–2507, 2013.</p>
<p>[8] S. Ji and J. Ye. An Accelerated Gradient Method for Trace Norm Minimization. In Proc. 26th ICML, pages 457–464, 2009.</p>
<p>[9] V. Koltchinskii, K. Lounici, and A. B. Tsybakov. Nuclear–Norm Penalization and Optimal Rates for Noisy Low–Rank Matrix Completion. Ann. Stat., 39(5):2302–2329, 2011.</p>
<p>[10] Z.-Q. Luo and P. Tseng. Error Bounds and Convergence Analysis of Feasible Descent Methods: A General Approach. Ann. Oper. Res., 46(1):157–178, 1993.</p>
<p>[11] S. Ma, D. Goldfarb, and L. Chen. Fixed Point and Bregman Iterative Methods for Matrix Rank Minimization. Math. Program., 128(1–2):321–353, 2011.</p>
<p>[12] Yu. Nesterov. Introductory Lectures on Convex Optimization: A Basic Course. Kluwer Academic Publishers, Boston, 2004.</p>
<p>[13] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed Minimum–Rank Solutions of Linear Matrix Equations via Nuclear Norm Minimization. SIAM Rev., 52(3):471–501, 2010.</p>
<p>[14] R. T. Rockafellar. Convex Analysis. Princeton Landmarks in Mathematics and Physics. Princeton University Press, Princeton, New Jersey, 1997.</p>
<p>[15] R. T. Rockafellar and R. J.-B. Wets. Variational Analysis, volume 317 of Grundlehren der mathematischen Wissenschaften. Springer–Verlag, Berlin Heidelberg, second edition, 2004.</p>
<p>[16] M. Schmidt, N. Le Roux, and F. Bach. Convergence Rates of Inexact Proximal–Gradient Methods for Convex Optimization. In Proc. NIPS 2011, pages 1458–1466, 2011.</p>
<p>[17] A. M.-C. So, Y. Ye, and J. Zhang. A Uniﬁed Theorem on SDP Rank Reduction. Math. Oper. Res., 33(4):910–920, 2008.</p>
<p>[18] W. So. Facial Structures of Schatten p–Norms. Linear and Multilinear Algebra, 27(3):207–212, 1990.</p>
<p>[19] K.-C. Toh and S. Yun. An Accelerated Proximal Gradient Algorithm for Nuclear Norm Regularized Linear Least Squares Problems. Pac. J. Optim., 6(3):615–640, 2010.</p>
<p>[20] R. Tomioka and K. Aihara. Classifying Matrices with a Spectral Regularization. In Proc. of the 24th ICML, pages 895–902, 2007.</p>
<p>[21] R. Tomioka, T. Suzuki, M. Sugiyama, and H. Kashima. A Fast Augmented Lagrangian Algorithm for Learning Low–Rank Matrices. In Proc. 27th ICML, pages 1087–1094, 2010.</p>
<p>[22] P. Tseng. Approximation Accuracy, Gradient Methods, and Error Bound for Structured Convex Optimization. Math. Program., 125(2):263–295, 2010.</p>
<p>[23] P. Tseng and S. Yun. A Coordinate Gradient Descent Method for Nonsmooth Separable Minimization. Math. Program., 117(1–2):387–423, 2009.</p>
<p>[24] M. White, Y. Yu, X. Zhang, and D. Schuurmans. Convex Multi–View Subspace Learning. In Proc. NIPS 2012, pages 1682–1690, 2012.</p>
<p>[25] H. Zhang, J. Jiang, and Z.-Q. Luo. On the Linear Convergence of a Proximal Gradient Method for a Class of Nonsmooth Convex Minimization Problems. J. Oper. Res. Soc. China, 1(2):163–186, 2013.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
