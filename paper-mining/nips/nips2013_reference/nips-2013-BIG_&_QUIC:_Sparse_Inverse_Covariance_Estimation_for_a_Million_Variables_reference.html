<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-45" href="../nips2013/nips-2013-BIG_%26_QUIC%3A_Sparse_Inverse_Covariance_Estimation_for_a_Million_Variables.html">nips2013-45</a> <a title="nips-2013-45-reference" href="#">nips2013-45-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>45 nips-2013-BIG & QUIC: Sparse Inverse Covariance Estimation for a Million Variables</h1>
<br/><p>Source: <a title="nips-2013-45-pdf" href="http://papers.nips.cc/paper/4923-big-quic-sparse-inverse-covariance-estimation-for-a-million-variables.pdf">pdf</a></p><p>Author: Cho-Jui Hsieh, Matyas A. Sustik, Inderjit Dhillon, Pradeep Ravikumar, Russell Poldrack</p><p>Abstract: The 1 -regularized Gaussian maximum likelihood estimator (MLE) has been shown to have strong statistical guarantees in recovering a sparse inverse covariance matrix even under high-dimensional settings. However, it requires solving a difﬁcult non-smooth log-determinant program with number of parameters scaling quadratically with the number of Gaussian variables. State-of-the-art methods thus do not scale to problems with more than 20, 000 variables. In this paper, we develop an algorithm B IG QUIC, which can solve 1 million dimensional 1 regularized Gaussian MLE problems (which would thus have 1000 billion parameters) using a single machine, with bounded memory. In order to do so, we carefully exploit the underlying structure of the problem. Our innovations include a novel block-coordinate descent method with the blocks chosen via a clustering scheme to minimize repeated computations; and allowing for inexact computation of speciﬁc components. In spite of these modiﬁcations, we are able to theoretically analyze our procedure and show that B IG QUIC can achieve super-linear or even quadratic convergence rates. 1</p><br/>
<h2>reference text</h2><p>[1] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre. Fast unfolding of community hierarchies in large networks. J. Stat Mech, 2008.</p>
<p>[2] T. Cai, W. Liu, and X. Luo. A constrained 1 minimization approach to sparse precision matrix estimation. Journal of American Statistical Association, 106:594–607, 2011.</p>
<p>[3] A. d’Aspremont, O. Banerjee, and L. E. Ghaoui. First-order methods for sparse covariance selection. SIAM Journal on Matrix Analysis and its Applications, 30(1):56–66, 2008.</p>
<p>[4] R. S. Dembo, S. C. Eisenstat, and T. Steihaug. Inexact Newton methods. SIAM J. Numerical Anal., 19(2):400–408, 1982.</p>
<p>[5] I. S. Dhillon, Y. Guan, and B. Kulis. Weighted graph cuts without eigenvectors: A multilevel approach. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 29:11:1944–1957, 2007.</p>
<p>[6] J. Duchi, S. Gould, and D. Koller. Projected subgradient methods for learning sparse Gaussians. UAI, 2008.</p>
<p>[7] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432–441, July 2008.</p>
<p>[8] C.-J. Hsieh, I. S. Dhillon, P. Ravikumar, and A. Banerjee. A divide-and-conquer method for sparse inverse covariance estimation. In NIPS, 2012.</p>
<p>[9] C.-J. Hsieh, M. A. Sustik, I. S. Dhillon, and P. Ravikumar. Sparse inverse covariance estimation using quadratic approximation. 2013.</p>
<p>[10] G. Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning irregular graphs. SIAM J. Sci. Comput., 20(1):359–392, 1999.</p>
<p>[11] J. D. Lee, Y. Sun, and M. A. Saunders. Proximal newton-type methods for minimizing composite functions. In NIPS, 2012.</p>
<p>[12] L. Li and K.-C. Toh. An inexact interior point method for 1 -reguarlized sparse covariance selection. Mathematical Programming Computation, 2:291–315, 2010.</p>
<p>[13] R. Mazumder and T. Hastie. Exact covariance thresholding into connected components for large-scale graphical lasso. Journal of Machine Learning Research, 13:723–736, 2012.</p>
<p>[14] N. Meinshausen and P. B¨ hlmann. High dimensional graphs and variable selection with the u lasso. Annals of Statistics, 34:1436–1462, 2006.</p>
<p>[15] P. Olsen, F. Oztoprak, J. Nocedal, and S. Rennie. Newton-like methods for sparse inverse covariance estimation. Technical report, Optimization Center, Northwestern University, 2012.</p>
<p>[16] B. Rolfs, B. Rajaratnam, D. Guillot, A. Maleki, and I. Wong. Iterative thresholding algorithm for sparse inverse covariance estimation. In NIPS, 2012.</p>
<p>[17] K. Scheinberg, S. Ma, and D. Goldfarb. Sparse inverse covariance selection via alternating linearization methods. NIPS, 2010.</p>
<p>[18] K. Scheinberg and I. Rish. Learning sparse Gaussian Markov networks using a greedy coordinate ascent approach. In J. Balczar, F. Bonchi, A. Gionis, and M. Sebag, editors, Machine Learning and Knowledge Discovery in Databases, volume 6323 of Lecture Notes in Computer Science, pages 196–212. Springer Berlin / Heidelberg, 2010.</p>
<p>[19] D. M. Witten, J. H. Friedman, and N. Simon. New insights and faster computations for the graphical lasso. Journal of Computational and Graphical Statistics, 20(4):892–900, 2011.  9</p>
<br/>
<br/><br/><br/></body>
</html>
