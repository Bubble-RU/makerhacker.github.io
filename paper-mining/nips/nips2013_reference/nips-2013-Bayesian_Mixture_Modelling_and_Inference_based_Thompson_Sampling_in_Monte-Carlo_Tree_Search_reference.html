<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-50" href="../nips2013/nips-2013-Bayesian_Mixture_Modelling_and_Inference_based_Thompson_Sampling_in_Monte-Carlo_Tree_Search.html">nips2013-50</a> <a title="nips-2013-50-reference" href="#">nips2013-50-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>50 nips-2013-Bayesian Mixture Modelling and Inference based Thompson Sampling in Monte-Carlo Tree Search</h1>
<br/><p>Source: <a title="nips-2013-50-pdf" href="http://papers.nips.cc/paper/5111-bayesian-mixture-modelling-and-inference-based-thompson-sampling-in-monte-carlo-tree-search.pdf">pdf</a></p><p>Author: Aijun Bai, Feng Wu, Xiaoping Chen</p><p>Abstract: Monte-Carlo tree search (MCTS) has been drawing great interest in recent years for planning and learning under uncertainty. One of the key challenges is the trade-off between exploration and exploitation. To address this, we present a novel approach for MCTS using Bayesian mixture modeling and inference based Thompson sampling and apply it to the problem of online planning in MDPs. Our algorithm, named Dirichlet-NormalGamma MCTS (DNG-MCTS), models the uncertainty of the accumulated reward for actions in the search tree as a mixture of Normal distributions. We perform inferences on the mixture in Bayesian settings by choosing conjugate priors in the form of combinations of Dirichlet and NormalGamma distributions and select the best action at each decision node using Thompson sampling. Experimental results conﬁrm that our algorithm advances the state-of-the-art UCT approach with better values on several benchmark problems. 1</p><br/>
<h2>reference text</h2><p>[1] S. Gelly and D. Silver. Monte-carlo tree search and rapid action value estimation in computer go. Artiﬁcial Intelligence, 175(11):1856–1875, 2011.</p>
<p>[2] Mark HM Winands, Yngvi Bjornsson, and J Saito. Monte carlo tree search in lines of action. IEEE Transactions on Computational Intelligence and AI in Games, 2(4):239–250, 2010.</p>
<p>[3] L. Kocsis and C. Szepesv´ ri. Bandit based monte-carlo planning. In European Conference on a Machine Learning, pages 282–293, 2006.</p>
<p>[4] D. Silver and J. Veness. Monte-carlo planning in large pomdps. In Advances in Neural Information Processing Systems, pages 2164–2172, 2010.</p>
<p>[5] Feng Wu, Shlomo Zilberstein, and Xiaoping Chen. Online planning for ad hoc autonomous agent teams. In International Joint Conference on Artiﬁcial Intelligence, pages 439–445, 2011.</p>
<p>[6] Arthur Guez, David Silver, and Peter Dayan. Efﬁcient bayes-adaptive reinforcement learning using sample-based search. In Advances in Neural Information Processing Systems, pages 1034–1042, 2012.</p>
<p>[7] John Asmuth and Michael L. Littman. Learning is planning: near bayes-optimal reinforcement learning via monte-carlo tree search. In Uncertainty in Artiﬁcial Intelligence, pages 19–26, 2011.</p>
<p>[8] William R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25:285–294, 1933.</p>
<p>[9] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances Neural Information Processing Systems, pages 2249–2257, 2011.</p>
<p>[10] Emilie Kaufmann, Nathaniel Korda, and R´ mi Munos. Thompson sampling: An optimal ﬁnite e time analysis. In Algorithmic Learning Theory, pages 199–213, 2012.</p>
<p>[11] Richard Dearden, Nir Friedman, and Stuart Russell. Bayesian q-learning. In AAAI Conference on Artiﬁcial Intelligence, pages 761–768, 1998.</p>
<p>[12] Gerald Tesauro, V. T. Rajan, and Richard Segal. Bayesian inference in monte-carlo tree search. In Uncertainty in Artiﬁcial Intelligence, pages 580–588, 2010.</p>
<p>[13] Galin L Jones. On the markov chain central limit theorem. Probability surveys, 1:299–320, 2004.</p>
<p>[14] Anirban DasGupta. Asymptotic theory of statistics and probability. Springer, 2008.</p>
<p>[15] Shipra Agrawal and Navin Goyal. Further optimal regret bounds for thompson sampling. In Artiﬁcial Intelligence and Statistics, pages 99–107, 2013.</p>
<p>[16] Blai Bonet and Hector Geffner. Action selection for mdps: Anytime ao* vs. uct. In AAAI Conference on Artiﬁcial Intelligence, pages 1749–1755, 2012.</p>
<p>[17] Christos H Papadimitriou and Mihalis Yannakakis. Shortest paths without a map. Theoretical Computer Science, 84(1):127–150, 1991.</p>
<p>[18] Patrick Eyerich, Thomas Keller, and Malte Helmert. High-quality policies for the canadian traveler’s problem. In AAAI Conference on Artiﬁcial Intelligence, pages 51–58, 2010.</p>
<p>[19] A.G. Barto, S.J. Bradtke, and S.P. Singh. Learning to act using real-time dynamic programming. Artiﬁcial Intelligence, 72(1-2):81–138, 1995.  9</p>
<br/>
<br/><br/><br/></body>
</html>
