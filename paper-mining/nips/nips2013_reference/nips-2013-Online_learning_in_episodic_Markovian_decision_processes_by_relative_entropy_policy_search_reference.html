<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-235" href="../nips2013/nips-2013-Online_learning_in_episodic_Markovian_decision_processes_by_relative_entropy_policy_search.html">nips2013-235</a> <a title="nips-2013-235-reference" href="#">nips2013-235-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>235 nips-2013-Online learning in episodic Markovian decision processes by relative entropy policy search</h1>
<br/><p>Source: <a title="nips-2013-235-pdf" href="http://papers.nips.cc/paper/4974-online-learning-in-episodic-markovian-decision-processes-by-relative-entropy-policy-search.pdf">pdf</a></p><p>Author: Alexander Zimin, Gergely Neu</p><p>Abstract: We study the problem of online learning in ﬁnite episodic Markov decision processes (MDPs) where the loss function is allowed to change between episodes. The natural performance measure in this learning problem is the regret deﬁned as the difference between the total loss of the best stationary policy and the total loss suffered by the learner. We assume that the learner is given access to a ﬁnite action space A and the state space X has a layered structure with L layers, so that state transitions are only possible between consecutive layers. We describe a variant of the recently proposed Relative Entropy Policy Search algorithm and show that its regret after T episodes is 2 L|X ||A|T log(|X ||A|/L) in the bandit setting and 2L T log(|X ||A|/L) in the full information setting, given that the learner has perfect knowledge of the transition probabilities of the underlying MDP. These guarantees largely improve previously known results under much milder assumptions and cannot be signiﬁcantly improved under general assumptions. 1</p><br/>
<h2>reference text</h2><p>[1] Abernethy, J., Hazan, E., and Rakhlin, A. (2008). Competing in the dark: An efﬁcient algorithm for bandit linear optimization. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), pages 263–274.</p>
<p>[2] Audibert, J. Y., Bubeck, S., and Lugosi, G. (2013). Regret in online combinatorial optimization. Mathematics of Operations Research. to appear.</p>
<p>[3] Bart´ k, G., P´ l, D., Szepesv´ ri, C., and Szita, I. (2011). Online learning. Lecture notes, Univero a a sity of Alberta. https://moodle.cs.ualberta.ca/ﬁle.php/354/notes.pdf.</p>
<p>[4] Boyd, S. and Vandenberghe, L. (2004). Convex Optimization. Cambridge University Press.</p>
<p>[5] Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction, Learning, and Games. Cambridge University Press, New York, NY, USA.</p>
<p>[6] Daniel, C., Neumann, G., and Peters, J. (2012). Hierarchical relative entropy policy search. In Proceedings of the Fifteenth International Conference on Artiﬁcial Intelligence and Statistics, volume 22 of JMLR Workshop and Conference Proceedings, pages 273–281.</p>
<p>[7] Dekel, O. and Hazan, E. (2013). Better rates for any adversarial deterministic mdp. In Dasgupta, S. and McAllester, D., editors, Proceedings of the 30th International Conference on Machine Learning (ICML-13), volume 28, pages 675–683. JMLR Workshop and Conference Proceedings.</p>
<p>[8] Even-Dar, E., Kakade, S. M., and Mansour, Y. (2005). Experts in a Markov decision process. In NIPS-17, pages 401–408.</p>
<p>[9] Even-Dar, E., Kakade, S. M., and Mansour, Y. (2009). Online Markov decision processes. Mathematics of Operations Research, 34(3):726–736.</p>
<p>[10] Gy¨ rgy, A., Linder, T., Lugosi, G., and Ottucs´ k, Gy.. (2007). The on-line shortest path probo a lem under partial monitoring. Journal of Machine Learning Research, 8:2369–2403.</p>
<p>[11] Kakade, S. (2001). A natural policy gradient. In Advances in Neural Information Processing Systems 14 (NIPS), pages 1531–1538.</p>
<p>[12] Koolen, W. M., Warmuth, M. K., and Kivinen, J. (2010). Hedging structured concepts. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), pages 93–105.</p>
<p>[13] Martinet, B. (1970). R´ gularisation d’in´ quations variationnelles par approximations succese e sives. ESAIM: Mathematical Modelling and Numerical Analysis - Mod´ lisation Math´ matique e e et Analyse Num´ rique, 4(R3):154–158. e</p>
<p>[14] Neu, G., Gy¨ rgy, A., and Szepesv´ ri, Cs. (2010a). The online loop-free stochastic shortesto a path problem. In Proceedings of the 23rd Annual Conference on Learning Theory (COLT), pages 231–243.</p>
<p>[15] Neu, G., Gy¨ rgy, A., and Szepesv´ ri, Cs. (2012). The adversarial stochastic shortest path o a problem with unknown transition probabilities. In AISTATS 2012, pages 805–813.</p>
<p>[16] Neu, G., Gy¨ rgy, A., Szepesv´ ri, Cs., and Antos, A. (2010b). Online Markov decision proo a cesses under bandit feedback. In NIPS-23, pages 1804–1812. CURRAN.</p>
<p>[17] Peters, J., M¨ lling, K., and Altun, Y. (2010). Relative entropy policy search. In AAAI 2010, u pages 1607–1612.</p>
<p>[18] Puterman, M. L. (1994). Markov Decision Processes: Discrete Stochastic Dynamic Programming. Wiley-Interscience.</p>
<p>[19] Rakhlin, A. (2009). Lecture notes on online learning.</p>
<p>[20] Rockafellar, R. T. (1976). Monotone Operators and the Proximal Point Algorithm. SIAM Journal on Control and Optimization, 14(5):877–898.</p>
<p>[21] Sutton, R. and Barto, A. (1998). Reinforcement Learning: An Introduction. MIT Press.</p>
<p>[22] Szepesv´ ri, Cs. (2010). Algorithms for Reinforcement Learning. Synthesis Lectures on Artiﬁa cial Intelligence and Machine Learning. Morgan & Claypool Publishers.</p>
<p>[23] Yu, J. Y., Mannor, S., and Shimkin, N. (2009). Markov decision processes with arbitrary reward processes. Mathematics of Operations Research, 34(3):737–757.</p>
<p>[24] Zinkevich, M. (2003). Online convex programming and generalized inﬁnitesimal gradient ascent. In Proceedings of the Twentieth International Conference on Machine Learning, pages 928–936. 9</p>
<br/>
<br/><br/><br/></body>
</html>
