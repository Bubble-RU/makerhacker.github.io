<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-103" href="../nips2013/nips-2013-Efficient_Exploration_and_Value_Function_Generalization_in_Deterministic_Systems.html">nips2013-103</a> <a title="nips-2013-103-reference" href="#">nips2013-103-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>103 nips-2013-Efficient Exploration and Value Function Generalization in Deterministic Systems</h1>
<br/><p>Source: <a title="nips-2013-103-pdf" href="http://papers.nips.cc/paper/4972-efficient-exploration-and-value-function-generalization-in-deterministic-systems.pdf">pdf</a></p><p>Author: Zheng Wen, Benjamin Van Roy</p><p>Abstract: We consider the problem of reinforcement learning over episodes of a ﬁnitehorizon deterministic system and as a solution propose optimistic constraint propagation (OCP), an algorithm designed to synthesize efﬁcient exploration and value function generalization. We establish that when the true value function Q⇤ lies within the hypothesis class Q, OCP selects optimal actions over all but at most dimE [Q] episodes, where dimE denotes the eluder dimension. We establish further efﬁciency and asymptotic performance guarantees that apply even if Q⇤ does not lie in Q, for the special case where Q is the span of pre-speciﬁed indicator functions over disjoint sets. 1</p><br/>
<h2>reference text</h2><br/>
<br/><br/><br/></body>
</html>
