<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>135 nips-2013-Heterogeneous-Neighborhood-based Multi-Task Local Learning Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-135" href="../nips2013/nips-2013-Heterogeneous-Neighborhood-based_Multi-Task_Local_Learning_Algorithms.html">nips2013-135</a> <a title="nips-2013-135-reference" href="#">nips2013-135-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>135 nips-2013-Heterogeneous-Neighborhood-based Multi-Task Local Learning Algorithms</h1>
<br/><p>Source: <a title="nips-2013-135-pdf" href="http://papers.nips.cc/paper/5211-heterogeneous-neighborhood-based-multi-task-local-learning-algorithms.pdf">pdf</a></p><p>Author: Yu Zhang</p><p>Abstract: All the existing multi-task local learning methods are deﬁned on homogeneous neighborhood which consists of all data points from only one task. In this paper, different from existing methods, we propose local learning methods for multitask classiﬁcation and regression problems based on heterogeneous neighborhood which is deﬁned on data points from all tasks. Speciﬁcally, we extend the knearest-neighbor classiﬁer by formulating the decision function for each data point as a weighted voting among the neighbors from all tasks where the weights are task-speciﬁc. By deﬁning a regularizer to enforce the task-speciﬁc weight matrix to approach a symmetric one, a regularized objective function is proposed and an efﬁcient coordinate descent method is developed to solve it. For regression problems, we extend the kernel regression to multi-task setting in a similar way to the classiﬁcation case. Experiments on some toy data and real-world datasets demonstrate the effectiveness of our proposed methods. 1</p><br/>
<h2>reference text</h2><p>[1] R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and unlabeled data. Journal of Machine Learning Research, 6:1817–1853, 2005.</p>
<p>[2] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In B. Sch¨ lkopf, J. C. Platt, and o T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 41–48, Vancouver, British Columbia, Canada, 2006.</p>
<p>[3] B. Bakker and T. Heskes. Task clustering and gating for bayesian multitask learning. Journal of Machine Learning Research, 4:83–99, 2003.</p>
<p>[4] J. Baxter. A Bayesian/information theoretic model of learning to learn via multiple task sampling. Machine Learning, 28(1):7–39, 1997.</p>
<p>[5] J. C. Bezdek and R. J. Hathaway. Convergence of alternating optimization. Neural, Parallel & Scientiﬁc Computations, 11(4):351–368, 2003.</p>
<p>[6] E. Bonilla, K. M. A. Chai, and C. Williams. Multi-task Gaussian process prediction. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 153–160, Vancouver, British Columbia, Canada, 2007.</p>
<p>[7] L. Bottou and V. Vapnik. Local learning algorithms. Neural Computation, 4(6):888–900, 1992.</p>
<p>[8] R. Caruana. Multitask learning. Machine Learning, 28(1):41–75, 1997.</p>
<p>[9] T. Evgeniou and M. Pontil. Regularized multi-task learning. In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 109–117, Seattle, Washington, USA, 2004.</p>
<p>[10] T. V. Gestel, J. A. K. Suykens, B. Baesens, S. Viaene, J. Vanthienen, G. Dedene, B. De Moor, and J. Vandewalle. Benchmarking least squares support vector machine classiﬁers. Machine Learning, 54(1):5–32, 2004.</p>
<p>[11] M. Grant and S. Boyd. CVX: Matlab software for disciplined convex programming, 2011.</p>
<p>[12] L. Jacob, F. Bach, and J.-P. Vert. Clustered multi-task learning: a convex formulation. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 745–752, Vancouver, British Columbia, Canada, 2008.</p>
<p>[13] A. Kumar and H. Daum´ III. Learning task grouping and overlap in multi-task learning. In Proceedings e of the 29 th International Conference on Machine Learning, Edinburgh, Scotland, UK, 2012.</p>
<p>[14] S. Parameswaran and K. Weinberger. Large margin multi-task metric learning. In J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R.S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 1867–1875, 2010.</p>
<p>[15] J. C. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods: Support Vector o Learning. MIT Press, 1998.</p>
<p>[16] S. Thrun. Is learning the n-th thing any easier than learning the ﬁrst? In D. S. Touretzky, M. Mozer, and M. E. Hasselmo, editors, Advances in Neural Information Processing Systems 8, pages 640–646, Denver, CO, 1995.</p>
<p>[17] S. Thrun and J. O’Sullivan. Discovering structure in multiple learning tasks: The TC algorithm. In Proceedings of the Thirteenth International Conference on Machine Learning, pages 489–497, Bari, Italy, 1996.</p>
<p>[18] M. Wu and B. Sch¨ lkopf. A local learning approach for clustering. In B. Sch¨ lkopf, J. C. Platt, and o o T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 1529–1536, Vancouver, British Columbia, Canada, 2006.</p>
<p>[19] M. Wu, K. Yu, S. Yu, and B. Sch¨ lkopf. Local learning projections. In Proceedings of the Twenty-Fourth o International Conference on Machine Learning, pages 1039–1046, Corvallis, Oregon, USA, 2007.</p>
<p>[20] Y. Zhang and D.-Y. Yeung. A convex formulation for learning task relationships in multi-task learning. In Proceedings of the 26th Conference on Uncertainty in Artiﬁcial Intelligence, pages 733–742, Catalina Island, California, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
