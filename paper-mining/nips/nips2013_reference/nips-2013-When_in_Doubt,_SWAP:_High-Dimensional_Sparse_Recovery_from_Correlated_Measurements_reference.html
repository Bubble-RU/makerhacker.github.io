<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-354" href="../nips2013/nips-2013-When_in_Doubt%2C_SWAP%3A_High-Dimensional_Sparse_Recovery_from_Correlated_Measurements.html">nips2013-354</a> <a title="nips-2013-354-reference" href="#">nips2013-354-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>354 nips-2013-When in Doubt, SWAP: High-Dimensional Sparse Recovery from Correlated Measurements</h1>
<br/><p>Source: <a title="nips-2013-354-pdf" href="http://papers.nips.cc/paper/5003-when-in-doubt-swap-high-dimensional-sparse-recovery-from-correlated-measurements.pdf">pdf</a></p><p>Author: Divyanshu Vats, Richard Baraniuk</p><p>Abstract: We consider the problem of accurately estimating a high-dimensional sparse vector using a small number of linear measurements that are contaminated by noise. It is well known that standard computationally tractable sparse recovery algorithms, such as the Lasso, OMP, and their various extensions, perform poorly when the measurement matrix contains highly correlated columns. We develop a simple greedy algorithm, called SWAP, that iteratively swaps variables until a desired loss function cannot be decreased any further. SWAP is surprisingly effective in handling measurement matrices with high correlations. We prove that SWAP can easily be used as a wrapper around standard sparse recovery algorithms for improved performance. We theoretically quantify the statistical guarantees of SWAP and complement our analysis with numerical results on synthetic and real data.</p><br/>
<h2>reference text</h2><p>[1] M. Segal, K. Dahlquist, and B. Conklin, “Regression approaches for microarray data analysis,” Journal of Computational Biology, vol. 10, no. 6, pp. 961–980, 2003.</p>
<p>[2] H. Zhu and G. Giannakis, “Sparse overcomplete representations for efﬁcient identiﬁcation of power line outages,” IEEE Transactions on Power Systems, vol. 27, no. 4, pp. 2215 –2224, nov. 2012.</p>
<p>[3] E. J. Cand` s, J. Romberg, and T. Tao, “Robust uncertainty principles: Exact signal reconstruce tion from highly incomplete frequency information,” IEEE Trans. Information Theory, vol. 52, no. 2, pp. 489–509, 2006.</p>
<p>[4] M. F. Duarte, M. A. Davenport, D. Takhar, J. N. Laska, T. Sun, K. F. Kelly, and R. G. Baraniuk, “Single-pixel imaging via compressive sampling,” IEEE Signal Processing Magazine, vol. 25, no. 2, pp. 83–91, Mar. 2008.</p>
<p>[5] N. Meinshausen and P. B¨ hlmann, “High-dimensional graphs and variable selection with the u Lasso,” Annals of Statistics, vol. 34, no. 3, pp. 1436, 2006.</p>
<p>[6] P. Ravikumar, M. Wainwright, and J. Lafferty, “High-dimensional Ising model selection using 1 -egularized logistic regression,” Annals of Statistics, vol. 38, no. 3, pp. 1287–1319, 2010. 8</p>
<p>[7] G. Varoquaux, A. Gramfort, and B. Thirion, “Small-sample brain mapping: sparse recovery on spatially correlated designs with randomization and clustering,” in Proceedings of the 29th International Conference on Machine Learning (ICML-12), 2012, pp. 1375–1382.</p>
<p>[8] P. Zhao and B. Yu, “On model selection consistency of Lasso,” Journal of Machine Learning Research, vol. 7, pp. 2541–2563, 2006.</p>
<p>[9] J. A. Tropp and A. C. Gilbert, “Signal recovery from random measurements via orthogonal matching pursuit,” IEEE Transactions Information Theory, vol. 53, no. 12, pp. 4655–4666, 2007.</p>
<p>[10] M. J. Wainwright, “Sharp thresholds for noisy and high-dimensional recovery of sparsity using 1 -constrained quadratic programming (Lasso),” IEEE Transactions Information Theory, vol. 55, no. 5, May 2009.</p>
<p>[11] N. Meinshausen and B. Yu, “Lasso-type recovery of sparse representations for highdimensional data,” Annals of Statistics, vol. 37, no. 1, pp. 246–270, 2009.</p>
<p>[12] P. J. Bickel, Y. Ritov, and A. B. Tsybakov, “Simultaneous analysis of Lasso and Dantzig selector,” Annals of Statistics, vol. 37, no. 4, pp. 1705–1732, 2009.</p>
<p>[13] P. B¨ hlmann and S. Van De Geer, Statistics for High-Dimensional Data: Methods, Theory and u Applications, Springer-Verlag New York Inc, 2011.</p>
<p>[14] H. Zou and T. Hastie, “Regularization and variable selection via the elastic net,” Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 67, no. 2, pp. 301–320, 2005.</p>
<p>[15] Y. She, “Sparse regression with exact clustering,” Electronic Journal Statistics, vol. 4, pp. 1055–1096, 2010.</p>
<p>[16] E. Grave, G. R. Obozinski, and F. R. Bach, “Trace Lasso: A trace norm regularization for correlated designs,” in Advances in Neural Information Processing Systems 24, J. Shawetaylor, R. Zemel, P. Bartlett, F. Pereira, and K. Weinberger, Eds., 2011, pp. 2187–2195.</p>
<p>[17] J. Huang, S. Ma, H. Li, and C. Zhang, “The sparse laplacian shrinkage estimator for highdimensional regression,” Annals of Statistics, vol. 39, no. 4, pp. 2021, 2011.</p>
<p>[18] P. B¨ hlmann, P. R¨ timann, S. van de Geer, and C.-H. Zhang, “Correlated variables in regresu u sion: clustering and sparse estimation,” Journal of Statistical Planning and Inference, vol. 143, pp. 1835–1858, Nov. 2013.</p>
<p>[19] J. Khan, J. S. Wei, M. Ringner, L. H. Saal, M. Ladanyi, F. Westermann, F. Berthold, M. Schwab, C. R. Antonescu, C. Peterson, et al., “Classiﬁcation and diagnostic prediction of cancers using gene expression proﬁling and artiﬁcial neural networks,” Nature medicine, vol. 7, no. 6, pp. 673–679, 2001.</p>
<p>[20] T. Zhang, “Adaptive forward-backward greedy algorithm for learning sparse representations,” IEEE Transactions Information Theory, vol. 57, no. 7, pp. 4689–4708, 2011.</p>
<p>[21] D. Needell and J. A. Tropp, “CoSaMP: Iterative signal recovery from incomplete and inaccurate samples,” Applied and Computational Harmonic Analysis, vol. 26, no. 3, pp. 301–321, 2009.</p>
<p>[22] T. Zhang, “Some sharp performance bounds for least squares regression with l1 regularization,” The Annals of Statistics, vol. 37, no. 5A, pp. 2109–2144, 2009.</p>
<p>[23] L. Wasserman and K. Roeder, “High dimensional variable selection,” Annals of statistics, vol. 37, no. 5A, pp. 2178, 2009.</p>
<p>[24] T. Zhang, “Analysis of multi-stage convex relaxation for sparse regularization,” Journal of Machine Learning Research, vol. 11, pp. 1081–1107, Mar. 2010.</p>
<p>[25] S. van de Geer, P. B¨ hlmann, and S. Zhou, “The adaptive and the thresholded lasso for potenu tially misspeciﬁed models (and a lower bound for the lasso),” Electronic Journal of Statistics, vol. 5, pp. 688–749, 2011.</p>
<p>[26] N. Meinshausen and P. B¨ hlmann, “Stability selection,” Journal of the Royal Statistical u Society: Series B (Statistical Methodology), vol. 72, no. 4, pp. 417–473, 2010.</p>
<p>[27] D. Vats and R. G. Baraniuk, “Swapping variables for high-dimensional sparse regression with correlated measurements,” arXiv:1312.1706, 2013.  9</p>
<br/>
<br/><br/><br/></body>
</html>
