<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-23" href="../nips2013/nips-2013-Active_Learning_for_Probabilistic_Hypotheses_Using_the_Maximum_Gibbs_Error_Criterion.html">nips2013-23</a> <a title="nips-2013-23-reference" href="#">nips2013-23-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>23 nips-2013-Active Learning for Probabilistic Hypotheses Using the Maximum Gibbs Error Criterion</h1>
<br/><p>Source: <a title="nips-2013-23-pdf" href="http://papers.nips.cc/paper/4958-active-learning-for-probabilistic-hypotheses-using-the-maximum-gibbs-error-criterion.pdf">pdf</a></p><p>Author: Nguyen Viet Cuong, Wee Sun Lee, Nan Ye, Kian Ming A. Chai, Hai Leong Chieu</p><p>Abstract: We introduce a new objective function for pool-based Bayesian active learning with probabilistic hypotheses. This objective function, called the policy Gibbs error, is the expected error rate of a random classiﬁer drawn from the prior distribution on the examples adaptively selected by the active learning policy. Exact maximization of the policy Gibbs error is hard, so we propose a greedy strategy that maximizes the Gibbs error at each iteration, where the Gibbs error on an instance is the expected error of a random classiﬁer selected from the posterior label distribution on that instance. We apply this maximum Gibbs error criterion to three active learning scenarios: non-adaptive, adaptive, and batch active learning. In each scenario, we prove that the criterion achieves near-maximal policy Gibbs error when constrained to a ﬁxed budget. For practical implementations, we provide approximations to the maximum Gibbs error criterion for Bayesian conditional random ﬁelds and transductive Naive Bayes. Our experimental results on a named entity recognition task and a text classiﬁcation task show that the maximum Gibbs error criterion is an effective active learning criterion for noisy models. 1</p><br/>
<h2>reference text</h2><p>[1] Andrew McCallum and Kamal Nigam. Employing EM and Pool-Based Active Learning for Text Classiﬁcation. In International Conference on Machine Learning (ICML), pages 350–358, 1998.</p>
<p>[2] Daniel Golovin and Andreas Krause. Adaptive Submodularity: Theory and Applications in Active Learning and Stochastic Optimization. Journal of Artiﬁcial Intelligence Research, 42(1):427–486, 2011.</p>
<p>[3] Yuxin Chen and Andreas Krause. Near-optimal Batch Mode Active Learning and Adaptive Submodular Optimization. In International Conference on Machine Learning (ICML), pages 160–168, 2013.</p>
<p>[4] Constantino Tsallis and Edgardo Brigatti. Nonextensive statistical mechanics: A brief introduction. Continuum Mechanics and Thermodynamics, 16(3):223–235, 2004.</p>
<p>[5] Steven CH Hoi, Rong Jin, Jianke Zhu, and Michael R Lyu. Batch Mode Active Learning and Its Application to Medical Image Classiﬁcation. In International Conference on Machine learning (ICML), pages 417–424. ACM, 2006.</p>
<p>[6] John Lafferty, Andrew McCallum, and Fernando CN Pereira. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In International Conference on Machine Learning (ICML), pages 282–289, 2001.</p>
<p>[7] Bassem Sayraﬁ, Dirk Van Gucht, and Marc Gyssens. The implication problem for measure-based constraints. Information Systems, 33(2):221–239, 2008.</p>
<p>[8] G.L. Nemhauser and L.A. Wolsey. Best Algorithms for Approximating the Maximum of a Submodular Set Function. Mathematics of Operations Research, 3(3):177–188, 1978.</p>
<p>[9] Sunita Sarawagi and William W. Cohen. Semi-Markov Conditional Random Fields for Information Extraction. Advances in Neural Information Processing Systems (NIPS), 17:1185–1192, 2004.</p>
<p>[10] Viet Cuong Nguyen, Nan Ye, Wee Sun Lee, and Hai Leong Chieu. Semi-Markov Conditional Random Field with High-Order Features. In ICML Workshop on Structured Sparsity: Learning and Inference, 2011.</p>
<p>[11] Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 Shared Task: LanguageIndependent Named Entity Recognition. In Proceedings of the 17th Conference on Natural Language Learning (HLT-NAACL 2003), pages 142–147, 2003.</p>
<p>[12] Burr Settles and Mark Craven. An Analysis of Active Learning Strategies for Sequence Labeling Tasks. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1070–1079. Association for Computational Linguistics, 2008.</p>
<p>[13] Thorsten Joachims. A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization. Technical report, DTIC Document, 1996.</p>
<p>[14] Burr Settles. Active Learning Literature Survey. Technical Report 1648, University of WisconsinMadison, 2009.</p>
<p>[15] Robert Nowak. Noisy Generalized Binary Search. Advances in Neural Information Processing Systems (NIPS), 22:1366–1374, 2009.</p>
<p>[16] Daniel Golovin, Andreas Krause, and Debajyoti Ray. Near-Optimal Bayesian Active Learning with Noisy Observations. In Advances in Neural Information Processing Systems (NIPS), pages 766–774, 2010.  9</p>
<br/>
<br/><br/><br/></body>
</html>
