<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-106" href="../nips2013/nips-2013-Eluder_Dimension_and_the_Sample_Complexity_of_Optimistic_Exploration.html">nips2013-106</a> <a title="nips-2013-106-reference" href="#">nips2013-106-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>106 nips-2013-Eluder Dimension and the Sample Complexity of Optimistic Exploration</h1>
<br/><p>Source: <a title="nips-2013-106-pdf" href="http://papers.nips.cc/paper/4909-eluder-dimension-and-the-sample-complexity-of-optimistic-exploration.pdf">pdf</a></p><p>Author: Dan Russo, Benjamin Van Roy</p><p>Abstract: This paper considers the sample complexity of the multi-armed bandit with dependencies among the arms. Some of the most successful algorithms for this problem use the principle of optimism in the face of uncertainty to guide exploration. The clearest example of this is the class of upper conﬁdence bound (UCB) algorithms, but recent work has shown that a simple posterior sampling algorithm, sometimes called Thompson sampling, can be analyzed in the same manner as optimistic approaches. In this paper, we develop a regret bound that holds for both classes of algorithms. This bound applies broadly and can be specialized to many model classes. It depends on a new notion we refer to as the eluder dimension, which measures the degree of dependence among action rewards. Compared to UCB algorithm regret bounds for speciﬁc model classes, our general bound matches the best available for linear models and is stronger than the best available for generalized linear models. 1</p><br/>
<h2>reference text</h2><p>[1] V. Dani, T.P. Hayes, and S.M. Kakade. Stochastic linear optimization under bandit feedback. In Proceedings of the 21st Annual Conference on Learning Theory (COLT), pages 355–366, 2008.</p>
<p>[2] Y. Abbasi-Yadkori, D. P´ l, and C. Szepesv´ ri. Improved algorithms for linear stochastic bandits. Advances a a in Neural Information Processing Systems, 24, 2011.</p>
<p>[3] P. Rusmevichientong and J.N. Tsitsiklis. Linearly parameterized bandits. Mathematics of Operations Research, 35(2):395–411, 2010.</p>
<p>[4] R. Kleinberg, A. Slivkins, and E. Upfal. Multi-armed bandits in metric spaces. In Proceedings of the 40th ACM Symposium on Theory of Computing, 2008.</p>
<p>[5] S. Bubeck, R. Munos, G. Stoltz, and C. Szepesv´ ri. X-armed bandits. Journal of Machine Learning a Research, 12:15871627, 2011.</p>
<p>[6] N. Srinivas, A. Krause, S.M. Kakade, and M. Seeger. Information-theoretic regret bounds for Gaussian process optimization in the bandit setting. Information Theory, IEEE Transactions on, 58(5):3250 –3265, may 2012. ISSN 0018-9448. doi: 10.1109/TIT.2011.2182033.</p>
<p>[7] S. Filippi, O. Capp´ , A. Garivier, and C. Szepesv´ ri. Parametric bandits: The generalized linear case. e a Advances in Neural Information Processing Systems, 23:1–9, 2010.</p>
<p>[8] Y. Abbasi-Yadkori, D. Pal, and C. Szepesv´ ri. Online-to-conﬁdence-set conversions and application to a sparse stochastic bandits. In Conference on Artiﬁcial Intelligence and Statistics (AISTATS), 2012.</p>
<p>[9] T.L. Lai and H. Robbins. Asymptotically efﬁcient adaptive allocation rules. Advances in applied mathematics, 6(1):4–22, 1985.</p>
<p>[10] T.L. Lai. Adaptive treatment allocation and the multi-armed bandit problem. The Annals of Statistics, pages 1091–1114, 1987.</p>
<p>[11] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235–256, 2002.</p>
<p>[12] O. Capp´ , A. Garivier, O.-A. Maillard, R. Munos, and G. Stoltz. Kullback-Leibler upper conﬁdence e bounds for optimal sequential allocation. Submitted to the Annals of Statistics.</p>
<p>[13] T. Jaksch, R. Ortner, and P. Auer. Near-optimal regret bounds for reinforcement learning. The Journal of Machine Learning Research, 99:1563–1600, 2010.</p>
<p>[14] P.L. Bartlett and A. Tewari. Regal: A regularization based algorithm for reinforcement learning in weakly communicating mdps. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artiﬁcial Intelligence, pages 35–42. AUAI Press, 2009.</p>
<p>[15] L. Kocsis and C. Szepesv´ ri. Bandit based monte-carlo planning. In Machine Learning: ECML 2006, a pages 282–293. Springer, 2006.</p>
<p>[16] D. Russo and B. Van Roy. Learning to optimize via posterior sampling. arXiv preprint arXiv:1301.2609, 2013.</p>
<p>[17] W.R. Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4):285–294, 1933.</p>
<p>[18] S.L. Scott. A modern Bayesian look at the multi-armed bandit. Applied Stochastic Models in Business and Industry, 26(6):639–658, 2010.</p>
<p>[19] O. Chapelle and L. Li. An empirical evaluation of Thompson sampling. In Neural Information Processing Systems (NIPS), 2011.</p>
<p>[20] S. Agrawal and N. Goyal. Analysis of Thompson sampling for the multi-armed bandit problem. 2012.</p>
<p>[21] S. Agrawal and N. Goyal. Further optimal regret bounds for Thompson sampling. arXiv preprint arXiv:1209.3353, 2012.</p>
<p>[22] E. Kauffmann, N. Korda, and R. Munos. Thompson sampling: an asymptotically optimal ﬁnite time analysis. In International Conference on Algorithmic Learning Theory, 2012.</p>
<p>[23] S. Agrawal and N. Goyal. Thompson sampling for contextual bandits with linear payoffs. arXiv preprint arXiv:1209.3352, 2012.</p>
<p>[24] A. Beygelzimer, J. Langford, L. Li, L. Reyzin, and R.E. Schapire. Contextual bandit algorithms with supervised learning guarantees. In Conference on Artiﬁcial Intelligence and Statistics (AISTATS), volume 15. JMLR Workshop and Conference Proceedings, 2011.</p>
<p>[25] K. Amin, M. Kearns, and U. Syed. Bandits, query learning, and the haystack dimension. In Proceedings of the 24th Annual Conference on Learning Theory (COLT), 2011.</p>
<p>[26] P. Auer. Using conﬁdence bounds for exploitation-exploration trade-offs. The Journal of Machine Learning Research, 3:397–422, 2003.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
