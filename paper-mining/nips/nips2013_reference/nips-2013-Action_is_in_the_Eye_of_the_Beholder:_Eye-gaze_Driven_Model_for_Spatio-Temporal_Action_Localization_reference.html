<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-22" href="../nips2013/nips-2013-Action_is_in_the_Eye_of_the_Beholder%3A_Eye-gaze_Driven_Model_for_Spatio-Temporal_Action_Localization.html">nips2013-22</a> <a title="nips-2013-22-reference" href="#">nips2013-22-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 nips-2013-Action is in the Eye of the Beholder: Eye-gaze Driven Model for Spatio-Temporal Action Localization</h1>
<br/><p>Source: <a title="nips-2013-22-pdf" href="http://papers.nips.cc/paper/5197-action-is-in-the-eye-of-the-beholder-eye-gaze-driven-model-for-spatio-temporal-action-localization.pdf">pdf</a></p><p>Author: Nataliya Shapovalova, Michalis Raptis, Leonid Sigal, Greg Mori</p><p>Abstract: We propose a weakly-supervised structured learning approach for recognition and spatio-temporal localization of actions in video. As part of the proposed approach, we develop a generalization of the Max-Path search algorithm which allows us to efﬁciently search over a structured space of multiple spatio-temporal paths while also incorporating context information into the model. Instead of using spatial annotations in the form of bounding boxes to guide the latent model during training, we utilize human gaze data in the form of a weak supervisory signal. This is achieved by incorporating eye gaze, along with the classiﬁcation, into the structured loss within the latent SVM learning framework. Experiments on a challenging benchmark dataset, UCF-Sports, show that our model is more accurate, in terms of classiﬁcation, and achieves state-of-the-art results in localization. In addition, our model can produce top-down saliency maps conditioned on the classiﬁcation label and localized latent paths. 1</p><br/>
<h2>reference text</h2><p>[1] M. Blaschko and C. Lampert. Learning to localize objects with structured output regression. ECCV, 2008.</p>
<p>[2] C. Chen and K. Grauman. Efﬁcient activity detection with max-subgraph search. In CVPR, 2012.</p>
<p>[3] K. G. Derpanis, M. Sizintsev, K. Cannons, and R. P. Wildes. Efﬁcient action spotting based on a spacetime oriented structure representation. In CVPR, 2010.</p>
<p>[4] D. V. Essen, B. Olshausen, C. Anderson, and J. Gallant. Pattern recognition, attention, and information bottlenecks in the primate visual system. SPIE Conference on Visual Information Processing: From Neurons to Chips, 1991.</p>
<p>[5] A. Fathi, Y. Li, and J. M. Rehg. Learning to recognize daily actions using gaze. In ECCV, 2012.</p>
<p>[6] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. IEEE PAMI, 2010.</p>
<p>[7] Y. Ke, R. Sukthankar, and M. Hebert. Event detection in crowded videos. In ICCV, 2007.</p>
<p>[8] A. Kovashka and K. Grauman. Learning a Hierarchy of Discriminative Space-Time Neighborhood Features for Human Action Recognition. In CVPR, 2010.</p>
<p>[9] T. Lan, Y. Wang, and G. Mori. Discriminative ﬁgure-centric models for joint action localization and recognition. In ICCV, 2011.</p>
<p>[10] I. Laptev. On space-time interest points. IJCV, 64, 2005.</p>
<p>[11] S. Mathe and C. Sminchisescu. Dynamic eye movement datasets and learnt saliency models for visual action recognition. In ECCV, 2012.</p>
<p>[12] M. Raptis, I. Kokkinos, and S. Soatto. Discovering discriminative action parts from mid-level video representations. In CVPR, 2012.</p>
<p>[13] M. Raptis and L. Sigal. Poselet key-framing: A model for human activity recognition. In CVPR, 2013.</p>
<p>[14] M. Rodriguez, J. Ahmed, and M. Shah. Action MACH: a spatio-temporal maximum average correlation height ﬁlter for action recognition. In CVPR, 2008.</p>
<p>[15] M. Ryoo and J. Aggarwal. Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities. In ICCV, 2009.</p>
<p>[16] N. Shapovalova, A. Vahdat, K. Cannons, T. Lan, and G. Mori. Similarity constrained latent support vector machine: An application to weakly supervised action classiﬁcation. In ECCV, 2012.</p>
<p>[17] P. Siva and T. Xiang. Weakly supervised action detection. In BMVC, 2011.</p>
<p>[18] D. Tran and J. Yuan. Optimal spatio-temporal path discovery for video event detection. In CVPR, 2011.</p>
<p>[19] D. Tran and J. Yuan. Max-margin structured output regression for spatio-temporal action localization. In NIPS, 2012.</p>
<p>[20] P. Turaga, R. Chellappa, V. Subrahmanian, and O. Udrea. Machine recognition of human activities: A survey. IEEE Transactions on Circuits and Systems for Video Technology, 18(11):1473–1488, 2008.</p>
<p>[21] E. Vig, M. Dorr, and D. Cox. Space-variant descriptor sampling for action recognition based on saliency and eye movements. In ECCV, 2012.</p>
<p>[22] H. Wang, M. M. Ullah, A. Kl¨ ser, I. Laptev, and C. Schmid. Evaluation of local spatio-temporal features a for action recognition. In BMVC, 2009.</p>
<p>[23] Y. Wang and G. Mori. Hidden part models for human action recognition: Probabilistic vs. max-margin. IEEE PAMI, 2010.</p>
<p>[24] D. Weinland, R. Ronfard, and E. Boyer. A survey of vision-based methods for action representation, segmentation and recognition. Computer Vision and Image Understanding, 115(2):224–241, 2011.</p>
<p>[25] J. Yang and M.-H. Yang. Top-down visual saliency via joint crf and dictionary learning. In CVPR, 2012.</p>
<p>[26] C.-N. J. Yu and T. Joachims. Learning structural svms with latent variables. In ICML, 2009.</p>
<p>[27] J. Yuan, Z. Liu, and Y. Wu. Discriminative subvolume search for efﬁcient action detection. In CVPR, 2009.</p>
<p>[28] J. Yuan, Z. Liu, and Y. Wu. Discriminative video pattern search for efﬁcient action detection. IEEE PAMI, 33(9), 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
