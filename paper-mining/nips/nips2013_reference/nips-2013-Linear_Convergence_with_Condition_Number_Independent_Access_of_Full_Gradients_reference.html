<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 nips-2013-Linear Convergence with Condition Number Independent Access of Full Gradients</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-175" href="../nips2013/nips-2013-Linear_Convergence_with_Condition_Number_Independent_Access_of_Full_Gradients.html">nips2013-175</a> <a title="nips-2013-175-reference" href="#">nips2013-175-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>175 nips-2013-Linear Convergence with Condition Number Independent Access of Full Gradients</h1>
<br/><p>Source: <a title="nips-2013-175-pdf" href="http://papers.nips.cc/paper/4940-linear-convergence-with-condition-number-independent-access-of-full-gradients.pdf">pdf</a></p><p>Author: Lijun Zhang, Mehrdad Mahdavi, Rong Jin</p><p>Abstract: For smooth and strongly convex optimizations, the optimal iteration complexity of √ the gradient-based algorithm is O( κ log 1/ǫ), where κ is the condition number. In the case that the optimization problem is ill-conditioned, we need to evaluate a large number of full gradients, which could be computationally expensive. In this paper, we propose to remove the dependence on the condition number by allowing the algorithm to access stochastic gradients of the objective function. To this end, we present a novel algorithm named Epoch Mixed Gradient Descent (EMGD) that is able to utilize two kinds of gradients. A distinctive step in EMGD is the mixed gradient descent, where we use a combination of the full and stochastic gradients to update the intermediate solution. Theoretical analysis shows that EMGD is able to ﬁnd an ǫ-optimal solution by computing O(log 1/ǫ) full gradients and O(κ2 log 1/ǫ) stochastic gradients. 1</p><br/>
<h2>reference text</h2><p>[1] A. Agarwal, P. L. Bartlett, P. Ravikumar, and M. J. Wainwright. Information-theoretic lower bounds on the oracle complexity of stochastic convex optimization. IEEE Transactions on Information Theory, 58(5):3235–3249, 2012.</p>
<p>[2] D. P. Bertsekas. A new class of incremental gradient methods for least squares problems. SIAM Journal on Optimization, 7(4):913–926, 1997.</p>
<p>[3] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[4] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge University Press, 2006.</p>
<p>[5] M. Friedlander and M. Schmidt. Hybrid deterministic-stochastic methods for data ﬁtting. SIAM Journal on Scientiﬁc Computing, 34(3):A1380–A1405, 2012.</p>
<p>[6] S. Ghadimi and G. Lan. Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: a generic algorithmic framework. SIAM Journal on Optimization, 22(4):1469– 1492, 2012.</p>
<p>[7] A. Gittens and J. A. Tropp. Tail bounds for all eigenvalues of a sum of random matrices. ArXiv e-prints, arXiv:1104.4513, 2011.</p>
<p>[8] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer Series in Statistics. Springer New York, 2009.</p>
<p>[9] E. Hazan and S. Kale. Beyond the regret minimization barrier: an optimal algorithm for stochastic strongly-convex optimization. In Proceedings of the 24th Annual Conference on Learning Theory, pages 421–436, 2011.</p>
<p>[10] A. Juditsky and Y. Nesterov. Primal-dual subgradient methods for minimizing uniformly convex functions. Technical report, 2010.</p>
<p>[11] G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133:365– 397, 2012.</p>
<p>[12] K. Marti. On solutions of stochastic programming problems by descent procedures with stochastic and deterministic directions. Methods of Operations Research, 33:281–293, 1979.</p>
<p>[13] K. Marti and E. Fuchs. Rates of convergence of semi-stochastic approximation procedures for solving stochastic optimization problems. Optimization, 17(2):243–265, 1986.</p>
<p>[14] A. Nemirovski, A. Juditsky, G. Lan, and A. Shapiro. Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609, 2009.</p>
<p>[15] A. Nemirovski and D. B. Yudin. Problem complexity and method efﬁciency in optimization. John Wiley & Sons Ltd, 1983.</p>
<p>[16] Y. Nesterov. A method for unconstrained convex minimization problem with the rate of convergence O(1/k2 ). Doklady AN SSSR (translated as Soviet. Math. Docl.), 269:543–547, 1983.</p>
<p>[17] Y. Nesterov. Introductory lectures on convex optimization: a basic course, volume 87 of Applied optimization. Kluwer Academic Publishers, 2004.</p>
<p>[18] Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 103(1):127– 152, 2005.</p>
<p>[19] Y. Nesterov. Gradient methods for minimizing composite objective function. Core discussion papers, 2007.</p>
<p>[20] D. P. Palomar and Y. C. Eldar, editors. Convex Optimization in Signal Processing and Communications. 2010, Cambridge University Press.</p>
<p>[21] A. Rakhlin, O. Shamir, and K. Sridharan. Making gradient descent optimal for strongly convex stochastic optimization. In Proceedings of the 29th International Conference on Machine Learning, pages 449–456, 2012.</p>
<p>[22] N. L. Roux, M. Schmidt, and F. Bach. A stochastic gradient method with an exponential convergence rate for ﬁnite training sets. In Advances in Neural Information Processing Systems 25, pages 2672–2680, 2012.</p>
<p>[23] S. Shalev-Shwartz and T. Zhang. Stochastic dual coordinate ascent methods for regularized loss minimization. Journal of Machine Learning Research, 14:567–599, 2013.</p>
<p>[24] S. Sra, S. Nowozin, and S. J. Wright, editors. Optimization for Machine Learning. The MIT Press, 2011.</p>
<p>[25] Q. Wu and D.-X. Zhou. Svm soft margin classiﬁers: Linear programming versus quadratic programming. Neural Computation, 17(5):1160–1187, 2005.</p>
<p>[26] L. Zhang, T. Yang, R. Jin, and X. He. O(log T ) projections for stochastic optimization of smooth and strongly convex functions. In Proceedings of the 30th International Conference on Machine Learning (ICML), pages 621–629, 2013.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
