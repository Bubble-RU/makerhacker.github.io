<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>120 nips-2013-Faster Ridge Regression via the Subsampled Randomized Hadamard Transform</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-120" href="../nips2013/nips-2013-Faster_Ridge_Regression_via_the_Subsampled_Randomized_Hadamard_Transform.html">nips2013-120</a> <a title="nips-2013-120-reference" href="#">nips2013-120-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>120 nips-2013-Faster Ridge Regression via the Subsampled Randomized Hadamard Transform</h1>
<br/><p>Source: <a title="nips-2013-120-pdf" href="http://papers.nips.cc/paper/5106-faster-ridge-regression-via-the-subsampled-randomized-hadamard-transform.pdf">pdf</a></p><p>Author: Yichao Lu, Paramveer Dhillon, Dean P. Foster, Lyle Ungar</p><p>Abstract: We propose a fast algorithm for ridge regression when the number of features is much larger than the number of observations (p n). The standard way to solve ridge regression in this setting works in the dual space and gives a running time of O(n2 p). Our algorithm Subsampled Randomized Hadamard Transform- Dual Ridge Regression (SRHT-DRR) runs in time O(np log(n)) and works by preconditioning the design matrix by a Randomized Walsh-Hadamard Transform with a subsequent subsampling of features. We provide risk bounds for our SRHT-DRR algorithm in the ﬁxed design setting and show experimental results on synthetic and real datasets. 1</p><br/>
<h2>reference text</h2><p>[1] Nir Ailon and Bernard Chazelle. Approximate nearest neighbors and the fast johnsonlindenstrauss transform. In STOC, pages 557–563, 2006.</p>
<p>[2] Nir Ailon and Edo Liberty. Fast dimension reduction using rademacher series on dual bch codes. Technical report, 2007.</p>
<p>[3] Francis Bach. Sharp analysis of low-rank kernel matrix approximations. CoRR, abs/1208.2015, 2012.</p>
<p>[4] Christos Boutsidis and Alex Gittens. Improved matrix algorithms via the subsampled randomized hadamard transform. CoRR, abs/1204.0062, 2012.</p>
<p>[5] Paramveer S. Dhillon, Dean P. Foster, Sham M. Kakade, and Lyle H. Ungar. A risk comparison of ordinary least squares vs ridge regression. Journal of Machine Learning Research, 14:1505– 1511, 2013.</p>
<p>[6] Petros Drineas, Michael W. Mahoney, S. Muthukrishnan, and Tamás Sarlós. Faster least squares approximation. CoRR, abs/0710.1435, 2007.</p>
<p>[7] Isabelle Guyon. Design of experiments for the nips 2003 variable selection benchmark. 2003.</p>
<p>[8] N. Halko, P. G. Martinsson, and J. A. Tropp. Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions. SIAM Rev., 53(2):217–288, May 2011.</p>
<p>[9] Nathan Halko, Per-Gunnar Martinsson, Yoel Shkolnisky, and Mark Tygert. An algorithm for the principal component analysis of large data sets. SIAM J. Scientiﬁc Computing, 33(5):2580– 2594, 2011.</p>
<p>[10] Daniel Hsu, Sham M. Kakade, and Tong Zhang. Analysis of a randomized approximation scheme for matrix multiplication. CoRR, abs/1211.5414, 2012.</p>
<p>[11] S. Jung and J.S. Marron. PCA consistency in high dimension, low sample size context. Annals of Statistics, 37:4104–4130, 2009.</p>
<p>[12] Quoc Le, Tamas Sarlos, and Alex Smola. Fastfood -approximating kernel expansions in loglinear time. ICML, 2013.</p>
<p>[13] W.F. Massy. Principal components regression in exploratory statistical research. Journal of the American Statistical Association, 60:234–256, 1965.</p>
<p>[14] Xiangrui Meng, Michael A. Saunders, and Michael W. Mahoney. Lsrn: A parallel iterative solver for strongly over- or under-determined systems. CoRR, abs/1109.5981, 2011. 8</p>
<p>[15] Ali Rahimi and Ben Recht. Random features for large-scale kernel machines. In In Neural Infomration Processing Systems, 2007.</p>
<p>[16] Vladimir Rokhlin, Arthur Szlam, and Mark Tygert. A randomized algorithm for principal component analysis. SIAM J. Matrix Analysis Applications, 31(3):1100–1124, 2009.</p>
<p>[17] Vladimir Rokhlin and Mark Tygert. A fast randomized algorithm for overdetermined linear least-squares regression. Proceedings of the National Academy of Sciences, 105(36):13212– 13217, September 2008.</p>
<p>[18] Tamas Sarlos. Improved approximation algorithms for large matrices via random projections. In In Proc. 47th Annu. IEEE Sympos. Found. Comput. Sci, pages 143–152. IEEE Computer Society, 2006.</p>
<p>[19] G. Saunders, A. Gammerman, and V. Vovk. Ridge regression learning algorithm in dual variables. In Proc. 15th International Conf. on Machine Learning, pages 515–521. Morgan Kaufmann, San Francisco, CA, 1998.</p>
<p>[20] Joel A. Tropp. Improved analysis of the subsampled randomized hadamard transform. CoRR, abs/1011.1595, 2010.</p>
<p>[21] Joel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computational Mathematics, 12(4):389–434, 2012.</p>
<p>[22] Mark Tygert. A fast algorithm for computing minimal-norm solutions to underdetermined systems of linear equations. CoRR, abs/0905.4745, 2009.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
