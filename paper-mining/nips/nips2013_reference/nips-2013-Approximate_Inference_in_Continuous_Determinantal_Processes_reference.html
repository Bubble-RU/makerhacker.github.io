<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 nips-2013-Approximate Inference in Continuous Determinantal Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-40" href="../nips2013/nips-2013-Approximate_Inference_in_Continuous_Determinantal_Processes.html">nips2013-40</a> <a title="nips-2013-40-reference" href="#">nips2013-40-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 nips-2013-Approximate Inference in Continuous Determinantal Processes</h1>
<br/><p>Source: <a title="nips-2013-40-pdf" href="http://papers.nips.cc/paper/4916-approximate-inference-in-continuous-determinantal-processes.pdf">pdf</a></p><p>Author: Raja Hafiz Affandi, Emily Fox, Ben Taskar</p><p>Abstract: Determinantal point processes (DPPs) are random point processes well-suited for modeling repulsion. In machine learning, the focus of DPP-based models has been on diverse subset selection from a discrete and ﬁnite base set. This discrete setting admits an efﬁcient sampling algorithm based on the eigendecomposition of the deﬁning kernel matrix. Recently, there has been growing interest in using DPPs deﬁned on continuous spaces. While the discrete-DPP sampler extends formally to the continuous case, computationally, the steps required are not tractable in general. In this paper, we present two efﬁcient DPP sampling schemes that apply to a wide range of kernel functions: one based on low rank approximations via Nystr¨ m o and random Fourier feature techniques and another based on Gibbs sampling. We demonstrate the utility of continuous DPPs in repulsive mixture modeling and synthesizing human poses spanning activity spaces. 1</p><br/>
<h2>reference text</h2><p>[1] P. Abbeel and A.Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proc. ICML, 2004.</p>
<p>[2] R. H. Affandi, A. Kulesza, and E. B. Fox. Markov determinantal point processes. In Proc. UAI, 2012.</p>
<p>[3] R.H. Affandi, A. Kulesza, E.B. Fox, and B. Taskar. Nystr¨ m approximation for large-scale o determinantal processes. In Proc. AISTATS, 2013.</p>
<p>[4] R. A. Bernstein and M. Gobbel. Partitioning of space in communities of ants. Journal of Animal Ecology, 48(3):931–942, 1979.</p>
<p>[5] A. Borodin and E.M. Rains. Eynard-Mehta theorem, Schur process, and their Pfafﬁan analogs. Journal of statistical physics, 121(3):291–317, 2005.</p>
<p>[6] CMU. Carnegie Mellon University graphics lab motion capture database. http://mocap.cs.cmu.edu/, 2009.</p>
<p>[7] D.J. Daley and D. Vere-Jones. An introduction to the theory of point processes: Volume I: Elementary theory and methods. Springer, 2003.</p>
<p>[8] G.E. Fasshauer and M.J. McCourt. Stable evaluation of Gaussian radial basis function interpolants. SIAM Journal on Scientiﬁc Computing, 34(2):737–762, 2012.</p>
<p>[9] J. Gillenwater, A. Kulesza, and B. Taskar. Discovering diverse and salient threads in document collections. In Proc. EMNLP, 2012.</p>
<p>[10] J.B. Hough, M. Krishnapur, Y. Peres, and B. Vir´ g. Determinantal processes and independence. a Probability Surveys, 3:206–229, 2006.</p>
<p>[11] A. Kulesza and B. Taskar. Structured determinantal point processes. In Proc. NIPS, 2010.</p>
<p>[12] A. Kulesza and B. Taskar. k-DPPs: Fixed-size determinantal point processes. In ICML, 2011.</p>
<p>[13] A. Kulesza and B. Taskar. Determinantal point processes for machine learning. Foundations and Trends in Machine Learning, 5(2–3), 2012.</p>
<p>[14] F. Lavancier, J. Møller, and E. Rubak. Statistical aspects of determinantal point processes. arXiv preprint arXiv:1205.4818, 2012.</p>
<p>[15] O. Macchi. The coincidence approach to stochastic point processes. Advances in Applied Probability, pages 83–122, 1975.</p>
<p>[16] B. Mat´ rn. Spatial variation. Springer-Verlag, 1986. e</p>
<p>[17] T. Neeff, G. S. Biging, L. V. Dutra, C. C. Freitas, and J. R. Dos Santos. Markov point processes for modeling of spatial forest patterns in Amazonia derived from interferometric height. Remote Sensing of Environment, 97(4):484–494, 2005.</p>
<p>[18] F. Petralia, V. Rao, and D. Dunson. Repulsive mixtures. In NIPS, 2012.</p>
<p>[19] A. Rahimi and B. Recht. Random features for large-scale kernel machines. NIPS, 2007.</p>
<p>[20] S. Richardson and P. J. Green. On Bayesian analysis of mixtures with an unknown number of components (with discussion). JRSS:B, 59(4):731–792, 1997.</p>
<p>[21] C.P. Robert and G. Casella. Monte Carlo Statistical Methods. Springer, 2nd edition, 2004. ¨</p>
<p>[22] J Schur. Uber potenzreihen, die im innern des einheitskreises beschr¨ nkt sind. Journal f¨ r die a u reine und angewandte Mathematik, 147:205–232, 1917.</p>
<p>[23] M. Stephens. Dealing with label switching in mixture models. JRSS:B, 62(4):795–809, 2000.</p>
<p>[24] C.A. Sugar and G.M. James. Finding the number of clusters in a dataset: An informationtheoretic approach. JASA, 98(463):750–763, 2003.</p>
<p>[25] L. A. Waller, A. S¨ rkk¨ , V. Olsbo, M. Myllym¨ ki, I.G. Panoutsopoulou, W.R. Kennedy, and a a a G. Wendelschafer-Crabb. Second-order spatial analysis of epidermal nerve ﬁbers. Statistics in Medicine, 30(23):2827–2841, 2011.</p>
<p>[26] J. Wang. Consistent selection of the number of clusters via crossvalidation. Biometrika, 97(4): 893–904, 2010.</p>
<p>[27] C.K.I. Williams and M. Seeger. Using the Nystr¨ m method to speed up kernel machines. NIPS, o 2000.</p>
<p>[28] T. Yang, Y.-F. Li, M. Mahdavi, R. Jin, and Z.-H. Zhou. Nystr¨ m method vs random fourier o features: A theoretical and empirical comparison. NIPS, 2012.</p>
<p>[29] J. Zou and R.P. Adams. Priors for diversity in generative latent variable models. In NIPS, 2012.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
