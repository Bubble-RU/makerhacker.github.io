<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-341" href="../nips2013/nips-2013-Universal_models_for_binary_spike_patterns_using_centered_Dirichlet_processes.html">nips2013-341</a> <a title="nips-2013-341-reference" href="#">nips2013-341-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>341 nips-2013-Universal models for binary spike patterns using centered Dirichlet processes</h1>
<br/><p>Source: <a title="nips-2013-341-pdf" href="http://papers.nips.cc/paper/5050-universal-models-for-binary-spike-patterns-using-centered-dirichlet-processes.pdf">pdf</a></p><p>Author: Il M. Park, Evan W. Archer, Kenneth Latimer, Jonathan W. Pillow</p><p>Abstract: Probabilistic models for binary spike patterns provide a powerful tool for understanding the statistical dependencies in large-scale neural recordings. Maximum entropy (or “maxent”) models, which seek to explain dependencies in terms of low-order interactions between neurons, have enjoyed remarkable success in modeling such patterns, particularly for small groups of neurons. However, these models are computationally intractable for large populations, and low-order maxent models have been shown to be inadequate for some datasets. To overcome these limitations, we propose a family of “universal” models for binary spike patterns, where universality refers to the ability to model arbitrary distributions over all 2m binary patterns. We construct universal models using a Dirichlet process centered on a well-behaved parametric base measure, which naturally combines the ﬂexibility of a histogram and the parsimony of a parametric model. We derive computationally efﬁcient inference methods using Bernoulli and cascaded logistic base measures, which scale tractably to large populations. We also establish a condition for equivalence between the cascaded logistic and the 2nd-order maxent or “Ising” model, making cascaded logistic a reasonable choice for base measure in a universal model. We illustrate the performance of these models using neural data. 1</p><br/>
<h2>reference text</h2><p>[1] I. E. Ohiorhenuan, F. Mechler, K. P. Purpura, A. M. Schmid, Q. Hu, and J. D. Victor. Sparse coding and high-order correlations in ﬁne-scale cortical networks. Nature, 466(7306):617–621, July 2010.</p>
<p>[2] P. Ravikumar, M. Wainwright, and J. Lafferty. High-dimensional Ising model selection using L1regularized logistic regression. The Annals of Statistics, 38(3):1287–1319, 2010.</p>
<p>[3] E. Ganmor, R. Segev, and E. Schneidman. Sparse low-order interaction network underlies a highly correlated and learnable neural population code. Proceedings of the National Academy of Sciences, 108(23):9679–9684, 2011.</p>
<p>[4] E. Schneidman, M. J. Berry, R. Segev, and W. Bialek. Weak pairwise correlations imply strongly correlated network states in a neural population. Nature, 440(7087):1007–1012, Apr 2006.</p>
<p>[5] J. Shlens, G. Field, J. Gauthier, M. Grivich, D. Petrusca, A. Sher, L. A. M., and E. J. Chichilnisky. The structure of multi-neuron ﬁring patterns in primate retina. J Neurosci, 26:8254–8266, 2006.</p>
<p>[6] P. Smolensky. Parallel distributed processing: explorations in the microstructure of cognition, vol. 1. chapter Information processing in dynamical systems: foundations of harmony theory, pages 194–281. MIT Press, Cambridge, MA, USA, 1986.</p>
<p>[7] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006.</p>
<p>[8] G. J. McLachlan and D. Peel. Finite mixture models. Wiley, 2000.</p>
<p>[9] M. Bethge and P. Berens. Near-maximum entropy models for binary neural representations of natural images. Advances in neural information processing systems, 20:97–104, 2008.</p>
<p>[10] P. M¨ ller and F. A. Quintana. Nonparametric bayesian data analysis. Statistical science, 19(1):95–110, u 2004.</p>
<p>[11] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581, 2006.</p>
<p>[12] W. Truccolo and J. P. Donoghue. Nonparametric modeling of neural point processes via stochastic gradient boosting regression. Neural computation, 19(3):672–705, 2007.</p>
<p>[13] R. P. Adams, I. Murray, and D. J. C. MacKay. Tractable nonparametric bayesian inference in poisson processes with gaussian process intensities. In Proceedings of the 26th Annual International Conference on Machine Learning. ACM New York, NY, USA, 2009.</p>
<p>[14] A. Kottas, S. Behseta, D. E. Moorman, V. Poynor, and C. R. Olson. Bayesian nonparametric analysis of neuronal intensity rates. Journal of Neuroscience Methods, 203(1):241–253, January 2012.</p>
<p>[15] B. J. Frey. Graphical models for machine learning and digital communication. MIT Press, 1998.</p>
<p>[16] M. Pachitariu, B. Petreska, and M. Sahani. Recurrent linear models of simultaneously-recorded neural populations. Advances in Neural Information Processing (NIPS), 2013.</p>
<p>[17] E. Archer, I. M. Park, and J. W. Pillow. Bayesian entropy estimation for binary spike train data using parametric prior knowledge. In Advances in Neural Information Processing Systems (NIPS), 2013.</p>
<p>[18] E. Cuthill and J. McKee. Reducing the bandwidth of sparse symmetric matrices. In Proceedings of the 1969 24th national conference, ACM ’69, pages 157–172, New York, NY, USA, 1969. ACM.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
