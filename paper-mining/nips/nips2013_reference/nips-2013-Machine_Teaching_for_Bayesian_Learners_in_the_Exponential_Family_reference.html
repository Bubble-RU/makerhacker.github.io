<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>181 nips-2013-Machine Teaching for Bayesian Learners in the Exponential Family</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-181" href="../nips2013/nips-2013-Machine_Teaching_for_Bayesian_Learners_in_the_Exponential_Family.html">nips2013-181</a> <a title="nips-2013-181-reference" href="#">nips2013-181-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>181 nips-2013-Machine Teaching for Bayesian Learners in the Exponential Family</h1>
<br/><p>Source: <a title="nips-2013-181-pdf" href="http://papers.nips.cc/paper/5042-machine-teaching-for-bayesian-learners-in-the-exponential-family.pdf">pdf</a></p><p>Author: Xiaojin Zhu</p><p>Abstract: What if there is a teacher who knows the learning goal and wants to design good training data for a machine learner? We propose an optimal teaching framework aimed at learners who employ Bayesian models. Our framework is expressed as an optimization problem over teaching examples that balance the future loss of the learner and the effort of the teacher. This optimization problem is in general hard. In the case where the learner employs conjugate exponential family models, we present an approximate algorithm for ﬁnding the optimal teaching set. Our algorithm optimizes the aggregate sufﬁcient statistics, then unpacks them into actual teaching examples. We give several examples to illustrate our framework. 1</p><br/>
<h2>reference text</h2><p>[1] D. Angluin. Queries revisited. Theor. Comput. Sci., 313(2):175–194, 2004.</p>
<p>[2] F. J. Balbach and T. Zeugmann. Teaching randomized learners. In COLT, pages 229–243. Springer, 2006.</p>
<p>[3] Y. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In ICML, 2009.</p>
<p>[4] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks against support vector machines. In ICML, 2012.</p>
<p>[5] L. D. Brown. Fundamentals of statistical exponential families: with applications in statistical decision theory. Institute of Mathematical Statistics, Hayworth, CA, USA, 1986.</p>
<p>[6] M. Cakmak and M. Lopes. Algorithmic and human teaching of sequential decision tasks. In AAAI Conference on Artiﬁcial Intelligence, 2012.</p>
<p>[7] N. Chater and M. Oaksford. The probabilistic mind: prospects for Bayesian cognitive science. OXFORD University Press, 2008.</p>
<p>[8] M. C. Frank and N. D. Goodman. Predicting Pragmatic Reasoning in Language Games. Science, 336(6084):998, May 2012.</p>
<p>[9] G. Gigu` re and B. C. Love. Limits in decision making arise from limits in memory retrieval. e Proceedings of the National Academy of Sciences, Apr. 2013.</p>
<p>[10] S. Goldman and M. Kearns. On the complexity of teaching. Journal of Computer and Systems Sciences, 50(1):20–31, 1995.</p>
<p>[11] S. Hanneke. Teaching dimension and the complexity of active learning. In COLT, page 6681, 2007.</p>
<p>[12] T. Heged¨ s. Generalized teaching dimensions and the query complexity of learning. In COLT, u pages 108–117, 1995.</p>
<p>[13] F. Khan, X. Zhu, and B. Mutlu. How do humans teach: On curriculum learning and teaching dimension. In Advances in Neural Information Processing Systems (NIPS) 25. 2011.</p>
<p>[14] H. Kobayashi and A. Shinohara. Complexity of teaching by a restricted number of examples. In COLT, pages 293–302, 2009.</p>
<p>[15] M. P. Kumar, B. Packer, and D. Koller. Self-paced learning for latent variable models. In NIPS, 2010.</p>
<p>[16] Y. J. Lee and K. Grauman. Learning the easy things ﬁrst: Self-paced visual category discovery. In CVPR, 2011.</p>
<p>[17] B. D. McCandliss, J. A. Fiez, A. Protopapas, M. Conway, and J. L. McClelland. Success and failure in teaching the [r]-[l] contrast to Japanese adults: Tests of a Hebbian model of plasticity and stabilization in spoken language perception. Cognitive, Affective, & Behavioral Neuroscience, 2(2):89–108, 2002.</p>
<p>[18] H. Pashler and M. C. Mozer. When does fading enhance perceptual category learning? Journal of Experimental Psychology: Learning, Memory, and Cognition, 2013. In press.</p>
<p>[19] A. N. Rafferty and T. L. Grifﬁths. Optimal language learning: The importance of starting representative. 32nd Annual Conference of the Cognitive Science Society, 2010.</p>
<p>[20] P. Shafto and N. Goodman. Teaching Games: Statistical Sampling Assumptions for Learning in Pedagogical Situations. In CogSci, pages 1632–1637, 2008.</p>
<p>[21] S. Singh, R. L. Lewis, A. G. Barto, and J. Sorg. Intrinsically motivated reinforcement learning: An evolutionary perspective. IEEE Trans. on Auton. Ment. Dev., 2(2):70–82, June 2010.</p>
<p>[22] J. B. Tenenbaum and T. L. Grifﬁths. The rational basis of representativeness. 23rd Annual Conference of the Cognitive Science Society, 2001.</p>
<p>[23] J. B. Tenenbaum, T. L. Grifﬁths, and C. Kemp. Theory-based Bayesian models of inductive learning and reasoning. Trends in Cognitive Sciences, 10(7):309–318, 2006.</p>
<p>[24] F. Xu and J. B. Tenenbaum. Word learning as Bayesian inference. Psychological review, 114(2), 2007.</p>
<p>[25] S. Zilles, S. Lange, R. Holte, and M. Zinkevich. Models of cooperative teaching and learning. Journal of Machine Learning Research, 12:349–384, 2011.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
