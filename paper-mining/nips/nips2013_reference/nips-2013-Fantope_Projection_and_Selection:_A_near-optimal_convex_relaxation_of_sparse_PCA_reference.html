<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-116" href="../nips2013/nips-2013-Fantope_Projection_and_Selection%3A_A_near-optimal_convex_relaxation_of_sparse_PCA.html">nips2013-116</a> <a title="nips-2013-116-reference" href="#">nips2013-116-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>116 nips-2013-Fantope Projection and Selection: A near-optimal convex relaxation of sparse PCA</h1>
<br/><p>Source: <a title="nips-2013-116-pdf" href="http://papers.nips.cc/paper/5136-fantope-projection-and-selection-a-near-optimal-convex-relaxation-of-sparse-pca.pdf">pdf</a></p><p>Author: Vincent Q. Vu, Juhee Cho, Jing Lei, Karl Rohe</p><p>Abstract: We propose a novel convex relaxation of sparse principal subspace estimation based on the convex hull of rank-d projection matrices (the Fantope). The convex problem can be solved efﬁciently using alternating direction method of multipliers (ADMM). We establish a near-optimal convergence rate, in terms of the sparsity, ambient dimension, and sample size, for estimation of the principal subspace of a general covariance matrix without assuming the spiked covariance model. In the special case of d = 1, our result implies the near-optimality of DSPCA (d’Aspremont et al. [1]) even when the solution is not rank 1. We also provide a general theoretical framework for analyzing the statistical properties of the method for arbitrary input matrices that extends the applicability and provable guarantees to a wide array of settings. We demonstrate this with an application to Kendall’s tau correlation matrices and transelliptical component analysis. 1</p><br/>
<h2>reference text</h2><p>[1]</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]</p>
<p>[19]</p>
<p>[20]</p>
<p>[21]</p>
<p>[22]</p>
<p>[23]</p>
<p>[24]</p>
<p>[25]</p>
<p>[26]</p>
<p>[27]</p>
<p>[28]</p>
<p>[29]</p>
<p>[30]  A. d’Aspremont et al. “A direct formulation of sparse PCA using semideﬁnite programming ”. In: SIAM Review 49.3 (2007). I. M. Johnstone and A. Y. Lu. “On consistency and sparsity for principal components analysis in high dimensions ”. In: JASA 104.486 (2009), pp. 682–693. I. T. Jolliffe, N. T. Trendaﬁlov, and M. Uddin. “A modiﬁed principal component technique based on the Lasso ”. In: JCGS 12 (2003), pp. 531–547. H. Zou, T. Hastie, and R. Tibshirani. “Sparse principal component analysis ”. In: JCGS 15.2 (2006), pp. 265–286. H. Shen and J. Z. Huang. “Sparse principal component analysis via regularized low rank matrix approximation ”. In: Journal of Multivariate Analysis 99 (2008), pp. 1015–1034. D. M. Witten, R. Tibshirani, and T. Hastie. “A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis ”. In: Biostatistics 10 (2009), pp. 515– 534. M. Journee et al. “Generalized power method for sparse principal component analysis ”. In: JMLR 11 (2010), pp. 517–553. B. K. Sriperumbudur, D. A. Torres, and G. R. G. Lanckriet. “A majorization-minimization approach to the sparse generalized eigenvalue problem ”. In: Machine Learning 85.1–2 (2011), pp. 3–39. Y. Zhang and L. E. Ghaoui. “Large-scale sparse principal component analysis with application to text data ”. In: NIPS 24. Ed. by J. Shawe-Taylor et al. 2011, pp. 532–539. X. Yuan and T. Zhang. “Truncated power method for sparse eigenvalue problems ”. In: JMLR 14 (2013), pp. 899–925. A. A. Amini and M. J. Wainwright. “High-dimensional analysis of semideﬁnite relaxations for sparse principal components ”. In: Ann. Statis. 37.5B (2009), pp. 2877–2921. A. Birnbaum et al. “Minimax bounds for sparse pca with noisy high-dimensional data ”. In: Ann. Statis. 41.3 (2013), pp. 1055–1084. V. Q. Vu and J. Lei. “Minimax rates of estimation for sparse PCA in high dimensions ”. In: AISTATS 15. Ed. by N. Lawrence and M. Girolami. Vol. 22. JMLR W&CP.; 2012, pp. 1278–1286. Q. Berthet and P. Rigollet. “Computational lower bounds for sparse PCA ”. In: (2013). arXiv: 1304. 0828. L. Mackey. “Deﬂation methods for sparse PCA ”. In: NIPS 21. Ed. by D. Koller et al. 2009, pp. 1017– 1024. Z. Ma. “Sparse principal component analysis and iterative thresholding ”. In: Ann. Statis. 41.2 (2013). T. T. Cai, Z. Ma, and Y. Wu. “Sparse PCA: optimal rates and adaptive estimation ”. In: Ann. Statis. (2013). to appear. arXiv: 1211.1309. V. Q. Vu and J. Lei. “Minimax sparse principal subspace estimation in high dimensions ”. In: Ann. Statis. (2013). to appear. arXiv: 1211.0373. S. Boyd et al. “Distributed optimization and statistical learning via the alternating direction method of multipliers ”. In: Foundations and Trends in Machine Learning 3.1 (2010), pp. 1–122. S. Ma. “Alternating direction method of multipliers for sparse principal component analysis ”. In: (2011). arXiv: 1111.6703. R. Bhatia. Matrix analysis. Springer-Verlag, 1997. J. Dattorro. Convex optimization & euclidean distance geometry. Meboo Publishing USA, 2005. K. Fan. “On a theorem of Weyl concerning eigenvalues of linear transformations I ”. In: Proceedings of the National Academy of Sciences 35.11 (1949), pp. 652–655. M. Overton and R. Womersley. “On the sum of the largest eigenvalues of a symmetric matrix ”. In: SIAM Journal on Matrix Analysis and Applications 13.1 (1992), pp. 41–45. S. N. Negahban et al. “A uniﬁed framework for the high-dimensional analysis of M -estimators with decomposable regularizers ”. In: Statistical Science 27.4 (2012), pp. 538–557. H. Liu et al. “High-dimensional semiparametric gaussian copula graphical models ”. In: Ann. Statis. 40.4 (2012), pp. 2293–2326. W. H. Kruskal. “Ordinal measures of association ”. In: JASA 53.284 (1958), pp. 814–861. H. Liu, J. Lafferty, and L. Wasserman. “The nonparanormal: semiparametric estimation of high dimensional undirected graphs ”. In: JMLR 10 (2009), pp. 2295–2328. F. Han and H. Liu. “Transelliptical component analysis ”. In: NIPS 25. Ed. by P. Bartlett et al. 2012, pp. 368–376. F. Lindskog, A. McNeil, and U. Schmock. “Kendall’s tau for elliptical distributions ”. In: Credit Risk. Ed. by G. Bol et al. Contributions to Economics. Physica-Verlag HD, 2003, pp. 149–156.  9</p>
<br/>
<br/><br/><br/></body>
</html>
