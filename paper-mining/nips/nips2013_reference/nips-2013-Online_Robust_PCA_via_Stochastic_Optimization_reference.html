<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>233 nips-2013-Online Robust PCA via Stochastic Optimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-233" href="../nips2013/nips-2013-Online_Robust_PCA_via_Stochastic_Optimization.html">nips2013-233</a> <a title="nips-2013-233-reference" href="#">nips2013-233-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>233 nips-2013-Online Robust PCA via Stochastic Optimization</h1>
<br/><p>Source: <a title="nips-2013-233-pdf" href="http://papers.nips.cc/paper/5131-online-robust-pca-via-stochastic-optimization.pdf">pdf</a></p><p>Author: Jiashi Feng, Huan Xu, Shuicheng Yan</p><p>Abstract: Robust PCA methods are typically based on batch optimization and have to load all the samples into memory during optimization. This prevents them from efﬁciently processing big data. In this paper, we develop an Online Robust PCA (OR-PCA) that processes one sample per time instance and hence its memory cost is independent of the number of samples, signiﬁcantly enhancing the computation and storage efﬁciency. The proposed OR-PCA is based on stochastic optimization of an equivalent reformulation of the batch RPCA. Indeed, we show that OR-PCA provides a sequence of subspace estimations converging to the optimum of its batch counterpart and hence is provably robust to sparse corruption. Moreover, OR-PCA can naturally be applied for tracking dynamic subspace. Comprehensive simulations on subspace recovering and tracking demonstrate the robustness and efﬁciency advantages of the OR-PCA over online PCA and batch RPCA methods. 1</p><br/>
<h2>reference text</h2><p>[1] M. Artac, M. Jogan, and A. Leonardis. Incremental pca for on-line visual learning and recognition. In Pattern Recognition, 2002. Proceedings. 16th International Conference on, volume 3, pages 781–784. IEEE, 2002.</p>
<p>[2] D.P. Bertsekas. Nonlinear programming. Athena Scientiﬁc, 1999.</p>
<p>[3] Samuel Burer and Renato Monteiro. A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization. Math. Progam., 2003.</p>
<p>[4] E.J. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? ArXiv:0912.3599, 2009.</p>
<p>[5] V. Chandrasekaran, S. Sanghavi, P.A. Parrilo, and A.S. Willsky. Rank-sparsity incoherence for matrix decomposition. SIAM Journal on Optimization, 21(2):572–596, 2011.</p>
<p>[6] M. Fazel. Matrix rank minimization with applications. PhD thesis, PhD thesis, Stanford University, 2002.</p>
<p>[7] J. Feng, H. Xu, and S. Yan. Robust PCA in high-dimension: A deterministic approach. In ICML, 2012.</p>
<p>[8] D.L. Fisk. Quasi-martingales. Transactions of the American Mathematical Society, 1965.</p>
<p>[9] N. Guan, D. Tao, Z. Luo, and B. Yuan. Online nonnegative matrix factorization with robust stochastic approximation. Neural Networks and Learning Systems, IEEE Transactions on, 23(7):1087–1099, 2012.</p>
<p>[10] Jun He, Laura Balzano, and John Lui. Online robust subspace tracking from partial information. arXiv preprint arXiv:1109.3827, 2011.</p>
<p>[11] P.J. Huber, E. Ronchetti, and MyiLibrary. Robust statistics. John Wiley & Sons, New York, 1981.</p>
<p>[12] M. Hubert, P.J. Rousseeuw, and K.V. Branden. Robpca: a new approach to robust principal component analysis. Technometrics, 2005.</p>
<p>[13] Y. Li. On incremental and robust subspace learning. Pattern recognition, 2004.</p>
<p>[14] Z. Lin, M. Chen, and Y. Ma. The augmented lagrange multiplier method for exact recovery of corrupted low-rank matrices. arXiv preprint arXiv:1009.5055, 2010.</p>
<p>[15] Z. Lin, A. Ganesh, J. Wright, L. Wu, M. Chen, and Y. Ma. Fast convex optimization algorithms for exact recovery of a corrupted low-rank matrix. Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), 2009.</p>
<p>[16] J. Mairal, F. Bach, J. Ponce, and G. Sapiro. Online learning for matrix factorization and sparse coding. JMLR, 2010.</p>
<p>[17] Morteza Mardani, Gonzalo Mateos, and G Giannakis. Dynamic anomalography: Tracking network anomalies via sparsity and low rank. 2012.</p>
<p>[18] Morteza Mardani, Gonzalo Mateos, and Georgios B Giannakis. Rank minimization for subspace tracking from incomplete data. In ICASSP, 2013.</p>
<p>[19] K. Pearson. On lines and planes of closest ﬁt to systems of points in space. Philosophical Magazine, 1901.</p>
<p>[20] C. Qiu, N. Vaswani, and L. Hogben. Recursive robust pca or recursive sparse recovery in large but structured noise. arXiv preprint arXiv:1211.3754, 2012.</p>
<p>[21] B. Recht, M. Fazel, and P.A. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM review, 52(3):471–501, 2010.</p>
<p>[22] Jasson Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In ICML, 2005.</p>
<p>[23] Pablo Sprechmann, Alex M Bronstein, and Guillermo Sapiro. Learning efﬁcient sparse and low rank models. arXiv preprint arXiv:1212.3631, 2012.</p>
<p>[24] H. Xu, C. Caramanis, and S. Mannor. Principal component analysis with contaminated data: The high dimensional case. In COLT, 2010.</p>
<p>[25] H. Xu, C. Caramanis, and S. Sanghavi. Robust pca via outlier pursuit. Information Theory, IEEE Transactions on, 58(5):3047–3064, 2012.  9</p>
<br/>
<br/><br/><br/></body>
</html>
