<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-163" href="../nips2013/nips-2013-Learning_a_Deep_Compact_Image_Representation_for_Visual_Tracking.html">nips2013-163</a> <a title="nips-2013-163-reference" href="#">nips2013-163-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>163 nips-2013-Learning a Deep Compact Image Representation for Visual Tracking</h1>
<br/><p>Source: <a title="nips-2013-163-pdf" href="http://papers.nips.cc/paper/5192-learning-a-deep-compact-image-representation-for-visual-tracking.pdf">pdf</a></p><p>Author: Naiyan Wang, Dit-Yan Yeung</p><p>Abstract: In this paper, we study the challenging problem of tracking the trajectory of a moving object in a video with possibly very complex background. In contrast to most existing trackers which only learn the appearance of the tracked object online, we take a different approach, inspired by recent advances in deep learning architectures, by putting more emphasis on the (unsupervised) feature learning problem. Speciﬁcally, by using auxiliary natural images, we train a stacked denoising autoencoder ofﬂine to learn generic image features that are more robust against variations. This is then followed by knowledge transfer from ofﬂine training to the online tracking process. Online tracking involves a classiﬁcation neural network which is constructed from the encoder part of the trained autoencoder as a feature extractor and an additional classiﬁcation layer. Both the feature extractor and the classiﬁer can be further tuned to adapt to appearance changes of the moving object. Comparison with the state-of-the-art trackers on some challenging benchmark video sequences shows that our deep learning tracker is more accurate while maintaining low computational cost with real-time performance when our MATLAB implementation of the tracker is used with a modest graphics processing unit (GPU). 1</p><br/>
<h2>reference text</h2><p>[1] A. Adam, E. Rivlin, and I. Shimshoni. Robust fragments-based tracking using the integral histogram. In CVPR, pages 798–805, 2006.</p>
<p>[2] M. Arulampalam, S. Maskell, N. Gordon, and T. Clapp. A tutorial on particle ﬁlters for online nonlinear/non-Gaussian Bayesian tracking. IEEE Transactions on Signal Processing, 50(2):174–188, 2002.</p>
<p>[3] B. Babenko, M. Yang, and S. Belongie. Robust object tracking with online multiple instance learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 33(8):1619–1632, 2011.</p>
<p>[4] C. Bao, Y. Wu, H. Ling, and H. Ji. Real time robust L1 tracker using accelerated proximal gradient approach. In CVPR, pages 1830–1837, 2012.</p>
<p>[5] A. Doucet, D. N. Freitas, and N. Gordon. Sequential Monte Carlo Methods In Practice. Springer, New York, 2001.</p>
<p>[6] H. Grabner, M. Grabner, and H. Bischof. Real-time tracking via on-line boosting. In BMVC, pages 47–56, 2006.</p>
<p>[7] H. Grabner, C. Leistner, and H. Bischof. Semi-supervised on-line boosting for robust tracking. In ECCV, pages 234–247, 2008.</p>
<p>[8] S. Hare, A. Saffari, and P. H. Torr. Struck: Structured output tracking with kernels. In ICCV, pages 263–270, 2011.</p>
<p>[9] G. Hinton. A practical guide to training restricted Boltzmann machines. In Neural Networks: Tricks of the Trade, pages 599–619. 2012.</p>
<p>[10] G. Hinton, L. Deng, D. Yu, G. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine, 29(6):82–97, 2012.</p>
<p>[11] X. Jia, H. Lu, and M. Yang. Visual tracking via adaptive structural local sparse appearance model. In CVPR, pages 1822–1829, 2012.</p>
<p>[12] Z. Kalal, J. Matas, and K. Mikolajczyk. P-N learning: Bootstrapping binary classiﬁers by structural constraints. In CVPR, pages 49–56, 2010.</p>
<p>[13] Z. Kalal, K. Mikolajczyk, and J. Matas. Tracking-learning-detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(7):1409–1422, 2012.</p>
<p>[14] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classiﬁcation with deep convolutional neural networks. In NIPS, pages 1106–1114, 2012.</p>
<p>[15] J. Kwon and K. Lee. Visual tracking decomposition. In CVPR, pages 1269–1276, 2010.</p>
<p>[16] X. Mei and H. Ling. Robust visual tracking using l1 minimization. In ICCV, pages 1436–1443, 2009.</p>
<p>[17] B. Olshausen and D. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37(23):3311–3326, 1997.</p>
<p>[18] D. Ross, J. Lim, R. Lin, and M. Yang. Incremental learning for robust visual tracking. International Journal of Computer Vision, 77(1):125–141, 2008.</p>
<p>[19] A. Torralba, R. Fergus, and W. Freeman. 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 30(11):1958– 1970, 2008.</p>
<p>[20] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11:3371–3408, 2010.</p>
<p>[21] D. Wang, H. Lu, and M. Yang. Online object tracking with sparse prototypes. IEEE Transactions on Image Processing, 22(1), 2013.</p>
<p>[22] Q. Wang, F. Chen, J. Yang, W. Xu, and M. Yang. Transferring visual prior for online object tracking. IEEE Transactions on Image Processing, 21(7):3296–3305, 2012.</p>
<p>[23] Y. Wu, J. Lim, and M. Yang. Online object tracking: A benchmark. In CVPR, 2013.</p>
<p>[24] K. Zhang, L. Zhang, and M.-H. Yang. Real-time compressive tracking. In ECCV, pages 864–877, 2012.</p>
<p>[25] T. Zhang, B. Ghanem, S. Liu, and N. Ahuja. Low-rank sparse learning for robust visual tracking. ECCV, pages 470–484, 2012.</p>
<p>[26] T. Zhang, B. Ghanem, S. Liu, and N. Ahuja. Robust visual tracking via multi-task sparse learning. In CVPR, pages 2042–2049, 2012.  9</p>
<br/>
<br/><br/><br/></body>
</html>
