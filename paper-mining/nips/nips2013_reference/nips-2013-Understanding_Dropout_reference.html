<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>339 nips-2013-Understanding Dropout</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-339" href="../nips2013/nips-2013-Understanding_Dropout.html">nips2013-339</a> <a title="nips-2013-339-reference" href="#">nips2013-339-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>339 nips-2013-Understanding Dropout</h1>
<br/><p>Source: <a title="nips-2013-339-pdf" href="http://papers.nips.cc/paper/4878-understanding-dropout.pdf">pdf</a></p><p>Author: Pierre Baldi, Peter J. Sadowski</p><p>Abstract: Dropout is a relatively new algorithm for training neural networks which relies on stochastically “dropping out” neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. Among other results, we also show how dropout performs stochastic gradient descent on a regularized error function. 1</p><br/>
<h2>reference text</h2><p>[1] P. Baldi and P. Sadowski. The Dropout Learning Algorithm. Artiﬁcial Intelligence, 2014. In press.</p>
<p>[2] E. F. Beckenbach and R. Bellman. Inequalities. Springer-Verlag Berlin, 1965.</p>
<p>[3] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-Farley, and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy), Austin, TX, June 2010. Oral Presentation.</p>
<p>[4] L. Bottou. Online algorithms and stochastic approximations. In D. Saad, editor, Online Learning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998.</p>
<p>[5] L. Bottou. Stochastic learning. In O. Bousquet and U. von Luxburg, editors, Advanced Lectures on Machine Learning, Lecture Notes in Artiﬁcial Intelligence, LNAI 3176, pages 146–168. Springer Verlag, Berlin, 2004.</p>
<p>[6] D. Cartwright and M. Field. A reﬁnement of the arithmetic mean-geometric mean inequality. Proceedings of the American Mathematical Society, pages 36–38, 1978.</p>
<p>[7] G. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. http://arxiv.org/abs/1207.0580, 2012.</p>
<p>[8] E. Neuman and J. S´ ndor. On the Ky Fan inequality and related inequalities i. MATHEMATIa CAL INEQUALITIES AND APPLICATIONS, 5:49–56, 2002.</p>
<p>[9] E. Neuman and J. Sandor. On the Ky Fan inequality and related inequalities ii. Bulletin of the Australian Mathematical Society, 72(1):87–108, 2005.</p>
<p>[10] S. Nitish. Improving Neural Networks with Dropout. PhD thesis, University of Toronto, Toronto, Canada, 2013.</p>
<p>[11] H. Robbins and D. Siegmund. A convergence theorem for non negative almost supermartingales and some applications. Optimizing methods in statistics, pages 233–257, 1971.</p>
<p>[12] D. Warde-Farley, I. Goodfellow, P. Lamblin, G. Desjardins, F. Bastien, and Y. Bengio. pylearn2. 2011. http://deeplearning.net/software/pylearn2.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
