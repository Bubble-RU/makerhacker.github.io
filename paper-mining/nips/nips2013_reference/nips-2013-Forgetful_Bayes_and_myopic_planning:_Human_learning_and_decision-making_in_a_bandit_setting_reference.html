<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-124" href="../nips2013/nips-2013-Forgetful_Bayes_and_myopic_planning%3A_Human_learning_and_decision-making_in_a_bandit_setting.html">nips2013-124</a> <a title="nips-2013-124-reference" href="#">nips2013-124-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>124 nips-2013-Forgetful Bayes and myopic planning: Human learning and decision-making in a bandit setting</h1>
<br/><p>Source: <a title="nips-2013-124-pdf" href="http://papers.nips.cc/paper/5180-forgetful-bayes-and-myopic-planning-human-learning-and-decision-making-in-a-bandit-setting.pdf">pdf</a></p><p>Author: Shunan Zhang, Angela J. Yu</p><p>Abstract: How humans achieve long-term goals in an uncertain environment, via repeated trials and noisy observations, is an important problem in cognitive science. We investigate this behavior in the context of a multi-armed bandit task. We compare human behavior to a variety of models that vary in their representational and computational complexity. Our result shows that subjects’ choices, on a trial-totrial basis, are best captured by a “forgetful” Bayesian iterative learning model [21] in combination with a partially myopic decision policy known as Knowledge Gradient [7]. This model accounts for subjects’ trial-by-trial choice better than a number of other previously proposed models, including optimal Bayesian learning and risk minimization, ε-greedy and win-stay-lose-shift. It has the added beneﬁt of being closest in performance to the optimal Bayesian model than all the other heuristic models that have the same computational complexity (all are signiﬁcantly less complex than the optimal model). These results constitute an advancement in the theoretical understanding of how humans negotiate the tension between exploration and exploitation in a noisy, imperfectly known environment. 1</p><br/>
<h2>reference text</h2><p>[1] J. Banks, M. Olson, and D. Porter. An experimental analysis of the bandit problem. Economic Theory, 10:55–77, 2013.</p>
<p>[2] R. Bellman. On the theory of dynamic programming. Proceedings of the National Academy of Sciences, 1952.</p>
<p>[3] R. Cho, L. Nystrom, E. Brown, A. Jones, T. Braver, P. Holmes, and J. D. Cohen. Mechanisms underlying dependencies of performance on stimulus history in a two-alternative forced-choice task. Cognitive, Affective and Behavioral Neuroscience, 2:283–299, 2002.</p>
<p>[4] J. D. Cohen, S. M. McClure, and A. J. Yu. Should I stay or should I go? Exploration versus exploitation. Philosophical Transactions of the Royal Society B: Biological Sciences, 362:933– 942, 2007.</p>
<p>[5] N. D. Daw, J. P. O’Doherty, P. Dayan, B. Seymour, and R. J. Dolan. Cortical substrates for exploratory decisions in humans. Nature, 441:876–879, 2006.</p>
<p>[6] A. Ejova, D. J. Navarro, and A. F. Perfors. When to walk away: The effect of variability on keeping options viable. In N. Taatgen, H. van Rijn, L. Schomaker, and J. Nerbonne, editors, Proceedings of the 31st Annual Conference of the Cognitive Science Society, Austin, TX, 2009.</p>
<p>[7] P. Frazier, W. Powell, and S. Dayanik. A knowledge-gradient policy for sequential information collection. SIAM Journal on Control and Optimization, 47:2410–2439, 2008.</p>
<p>[8] W. R. Garner. An informational analysis of absolute judgments of loudness. Journal of Experimental Psychology, 46:373–380, 1953.</p>
<p>[9] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin. Bayesian data analysis. Chapman & Hall/CRC, Boca Raton, FL, 2 edition, 2004.</p>
<p>[10] J. C. Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical Society, 41:148–177, 1979.</p>
<p>[11] L. P. Kaebling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. Journal of Artiﬁcial Intelligence Research, 4:237–285, 1996.</p>
<p>[12] M. D. Lee, S. Zhang, M. Munro, and M. Steyvers. Psychological models of human and optimal performance in bandit problems. Cognitive Systems Research, 12:164–174, 2011.</p>
<p>[13] M. I. Posner and Y. Cohen. Components of visual orienting. Attention and Performance Vol. X, 1984.</p>
<p>[14] W. Powell and I. Ryzhov. Optimal Learning. Wiley, 1 edition, 2012.</p>
<p>[15] H. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58:527–535, 1952.</p>
<p>[16] I. Ryzhov, W. Powell, and P. Frazier. The knowledge gradient algorithm for a general class of online learning problems. Operations Research, 60:180–195, 2012.</p>
<p>[17] J. Shin and D. Ariely. Keeping doors open: The effect of unavailability on incentives to keep options viable. MANAGEMENT SCIENCE, 50:575–586, 2004.</p>
<p>[18] M. Steyvers, M. D. Lee, and E.-J. Wagenmakers. A bayesian analysis of human decisionmaking on bandit problems. Journal of Mathematical Psychology, 53:168–179, 2009.</p>
<p>[19] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.</p>
<p>[20] M. C. Treisman and T. C. Williams. A theory of criterion setting with an application to sequential dependencies. Psychological Review, 91:68–111, 1984.</p>
<p>[21] A. J. Yu and J. D. Cohen. Sequential effects: Superstition or rational behavior? In Advances in Neural Information Processing Systems, volume 21, pages 1873–1880, Cambridge, MA., 2009. MIT Press.</p>
<p>[22] S. Zhang and A. J. Yu. Cheap but clever: Human active learning in a bandit setting. In Proceedings of the Cognitive Science Society Conference, 2013.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
