<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>90 nips-2013-Direct 0-1 Loss Minimization and Margin Maximization with Boosting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-90" href="../nips2013/nips-2013-Direct_0-1_Loss_Minimization_and_Margin_Maximization_with_Boosting.html">nips2013-90</a> <a title="nips-2013-90-reference" href="#">nips2013-90-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>90 nips-2013-Direct 0-1 Loss Minimization and Margin Maximization with Boosting</h1>
<br/><p>Source: <a title="nips-2013-90-pdf" href="http://papers.nips.cc/paper/5214-direct-0-1-loss-minimization-and-margin-maximization-with-boosting.pdf">pdf</a></p><p>Author: Shaodan Zhai, Tian Xia, Ming Tan, Shaojun Wang</p><p>Abstract: We propose a boosting method, DirectBoost, a greedy coordinate descent algorithm that builds an ensemble classiﬁer of weak classiﬁers through directly minimizing empirical classiﬁcation error over labeled training examples; once the training classiﬁcation error is reduced to a local coordinatewise minimum, DirectBoost runs a greedy coordinate ascent algorithm that continuously adds weak classiﬁers to maximize any targeted arbitrarily deﬁned margins until reaching a local coordinatewise maximum of the margins in a certain sense. Experimental results on a collection of machine-learning benchmark datasets show that DirectBoost gives better results than AdaBoost, LogitBoost, LPBoost with column generation and BrownBoost, and is noise tolerant when it maximizes an n′ th order bottom sample margin. 1</p><br/>
<h2>reference text</h2><p>[1] P. Bartlett and M. Traskin. AdaBoost is consistent. Journal of Machine Learning Research, 8:2347–2368, 2007.</p>
<p>[2] M. Bazaraa, H. Sherali and C. Shetty. Nonlinear Programming: Theory and Algorithms, 3rd Edition. Wiley-Interscience, 2006.</p>
<p>[3] D. P. Bertsekas. A distributed algorithm for the assignment problem. Technical Report, MIT, 1979.</p>
<p>[4] D. Bertsekas. Network Optimization: Continuous and Discrete Models. Athena Scientiﬁc, 1998.</p>
<p>[5] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.</p>
<p>[6] A. Demiriz, K. Bennett and J. Shawe-Taylor. Linear programming boosting via column generation, Machine Learning, 46:225–254, 2002.</p>
<p>[7] L. Devroye, L. Gy¨ rﬁ and G. Lugosi. A Probabilistic Theory of Pattern Recognition Springer, New York, o 1996.</p>
<p>[8] A. Frank and A. Asuncion. UCI Machine Learning Repository. School of Information and Computer Science, University of California at Irvine, 2006.</p>
<p>[9] Y. Freund and R. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.</p>
<p>[10] Y. Freund. An adaptive version of the boost by majority algorithm. Machine Learning, 43(3):293–318, 2001.</p>
<p>[11] J. Friedman, T. Hastie and R. Tibshirani. Additive logistic regression: A statistical view of boosting. The Annals of Statistics, 28(2):337–374, 2000.</p>
<p>[12] K. Glocer. Entropy regularization and soft margin maximization. Ph.D. Dissertation, UCSC, 2009.</p>
<p>[13] K. Hoffgen, H. Simon and K. van Horn. Robust trainability of single neurons. Journal of Computer and System Sciences, 50(1):114–125, 1995.</p>
<p>[14] P. Long and R. Servedio. Random classiﬁcation noise defeats all convex potential boosters. Machine Learning, 78:287-304, 2010.</p>
<p>[15] E. Mammen and A. Tsybakov. Smooth discrimination analysis. The Annals of Statistics, 27, 1808-1829, 1999.</p>
<p>[16] D. McAllester, T. Hazan and J. Keshet. Direct loss minimization for structured prediction. Neural Information Processing Systems (NIPS), 1594-1602, 2010.</p>
<p>[17] R. Schapire, Y. Freund, P. Bartlett and W. Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. The Annals of Statistics, 26(5):1651–1686, 1998.</p>
<p>[18] R. Schapire and Y. Freund. Boosting: Foundations and Algorithms. MIT Press, 2012.</p>
<p>[19] S. Shalev-Shwartz and Y. Singer. On the equivalence of weak learnability and linear separability: new relaxations and efﬁcient boosting algorithms. Machine Learning, 80(2-3): 141-163, 2010.</p>
<p>[20] I. Steinwart. Consistency of support vector machines and other regularized kernel classiﬁers. IEEE Transactions on Information Theory, 51(1):128-142, 2005.</p>
<p>[21] P. Tseng and D. Bertsekas. Relaxation methods for strictly convex costs and linear constraints. Mathematics of Operations Research, 16:462-481, 1991.</p>
<p>[22] P. Tseng. Convergence of block coordinate descent method for nondifferentiable minimization. Journal of Optimization Theory and Applications, 109(3):475–494, 2001.</p>
<p>[23] A. Tsybakov. Optimal aggregation of classiﬁers in statistical learning. The Annals of Statistics, 32(1):135166, 2004.</p>
<p>[24] V. Vapnik. Statistical Learning Theory. John Wiley, 1998.</p>
<p>[25] M. Warmuth, K. Glocer and G. Ratsch. Boosting algorithms for maximizing the soft margin. Advances in Neural Information Processing Systems (NIPS), 21, 1585-1592, 2007.</p>
<p>[26] M. Warmuth, K. Glocer and S. Vishwanathan. Entropy regularized LPBoost. The 19th International conference on Algorithmic Learning Theory (ALT), 256-271, 2008.</p>
<p>[27] S. Zhai, T. Xia, M. Tan and S. Wang. Direct 0-1 loss minimization and margin maximization with boosting. Technical Report, 2013.</p>
<p>[28] T. Zhang. Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. The Annals of Statistics, 32(1):56–85, 2004.</p>
<p>[29] T. Zhang and B. Yu. Boosting with early stopping: Convergence and consistency. The Annals of Statistics, 33:1538–1579, 2005.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
