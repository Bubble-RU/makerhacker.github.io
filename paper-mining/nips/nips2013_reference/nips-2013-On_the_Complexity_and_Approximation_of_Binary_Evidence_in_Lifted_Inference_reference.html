<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>220 nips-2013-On the Complexity and Approximation of Binary Evidence in Lifted Inference</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-220" href="../nips2013/nips-2013-On_the_Complexity_and_Approximation_of_Binary_Evidence_in_Lifted_Inference.html">nips2013-220</a> <a title="nips-2013-220-reference" href="#">nips2013-220-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>220 nips-2013-On the Complexity and Approximation of Binary Evidence in Lifted Inference</h1>
<br/><p>Source: <a title="nips-2013-220-pdf" href="http://papers.nips.cc/paper/4861-on-the-complexity-and-approximation-of-binary-evidence-in-lifted-inference.pdf">pdf</a></p><p>Author: Guy van den Broeck, Adnan Darwiche</p><p>Abstract: Lifted inference algorithms exploit symmetries in probabilistic models to speed up inference. They show impressive performance when calculating unconditional probabilities in relational models, but often resort to non-lifted inference when computing conditional probabilities. The reason is that conditioning on evidence breaks many of the model’s symmetries, which can preempt standard lifting techniques. Recent theoretical results show, for example, that conditioning on evidence which corresponds to binary relations is #P-hard, suggesting that no lifting is to be expected in the worst case. In this paper, we balance this negative result by identifying the Boolean rank of the evidence as a key parameter for characterizing the complexity of conditioning in lifted inference. In particular, we show that conditioning on binary evidence with bounded Boolean rank is efﬁcient. This opens up the possibility of approximating evidence by a low-rank Boolean matrix factorization, which we investigate both theoretically and empirically. 1</p><br/>
<h2>reference text</h2><p>[1] L. Getoor and B. Taskar, editors. An Introduction to Statistical Relational Learning. MIT Press, 2007.</p>
<p>[2] Luc De Raedt, Paolo Frasconi, Kristian Kersting, and Stephen Muggleton, editors. Probabilistic inductive logic programming: theory and applications. Springer-Verlag, 2008.</p>
<p>[3] David Poole. First-order probabilistic inference. In Proceedings of IJCAI, pages 985–991, 2003.</p>
<p>[4] Manfred Jaeger and Guy Van den Broeck. Liftability of probabilistic inference: Upper and lower bounds. In Proceedings of the 2nd International Workshop on Statistical Relational AI,, 2012.</p>
<p>[5] Guy Van den Broeck. On the completeness of ﬁrst-order knowledge compilation for lifted probabilistic inference. In Advances in Neural Information Processing Systems 24 (NIPS), pages 1386–1394, 2011.</p>
<p>[6] Rodrigo de Salvo Braz, Eyal Amir, and Dan Roth. Lifted ﬁrst-order probabilistic inference. In Proceedings of the International Joint Conference on Artiﬁcial Intelligence (IJCAI), pages 1319–1325, 2005.</p>
<p>[7] B. Milch, L.S. Zettlemoyer, K. Kersting, M. Haimes, and L.P. Kaelbling. Lifted probabilistic inference with counting formulas. Proceedings of the 23rd AAAI Conference on Artiﬁcial Intelligence, 2008.</p>
<p>[8] Guy Van den Broeck, Nima Taghipour, Wannes Meert, Jesse Davis, and Luc De Raedt. Lifted probabilistic inference by ﬁrst-order knowledge compilation. In Proceedings of IJCAI, pages 2178–2185, 2011.</p>
<p>[9] N. Taghipour, D. Fierens, J. Davis, and H. Blockeel. Lifted variable elimination with arbitrary constraints. In Proceedings of the 15th International Conference on Artiﬁcial Intelligence and Statistics, 2012.</p>
<p>[10] H.H. Bui, T.N. Huynh, and R. de Salvo Braz. Exact lifted inference with distinct soft evidence on every object. In Proceedings of the 26th AAAI Conference on Artiﬁcial Intelligence, 2012.</p>
<p>[11] Guy Van den Broeck and Jesse Davis. Conditioning in ﬁrst-order knowledge compilation and lifted probabilistic inference. In Proceedings of the 26th AAAI Conference on Artiﬁcial Intelligence,, 2012.</p>
<p>[12] Vibhav Gogate and Pedro Domingos. Probabilistic theorem proving. In Proceedings of the 27th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 256–265, 2011.</p>
<p>[13] A. Jha, V. Gogate, A. Meliou, and D. Suciu. Lifted inference seen from the other side: The tractable features. In Proceedings of the 24th Conference on Neural Information Processing Systems (NIPS), 2010.</p>
<p>[14] Guy Van den Broeck, Wannes Meert, and Jesse Davis. Lifted generative parameter learning. In Statistical Relational AI (StaRAI) workshop, July 2013.</p>
<p>[15] K. Kersting, B. Ahmadi, and S. Natarajan. Counting belief propagation. In Proceedings of the 25th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), pages 277–284, 2009.</p>
<p>[16] M. Richardson and P. Domingos. Markov logic networks. Machine learning, 62(1):107–136, 2006.</p>
<p>[17] D. Seung and L. Lee. Algorithms for non-negative matrix factorization. Advances in neural information processing systems, 13:556–562, 2001.</p>
<p>[18] M. Berry, M. Browne, A. Langville, V. Pauca, and R. Plemmons. Algorithms and applications for approximate nonnegative matrix factorization. In Computational Statistics and Data Analysis, 2006.</p>
<p>[19] Pauli Miettinen, Taneli Mielik¨ inen, Aristides Gionis, Gautam Das, and Heikki Mannila. The discrete a basis problem. In Knowledge Discovery in Databases, pages 335–346. Springer, 2006.</p>
<p>[20] Pauli Miettinen, Taneli Mielikainen, Aristides Gionis, Gautam Das, and Heikki Mannila. The discrete basis problem. IEEE Transactions on Knowledge and Data Engineering, 20(10):1348–1362, 2008.</p>
<p>[21] Pauli Miettinen. Sparse Boolean matrix factorizations. In IEEE 10th International Conference on Data Mining (ICDM), pages 935–940. IEEE, 2010.</p>
<p>[22] Boris Mirkin. Mathematical classiﬁcation and clustering, volume 11. Kluwer Academic Pub, 1996.</p>
<p>[23] Floris Geerts, Bart Goethals, and Taneli Mielik¨ inen. Tiling databases. In Discovery science, 2004. a</p>
<p>[24] Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps. Social networks, 5(2):109–137, 1983.</p>
<p>[25] Pauli Miettinen. Matrix decomposition methods for data mining: Computational complexity and algorithms. PhD thesis, 2009.</p>
<p>[26] Guy Van den Broeck. Lifted Inference and Learning in Statistical Relational Models. PhD thesis, KU Leuven, January 2013.</p>
<p>[27] Hans L Bodlaender. Treewidth: Algorithmic techniques and results. Springer, 1997.</p>
<p>[28] M. Craven and S. Slattery. Relational learning with statistical predicate invention: Better models for hypertext. Machine Learning Journal, 43(1/2):97–119, 2001.</p>
<p>[29] Mathias Niepert. Markov chains on orbits of permutation groups. In Proceedings of the 28th Conference on Uncertainty in Artiﬁcial Intelligence (UAI), 2012.</p>
<p>[30] Mathias Niepert. Symmetry-aware marginal density estimation. In Proceedings of the 27th Conference on Artiﬁcial Intelligence (AAAI), 2013.</p>
<p>[31] Jesse Davis and Pedro Domingos. Deep transfer via second-order markov logic. In Proceedings of the 26th annual international conference on machine learning, pages 217–224, 2009.</p>
<p>[32] R. de Salvo Braz, S. Natarajan, H. Bui, J. Shavlik, and S. Russell. Anytime lifted belief propagation. Proceedings of the 6th International Workshop on Statistical Relational Learning, 2009.</p>
<p>[33] K. Kersting, Y. El Massaoudi, B. Ahmadi, and F. Hadiji. Informed lifting for message-passing. In Proceedings of the 24th AAAI Conference on Artiﬁcial Intelligence,, 2010.  9</p>
<br/>
<br/><br/><br/></body>
</html>
