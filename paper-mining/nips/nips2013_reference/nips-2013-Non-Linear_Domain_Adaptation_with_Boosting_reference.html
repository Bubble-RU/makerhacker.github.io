<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>211 nips-2013-Non-Linear Domain Adaptation with Boosting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-211" href="../nips2013/nips-2013-Non-Linear_Domain_Adaptation_with_Boosting.html">nips2013-211</a> <a title="nips-2013-211-reference" href="#">nips2013-211-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>211 nips-2013-Non-Linear Domain Adaptation with Boosting</h1>
<br/><p>Source: <a title="nips-2013-211-pdf" href="http://papers.nips.cc/paper/5200-non-linear-domain-adaptation-with-boosting.pdf">pdf</a></p><p>Author: Carlos J. Becker, Christos M. Christoudias, Pascal Fua</p><p>Abstract: A common assumption in machine vision is that the training and test samples are drawn from the same distribution. However, there are many problems when this assumption is grossly violated, as in bio-medical applications where different acquisitions can generate drastic variations in the appearance of the data due to changing experimental conditions. This problem is accentuated with 3D data, for which annotation is very time-consuming, limiting the amount of data that can be labeled in new acquisitions for training. In this paper we present a multitask learning algorithm for domain adaptation based on boosting. Unlike previous approaches that learn task-speciﬁc decision boundaries, our method learns a single decision boundary in a shared feature space, common to all tasks. We use the boosting-trick to learn a non-linear mapping of the observations in each task, with no need for speciﬁc a-priori knowledge of its global analytical form. This yields a more parameter-free domain adaptation approach that successfully leverages learning on new tasks where labeled data is scarce. We evaluate our approach on two challenging bio-medical datasets and achieve a signiﬁcant improvement over the state of the art. 1</p><br/>
<h2>reference text</h2><p>[1] Jiang, J.: A literature survey on domain adaptation of statistical classiﬁers. (2008)</p>
<p>[2] Caruana, R.: Multitask Learning. Machine Learning 28 (1997)</p>
<p>[3] Evgeniou, T., Micchelli, C., Pontil, M.: Learning Multiple Tasks with Kernel Methods. JMLR 6 (2005)</p>
<p>[4] Bach, F.R., Jordan, M.I.: Kernel Independent Component Analysis. JMLR 3 (2002) 1–48</p>
<p>[5] Ek, C.H., Torr, P.H., Lawrence, N.D.: Ambiguity Modelling in Latent Spaces. In: MLMI. (2008)</p>
<p>[6] Salzmann, M., Ek, C.H., Urtasun, R., Darrell, T.: Factorized Orthogonal Latent Spaces. In: AISTATS. (2010)</p>
<p>[7] Memisevic, R., Sigal, L., Fleet, D.J.: Shared Kernel Information Embedding for Discriminative Inference. PAMI (April 2012) 778–790</p>
<p>[8] Hastie, T., Tibshirani, R., Friedman, J.: The Elements of Statistical Learning. Springer (2001)</p>
<p>[9] Zheng, Z., Zha, H., Zhang, T., Chapelle, O., Sun, G.: A General Boosting Method and Its Application to Learning Ranking Functions for Web Search. In: NIPS. (2007)</p>
<p>[10] Chapelle, O., Shivaswamy, P., Vadrevu, S., Weinberger, K., Zhang, Y., Tseng, B.: Boosted Multi-Task Learning. Machine Learning (2010)</p>
<p>[11] Turetken, E., Benmansour, F., Fua, P.: Automated Reconstruction of Tree Structures Using Path Classiﬁers and Mixed Integer Programming. In: CVPR. (June 2012)</p>
<p>[12] Baxter, J.: A Model of Inductive Bias Learning. Journal of Artiﬁcial Intelligence Research (2000)</p>
<p>[13] Ando, R.K., Zhang, T.: A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data. JMLR 6 (2005) 1817–1853</p>
<p>[14] Daum´ , H.: Bayesian Multitask Learning with Latent Hierarchies. In: UAI. (2009) e</p>
<p>[15] Kumar, A., Daum´ , H.: Learning Task Grouping and Overlap in Multi-task Learning. In: e ICML. (2012)</p>
<p>[16] Xue, Y., Liao, X., Carin, L., Krishnapuram, B.: Multi-task Learning for Classiﬁcation with Dirichlet Process Priors. JMLR 8 (2007)</p>
<p>[17] Jacob, L., Bach, F., Vert, J.P.: Clustered Multi-task Learning: a Convex Formulation. In: NIPS. (2008)</p>
<p>[18] Saenko, K., Kulis, B., Fritz, M., Darrell, T.: Adapting Visual Category Models to New Domains. In: ECCV. (2010)</p>
<p>[19] Shon, A.P., Grochow, K., Hertzmann, A., Rao, R.P.N.: Learning Shared Latent Structure for Image Synthesis and Robotic Imitation. In: NIPS. (2006) 1233–1240</p>
<p>[20] Kulis, B., Saenko, K., Darrell, T.: What You Saw is Not What You Get: Domain Adaptation Using Asymmetric Kernel Transforms. In: CVPR. (2011)</p>
<p>[21] Gopalan, R., Li, R., Chellappa, R.: Domain Adaptation for Object Recognition: An Unsupervised Approach. In: ICCV. (2011)</p>
<p>[22] Rosset, S., Zhu, J., Hastie, T.: Boosting as a Regularized Path to a Maximum Margin Classiﬁer. JMLR (2004)</p>
<p>[23] Caruana, R., Niculescu-Mizil, A.: An Empirical Comparison of Supervised Learning Algorithms. In: ICML. (2006)</p>
<p>[24] Viola, P., Jones, M.: Rapid Object Detection Using a Boosted Cascade of Simple Features. In: CVPR. (2001)</p>
<p>[25] Ali, K., Fleuret, F., Hasler, D., Fua, P.: A Real-Time Deformable Detector. PAMI 34(2) (February 2012) 225–239</p>
<p>[26] Freund, Y., Schapire, R.: A Short Introduction to Boosting (1999) Journal of Japanese Society for Artiﬁcial Intelligence, 14(5):771-780.</p>
<p>[27] Becker, C., Ali, K., Knott, G., Fua, P.: Learning Context Cues for Synapse Segmentation. TMI (2013) In Press.  9</p>
<br/>
<br/><br/><br/></body>
</html>
