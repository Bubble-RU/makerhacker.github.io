<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>41 nips-2013-Approximate inference in latent Gaussian-Markov models from continuous time observations</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-41" href="../nips2013/nips-2013-Approximate_inference_in_latent_Gaussian-Markov_models_from_continuous_time_observations.html">nips2013-41</a> <a title="nips-2013-41-reference" href="#">nips2013-41-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>41 nips-2013-Approximate inference in latent Gaussian-Markov models from continuous time observations</h1>
<br/><p>Source: <a title="nips-2013-41-pdf" href="http://papers.nips.cc/paper/4885-approximate-inference-in-latent-gaussian-markov-models-from-continuous-time-observations.pdf">pdf</a></p><p>Author: Botond Cseke, Manfred Opper, Guido Sanguinetti</p><p>Abstract: We propose an approximate inference algorithm for continuous time Gaussian Markov process models with both discrete and continuous time likelihoods. We show that the continuous time limit of the expectation propagation algorithm exists and results in a hybrid ﬁxed point iteration consisting of (1) expectation propagation updates for discrete time terms and (2) variational updates for the continuous time term. We introduce postinference corrections methods that improve on the marginals of the approximation. This approach extends the classical Kalman-Bucy smoothing procedure to non-Gaussian observations, enabling continuous-time inference in a variety of models, including spiking neuronal models (state-space models with point process observations) and box likelihood models. Experimental results on real and simulated data demonstrate high distributional accuracy and signiﬁcant computational savings compared to discrete-time approaches in a neural application. 1</p><br/>
<h2>reference text</h2><p>C. Archambeau, D. Cornford, M. Opper, and J. Shawe-Taylor. Gaussian process approximations of stochastic differential equations. Journal of Machine Learning Research - Proceedings Track, 1:1–16, 2007. B. Cseke and T. Heskes. Properties of Bethe free energies and message passing in Gaussian models. Journal of Artiﬁcial Intelligence Research, 41:1–24, 2011a. B. Cseke and T. Heskes. Approximate marginals in latent Gaussian models. Journal of Machine Learning Research, 12:417–457, 2011b. T. A. Davis. Direct Methods for Sparse Linear Systems (Fundamentals of Algorithms 2). Society for Industrial and Applied Mathematics, Philadelphia, 2006. P. M. Di Lorenzo and J. D. Victor. Taste response variability and temporal coding in the nucleus of the solitary tract of the rat. Journal of Neurophysiology, 90:1418–1431, 2003. C. W. Gardiner. Handbook of stochastic methods: for physics, chemistry and the natural sciences. Springer series in synergetics, 13. Springer, 2002. T. Heskes, M. Opper, W. Wiegerinck, O. Winther, and O. Zoeter. Approximate inference techniques with expectation constraints. Journal of Statistical Mechanics: Theory and Experiment, 2005. H. J. Kappen, V. G´ mez, and M. Opper. Optimal control as a graphical model inference problem. Machine Learning, o 87(2):159–182, 2012. J. F. C. Kingman. Poisson Processes. Oxford Statistical Science Series. Oxford University Press, New York, 1992. S. L. Lauritzen. Graphical Models. Oxford Statistical Science Series. Oxford University Press, New York, 1996. J. H. Macke, L. Buesing, J. P. Cunningham, B. M. Yu, K. V. Shenoy, and M. Sahani. Empirical models of spiking in neural populations. In Advances in Neural Information Processing Systems 24, pages 1350–1358. 2011. T. P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, MIT, 2001. I. Murray, R. P. Adams, and D. J.C. MacKay. Elliptical slice sampling. In Proceedings of the 13th International Conference on Artiﬁcial Intelligence and Statistics, pages 541–548. 2010. A. Ocone, A.J. Millar, and G. Sanguinetti. Hybrid regulatory models: a statistically tractable approach to model regulatory network dynamics. Bioinformatics, 29(7):910–916, 2013. B. Øksendal. Stochastic differential equations. Universitext. Springer, 2010. M. Opper and G. Sanguinetti. Variational inference for Markov jump processes. In Advances in Neural Information Processing Systems 20, 2008. M. Opper and O. Winther. Gaussian processes for classiﬁcation: Mean-ﬁeld algorithms. Neural Computation, 12(11): 2655–2684, 2000. M. Opper and O. Winther. Expectation consistent approximate inference. Journal of Machine Learing Research, 6: 2177–2204, 2005. M. Opper, U. Paquet, and O. Winther. Improving on Expectation Propagation. In Advances in Neural Information Processing Systems 21, pages 1241–1248. MIT, Cambridge, MA, US, 2009. M. Opper, A. Ruttor, and G. Sanguinetti. Approximate inference in continuous time Gaussian-Jump processes. In Advances in Neural Information Processing Systems 23, pages 1831–1839, 2010. V. Rao and Y-W Teh. MCMC for continuous-time discrete-state systems. In Advances in Neural Information Processing Systems 25, pages 710–718, 2012. S. S¨ rkk¨ . Recursive Bayesian Inference on Stochastic Differential Equations. PhD thesis, Helsinki University of a a Technology, 2006. A. C. Smith and E. N. Brown. Estimating a state-space model from point process observations. Neural Computation, 15(5):965–991, 2003. W. Wiegerinck and T. Heskes. Fractional Belief Propagation. In Advances in Neural Information Processing Systems 15, pages 438–445, Cambridge, MA, 2003. The MIT Press. J. S. Yedidia, W. T. Freeman, and Y. Weiss. Generalized belief propagation. In Advances in Neural Information Processing Systems 12, pages 689–695, Cambridge, MA, 2000. The MIT Press. A. Zammit Mangion, K. Yuan, V. Kadirkamanathan, M. Niranjan, and G. Sanguinetti. Online variational inference for state-space models with point-process observations. Neural Computation, 23(8):1967–1999, 2011. A. Zammit-Mangion, G. Dewar, M., Kadirkamanathan V., A., and G. Sanguinetti. Point process modelling of the Afghan war diary. Proceeding of the National Academy of Sciences, 2012. doi: 10.1073/pnas.1203177109.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
