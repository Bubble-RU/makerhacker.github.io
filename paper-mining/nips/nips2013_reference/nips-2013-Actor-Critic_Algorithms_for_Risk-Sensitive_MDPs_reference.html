<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-24" href="../nips2013/nips-2013-Actor-Critic_Algorithms_for_Risk-Sensitive_MDPs.html">nips2013-24</a> <a title="nips-2013-24-reference" href="#">nips2013-24-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>24 nips-2013-Actor-Critic Algorithms for Risk-Sensitive MDPs</h1>
<br/><p>Source: <a title="nips-2013-24-pdf" href="http://papers.nips.cc/paper/4917-actor-critic-algorithms-for-risk-sensitive-mdps.pdf">pdf</a></p><p>Author: Prashanth L.A., Mohammad Ghavamzadeh</p><p>Abstract: In many sequential decision-making problems we may want to manage risk by minimizing some measure of variability in rewards in addition to maximizing a standard criterion. Variance-related risk measures are among the most common risk-sensitive criteria in ﬁnance and operations research. However, optimizing many such criteria is known to be a hard problem. In this paper, we consider both discounted and average reward Markov decision processes. For each formulation, we ﬁrst deﬁne a measure of variability for a policy, which in turn gives us a set of risk-sensitive criteria to optimize. For each of these criteria, we derive a formula for computing its gradient. We then devise actor-critic algorithms for estimating the gradient and updating the policy parameters in the ascent direction. We establish the convergence of our algorithms to locally risk-sensitive optimal policies. Finally, we demonstrate the usefulness of our algorithms in a trafﬁc signal control application. 1</p><br/>
<h2>reference text</h2><p>[1] D. Bertsekas. Nonlinear programming. Athena Scientiﬁc, 1999.</p>
<p>[2] S. Bhatnagar, R. Sutton, M. Ghavamzadeh, and M. Lee. Natural actor-critic algorithms. Automatica, 45 (11):2471–2482, 2009.</p>
<p>[3] S. Bhatnagar, H. Prasad, and L.A. Prashanth. Stochastic Recursive Algorithms for Optimization, volume 434. Springer, 2013.</p>
<p>[4] V. Borkar. A sensitivity formula for the risk-sensitive cost and the actor-critic algorithm. Systems & Control Letters, 44:339–346, 2001.</p>
<p>[5] V. Borkar. Q-learning for risk-sensitive control. Mathematics of Operations Research, 27:294–311, 2002.</p>
<p>[6] H. Chen, T. Duncan, and B. Pasik-Duncan. A Kiefer-Wolfowitz algorithm with randomized differences. IEEE Transactions on Automatic Control, 44(3):442–453, 1999.</p>
<p>[7] E. Delage and S. Mannor. Percentile optimization for Markov decision processes with parameter uncertainty. Operations Research, 58(1):203–213, 2010.</p>
<p>[8] J. Filar, L. Kallenberg, and H. Lee. Variance-penalized Markov decision processes. Mathematics of Operations Research, 14(1):147–161, 1989.</p>
<p>[9] J. Filar, D. Krass, and K. Ross. Percentile performance criteria for limiting average Markov decision processes. IEEE Transaction of Automatic Control, 40(1):2–10, 1995.</p>
<p>[10] R. Howard and J. Matheson. Risk sensitive Markov decision processes. Management Science, 18(7): 356–369, 1972.</p>
<p>[11] V. Katkovnik and Y. Kulchitsky. Convergence of a class of random search algorithms. Automatic Remote Control, 8:81–87, 1972.</p>
<p>[12] A. Nilim and L. El Ghaoui. Robust control of Markov decision processes with uncertain transition matrices. Operations Research, 53(5):780–798, 2005.</p>
<p>[13] J. Peters, S. Vijayakumar, and S. Schaal. Natural actor-critic. In Proceedings of the Sixteenth European Conference on Machine Learning, pages 280–291, 2005.</p>
<p>[14] L.A. Prashanth and S. Bhatnagar. Reinforcement learning with average cost for adaptive control of trafﬁc lights at intersections. In Proceedings of the Fourteenth International IEEE Conference on Intelligent Transportation Systems, pages 1640–1645. IEEE, 2011.</p>
<p>[15] L.A. Prashanth and S. Bhatnagar. Reinforcement Learning With Function Approximation for Trafﬁc Signal Control. IEEE Transactions on Intelligent Transportation Systems, 12(2):412–421, june 2011.</p>
<p>[16] L.A. Prashanth and S. Bhatnagar. Threshold Tuning Using Stochastic Optimization for Graded Signal Control. IEEE Transactions on Vehicular Technology, 61(9):3865–3880, Nov. 2012.</p>
<p>[17] L.A. Prashanth and M. Ghavamzadeh. Actor-Critic Algorithms for Risk-Sensitive MDPs. Technical report inria-00794721, INRIA, 2013.</p>
<p>[18] W. Sharpe. Mutual fund performance. Journal of Business, 39(1):119–138, 1966.</p>
<p>[19] M. Sobel. The variance of discounted Markov decision processes. Applied Probability, pages 794–802, 1982.</p>
<p>[20] J. Spall. Multivariate stochastic approximation using a simultaneous perturbation gradient approximation. IEEE Transactions on Automatic Control, 37(3):332–341, 1992.</p>
<p>[21] R. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement learning with function approximation. In Proceedings of Advances in Neural Information Processing Systems 12, pages 1057–1063, 2000.</p>
<p>[22] A. Tamar, D. Di Castro, and S. Mannor. Policy gradients with variance related risk criteria. In Proceedings of the Twenty-Ninth International Conference on Machine Learning, pages 387–396, 2012.</p>
<p>[23] A. Tamar, D. Di Castro, and S. Mannor. Temporal difference methods for the variance of the reward to go. In Proceedings of the Thirtieth International Conference on Machine Learning, pages 495–503, 2013.</p>
<p>[24] H. Xu and S. Mannor. Distributionally robust Markov decision processes. Mathematics of Operations Research, 37(2):288–300, 2012.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
