<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-43" href="../nips2013/nips-2013-Auxiliary-variable_Exact_Hamiltonian_Monte_Carlo_Samplers_for_Binary_Distributions.html">nips2013-43</a> <a title="nips-2013-43-reference" href="#">nips2013-43-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>43 nips-2013-Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary Distributions</h1>
<br/><p>Source: <a title="nips-2013-43-pdf" href="http://papers.nips.cc/paper/5045-auxiliary-variable-exact-hamiltonian-monte-carlo-samplers-for-binary-distributions.pdf">pdf</a></p><p>Author: Ari Pakman, Liam Paninski</p><p>Abstract: We present a new approach to sample from generic binary distributions, based on an exact Hamiltonian Monte Carlo algorithm applied to a piecewise continuous augmentation of the binary distribution of interest. An extension of this idea to distributions over mixtures of binary and possibly-truncated Gaussian or exponential variables allows us to sample from posteriors of linear and probit regression models with spike-and-slab priors and truncated parameters. We illustrate the advantages of these algorithms in several examples in which they outperform the Metropolis or Gibbs samplers. 1</p><br/>
<h2>reference text</h2><p>[1] R Neal. MCMC Using Hamiltonian Dynamics. Handbook of Markov Chain Monte Carlo, pages 113–162, 2011.</p>
<p>[2] Ari Pakman and Liam Paninski. Exact Hamiltonian Monte Carlo for Truncated Multivariate Gaussians. Journal of Computational and Graphical Statistics, 2013, arXiv:1208.4118.</p>
<p>[3] John A Hertz, Anders S Krogh, and Richard G Palmer. Introduction to the theory of neural computation, volume 1. Westview press, 1991.</p>
<p>[4] Yichuan Zhang, Charles Sutton, Amos Storkey, and Zoubin Ghahramani. Continuous Relaxations for Discrete Hamiltonian Monte Carlo. In Advances in Neural Information Processing Systems 25, pages 3203–3211, 2012.</p>
<p>[5] M.D. Hoffman and A. Gelman. The No-U-Turn sampler: adaptively setting path lengths in Hamiltonian Monte Carlo. Arxiv preprint arXiv:1111.4246, 2011.</p>
<p>[6] T. Park and G. Casella. The Bayesian lasso. Journal of the American Statistical Association, 103(482):681–686, 2008.</p>
<p>[7] C.M. Carvalho, N.G. Polson, and J.G. Scott. The horseshoe estimator for sparse signals. Biometrika, 97(2):465–480, 2010.</p>
<p>[8] T.J. Mitchell and J.J. Beauchamp. Bayesian variable selection in linear regression. Journal of the American Statistical Association, 83(404):1023–1032, 1988.</p>
<p>[9] E.I. George and R.E. McCulloch. Variable selection via Gibbs sampling. Journal of the American Statistical Association, 88(423):881–889, 1993.</p>
<p>[10] S. Mohamed, K. Heller, and Z. Ghahramani. Bayesian and L1 approaches to sparse unsupervised learning. arXiv preprint arXiv:1106.1157, 2011.</p>
<p>[11] I.J. Goodfellow, A. Courville, and Y. Bengio. Spike-and-slab sparse coding for unsupervised feature discovery. arXiv preprint arXiv:1201.3382, 2012.</p>
<p>[12] Yutian Chen and Max Welling. Bayesian structure learning for Markov random ﬁelds with a spike and slab prior. arXiv preprint arXiv:1206.1088, 2012.</p>
<p>[13] Peter J Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. Biometrika, 82(4):711–732, 1995.</p>
<p>[14] Mark E.J. Newman and Gerard T. Barkema. Monte Carlo methods in statistical physics. Oxford: Clarendon Press, 1999., 1, 1999.</p>
<p>[15] Alan D Sokal. Monte Carlo methods in statistical mechanics: foundations and new algorithms, 1989.</p>
<p>[16] Fugao Wang and David P Landau. Efﬁcient, multiple-range random walk algorithm to calculate the density of states. Physical Review Letters, 86(10):2050–2053, 2001.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
