<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>244 nips-2013-Parametric Task Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-244" href="../nips2013/nips-2013-Parametric_Task_Learning.html">nips2013-244</a> <a title="nips-2013-244-reference" href="#">nips2013-244-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>244 nips-2013-Parametric Task Learning</h1>
<br/><p>Source: <a title="nips-2013-244-pdf" href="http://papers.nips.cc/paper/5213-parametric-task-learning.pdf">pdf</a></p><p>Author: Ichiro Takeuchi, Tatsuya Hongo, Masashi Sugiyama, Shinichi Nakajima</p><p>Abstract: We introduce an extended formulation of multi-task learning (MTL) called parametric task learning (PTL) that can systematically handle inﬁnitely many tasks parameterized by a continuous parameter. Our key ﬁnding is that, for a certain class of PTL problems, the path of the optimal task-wise solutions can be represented as piecewise-linear functions of the continuous task parameter. Based on this fact, we employ a parametric programming technique to obtain the common shared representation across all the continuously parameterized tasks. We show that our PTL formulation is useful in various scenarios such as learning under non-stationarity, cost-sensitive learning, and quantile regression. We demonstrate the advantage of our approach in these scenarios.</p><br/>
<h2>reference text</h2><p>[1] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Advances in Neural Information Processing Systems, volume 19, pages 41–48. 2007.</p>
<p>[2] A. Argyriou, C. A. Micchelli, M. Pontil, and Y. Ying. A spectral regularization framework for multi-task structure learning. In Advances in Neural Information Processing Systems, volume 20, pages 25–32. 2008.</p>
<p>[3] L. Cao and F. Tay. Support vector machine with adaptive parameters in ﬁnantial time series forecasting. IEEE Transactions on Neural Networks, 14(6):1506–1518, 2003.</p>
<p>[4] F. R. Bach, D. Heckerman, and E. Horvits. Considering cost asymmetry in learning classiﬁers. Journal of Machine Learning Research, 7:1713–41, 2006.</p>
<p>[5] R. Koenker. Quantile Regression. Cambridge University Press, 2005.</p>
<p>[6] K. Ritter. On parametric linear and quadratic programming problems. mathematical Programming: Proceedings of the International Congress on Mathematical Programming, pages 307–335, 1984.</p>
<p>[7] E. L. Allgower and K. George. Continuation and path following. Acta Numerica, 2:1–63, 1993.</p>
<p>[8] T. Gal. Postoptimal Analysis, Parametric Programming, and Related Topics. Walter de Gruyter, 1995.</p>
<p>[9] M. J. Best. An algorithm for the solution of the parametric quadratic programming problem. Applied Mathemetics and Parallel Computing, pages 57–76, 1996.</p>
<p>[10] M. Fazel, H. Hindi, and S. P. Boyd. A rank minimization heuristic with application to minimum order system approximation. In Proceedings of the American Control Conference, volume 6, pages 4734–4739, 2001.</p>
<p>[11] B. A. Turlach, W. N. Venables, and S. J. Wright. Simultaneous variable selection. Technometrics, 47:349–363, 2005.</p>
<p>[12] G. Obozinski, B. Taskar, and M. Jordan. Joint covariate selection and joint sbspace selection for multiple classiﬁcation problems. Statistics and Computing, 20(2):231–252, 2010.</p>
<p>[13] M. R. Osborne, B. Presnell, and B. A. Turlach. A new approach to variable selection in least squares problems. IMA Journal of Numerical Analysis, 20(20):389–404, 2000.</p>
<p>[14] B. Efron and R. Tibshirani. Least angle regression. Annals of Statistics, 32(2):407–499, 2004.</p>
<p>[15] T. Hastie, S. Rosset, R. Tibshirani, and J. Zhu. The entire regularization path for the support vector machine. Journal of Machine Learning Research, 5:1391–415, 2004.</p>
<p>[16] Y. Lin, Y. Lee, and G. Wahba. Support vector machines for classiﬁcation in nonstandard situations. Machine Learning, 46:191–202, 2002.</p>
<p>[17] M. A. Davenport, R. G. Baraniuk, and C. D. Scott. Tuning support vector machine for minimax and Neyman-Pearson classiﬁcation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2010.</p>
<p>[18] G. Lee and C. Scott. Nested support vector machines. IEEE Transactions on Signal Processing, 58(3):1648–1660, 2010.</p>
<p>[19] R. Koenker. Quantile Regression. Cambridge University Press, 2005.</p>
<p>[20] I. Takeuchi, Q. V. Le, T. Sears, and A. J. Smola. Nonparametric quantile estimation. Journal of Machine Learning Research, 7:1231–1264, 2006.</p>
<p>[21] L. K. Bachrach, T. Hastie, M. C. Wang, B. Narasimhan, and R. Marcus. Acquisition in healthy Asian, hispanic, black and caucasian youth. a longitudinal study. The Journal of Clinical Endocrinology and Metabolism, 84:4702–4712, 1999.</p>
<p>[22] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
