<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-144" href="../nips2013/nips-2013-Inverse_Density_as_an_Inverse_Problem%3A_the_Fredholm_Equation_Approach.html">nips2013-144</a> <a title="nips-2013-144-reference" href="#">nips2013-144-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>144 nips-2013-Inverse Density as an Inverse Problem: the Fredholm Equation Approach</h1>
<br/><p>Source: <a title="nips-2013-144-pdf" href="http://papers.nips.cc/paper/4897-inverse-density-as-an-inverse-problem-the-fredholm-equation-approach.pdf">pdf</a></p><p>Author: Qichao Que, Mikhail Belkin</p><p>Abstract: q We address the problem of estimating the ratio p where p is a density function and q is another density, or, more generally an arbitrary function. Knowing or approximating this ratio is needed in various problems of inference and integration often referred to as importance sampling in statistical inference. It is also closely related to the problem of covariate shift in transfer learning. Our approach is based on reformulating the problem of estimating the ratio as an inverse problem in terms of an integral operator corresponding to a kernel, known as the Fredholm problem of the ﬁrst kind. This formulation, combined with the techniques of regularization leads to a principled framework for constructing algorithms and for analyzing them theoretically. The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized Estimator) is ﬂexible, simple and easy to implement. We provide detailed theoretical analysis including concentration bounds and convergence rates for the Gaussian kernel for densities deﬁned on Rd and smooth d-dimensional sub-manifolds of the Euclidean space. Model selection for unsupervised or semi-supervised inference is generally a difﬁcult problem. It turns out that in the density ratio estimation setting, when samples from both distributions are available, simple completely unsupervised model selection methods are available. We call this mechanism CD-CV for Cross-Density Cross-Validation. We show encouraging experimental results including applications to classiﬁcation within the covariate shift framework. 1</p><br/>
<h2>reference text</h2><p>[1] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. JMLR, 7:2399–2434, 2006.</p>
<p>[2] S. Bickel, M. Br¨ ckner, and T. Scheffer. Discriminative learning for differing training and test u distributions. In ICML, 2007.</p>
<p>[3] E. De Vito, L. Rosasco, A. Caponnetto, U. De Giovannini, and F. Odone. Learning from examples as an inverse problem. JMLR, 6:883, 2006.</p>
<p>[4] E. De Vito, L. Rosasco, and A. Toigo. Spectral regularization for support estimation. In NIPS, pages 487–495, 2010.</p>
<p>[5] H. W. Engl, M. Hanke, and A. Neubauer. Regularization of inverse problems. Springer, 1996.</p>
<p>[6] A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, and B. Sch¨ lkopf. Covariate o shift by kernel mean matching. Dataset shift in machine learning, pages 131–160, 2009.</p>
<p>[7] S. Gr¨ new¨ lder, A. Gretton, and J. Shawe-Taylor. Smooth operators. In ICML, 2013. u a</p>
<p>[8] S. Gr¨ new¨ lder, G. Lever, L. Baldassarre, S. Patterson, A. Gretton, and M. Pontil. Conditional u a mean embeddings as regressors. In ICML, 2012.</p>
<p>[9] J. Huang, A. Gretton, K. M. Borgwardt, B. Sch¨ lkopf, and A. Smola. Correcting sample o selection bias by unlabeled data. In NIPS, pages 601–608, 2006.</p>
<p>[10] T. Kanamori, S. Hido, and M. Sugiyama. A least-squares approach to direct importance estimation. JMLR, 10:1391–1445, 2009.</p>
<p>[11] J. S. Kim and C. Scott. Robust kernel density estimation. In ICASSP, pages 3381–3384, 2008.</p>
<p>[12] S. Mukherjee and V. Vapnik. Support vector method for multivariate density estimation. In Center for Biological and Computational Learning. Department of Brain and Cognitive Sciences, MIT. CBCL, volume 170, 1999.</p>
<p>[13] R. M. Neal. Annealed importance sampling. Statistics and Computing, 11(2):125–139, 2001.</p>
<p>[14] X. Nguyen, M. J. Wainwright, and M. I. Jordan. Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization. NIPS, 20:1089–1096, 2008.</p>
<p>[15] S. J. Pan and Q. Yang. A survey on transfer learning. Knowledge and Data Engineering, IEEE Transactions on, 22(10):1345–1359, 2010.</p>
<p>[16] B. Sch¨ lkopf and A. J. Smola. Learning with kernels: Support vector machines, regularization, o optimization, and beyond. MIT press, 2001.</p>
<p>[17] J. Shawe-Taylor and N. Cristianini. Kernel methods for pattern analysis. Cambridge university press, 2004.</p>
<p>[18] T. Shi, M. Belkin, and B. Yu. Data spectroscopy: Eigenspaces of convolution operators and clustering. The Annals of Statistics, 37(6B):3960–3984, 2009.</p>
<p>[19] H. Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90(2):227–244, 2000.</p>
<p>[20] A. J. Smola and B. Sch¨ lkopf. On a kernel-based method for pattern recognition, regression, o approximation, and operator inversion. Algorithmica, 22(1):211–231, 1998.</p>
<p>[21] I. Steinwart and A. Christmann. Support vector machines. Springer, 2008.</p>
<p>[22] M. Sugiyama, M. Krauledat, and K. M¨ ller. Covariate shift adaptation by importance weighted u cross validation. JMLR, 8:985–1005, 2007.</p>
<p>[23] Masashi Sugiyama, Shinichi Nakajima, Hisashi Kashima, Paul Von Buenau, and Motoaki Kawanabe. Direct importance estimation with model selection and its application to covariate shift adaptation. NIPS, 20:1433–1440, 2008.</p>
<p>[24] A. Tsybakov. Introduction to nonparametric estimation. Springer, 2009.</p>
<p>[25] C. Williams and M. Seeger. The effect of the input density distribution on kernel-based classiﬁers. In ICML, 2000.</p>
<p>[26] Y. Yu and C. Szepesv´ ri. Analysis of kernel mean matching under covariate shift. In ICML, a 2012.</p>
<p>[27] B. Zadrozny. Learning and evaluating classiﬁers under sample selection bias. In ICML, 2004.  9</p>
<br/>
<br/><br/><br/></body>
</html>
