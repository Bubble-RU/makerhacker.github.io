<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>209 nips-2013-New Subsampling Algorithms for Fast Least Squares Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-209" href="../nips2013/nips-2013-New_Subsampling_Algorithms_for_Fast_Least_Squares_Regression.html">nips2013-209</a> <a title="nips-2013-209-reference" href="#">nips2013-209-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>209 nips-2013-New Subsampling Algorithms for Fast Least Squares Regression</h1>
<br/><p>Source: <a title="nips-2013-209-pdf" href="http://papers.nips.cc/paper/5105-new-subsampling-algorithms-for-fast-least-squares-regression.pdf">pdf</a></p><p>Author: Paramveer Dhillon, Yichao Lu, Dean P. Foster, Lyle Ungar</p><p>Abstract: We address the problem of fast estimation of ordinary least squares (OLS) from large amounts of data (n p). We propose three methods which solve the big data problem by subsampling the covariance matrix using either a single or two stage estimation. All three run in the order of size of input i.e. O(np) and our best method, Uluru, gives an error bound of O( p/n) which is independent of the amount of subsampling as long as it is above a threshold. We provide theoretical bounds for our algorithms in the ﬁxed design (with Randomized Hadamard preconditioning) as well as sub-Gaussian random design setting. We also compare the performance of our methods on synthetic and real-world datasets and show that if observations are i.i.d., sub-Gaussian then one can directly subsample without the expensive Randomized Hadamard preconditioning without loss of accuracy. 1</p><br/>
<h2>reference text</h2><p>[1] Boutsidis, C., Gittens, A.: Improved matrix algorithms via the subsampled randomized hadamard transform. CoRR abs/1204.0062 (2012)</p>
<p>[2] Tygert, M.: A fast algorithm for computing minimal-norm solutions to underdetermined systems of linear equations. CoRR abs/0905.4745 (2009)</p>
<p>[3] Rokhlin, V., Tygert, M.: A fast randomized algorithm for overdetermined linear least-squares regression. Proceedings of the National Academy of Sciences 105(36) (September 2008) 13212–13217</p>
<p>[4] Drineas, P., Mahoney, M.W., Muthukrishnan, S., Sarl´ s, T.: Faster least squares approximao tion. CoRR abs/0710.1435 (2007)</p>
<p>[5] Mahoney, M.W.: Randomized algorithms for matrices and data. (April 2011)</p>
<p>[6] Ailon, N., Chazelle, B.: Approximate nearest neighbors and the fast johnson-lindenstrauss transform. In: STOC. (2006) 557–563</p>
<p>[7] Avron, H., Maymounkov, P., Toledo, S.: Blendenpik: Supercharging lapack’s least-squares solver. SIAM J. Sci. Comput. 32(3) (April 2010) 1217–1236</p>
<p>[8] Vershynin, R.: How Close is the Sample Covariance Matrix to the Actual Covariance Matrix? Journal of Theoretical Probability 25(3) (September 2012) 655–686</p>
<p>[9] Golub, G.H., Van Loan, C.F.: Matrix Computations (Johns Hopkins Studies in Mathematical Sciences)(3rd Edition). 3rd edn. The Johns Hopkins University Press (October 1996)</p>
<p>[10] Vershynin, R.: Introduction to the non-asymptotic analysis of random matrices. CoRR abs/1011.3027 (2010)</p>
<p>[11] Dhillon, P.S., Rodu, J., Foster, D., Ungar, L.: Two step cca: A new spectral method for estimating vector models of words. In: Proceedings of the 29th International Conference on Machine learning. ICML’12 (2012)</p>
<p>[12] Dhillon, P.S., Foster, D., Ungar, L.: Multi-view learning of word embeddings via cca. In: Advances in Neural Information Processing Systems (NIPS). Volume 24. (2011)</p>
<p>[13] Shalev-Shwartz, S., Zhang, T.: Stochastic dual coordinate ascent methods for regularized loss minimization. CoRR abs/1209.1873 (2012)  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
