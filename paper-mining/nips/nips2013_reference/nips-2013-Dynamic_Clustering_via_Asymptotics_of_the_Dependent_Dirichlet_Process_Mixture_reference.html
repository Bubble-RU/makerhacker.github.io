<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-100" href="../nips2013/nips-2013-Dynamic_Clustering_via_Asymptotics_of_the_Dependent_Dirichlet_Process_Mixture.html">nips2013-100</a> <a title="nips-2013-100-reference" href="#">nips2013-100-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>100 nips-2013-Dynamic Clustering via Asymptotics of the Dependent Dirichlet Process Mixture</h1>
<br/><p>Source: <a title="nips-2013-100-pdf" href="http://papers.nips.cc/paper/5094-dynamic-clustering-via-asymptotics-of-the-dependent-dirichlet-process-mixture.pdf">pdf</a></p><p>Author: Trevor Campbell, Miao Liu, Brian Kulis, Jonathan P. How, Lawrence Carin</p><p>Abstract: This paper presents a novel algorithm, based upon the dependent Dirichlet process mixture model (DDPMM), for clustering batch-sequential data containing an unknown number of evolving clusters. The algorithm is derived via a lowvariance asymptotic analysis of the Gibbs sampling algorithm for the DDPMM, and provides a hard clustering with convergence guarantees similar to those of the k-means algorithm. Empirical results from a synthetic test with moving Gaussian clusters and a test with real ADS-B aircraft trajectory data demonstrate that the algorithm requires orders of magnitude less computational time than contemporary probabilistic and hard clustering algorithms, while providing higher accuracy on the examined datasets. 1</p><br/>
<h2>reference text</h2><p>[1] Yee Whye Teh. Dirichlet processes. In Encyclopedia of Machine Learning. Springer, New York, 2010.</p>
<p>[2] Radford M. Neal. Markov chain sampling methods for dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9(2):249–265, 2000.</p>
<p>[3] David M. Blei and Michael I. Jordan. Variational inference for dirichlet process mixtures. Bayesian Analysis, 1(1):121–144, 2006.</p>
<p>[4] Carlos M. Carvalho, Hedibert F. Lopes, Nicholas G. Polson, and Matt A. Taddy. Particle learning for general mixtures. Bayesian Analysis, 5(4):709–740, 2010.</p>
<p>[5] Steven N. MacEachern. Dependent nonparametric processes. In Proceedings of the Bayesian Statistical Science Section. American Statistical Association, 1999.</p>
<p>[6] Dahua Lin, Eric Grimson, and John Fisher. Construction of dependent dirichlet processes based on poisson processes. In Neural Information Processing Systems, 2010.</p>
<p>[7] Matt Hoffman, David Blei, Chong Wang, and John Paisley. Stochastic variational inference. arXiv ePrint 1206.7051, 2012.</p>
<p>[8] Finale Doshi-Velez and Zoubin Ghahramani. Accelerated sampling for the indian buffet process. In Proceedings of the International Conference on Machine Learning, 2009.</p>
<p>[9] Felix Endres, Christian Plagemann, Cyrill Stachniss, and Wolfram Burgard. Unsupervised discovery of object classes from range data using latent dirichlet allocation. In Robotics Science and Systems, 2005.</p>
<p>[10] Matthias Luber, Kai Arras, Christian Plagemann, and Wolfram Burgard. Classifying dynamic objects: An unsupervised learning approach. In Robotics Science and Systems, 2004.</p>
<p>[11] Zhikun Wang, Marc Deisenroth, Heni Ben Amor, David Vogt, Bernard Sch¨ lkopf, and Jan Peters. Probo abilistic modeling of human movements for intention inference. In Robotics Science and Systems, 2008.</p>
<p>[12] Stuart P. Lloyd. Least squares quantization in pcm. IEEE Transactions on Information Theory, 28(2):129– 137, 1982.</p>
<p>[13] Dan Pelleg and Andrew Moore. X-means: Extending k-means with efﬁcient estimation of the number of clusters. In Proceedings of the 17th International Conference on Machine Learning, 2000.</p>
<p>[14] Robert Tibshirani, Guenther Walther, and Trevor Hastie. Estimating the number of clusters in a data set via the gap statistic. Journal of the Royal Statistical Society B, 63(2):411–423, 2001.</p>
<p>[15] Brian Kulis and Michael I. Jordan. Revisiting k-means: New algorithms via bayesian nonparametrics. In Proceedings of the 29th International Conference on Machine Learning (ICML), Edinburgh, Scotland, 2012.</p>
<p>[16] Thomas S. Ferguson. A bayesian analysis of some nonparametric problems. The Annals of Statistics, 1(2):209–230, 1973.</p>
<p>[17] Jayaram Sethuraman. A constructive deﬁnition of dirichlet priors. Statistica Sinica, 4:639–650, 1994.</p>
<p>[18] Tsunenori Ishioka. Extended k-means with an efﬁcient estimation of the number of clusters. In Proceedings of the 2nd International Conference on Intelligent Data Engineering and Automated Learning, pages 17–22, 2000.</p>
<p>[19] Myra Spiliopoulou, Irene Ntoutsi, Yannis Theodoridis, and Rene Schult. Monic - modeling and monitoring cluster transitions. In Proceedings of the 12th International Conference on Knowledge Discovering and Data Mining, pages 706–711, 2006.</p>
<p>[20] Panos Kalnis, Nikos Mamoulis, and Spiridon Bakiras. On discovering moving clusters in spatio-temporal data. In Proceedings of the 9th International Symposium on Spatial and Temporal Databases, pages 364–381. Springer, 2005.</p>
<p>[21] Deepayan Chakraborti, Ravi Kumar, and Andrew Tomkins. Evolutionary clustering. In Proceedings of the SIGKDD International Conference on Knowledge Discovery and Data Mining, 2006.</p>
<p>[22] Kevin Xu, Mark Kliger, and Alfred Hero III. Adaptive evolutionary clustering. Data Mining and Knowledge Discovery, pages 1–33, 2012.</p>
<p>[23] Carlos M. Carvalho, Michael S. Johannes, Hedibert F. Lopes, and Nicholas G. Polson. Particle learning and smoothing. Statistical Science, 25(1):88–106, 2010.</p>
<p>[24] Carine Hue, Jean-Pierre Le Cadre, and Patrick P´ rez. Tracking multiple objects with particle ﬁltering. e IEEE Transactions on Aerospace and Electronic Systems, 38(3):791–812, 2002.</p>
<p>[25] Jaco Vermaak, Arnaud Doucet, and Partick P´ rez. Maintaining multi-modality through mixture tracking. e In Proceedings of the 9th IEEE International Conference on Computer Vision, 2003.</p>
<p>[26] Jasper Snoek, Hugo Larochelle, and Ryan Adams. Practical bayesian optimization of machine learning algorithms. In Neural Information Processing Systems, 2012.  9</p>
<br/>
<br/><br/><br/></body>
</html>
