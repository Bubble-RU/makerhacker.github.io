<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>292 nips-2013-Sequential Transfer in Multi-armed Bandit with Finite Set of Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-292" href="../nips2013/nips-2013-Sequential_Transfer_in_Multi-armed_Bandit_with_Finite_Set_of_Models.html">nips2013-292</a> <a title="nips-2013-292-reference" href="#">nips2013-292-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>292 nips-2013-Sequential Transfer in Multi-armed Bandit with Finite Set of Models</h1>
<br/><p>Source: <a title="nips-2013-292-pdf" href="http://papers.nips.cc/paper/5107-sequential-transfer-in-multi-armed-bandit-with-finite-set-of-models.pdf">pdf</a></p><p>Author: Mohammad Gheshlaghi azar, Alessandro Lazaric, Emma Brunskill</p><p>Abstract: Learning from prior tasks and transferring that experience to improve future performance is critical for building lifelong learning agents. Although results in supervised and reinforcement learning show that transfer may signiﬁcantly improve the learning performance, most of the literature on transfer is focused on batch learning tasks. In this paper we study the problem of sequential transfer in online learning, notably in the multi–armed bandit framework, where the objective is to minimize the total regret over a sequence of tasks by transferring knowledge from prior tasks. We introduce a novel bandit algorithm based on a method-of-moments approach for estimating the possible tasks and derive regret bounds for it. 1</p><br/>
<h2>reference text</h2><p>Agarwal, A., Dudík, M., Kale, S., Langford, J., and Schapire, R. E. (2012). Contextual bandit learning with predictable rewards. In Proceedings of the 15th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS’12). Anandkumar, A., Foster, D. P., Hsu, D., Kakade, S., and Liu, Y.-K. (2012a). A spectral algorithm for latent dirichlet allocation. In Proceedings of Advances in Neural Information Processing Systems 25 (NIPS’12), pages 926–934. Anandkumar, A., Ge, R., Hsu, D., and Kakade, S. M. (2013). A tensor spectral approach to learning mixed membership community models. Journal of Machine Learning Research, 1:65. Anandkumar, A., Ge, R., Hsu, D., Kakade, S. M., and Telgarsky, M. (2012b). Tensor decompositions for learning latent variable models. CoRR, abs/1210.7559. Anandkumar, A., Hsu, D., and Kakade, S. M. (2012c). A method of moments for mixture models and hidden markov models. In Proceeding of the 25th Annual Conference on Learning Theory (COLT’12), volume 23, pages 33.1–33.34. Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002). Finite-time analysis of the multi-armed bandit problem. Machine Learning, 47:235–256. Azar, M. G., Lazaric, A., and Brunskill, E. (2013). Sequential transfer in multi-armed bandit with ﬁnite set of models. CoRR, abs/1307.6887. Cavallanti, G., Cesa-Bianchi, N., and Gentile, C. (2010). Linear algorithms for online multitask classiﬁcation. Journal of Machine Learning Research, 11:2901–2934. Cesa-Bianchi, N. and Lugosi, G. (2006). Prediction, Learning, and Games. Cambridge University Press. Dekel, O., Long, P. M., and Singer, Y. (2006). Online multitask learning. In Proceedings of the 19th Annual Conference on Learning Theory (COLT’06), pages 453–467. Garivier, A. and Moulines, E. (2011). On upper-conﬁdence bound policies for switching bandit problems. In Proceedings of the 22nd international conference on Algorithmic learning theory, ALT’11, pages 174–188, Berlin, Heidelberg. Springer-Verlag. Kleibergen, F. and Paap, R. (2006). Generalized reduced rank tests using the singular value decomposition. Journal of Econometrics, 133(1):97–126. Langford, J. and Zhang, T. (2007). The epoch-greedy algorithm for multi-armed bandits with side information. In Proceedings of Advances in Neural Information Processing Systems 20 (NIPS’07). Lazaric, A. (2011). Transfer in reinforcement learning: a framework and a survey. In Wiering, M. and van Otterlo, M., editors, Reinforcement Learning: State of the Art. Springer. Lugosi, G., Papaspiliopoulos, O., and Stoltz, G. (2009). Online multi-task learning with hard constraints. In Proceedings of the 22nd Annual Conference on Learning Theory (COLT’09). Mann, T. A. and Choe, Y. (2012). Directed exploration in reinforcement learning with transferred knowledge. In Proceedings of the Tenth European Workshop on Reinforcement Learning (EWRL’12). Pan, S. J. and Yang, Q. (2010). A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 22(10):1345–1359. Robbins, H. (1952). Some aspects of the sequential design of experiments. Bulletin of the AMS, 58:527–535. Saha, A., Rai, P., Daumé III, H., and Venkatasubramanian, S. (2011). Online learning of multiple tasks and their relationships. In Proceedings of the 14th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS’11), Ft. Lauderdale, Florida. Stewart, G. W. and Sun, J.-g. (1990). Matrix perturbation theory. Academic press. Taylor, M. E. (2009). Transfer in Reinforcement Learning Domains. Springer-Verlag. Wedin, P. (1972). Perturbation bounds in connection with singular value decomposition. BIT Numerical Mathematics, 12(1):99–111.  9</p>
<br/>
<br/><br/><br/></body>
</html>
