<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-42" href="../nips2013/nips-2013-Auditing%3A_Active_Learning_with_Outcome-Dependent_Query_Costs.html">nips2013-42</a> <a title="nips-2013-42-reference" href="#">nips2013-42-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>42 nips-2013-Auditing: Active Learning with Outcome-Dependent Query Costs</h1>
<br/><p>Source: <a title="nips-2013-42-pdf" href="http://papers.nips.cc/paper/4956-auditing-active-learning-with-outcome-dependent-query-costs.pdf">pdf</a></p><p>Author: Sivan Sabato, Anand D. Sarwate, Nati Srebro</p><p>Abstract: We propose a learning setting in which unlabeled data is free, and the cost of a label depends on its value, which is not known in advance. We study binary classiﬁcation in an extreme case, where the algorithm only pays for negative labels. Our motivation are applications such as fraud detection, in which investigating an honest transaction should be avoided if possible. We term the setting auditing, and consider the auditing complexity of an algorithm: the number of negative labels the algorithm requires in order to learn a hypothesis with low relative error. We design auditing algorithms for simple hypothesis classes (thresholds and rectangles), and show that with these algorithms, the auditing complexity can be signiﬁcantly lower than the active label complexity. We also show a general competitive approach for learning with outcome-dependent costs. 1</p><br/>
<h2>reference text</h2><p>M. F. Balcan, A. Beygelzimer, and J. Langford. Agnostic active learning. In Proceedings of the 23rd international conference on Machine learning (ICML), pages 65–72, 2006. P. L. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: Risk bounds and structural results. Journal of Machine Learning Research, 3:463–482, 2002. A. Beygelzimer, S. Dasgupta, and J. Langford. Importance weighted active learning. In Proceedings of the 26th Annual International Conference on Machine Learning (ICML), pages 49–56. ACM, 2009. J. Blocki, N. Christin, A. Dutta, and A. Sinha. Regret minimizing audits: A learning-theoretic basis for privacy protection. In Proceedings of 24th IEEE Computer Security Foundations Symposium, 2011. A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the VapnikChervonenkis dimension. Journal of the ACM, 36(4):929–965, Oct. 1989. D. Cohn, L. Atlas, and R. Ladner. Improving generalization with active learning. Machine Learning, 15:201–221, 1994. S. Dasgupta. Analysis of a greedy active learning strategy. In L. K. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, pages 337–344. MIT Press, Cambridge, MA, 2005. S. Dasgupta, D. Hsu, and C. Monteleoni. A general agnostic active learning algorithm. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural Information Processing Systems 20, pages 353–360. MIT Press, Cambridge, MA, 2008. L. Devroye and G. Lugosi. Lower bounds in pattern recognition and learning. Pattern Recognition, 28(7):1011–1018, 1995. U. Feige. A threshold of ln n for approximating set cover. Journal of the ACM (JACM), 45(4): 634–652, 1998. D. Golovin and A. Krause. Adaptive submodularity: Theory and applications in active learning and stochastic optimization. Journal of Artiﬁcial Intelligence Research, 42:427–486, 2011. A. Gonen, S. Sabato, and S. Shalev-Shwartz. Efﬁcient active learning of halfspaces: an aggressive approach. In The 30th International Conference on Machine Learning (ICML), 2013. S. Hanneke. A bound on the label complexity of agnostic active learning. In Proceedings of the 24th international conference on Machine learning, pages 353–360. ACM, 2007a. S. Hanneke. Teaching dimension and the complexity of active learning. In Learning Theory, pages 66–81. Springer, 2007b. S. Hanneke. Rates of convergence in active learning. The Annals of Statistics, 39(1):333–361, 2011. L. Hyaﬁl and R. L. Rivest. Constructing optimal binary decision trees is NP-complete. Information Processing Letters, 5(1):15–17, May 1976. A. Kapoor, E. Horvitz, and S. Basu. Selective supervision: Guiding supervised learning with decision-theoretic active learning. In Proceedings of IJCAI, 2007. M. Kearns. Efﬁcient noise-tolerant learning from statistical queries. Journal of the ACM (JACM), 45(6):983–1006, 1998. S. R. Kulkarni, S. K. Mitter, and J. N. Tsitsiklis. Active learning using arbitrary binary valued queries. Machine Learning, 11(1):23–35, 1993. P. M. Long and L. Tan. PAC learning axis-aligned rectangles with respect to product distributions from multiple-instance examples. Machine Learning, 30(1):7–21, 1998. D. D. Margineantu. Active cost-sensitive learning. In Proceedings of IJCAI, 2007. S. Sabato, A. D. Sarwate, and N. Srebro. Auditing: Active learning with outcome-dependent query costs. arXiv preprint arXiv:1306.2347, 2013. B. Settles, M. Craven, and L. Friedlan. Active learning with real annotation costs. In Proceedings of the NIPS Workshop on Cost-Sensitive Learning, 2008. V. N. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, XVI(2):264–280, 1971. 9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
