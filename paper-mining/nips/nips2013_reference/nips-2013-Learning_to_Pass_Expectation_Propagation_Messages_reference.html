<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>168 nips-2013-Learning to Pass Expectation Propagation Messages</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-168" href="../nips2013/nips-2013-Learning_to_Pass_Expectation_Propagation_Messages.html">nips2013-168</a> <a title="nips-2013-168-reference" href="#">nips2013-168-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>168 nips-2013-Learning to Pass Expectation Propagation Messages</h1>
<br/><p>Source: <a title="nips-2013-168-pdf" href="http://papers.nips.cc/paper/5070-learning-to-pass-expectation-propagation-messages.pdf">pdf</a></p><p>Author: Nicolas Heess, Daniel Tarlow, John Winn</p><p>Abstract: Expectation Propagation (EP) is a popular approximate posterior inference algorithm that often provides a fast and accurate alternative to sampling-based methods. However, while the EP framework in theory allows for complex nonGaussian factors, there is still a signiﬁcant practical barrier to using them within EP, because doing so requires the implementation of message update operators, which can be difﬁcult and require hand-crafted approximations. In this work, we study the question of whether it is possible to automatically derive fast and accurate EP updates by learning a discriminative model (e.g., a neural network or random forest) to map EP message inputs to EP message outputs. We address the practical concerns that arise in the process, and we provide empirical analysis on several challenging and diverse factors, indicating that there is a space of factors where this approach appears promising. 1</p><br/>
<h2>reference text</h2><p>[1] S. Barthelm´ and N. Chopin. ABC-EP: Expectation Propagation for likelihood-free Bayesian e computation. In Proceedings of the 28th International Conference on Machine Learning, 2011.</p>
<p>[2] J. Domke. Parameter learning with truncated message-passing. In Computer Vision and Pattern Recognition (CVPR). IEEE, 2011.</p>
<p>[3] J. Domke. Learning graphical model parameters with approximate marginal inference. Pattern Analysis and Machine Intelligence (PAMI), 2013.</p>
<p>[4] N.D. Goodman, V.K. Mansinghka, D.M. Roy, K. Bonawitz, and J.B. Tenenbaum. Church: A language for generative models. In Proc. of Uncertainty in Artiﬁcial Intelligence (UAI), 2008.</p>
<p>[5] R. Herbrich, T.P. Minka, and T. Graepel. Trueskill: A Bayesian skill rating system. Advances in Neural Information Processing Systems, 19:569, 2007.</p>
<p>[6] T.P. Minka. A family of algorithms for approximate Bayesian inference. PhD thesis, Massachusetts Institute of Technology, 2001.</p>
<p>[7] T.P. Minka and J. Winn. Gates: A graphical notation for mixture models. In Advances in Neural Information Processing Systems, 2008.</p>
<p>[8] T.P. Minka, J.M. Winn, J.P. Guiver, and D.A. Knowles. Infer.NET 2.5, 2012. Microsoft Research. http://research.microsoft.com/infernet.</p>
<p>[9] P. Kohli R. Shapovalov, D. Vetrov. Spatial inference machines. In Computer Vision and Pattern Recognition (CVPR). IEEE, 2013.</p>
<p>[10] S. Ross, D. Munoz, M. Hebert, and J.A. Bagnell. Learning message-passing inference machines for structured prediction. In Computer Vision and Pattern Recognition (CVPR). IEEE, 2011.</p>
<p>[11] D.B. Rubin. Bayesianly justiﬁable and relevant frequency calculations for the applies statistician. The Annals of Statistics, pages 1151–1172, 1984.</p>
<p>[12] Stan Development Team. Stan: A C++ library for probability and sampling, version 1.3, 2013.</p>
<p>[13] D.H. Stern, R. Herbrich, and T. Graepel. Matchbox: Large scale online Bayesian recommendations. In Proceedings of the 18th international conference on World Wide Web, pages 111–120. ACM, 2009.</p>
<p>[14] A. Thomas. BUGS: A statistical modelling package. RTA/BCS Modular Languages Newsletter, 1994.</p>
<p>[15] D. Wingate, N.D. Goodman, A. Stuhlmueller, and J. Siskind. Nonstandard interpretations of probabilistic programs for efﬁcient inference. In Advances in Neural Information Processing Systems, 2011.</p>
<p>[16] D. Wingate and T. Weber. Automated variational inference in probabilistic programming. In arXiv:1301.1299, 2013.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
