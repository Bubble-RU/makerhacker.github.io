<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>115 nips-2013-Factorized Asymptotic Bayesian Inference for Latent Feature Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-115" href="../nips2013/nips-2013-Factorized_Asymptotic_Bayesian_Inference_for_Latent_Feature_Models.html">nips2013-115</a> <a title="nips-2013-115-reference" href="#">nips2013-115-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>115 nips-2013-Factorized Asymptotic Bayesian Inference for Latent Feature Models</h1>
<br/><p>Source: <a title="nips-2013-115-pdf" href="http://papers.nips.cc/paper/5171-factorized-asymptotic-bayesian-inference-for-latent-feature-models.pdf">pdf</a></p><p>Author: Kohei Hayashi, Ryohei Fujimaki</p><p>Abstract: This paper extends factorized asymptotic Bayesian (FAB) inference for latent feature models (LFMs). FAB inference has not been applicable to models, including LFMs, without a speciﬁc condition on the Hessian matrix of a complete loglikelihood, which is required to derive a “factorized information criterion” (FIC). Our asymptotic analysis of the Hessian matrix of LFMs shows that FIC of LFMs has the same form as those of mixture models. FAB/LFMs have several desirable properties (e.g., automatic hidden states selection and parameter identiﬁability) and empirically perform better than state-of-the-art Indian Buffet processes in terms of model selection, prediction, and computational efﬁciency. 1</p><br/>
<h2>reference text</h2><p>[1] T. Broderick, B. Kulis, and M. I. Jordan. MAD-Bayes: MAP-based Asymptotic Derivations from Bayes. In ICML, 2013.</p>
<p>[2] F. Doshi-Velez and Z. Ghahramani. Accelerated sampling for the indian buffet process. In ICML, 2009.</p>
<p>[3] F. Doshi-Velez, K. T. Miller, J. Van Gael, and Y. W. Teh. Variational inference for the Indian buffet process. In AISTATS, 2009.</p>
<p>[4] A. Frank and A. Asuncion. UCI machine learning repository, 2010.</p>
<p>[5] R. Fujimaki and K. Hayashi. Factorized asymptotic bayesian hidden markov model. In ICML, 2012.</p>
<p>[6] R. Fujimaki and S. Morinaga. Factorized asymptotic bayesian inference for mixture modeling. In AISTATS, 2012.</p>
<p>[7] A. S. Georghiades, P. N. Belhumeur, and D. J. Kriegman. From few to many: Illumination cone models for face recognition under variable lighting and pose. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23:643–660, 2001.</p>
<p>[8] Z. Ghahramani. Factorial learning and the EM algorithm. In NIPS, 1995.</p>
<p>[9] Z. Ghahramani, T. L. Grifﬁths, and P. Sollich. Bayesian nonparametric latent feature models (with discussion). In 8th Valencia International Meeting on Bayesian Statistics, 2006.</p>
<p>[10] T. Grifﬁths and Z. Ghahramani. Inﬁnite latent feature models and the indian buffet process, 2005.</p>
<p>[11] T. L. Grifﬁths and Z. Ghahramani. The indian buffet process: An introduction and review. JMLR, 12:1185–1224, 2011.</p>
<p>[12] U. Hoffmann, G. Garcia, J. M. Vesin, K. Diserens, and T. Ebrahimi. A boosting approach to p300 detection with application to brain-computer interfaces. In International IEEE EMBS Conference on Neural Engineering, pages 97–100. 2005.</p>
<p>[13] J. J. Hull. A database for handwritten text recognition research. IEEE Transactions on Pattern Analysis and Machine Intelligence, 16(5):550–554, 1994.</p>
<p>[14] M. W. Kadous. Temporal Classiﬁcation: Extending the Classiﬁcation Paradigm to Multivariate Time Series. PhD thesis, School of Computer Science & Engineering, University of New South Wales, 2002.</p>
<p>[15] J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1–63, 1997.</p>
<p>[16] K. Miller, T. Grifﬁths, and M. Jordan. Nonparametric latent feature models for link prediction. In NIPS, 2009.</p>
<p>[17] K. T. Miller. Bayesian Nonparametric Latent Feature Models. PhD thesis, University of California, Berkeley, 2011.</p>
<p>[18] S. Nakajima, M. Sugiyama, and D. Babacan. On bayesian PCA: Automatic dimensionality selection and analytic solution. In ICML, 2011.</p>
<p>[19] K. Palla, D. A. Knowles, and Z. Ghahramani. An inﬁnite latent attribute model for network data. In ICML, 2012.</p>
<p>[20] C. Peterson and J. Anderson. A mean ﬁeld theory learning algorithm for neural networks. Complex systems, 1:995–1019, 1987.</p>
<p>[21] G. E. Poliner and D. P. W. Ellis. A discriminative model for polyphonic piano transcription. EURASIP Journal of Advances in Signal Processing, 2007(1):154, 2007.</p>
<p>[22] C. Reed and Z. Ghahramani. Scaling the indian buffet process via submodular maximization. In ICML, 2013.</p>
<p>[23] G. Schwarz. Estimating the dimension of a model. The Annals of Statistics, 6(2):461–464, 1978.</p>
<p>[24] M. Tipping and C. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society. Series B, 61(3):611–622, 1999.</p>
<p>[25] M. J. Wainwright and M. I. Jordan. Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning, 1(1-2):1–305, 2008.</p>
<p>[26] S. Watanabe. Algebraic analysis for nonidentiﬁable learning machines. Neural Computation, 13(4):899– 933, 2001.</p>
<p>[27] S. Watanabe. Algebraic Geometry and Statistical Learning Theory (Cambridge Monographs on Applied and Computational Mathematics). Cambridge University Press, 2009.</p>
<p>[28] R. Wong. Asymptotic Approximation of Integrals (Classics in Applied Mathematics). SIAM, 2001.</p>
<p>[29] A. L. Yuille and A. Rangarajan. The Concave-Convex procedure. Neural Computation, 15(4):915–936, 2003.</p>
<p>[30] R. S. Zemel and G. E. Hinton. Learning population codes by minimizing description length. Neural Computation, 7(3):11–18, 1994.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
