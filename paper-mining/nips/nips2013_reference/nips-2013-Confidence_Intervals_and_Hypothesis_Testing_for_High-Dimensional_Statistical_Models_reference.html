<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-68" href="../nips2013/nips-2013-Confidence_Intervals_and_Hypothesis_Testing_for_High-Dimensional_Statistical_Models.html">nips2013-68</a> <a title="nips-2013-68-reference" href="#">nips2013-68-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>68 nips-2013-Confidence Intervals and Hypothesis Testing for High-Dimensional Statistical Models</h1>
<br/><p>Source: <a title="nips-2013-68-pdf" href="http://papers.nips.cc/paper/4931-confidence-intervals-and-hypothesis-testing-for-high-dimensional-statistical-models.pdf">pdf</a></p><p>Author: Adel Javanmard, Andrea Montanari</p><p>Abstract: Fitting high-dimensional statistical models often requires the use of non-linear parameter estimation procedures. As a consequence, it is generally impossible to obtain an exact characterization of the probability distribution of the parameter estimates. This in turn implies that it is extremely challenging to quantify the uncertainty associated with a certain parameter estimate. Concretely, no commonly accepted procedure exists for computing classical measures of uncertainty and statistical signiﬁcance as conﬁdence intervals or p-values. We consider here a broad class of regression problems, and propose an efﬁcient algorithm for constructing conﬁdence intervals and p-values. The resulting conﬁdence intervals have nearly optimal size. When testing for the null hypothesis that a certain parameter is vanishing, our method has nearly optimal power. Our approach is based on constructing a ‘de-biased’ version of regularized Mestimators. The new construction improves over recent work in the ﬁeld in that it does not assume a special structure on the design matrix. Furthermore, proofs are remarkably simple. We test our method on a diabetes prediction problem. 1</p><br/>
<h2>reference text</h2><p>[1] Practice Fusion Diabetes Classiﬁcation. http://www.kaggle.com/c/pf2012-diabetes, 2012. Kaggle competition dataset.  8</p>
<p>[2] P. J. Bickel, Y. Ritov, and A. B. Tsybakov. Simultaneous analysis of Lasso and Dantzig selector. Amer. J. of Mathematics, 37:1705–1732, 2009.</p>
<p>[3] P. B¨ hlmann. Statistical signiﬁcance in high-dimensional linear models. arXiv:1202.1377, 2012. u</p>
<p>[4] P. B¨ hlmann and S. van de Geer. Statistics for high-dimensional data. Springer-Verlag, 2011. u</p>
<p>[5] E. Cand` s and Y. Plan. Near-ideal model selection by e 37(5A):2145–2177, 2009.  1  minimization. The Annals of Statistics,</p>
<p>[6] E. J. Cand´ s and T. Tao. Decoding by linear programming. IEEE Trans. on Inform. Theory, 51:4203– e 4215, 2005.</p>
<p>[7] S. Chen and D. Donoho. Examples of basis pursuit. In Proceedings of Wavelet Applications in Signal and Image Processing III, San Diego, CA, 1995.</p>
<p>[8] E. Greenshtein and Y. Ritov. Persistence in high-dimensional predictor selection and the virtue of overparametrization. Bernoulli, 10:971–988, 2004.</p>
<p>[9] A. Javanmard and A. Montanari. Conﬁdence Intervals and Hypothesis Testing for High-Dimensional Regression. arXiv:1306.3171, 2013.</p>
<p>[10] A. Javanmard and A. Montanari. Hypothesis testing in high-dimensional regression under the gaussian random design model: Asymptotic theory. arXiv:1301.4240, 2013.</p>
<p>[11] A. Javanmard and A. Montanari. Nearly Optimal Sample Size in Hypothesis Testing for HighDimensional Regression. arXiv:1311.0274, 2013.</p>
<p>[12] Y. Koren, R. Bell, and C. Volinsky. Matrix factorization techniques for recommender systems. Computer, 42(8):30–37, August 2009.</p>
<p>[13] E. Lehmann and G. Casella. Theory of point estimation. Springer, 2 edition, 1998.</p>
<p>[14] E. Lehmann and J. Romano. Testing statistical hypotheses. Springer, 2005.</p>
<p>[15] R. Lockhart, J. Taylor, R. Tibshirani, and R. Tibshirani. A signiﬁcance test for the lasso. arXiv preprint arXiv:1301.7161, 2013.</p>
<p>[16] M. Lustig, D. Donoho, J. Santos, and J. Pauly. Compressed sensing mri. IEEE Signal Processing Magazine, 25:72–82, 2008.</p>
<p>[17] N. Meinshausen and P. B¨ hlmann. High-dimensional graphs and variable selection with the lasso. u Ann. Statist., 34:1436–1462, 2006.</p>
<p>[18] N. Meinshausen and P. B¨ hlmann. Stability selection. J. R. Statist. Soc. B, 72:417–473, 2010. u</p>
<p>[19] J. Minnier, L. Tian, and T. Cai. A perturbation method for inference on regularized regression estimates. Journal of the American Statistical Association, 106(496), 2011.</p>
<p>[20] S. N. Negahban, P. Ravikumar, M. J. Wainwright, and B. Yu. A uniﬁed framework for high-dimensional analysis of m-estimators with decomposable regularizers. Statistical Science, 27(4):538–557, 2012.</p>
<p>[21] J. Peng, J. Zhu, A. Bergamaschi, W. Han, D.-Y. Noh, J. R. Pollack, and P. Wang. Regularized multivariate regression for identifying master predictors with application to integrative genomics study of breast cancer. The Annals of Applied Statistics, 4(1):53–77, 2010.</p>
<p>[22] M. Rudelson and S. Zhou. Reconstruction from anisotropic random measurements. IEEE Transactions on Information Theory, 59(6):3434–3447, 2013.</p>
<p>[23] T. Sun and C.-H. Zhang. Scaled sparse linear regression. Biometrika, 99(4):879–898, 2012.</p>
<p>[24] R. Tibshirani. Regression shrinkage and selection with the Lasso. J. Royal. Statist. Soc B, 58:267–288, 1996.</p>
<p>[25] S. van de Geer, P. B¨ hlmann, and Y. Ritov. On asymptotically optimal conﬁdence regions and tests for u high-dimensional models. arXiv:1303.0518, 2013.</p>
<p>[26] A. W. Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.</p>
<p>[27] M. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using quadratic programming. IEEE Trans. on Inform. Theory, 55:2183–2202, 2009.  1 -constrained</p>
<p>[28] L. Wasserman. All of statistics: a concise course in statistical inference. Springer Verlag, 2004.</p>
<p>[29] L. Wasserman and K. Roeder. High dimensional variable selection. Annals of statistics, 37(5A):2178, 2009.</p>
<p>[30] C.-H. Zhang and S. Zhang. Conﬁdence Intervals for Low-Dimensional Parameters in High-Dimensional Linear Models. arXiv:1110.2563, 2011.</p>
<p>[31] P. Zhao and B. Yu. On model selection consistency of Lasso. The Journal of Machine Learning Research, 7:2541–2563, 2006.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
