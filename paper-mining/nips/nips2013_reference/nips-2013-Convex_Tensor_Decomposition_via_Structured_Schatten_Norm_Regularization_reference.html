<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-74" href="../nips2013/nips-2013-Convex_Tensor_Decomposition_via_Structured_Schatten_Norm_Regularization.html">nips2013-74</a> <a title="nips-2013-74-reference" href="#">nips2013-74-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>74 nips-2013-Convex Tensor Decomposition via Structured Schatten Norm Regularization</h1>
<br/><p>Source: <a title="nips-2013-74-pdf" href="http://papers.nips.cc/paper/4985-convex-tensor-decomposition-via-structured-schatten-norm-regularization.pdf">pdf</a></p><p>Author: Ryota Tomioka, Taiji Suzuki</p><p>Abstract: We study a new class of structured Schatten norms for tensors that includes two recently proposed norms (“overlapped” and “latent”) for convex-optimizationbased tensor decomposition. We analyze the performance of “latent” approach for tensor decomposition, which was empirically found to perform better than the “overlapped” approach in some settings. We show theoretically that this is indeed the case. In particular, when the unknown true tensor is low-rank in a speciﬁc unknown mode, this approach performs as well as knowing the mode with the smallest rank. Along the way, we show a novel duality result for structured Schatten norms, which is also interesting in the general context of structured sparsity. We conﬁrm through numerical simulations that our theory can precisely predict the scaling behaviour of the mean squared error. 1</p><br/>
<h2>reference text</h2><p>[1] A. Agarwal, S. Negahban, and M. J. Wainwright. Noisy matrix decomposition via convex relaxation: Optimal rates in high dimensions. The Annals of Statistics, 40(2):1171–1197, 2012.</p>
<p>[2] F. Bach, R. Jenatton, J. Mairal, and G. Obozinski. Convex optimization with sparsity-inducing norms. In Optimization for Machine Learning. MIT Press, 2011.  8</p>
<p>[3] E. J. Candes, X. Li, Y. Ma, and J. Wright. Robust principal component analysis? arXiv:0912.3599, 2009.  Technical report,</p>
<p>[4] V. Chandrasekaran, B. Recht, P. Parrilo, and A. Willsky. The convex geometry of linear inverse problems, prepint. Technical report, arXiv:1012.0621v2, 2010.</p>
<p>[5] L. De Lathauwer, B. De Moor, and J. Vandewalle. A multilinear singular value decomposition. SIAM J. Matrix Anal. Appl., 21(4):1253–1278, 2000.</p>
<p>[6] L. De Lathauwer, B. De Moor, and J. Vandewalle. On the best rank-1 and rank-(R1 , R2 , . . . , RN ) approximation of higher-order tensors. SIAM J. Matrix Anal. Appl., 21(4):1324–1342, 2000.</p>
<p>[7] M. Fazel, H. Hindi, and S. P. Boyd. A Rank Minimization Heuristic with Application to Minimum Order System Approximation. In Proc. of the American Control Conference, 2001.</p>
<p>[8] R. Foygel and N. Srebro. Concentration-based guarantees for low-rank matrix reconstruction. Technical report, arXiv:1102.3923, 2011.</p>
<p>[9] S. Gandy, B. Recht, and I. Yamada. Tensor completion and low-n-rank tensor recovery via convex optimization. Inverse Problems, 27:025010, 2011.</p>
<p>[10] F. L. Hitchcock. The expression of a tensor or a polyadic as a sum of products. J. Math. Phys., 6(1): 164–189, 1927.</p>
<p>[11] D. Hsu, S. M. Kakade, and T. Zhang. Robust matrix decomposition with sparse corruptions. Information Theory, IEEE Transactions on, 57(11):7221–7234, 2011.</p>
<p>[12] A. Jalali, P. Ravikumar, S. Sanghavi, and C. Ruan. A dirty model for multi-task learning. In Advances in NIPS 23, pages 964–972. 2010.</p>
<p>[13] R. Jenatton, J. Audibert, and F. Bach. Structured variable selection with sparsity-inducing norms. J. Mach. Learn. Res., 12:2777–2824, 2011.</p>
<p>[14] T. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM Review, 51(3):455–500, 2009.</p>
<p>[15] J. Liu, P. Musialski, P. Wonka, and J. Ye. Tensor completion for estimating missing values in visual data. In Prof. ICCV, 2009.</p>
<p>[16] J. Mairal, R. Jenatton, G. Obozinski, and F. Bach. Convex and network ﬂow optimization for structured sparsity. J. Mach. Learn. Res., 12:2681–2720, 2011.</p>
<p>[17] A. Maurer and M. Pontil. Structured sparsity and generalization. Technical report, arXiv:1108.3476, 2011.</p>
<p>[18] M. Mørup. Applications of tensor (multiway array) factorizations and decompositions in data mining. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 1(1):24–40, 2011.</p>
<p>[19] C. Mu, B. Huang, J. Wright, and D. Goldfarb. Square deal: Lower bounds and improved relaxations for tensor recovery. arXiv preprint arXiv:1307.5870, 2013.</p>
<p>[20] S. Negahban, P. Ravikumar, M. Wainwright, and B. Yu. A uniﬁed framework for high-dimensional analysis of m-estimators with decomposable regularizers. In Advances in NIPS 22, pages 1348–1356. 2009.</p>
<p>[21] G. Obozinski, L. Jacob, and J.-P. Vert. Group lasso with overlaps: the latent group lasso approach. Technical report, arXiv:1110.0413, 2011.</p>
<p>[22] B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471–501, 2010.</p>
<p>[23] M. Signoretto, L. De Lathauwer, and J. Suykens. Nuclear norms for tensors and their use for convex multilinear estimation. Technical Report 10-186, ESAT-SISTA, K.U.Leuven, 2010.</p>
<p>[24] N. Srebro and A. Shraibman. Rank, trace-norm and max-norm. In Proc. of the 18th Annual Conference on Learning Theory (COLT), pages 545–560. Springer, 2005.</p>
<p>[25] R. Tomioka, K. Hayashi, and H. Kashima. Estimation of low-rank tensors via convex optimization. Technical report, arXiv:1010.0789, 2011.</p>
<p>[26] R. Tomioka, T. Suzuki, K. Hayashi, and H. Kashima. Statistical performance of convex tensor decomposition. In Advances in NIPS 24, pages 972–980. 2011.</p>
<p>[27] L. R. Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):279–311, 1966.</p>
<p>[28] R. Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv:1011.3027, 2010.  9  Technical report,</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
