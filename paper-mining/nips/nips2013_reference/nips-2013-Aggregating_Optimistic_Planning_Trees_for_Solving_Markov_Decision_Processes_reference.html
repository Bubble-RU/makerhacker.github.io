<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2013" href="../home/nips2013_home.html">nips2013</a> <a title="nips-2013-32" href="../nips2013/nips-2013-Aggregating_Optimistic_Planning_Trees_for_Solving_Markov_Decision_Processes.html">nips2013-32</a> <a title="nips-2013-32-reference" href="#">nips2013-32-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>32 nips-2013-Aggregating Optimistic Planning Trees for Solving Markov Decision Processes</h1>
<br/><p>Source: <a title="nips-2013-32-pdf" href="http://papers.nips.cc/paper/4973-aggregating-optimistic-planning-trees-for-solving-markov-decision-processes.pdf">pdf</a></p><p>Author: Gunnar Kedenburg, Raphael Fonteneau, Remi Munos</p><p>Abstract: This paper addresses the problem of online planning in Markov decision processes using a randomized simulator, under a budget constraint. We propose a new algorithm which is based on the construction of a forest of planning trees, where each tree corresponds to a random realization of the stochastic environment. The trees are constructed using a “safe” optimistic planning strategy combining the optimistic principle (in order to explore the most promising part of the search space ﬁrst) with a safety principle (which guarantees a certain amount of uniform exploration). In the decision-making step of the algorithm, the individual trees are aggregated and an immediate action is recommended. We provide a ﬁnite-sample analysis and discuss the trade-off between the principles of optimism and safety. We also report numerical results on a benchmark problem. Our algorithm performs as well as state-of-the-art optimistic planning algorithms, and better than a related algorithm which additionally assumes the knowledge of all transition distributions. 1</p><br/>
<h2>reference text</h2><p>[1] P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite time analysis of multiarmed bandit problems. Machine Learning, 47:235–256, 2002.</p>
<p>[2] L. Busoniu and R. Munos. Optimistic planning for Markov decision processes. In International Conference on Artiﬁcial Intelligence and Statistics (AISTATS), JMLR W & CP 22, pages 182–189, 2012.</p>
<p>[3] E. F. Camacho and C. Bordons. Model Predictive Control. Springer, 2004.</p>
<p>[4] R. Coulom. Efﬁcient selectivity and backup operators in Monte-Carlo tree search. Computers and Games, pages 72–83, 2007.</p>
<p>[5] B. Defourny, D. Ernst, and L. Wehenkel. Lazy planning under uncertainty by optimizing decisions on an ensemble of incomplete disturbance trees. In Recent Advances in Reinforcement Learning - European Workshop on Reinforcement Learning (EWRL), pages 1–14, 2008.</p>
<p>[6] R. Fonteneau, L. Busoniu, and R. Munos. Optimistic planning for belief-augmented Markov decision processes. In IEEE International Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), 2013.</p>
<p>[7] S. Gelly, Y. Wang, R. Munos, and O. Teytaud. Modiﬁcation of UCT with patterns in MonteCarlo go. Technical report, INRIA RR-6062, 2006.</p>
<p>[8] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of minimum cost paths. Systems Science and Cybernetics, IEEE Transactions on, 4(2):100–107, 1968.</p>
<p>[9] J. F. Hren and R. Munos. Optimistic planning of deterministic systems. Recent Advances in Reinforcement Learning, pages 151–164, 2008.</p>
<p>[10] J. E. Ingersoll. Theory of Financial Decision Making. Rowman and Littleﬁeld Publishers, Inc., 1987.</p>
<p>[11] M. Kearns, Y. Mansour, and A. Y. Ng. A sparse sampling algorithm for near-optimal planning in large Markov decision processes. Machine Learning, 49(2-3):193–208, 2002.</p>
<p>[12] L. Kocsis and C. Szepesvári. Bandit based Monte-Carlo planning. Machine Learning: ECML 2006, pages 282–293, 2006.</p>
<p>[13] R. Munos. From bandits to Monte-Carlo Tree Search: The optimistic principle applied to optimization and planning. To appear in Foundations and Trends in Machine Learning, 2013.</p>
<p>[14] S. A. Murphy. Optimal dynamic treatment regimes. Journal of the Royal Statistical Society, Series B, 65(2):331–366, 2003.</p>
<p>[15] J. Peters, S. Vijayakumar, and S. Schaal. Reinforcement learning for humanoid robotics. In IEEE-RAS International Conference on Humanoid Robots, pages 1–20, 2003.</p>
<p>[16] R. S. Sutton and A. G. Barto. Reinforcement Learning. MIT Press, 1998.</p>
<p>[17] T. J. Walsh, S. Goschin, and M. L. Littman. Integrating sample-based planning and model-based reinforcement learning. In AAAI Conference on Artiﬁcial Intelligence, 2010.  9</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
