<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>113 nips-2002-Information Diffusion Kernels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-113" href="../nips2002/nips-2002-Information_Diffusion_Kernels.html">nips2002-113</a> <a title="nips-2002-113-reference" href="#">nips2002-113-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>113 nips-2002-Information Diffusion Kernels</h1>
<br/><p>Source: <a title="nips-2002-113-pdf" href="http://papers.nips.cc/paper/2216-information-diffusion-kernels.pdf">pdf</a></p><p>Author: Guy Lebanon, John D. Lafferty</p><p>Abstract: A new family of kernels for statistical learning is introduced that exploits the geometric structure of statistical models. Based on the heat equation on the Riemannian manifold deﬁned by the Fisher information metric, information diffusion kernels generalize the Gaussian kernel of Euclidean space, and provide a natural way of combining generative statistical modeling with non-parametric discriminative learning. As a special case, the kernels give a new approach to applying kernel-based learning algorithms to discrete data. Bounds on covering numbers for the new kernels are proved using spectral theory in differential geometry, and experimental results are presented for text classiﬁcation.</p><br/>
<h2>reference text</h2><p>[1] S. Amari and H. Nagaoka. Methods of Information Geometry, volume 191 of Translations of Mathematical Monographs. American Mathematical Society, 2000.</p>
<p>[2] A. Grigor’yan and M. Noguchi. The heat kernel on hyperbolic space. Bulletin of the London Mathematical Society, 30:643–650, 1998.</p>
<p>[3] Y. Guo, P. L. Bartlett, J. Shawe-Taylor, and R. C. Williamson. Covering numbers for support vector machines. IEEE Trans. Information Theory, 48(1), January 2002.</p>
<p>[4] T. S. Jaakkola and D. Haussler. Exploiting generative models in discriminative classiﬁers. In Advances in Neural Information Processing Systems, volume 11, 1998.</p>
<p>[5] T. Joachims, N. Cristianini, and J. Shawe-Taylor. Composite kernels for hypertext categorisation. In Proceedings of the International Conference on Machine Learning (ICML), 2001.</p>
<p>[6] R. E. Kass and P. W. Vos. Geometrical Foundations of Asymptotic Inference. Wiley Series in Probability and Statistics. John Wiley & Sons, 1997.</p>
<p>[7] R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In Proceedings of the International Conference on Machine Learning (ICML), 2002.</p>
<p>[8] P. Li and S.-T. Yau. Estimates of eigenvalues of a compact Riemannian manifold. In Geometry of the Laplace Operator, volume 36 of Proceedings of Symposia in Pure Mathematics, pages 205–239, 1980.</p>
<p>[9] R. Schoen and S.-T. Yau. Lectures on Differential Geometry, volume 1 of Conference Proceedings and Lecture Notes in Geometry and Topology. International Press, 1994.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
