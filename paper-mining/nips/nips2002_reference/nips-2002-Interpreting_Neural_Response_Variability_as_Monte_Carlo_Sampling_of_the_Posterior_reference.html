<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-116" href="../nips2002/nips-2002-Interpreting_Neural_Response_Variability_as_Monte_Carlo_Sampling_of_the_Posterior.html">nips2002-116</a> <a title="nips-2002-116-reference" href="#">nips2002-116-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>116 nips-2002-Interpreting Neural Response Variability as Monte Carlo Sampling of the Posterior</h1>
<br/><p>Source: <a title="nips-2002-116-pdf" href="http://papers.nips.cc/paper/2152-interpreting-neural-response-variability-as-monte-carlo-sampling-of-the-posterior.pdf">pdf</a></p><p>Author: Patrik O. Hoyer, Aapo Hyvärinen</p><p>Abstract: The responses of cortical sensory neurons are notoriously variable, with the number of spikes evoked by identical stimuli varying signiﬁcantly from trial to trial. This variability is most often interpreted as ‘noise’, purely detrimental to the sensory system. In this paper, we propose an alternative view in which the variability is related to the uncertainty, about world parameters, which is inherent in the sensory stimulus. Speciﬁcally, the responses of a population of neurons are interpreted as stochastic samples from the posterior distribution in a latent variable model. In addition to giving theoretical arguments supporting such a representational scheme, we provide simulations suggesting how some aspects of response variability might be understood in this framework.</p><br/>
<h2>reference text</h2><p>[1] A. F. Dean. The variability of discharge of simple cells in the cat striate cortex. Experimental Brain Research, 44:437–440, 1981.</p>
<p>[2] D. J. Tolhurst, J. A. Movshon, and A. F. Dean. The statistical reliability of signals in single neurons in cat and monkey visual cortex. Vision Research, 23:775–785, 1983.</p>
<p>[3] A. J. Parker and W. T. Newsome. Sense and the single neuron: Probing the physiology of perception. Annual Review of Neuroscience, 21:227–277, 1998.</p>
<p>[4] G. R. Holt, W. R. Softky, C. Koch, and R. J. Douglas. Comparison of discharge variability in vitro and in vivo in cat visual cortex neurons. Journal of Neurophysiology, 75:1806–1814, 1996.</p>
<p>[5] R. Blake and N. K. Logothetis. Visual competition. Nature Reviews Neuroscience, 3:13–21, 2002.</p>
<p>[6] M. Rudolph and A. Destexhe. Do neocortical pyramidal neurons display stochastic resonance? Journal of Computational Neuroscience, 11:19–42, 2001.</p>
<p>[7] J. S. Anderson, I. Lampl, D. C. Gillespie, and D. Ferster. The contribution of noise to contrast invariance of orientation tuning in cat visual cortex. Science, 290:1968–1972, 2000.</p>
<p>[8] D. C. Knill and W. Richards, editors. Perception as Bayesian Inference. Cambridge University Press, 1996.</p>
<p>[9] R. P. N. Rao, B. A. Olshausen, and M. S. Lewicki, editors. Probabilistic Models of the Brain. MIT Press, 2002.</p>
<p>[10] D. Kersten and P. Schrater. Pattern inference theory: A probabilistic approach to vision. In R. Mausfeld and D. Heyer, editors, Perception and the Physical World. Wiley & Sons, 2002.</p>
<p>[11] P. Dayan. Recognition in hierarchical models. In F. Cucker and M. Shub, editors, Foundations of Computational Mathematics. Springer, Berlin, Germany, 1997.</p>
<p>[12] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by V1? Vision Research, 37:3311–3325, 1997.</p>
<p>[13] R. P. N. Rao and D. H. Ballard. Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive ﬁeld effects. Nature Neuroscience, 2(1):79–87, 1999.</p>
<p>[14] A. J. Bell and T. J. Sejnowski. The ‘independent components’ of natural scenes are edge ﬁlters. Vision Research, 37:3327–3338, 1997.</p>
<p>[15] R. S. Zemel, P. Dayan, and A. Pouget. Probabilistic interpretation of population codes. Neural Computation, 10(2):403–430, 1998.</p>
<p>[16] H. B. Barlow. Redundancy reduction revisited. Network: Computation in Neural Systems, 12:241–253, 2001.</p>
<p>[17] A. Hyv¨ rinen. Fast and robust ﬁxed-point algorithms for independent component analysis. a IEEE Trans. on Neural Networks, 10(3):626–634, 1999.</p>
<p>[18] P. O. Hoyer. Modeling receptive ﬁelds with non-negative sparse coding. In E. De Schutter, editor, Computational Neuroscience: Trends in Research 2003. Elsevier, Amsterdam, 2003. In press.</p>
<p>[19] N. K. Logothetis and J. D. Schall. Neuronal correlates of subjective visual perception. Science, 245:761–763, 1989.</p>
<p>[20] S. Geman and D. Geman. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6:721–741, 1984.  Appendix: MCMC sampling of the non-negative ICA posterior    ©  ¦ ¨ ¤  The posterior probability of , upon observing , is given by  (3)      ©§ ¨¦¥¡ £    ¡  %  ¡  £ ¤¡    !   ¢       $  ¦ ¨¡ ¦   ¡¡ ¦   ¤ ¢ ¦ ¤  §¢£ ¡ ¤ ¢ ¡ ¨ £¡   ¤ ¢  Taking the (natural) logarithm yields  ©   (4)      %    ¥  ¨ ©  % ¥    (   %  ¡ !  ¡  §  ¥ §     ¥ ¦ ¤¡ £  ¡  ¥ ¦         !  ¦ ¡ ¨ £¡   ¤  ¡ ¢   ©  ¢ ©  ¡ ¢   where is a vector of all ones. The crucial thing to note is that this function is quadratic in . Thus, the posterior distribution has the form of a gaussian, except that of course it is only deﬁned for non-negative . Rejection sampling might look tempting, but unfortunately does not work well in high dimensions. Thus, we will instead opt for a Markov Chain Monte Carlo approach. Implementing Gibbs sampling [20] is quite straightforward. The posterior distribution of , given and all other hidden variables , is a one-dimensional density that we will call cut-gaussian,         © ¨  ¨  % # §$</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
