<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-68" href="../nips2002/nips-2002-Discriminative_Densities_from_Maximum_Contrast_Estimation.html">nips2002-68</a> <a title="nips-2002-68-reference" href="#">nips2002-68-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>68 nips-2002-Discriminative Densities from Maximum Contrast Estimation</h1>
<br/><p>Source: <a title="nips-2002-68-pdf" href="http://papers.nips.cc/paper/2336-discriminative-densities-from-maximum-contrast-estimation.pdf">pdf</a></p><p>Author: Peter Meinicke, Thorsten Twellmann, Helge Ritter</p><p>Abstract: We propose a framework for classiﬁer design based on discriminative densities for representation of the differences of the class-conditional distributions in a way that is optimal for classiﬁcation. The densities are selected from a parametrized set by constrained maximization of some objective function which measures the average (bounded) difference, i.e. the contrast between discriminative densities. We show that maximization of the contrast is equivalent to minimization of an approximation of the Bayes risk. Therefore using suitable classes of probability density functions, the resulting maximum contrast classiﬁers (MCCs) can approximate the Bayes rule for the general multiclass case. In particular for a certain parametrization of the density functions we obtain MCCs which have the same functional form as the well-known Support Vector Machines (SVMs). We show that MCC-training in general requires some nonlinear optimization but under certain conditions the problem is concave and can be tackled by a single linear program. We indicate the close relation between SVM- and MCC-training and in particular we show that Linear Programming Machines can be viewed as an approximate realization of MCCs. In the experiments on benchmark data sets, the MCC shows a competitive classiﬁcation performance.</p><br/>
<h2>reference text</h2><p>[1] C. M. Bishop. Neural Networks for Pattern Recognition. Clarendon Press, Oxford, 1995.</p>
<p>[2] C. Cortes and V. Vapnik. Support-vector networks. Machine Learning, 20(3):273–297, 1995.</p>
<p>[3] R. O. Duda and P. E. Hart. Pattern Classiﬁcation and Scene Analysis. Wiley, New York, 1973.</p>
<p>[4] T. Graepel, R. Herbrich, B. Scholkopf, A. Smola, P. Bartlett, K. Robert-Muller, K. Obermayer, and B. Williamson. Classiﬁcation on proximity data with lp–machines, 1999.</p>
<p>[5] S.S. Keerthi, S.K. Shevade, C. Bhattacharyya, and K.R.K. Murthy. Improvements to platt’s SMO algorithm for SVM classiﬁer design. Technical report, Dept of CSA, IISc, Bangalore, India, 1999.</p>
<p>[6] P. Meinicke, T. Twellmann, and H. Ritter. Maximum contrast classiﬁers. In Proc. of the Int. Conf. on Artiﬁcial Neural Networks, Berlin, 2002. Springer. in press.</p>
<p>[7] J. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods — Support o Vector Learning, pages 185–208, Cambridge, MA, 1999. MIT Press.</p>
<p>[8] G. R¨ tsch, T. Onoda, and K.-R. M¨ ller. Soft margins for AdaBoost. Technical Report NC-TRa u 1998-021, Department of Computer Science, Royal Holloway, University of London, Egham, UK, August 1998. Submitted to Machine Learning.</p>
<p>[9] B. D. Ripley. Pattern Recognition and Neural Networks. Cambridge University Press, Cambridge, 1996.</p>
<p>[10] B. Sch¨ lkopf and A. J. Smola. Learning with Kernels. MIT Press, 2002. o</p>
<p>[11] D. W. Scott. Multivariate Density Estimation. Wiley, 1992.</p>
<p>[12] V. N. Vapnik. The Nature of Statistical Learning Theory. Springer, New York, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
