<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>138 nips-2002-Manifold Parzen Windows</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-138" href="../nips2002/nips-2002-Manifold_Parzen_Windows.html">nips2002-138</a> <a title="nips-2002-138-reference" href="#">nips2002-138-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>138 nips-2002-Manifold Parzen Windows</h1>
<br/><p>Source: <a title="nips-2002-138-pdf" href="http://papers.nips.cc/paper/2203-manifold-parzen-windows.pdf">pdf</a></p><p>Author: Pascal Vincent, Yoshua Bengio</p><p>Abstract: The similarity between objects is a fundamental element of many learning algorithms. Most non-parametric methods take this similarity to be ﬁxed, but much recent work has shown the advantages of learning it, in particular to exploit the local invariances in the data or to capture the possibly non-linear manifold on which most of the data lies. We propose a new non-parametric kernel density estimation method which captures the local structure of an underlying manifold through the leading eigenvectors of regularized local covariance matrices. Experiments in density estimation show signiﬁcant improvements with respect to Parzen density estimators. The density estimators can also be used within Bayes classiﬁers, yielding classiﬁcation rates similar to SVMs and much superior to the Parzen classiﬁer.</p><br/>
<h2>reference text</h2><p>[1] P. Vincent and Y. Bengio. K-local hyperplane and convex distance nearest neighbor algorithms. In T.G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems, volume 14. The MIT Press, 2002.</p>
<p>[2] E. Parzen. On the estimation of a probability density function and mode. Annals of Mathematical Statistics, 33:1064–1076, 1962.</p>
<p>[3] G.E. Hinton, M. Revow, and P. Dayan. Recognizing handwritten digits using mixtures of linear models. In G. Tesauro, D.S. Touretzky, and T.K. Leen, editors, Advances in Neural Information Processing Systems 7, pages 1015–1022. MIT Press, Cambridge, MA, 1995.</p>
<p>[4] M.E. Tipping and C.M. Bishop. Mixtures of probabilistic principal component analysers. Neural Computation, 11(2):443–482, 1999.</p>
<p>[5] Z. Ghahramani and G.E. Hinton. The EM algorithm for mixtures of factor analyzers. Technical Report CRG-TR-96-1, Dpt. of Comp. Sci., Univ. of Toronto, 21 1996.</p>
<p>[6] Z. Ghahramani and M. J. Beal. Variational inference for Bayesian mixtures of factor analysers. In Advances in Neural Information Processing Systems 12, Cambridge, MA, 2000. MIT Press.</p>
<p>[7] P. Y. Simard, Y. A. LeCun, J. S. Denker, and B. Victorri. Transformation invariance in pattern recognition — tangent distance and tangent propagation. Lecture Notes in Computer Science, 1524, 1998.</p>
<p>[8] D. Keysers, J. Dahmen, and H. Ney. A probabilistic view on tangent distance. In 22nd Symposium of the German Association for Pattern Recognition, Kiel, Germany, 2000.</p>
<p>[9] J. Dahmen, D. Keysers, M. Pitz, and H. Ney. Structured covariance matrices for statistical image object recognition. In 22nd Symposium of the German Association for Pattern Recognition, Kiel, Germany, 2000.</p>
<p>[10] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, Dec. 2000.</p>
<p>[11] Y. Whye Teh and S. Roweis. Automatic alignment of local representations. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems, volume 15. The MIT Press, 2003.</p>
<p>[12] V. de Silva and J.B. Tenenbaum. Global versus local approaches to nonlinear dimensionality reduction. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems, volume 15. The MIT Press, 2003.</p>
<p>[13] M. Brand. Charting a manifold. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems, volume 15. The MIT Press, 2003.</p>
<p>[14] R. D. Short and K. Fukunaga. The optimal distance measure for nearest neighbor classiﬁcation. IEEE Transactions on Information Theory, 27:622–627, 1981.</p>
<p>[15] J. Myles and D. Hand. The multi-class measure problem in nearest neighbour discrimination rules. Pattern Recognition, 23:1291–1297, 1990.</p>
<p>[16] J. Friedman. Flexible metric nearest neighbor classiﬁcation. Technical Report 113, Stanford University Statistics Department, 1994.</p>
<p>[17] T. Hastie and R. Tibshirani. Discriminant adaptive nearest neighbor classiﬁcation and regression. In David S. Touretzky, Michael C. Mozer, and Michael E. Hasselmo, editors, Advances in Neural Information Processing Systems, volume 8, pages 409–415. The MIT Press, 1996.</p>
<p>[18] A.J. Inzenman. Recent developments in nonparametric density estimation. Journal of the American Statistical Association, 86(413):205–224, 1991.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
