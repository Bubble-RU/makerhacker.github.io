<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>179 nips-2002-Scaling of Probability-Based Optimization Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-179" href="../nips2002/nips-2002-Scaling_of_Probability-Based_Optimization_Algorithms.html">nips2002-179</a> <a title="nips-2002-179-reference" href="#">nips2002-179-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>179 nips-2002-Scaling of Probability-Based Optimization Algorithms</h1>
<br/><p>Source: <a title="nips-2002-179-pdf" href="http://papers.nips.cc/paper/2138-scaling-of-probability-based-optimization-algorithms.pdf">pdf</a></p><p>Author: J. L. Shapiro</p><p>Abstract: Population-based Incremental Learning is shown require very sensitive scaling of its learning rate. The learning rate must scale with the system size in a problem-dependent way. This is shown in two problems: the needle-in-a haystack, in which the learning rate must vanish exponentially in the system size, and in a smooth function in which the learning rate must vanish like the square root of the system size. Two methods are proposed for removing this sensitivity. A learning dynamics which obeys detailed balance is shown to give consistent performance over the entire range of learning rates. An analog of mutation is shown to require a learning rate which scales as the inverse system size, but is problem independent. 1</p><br/>
<h2>reference text</h2><p>[1] S. Baluja. Population-based incremental learning: A method for integrating genetic search based function optimization and competive learning. Technical Report CMUCS-94-163 , Computer Science Department , Carnegie Mellon University, 1994.</p>
<p>[2] A. Johnson and J. L. Shapiro. The importance of selection mechanisms in distribution estimation algorithms. In Proceedings of the 5th International Conference on Artificial Evolution AE01, 2001.</p>
<p>[3] P. Larraiiaga and J. A. Lozano. Estimation of Distribution Algorithms, A New Tool for Evolutionary Computation. Kluwer Academic Publishers, 2001.</p>
<p>[4] Eckhard Limpert , Werner A. Stahel, and Markus Abbt . Log-normal distributions across the sciences: Keys and clues. BioScience, 51(5):341-352, 2001.</p>
<p>[5] H. Miihlenbein. The equation for response to selection and its use for prediction. Evolutionary Computation, 5(3):303- 346, 1997.</p>
<p>[6] M. Pelikan, D. E . Goldberg, and F. Lobo. A survey of optimization by building and using probabilistic models. Technical report, University of Illinois at UrbanaChampaign, Illinois Genetic Algorithms Laboratory, 1999.</p>
<p>[7] Jonathan L. Shapiro and Adam Priigel-Bennett. Maximum entropy analysis of genetic algorithm operators. Lecture Notes in Computer Science, 993:14- 24, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
