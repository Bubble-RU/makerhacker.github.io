<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>21 nips-2002-Adaptive Classification by Variational Kalman Filtering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-21" href="../nips2002/nips-2002-Adaptive_Classification_by_Variational_Kalman_Filtering.html">nips2002-21</a> <a title="nips-2002-21-reference" href="#">nips2002-21-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>21 nips-2002-Adaptive Classification by Variational Kalman Filtering</h1>
<br/><p>Source: <a title="nips-2002-21-pdf" href="http://papers.nips.cc/paper/2227-adaptive-classification-by-variational-kalman-filtering.pdf">pdf</a></p><p>Author: Peter Sykacek, Stephen J. Roberts</p><p>Abstract: We propose in this paper a probabilistic approach for adaptive inference of generalized nonlinear classiﬁcation that combines the computational advantage of a parametric solution with the ﬂexibility of sequential sampling techniques. We regard the parameters of the classiﬁer as latent states in a ﬁrst order Markov process and propose an algorithm which can be regarded as variational generalization of standard Kalman ﬁltering. The variational Kalman ﬁlter is based on two novel lower bounds that enable us to use a non-degenerate distribution over the adaptation rate. An extensive empirical evaluation demonstrates that the proposed method is capable of infering competitive classiﬁers both in stationary and non-stationary environments. Although we focus on classiﬁcation, the algorithm is easily extended to other generalized nonlinear models.</p><br/>
<h2>reference text</h2><p>[1] S.-I. Amari. A theory of adaptive pattern classiﬁers. IEEE Transactions on Electronic Computers, 16:299–307, 1967.</p>
<p>[2] H. Attias. Inferring parameters and structure of latent variable models by variational Bayes. In Proc. 15th Conf. on Uncertainty in AI, 1999, 1999.</p>
<p>[3] J. O. Berger. Statistical Decision Theory and Bayesian Analysis. Springer, New York, 1985.</p>
<p>[4] C.L. Blake and C.J. Merz. UCI repository of machine learning databases. http://www.ics.uci.edu/ mlearn/MLRepository.html, 1998. University of California, Irvine, Dept. of Information and Computer Sciences.</p>
<p>[5] D. S. Broomhead and D. Lowe. Multivariable functional interpolation and adaptive networks. Complex Systems, 2:321–355, 1988.</p>
<p>[6] J.F.G. de Freitas, M. Niranjan, and A.H. Gee. Regularisation in Sequential Learning Algorithms. In M. Jordan, M. Kearns, and S. Solla, editors, Advances in Neural Information Processing Systems (NIPS 10), pages 458–464, 1998.</p>
<p>[7] A. Doucet, J. F. G. de Freitas, and N. Gordon, editors. Sequential Monte Carlo Methods in Practice. Springer-Verlag, 2001.</p>
<p>[8] Z. Ghahramani and M. J. Beal. Variational inference for Bayesian mixture of factor analysers. In Advances in Neural Information Processing Systems 12, pages 449–455, 2000.</p>
<p>[9] T. S. Jaakkola and M. I. Jordan. Bayesian parameter estimation via variational methods. Statistics and Computing, 10:25–37, 2000.</p>
<p>[10] A.H. Jazwinski. Adaptive ﬁltering. Automatica, pages 475–485, 1969.</p>
<p>[11] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. In M. I. Jordan, editor, Learning in Graphical Models. MIT Press, Cambridge, MA, 1999.</p>
<p>[12] R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Trans. ASME, J. Basic Eng., 82:35–45, 1960.</p>
<p>[13] N. Metropolis, A. Rosenbluth, M. Rosenbluth, A. Teller, and E. Teller. Equations of state calculations by fast computing machines. Journal of Chemical Physics, 21:1087–1091, 1953.</p>
<p>[14] J. Moody and C. J. Darken. Fast learning in networks of locally-tuned processing units. Neural Computation, 1:281–294, 1989.</p>
<p>[15] W. Penny, S. Roberts, E. Curran, and M. Stokes. EEG-based communication: a pattern recognition approach. IEEE Trans. Rehab. Eng., pages 214–216, 2000.</p>
<p>[16] B. D. Ripley. Pattern Recognition and Neural Networks. Cambridge University Press, Cambridge, 1996.</p>
<p>[17] Masa-aki Sato. Online model selection based on the variational Bayes. Neural Computation, pages 1649–1681, 2001.</p>
<p>[18] P. Sykacek and S. Roberts. Bayesian time series classiﬁcation. In T.G. Dietterich, S. Becker, and Z. Gharamani, editors, Advances in Neural Processing Systems 14, pages 937–944. MIT Press, 2002.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
