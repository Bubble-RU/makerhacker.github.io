<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>130 nips-2002-Learning in Zero-Sum Team Markov Games Using Factored Value Functions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-130" href="../nips2002/nips-2002-Learning_in_Zero-Sum_Team_Markov_Games_Using_Factored_Value_Functions.html">nips2002-130</a> <a title="nips-2002-130-reference" href="#">nips2002-130-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>130 nips-2002-Learning in Zero-Sum Team Markov Games Using Factored Value Functions</h1>
<br/><p>Source: <a title="nips-2002-130-pdf" href="http://papers.nips.cc/paper/2228-learning-in-zero-sum-team-markov-games-using-factored-value-functions.pdf">pdf</a></p><p>Author: Michail G. Lagoudakis, Ronald Parr</p><p>Abstract: We present a new method for learning good strategies in zero-sum Markov games in which each side is composed of multiple agents collaborating against an opposing team of agents. Our method requires full observability and communication during learning, but the learned policies can be executed in a distributed manner. The value function is represented as a factored linear architecture and its structure determines the necessary computational resources and communication bandwidth. This approach permits a tradeoff between simple representations with little or no communication between agents and complex, computationally intensive representations with extensive coordination between agents. Thus, we provide a principled means of using approximation to combat the exponential blowup in the joint action space of the participants. The approach is demonstrated with an example that shows the efﬁciency gains over naive enumeration.</p><br/>
<h2>reference text</h2><p>[1] R. Dechter. Bucket elimination: A unifying framework for reasoning. Artiﬁcial Intelligence, 113(1–2):41–85, 1999.</p>
<p>[2] Carlos Guestrin, Daphne Koller, and Ronald Parr. Multiagent planning with factored MDPs. In Proceeding of the 14th Neural Information Processing Systems (NIPS-14), pages 1523–1530, Vancouver, Canada, December 2001.</p>
<p>[3] Carlos Guestrin, Daphne Koller, and Ronald Parr. Solving factored POMDPs with linear value functions. In IJCAI-01 workshop on Planning under Uncertainty and Incomplete Information, 2001.</p>
<p>[4] Carlos Guestrin, Michail G. Lagoudakis, and Ronald Parr. Coordinated reinforcement learning. In Proceedings of the 19th International Conference on Machine Learning (ICML-02), pages 227–234, Sydney, Australia, July 2002.</p>
<p>[5] Michail Lagoudakis and Ronald Parr. Model free least squares policy iteration. In Proceedings of the 14th Neural Information Processing Systems (NIPS-14), pages 1547–1554, Vancouver, Canada, December 2001.</p>
<p>[6] Michail Lagoudakis and Ronald Parr. Value function approximation in zero sum Markov games. In Proceedings of the 18th Conference on Uncertainty in Artiﬁcial Intelligence (UAI 2002), pages 283–292, Edmonton, Canada, 2002.</p>
<p>[7] Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Proceedings of the 11th International Conference on Machine Learning (ICML-94), pages 157– 163, San Francisco, CA, 1994. Morgan Kaufmann.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
