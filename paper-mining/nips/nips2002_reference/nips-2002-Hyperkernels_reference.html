<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>106 nips-2002-Hyperkernels</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-106" href="../nips2002/nips-2002-Hyperkernels.html">nips2002-106</a> <a title="nips-2002-106-reference" href="#">nips2002-106-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>106 nips-2002-Hyperkernels</h1>
<br/><p>Source: <a title="nips-2002-106-pdf" href="http://papers.nips.cc/paper/2193-hyperkernels.pdf">pdf</a></p><p>Author: Cheng S. Ong, Robert C. Williamson, Alex J. Smola</p><p>Abstract: We consider the problem of choosing a kernel suitable for estimation using a Gaussian Process estimator or a Support Vector Machine. A novel solution is presented which involves deﬁning a Reproducing Kernel Hilbert Space on the space of kernels itself. By utilizing an analog of the classical representer theorem, the problem of choosing a kernel from a parameterized family of kernels (e.g. of varying width) is reduced to a statistical estimation problem akin to the problem of minimizing a regularized risk functional. Various classical settings for model or kernel selection are special cases of our framework.</p><br/>
<h2>reference text</h2><p>[1] G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. Jordan. Learning the kernel matrix with semideﬁnite programming. In ICML. Morgan Kaufmann, 2002.</p>
<p>[2] C. K. I. Williams. Prediction with Gaussian processes: From linear regression to linear prediction and beyond. In M. I. Jordan, editor, Learning and Inference in Graphical Models. Kluwer Academic, 1998.</p>
<p>[3] O. Chapelle, V. Vapnik, O. Bousquet, and S. Mukherjee. Choosing kernel parameters for support vector machines. Machine Learning, 2002. Forthcoming.</p>
<p>[4] G. Wahba. Spline Models for Observational Data, volume 59 of CBMS-NSF Regional Conference Series in Applied Mathematics. SIAM, Philadelphia, 1990.</p>
<p>[5] K. Crammer, J. Keshet, and Y. Singer. Kernel design using boosting. In Advances in Neural Information Processing Systems 15, 2002. In press.</p>
<p>[6] O. Bousquet and D. Herrmann. On the complexity of learning the kernel matrix. In Advances in Neural Information Processing Systems 15, 2002. In press.</p>
<p>[7] N. Cristianini, A. Elisseeff, and J. Shawe-Taylor. On optimizing kernel alignment. Technical Report NC2-TR-2001-087, NeuroCOLT, http://www.neurocolt.com, 2001.</p>
<p>[8] B. Sch¨ lkopf and A. J. Smola. Learning with Kernels. MIT Press, 2002. o</p>
<p>[9] S. Fine and K. Scheinberg. Efﬁcient SVM training using low-rank kernel representation. Technical report, IBM Watson Research Center, New York, 2000.</p>
<p>[10] Y. Freund and R. E. Schapire. Experiments with a new boosting algorithm. In ICML, pages 148–146. Morgan Kaufmann Publishers, 1996.</p>
<p>[11] G. R¨ tsch, T. Onoda, and K. R. M¨ ller. Soft margins for adaboost. Machine Learning, a u 42(3):287–320, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
