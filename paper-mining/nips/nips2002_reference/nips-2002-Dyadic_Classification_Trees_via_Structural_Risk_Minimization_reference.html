<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-72" href="../nips2002/nips-2002-Dyadic_Classification_Trees_via_Structural_Risk_Minimization.html">nips2002-72</a> <a title="nips-2002-72-reference" href="#">nips2002-72-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>72 nips-2002-Dyadic Classification Trees via Structural Risk Minimization</h1>
<br/><p>Source: <a title="nips-2002-72-pdf" href="http://papers.nips.cc/paper/2198-dyadic-classification-trees-via-structural-risk-minimization.pdf">pdf</a></p><p>Author: Clayton Scott, Robert Nowak</p><p>Abstract: Classiﬁcation trees are one of the most popular types of classiﬁers, with ease of implementation and interpretation being among their attractive features. Despite the widespread use of classiﬁcation trees, theoretical analysis of their performance is scarce. In this paper, we show that a new family of classiﬁcation trees, called dyadic classiﬁcation trees (DCTs), are near optimal (in a minimax sense) for a very broad range of classiﬁcation problems. This demonstrates that other schemes (e.g., neural networks, support vector machines) cannot perform signiﬁcantly better than DCTs in many cases. We also show that this near optimal performance is attained with linear (in the number of training data) complexity growing and pruning algorithms. Moreover, the performance of DCTs on benchmark datasets compares favorably to that of standard CART, which is generally more computationally intensive and which does not possess similar near optimality properties. Our analysis stems from theoretical results on structural risk minimization, on which the pruning rule for DCTs is based.</p><br/>
<h2>reference text</h2><p>[1] L. Breiman, J. Friedman, R. Olshen, and C. Stone, Wadsworth, Belmont, CA, 1984.  Classiﬁcation and Regression Trees,</p>
<p>[2] V. Vapnik, Estimation of Dependencies Based on Empirical Data, Springer-Verlag, New York, 1982.</p>
<p>[3] G. Lugosi and K. Zeger, “Concept learning using complexity regularization,” IEEE Transactions on Information Theory, vol. 42, no. 1, pp. 48–54, 1996.</p>
<p>[4] L. Devroye, L. Gy¨ rﬁ, and G. Lugosi, A Probabilistic Theory of Pattern Recognition, Springer, o New York, 1996.</p>
<p>[5] C. Scott and R. Nowak, “Complexity-regularized dyadic classiﬁcation trees: Efﬁcient pruning and rates of convergence,” Tech. Rep. TREE0201, Rice University, 2002, available at http://www.dsp.rice.edu/ cscott.</p>
<p>[6] A. Nobel, “Analysis of a complexity based pruning scheme for classiﬁcation trees,” IEEE Transactions on Information Theory, vol. 48, no. 8, pp. 2362–2368, 2002.</p>
<p>[7] A. B. Tsybakov, “Optimal aggregation of classiﬁers in statistical learning,” preprint, 2001, available at http://www.proba.jussieu.fr/mathdoc/preprints/.</p>
<p>[8] K. Falconer, Fractal Geometry: Mathematical Foundations and Applications, Wiley, West Sussex, England, 1990.</p>
<p>[9] J. S. Marron, “Optimal rates of convergence to Bayes risk in nonparametric discrimination,” Annals of Statistics, vol. 11, no. 4, pp. 1142–1155, 1983.</p>
<p>[10] Y. Yang, “Minimax nonparametric classiﬁcation–Part I: Rates of convergence,” IEEE Transactions on Information Theory, vol. 45, no. 7, pp. 2271–2284, 1999.</p>
<p>[11] E. Mammen and A. B. Tsybakov, “Smooth discrimination analysis,” Annals of Statistics, vol. 27, pp. 1808–1829, 1999.</p>
<p>[12] P. Chou, T. Lookabaugh, and R. Gray, “Optimal pruning with applications to tree-structured source coding and modeling,” IEEE Transactions on Information Theory, vol. 35, no. 2, pp. 299–315, 1989.</p>
<p>[13] B. Ripley, Pattern Recognition and Neural Networks, Cambridge University Press, Cambridge, UK, 1996.</p>
<p>[14] R. Quinlan, C4.5: Programs for Machine Learning, Morgan Kaufmann, San Mateo, 1993.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
