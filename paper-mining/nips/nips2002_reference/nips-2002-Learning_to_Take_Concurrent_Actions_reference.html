<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>134 nips-2002-Learning to Take Concurrent Actions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-134" href="../nips2002/nips-2002-Learning_to_Take_Concurrent_Actions.html">nips2002-134</a> <a title="nips-2002-134-reference" href="#">nips2002-134-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>134 nips-2002-Learning to Take Concurrent Actions</h1>
<br/><p>Source: <a title="nips-2002-134-pdf" href="http://papers.nips.cc/paper/2204-learning-to-take-concurrent-actions.pdf">pdf</a></p><p>Author: Khashayar Rohanimanesh, Sridhar Mahadevan</p><p>Abstract: We investigate a general semi-Markov Decision Process (SMDP) framework for modeling concurrent decision making, where agents learn optimal plans over concurrent temporally extended actions. We introduce three types of parallel termination schemes – all, any and continue – and theoretically and experimentally compare them. 1</p><br/>
<h2>reference text</h2><p>[1] Craig Boutilier and Ronen Brafman. Planning with concurrent interacting actions. In Proceedings of the Fourteenth National Conference on Artiﬁcial Intelligence (AAAI ’97), 1997.</p>
<p>[2] P. Cichosz. Learning multidimensional control actions from delayed reinforcements. In Eighth International Symposium on System-Modelling-Control (SMC-8), Zakopane, Poland, 1995.</p>
<p>[3] C. A. Knoblock. Generating parallel execution plans with a partial-order planner. In Proceedings of the Second International Conference on Artiﬁcial Intelligence Planning Systems , Chicago, IL, 1994., 1994.</p>
<p>[4] Ray Reiter. Natural actions, concurrency and continuous time in the situation calculus. Principles of Knowledge Representation and Reasoning: Proceedings of the Fifth International Conference (KR’96), Cambridge MA., November 5-8, 1996, 1996.</p>
<p>[5] Khashayar Rohanimanesh and Sridhar Mahadevan. Decision-theoretic planning with concurrent temporally extended actions. In Proceedings of the 17th Conference on Uncertainty in Artiﬁcial Intelligence, 2001.</p>
<p>[6] S. Singh and David Cohn. How to dynamically merge markov decision processes. Proceedings of NIPS 11, 1998.</p>
<p>[7] R. Sutton, D. Precup, and S. Singh. Between MDPs and Semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artiﬁcial Intelligence, pages 181–211, 1999.</p>
<p>[8] Glynn Winskel. Topics in concurrency: Part ii comp. sci. lecture notes. Computer Science course at the University of Cambridge, 2002.  500000</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
