<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-182" href="../nips2002/nips-2002-Shape_Recipes%3A_Scene_Representations_that_Refer_to_the_Image.html">nips2002-182</a> <a title="nips-2002-182-reference" href="#">nips2002-182-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>182 nips-2002-Shape Recipes: Scene Representations that Refer to the Image</h1>
<br/><p>Source: <a title="nips-2002-182-pdf" href="http://papers.nips.cc/paper/2268-shape-recipes-scene-representations-that-refer-to-the-image.pdf">pdf</a></p><p>Author: William T. Freeman, Antonio Torralba</p><p>Abstract: The goal of low-level vision is to estimate an underlying scene, given an observed image. Real-world scenes (eg, albedos or shapes) can be very complex, conventionally requiring high dimensional representations which are hard to estimate and store. We propose a low-dimensional representation, called a scene recipe, that relies on the image itself to describe the complex scene conﬁgurations. Shape recipes are an example: these are the regression coefﬁcients that predict the bandpassed shape from image data. We describe the beneﬁts of this representation, and show two uses illustrating their properties: (1) we improve stereo shape estimates by learning shape recipes at low resolution and applying them at full resolution; (2) Shape recipes implicitly contain information about lighting and materials and we use them for material segmentation.</p><br/>
<h2>reference text</h2><p>[1] E. H. Adelson. Lightness perception and lightness illusions. In M. Gazzaniga, editor, The New Cognitive Neurosciences, pages 339–351. MIT Press, 2000.</p>
<p>[2] C. M. Bishop. Neural networks for pattern recognition. Oxford, 1995.</p>
<p>[3] A. Gilchrist et al. An anchoring theory of lightness. Psychological Review, 106(4):795–834, 1999.</p>
<p>[4] W. T. Freeman. The generic viewpoint assumption in a framework for visual perception. Nature, 368(6471):542–545, April 7 1994.</p>
<p>[5] B. K. P. Horn and M. J. Brooks, editors. Shape from shading. The MIT Press, Cambridge, MA, 1989.</p>
<p>[6] T. Leung and J. Malik. Representing and recognizing the visual appearance of materials using three-dimensional textons. Intl. J. Comp. Vis., 43(1):29–44, 2001.</p>
<p>[7] A. P. Pentland. Linear shape from shading. Intl. J. Comp. Vis., 1(4):153–162, 1990.</p>
<p>[8] M. Pollefeys, R. Koch, and L. V. Gool. A simple and efﬁcient rectiﬁcation method for general motion. In Intl. Conf. on Computer Vision (ICCV), pages 496–501, 1999.</p>
<p>[9] R. A. Rensink. The dynamic representation of scenes. Vis. Cognition, 7:17–42, 2000.</p>
<p>[10] S. Sclaroff and A. Pentland. Generalized implicit functions for computer graphics. In Proc. SIGGRAPH 91, volume 25, pages 247–250, 1991. In Computer Graphics, Annual Conference Series.</p>
<p>[11] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000.</p>
<p>[12] E. P. Simoncelli. Statistical models for images: Compression, restoration and synthesis. In 31st Asilomar Conf. on Sig., Sys. and Computers, Paciﬁc Grove, CA, 1997.</p>
<p>[13] E. P. Simoncelli and W. T. Freeman. The steerable pyramid: a ﬂexible architecture for multi-scale derivative computation. In 2nd Annual Intl. Conf. on Image Processing, Washington, DC, 1995. IEEE.</p>
<p>[14] R. Szeliski. Bayesian modeling of uncertainty in low-level vision. Intl. J. Comp. Vis., 5(3):271–301, 1990.</p>
<p>[15] M. F. Tappen, W. T. Freeman, and E. H. Adelson. Recovering intrinsic images from a single image. In Adv. in Neural Info. Proc. Systems, volume 15. MIT Press, 2003.</p>
<p>[16] A. Torralba and W. T. Freeman. Properties and applications of shape recipes. Technical Report AIM-2002-019, MIT AI lab, 2002.</p>
<p>[17] Y. Weiss. Bayesian motion estimation and segmentation. PhD thesis, M.I.T., 1998.</p>
<p>[18] Z. Zhang. Determining the epipolar geometry and its uncertainty: A review. Technical Report 2927, Sophia-Antipolis Cedex, France, 1996. see http://wwwsop.inria.fr/robotvis/demo/f-http/html/.</p>
<p>[19] C. L. Zitnick and T. Kanade. A cooperative algorithm for stereo matching and occlusion detection. IEEE Pattern Analysis and Machine Intelligence, 22(7), July 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
