<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>129 nips-2002-Learning in Spiking Neural Assemblies</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-129" href="../nips2002/nips-2002-Learning_in_Spiking_Neural_Assemblies.html">nips2002-129</a> <a title="nips-2002-129-reference" href="#">nips2002-129-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>129 nips-2002-Learning in Spiking Neural Assemblies</h1>
<br/><p>Source: <a title="nips-2002-129-pdf" href="http://papers.nips.cc/paper/2330-learning-in-spiking-neural-assemblies.pdf">pdf</a></p><p>Author: David Barber</p><p>Abstract: We consider a statistical framework for learning in a class of networks of spiking neurons. Our aim is to show how optimal local learning rules can be readily derived once the neural dynamics and desired functionality of the neural assembly have been speciﬁed, in contrast to other models which assume (sub-optimal) learning rules. Within this framework we derive local rules for learning temporal sequences in a model of spiking neurons and demonstrate its superior performance to correlation (Hebbian) based approaches. We further show how to include mechanisms such as synaptic depression and outline how the framework is readily extensible to learning in networks of highly complex spiking neurons. A stochastic quantal vesicle release mechanism is considered and implications on the complexity of learning discussed. 1</p><br/>
<h2>reference text</h2><p>[1] L.F. Abbott, J.A. Varela, K. Sen, and S.B. Nelson, Synaptic depression and cortical gain control, Science 275 (1997), 220–223.</p>
<p>[2] D. Barber, Dynamic Bayesian Networks with Deterministic Latent Tables, Neural Information Processing Systems (2003).</p>
<p>[3] D. Barber and F. Agakov, Correlated sequence learning in a network of spiking neurons using maximum likelihood, Tech. Report EDI-INF-RR-0149, School of Informatics, 5 Forrest Hill, Edinburgh, UK, 2002.</p>
<p>[4] C. Chrisodoulou, G. Bugmann, and T.G. Clarkson, A Spiking Neuron Model : Applications and Learning, Neural Networks 15 (2002), 891–908.</p>
<p>[5] A. D¨ring, A.C.C. Coolen, and D. Sherrington, Phase diagram and storage capacity u of sequence processing neural networks, Journal of Physics A 31 (1998), 8607–8621.</p>
<p>[6] W. Gerstner, R. Ritz, and J.L. van Hemmen, Why Spikes? Hebbian Learning and retrieval of time-resolved excitation patterns, Biological Cybernetics 69 (1993), 503– 515.</p>
<p>[7] M. I. Jordan, Learning in Graphical Models, MIT Press, 1998.</p>
<p>[8] R. Kempter, W. Gerstner, and J.L. van Hemmen, Hebbian learning and spiking neurons, Physical Review E 59 (1999), 4498–4514.</p>
<p>[9] C. Koch, Biophysics of Computation, Oxford University Press, 1998.</p>
<p>[10] W. Maass and C. Bishop, Pulsed Neural Networks, MIT Press, 2001.</p>
<p>[11] H. Markram, J. Lubke, M. Frotscher, and B. Sakmann, Regulation of synaptic eﬃcacy by coindence of postsynaptic APs and EPSPs, Science 275 (1997), 213–215.</p>
<p>[12] S.J. Martin, P.D. Grimwood, and R.G.M. Morris, Synaptic Plasticity and Memory: An Evaluation of the Hypothesis, Annual Reviews Neuroscience 23 (2000), 649–711.</p>
<p>[13] T. Natschl¨ger, W. Maass, and A. Zador, Eﬃcient Temporal Processing with Biologa ically Realistic Dynamic Synapses, Tech Report (2002).</p>
<p>[14] L. Pantic, J.T. Joaquin, H.J. Kappen, and S.C.A.M. Gielen, Associatice Memory with Dynamic Synapses, Neural Computation 14 (2002), 2903–2923.</p>
<p>[15] M. Tsodyks, K. Pawelzik, and H. Markram, Neural Networks with Dynamic Synapses, Neural Computation 10 (1998), 821–835.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
