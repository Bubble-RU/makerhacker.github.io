<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>119 nips-2002-Kernel Dependency Estimation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-119" href="../nips2002/nips-2002-Kernel_Dependency_Estimation.html">nips2002-119</a> <a title="nips-2002-119-reference" href="#">nips2002-119-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>119 nips-2002-Kernel Dependency Estimation</h1>
<br/><p>Source: <a title="nips-2002-119-pdf" href="http://papers.nips.cc/paper/2297-kernel-dependency-estimation.pdf">pdf</a></p><p>Author: Jason Weston, Olivier Chapelle, Vladimir Vapnik, André Elisseeff, Bernhard Schölkopf</p><p>Abstract: We consider the learning problem of finding a dependency between a general class of objects and another, possibly different, general class of objects. The objects can be for example: vectors, images, strings, trees or graphs. Such a task is made possible by employing similarity measures in both input and output spaces using kernel functions, thus embedding the objects into vector spaces. We experimentally validate our approach on several tasks: mapping strings to strings, pattern recognition, and reconstruction from partial images. 1</p><br/>
<h2>reference text</h2><p>[1] N. Cristianini, A. Elisseeff, and J. Shawe-Taylor. On optimizing kernel alignment . Technical Report 2001-087, NeuroCOLT, 200l.</p>
<p>[2] I. Frank and J . Friedman. A Statistical View of Some Chemometrics Regression Tools. Technometrics , 35(2):109- 147, 1993.</p>
<p>[3] T. Graepel, R. Herbrich, P. Bollmann-Sdorra, and K Obermayer. Classification on pairwise proximity data. NIPS, 11:438- 444, 1999.</p>
<p>[4] T. Hastie, R. Tibshirani, and J. Friedman. The Elem ents of Statistical Learning. Springer-Verlag, New York , 200l.</p>
<p>[5] D. Haussler. Convolutional kernels on discrete structures. Technical Report UCSCCRL-99-10 , Computer Science Department, University of California at Santa Cruz , 1999.</p>
<p>[6] J . Li, A. N. Michel, and W . Porod. Analysis and synthesis of a class of neural networks: linear systems operating on a closed hypercube. IEEE Trans . on Circuits and Systems, 36(11) :1405- 22 , 1989.</p>
<p>[7] H. Lodhi , C. Saunders, J . Shawe-Taylor, N . Cristianini, and C. Watkins. Text classification using string kernels. Journal of Machine Learning Research, 2:419- 444 , 2002.</p>
<p>[8] S. Mika, G. Ratsch, J. Weston , B. Sch6lkopf, and K-R. Miiller. Fisher discriminant analysis with kernels. In Y.-H. Hu , J . Larsen , E. Wilson, and S. Douglas, editors, N eural N etworks for Signal Processing IX, pages 41- 48 . IEEE, 1999.</p>
<p>[9] C. Saunders, V. Vovk , and A. Gammerman. Ridge regression learning algorithm in dual variables. In J . Shavlik, editor, Machine Learning Proceedings of the Fifteenth International Conference(ICML '98), San Francisco, CA , 1998. Morgan Kaufmann.</p>
<p>[10] B. Sch6lkopf, S. Mika, C. Burges, P. Knirsch, K-R. Miiller, G. Ratsch, and A. J. Smola. Input space vs. feature space in kernel-based methods. IEEE-NN, 10(5):10001017, 1999.</p>
<p>[11] B. Sch6lkopf and A. J. Smola. Learning with K ern els. MIT Press, Cambridge, MA, 2002.</p>
<p>[12] V . Vapnik. Statistical Learning Theory. John Wiley and Sons, New York, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
