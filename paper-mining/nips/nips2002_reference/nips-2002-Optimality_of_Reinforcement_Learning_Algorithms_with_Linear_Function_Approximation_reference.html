<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-159" href="../nips2002/nips-2002-Optimality_of_Reinforcement_Learning_Algorithms_with_Linear_Function_Approximation.html">nips2002-159</a> <a title="nips-2002-159-reference" href="#">nips2002-159-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>159 nips-2002-Optimality of Reinforcement Learning Algorithms with Linear Function Approximation</h1>
<br/><p>Source: <a title="nips-2002-159-pdf" href="http://papers.nips.cc/paper/2322-optimality-of-reinforcement-learning-algorithms-with-linear-function-approximation.pdf">pdf</a></p><p>Author: Ralf Schoknecht</p><p>Abstract: There are several reinforcement learning algorithms that yield approximate solutions for the problem of policy evaluation when the value function is represented with a linear function approximator. In this paper we show that each of the solutions is optimal with respect to a specific objective function. Moreover, we characterise the different solutions as images of the optimal exact value function under different projection operations. The results presented here will be useful for comparing the algorithms in terms of the error they achieve relative to the error of the optimal approximate solution. 1</p><br/>
<h2>reference text</h2><p>[1] L. C. Baird. Residual algorithms: Reinforcement learning with function approximation. Proc. of the Twelfth International Conference on Machine Learning, 1995.</p>
<p>[2] D. P. Bertsekas and J. N. Tsitsiklis. Neuro Dynamic Programming. Athena Scientific, Belmont, Massachusetts, 1996.</p>
<p>[3] J .A. Boyan. Least-squares temporal difference learning. In Proceeding of the Sixteenth International Conference on Machine Learning, pages 49- 56, 1999.</p>
<p>[4] S.J Bradtke and A.G. Barto. Linear least-squares algorithms for temporal difference learning. Machine Learning, 22:33- 57, 1996.</p>
<p>[5] A. Greenbaum . Iterative Methods for Solving Linear Systems. SIAM , 1997.</p>
<p>[6] D. Koller and R. Parr. Policy iteration for factored mdps. In Proc. of the Sixteenth Conference on Uncertainty in Artificial Intelligence (UAI) , pages 326- 334, 2000.</p>
<p>[7] M. G. Lagoudakis and R . Parr. Model-free least-squares policy iteration. In Advances in Neural Information Processing Systems, volume 14, 2002.</p>
<p>[8] R. Schoknecht and A. Merke. Convergent combinations of reinforcement learning with function approximation. In Advances in Neural Information Processing Syst ems, volume 15, 2003.</p>
<p>[9] R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning, 3:9- 44, 1988.</p>
<p>[10] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 1997.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
