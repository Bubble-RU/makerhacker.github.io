<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-123" href="../nips2002/nips-2002-Learning_Attractor_Landscapes_for_Learning_Motor_Primitives.html">nips2002-123</a> <a title="nips-2002-123-reference" href="#">nips2002-123-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>123 nips-2002-Learning Attractor Landscapes for Learning Motor Primitives</h1>
<br/><p>Source: <a title="nips-2002-123-pdf" href="http://papers.nips.cc/paper/2140-learning-attractor-landscapes-for-learning-motor-primitives.pdf">pdf</a></p><p>Author: Auke J. Ijspeert, Jun Nakanishi, Stefan Schaal</p><p>Abstract: Many control problems take place in continuous state-action spaces, e.g., as in manipulator robotics, where the control objective is often deﬁned as ﬁnding a desired trajectory that reaches a particular goal state. While reinforcement learning oﬀers a theoretical framework to learn such control policies from scratch, its applicability to higher dimensional continuous state-action spaces remains rather limited to date. Instead of learning from scratch, in this paper we suggest to learn a desired complex control policy by transforming an existing simple canonical control policy. For this purpose, we represent canonical policies in terms of diﬀerential equations with well-deﬁned attractor properties. By nonlinearly transforming the canonical attractor dynamics using techniques from nonparametric regression, almost arbitrary new nonlinear policies can be generated without losing the stability properties of the canonical system. We demonstrate our techniques in the context of learning a set of movement skills for a humanoid robot from demonstrations of a human teacher. Policies are acquired rapidly, and, due to the properties of well formulated diﬀerential equations, can be re-used and modiﬁed on-line under dynamic changes of the environment. The linear parameterization of nonparametric regression moreover lends itself to recognize and classify previously learned movement skills. Evaluations in simulations and on an actual 30 degree-offreedom humanoid robot exemplify the feasibility and robustness of our approach. 1</p><br/>
<h2>reference text</h2><p>[1] R. Sutton and A.G. Barto. Reinforcement learning: an introduction. MIT Press, 1998.</p>
<p>[2] F.A. Mussa-Ivaldi. Nonlinear force ﬁelds: a distributed system of control primitives for representing and learning movements. In IEEE International Symposium on Computational Intelligence in Robotics and Automation, pages 84–90. IEEE, Computer Society, Los Alamitos, 1997.</p>
<p>[3] A.J. Ijspeert, J. Nakanishi, and S. Schaal. Movement imitation with nonlinear dynamical systems in humanoid robots. In IEEE International Conference on Robotics and Automation (ICRA2002), pages 1398–1403. 2002.</p>
<p>[4] A.J. Ijspeert, J. Nakanishi, and S. Schaal. Learning rhythmic movements by demonstration using nonlinear oscillators. In Proceedings of the IEEE/RSJ Int. Conference on Intelligent Robots and Systems (IROS2002), pages 958–963. 2002.</p>
<p>[5] S. Kawamura and N. Fukao. Interpolation for input torque patterns obtained through learning control. In Proceedings of The Third International Conference on Automation, Robotics and Computer Vision (ICARCV’94). 1994.</p>
<p>[6] H. Miyamoto, S. Schaal, F. Gandolfo, Y. Koike, R. Osu, E. Nakano, Y. Wada, and M. Kawato. A kendama learning robot based on bi-directional theory. Neural Networks, 9:1281–1302, 1996.</p>
<p>[7] S. Schaal. Learning from demonstration. In M. C. Mozer, M. Jordan, and T. Petsche, editors, Advances in Neural Information Processing Systems 9, pages 1040–1046. Cambridge, MA, MIT Press, 1997.</p>
<p>[8] S. Schaal and C.G. Atkeson. Constructive incremental learning from only local information. Neural Computation, 10(8):2047–2084, 1998.</p>
<p>[9] C. G. Atkeson, J. Hale, M. Kawato, S. Kotosaka, F. Pollick, M. Riley, S. Schaal, S. Shibata, G. Tevatia, A. Ude, and S. Vijayakumar. Using humanoid robots to study human behavior. IEEE Intelligent Systems, 15:46–56, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
