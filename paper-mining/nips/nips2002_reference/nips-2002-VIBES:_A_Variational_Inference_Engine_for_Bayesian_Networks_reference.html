<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-204" href="../nips2002/nips-2002-VIBES%3A_A_Variational_Inference_Engine_for_Bayesian_Networks.html">nips2002-204</a> <a title="nips-2002-204-reference" href="#">nips2002-204-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>204 nips-2002-VIBES: A Variational Inference Engine for Bayesian Networks</h1>
<br/><p>Source: <a title="nips-2002-204-pdf" href="http://papers.nips.cc/paper/2172-vibes-a-variational-inference-engine-for-bayesian-networks.pdf">pdf</a></p><p>Author: Christopher M. Bishop, David Spiegelhalter, John Winn</p><p>Abstract: In recent years variational methods have become a popular tool for approximate inference and learning in a wide variety of probabilistic models. For each new application, however, it is currently necessary ﬁrst to derive the variational update equations, and then to implement them in application-speciﬁc code. Each of these steps is both time consuming and error prone. In this paper we describe a general purpose inference engine called VIBES (‘Variational Inference for Bayesian Networks’) which allows a wide variety of probabilistic models to be implemented and solved variationally without recourse to coding. New models are speciﬁed either through a simple script or via a graphical interface analogous to a drawing package. VIBES then automatically generates and solves the variational equations. We illustrate the power and ﬂexibility of VIBES using examples from Bayesian mixture modelling. 1</p><br/>
<h2>reference text</h2><p>[1] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. In M. I. Jordan, editor, Learning in Graphical Models, pages 105–162. Kluwer, 1998.</p>
<p>[2] R. M. Neal and G. E. Hinton. A new view of the EM algorithm that justiﬁes incremental and other variants. In M. I. Jordan, editor, Learning in Graphical Models, pages 355–368. Kluwer, 1998.</p>
<p>[3] D J Lunn, A Thomas, N G Best, and D J Spiegelhalter. WinBUGS – a Bayesian modelling framework: concepts, structure and extensibility. Statistics and Computing, 10:321–333, 2000. http://www.mrc-bsu.cam.ac.uk/bugs/.</p>
<p>[4] Z. Ghahramani and M. J. Beal. Propagation algorithms for variational Bayesian learning. In T. K. Leen, T. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems, volume 13, Cambridge MA, 2001. MIT Press.</p>
<p>[5] H. Attias. A variational Bayesian framework for graphical models. In S. Solla, T. K. Leen, and K-L Muller, editors, Advances in Neural Information Processing Systems, volume 12, pages 209–215, Cambridge MA, 2000. MIT Press.</p>
<p>[6] C. M. Bishop. Variational principal components. In Proceedings Ninth International Conference on Artiﬁcial Neural Networks, ICANN’99, volume 1, pages 509–514. IEE, 1999.</p>
<p>[7] Christopher M. Bishop and John Winn. Structured variational distributions in VIBES. In Proceedings Artiﬁcial Intelligence and Statistics, Key West, Florida, 2003. Accepted for publication.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
