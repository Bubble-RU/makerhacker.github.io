<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-70" href="../nips2002/nips-2002-Distance_Metric_Learning_with_Application_to_Clustering_with_Side-Information.html">nips2002-70</a> <a title="nips-2002-70-reference" href="#">nips2002-70-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>70 nips-2002-Distance Metric Learning with Application to Clustering with Side-Information</h1>
<br/><p>Source: <a title="nips-2002-70-pdf" href="http://papers.nips.cc/paper/2164-distance-metric-learning-with-application-to-clustering-with-side-information.pdf">pdf</a></p><p>Author: Eric P. Xing, Michael I. Jordan, Stuart Russell, Andrew Y. Ng</p><p>Abstract: Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many “plausible” ways, and if a clustering algorithm such as K-means initially fails to ﬁnd one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufﬁciently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider “similar.” For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, , learns a distance metric over if desired, dissimilar) pairs of points in that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efﬁcient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to signiﬁcantly improve clustering performance. £ ¤¢ £ ¥¢</p><br/>
<h2>reference text</h2><p>[1] C. Atkeson, A. Moore, and S. Schaal. Locally weighted learning. AI Review, 1996.</p>
<p>[2] T. Cox and M. Cox. Multidimensional Scaling. Chapman & Hall, London, 1994.</p>
<p>[3] C. Domeniconi and D. Gunopulos. Adaptive nearest neighbor classiﬁcation using support vector machines. In Advances in Neural Information Processing Systems 14. MIT Press, 2002.</p>
<p>[4] G. H. Golub and C. F. Van Loan. Matrix Computations. Johns Hopkins Univ. Press, 1996.</p>
<p>[5] T. Hastie and R. Tibshirani. Discriminant adaptive nearest neighbor classiﬁcation. IEEE Transactions on Pattern Analysis and Machine Learning, 18:607–616, 1996.</p>
<p>[6] T.S. Jaakkola and D. Haussler. Exploiting generative models in discriminaive classiﬁer. In Proc. of Tenth Conference on Advances in Neural Information Processing Systems, 1999.</p>
<p>[7] I.T. Jolliffe. Principal Component Analysis. Springer-Verlag, New York, 1989.</p>
<p>[8] R. Rockafellar. Convex Analysis. Princeton Univ. Press, 1970.</p>
<p>[9] S.T. Roweis and L.K. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science 290: 2323-2326.</p>
<p>[10] B. Scholkopf and A. Smola. Learning with Kernels. In Press, 2001.</p>
<p>[11] N. Tishby, F. Pereira, and W. Bialek. The information bottleneck method. In Proc. of the 37th Allerton Conference on Communication, Control and Computing, 1999.</p>
<p>[12] K. Wagstaff, C. Cardie, S. Rogers, and S. Schroedl. Constrained k-means clustering with background knowledge. In Proc. 18th International Conference on Machine Learning, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
