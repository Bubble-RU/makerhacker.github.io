<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>195 nips-2002-The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on such Singularities</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-195" href="../nips2002/nips-2002-The_Effect_of_Singularities_in_a_Learning_Machine_when_the_True_Parameters_Do_Not_Lie_on_such_Singularities.html">nips2002-195</a> <a title="nips-2002-195-reference" href="#">nips2002-195-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>195 nips-2002-The Effect of Singularities in a Learning Machine when the True Parameters Do Not Lie on such Singularities</h1>
<br/><p>Source: <a title="nips-2002-195-pdf" href="http://papers.nips.cc/paper/2255-the-effect-of-singularities-in-a-learning-machine-when-the-true-parameters-do-not-lie-on-such-singularities.pdf">pdf</a></p><p>Author: Sumio Watanabe, Shun-ichi Amari</p><p>Abstract: A lot of learning machines with hidden variables used in information science have singularities in their parameter spaces. At singularities, the Fisher information matrix becomes degenerate, resulting that the learning theory of regular statistical models does not hold. Recently, it was proven that, if the true parameter is contained in singularities, then the coeﬃcient of the Bayes generalization error is equal to the pole of the zeta function of the Kullback information. In this paper, under the condition that the true parameter is almost but not contained in singularities, we show two results. (1) If the dimension of the parameter from inputs to hidden units is not larger than three, then there exits a region of true parameters where the generalization error is larger than those of regular models, however, if otherwise, then for any true parameter, the generalization error is smaller than those of regular models. (2) The symmetry of the generalization error and the training error does not hold in singular models in general. 1</p><br/>
<h2>reference text</h2><p>[1] Amari,S., Park,H., and Ozeki,T. (2002) Geometrical singularities in the neuromanifold of multilayer perceptrons. Advances in Neural Information Processing Systems, Vol.14.</p>
<p>[2] Hartigan, J.A. (1985) A Failure of likelihood asymptotics for normal mixtures. Proceedings of the Berkeley Conference in Honor of J.Neyman and J.Kiefer, Vol.2, pp.807-810.</p>
<p>[3] Hironaka, H. (1964). Resolution of singularities of an algebraic variety over a ﬁeld of characteristic zero. Annals of Mathematics, 79, 109-326.</p>
<p>[4] Rusakov, D, Geiger,D.(2002) Asymptotic model selection for naive Bayesian networks. Proc. of UAI02.</p>
<p>[5] Watanabe, S. (1999). Algebraic analysis for singular statistical estimation. Lecture Notes in Computer Science, 1720, 39-50.</p>
<p>[6] Watanabe, S.,(2001) Algebraic analysis for nonidentiﬁable learning machines. Neural Computation, 13,(4), pp.899-933.</p>
<p>[7] Watanabe, S. (2001) Algebraic information geometry for learning machines with singularities. Advances in Neural Information Processing Systems, Vol.13, 329-336.</p>
<p>[8] Watanabe, S. (2001) Algebraic geometrical methods for hierarchical learning machines. International Journal of Neural Networks, Vol.14, No.8, 1049-1060.</p>
<p>[9] Watanabe,S., & Amari,S.-I.(2003) Learning coeﬃcients of layered models when the true distriburion mismatches the singularities.Neural Computation, to appear.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
