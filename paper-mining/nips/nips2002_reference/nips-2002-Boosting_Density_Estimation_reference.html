<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>46 nips-2002-Boosting Density Estimation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-46" href="../nips2002/nips-2002-Boosting_Density_Estimation.html">nips2002-46</a> <a title="nips-2002-46-reference" href="#">nips2002-46-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>46 nips-2002-Boosting Density Estimation</h1>
<br/><p>Source: <a title="nips-2002-46-pdf" href="http://papers.nips.cc/paper/2298-boosting-density-estimation.pdf">pdf</a></p><p>Author: Saharon Rosset, Eran Segal</p><p>Abstract: Several authors have suggested viewing boosting as a gradient descent search for a good ﬁt in function space. We apply gradient-based boosting methodology to the unsupervised learning problem of density estimation. We show convergence properties of the algorithm and prove that a strength of weak learnability property applies to this problem as well. We illustrate the potential of this approach through experiments with boosting Bayesian networks to learn density models.</p><br/>
<h2>reference text</h2><p>[1] Kegg: Kyoto encyclopedia of genes and genomes. In http://www.genome.ad.jp/kegg.</p>
<p>[2] C. M. Bishop. Neural Networks for Pattern Recognition. Oxford University Press, Oxford, U.K., 1995.</p>
<p>[3] L. Breiman, J.H. Friedman, R. Olshen, and C. Stone. Classiﬁcation and Regression Trees. Wardsworth International Group, 1984.</p>
<p>[4] P. Cheeseman and J. Stutz. Bayesian Classiﬁcation (AutoClass): Theory and Results. AAAI Press, 1995.</p>
<p>[5] R. O. Duda and P. E. Hart. Pattern Classiﬁcation and Scene Analysis. John Wiley & Sons, New York, 1973.</p>
<p>[6] Y. Freund and R.E. Scahpire. A decision theoretic generalization of on-line learning and an application to boosting. In the 2nd Eurpoean Conference on Computational Learning Theory, 1995.</p>
<p>[7] J.H. Friedman. Greedy function approximation: A gradient boosting machine. Annals of Statistics, Vol. 29 No. 5, 2001.</p>
<p>[8] J.H. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. Annals of Statistics, Vol. 28 pp. 337-407, 2000.</p>
<p>[9] N. Friedman and D. Koller. Being bayesian about network structure: A bayesian approach to structure discovery in bayesian networks. Machine Learning Journal, 2002.</p>
<p>[10] A.P. Gasch, P.T. Spellman, C.M. Kao, O.Carmel-Harel, M.B. Eisen, G.Storz, D.Botstein, and P.O. Brown. Genomic expression program in the response of yeast cells to environmental changes. Mol. Bio. Cell, 11:4241–4257, 2000.</p>
<p>[11] D. Heckerman. A tutorial on learning with Bayesian networks. In M. I. Jordan, editor, Learning in Graphical Models. MIT Press, Cambridge, MA, 1998.</p>
<p>[12] T. R. Hughes et al. Functional discovery via a compendium of expression proﬁles. Cell, 102(1):109–26, 2000.</p>
<p>[13] L. Mason, J. Baxter, P. Bartlett, and P. Frean. Boosting algorithms as gradient descent in function space. In Proc. NIPS, number 12, pages 512–518, 1999.</p>
<p>[14] M. Meila and T. Jaakkola. Tractable bayesian learning of tree belief networks. Technical Report CMU-RI-TR-00-15, Robotics institute, Carnegie Mellon University, 2000.</p>
<p>[15] D. Pe’er, A. Regev, G. Elidan, and N. Friedman. Inferring subnetworks from perturbed expression proﬁles. In ISMB’01, 2001.</p>
<p>[16] J.R. Quinlan. C4.5 - Programs for Machine Learning. Morgan-Kaufmann, 1993.</p>
<p>[17] S. Rosset, J. Zhu, and T. Hastie. Boosting as a regularized path to a margin maximizer. Submitted to NIPS 2002.</p>
<p>[18] R.E. Scahpire, Y. Freund, P. Bartlett, and W.S. Lee. Boosting the margin: a new explanation for the effectiveness of voting methods. Annals of Statistics, Vol. 26 No. 5, 1998.</p>
<p>[19] P. T. Spellman et al. Comprehensive identiﬁcation of cell cycle-regulated genes of the yeast saccharomyces cerevisiae by microarray hybridization. Mol. Biol. Cell, 9(12):3273–97, 1998.</p>
<p>[20] B. Thiesson, C. Meek, and D. Heckerman. Learning mixtures of dag models. Technical Report MSR-TR-98-12, Microsoft Research, 1997.</p>
<p>[21] R.S. Zemel and T. Pitassi. A gradient-based boosting algorithm for regression problems. In Proc. NIPS, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
