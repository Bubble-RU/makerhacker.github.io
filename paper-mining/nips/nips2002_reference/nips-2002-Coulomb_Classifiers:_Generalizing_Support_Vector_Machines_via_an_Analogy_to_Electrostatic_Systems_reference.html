<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-62" href="../nips2002/nips-2002-Coulomb_Classifiers%3A_Generalizing_Support_Vector_Machines_via_an_Analogy_to_Electrostatic_Systems.html">nips2002-62</a> <a title="nips-2002-62-reference" href="#">nips2002-62-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 nips-2002-Coulomb Classifiers: Generalizing Support Vector Machines via an Analogy to Electrostatic Systems</h1>
<br/><p>Source: <a title="nips-2002-62-pdf" href="http://papers.nips.cc/paper/2148-coulomb-classifiers-generalizing-support-vector-machines-via-an-analogy-to-electrostatic-systems.pdf">pdf</a></p><p>Author: Sepp Hochreiter, Michael C. Mozer, Klaus Obermayer</p><p>Abstract: We introduce a family of classiﬁers based on a physical analogy to an electrostatic system of charged conductors. The family, called Coulomb classiﬁers, includes the two best-known support-vector machines (SVMs), the ν–SVM and the C–SVM. In the electrostatics analogy, a training example corresponds to a charged conductor at a given location in space, the classiﬁcation function corresponds to the electrostatic potential function, and the training objective function corresponds to the Coulomb energy. The electrostatic framework provides not only a novel interpretation of existing algorithms and their interrelationships, but it suggests a variety of new methods for SVMs including kernels that bridge the gap between polynomial and radial-basis functions, objective functions that do not require positive-deﬁnite kernels, regularization techniques that allow for the construction of an optimal classiﬁer in Minkowski space. Based on the framework, we propose novel SVMs and perform simulation studies to show that they are comparable or superior to standard SVMs. The experiments include classiﬁcation tasks on data which are represented in terms of their pairwise proximities, where a Coulomb Classiﬁer outperformed standard SVMs. 1</p><br/>
<h2>reference text</h2><p>[1] M. A. Aizerman, E. M. Braverman, and L. I. Rozono´r. Theoretical foundations e of the potential function method in pattern recognition learning. Automation and Remote Control, 25:821–837, 1964.</p>
<p>[2] C. J. C. Burges. A tutorial on support vector machines for pattern recognition. Data Mining and Knowledge Discovery, 2(2):1–47, 1998.</p>
<p>[3] T. Graepel, R. Herbrich, B. Sch¨lkopf, A. J. Smola, P. L. Bartlett, K.-R. o M¨ller, K. Obermayer, and R. C. Williamson. Classiﬁcation on proximity data u with LP–machines. In Proceedings of the Ninth International Conference on Artiﬁcial Neural Networks, pages 304–309, 1999.</p>
<p>[4] S. Hochreiter and M. C. Mozer. Coulomb classiﬁers: Reinterpreting SVMs as electrostatic systems. Technical Report CU-CS-921-01, Department of Computer Science, University of Colorado, Boulder, 2001.</p>
<p>[5] T. Hofmann and J. Buhmann. Pairwise data clustering by deterministic annealing. IEEE Trans. Pattern Anal. and Mach. Intelligence, 19(1):1–14, 1997.</p>
<p>[6] J. Mercer. Functions of positive and negative type and their connection with the theory of integral equations. Philosophical Transactions of the Royal Society of London A, 209:415–446, 1909.</p>
<p>[7] G. R¨tsch, T. Onoda, and K.-R. M¨ller. Soft margins for AdaBoost. Technical a u Report NC-TR-1998-021, Dep. of Comp. Science, Univ. of London, 1998.</p>
<p>[8] J. W. Scannell, C. Blakemore, and M. P. Young. Analysis of connectivity in the cat cerebral cortex. The Journal of Neuroscience, 15(2):1463–1483, 1995.</p>
<p>[9] B. Sch¨lkopf and A. J. Smola. Learning with Kernels — Support Vector Mao chines, Regularization, Optimization, and Beyond. MIT Press, 2002.</p>
<p>[10] M. Schwartz. Principles of Electrodynamics. Dover Publications, NY, 1987. Republication of McGraw-Hill Book 1972.</p>
<p>[11] V. Vapnik. The nature of statistical learning theory. Springer, NY, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
