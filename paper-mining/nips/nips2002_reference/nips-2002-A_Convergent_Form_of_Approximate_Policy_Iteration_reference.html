<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>3 nips-2002-A Convergent Form of Approximate Policy Iteration</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-3" href="../nips2002/nips-2002-A_Convergent_Form_of_Approximate_Policy_Iteration.html">nips2002-3</a> <a title="nips-2002-3-reference" href="#">nips2002-3-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>3 nips-2002-A Convergent Form of Approximate Policy Iteration</h1>
<br/><p>Source: <a title="nips-2002-3-pdf" href="http://papers.nips.cc/paper/2143-a-convergent-form-of-approximate-policy-iteration.pdf">pdf</a></p><p>Author: Theodore J. Perkins, Doina Precup</p><p>Abstract: We study a new, model-free form of approximate policy iteration which uses Sarsa updates with linear state-action value function approximation for policy evaluation, and a “policy improvement operator” to generate a new policy based on the learned state-action values. We prove that if the policy improvement operator produces -soft policies and is Lipschitz continuous in the action values, with a constant that is not too large, then the approximate policy iteration algorithm converges to a unique solution from any initial policy. To our knowledge, this is the ﬁrst convergence result for any form of approximate policy iteration under similar computational-resource assumptions.</p><br/>
<h2>reference text</h2><p>[1] L. C. Baird. Residual algorithms: Reinforcement learning with function approximation. In Proceedings of the Twelfth International Conference on Machine Learning, pages 30–37. Morgan Kaufmann, 1995.</p>
<p>[2] A. G. Barto, S. J. Bradtke, and S. P. Singh. Learning to act using real-time dynamic programming. Artiﬁcial Intelligence, 72(1):81–138, 1995.</p>
<p>[3] D. P. Bertsekas. Dynamic Programming and Optimal Control, Volumes 1 and 2. Athena Scientiﬁc, 2001.</p>
<p>[4] D. P. Bertsekas and J. N. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientiﬁc, 1996.</p>
<p>[5] D. P. De Farias and B. Van Roy. On the existence of ﬁxed points for approximate value iteration and temporal-difference learning. Journal of Opt. Theory and Applications, 105(3), 2000.</p>
<p>[6] G. Gordon. Chattering in Sarsa( ). www.cs.cmu.edu/ ggordon, 1996.  CMU Learning Lab Internal Report. Available at  ¡</p>
<p>[7] G. Gordon. Approximate Solutions to Markov Decision Processes. PhD thesis, Carnegie Mellon University, 1999.</p>
<p>[8] G. J. Gordon. Reinforcement learning with function approximation converges to a region. Advances in Neural Information Processing Systems 13, pages 1040–1046. MIT Press, 2001.</p>
<p>[9] C. D. Meyer. Matrix Analysis and Applied Linear Algebra. SIAM, 2000.</p>
<p>[10] T. J. Perkins and M. D. Pendrith. On the existence of ﬁxed points for Q-learning and Sarsa in partially observable domains. In Proceedings of the Nineteenth International Conference on Machine Learning, 2002.</p>
<p>[11] M. L. Puterman. Markov Decision Processes: Disrete Stochastic Dynamic Programming. John Wiley & Sons, Inc, New York, 1994.</p>
<p>[12] E. Seneta. Sensitivity analysis, ergodicity coefﬁcients, and rank-one updates for ﬁnite markov chains. In W. J. Stewart, editor, Numerical Solutions of Markov Chains. Dekker, NY, 1991.</p>
<p>[13] S. Singh, T. Jaakkola, M. L. Littman, and C. Szepesvari. Convergence results for single-step on-policy reinforcement-learning algorithms. Machine Learning, 38(3):287–308, 2000.</p>
<p>[14] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press/Bradford Books, Cambridge, Massachusetts, 1998.</p>
<p>[15] G. J. Tesauro. TD-Gammon, a self-teaching backgammon program, achieves master-level play. Neural Computation, 6(2):215–219, 1994.</p>
<p>[16] J. N. Tsitsiklis and B. Van Roy. Optimal stopping of markov processes: Hilbert space theory, approximation algorithms, and an application to pricing high-dimensional ﬁnancial derivatives. IEEE Transactions on Automatic Control, 44(10):1840–1851, 1999.</p>
<p>[17] J. N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674–690, 1997.  £  a  © 2©   © ¨¥ §  H  £ £) $ w £ $ £)    £  ¤  , let  £  !  £  & 2 ) $ AA wy ! ) $ £  1. for all , 2. iff is non-singular, 3. for any , 4. is continuous.   £  Lemma 7 For -by- matrix  £  Appendix  . Then:  ,  £  ¤   £  £  £  ¤  £  ¤    ¤  Proof: The ﬁrst three points readily follow from elementary arguments. We focus on the last point. We want to show that given a sequence of matrices , that converge to some , then . Note that means that . Let . Then  ¡ $ ! w £ H)s ) £ $ ¤ £ ) ¡ £ $ ¤ X ¦¤AH¥ ¥ I)s £ ¤ £ £ ¡  £ £ s ) £ ' ¡ £ £ © ' £ s  s £ £ X §¦Q¡!</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
