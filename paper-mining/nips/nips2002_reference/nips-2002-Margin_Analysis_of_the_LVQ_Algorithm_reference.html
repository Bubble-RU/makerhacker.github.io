<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>140 nips-2002-Margin Analysis of the LVQ Algorithm</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-140" href="../nips2002/nips-2002-Margin_Analysis_of_the_LVQ_Algorithm.html">nips2002-140</a> <a title="nips-2002-140-reference" href="#">nips2002-140-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>140 nips-2002-Margin Analysis of the LVQ Algorithm</h1>
<br/><p>Source: <a title="nips-2002-140-pdf" href="http://papers.nips.cc/paper/2261-margin-analysis-of-the-lvq-algorithm.pdf">pdf</a></p><p>Author: Koby Crammer, Ran Gilad-bachrach, Amir Navot, Naftali Tishby</p><p>Abstract: Prototypes based algorithms are commonly used to reduce the computational complexity of Nearest-Neighbour (NN) classiﬁers. In this paper we discuss theoretical and algorithmical aspects of such algorithms. On the theory side, we present margin based generalization bounds that suggest that these kinds of classiﬁers can be more accurate then the 1-NN rule. Furthermore, we derived a training algorithm that selects a good set of prototypes using large margin principles. We also show that the 20 years old Learning Vector Quantization (LVQ) algorithm emerges naturally from our framework. 1</p><br/>
<h2>reference text</h2><p>[1] E. Fix and j. Hodges. Discriminatory analysis. nonparametric discrimination: Consistency properties. Technical Report 4, USAF school of Aviation Medicine, 1951.</p>
<p>[2] P. Y. Simard, Y. A. Le Cun, and J. Denker. Efﬁcient pattern recognition using a new transformation distance. In Advances in Neural Information Processing Systems, volume 5, pages 50–58. 1993.</p>
<p>[3] P. Indyk and R. Motwani. Approximate nearest neighbors: towards removing the curse of dimensionality. In Proceedings of the 30th ACM Symposium on the Theory of Computing, pages 604–613, 1998.</p>
<p>[4] V. Vapnik. The Nature Of Statistical Learning Theory. Springer-Verlag, 1995.</p>
<p>[5] Y. Freund and R. E. Schapire. A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences, 55(1):119–139, 1997.</p>
<p>[6] R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee. Boosting the margin : A new explanation for the effectiveness of voting methods. Annals of Statistics, 1998.</p>
<p>[7] Llew Mason, P. Bartlett, and J. Baxter. Direct optimization of margins improves generalization in combined classiﬁer. Advances in Neural Information Processing Systems, 11:288–294, 1999.</p>
<p>[8] C. Campbell, N. Cristianini, and A. Smola. Query learning with large margin classiﬁers. In International Conference on Machine Learning, 2000.</p>
<p>[9] T. Kohonen. Self-Organizing Maps. Springer-Verlag, 1995.</p>
<p>[10] L. Buckingham and S. Geva. Lvq is a maximum margin algorithm. In Paciﬁc Knowledge Acquisition Workshop PKAW’2000, 2000.</p>
<p>[11] L. Devroye, L. Gyorﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, New York, 1996.</p>
<p>[12] Y. Singer and D. D. Lewis. Machine learning for information retrieval: Advanced techniques. presented at ACM SIGIR 2000, 2000.</p>
<p>[13] T. Kohonen, J. Hynninen, J. Kangas, and K. Laaksonen, J. Torkkola. Lvq pak, the learning vector quantization program package. http://www.cis.hut.ﬁ/research/lvq pak, 1995.</p>
<p>[14] Y. Freund. Boosting a weak learning algorithm by majority. Information and Computation, 121(2):256–285, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
