<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>142 nips-2002-Maximum Likelihood and the Information Bottleneck</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-142" href="../nips2002/nips-2002-Maximum_Likelihood_and_the_Information_Bottleneck.html">nips2002-142</a> <a title="nips-2002-142-reference" href="#">nips2002-142-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>142 nips-2002-Maximum Likelihood and the Information Bottleneck</h1>
<br/><p>Source: <a title="nips-2002-142-pdf" href="http://papers.nips.cc/paper/2214-maximum-likelihood-and-the-information-bottleneck.pdf">pdf</a></p><p>Author: Noam Slonim, Yair Weiss</p><p>Abstract: The information bottleneck (IB) method is an information-theoretic formulation for clustering problems. Given a joint distribution , this method constructs a new variable that deﬁnes partitions over the values of that are informative about . Maximum likelihood (ML) of mixture models is a standard statistical approach to clustering problems. In this paper, we ask: how are the two methods related ? We deﬁne a simple mapping between the IB problem and the ML problem for the multinomial mixture model. We show that under this mapping the or problems are strongly related. In fact, for uniform input distribution over for large sample size, the problems are mathematically equivalent. Speciﬁcally, in these cases, every ﬁxed point of the IB-functional deﬁnes a ﬁxed point of the (log) likelihood and vice versa. Moreover, the values of the functionals at the ﬁxed points are equal under simple transformations. As a result, in these cases, every algorithm that solves one of the problems, induces a solution for the other.   ©§ ¥£ ¨¦¤¢   </p><br/>
<h2>reference text</h2><p>[1] N. Tishby, F. Pereira, and W. Bialek. The Information Bottleneck method. In Proc. 37th Allerton Conference on Communication and Computation, 1999.</p>
<p>[2] N. Slonim. The Information Bottleneck: theory and applications. Ph.D. thesis, The Hebrew University, 2002.</p>
<p>[3] T. M. Cover and J. A. Thomas. Elements of Information Theory. John Wiley & Sons, New York, 1991.</p>
<p>[4] T. Hofmann, J. Puzicha, and M. I. Jordan. Learning from dyadic data. In Proc. of NIPS-11, 1998.</p>
<p>[5] J. Puzicha, T. Hofmann, and J. M. Buhmann. Histogram clustering for unsupervised segmentation and image retrieval. In Pattern Recognition Letters 20(9), 899-909, 1999.</p>
<p>[6] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum Likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, vol. 39, pp. 1-38, 1977.</p>
<p>[7] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. In M. I. Jordan (editor), Learning in Graphical Models, pp. 355-368, 1998.</p>
<p>[8] L. Hermes, T. z¨ ller, and J. M. Buhmann. Parametric distributional clustering for image segmentation. In Proc. of European o Conference on Computer Vision (ECCV), 2002</p>
<p>[9] K. Lang. Learning to ﬁlter netnews. In Proc. of the 12th Int. Conf. on Machine Learning, 1995.</p>
<p>[10] N. Slonim, N. Friedman, and N. Tishby. Unsupervised document classiﬁcation using sequential information maximization. In Proc. of SIGIR-25, 2002.</p>
<p>[11] N. Friedman, O. Mosenzon, N. Slonim, and N. Tishby. Multivariate Information Bottleneck. In Proc. of UAI-17, 2001. §  The KL with respect to is deﬁned as the minimum over all the members in . Therefore, here, both arguments of the KL are changing during the process, and the distributions involved in the minimization are over all the three random variables. ¨  ¨</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
