<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-155" href="../nips2002/nips-2002-Nonparametric_Representation_of_Policies_and_Value_Functions%3A_A_Trajectory-Based_Approach.html">nips2002-155</a> <a title="nips-2002-155-reference" href="#">nips2002-155-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>155 nips-2002-Nonparametric Representation of Policies and Value Functions: A Trajectory-Based Approach</h1>
<br/><p>Source: <a title="nips-2002-155-pdf" href="http://papers.nips.cc/paper/2213-nonparametric-representation-of-policies-and-value-functions-a-trajectory-based-approach.pdf">pdf</a></p><p>Author: Christopher G. Atkeson, Jun Morimoto</p><p>Abstract: A longstanding goal of reinforcement learning is to develop nonparametric representations of policies and value functions that support rapid learning without suffering from interference or the curse of dimensionality. We have developed a trajectory-based approach, in which policies and value functions are represented nonparametrically along trajectories. These trajectories, policies, and value functions are updated as the value function becomes more accurate or as a model of the task is updated. We have applied this approach to periodic tasks such as hopping and walking, which required handling discount factors and discontinuities in the task dynamics, and using function approximation to represent value functions at discontinuities. We also describe extensions of the approach to make the policies more robust to modeling error and sensor noise.</p><br/>
<h2>reference text</h2><p>[1] Richard S. Sutton. Integrated architectures for learning , planning and reacting based on approximating dynamic programming. In Proceedings 7th International Conference on Machine Learning., 1990.</p>
<p>[2] C. Atkeson and J. Santamaria. A comparison of direct and model-based reinforcement learning, 1997.</p>
<p>[3] Christopher G. Atkeson. Using local trajectory optimizers to speed up global optimization in dynamic programming. In Jack D. Cowan, Gerald Tesauro, and Joshua Alspector, editors, Advances in Neural Information Processing Systems, volume 6, pages 663–670. Morgan Kaufmann Publishers, Inc., 1994.</p>
<p>[4] P. Dyer and S. R. McReynolds. The Computation and Theory of Optimal Control. Academic Press, New York, NY, 1970.</p>
<p>[5] D. H. Jacobson and D. Q. Mayne. Differential Dynamic Programming. Elsevier, New York, NY, 1970.</p>
<p>[6] Christopher G. Atkeson and Stefan Schaal. Robot learning from demonstration. In Proc. 14th International Conference on Machine Learning, pages 12–20. Morgan Kaufmann, 1997.</p>
<p>[7] C. G. Atkeson, A. W. Moore, and S. Schaal. Locally weighted learning. Artiﬁcial Intelligence Review, 11:11–73, 1997.</p>
<p>[8] W. Schwind and D. Koditschek. Control of forward velocity for a simpliﬁed planar hopping robot. In International Conference on Robotics and Automation, volume 1, pages 691–6, 1995.</p>
<p>[9] J. Andrew Bagnell and Jeff Schneider. Autonomous helicopter control using reinforcement learning policy search methods. In International Conference on Robotics and Automation, 2001.</p>
<p>[10] M. Garcia, A. Chatterjee, and A. Ruina. Efﬁciency, speed, and scaling of two-dimensional passive-dynamic walking. Dynamics and Stability of Systems, 15(2):75–99, 2000.</p>
<p>[11] K. Zhou, J. C. Doyle, and K. Glover. Robust Optimal Control. PRENTICE HALL, New Jersey, 1996.</p>
<p>[12] J. Morimoto and K. Doya. Robust Reinforcement Learning. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Processing Systems 13, pages 1061–1067. MIT Press, Cambridge, MA, 2001.</p>
<p>[13] R. Neuneier and O. Mihatsch. Risk Sensitive Reinforcement Learning. In M. S. Kearns, S. A. Solla, and D. A. Cohn, editors, Advances in Neural Information Processing Systems 11, pages 1031–1037. MIT Press, Cambridge, MA, USA, 1998.</p>
<p>[14] S. P. Coraluppi and S. I. Marcus. Risk-Sensitive and Minmax Control of Discrete-Time FiniteState Markov Decision Processes. Automatica, 35:301–309, 1999.</p>
<p>[15] J. Morimoto and C. Atkeson. Minimax differential dynamic programming: An application to robust biped walking. In Advances in Neural Information Processing Systems 15. MIT Press, Cambridge, MA, 2002.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
