<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>115 nips-2002-Informed Projections</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-115" href="../nips2002/nips-2002-Informed_Projections.html">nips2002-115</a> <a title="nips-2002-115-reference" href="#">nips2002-115-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>115 nips-2002-Informed Projections</h1>
<br/><p>Source: <a title="nips-2002-115-pdf" href="http://papers.nips.cc/paper/2194-informed-projections.pdf">pdf</a></p><p>Author: David Tax</p><p>Abstract: Low rank approximation techniques are widespread in pattern recognition research — they include Latent Semantic Analysis (LSA), Probabilistic LSA, Principal Components Analysus (PCA), the Generative Aspect Model, and many forms of bibliometric analysis. All make use of a low-dimensional manifold onto which data are projected. Such techniques are generally “unsupervised,” which allows them to model data in the absence of labels or categories. With many practical problems, however, some prior knowledge is available in the form of context. In this paper, I describe a principled approach to incorporating such information, and demonstrate its application to PCA-based approximations of several data sets. 1</p><br/>
<h2>reference text</h2><p>[1] D. Blei, A. Ng, and M. I. Jordan. Latent dirichlet allocation. In Advances in Neural Information Processing Systems 14, 2002.</p>
<p>[2] C.J.C. Burges, J.C. Platt, and S. Jana. Extracting noise-robust features from audio data. In Proceedings of ICASSP, 2002.</p>
<p>[3] D. Cohn and T. Hofmann. The missing link - a probabilistic model of document content and hypertext connectivity. In T. Leen et al., editor, Advances in Neural Information Processing Systems 13, 2001.</p>
<p>[4] M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Nigam, and S. Slattery. Learning to extract symbolic knowledge from the world wide web. In Proceedings of the 15th National Conference on Artiﬁcial Intelligence (AAAI-98), 1998.</p>
<p>[5] S. Dumais, G. Furnas, T. Landauer, S. Deerwester, and R. Harshman. Using latent semantic analysis to improve access to textual information. In Proceedings of the Conference on Human Factors in Computing Systems CHI’88, 1988.</p>
<p>[6] T. Hofmann. Probabilistic latent semantic analysis. In Proc. of Uncertainty in Artiﬁcial Intelligence, UAI’99, Stockholm, 1999.</p>
<p>[7] M. Littman, S. Dumais, and T. Landauer. Automatic cross-language information retrieval using latent semantic indexing. In G. Grefenstette, editor, Cross Language Information Retrieval. Kluwer, 1998.</p>
<p>[8] D. Lowe and M. E. Tipping. Feed-forward neural networks and topographic mappings for exploratory data analysis. Neural Computing and Applications, 4:83–95, 1996.</p>
<p>[9] A. K. McCallum. Bow: A toolkit for statistical language modeling, text retrieval, classiﬁcation and clustering. http://www.cs.cmu.edu/ mccallum/bow, 1996.</p>
<p>[10] K. Nigam, A. K. McCallum, S. Thrun, and T. M. Mitchell. Learning to classify text from labeled and unlabeled documents. In Proceedings of AAAI-98, pages 792–799, Madison, US, 1998. AAAI Press, Menlo Park, US.</p>
<p>[11] J. Platt, C. Burges, S. Swenson, C. Weare, and A. Zheng. Learning a gaussian process prior for automatically generating music playlists. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14. MIT Press, 2002.</p>
<p>[12] B. D. Ripley. Pattern Recognition and Neural Networks. Cambridge: University Press, 1996.</p>
<p>[13] S. Roweis. EM algorithms for PCA and SPCA. In M. I. Jordan, M. J. Kearns, and S. A. Solla, editors, Advances in Neural Information Processing Systems, volume 10. MIT Press, 1998.</p>
<p>[14] S. Roweis and L. Saul. Nonlinear dimensionality reduction by locally linear embedding. Science, 290(5500):2323–2326, Dec 2000.</p>
<p>[15] M. E. Tipping and D. Lowe. Shadow targets: A novel algorithm for topographic projections by radial basis functions. Neurocomputing, 19(1):211–222, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
