<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-127" href="../nips2002/nips-2002-Learning_Sparse_Topographic_Representations_with_Products_of_Student-t_Distributions.html">nips2002-127</a> <a title="nips-2002-127-reference" href="#">nips2002-127-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>127 nips-2002-Learning Sparse Topographic Representations with Products of Student-t Distributions</h1>
<br/><p>Source: <a title="nips-2002-127-pdf" href="http://papers.nips.cc/paper/2177-learning-sparse-topographic-representations-with-products-of-student-t-distributions.pdf">pdf</a></p><p>Author: Max Welling, Simon Osindero, Geoffrey E. Hinton</p><p>Abstract: We propose a model for natural images in which the probability of an image is proportional to the product of the probabilities of some ﬁlter outputs. We encourage the system to ﬁnd sparse features by using a Studentt distribution to model each ﬁlter output. If the t-distribution is used to model the combined outputs of sets of neurally adjacent ﬁlters, the system learns a topographic map in which the orientation, spatial frequency and location of the ﬁlters change smoothly across the map. Even though maximum likelihood learning is intractable in our model, the product form allows a relatively efﬁcient learning procedure that works well even for highly overcomplete sets of ﬁlters. Once the model has been learned it can be used as a prior to derive the “iterated Wiener ﬁlter” for the purpose of denoising images.</p><br/>
<h2>reference text</h2><p>[1] G.E. Hinton. Training products of experts by minimizing contrastive divergence. Neural Computation, 14:1771–1800, 2002.</p>
<p>[2] G.E. Hinton, M. Welling, Y.W. Teh, and K. Osindero. A new view of ICA. In Int. Conf. on Independent Component Analysis and Blind Source Separation, 2001.</p>
<p>[3] A. Hyvarinen. Sparse code shrinkage: Denoising of nongaussian data by maximum likelihood estimation. Neural Computation, 11(7):1739–1768, 1999.</p>
<p>[4] A. Hyvarinen, P.O. Hoyer, and M. Inki. Topographic independent component analysis. Neural Computation, 13(7):1525–1558, 2001.</p>
<p>[5] S. Della Pietra, V.J. Della Pietra, and J.D. Lafferty. Inducing features of random ﬁelds. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(4):380–393, 1997.</p>
<p>[6] E.P. Simoncelli. Modeling the joint statistics of images in the wavelet domain. In Proc SPIE, 44th Annual Meeting, volume 3813, pages 188–195, Denver, 1999.</p>
<p>[7] V. Strela, J. Portilla, and E. Simoncelli. Image denoising using a local Gaussian scale mixture model in the wavelet domain. In Proc. SPIE, 45th Annual Meeting, San Diego, 2000.</p>
<p>[8] M.J. Wainwright and E.P. Simoncelli. Scale mixtures of Gaussians and the statistics of natural images. In Advances Neural Information Processing Systems, volume 12, pages 855–861, 2000.</p>
<p>[9] S.C. Zhu, Z.N. Wu, and D. Mumford. Minimax entropy principle and its application to texture modeling. Neural Computation, 9(8):1627–1660, 1997.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
