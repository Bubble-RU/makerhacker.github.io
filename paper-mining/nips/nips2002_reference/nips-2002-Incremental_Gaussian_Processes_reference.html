<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 nips-2002-Incremental Gaussian Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-110" href="../nips2002/nips-2002-Incremental_Gaussian_Processes.html">nips2002-110</a> <a title="nips-2002-110-reference" href="#">nips2002-110-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 nips-2002-Incremental Gaussian Processes</h1>
<br/><p>Source: <a title="nips-2002-110-pdf" href="http://papers.nips.cc/paper/2173-incremental-gaussian-processes.pdf">pdf</a></p><p>Author: Joaquin Quiñonero-candela, Ole Winther</p><p>Abstract: In this paper, we consider Tipping’s relevance vector machine (RVM) [1] and formalize an incremental training strategy as a variant of the expectation-maximization (EM) algorithm that we call Subspace EM (SSEM). Working with a subset of active basis functions, the sparsity of the RVM solution will ensure that the number of basis functions and thereby the computational complexity is kept low. We also introduce a mean ﬁeld approach to the intractable classiﬁcation model that is expected to give a very good approximation to exact Bayesian inference and contains the Laplace approximation as a special case. We test the algorithms on two large data sets with O(103 − 104 ) examples. The results indicate that Bayesian learning of large data sets, e.g. the MNIST database is realistic.</p><br/>
<h2>reference text</h2><p>[1] Michael E. Tipping, “Sparse bayesian learning and the relevance vector machine,” Journal of Machine Learning Research, vol. 1, pp. 211–244, 2001.</p>
<p>[2] Vladimir N. Vapnik, Statistical Learning Theory, Wiley, New York, 1998.</p>
<p>[3] Bernhard Sch¨ lkopf and Alex J. Smola, Learning with Kernels, MIT Press, Cambridge, 2002. o</p>
<p>[4] Carl E. Rasmussen, Evaluation of Gaussian Processes and Other Methods for Non-linear Regression, Ph.D. thesis, Dept. of Computer Science, University of Toronto, 1996.</p>
<p>[5] Chris K. I. Williams and Carl E. Rasmussen, “Gaussian Proceses for Regression,” in Advances in Neural Information Processing Systems, 1996, number 8, pp. 514–520.</p>
<p>[6] D. J. C. Mackay, “Gaussian Processes: A replacement for supervised Neural Networks?,” Tech. Rep., Cavendish Laboratory, Cambridge University, 1997, Notes for a tutorial at NIPS 1997.</p>
<p>[7] Radford M. Neal, Bayesian Learning for Neural Networks, Springer, New York, 1996.</p>
<p>[8] Manfred Opper and Ole Winther, “Gaussian processes for classiﬁcation: Mean ﬁeld algorithms,” Neural Computation, vol. 12, pp. 2655–2684, 2000.</p>
<p>[9] Michael Tipping and Anita Faul, “Fast marginal likelihood maximisation for sparse bayesian models,” in International Workshop on Artiﬁcial Intelligence and Statistics, 2003.</p>
<p>[10] N. M. Dempster, A.P. Laird, and D. B. Rubin, “Maximum likelihood from incomplete data via the EM algorithm,” J. R. Statist. Soc. B, vol. 39, pp. 185–197, 1977.</p>
<p>[11] Chris Williams and Mathias Seeger, “Using the Nystr¨ m method to speed up kernel machines,” o in Advances in Neural Information Processing Systems, 2001, number 13, pp. 682–688.</p>
<p>[12] Alex J. Smola and Peter L. Bartlett, “Sparse greedy gaussian process regression,” in Advances in Neural Information Processing Systems, 2001, number 13, pp. 619–625.</p>
<p>[13] Lehel Csat´ and Manfred Opper, “Sparse representation for gaussian process models,” in o Advances in Neural Information Processing Systems, 2001, number 13, pp. 444–450.</p>
<p>[14] Volker Tresp, “Mixtures of gaussian processes,” in Advances in Neural Information Processing Systems, 2000, number 12, pp. 654–660.</p>
<p>[15] Carl E. Rasmussen and Zoubin Ghahramani, “Inﬁnite mixtures of gaussian process experts,” in Advances in Neural Information Processing Systems, 2002, number 14.</p>
<p>[16] Joaquin Qui˜ onero-Candela and Lars Kai Hansen, “Time series prediction based on the relen vance vector machine with adaptive kernels,” in International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2002.</p>
<p>[17] Michael E. Tipping, “The relevance vector machine,” in Advances in Neural Information Processing Systems, 2000, number 12, pp. 652–658.</p>
<p>[18] David J. C. MacKay, “Bayesian interpolation,” Neural Computation, vol. 4, no. 3, pp. 415–447, 1992.</p>
<p>[19] Claus Svarer, Lars K. Hansen, Jan Larsen, and Carl E. Rasmussen, “Designer networks for time series processing,” in IEEE NNSP Workshop, 1993, pp. 78–87.</p>
<p>[20] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning applied to document recognition,” in Poceedings of the IEEE, 1998, vol. 86, pp. 2278–2324.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
