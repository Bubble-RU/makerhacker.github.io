<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>69 nips-2002-Discriminative Learning for Label Sequences via Boosting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-69" href="../nips2002/nips-2002-Discriminative_Learning_for_Label_Sequences_via_Boosting.html">nips2002-69</a> <a title="nips-2002-69-reference" href="#">nips2002-69-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>69 nips-2002-Discriminative Learning for Label Sequences via Boosting</h1>
<br/><p>Source: <a title="nips-2002-69-pdf" href="http://papers.nips.cc/paper/2333-discriminative-learning-for-label-sequences-via-boosting.pdf">pdf</a></p><p>Author: Yasemin Altun, Thomas Hofmann, Mark Johnson</p><p>Abstract: This paper investigates a boosting approach to discriminative learning of label sequences based on a sequence rank loss function. The proposed method combines many of the advantages of boosting schemes with the efficiency of dynamic programming methods and is attractive both, conceptually and computationally. In addition, we also discuss alternative approaches based on the Hamming loss for label sequences. The sequence boosting algorithm offers an interesting alternative to methods based on HMMs and the more recently proposed Conditional Random Fields. Applications areas for the presented technique range from natural language processing and information extraction to computational biology. We include experiments on named entity recognition and part-of-speech tagging which demonstrate the validity and competitiveness of our approach. 1</p><br/>
<h2>reference text</h2><p>[1] M. Collins. Discriminative reranking for natural language parsing. In Proceedings 17th International Conference on Machine Learning, pages 175- 182. Morgan Kaufmann , San Francisco , CA, 2000.</p>
<p>[2] M. Collins. Ranking algorithms for named- entity extraction: Boosting and the voted perceptron. In Proceedings 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 489- 496, 2002.</p>
<p>[3] R. Durbin , S. Eddy, A. Krogh, and G. Mitchison. Biological Sequence Analysis: Probabilistic Models of Proteins and Nucleic Acids. Cambridge University Press, 1998.</p>
<p>[4] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic regression: a statistical view of boosting. Annals of Statistics, 28:337- 374, 2000.</p>
<p>[5] S. Kakade, Y.W. Teh, and S. Roweis. An alternative objective function for Markovian fields. In Proceedings 19th International Conference on Machine Learning, 2002.</p>
<p>[6] J . Lafferty, A. McCallum, and F . Pereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proc. 18th International Conf. on Machine Learning, pages 282- 289. Morgan Kaufmann, San Francisco, CA, 200l.</p>
<p>[7] C. Manning and H. Schiitze. Foundations of Statistical Natural Language Processing. MIT Press, 1999.</p>
<p>[8] T. Minka. Algorithms for maximum-likelihood logistic regression. Technical report , CMU, Department of Statistics, TR 758 , 200l.</p>
<p>[9] R. Schapire and Y. Singer. Improved boosting algorithms using confidence-rated predictions. Machine Learning, 37(3):297- 336, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
