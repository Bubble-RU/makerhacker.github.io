<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-88" href="../nips2002/nips-2002-Feature_Selection_and_Classification_on_Matrix_Data%3A_From_Large_Margins_to_Small_Covering_Numbers.html">nips2002-88</a> <a title="nips-2002-88-reference" href="#">nips2002-88-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>88 nips-2002-Feature Selection and Classification on Matrix Data: From Large Margins to Small Covering Numbers</h1>
<br/><p>Source: <a title="nips-2002-88-pdf" href="http://papers.nips.cc/paper/2321-feature-selection-and-classification-on-matrix-data-from-large-margins-to-small-covering-numbers.pdf">pdf</a></p><p>Author: Sepp Hochreiter, Klaus Obermayer</p><p>Abstract: We investigate the problem of learning a classiﬁcation task for datasets which are described by matrices. Rows and columns of these matrices correspond to objects, where row and column objects may belong to diﬀerent sets, and the entries in the matrix express the relationships between them. We interpret the matrix elements as being produced by an unknown kernel which operates on object pairs and we show that - under mild assumptions - these kernels correspond to dot products in some (unknown) feature space. Minimizing a bound for the generalization error of a linear classiﬁer which has been obtained using covering numbers we derive an objective function for model selection according to the principle of structural risk minimization. The new objective function has the advantage that it allows the analysis of matrices which are not positive deﬁnite, and not even symmetric or square. We then consider the case that row objects are interpreted as features. We suggest an additional constraint, which imposes sparseness on the row objects and show, that the method can then be used for feature selection. Finally, we apply this method to data obtained from DNA microarrays, where “column” objects correspond to samples, “row” objects correspond to genes and matrix elements correspond to expression levels. Benchmarks are conducted using standard one-gene classiﬁcation and support vector machines and K-nearest neighbors after standard feature selection. Our new method extracts a sparse set of genes and provides superior classiﬁcation results. 1</p><br/>
<h2>reference text</h2><p>[1] B. E. Boser, I. M. Guyon, and V. N. Vapnik. A training algorithm for optimal margin classiﬁers. In Proc. of the 5th Annual ACM Workshop on Computational Learning Theory, pages 144–152. ACM Press, Pittsburgh, PA, 1992.</p>
<p>[2] C. Cortes and V. N. Vapnik. Support vector networks. Machine Learning, 20:273–297, 1995.</p>
<p>[3] R. Golub, D. K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. P. Mesirov, H. Coller, M. Loh, J. R. Downing, M. A. Caligiuri, C. D. Bloomﬁeld, and E. S. Lander. Molecular classiﬁcation of cancer: Class discovery and class prediction by gene expression monitoring. Science, 286(5439):531–537, 1999.</p>
<p>[4] T. Graepel, R. Herbrich, P. Bollmann-Sdorra, and K. Obermayer. Classiﬁcation on pairwise proximity data. In NIPS 11, pages 438–444, 1999.</p>
<p>[5] T. Graepel, R. Herbrich, B. Sch¨lkopf, A. J. Smola, P. L. Bartlett, K.-R. o M¨ller, K. Obermayer, and R. C. Williamson. Classiﬁcation on proximity data u with LP–machines. In ICANN 99, pages 304–309, 1999.</p>
<p>[6] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for cancer classiﬁcation using support vector machines. Mach. Learn., 46:389–422, 2002.</p>
<p>[7] S. Hochreiter and K. Obermayer. Classiﬁcation of pairwise proximity data with support vectors. In The Learning Workshop. Y. LeCun and Y. Bengio, 2002.</p>
<p>[8] T. Hofmann and J. Buhmann. Pairwise data clustering by deterministic annealing. IEEE Trans. on Pat. Analysis and Mach. Intell., 19(1):1–14, 1997.</p>
<p>[9] S. L. Pomeroy, P. Tamayo, M. Gaasenbeek, L. M. Sturla, M. Angelo, M. E. McLaughlin, J. Y. H. Kim, L. C. Goumnerova, P. M. Black, C. Lau, J. C. Allen, D. Zagzag, J. M. Olson, T. Curran, C. Wetmore, J. A. Biegel, T. Poggio, S. Mukherjee, R. Rifkin, A. Califano, G. Stolovitzky, D. N. Louis, J. P. Mesirov, E. S. Lander, and T. R. Golub. Prediction of central nervous system embryonal tumour outcome based on gene expression. Nature, 415(6870):436–442, 2002.</p>
<p>[10] V. Roth, J. Buhmann, and J. Laub. Pairwise clustering is equivalent to classical k-means. In The Learning Workshop. Y. LeCun and Y. Bengio, 2002.</p>
<p>[11] B. Sch¨lkopf and A. J. Smola. Learning with kernels — Support Vector Mao chines, Reglarization, Optimization, and Beyond. MIT Press, Cambridge, 2002.</p>
<p>[12] J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and M. Anhtony. A framework for structural risk minimisation. In Comp. Learn. Th., pages 68–76, 1996.</p>
<p>[13] J. Shawe-Taylor, P. L. Bartlett, R. C. Williamson, and M. Anhtony. Structural risk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory, 44:1926–1940, 1998.</p>
<p>[14] J. Shawe-Taylor and N. Cristianini. On the generalisation of soft margin algorithms. Technical Report NC2-TR-2000-082, NeuroCOLT2, Department of Computer Science, Royal Holloway, University of London, 2000.</p>
<p>[15] V. Vapnik. The nature of statistical learning theory. Springer, NY, 1995.</p>
<p>[16] J. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, and V. Vapnik. Feature selection for SVMs. In NIPS 12, pages 668–674, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
