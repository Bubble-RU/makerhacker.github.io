<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-73" href="../nips2002/nips-2002-Dynamic_Bayesian_Networks_with_Deterministic_Latent_Tables.html">nips2002-73</a> <a title="nips-2002-73-reference" href="#">nips2002-73-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>73 nips-2002-Dynamic Bayesian Networks with Deterministic Latent Tables</h1>
<br/><p>Source: <a title="nips-2002-73-pdf" href="http://papers.nips.cc/paper/2343-dynamic-bayesian-networks-with-deterministic-latent-tables.pdf">pdf</a></p><p>Author: David Barber</p><p>Abstract: The application of latent/hidden variable Dynamic Bayesian Networks is constrained by the complexity of marginalising over latent variables. For this reason either small latent dimensions or Gaussian latent conditional tables linearly dependent on past states are typically considered in order that inference is tractable. We suggest an alternative approach in which the latent variables are modelled using deterministic conditional probability tables. This specialisation has the advantage of tractable inference even for highly complex non-linear/non-Gaussian visible conditional probability tables. This approach enables the consideration of highly complex latent dynamics whilst retaining the beneﬁts of a tractable probabilistic model. 1</p><br/>
<h2>reference text</h2><p>[1] C.M. Bishop, Neural Networks for Pattern Recognition, Oxford University Press, 1995.</p>
<p>[2] H.A. Bourlard and N. Morgan, Connectionist Speech Recognition. A Hybrid Approach., Kluwer, 1994.</p>
<p>[3] A. Doucet, N. de Freitas, and N. J. Gordon, Sequential Monte Carlo Methods in Practice, Springer, 2001.</p>
<p>[4] J. Hertz, A. Krogh, and R. Palmer, Introduction to the theory of neural computation., Addison-Wesley, 1991.</p>
<p>[5] M. I. Jordan, Learning in Graphical Models, MIT Press, 1998.</p>
<p>[6] J.F. Kolen and S.C. Kramer, Dynamic Recurrent Networks, IEEE Press, 2001.</p>
<p>[7] A. Krogh and S.K. Riis, Hidden Neural Networks, Neural Computation 11 (1999), 541–563.</p>
<p>[8] M. Kudo, J. Toyama, and M. Shimbo, Multidimensional Curve Classiﬁcation Using Passing-Through Regions, Pattern Recognition Letters 20 (1999), no. 11-13, 1103– 1111.</p>
<p>[9] L.R. Rabiner and B.H. Juang, An introduction to hidden Markov models, IEEE Transactions on Acoustics Speech, Signal Processing 3 (1986), no. 1, 4–16.</p>
<p>[10] M. West and J. Harrison, Bayesian forecasting and dynamic models, Springer, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
