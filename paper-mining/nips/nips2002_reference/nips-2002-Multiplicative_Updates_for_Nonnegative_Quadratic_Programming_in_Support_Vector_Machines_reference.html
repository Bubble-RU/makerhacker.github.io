<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-151" href="../nips2002/nips-2002-Multiplicative_Updates_for_Nonnegative_Quadratic_Programming_in_Support_Vector_Machines.html">nips2002-151</a> <a title="nips-2002-151-reference" href="#">nips2002-151-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>151 nips-2002-Multiplicative Updates for Nonnegative Quadratic Programming in Support Vector Machines</h1>
<br/><p>Source: <a title="nips-2002-151-pdf" href="http://papers.nips.cc/paper/2280-multiplicative-updates-for-nonnegative-quadratic-programming-in-support-vector-machines.pdf">pdf</a></p><p>Author: Fei Sha, Lawrence K. Saul, Daniel D. Lee</p><p>Abstract: We derive multiplicative updates for solving the nonnegative quadratic programming problem in support vector machines (SVMs). The updates have a simple closed form, and we prove that they converge monotonically to the solution of the maximum margin hyperplane. The updates optimize the traditionally proposed objective function for SVMs. They do not involve any heuristics such as choosing a learning rate or deciding which variables to update at each iteration. They can be used to adjust all the quadratic programming variables in parallel with a guarantee of improvement at each iteration. We analyze the asymptotic convergence of the updates and show that the coefﬁcients of non-support vectors decay geometrically to zero at a rate that depends on their margins. In practice, the updates converge very rapidly to good classiﬁers.</p><br/>
<h2>reference text</h2><p>[1] L. Baum. An inequality and associated maximization technique in statistical estimation of probabilistic functions of Markov processes. Inequalities, 3:1–8, 1972.</p>
<p>[2] C. L. Blake and C. J. Merz. UCI repository of machine learning databases, 1998.</p>
<p>[3] C. J. C. Burges. A tutorial on support vector machines for pattern recognition. Knowledge Discovery and Data Mining, 2(2):121–167, 1998.</p>
<p>[4] M. Collins, R. Schapire, and Y. Singer. Logistic regression, adaBoost, and Bregman distances. In Proceedings of the Thirteenth Annual Conference on Computational Learning Theory, 2000.</p>
<p>[5] N. Cristianini, C. Campbell, and J. Shawe-Taylor. Multiplicative updatings for support vector machines. In Proceedings of ESANN’99, pages 189–194, 1999.</p>
<p>[6] J. N. Darroch and D. Ratcliff. Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics, 43:1470–1480, 1972.</p>
<p>[7] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39:1–37, 1977.</p>
<p>[8] C. Gentile. A new approximate maximal margin classiﬁcation algorithm. Journal of Machine Learning Research, 2:213–242, 2001.</p>
<p>[9] R. P. Gorman and T. J. Sejnowski. Analysis of hidden units in a layered network trained to classify sonar targets. Neural Networks, 1(1):75–89, 1988.</p>
<p>[10] J. Kivinen and M. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1–63, 1997.</p>
<p>[11] D. D. Lee and H. S. Seung. Learning the parts of objects with nonnegative matrix factorization. Nature, 401:788–791, 1999.</p>
<p>[12] D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. In T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances in Neural and Information Processing Systems, volume 13, Cambridge, MA, 2001. MIT Press.</p>
<p>[13] O. L. Mangasarian and D. R. Musicant. Lagrangian support vector machines. Journal of Machine Learning Research, 1:161–177, 2001.</p>
<p>[14] O. L. Mangasarian and W. H. Wolberg. Cancer diagnosis via linear programming. SIAM News, 23(5):1–18, 1990.</p>
<p>[15] J. Platt. Fast training of support vector machines using sequential minimal optimization. In B. Sch¨ lkopf, C. J. C. Burges, and A. J. Smola, editors, Advances in Kernel Methods — Support o Vector Learning, pages 185–208, Cambridge, MA, 1999. MIT Press.</p>
<p>[16] L. K. Saul and D. D. Lee. Multiplicative updates for classiﬁcation by mixture models. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural and Information Processing Systems, volume 14, Cambridge, MA, 2002. MIT Press.</p>
<p>[17] B. Sch¨ lkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002. o</p>
<p>[18] V. Vapnik. Statistical Learning Theory. Wiley, N.Y., 1998.  A  Proof of Theorem 1  The proof of monotonic convergence in the objective function F (v), eq. (1), is based on the derivation of an auxiliary function. Similar techniques have been used for many models in statistical learning[1, 4, 6, 7, 12, 16]. An auxiliary function G(˜ , v) has the two crucial v ˜ properties that F (˜ ) ≤ G(˜ , v) and F (v) = G(v, v) for all nonnegative v,v. From such v v an auxiliary function, we can derive the update rule v = arg minv G(˜ , v) which never v ˜ increases (and generally decreases) the objective function F (v): F (v ) ≤ G(v , v) ≤ G(v, v) = F (v).  (10)  By iterating this procedure, we obtain a series of estimates that improve the objective func˜ tion. For nonnegative quadratic programming, we derive an auxiliary function G( v, v) by decomposing F (v) in eq. (1) into three terms and then bounding each term separately: F (v) G(˜ , v) v  = =  1 2 1 2  A+ vi vj − ij ij  i  1 2  A− vi vj + ij ij  (A+ v)i 2 1 vi − ˜ vi 2  bi vi ,  (11)  i  A− vi vj ij ij  1 + log  vi vj ˜˜ vi vj  +  bi vi . (12) ˜ i  It can be shown that F (˜ ) ≤ G(˜ , v). The minimization of G(˜ , v) is performed by v v v setting its derivative to zero, leading to the multiplicative updates in eq. (3). The updates  move each element vi in the same direction as −∂F/∂vi , with ﬁxed points occurring only ∗ if vi = 0 or ∂F/∂vi = 0. Since the overall optimization is convex, all minima of F (v) are global minima. The updates converge to the unique global minimum if it exists.  B  Proof of Theorem 2  The proof of the bound on the asymptotic rate of convergence relies on the repeated use of equalities and inequalities that hold at the ﬁxed point α∗ . For example, if α∗ = 0 is a i non-support vector coefﬁcient, then (∂L/∂αi )|α∗ ≥ 0 implies (A+ α∗ )i −(A− α∗ )i ≥ 1. As + − shorthand, let zi = (A+ α∗ )i and zi = (A− α∗ )i . Then we have the following result: + 1 2zi = (13) γi 1 + 1 + 4z + z − i  i  + 2zi  ≥ 1+  (14)  + − + − (zi − zi )2 + 4zi zi  − + + 2zi zi − z i − 1 + − = 1+ + − 1 + z i + zi zi + z i + 1  =  (15)  + − zi − z i − 1 . (16) + 2zi To prove the theorem, we need to express this result in terms of kernel dot products. We can rewrite the variables in the numerator of eq. (16) as:  ≥ 1+  + − zi − z i =  Aij α∗ = j  yi yj K(xi , xj )α∗ = yi K(xi , w) = |K(xi , w)|, (17) j  j  j  where w = j α∗ xj yj is the normal vector to the maximum margin hyperplane. Likewise, j we can obtain a bound on the denominator of eq. (16) by: + zi  A + α∗ ij j  =  (18)  j  α∗ j  ≤ max A+ ik k  (19)  j  α∗ j  ≤ max |K(xi , xk )| k  K(xi , xi ) max  ≤  α∗ j  K(xk , xk )  k  (21)  j  K(xi , xi ) max  =  (20)  j  K(xk , xk )K(w, w).  k  (22)  Eq. (21) is an application of the Cauchy-Schwartz inequality for kernels, while eq. (22) exploits the observation that: Ajk α∗ α∗ = j k  K(w, w) = jk  α∗ j j  Ajk α∗ = k k  α∗ . j  (23)  j  The last step in eq. (23) is obtained by recognizing that α∗ is nonzero only for the coefﬁj cients of support vectors, and that in this case the optimality condition (∂L/∂α j )|α∗ = 0 implies k Ajk α∗ = 1. Finally, substituting eqs. (17) and (22) into eq. (16) gives: k 1 |K(xi , w)| − 1 ≥ 1+ . (24) γi 2 K(xi , xi ) maxk K(xk , xk )K(w, w) This reduces in a straightforward way to the claim of the theorem.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
