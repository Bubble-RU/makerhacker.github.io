<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>139 nips-2002-Margin-Based Algorithms for Information Filtering</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-139" href="../nips2002/nips-2002-Margin-Based_Algorithms_for_Information_Filtering.html">nips2002-139</a> <a title="nips-2002-139-reference" href="#">nips2002-139-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>139 nips-2002-Margin-Based Algorithms for Information Filtering</h1>
<br/><p>Source: <a title="nips-2002-139-pdf" href="http://papers.nips.cc/paper/2154-margin-based-algorithms-for-information-filtering.pdf">pdf</a></p><p>Author: Nicolò Cesa-bianchi, Alex Conconi, Claudio Gentile</p><p>Abstract: In this work, we study an information ﬁltering model where the relevance labels associated to a sequence of feature vectors are realizations of an unknown probabilistic linear function. Building on the analysis of a restricted version of our model, we derive a general ﬁltering rule based on the margin of a ridge regression estimator. While our rule may observe the label of a vector only by classfying the vector as relevant, experiments on a real-world document ﬁltering problem show that the performance of our rule is close to that of the on-line classiﬁer which is allowed to observe all labels. These empirical results are complemented by a theoretical analysis where we consider a randomized variant of our rule and prove that its expected number of mistakes is never much larger than that of the optimal ﬁltering rule which knows the hidden linear model.</p><br/>
<h2>reference text</h2><p>[1] Abe, N., and Long, P.M. (1999). Associative reinforcement learning using linear probabilistic concepts. In Proc. ICML’99, Morgan Kaufmann.</p>
<p>[2] Auer, P. (2000). Using Upper Conﬁdence Bounds for Online Learning. In Proc. FOCS’00, IEEE, pages 270–279.</p>
<p>[3] Azoury, K., and Warmuth, M.K. (2001). Relative loss bounds for on-line density estimation with the exponential family of distributions, Machine Learning, 43:211–246.</p>
<p>[4] Censor, Y., and Lent, A. (1981). An iterative row-action method for interval convex programming. Journal of Optimization Theory and Applications, 34(3), 321–353.</p>
<p>[5] Cesa-Bianchi, N. (1999). Analysis of two gradient-based algorithms for on-line regression. Journal of Computer and System Sciences, 59(3):392–411.</p>
<p>[6] Cesa-Bianchi, N., Conconi, A., and Gentile, C. (2002). A second-order Perceptron algorithm. In Proc. COLT’02, pages 121–137. LNAI 2375, Springer.</p>
<p>[7] Cesa-Bianchi, N., Long, P.M., and Warmuth, M.K. (1996). Worst-case quadratic loss bounds for prediction using linear functions and gradient descent. IEEE Trans. NN, 7(3):604–619.</p>
<p>[8] Gavald` , R., and Watanabe, O. (2001). Sequential sampling algorithms: Uniﬁed analysis and a lower bounds. In Proc. SAGA’01, pages 173–187. LNCS 2264, Springer.</p>
<p>[9] Helmbold, D.P., Littlestone, N., and Long, P.M. (2000). Apple tasting. Information and Computation, 161(2):85–139.</p>
<p>[10] Herbster, M. and Warmuth, M.K. (1998). Tracking the best regressor, in Proc. COLT’98, ACM, pages 24–31.</p>
<p>[11] Hoeffding, W. (1963). Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13–30.</p>
<p>[12] Hoerl, A., and Kennard, R. (1970). Ridge regression: biased estimation for nonorthogonal problems. Technometrics, 12:55–67.</p>
<p>[13] Vapnik, V. (1998). Statistical learning theory. New York: J. Wiley & Sons.</p>
<p>[14] Voorhees, E., Harman, D. (2001). The tenth Text REtrieval Conference. TR 500-250, NIST.</p>
<p>[15] Vovk, V. (2001). Competitive on-line statistics. International Statistical Review, 69:213–248.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
