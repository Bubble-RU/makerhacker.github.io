<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>201 nips-2002-Transductive and Inductive Methods for Approximate Gaussian Process Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-201" href="../nips2002/nips-2002-Transductive_and_Inductive_Methods_for_Approximate_Gaussian_Process_Regression.html">nips2002-201</a> <a title="nips-2002-201-reference" href="#">nips2002-201-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>201 nips-2002-Transductive and Inductive Methods for Approximate Gaussian Process Regression</h1>
<br/><p>Source: <a title="nips-2002-201-pdf" href="http://papers.nips.cc/paper/2230-transductive-and-inductive-methods-for-approximate-gaussian-process-regression.pdf">pdf</a></p><p>Author: Anton Schwaighofer, Volker Tresp</p><p>Abstract: Gaussian process regression allows a simple analytical treatment of exact Bayesian inference and has been found to provide good performance, yet scales badly with the number of training data. In this paper we compare several approaches towards scaling Gaussian processes regression to large data sets: the subset of representers method, the reduced rank approximation, online Gaussian processes, and the Bayesian committee machine. Furthermore we provide theoretical insight into some of our experimental results. We found that subset of representers methods can give good and particularly fast predictions for data sets with high and medium noise levels. On complex low noise data sets, the Bayesian committee machine achieves signiﬁcantly better accuracy, yet at a higher computational cost.</p><br/>
<h2>reference text</h2><p>[1] Blake, C. and Merz, C. UCI repository of machine learning databases. 1998.</p>
<p>[2] Csat´ , L. and Opper, M. Sparse online gaussian processes. Neural Computation, 14(3):641– o 668, 2002.</p>
<p>[3] Leen, T. K., Dietterich, T. G., and Tresp, V., eds. Advances in Neural Information Processing Systems 13. MIT Press, 2001.</p>
<p>[4] MacKay, D. J. Introduction to Gaussian processes. In C. M. Bishop, ed., Neural Networks and Machine Learning, vol. 168 of NATO Asi Series. Series F, Computer and Systems Sciences. Springer Verlag, 1998.</p>
<p>[5] Rasmussen, C. E. Reduced rank Gaussian process learning, 2002. Unpublished Manuscript.</p>
<p>[6] Smola, A. and Sch¨ olkopf, B. Sparse greedy matrix approximation for machine learning. In P. Langely, ed., Proceedings of ICML00. Morgan Kaufmann, 2000.</p>
<p>[7] Smola, A. J. and Bartlett, P. Sparse greedy gaussian process regression. In [3], pp. 619–625.</p>
<p>[8] Tresp, V. A Bayesian committee machine. Neural Computation, 12(11):2719–2741, 2000.</p>
<p>[9] Tresp, V. The generalized bayesian committee machine. In Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 130–139. Boston, MA USA, 2000.</p>
<p>[10] Vapnik, V. N. The nature of statistical learning theory. Springer Verlag, 1995.</p>
<p>[11] Williams, C. K., Rasmussen, C. E., Schwaighofer, A., and Tresp, V. Observations on the Nystr¨ method for Gaussian process prediction. Tech. rep., Available from the authors’ web om pages, 2002.</p>
<p>[12] Williams, C. K. I. and Seeger, M. Using the nystr¨ method to speed up kernel machines. In om</p>
<p>[3], pp. 682–688.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
