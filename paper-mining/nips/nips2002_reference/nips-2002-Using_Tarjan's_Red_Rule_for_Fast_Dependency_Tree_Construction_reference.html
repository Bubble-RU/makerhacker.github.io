<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-203" href="../nips2002/nips-2002-Using_Tarjan%27s_Red_Rule_for_Fast_Dependency_Tree_Construction.html">nips2002-203</a> <a title="nips-2002-203-reference" href="#">nips2002-203-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>203 nips-2002-Using Tarjan's Red Rule for Fast Dependency Tree Construction</h1>
<br/><p>Source: <a title="nips-2002-203-pdf" href="http://papers.nips.cc/paper/2281-using-tarjans-red-rule-for-fast-dependency-tree-construction.pdf">pdf</a></p><p>Author: Dan Pelleg, Andrew W. Moore</p><p>Abstract: We focus on the problem of efﬁcient learning of dependency trees. It is well-known that given the pairwise mutual information coefﬁcients, a minimum-weight spanning tree algorithm solves this problem exactly and in polynomial time. However, for large data-sets it is the construction of the correlation matrix that dominates the running time. We have developed a new spanning-tree algorithm which is capable of exploiting partial knowledge about edge weights. The partial knowledge we maintain is a probabilistic conﬁdence interval on the coefﬁcients, which we derive by examining just a small sample of the data. The algorithm is able to ﬂag the need to shrink an interval, which translates to inspection of more data for the particular attribute pair. Experimental results show running time that is near-constant in the number of records, without signiﬁcant loss in accuracy of the generated trees. Interestingly, our spanning-tree algorithm is based solely on Tarjan’s red-edge rule, which is generally considered a guaranteed recipe for bad performance. 1</p><br/>
<h2>reference text</h2><p>[1] Nir Friedman, Iftach Nachman, and Dana Pe´ r. Learning bayesian network struce ture from massive datasets: The ”sparse candidate” algorithm. In Proceedings of the 15th Conference on Uncertainty in Artiﬁcial Intelligence (UAI-99), pages 206–215, Stockholm, Sweden, 1999.</p>
<p>[2] C. K. Chow and C. N. Liu. Approximating discrete probability distributions with dependence trees. IEEE Transactions on Information Theory, 14:462–467, 1968.</p>
<p>[3] Marina Meila. Learning with Mixtures of Trees. PhD thesis, Massachusetts Institute of Technology, 1999.</p>
<p>[4] N. Friedman, M. Goldszmidt, and T. J. Lee. Bayesian Network Classiﬁcation with Continuous Attributes: Getting the Best of Both Discretization and Parametric Fitting. In Jude Shavlik, editor, International Conference on Machine Learning, 1998.</p>
<p>[5] Robert Endre Tarjan. Data structures and network algorithms, volume 44 of CBMSNSF Reg. Conf. Ser. Appl. Math. SIAM, 1983.</p>
<p>[6] Oded Maron and Andrew W. Moore. Hoeffding races: Accelerating model selection search for classiﬁcation and function approximation. In Jack D. Cowan, Gerald Tesauro, and Joshua Alspector, editors, Advances in Neural Information Processing Systems, volume 6, pages 59–66, Denver, Colorado, 1994. Morgan Kaufmann.</p>
<p>[7] Andrew W. Moore and Mary S. Lee. Efﬁcient algorithms for minimizing cross validation error. In Proceedings of the 11th International Conference on Machine Learning (ICML-94), pages 190–198. Morgan Kaufmann, 1994.</p>
<p>[8] Pedro Domingos and Geoff Hulten. Mining high-speed data streams. In Raghu Ramakrishnan, Sal Stolfo, Roberto Bayardo, and Ismail Parsa, editors, Proceedings of the 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-00), pages 71–80, N. Y., August 20–23 2000. ACM Press.</p>
<p>[9] Pedro Domingos and Geoff Hulten. A general method for scaling up machine learning algorithms and its application to clustering. In Carla Brodley and Andrea Danyluk, editors, Proceeding of the 17th International Conference on Machine Learning, San Francisco, CA, 2001. Morgan Kaufmann.</p>
<p>[10] Pedro Domingos and Geoff Hulten. Learning from inﬁnite data in ﬁnite time. In Proceedings of the 14th Neural Information Processing Systems (NIPS-2001), Vancouver, British Columbia, Canada, 2001.</p>
<p>[11] Marina Meila. An accelerated Chow and Liu algorithm: ﬁtting tree distributions to high dimensional sparse data. In Proceedings of the Sixteenth International Conference on Machine Learning (ICML-99), Bled, Slovenia, 1999.</p>
<p>[12] Fazlollah Reza. An Introduction to Information Theory, pages 282–283. Dover Publications, New York, 1994.</p>
<p>[13] Dan Pelleg and Andrew Moore. Using Tarjan’s red rule for fast dependency tree construction. Technical Report CMU-CS-02-116, Carnegie-Mellon University, 2002.</p>
<p>[14] C.L. Blake and C.J. Merz. UCI repository of machine learning databases, 1998. http://www.ics.uci.edu/∼mlearn/MLRepository.html.</p>
<p>[15] S. Hettich and S. D. Bay. The UCI KDD archive, 1999. http:// kdd.ics.uci.edu.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
