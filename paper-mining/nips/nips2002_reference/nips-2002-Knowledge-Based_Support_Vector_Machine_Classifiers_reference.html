<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-121" href="../nips2002/nips-2002-Knowledge-Based_Support_Vector_Machine_Classifiers.html">nips2002-121</a> <a title="nips-2002-121-reference" href="#">nips2002-121-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>121 nips-2002-Knowledge-Based Support Vector Machine Classifiers</h1>
<br/><p>Source: <a title="nips-2002-121-pdf" href="http://papers.nips.cc/paper/2222-knowledge-based-support-vector-machine-classifiers.pdf">pdf</a></p><p>Author: Glenn M. Fung, Olvi L. Mangasarian, Jude W. Shavlik</p><p>Abstract: Prior knowledge in the form of multiple polyhedral sets, each belonging to one of two categories, is introduced into a reformulation of a linear support vector machine classifier. The resulting formulation leads to a linear program that can be solved efficiently. Real world examples, from DNA sequencing and breast cancer prognosis, demonstrate the effectiveness of the proposed method. Numerical results show improvement in test set accuracy after the incorporation of prior knowledge into ordinary, data-based linear support vector machine classifiers. One experiment also shows that a linear classifier, based solely on prior knowledge, far outperforms the direct application of prior knowledge rules to classify data. Keywords: use and refinement of prior knowledge, support vector machines, linear programming 1</p><br/>
<h2>reference text</h2><p>[1] P. Auer. On learning from multi-instance examples: Empirical evaluation of a theoretical approach. pages 21- 29, 1987.</p>
<p>[2] P. S. Bradley and O. L. Mangasarian. Feature selection via concave minimization and support vector machines. In J. Shavlik, editor, Machine Learning Proceedings of the Fifteenth International Conference{ICML '98), pages 82-90, San</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]  Francisco, California, 1998. Morgan Kaufmann. ftp:/ /ftp.cs.wisc.edu/mathprog/ tech-reports / 98-03. ps. V. Cherkassky and F. Mulier. Learning from Data - Concepts, Theory and Methods. John Wiley & Sons, New York, 1998. S. Cost and S. Salzberg. A weighted nearest neighbor algorithm for learning with symbolic features. Machine Learning, 10:57-58, 1993. T. G. Dietterich, R. H. Lathrop, and T. Lozano-Perez. Solving the multipleinstance problem with axis-parallel rectangles. Artificial Intelligence, 89:31-71, 1998. G. Fung, O. L. Mangasarian, and J. Shavlik. Knowledge-based support vector machine classifiers. Technical Report 01-09, Data Mining Institute, Computer Sciences Department, University of Wisconsin, Madison, Wisconsin, November 2001. ftp:/ /ftp.cs.wisc.edu/pub/dmi/tech-reports/01-09.ps. F. Girosi and N. Chan. Prior knowledge and the creation of </p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
