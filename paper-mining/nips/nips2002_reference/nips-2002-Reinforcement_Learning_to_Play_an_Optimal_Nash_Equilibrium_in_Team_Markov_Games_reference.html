<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-175" href="../nips2002/nips-2002-Reinforcement_Learning_to_Play_an_Optimal_Nash_Equilibrium_in_Team_Markov_Games.html">nips2002-175</a> <a title="nips-2002-175-reference" href="#">nips2002-175-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>175 nips-2002-Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games</h1>
<br/><p>Source: <a title="nips-2002-175-pdf" href="http://papers.nips.cc/paper/2171-reinforcement-learning-to-play-an-optimal-nash-equilibrium-in-team-markov-games.pdf">pdf</a></p><p>Author: Xiaofeng Wang, Tuomas Sandholm</p><p>Abstract: Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conﬂicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the ﬁrst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm’s parameters are easy to set to meet the convergence conditions.</p><br/>
<h2>reference text</h2><p>C.Boutilier. Planning, learning and coordination in multi-agent decision processes. In TARK, 1996. C.Claus and C.Boutilier. The dynamics of reinforcement learning in cooperative multi-agent systems. In AAAI, 1998. D.Fudenberg and D.K.Levine. The theory of learning in games. MIT Press, 1998. D.L.Isaacson and R.W.Madsen. Markov chain: theory and applications. John Wiley and Sons, Inc, 1976. G.Wei . Learning to coordinate actions in multi-agent systems. In IJCAI, 1993. J.Hu and W.P.Wellman. Multiagent reinforcement learning: theoretical framework and an algorithm. In ICML, 1998. M.Kandori, G.J.Mailath, and R.Rob. Learning, mutation, and long run equilibria in games. Econometrica, 61(1):29–56, 1993. M.Littman. Friend-or-Foe Q-learning in general sum game. In ICML, 2001. M.L.Littman. Value-function reinforcement learning in markov games. J. of Cognitive System Research, 2:55–66, 2000. M.L.Purterman. Markov decision processes-discrete stochastic dynamic programming. John Wiley, 1994. M.Tan. Multi-agent reinforcement learning: independent vs. cooperative agents. In ICML, 1993. R.A.Howard. Dynamic programming and Markov processes. MIT Press, 1960. R. Selten. Spieltheoretische behandlung eines oligopolmodells mit nachfragetr¨ gheit. Zeitschrift f¨ r die gesamte Staatswisa u senschaft, 12:301–324, 1965. S. Singh, T.Jaakkola, M.L.Littman, and C.Szepesvari. Convergence results for single-step on-policy reinforcement learning algorithms. Machine Learning, 2000. S.Sen, M.Sekaran, and J. Hale. Learning to coordinate without sharing information. In AAAI, 1994. F. Thusijsman. Optimality and equilibrium in stochastic games. Centrum voor Wiskunde en Informatica, 1992. T.Sandholm and R.Crites. Learning in the iterated prisoner’s dilemma. Biosystems, 37:147–166, 1995. H. Young. The evolution of conventions. Econometrica, 61(1):57–84, 1993.      § ) 3 ¥ 1  § 1 ) 3 ¥ v  v    Theorem 3 requires Lemma 4, we do have  .  . If Condition (1) of our main theorem is satisﬁed (  v</p>
<p>[15]</p>
<p>[16]</p>
<p>[17]</p>
<p>[18]  q</p>
<p>[14]  § 1  9) 3 ¥</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]  ©</p>
<p>[1]</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]  ), then by</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
