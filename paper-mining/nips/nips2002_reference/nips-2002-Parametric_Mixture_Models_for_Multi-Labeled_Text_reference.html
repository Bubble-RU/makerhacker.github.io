<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2002" href="../home/nips2002_home.html">nips2002</a> <a title="nips-2002-162" href="../nips2002/nips-2002-Parametric_Mixture_Models_for_Multi-Labeled_Text.html">nips2002-162</a> <a title="nips-2002-162-reference" href="#">nips2002-162-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>162 nips-2002-Parametric Mixture Models for Multi-Labeled Text</h1>
<br/><p>Source: <a title="nips-2002-162-pdf" href="http://papers.nips.cc/paper/2244-parametric-mixture-models-for-multi-labeled-text.pdf">pdf</a></p><p>Author: Naonori Ueda, Kazumi Saito</p><p>Abstract: We propose probabilistic generative models, called parametric mixture models (PMMs), for multiclass, multi-labeled text categorization problem. Conventionally, the binary classiﬁcation approach has been employed, in which whether or not text belongs to a category is judged by the binary classiﬁer for every category. In contrast, our approach can simultaneously detect multiple categories of text using PMMs. We derive eﬃcient learning and prediction algorithms for PMMs. We also empirically show that our method could signiﬁcantly outperform the conventional binary methods when applied to multi-labeled text categorization using real World Wide Web pages. 1</p><br/>
<h2>reference text</h2><p>[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. to appear Advances in Neural Information Processing Systems 14. MIT Press.</p>
<p>[2] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society B, 39:1-38. 1977.</p>
<p>[3] S. T. Dumais, J. Platt, D. Heckerman, & M. Sahami. Inductive learning algorithms and representations for text categorization. In Proc. of ACM-CIKM’98, 1998.</p>
<p>[4] T. Joachims. Text categorization with support vector machines: Learning with many relevant features. In Proc. of the European Conference on Machine Learning, 137-142, Berlin, 1998.</p>
<p>[5] D. Lewis & M. Ringuette. A comparison of two learning algorithms for text categorization. In Third Anual Symposium on Document Analysis and Information Retrieval, 81-93. 1994.</p>
<p>[6] K. Morik, P. Brockhausen, and T. Joachims. Combining statistical learning with knowledge-based approach. A case study in intensive care monitoring. In Proc. of International Conference on Machine Learning (ICML’99), 1999.</p>
<p>[7] K. Nigam, A. K. McCallum, S. Thrun, & T. Mitchell. Text classiﬁcation from labeled and unlabeled documents using EM. Machine Learning, 39:103-134, 2000.</p>
<p>[8] Y. Yang & J. Pederson. A comparative study on feature selection in text categorization. In Proc of International Conference on Machine Learning, 412-420, 1997.</p>
<p>[9] V. N. Vapnik. Statistical learning theory. John Wiley & Sons, Inc., New York. 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
