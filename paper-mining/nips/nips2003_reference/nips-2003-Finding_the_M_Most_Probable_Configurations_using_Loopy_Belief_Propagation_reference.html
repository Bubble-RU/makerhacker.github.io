<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>74 nips-2003-Finding the M Most Probable Configurations using Loopy Belief Propagation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-74" href="../nips2003/nips-2003-Finding_the_M_Most_Probable_Configurations_using_Loopy_Belief_Propagation.html">nips2003-74</a> <a title="nips-2003-74-reference" href="#">nips2003-74-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>74 nips-2003-Finding the M Most Probable Configurations using Loopy Belief Propagation</h1>
<br/><p>Source: <a title="nips-2003-74-pdf" href="http://papers.nips.cc/paper/2349-finding-the-m-most-probable-configurations-using-loopy-belief-propagation.pdf">pdf</a></p><p>Author: Chen Yanover, Yair Weiss</p><p>Abstract: Loopy belief propagation (BP) has been successfully used in a number of diﬃcult graphical models to ﬁnd the most probable conﬁguration of the hidden variables. In applications ranging from protein folding to image analysis one would like to ﬁnd not just the best conﬁguration but rather the top M . While this problem has been solved using the junction tree formalism, in many real world problems the clique size in the junction tree is prohibitively large. In this work we address the problem of ﬁnding the M best conﬁgurations when exact inference is impossible. We start by developing a new exact inference algorithm for calculating the best conﬁgurations that uses only max-marginals. For approximate inference, we replace the max-marginals with the beliefs calculated using max-product BP and generalized BP. We show empirically that the algorithm can accurately and rapidly approximate the M best conﬁgurations in graphs with hundreds of variables. 1</p><br/>
<h2>reference text</h2><p>[1] A. Cano, S. Moral, and A. Salmer´n. Penniless propagation in join trees. Journal of o Intelligent Systems, 15:1010–1027, 2000.</p>
<p>[2] R. Cowell. Advanced inference in Bayesian networks. In M.I. Jordan, editor, Learning in Graphical Models. MIT Press, 1998.</p>
<p>[3] P. Dawid. Applications of a general propagation algorithm for probabilistic expert systems. Statistics and Computing, 2:25–36, 1992.</p>
<p>[4] R. Dechter and I. Rish. A scheme for approximating probabilistic inference. In Uncertainty in Artiﬁcial Intelligence (UAI 97), 1997.</p>
<p>[5] A. Doucet, N. de Freitas, K. Murphy, and S. Russell. Rao-blackwellised particle ﬁltering for dynamic bayesian networks. In Proceedings UAI 2000. Morgan Kaufmann, 2000.</p>
<p>[6] B.J. Frey, R. Koetter, and N. Petrovic. Very loopy belief propagation for unwrapping phase images. In Adv. Neural Information Processing Systems 14. MIT Press, 2001.</p>
<p>[7] T.S. Jaakkola and M.I. Jordan. Variational probabilistic inference and the QMR-DT database. JAIR, 10:291–322, 1999.</p>
<p>[8] Andrew R. Leach and Andrew P. Lemon. Exploring the conformational space of protein side chains using dead-end elimination and the A* algorithm. Proteins: Structure, Function, and Genetics, 33(2):227–239, 1998.</p>
<p>[9] A. Levin, A. Zomet, and Y. Weiss. Learning to perceive transparency from the statistics of natural scenes. In Proceedings NIPS 2002. MIT Press, 2002.</p>
<p>[10] Kevin Murphy. The bayes net toolbox for matlab. Computing Science and Statistics, 33, 2001.</p>
<p>[11] D. Nilsson. An eﬃcient algorithm for ﬁnding the M most probable conﬁgurations in probabilistic expert systems. Statistics and Computing, 8:159–173, 1998.</p>
<p>[12] Judea Pearl. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Morgan Kaufmann, 1988.</p>
<p>[13] L.R. Rabiner. A tutorial on hidden Markov models and selected applications in speech recognition. Proc. IEEE, 77(2):257–286, 1989.</p>
<p>[14] M. J. Wainwright, T. Jaakkola, and A. S. Willsky. Exact map estimates by (hyper)tree agreement. In Proceedings NIPS 2002. MIT Press, 2002.</p>
<p>[15] M. J. Wainwright, T. Jaakkola, and A. S. Willsky. Tree consistency and bounds on the performance of the max-product algorithm and its generalizations. Technical Report P-2554, MIT LIDS, 2002.</p>
<p>[16] Y. Weiss and W.T. Freeman. On the optimality of solutions of the max-product belief propagation algorithm in arbitrary graphs. IEEE Transactions on Information Theory, 47(2):723–735, 2001.</p>
<p>[17] C. Yanover and Y. Weiss. Approximate inference and protein folding. In Proceedings NIPS 2002. MIT Press, 2002.</p>
<p>[18] J. Yedidia, W. Freeman, and Y. Weiss. Understanding belief propagation and its generalizations. In G. Lakemeyer and B. Nebel, editors, Exploring Artiﬁcial Intelligence in the New Millennium. Morgan Kaufmann, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
