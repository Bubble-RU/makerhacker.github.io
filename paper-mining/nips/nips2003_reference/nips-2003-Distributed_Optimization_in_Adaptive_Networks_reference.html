<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>55 nips-2003-Distributed Optimization in Adaptive Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-55" href="../nips2003/nips-2003-Distributed_Optimization_in_Adaptive_Networks.html">nips2003-55</a> <a title="nips-2003-55-reference" href="#">nips2003-55-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>55 nips-2003-Distributed Optimization in Adaptive Networks</h1>
<br/><p>Source: <a title="nips-2003-55-pdf" href="http://papers.nips.cc/paper/2448-distributed-optimization-in-adaptive-networks.pdf">pdf</a></p><p>Author: Ciamac C. Moallemi, Benjamin V. Roy</p><p>Abstract: We develop a protocol for optimizing dynamic behavior of a network of simple electronic components, such as a sensor network, an ad hoc network of mobile devices, or a network of communication switches. This protocol requires only local communication and simple computations which are distributed among devices. The protocol is scalable to large networks. As a motivating example, we discuss a problem involving optimization of power consumption, delay, and buffer overﬂow in a sensor network. Our approach builds on policy gradient methods for optimization of Markov decision processes. The protocol can be viewed as an extension of policy gradient methods to a context involving a team of agents optimizing aggregate performance through asynchronous distributed communication and computation. We establish that the dynamics of the protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective. 1</p><br/>
<h2>reference text</h2><p>[1] P. L. Bartlett and J. Baxter. Stochastic Optimization of Controlled Markov Decision Processes. In IEEE Conference on Decision and Control, pages 124–129, 2000.</p>
<p>[2] P. L. Bartlett and J. Baxter. Estimation and Approximation Bounds for Gradient-Based Reinforcement Learning. Journal of Computer and System Sciences, 64:133–150, 2002.</p>
<p>[3] J. Baxter and P. L. Bartlett. Inﬁnite-Horizon Gradient-Based Policy Search. Journal of Artiﬁcial Intelligence Research, 15:319–350, 2001.</p>
<p>[4] J. Baxter, P. L. Bartlett, and L. Weaver. Inﬁnite-Horizon Gradient-Based Policy Search: II. Gradient Ascent Algorithms and Experiments. Journal of Artiﬁcial Intelligence Research, 15:351–381, 2001.</p>
<p>[5] T. Jaakkola, S. P. Singh, and M. I. Jordan. Reinforcement Learning Algorithms for Partially Observable Markov Decision Problems. In Advances in Neural Information Processing Systems 7, pages 345–352, 1995.</p>
<p>[6] H. J. Kushner and G. Yin. Stochastic Approximation Algorithms and Applications. SpringerVerlag, New York, NY, 1997.</p>
<p>[7] P. Marbach, O. Mihatsch, and J.N. Tsitsiklis. Call Admission Control and Routing in Integrated Service Networks. In IEEE Conference on Decision and Control, 1998.</p>
<p>[8] P. Marbach and J.N. Tsitsiklis. Simulation–Based Optimization of Markov Reward Processes. IEEE Transactions on Automatic Control, 46(2):191–209, 2001.</p>
<p>[9] C. C. Moallemi and B. Van Roy. Appendix to NIPS Submission. URL: http://www. moallemi.com/ciamac/papers/nips-2003-appendix.pdf, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
