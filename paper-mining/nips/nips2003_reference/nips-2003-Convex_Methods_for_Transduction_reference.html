<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>48 nips-2003-Convex Methods for Transduction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-48" href="../nips2003/nips-2003-Convex_Methods_for_Transduction.html">nips2003-48</a> <a title="nips-2003-48-reference" href="#">nips2003-48-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>48 nips-2003-Convex Methods for Transduction</h1>
<br/><p>Source: <a title="nips-2003-48-pdf" href="http://papers.nips.cc/paper/2507-convex-methods-for-transduction.pdf">pdf</a></p><p>Author: Tijl D. Bie, Nello Cristianini</p><p>Abstract: The 2-class transduction problem, as formulated by Vapnik [1], involves ﬁnding a separating hyperplane for a labelled data set that is also maximally distant from a given set of unlabelled test points. In this form, the problem has exponential computational complexity in the size of the working set. So far it has been attacked by means of integer programming techniques [2] that do not scale to reasonable problem sizes, or by local search procedures [3]. In this paper we present a relaxation of this task based on semideﬁnite programming (SDP), resulting in a convex optimization problem that has polynomial complexity in the size of the data set. The results are very encouraging for mid sized data sets, however the cost is still too high for large scale problems, due to the high dimensional search space. To this end, we restrict the feasible region by introducing an approximation based on solving an eigenproblem. With this approximation, the computational cost of the algorithm is such that problems with more than 1000 points can be treated. 1</p><br/>
<h2>reference text</h2><p>[1] V. N. Vapnik. Statistical Learning Theory. Springer, 1998.</p>
<p>[2] K. Bennett and A. Demiriz. Semi-supervised support vector machines. In M. S. Kearns, S. A. Solla, and D. A. Cohn, editors, Advances in Neural Information Processing Systems 11, Cambridge, MA, 1999. MIT Press.</p>
<p>[3] T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In Proc. of the International Conference on Machine Learning (ICML), 1999.</p>
<p>[4] N. Cristianini, J. Kandola, A. Elisseeﬀ, and J. Shawe-Taylor. On optimizing kernel alignment. Submitted for publication, 2003.</p>
<p>[5] S. D. Kamvar, D. Klein, and C. D. Manning. Spectral learning. In Proc. of the International Joint Conference on Artiﬁcial Intelligence (IJCAI), 2003.</p>
<p>[6] O. Chapelle, J. Weston, and B. Sch¨lkopf. Cluster kernels for semi-supervised o learning. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, Cambridge, MA, 2003. MIT Press.</p>
<p>[7] C. Helmberg. Semideﬁnite Programming for Combinatorial Optimization. Habilitationsschrift, TU Berlin, January 2000. ZIB-Report ZR-00-34, KonradZuse-Zentrum Berlin, 2000.</p>
<p>[8] G. Lanckriet, N. Cristianini, P. Bartlett, L. El Ghaoui, and M. I. Jordan. Learning the kernel matrix with semideﬁnite programming. Journal of Machine Learning Research (JMLR), 5:27–72, 2004.</p>
<p>[9] T. Poggio, S. Mukherjee, R. Rifkin, A. Rakhlin, and A. Verri. b. In Proceedings of the Conference on Uncertainty in Geometric Computations, 2001.</p>
<p>[10] R. A. Horn and C. R. Johnson. Matrix Analysis. Cambridge University Press, 1985.</p>
<p>[11] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22(8):888–905, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
