<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-64" href="../nips2003/nips-2003-Estimating_Internal_Variables_and_Paramters_of_a_Learning_Agent_by_a_Particle_Filter.html">nips2003-64</a> <a title="nips-2003-64-reference" href="#">nips2003-64-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>64 nips-2003-Estimating Internal Variables and Paramters of a Learning Agent by a Particle Filter</h1>
<br/><p>Source: <a title="nips-2003-64-pdf" href="http://papers.nips.cc/paper/2418-estimating-internal-variables-and-paramters-of-a-learning-agent-by-a-particle-filter.pdf">pdf</a></p><p>Author: Kazuyuki Samejima, Kenji Doya, Yasumasa Ueda, Minoru Kimura</p><p>Abstract: When we model a higher order functions, such as learning and memory, we face a difﬁculty of comparing neural activities with hidden variables that depend on the history of sensory and motor signals and the dynamics of the network. Here, we propose novel method for estimating hidden variables of a learning agent, such as connection weights from sequences of observable variables. Bayesian estimation is a method to estimate the posterior probability of hidden variables from observable data sequence using a dynamic model of hidden and observable variables. In this paper, we apply particle ﬁlter for estimating internal parameters and metaparameters of a reinforcement learning model. We veriﬁed the effectiveness of the method using both artiﬁcial data and real animal behavioral data. 1</p><br/>
<h2>reference text</h2><p>[1] Sutton RS & Barto AG (1998) Reinforcement Learning: An Introduction, MIT Press, Cambridge, MA.</p>
<p>[2] Schultz W, Dayan P, Montague PR (1997) A neural substrate of prediction and reward. Science. 14;275(5306):1593-1599</p>
<p>[3] Doucet A, de Freitas N and Gordon. N, (2001) An introduction to sequential Monte Carlo methods, In Sequential Monte Carlo Methods in Practice, Doucet A, de Freitas N & Gordon N eds, Springer-Verlag, pp.3-14.</p>
<p>[4] Ueda Y, Samejima K, Doya K, & Kimura M (2002) Reward value dependent striate neuron activity of monkey performing trial-and-error behavioral decision task, Abst. of Soc Neurosci, 765.13.</p>
<p>[5] O’Doherty, Dayan P, Friston K , Critchley H and Dolan R (2003) Temporal difference models and reward-related learning in human brain, Neuron 28, 329-337.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
