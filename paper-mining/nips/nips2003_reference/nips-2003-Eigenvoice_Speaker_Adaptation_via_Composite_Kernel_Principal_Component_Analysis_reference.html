<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>60 nips-2003-Eigenvoice Speaker Adaptation via Composite Kernel Principal Component Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-60" href="../nips2003/nips-2003-Eigenvoice_Speaker_Adaptation_via_Composite_Kernel_Principal_Component_Analysis.html">nips2003-60</a> <a title="nips-2003-60-reference" href="#">nips2003-60-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>60 nips-2003-Eigenvoice Speaker Adaptation via Composite Kernel Principal Component Analysis</h1>
<br/><p>Source: <a title="nips-2003-60-pdf" href="http://papers.nips.cc/paper/2421-eigenvoice-speaker-adaptation-via-composite-kernel-principal-component-analysis.pdf">pdf</a></p><p>Author: James T. Kwok, Brian Mak, Simon Ho</p><p>Abstract: Eigenvoice speaker adaptation has been shown to be effective when only a small amount of adaptation data is available. At the heart of the method is principal component analysis (PCA) employed to ﬁnd the most important eigenvoices. In this paper, we postulate that nonlinear PCA, in particular kernel PCA, may be even more effective. One major challenge is to map the feature-space eigenvoices back to the observation space so that the state observation likelihoods can be computed during the estimation of eigenvoice weights and subsequent decoding. Our solution is to compute kernel PCA using composite kernels, and we will call our new method kernel eigenvoice speaker adaptation. On the TIDIGITS corpus, we found that compared with a speaker-independent model, our kernel eigenvoice adaptation method can reduce the word error rate by 28–33% while the standard eigenvoice approach can only match the performance of the speaker-independent model. 1</p><br/>
<h2>reference text</h2><p>[1] B. Sch¨ lkopf and A.J. Smola. Learning with Kernels. MIT, 2002. o</p>
<p>[2] B. Sch¨ lkopf, A. Smola, and K.R. M¨ ller. Nonlinear component analysis as a kernel o u eigenvalue problem. Neural Computation, 10:1299–1319, 1998.</p>
<p>[3] R. Kuhn, J.-C. Junqua, P. Nguyen, and N. Niedzielski. Rapid Speaker Adaptation in Eigenvoice Space. IEEE Transactions on Speech and Audio Processing, 8(4):695–707, Nov 2000.</p>
<p>[4] A.P. Dempster, N.M. Laird, and D.B. Rubin. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B, 39(1):1– 38, 1977.</p>
<p>[5] R.G. Leonard. A Database for Speaker-Independent Digit Recognition. In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, volume 3, pages 4211–4214, 1984.</p>
<p>[6] A.J. Smola, O.L. Mangasarian, and B. Sch¨ lkopf. Sparse kernel feature analysis. Techo nical Report 99-03, Data Mining Institute, University of Wisconsin, Madison, 1999.</p>
<p>[7] M.G. Genton. Classes of kernels for machine learning: A statistics perspective. Journal of Machine Learning Research, 2:299–312, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
