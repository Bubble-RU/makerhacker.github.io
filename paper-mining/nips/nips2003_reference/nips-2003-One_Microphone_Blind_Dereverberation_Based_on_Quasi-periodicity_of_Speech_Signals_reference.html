<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>144 nips-2003-One Microphone Blind Dereverberation Based on Quasi-periodicity of Speech Signals</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-144" href="../nips2003/nips-2003-One_Microphone_Blind_Dereverberation_Based_on_Quasi-periodicity_of_Speech_Signals.html">nips2003-144</a> <a title="nips-2003-144-reference" href="#">nips2003-144-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>144 nips-2003-One Microphone Blind Dereverberation Based on Quasi-periodicity of Speech Signals</h1>
<br/><p>Source: <a title="nips-2003-144-pdf" href="http://papers.nips.cc/paper/2436-one-microphone-blind-dereverberation-based-on-quasi-periodicity-of-speech-signals.pdf">pdf</a></p><p>Author: Tomohiro Nakatani, Masato Miyoshi, Keisuke Kinoshita</p><p>Abstract: Speech dereverberation is desirable with a view to achieving, for example, robust speech recognition in the real world. However, it is still a challenging problem, especially when using a single microphone. Although blind equalization techniques have been exploited, they cannot deal with speech signals appropriately because their assumptions are not satisﬁed by speech signals. We propose a new dereverberation principle based on an inherent property of speech signals, namely quasi-periodicity. The present methods learn the dereverberation ﬁlter from a lot of speech data with no prior knowledge of the data, and can achieve high quality speech dereverberation especially when the reverberation time is long. 1</p><br/>
<h2>reference text</h2><p>[1] Baba, A., Lee, A., Saruwatari, H., and Shikano, K., “Speech recognition by reverberation adapted acoustic model,” Proc. of ASJ general meeting, pp. 27–28, Akita, Japan, Sep., 2002.</p>
<p>[2] Amari, S., Douglas, S. C., Cichocki, A., and Yang, H. H., “Multichannel blind deconvolution and equalization using the natural gradient,” Proc. IEEE Workshop on Signal Processing Advances in Wireless Communications, Paris, pp. 101-104, April 1997.</p>
<p>[3] Yegnanarayana, B., and Murthy, P. S., “Enhancement of reverberant speech using LP residual signal,” IEEE Trans. SAP vol. 8, no. 3, pp. 267–281, 2000.</p>
<p>[4] Nakatani, T., Miyoshi, M., and Kinoshita, K., “Implementation and effects of single channel dereverberation based on the harmonic structure of speech,” Proc. IWAENC2003, Sep., 2003.</p>
<p>[5] Nakatani, T., and Miyoshi, M., “Blind dereverberation of single channel speech signal based on harmonic structure,” Proc. ICASSP-2003, vol. 1, pp. 92–95, Apr., 2003.</p>
<p>[6] Yegnanarayana, B., and Ramakrishna, B. S., “Intelligibility of speech under nonexponential decay conditions,” JASA, vol. 58, pp. 853–857, Oct. 1975.</p>
<p>[7] http://www.kecl.ntt.co.jp/icl/signal/nakatani/sound-demos/dm/derev-demos.html</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
