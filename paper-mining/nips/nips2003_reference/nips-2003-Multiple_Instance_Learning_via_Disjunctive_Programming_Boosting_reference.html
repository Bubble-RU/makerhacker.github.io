<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-132" href="../nips2003/nips-2003-Multiple_Instance_Learning_via_Disjunctive_Programming_Boosting.html">nips2003-132</a> <a title="nips-2003-132-reference" href="#">nips2003-132-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>132 nips-2003-Multiple Instance Learning via Disjunctive Programming Boosting</h1>
<br/><p>Source: <a title="nips-2003-132-pdf" href="http://papers.nips.cc/paper/2478-multiple-instance-learning-via-disjunctive-programming-boosting.pdf">pdf</a></p><p>Author: Stuart Andrews, Thomas Hofmann</p><p>Abstract: Learning from ambiguous training data is highly relevant in many applications. We present a new learning algorithm for classiﬁcation problems where labels are associated with sets of pattern instead of individual patterns. This encompasses multiple instance learning as a special case. Our approach is based on a generalization of linear programming boosting and uses results from disjunctive programming to generate successively stronger linear relaxations of a discrete non-convex problem. 1</p><br/>
<h2>reference text</h2><p>[1] Stuart Andrews, Ioannis Tsochantaridis, and Thomas Hofmann. Support vector machines for multiple-instance learning. In Advances in Neural Information Processing Systems, volume 15. MIT Press, 2003.</p>
<p>[2] Egon Balas. Disjunctive programming and a hierarchy of relaxations for discrete optimization problems. SIAM Journal on Algebraic and Discrete Methods, 6(3):466– 486, July 1985.</p>
<p>[3] A. Demirez and K. Bennett. Optimization approaches to semisupervised learning. In M. Ferris, O. Mangasarian, and J. Pang, editors, Applications and Algorithms of Complementarity. Kluwer Academic Publishers, Boston, 2000.</p>
<p>[4] Ayhan Demiriz, Kristin P. Bennett, and John Shawe-Taylor. Linear programming boosting via column generation. Machine Learning, 46(1-3):225–254, 2002.</p>
<p>[5] T. G. Dietterich, R. H. Lathrop, and T. Lozano-Perez. Solving the multiple instance problem with axis-parallel rectangles. Artiﬁcial Intelligence, 89(1-2):31–71, 1997.</p>
<p>[6] T. G¨rtner, P. A. Flach, A. Kowalczyk, and A. J. Smola. Multi-instance kernels. In a Proc. 19th International Conf. on Machine Learning. Morgan Kaufmann, San Francisco, CA, 2002.</p>
<p>[7] A.J. Grove and D. Schuurmans. Boosting in the limit: Maximizing the margin of learned ensembles. In Proceedings of the Fifteenth National Conference on Artiﬁcal Intelligence, 1998.</p>
<p>[8] T. Joachims. Transductive inference for text classiﬁcation using support vector machines. In Proceedings 16th International Conference on Machine Learning, pages 200–209. Morgan Kaufmann, San Francisco, CA, 1999.</p>
<p>[9] Sangbum Lee and Ignacio E. Grossmann. New algorithms for nonlinear generalized disjunctive programming. Computers and Chemical Engineering Journal, 24(910):2125–2141, October 2000.</p>
<p>[10] O. Maron and A. L. Ratan. Multiple-instance learning for natural scene classiﬁcation. In Proc. 15th International Conf. on Machine Learning, pages 341–349. Morgan Kaufmann, San Francisco, CA, 1998.</p>
<p>[11] J. Ramon and L. De Raedt. Multi instance neural networks. In Proceedings of ICML2000, Workshop on Attribute-Value and Relational Learning, 2000.</p>
<p>[12] G. R¨tsch, T. Onoda, and K.-R. M¨ller. Soft margins for AdaBoost. Technical Report a u NC-TR-1998-021, Department of Computer Science, Royal Holloway, University of London, Egham, UK, 1998.</p>
<p>[13] Gunnar R¨tsch, Sebastian Mika, Bernhard Sch¨lkopf, and Klaus-Robert M¨ller. Cona o u structing boosting algorithms from svms: an application to one-class classiﬁcation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(9):1184–1199, 2002.</p>
<p>[14] Qi Zhang and Sally A. Goldman. EM-DD: An improved multiple-instance learning technique. In Advances in Neural Information Processing Systems, volume 14. MIT Press, 2002.  Appendix x x x x The primal variables are αk , αk , ξi , ξi , ξj , and ηi . The dual variables are ux and x uj for the margin constraints, and ρik , σi , and θi for the equality constraints on αk , ξ and η, respectively.  The Lagrangian is given by  L=  k  αk + C   ξi +  i  ux j  − i  −  x∈Xi  ρik  ξj  −  x αk  x∈Xi  k  +  −  σi i  i  x∈Xi  x∈Xi  −  σij  x∈Xi  ˜x x ξj ξj − i  x∈Xi  j  ηi ηi . ˜x x i  x∈Xi  Taking derivatives w.r.t. primal variables, leads to the following dual max  θi i  s.t.  ux , j  θi ≤ ux + i  ux ≤ C, i  ux ≤ σij , j  σij ≤ C i  j  yi ux hk (x) + i  yj ux hk (xj ) ≤ ρik , j j  ρik = 1 i  x ξj  ξj −  i,j  x∈Xi  ˜x x ξi ξi −  −  k  x ξi  ξi −  x ηi  1−  θi i  x∈Xi  αk αk ˜x x x∈Xi  i  x x x αk hk (x) + ξi − ηi  yi  k  αk −  −  ux i  x x x αk hk (xj ) + ξj − ηi  yj  j  i,k  i  j  </p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
