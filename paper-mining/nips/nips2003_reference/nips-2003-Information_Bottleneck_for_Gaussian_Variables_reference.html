<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>92 nips-2003-Information Bottleneck for Gaussian Variables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-92" href="../nips2003/nips-2003-Information_Bottleneck_for_Gaussian_Variables.html">nips2003-92</a> <a title="nips-2003-92-reference" href="#">nips2003-92-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>92 nips-2003-Information Bottleneck for Gaussian Variables</h1>
<br/><p>Source: <a title="nips-2003-92-pdf" href="http://papers.nips.cc/paper/2457-information-bottleneck-for-gaussian-variables.pdf">pdf</a></p><p>Author: Gal Chechik, Amir Globerson, Naftali Tishby, Yair Weiss</p><p>Abstract: The problem of extracting the relevant aspects of data was addressed through the information bottleneck (IB) method, by (soft) clustering one variable while preserving information about another - relevance - variable. An interesting question addressed in the current work is the extension of these ideas to obtain continuous representations that preserve relevant information, rather than discrete clusters. We give a formal deﬁnition of the general continuous IB problem and obtain an analytic solution for the optimal representation for the important case of multivariate Gaussian variables. The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized correlation matrix Σx|y Σ−1 , which x is also the basis obtained in Canonical Correlation Analysis. However, in Gaussian IB, the compression tradeoﬀ parameter uniquely determines the dimension, as well as the scale of each eigenvector. This introduces a novel interpretation where solutions of diﬀerent ranks lie on a continuum parametrized by the compression level. Our analysis also provides an analytic expression for the optimal tradeoﬀ - the information curve - in terms of the eigenvalue spectrum. 1</p><br/>
<h2>reference text</h2><p>[1] N. Tishby, F.C. Pereira, and W. Bialek. The information bottleneck method. In Proc. of 37th Allerton Conference on communication and computation, 1999.</p>
<p>[2] N. Slonim. Information Bottlneck theory and applications. PhD thesis, Hebrew University of Jerusalem, 2003.</p>
<p>[3] H. Hotelling. The most predictable criterion. Journal of Educational Psychology,, 26:139–142, 1935.</p>
<p>[4] S. Becker and G.E. Hinton. A self-organizing neural network that discovers surfaces in random-dot stereograms. Nature, 355(6356):161–163, 1992.</p>
<p>[5] S. Becker. Mutual information maximization: Models of cortical self-organization. Network: Computation in Neural Systems, pages 7–31, 1996.</p>
<p>[6] T. Berger abd R. Zamir. A semi-continuous version of the berger-yeung problem. IEEE Transactions on Information Theory, pages 1520–1526, 1999.</p>
<p>[7] G. Chechik and A. Globerson. Information bottleneck and linear projections of gaussian processes. Technical Report 4, Hebrew University, May 2003.</p>
<p>[8] A.D. Wyner. On source coding with side information at the decoder. IEEE Trans. on Info Theory, IT-21:294–300, 1975.</p>
<p>[9] R. Gilad-Bachrach, A. Navot, and N. Tishby. An information theoretic tradeoﬀ between complexity and accuracy. In Proceedings of the COLT, Washington., 2003.</p>
<p>[10] G. Chechik and N. Tishby. Extracting relevant structures with side information. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, 2002.</p>
<p>[11] N. Slonim and Y. Weiss. Maximum likelihood and the information bottleneck. In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, 2002.</p>
<p>[12] S. Mika, G. Ratsch, J. Weston, B. Scholkopf, A. Smola, and K. Muller. Invariant feature extraction and classiﬁcation in kernel spaces. In S.A. Solla, T.K. Leen, and K.R. Muller, editors, Advances in Neural Information Processing Systems 12, 2000.</p>
<p>[13] A.J. Bell and T.J. Sejnowski. An information maximization approach to blind seperation and blind deconvolution. Neural Computation, 7:1129–1159, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
