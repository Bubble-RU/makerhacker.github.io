<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>148 nips-2003-Online Passive-Aggressive Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-148" href="../nips2003/nips-2003-Online_Passive-Aggressive_Algorithms.html">nips2003-148</a> <a title="nips-2003-148-reference" href="#">nips2003-148-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>148 nips-2003-Online Passive-Aggressive Algorithms</h1>
<br/><p>Source: <a title="nips-2003-148-pdf" href="http://papers.nips.cc/paper/2360-online-passive-aggressive-algorithms.pdf">pdf</a></p><p>Author: Shai Shalev-shwartz, Koby Crammer, Ofer Dekel, Yoram Singer</p><p>Abstract: We present a uniﬁed view for online classiﬁcation, regression, and uniclass problems. This view leads to a single algorithmic framework for the three problems. We prove worst case loss bounds for various algorithms for both the realizable case and the non-realizable case. A conversion of our main online algorithm to the setting of batch learning is also discussed. The end result is new algorithms and accompanying loss bounds for the hinge-loss. 1</p><br/>
<h2>reference text</h2><p>[1] H. H. Bauschke and J. M. Borwein. On projection algorithms for solving convex feasibility problems. SIAM Review, 1996.</p>
<p>[2] Y. Censor and S. A. Zenios. Parallel Optimization.. Oxford University Press, 1997.</p>
<p>[3] K. Crammer and Y. Singer. Ultraconservative online algorithms for multiclass problems. Jornal of Machine Learning Research, 3:951–991, 2003.</p>
<p>[4] Y. Freund and R. E. Schapire. Large margin classiﬁcation using the perceptron algorithm. Machine Learning, 37(3):277–296, 1999.</p>
<p>[5] C. Gentile. A new approximate maximal margin classiﬁcation algorithm. Journal of Machine Learning Research, 2:213–242, 2001.</p>
<p>[6] C. Gentile and M. Warmuth. Linear hinge loss and average margin. In NIPS’98.</p>
<p>[7] D. P. Helmbold, R. E. Schapire, Y. Singer, and M. K. Warmuth. A comparison of new and old algorithms for a mixture estimation problem. In COLT’95.</p>
<p>[8] M. Herbster. COLT’01.  Learning additive models online with fast evaluating kernels.  In</p>
<p>[9] J. Kivinen, D. P. Helmbold, and M. Warmuth. Relative loss bounds for single neurons. IEEE Transactions on Neural Networks, 10(6):1291–1304, 1999.</p>
<p>[10] J. Kivinen, A. J. Smola, and R. C. Williamson. Online learning with kernels. In NIPS’02.</p>
<p>[11] J. Kivinen and M. K. Warmuth. Exponentiated gradient versus gradient descent for linear predictors. Information and Computation, 132(1):1–64, January 1997.</p>
<p>[12] J. Kivinen and M. K. Warmuth. Relative loss bounds for multidimensional regression problems. Journal of Machine Learning, 45(3):301–329, July 2001.</p>
<p>[13] N. Klasner and H. U. Simon. From noise-free to noise-tolerant and from on-line to batch learning. In COLT’95.</p>
<p>[14] Y. Li and P. M. Long. The relaxed online maximum margin algorithm. Machine Learning, 46(1–3):361–387, 2002.</p>
<p>[15] V. N. Vapnik. Statistical Learning Theory. Wiley, 1998.</p>
<p>[16] E. Xing, A. Y. Ng, M. Jordan, and S. Russel. Distance metric learning, with application to clustering with side-information. In NIPS’03.</p>
<p>[17] M. Zinkevich. Online convex programming and generalized inﬁnitesimal gradient ascent. In ICML’03.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
