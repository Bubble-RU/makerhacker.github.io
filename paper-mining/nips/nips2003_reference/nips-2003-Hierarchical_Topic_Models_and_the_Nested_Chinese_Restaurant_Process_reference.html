<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-83" href="../nips2003/nips-2003-Hierarchical_Topic_Models_and_the_Nested_Chinese_Restaurant_Process.html">nips2003-83</a> <a title="nips-2003-83-reference" href="#">nips2003-83-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>83 nips-2003-Hierarchical Topic Models and the Nested Chinese Restaurant Process</h1>
<br/><p>Source: <a title="nips-2003-83-pdf" href="http://papers.nips.cc/paper/2466-hierarchical-topic-models-and-the-nested-chinese-restaurant-process.pdf">pdf</a></p><p>Author: Thomas L. Griffiths, Michael I. Jordan, Joshua B. Tenenbaum, David M. Blei</p><p>Abstract: We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 1</p><br/>
<h2>reference text</h2><p>´</p>
<p>[1] D. Aldous. Exchangeability and related topics. In Ecole d’´ t´ de probabilit´ s de Saint-Flour, ee e XIII—1983, pages 1–198. Springer, Berlin, 1985.</p>
<p>[2] E. Segal, D. Koller, and D. Ormoneit. Probabilistic abstraction hierarchies. In Advances in Neural Information Processing Systems 14.</p>
<p>[3] T. Hofmann. The cluster-abstraction model: Unsupervised learning of topic hierarchies from text data. In IJCAI, pages 682–687, 1999.</p>
<p>[4] T. Ferguson. A Bayesian analysis of some nonparametric problems. The Annals of Statistics, 1:209–230, 1973.</p>
<p>[5] J. Pitman. Combinatorial Stochastic Processes. Notes for St. Flour Summer School. 2002.</p>
<p>[6] J. Ishwaran and L. James. Generalized weighted Chinese restaurant processes for species sampling mixture models. Statistica Sinica, 13:1211–1235, 2003.</p>
<p>[7] R. Neal. Markov chain sampling methods for Dirichlet process mixture models. Journal of Computational and Graphical Statistics, 9(2):249–265, June 2000.</p>
<p>[8] M. West, P. Muller, and M. Escobar. Hierarchical priors and mixture models, with application in regression and density estimation. In Aspects of Uncertainty. John Wiley.</p>
<p>[9] M. Beal, Z. Ghahramani, and C. Rasmussen. The inﬁnite hidden Markov model. In Advances in Neural Information Processing Systems 14.</p>
<p>[10] C. Rasmussen and Z. Ghahramani. Inﬁnite mixtures of Gaussian process experts. In Advances in Neural Information Processing Systems 14.</p>
<p>[11] D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research, 3:993–1022, January 2003.</p>
<p>[12] T. Grifﬁths and M. Steyvers. A probabilistic approach to semantic representation. In Proceedings of the 24th Annual Conference of the Cognitive Science Society, 2002.</p>
<p>[13] R. Kass and A. Raftery. Bayes factors. Journal of the American Statistical Association, 90(430):773–795, 1995.</p>
<p>[14] S. Roweis. NIPS abstracts, 1987–1999. http://www.cs.toronto.edu/ roweis/data.html.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
