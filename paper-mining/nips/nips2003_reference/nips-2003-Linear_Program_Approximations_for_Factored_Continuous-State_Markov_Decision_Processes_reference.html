<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-116" href="../nips2003/nips-2003-Linear_Program_Approximations_for_Factored_Continuous-State_Markov_Decision_Processes.html">nips2003-116</a> <a title="nips-2003-116-reference" href="#">nips2003-116-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>116 nips-2003-Linear Program Approximations for Factored Continuous-State Markov Decision Processes</h1>
<br/><p>Source: <a title="nips-2003-116-pdf" href="http://papers.nips.cc/paper/2430-linear-program-approximations-for-factored-continuous-state-markov-decision-processes.pdf">pdf</a></p><p>Author: Milos Hauskrecht, Branislav Kveton</p><p>Abstract: Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with ﬁnite state spaces. In this work we show that ALP solutions are not limited only to MDPs with ﬁnite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors. 1</p><br/>
<h2>reference text</h2><p>[1] D.P. Bertsekas. A counter-example to temporal differences learning. Neural Computation, 7:270–279, 1994.</p>
<p>[2] D.P. Bertsekas. Dynamic programming and optimal control. Athena Scientiﬁc, 1995.</p>
<p>[3] D.P. Bertsekas and J.N. Tsitsiklis. Neuro-dynamic Programming. Athena Sc., 1996.</p>
<p>[4] C.S. Chow and J.N. Tsitsiklis. An optimal one-way multigrid algorithm for discretetime stochastic control. IEEE Transactions on Automatic Control, 36:898–914, 1991.</p>
<p>[5] D. P. de Farias and B. Van Roy. On constraint sampling for the linear programming approach to approximate dynamic programming. Mathematics of Operations Research, submitted, 2001.</p>
<p>[6] D.P. de Farias and B. Van Roy. The Linear Programming Approach to Approximate Dynamic Programming. In Operations Research, 51:6, 2003.</p>
<p>[7] T. Dean and K. Kanazawa. A model for reasoning about persistence and causation. Computational Intelligence, 5:142–150, 1989.</p>
<p>[8] C. Guestrin, D. Koller, and R. Parr. Max-norm projections for factored MDPs. In Proceedings of the Seventeenth International Joint Conference on Artiﬁcial Intelligence, pages 673–682, 2001.</p>
<p>[9] D. Koller and R. Parr. Computing factored value functions for policies in structured MDPs. In Proceedings of the 16th International Joint Conference on Artiﬁcial Intelligence, pages 1332–1339, 1999.</p>
<p>[10] B. Kveton and M. Hauskrecht. Heuristics reﬁnements of approximate linear programming for factored continuous-state Markov decision processes. In 14Th International Conference on Automated Planning and Scheduling, to appear, 2004.</p>
<p>[11] P. Poupart, C. Boutilier, R. Patrascu, and D. Schuurmans. Piecewise linear value function approximation for factored MDPs. In Proceedings of the Eighteenth National Conference on AI, pages 292–299, 2002.</p>
<p>[12] M.L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley, New York, 1994.</p>
<p>[13] B. Van Roy. Learning and value function approximation in complex decision problems. PhD thesis, Massachussetts Institute of Technology, 1998.</p>
<p>[14] J. Rust. Using randomization to break the course of dimensionality. Econometrica, 65:487–516, 1997.</p>
<p>[15] D. Schuurmans and R.Patrascu. Direct value-approximation for factored MDPs. In Advances in Neural Information Processing Systems 14, MIT Press, 2002.</p>
<p>[16] R. S. Sutton and A. G. Barto. Reinforcement Learning: An introduction. 1998.</p>
<p>[17] M. Trick and E.S Zin. A linear programming approach to solving stochastic dynamic programs, TR, 1993.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
