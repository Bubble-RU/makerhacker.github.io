<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-72" href="../nips2003/nips-2003-Fast_Feature_Selection_from_Microarray_Expression_Data_via_Multiplicative_Large_Margin_Algorithms.html">nips2003-72</a> <a title="nips-2003-72-reference" href="#">nips2003-72-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>72 nips-2003-Fast Feature Selection from Microarray Expression Data via Multiplicative Large Margin Algorithms</h1>
<br/><p>Source: <a title="nips-2003-72-pdf" href="http://papers.nips.cc/paper/2527-fast-feature-selection-from-microarray-expression-data-via-multiplicative-large-margin-algorithms.pdf">pdf</a></p><p>Author: Claudio Gentile</p><p>Abstract: New feature selection algorithms for linear threshold functions are described which combine backward elimination with an adaptive regularization method. This makes them particularly suitable to the classiﬁcation of microarray expression data, where the goal is to obtain accurate rules depending on few genes only. Our algorithms are fast and easy to implement, since they center on an incremental (large margin) algorithm which allows us to avoid linear, quadratic or higher-order programming methods. We report on preliminary experiments with ﬁve known DNA microarray datasets. These experiments suggest that multiplicative large margin algorithms tend to outperform additive algorithms (such as SVM) on feature selection tasks. 1</p><br/>
<h2>reference text</h2><p>[1] Alizadeh, A., et al. (2000). Distinct types of diffuse large b-cell lymphoma identiﬁed by gene expression proﬁling. Nature, 403, 503–511.</p>
<p>[2] Alon, U., et al. (1999). Broad patterns of gene expression revealed by clustering analysis of tumor and normal colon cancer tissues probed by oligonucleotide arrays. Cell Biol., 96, 6745– 6750.</p>
<p>[3] Ben-Dor, A., et al. (2000). Tissue classiﬁcation with gene expression proﬁles. J. Comput. Biol., 7, 559–584.</p>
<p>[4] Bradley, P., & Mangasarian, O. (1998). Feature selection via concave minimization and support vector machines. Proc. 15th ICML (pp. 82–90).</p>
<p>[5] Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3), 273–297.</p>
<p>[6] Dudoit, S., Fridlyand, J., & Speed T.P. (2002). Comparison of discrimination methods for the classiﬁcation of tumors using gene expression data. JASA, 97(457), 77–87.</p>
<p>[7] Fodor, S. (1997). Massively parallel genomics. Science, 277, 393–395.</p>
<p>[8] Gentile, C. (2001a). A new approximate maximal margin classiﬁcation algorithm. JMLR, 2, 213–242.</p>
<p>[9] Gentile, C. (2001b). The robustness of the p-norm algorithms. Machine Learning J., to appear.</p>
<p>[10] Golub, T., et al. (1999). Molecular classiﬁcation of cancer: Class discovery and class prediction by gene expression. Science, 286, 531–537.</p>
<p>[11] Grove, A., Littlestone, N., & Schuurmans, D. (2001). General convergence results for linear discriminant updates. Machine Learning Journal, 43(3), 173–210.</p>
<p>[12] Gruvberger, S., et al. (2001). Estrogen receptor status in breast cancer is associated with remarkably distinct gene expression patterns. Cancer Res., 61, 5979–5984.</p>
<p>[13] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V. (2002). Gene selection for cancer classiﬁcation using support vector machines. Machine Learning Journal, 46(1-3), 389–422.</p>
<p>[14] Kivinen, J., Warmuth, M., & Auer, P. (1997). The perceptron algorithm vs. winnow: linear vs. logarithmic mistake bounds when few input variables are relevant. AI, 97, 325–343.</p>
<p>[15] Kohavi, R., & John, G. (1997). Wrappers for feature subset selection. AI, 97, 273–324.</p>
<p>[16] Littlestone, N. (1988). Learning quickly when irrelevant attributes abound: A new linearthreshold algorithm. Machine Learning, 2, 285–318.</p>
<p>[17] Mangasarian, O. (1997). Mathematical programming in data mining. DMKD, 42(1), 183–201.</p>
<p>[18] Singh, D., et al. (2002). Gene expression correlates of clinical prostate cancer behavior. Cancer Cell, 1.</p>
<p>[19] Tibshirani, R. (1995). Regression selection and shrinkage via the lasso. JRSS B, 1, 267–288.</p>
<p>[20] Weston, J., Elisseeff, A., Scholkopf, B., & Tipping, M. (2002). The use of zero-norm with linear models and kernel methods. JMLR, to appear.</p>
<p>[21] Weston, J., Mukherjee, S., Chapelle, O., Pontil, M., Poggio, T., & Vapnik, V. (2000). Feature selection for svms. Proc. NIPS 13.</p>
<p>[22] Xing, E., Jordan, M., & Karp, R. (2001). Feature selection for high-dimensional genomic microarray data. Proc. 18th ICML. 7 The reader might object that the number of selected features can depend on the value of parameter α in ALMAp . In practice, however, we observed that α does not have a big inﬂuence on this number.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
