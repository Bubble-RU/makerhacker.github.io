<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 nips-2003-Laplace Propagation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-100" href="../nips2003/nips-2003-Laplace_Propagation.html">nips2003-100</a> <a title="nips-2003-100-reference" href="#">nips2003-100-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>100 nips-2003-Laplace Propagation</h1>
<br/><p>Source: <a title="nips-2003-100-pdf" href="http://papers.nips.cc/paper/2444-laplace-propagation.pdf">pdf</a></p><p>Author: Eleazar Eskin, Alex J. Smola, S.v.n. Vishwanathan</p><p>Abstract: We present a novel method for approximate inference in Bayesian models and regularized risk functionals. It is based on the propagation of mean and variance derived from the Laplace approximation of conditional probabilities in factorizing distributions, much akin to Minka’s Expectation Propagation. In the jointly normal case, it coincides with the latter and belief propagation, whereas in the general case, it provides an optimization strategy containing Support Vector chunking, the Bayes Committee Machine, and Gaussian Process chunking as special cases. 1</p><br/>
<h2>reference text</h2><p>[1] C. L. Blake and C. J. Merz. UCI repository of machine learning databases, 1998.</p>
<p>[2] R. Collobert, S. Bengio, and Y. Bengio. A parallel mixture of svms for very large scale problems. In Advances in Neural Information Processing Systems. MIT Press, 2002.</p>
<p>[3] S. Fine and K. Scheinberg. Efﬁcient SVM training using low-rank kernel representations. Journal of Machine Learning Research, 2:243–264, Dec 2001. http://www.jmlr.org.</p>
<p>[4] T. Joachims. Making large-scale SVM learning practical. In B. Sch¨ lkopf, C. J. C. o Burges, and A. J. Smola, editors, Advances in Kernel Methods—Support Vector Learning, pages 169–184, Cambridge, MA, 1999. MIT Press.</p>
<p>[5] M. I. Jordan, Z. Gharamani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. In Learning in Graphical Models, volume M. I. Jordan, pages 105–162. Kluwer Academic, 1998.</p>
<p>[6] T. Minka. Expectation Propagation for approximative Bayesian inference. PhD thesis, MIT Media Labs, Cambridge, USA, 2001.</p>
<p>[7] J. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan-Kaufman, 1988.</p>
<p>[8] J. C. Platt. Sequential minimal optimization: A fast algorithm for training support vector machines. Technical Report MSR-TR-98-14, Microsoft Research, 1998.</p>
<p>[9] V. Tresp. A Bayesian committee machine. Neural Computation, 12(11):2719–2741, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
