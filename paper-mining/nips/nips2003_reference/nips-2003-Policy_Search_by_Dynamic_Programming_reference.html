<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>158 nips-2003-Policy Search by Dynamic Programming</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-158" href="../nips2003/nips-2003-Policy_Search_by_Dynamic_Programming.html">nips2003-158</a> <a title="nips-2003-158-reference" href="#">nips2003-158-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>158 nips-2003-Policy Search by Dynamic Programming</h1>
<br/><p>Source: <a title="nips-2003-158-pdf" href="http://papers.nips.cc/paper/2378-policy-search-by-dynamic-programming.pdf">pdf</a></p><p>Author: J. A. Bagnell, Sham M. Kakade, Jeff G. Schneider, Andrew Y. Ng</p><p>Abstract: We consider the policy search approach to reinforcement learning. We show that if a “baseline distribution” is given (indicating roughly how often we expect a good policy to visit each state), then we can derive a policy search algorithm that terminates in a ﬁnite number of steps, and for which we can provide non-trivial performance guarantees. We also demonstrate this algorithm on several grid-world POMDPs, a planar biped walking robot, and a double-pole balancing problem. 1</p><br/>
<h2>reference text</h2><p>[1] E. Amaldi and V. Kann. On the approximability of minimizing nonzero variables or unsatisﬁed relations in linear systems. Theoretical Comp. Sci., 1998.</p>
<p>[2] C. Atkeson and J. Morimoto. Non-parametric representation of a policies and value functions: A trajectory based approach. In NIPS 15, 2003.</p>
<p>[3] F. Gomez. http://www.cs.utexas.edu/users/nn/pages/software/software.html.</p>
<p>[4] Sham Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis, University College London, 2003.</p>
<p>[5] Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proc. 19th International Conference on Machine Learning, 2002.</p>
<p>[6] Michael Kearns, Yishay Mansour, and Andrew Y. Ng. Approximate planning in large POMDPs via reusable trajectories. (extended version of paper in NIPS 12), 1999.</p>
<p>[7] M. Littman. Memoryless policies: theoretical limitations and practical results. In Proc. 3rd Conference on Simulation of Adaptive Behavior, 1994.</p>
<p>[8] Andrew Y. Ng and Michael I. Jordan. P EGASUS: A policy search method for large MDPs and POMDPs. In Proc. 16th Conf. Uncertainty in Artiﬁcial Intelligence, 2000.</p>
<p>[9] Ronald J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229–256, 1992. 3 In our setting, we use weighted logistic regression and minimize − (θ) = P (i) − i w log p(y (i) |s(i) , θ) where p(y = 1|s, θ) = 1/(1 + exp(−θ T s)). It is straightforward to show that this is a (convex) upper-bound on the objective function T (θ).</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
