<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-101" href="../nips2003/nips-2003-Large_Margin_Classifiers%3A_Convex_Loss%2C_Low_Noise%2C_and_Convergence_Rates.html">nips2003-101</a> <a title="nips-2003-101-reference" href="#">nips2003-101-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>101 nips-2003-Large Margin Classifiers: Convex Loss, Low Noise, and Convergence Rates</h1>
<br/><p>Source: <a title="nips-2003-101-pdf" href="http://papers.nips.cc/paper/2416-large-margin-classifiers-convex-loss-low-noise-and-convergence-rates.pdf">pdf</a></p><p>Author: Peter L. Bartlett, Michael I. Jordan, Jon D. Mcauliffe</p><p>Abstract: Many classiﬁcation algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function—that it satisfy a pointwise form of Fisher consistency for classiﬁcation. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a reﬁned version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a ﬁnite-dimensional base class.</p><br/>
<h2>reference text</h2><p>Bartlett, P. L., Jordan, M. I., and McAuliffe, J. M. (2003). Convexity, classiﬁcation and risk bounds. Technical Report 638, Dept. of Statistics, UC Berkeley. [www.stat.berkeley.edu/tech-reports]. Boyd, S. and Vandenberghe, L. (2003). Convex Optimization. [www.stanford.edu/∼boyd]. Jiang, W. (2003). Process consistency for Adaboost. Annals of Statistics, in press. Lee, W. S., Bartlett, P. L., and Williamson, R. C. (1996). Efﬁcient agnostic learning of neural networks with bounded fan-in. IEEE Transactions on Information Theory, 42(6):2118–2132. Lin, Y. (2001). A note on margin-based loss functions in classiﬁcation. Technical Report 1044r, Department of Statistics, University of Wisconsin. Lugosi, G. and Vayatis, N. (2003). On the Bayes risk consistency of regularized boosting methods. Annals of Statistics, in press. Mannor, S., Meir, R., and Zhang, T. (2002). The consistency of greedy algorithms for classiﬁcation. In Proceedings of the Annual Conference on Computational Learning Theory, pages 319–333. Mendelson, S. (2002). Improving the sample complexity using global data. IEEE Transactions on Information Theory, 48(7):1977–1991. Steinwart, I. (2002). Consistency of support vector machines and other regularized classiﬁers. Technical Report 02-03, University of Jena, Department of Mathematics and Computer Science. Tsybakov, A. (2001). Optimal aggregation of classiﬁers in statistical learning. Technical Report PMA-682, Universit´ Paris VI. e Zhang, T. (2003). Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization. Annals of Statistics, in press.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
