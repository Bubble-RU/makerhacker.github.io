<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-57" href="../nips2003/nips-2003-Dynamical_Modeling_with_Kernels_for_Nonlinear_Time_Series_Prediction.html">nips2003-57</a> <a title="nips-2003-57-reference" href="#">nips2003-57-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>57 nips-2003-Dynamical Modeling with Kernels for Nonlinear Time Series Prediction</h1>
<br/><p>Source: <a title="nips-2003-57-pdf" href="http://papers.nips.cc/paper/2516-dynamical-modeling-with-kernels-for-nonlinear-time-series-prediction.pdf">pdf</a></p><p>Author: Liva Ralaivola, Florence D'alché-buc</p><p>Abstract: We consider the question of predicting nonlinear time series. Kernel Dynamical Modeling (KDM), a new method based on kernels, is proposed as an extension to linear dynamical models. The kernel trick is used twice: ﬁrst, to learn the parameters of the model, and second, to compute preimages of the time series predicted in the feature space by means of Support Vector Regression. Our model shows strong connection with the classic Kalman Filter model, with the kernel feature space as hidden state space. Kernel Dynamical Modeling is tested against two benchmark time series and achieves high quality predictions. 1</p><br/>
<h2>reference text</h2><p>[1] B. Boser, I. Guyon, and V. Vapnik. A Training Algorithm for Optimal Margin Classiﬁers. In Proc. of the 5th Annual Workshop on Comp. Learning Theory, volume 5, 1992.</p>
<p>[2] G. Dorffner. Neural networks for time series processing. Neural Network World, 6(4):447–468, 1996.</p>
<p>[3] Z. Ghahramani and S. Roweis. Learning nonlinear dynamical systems using an em algorithm. In M. S. Kearns, S. A. Solla, and D. A. Cohn, editors, Advances in Neural Information Processing Systems, volume 11, pages 599–605. MIT Press, 1999.</p>
<p>[4] S. Julier and J. Uhlmann. A New Extension of the Kalman Filter to Nonlinear Systems. In Int. Symp. Aerospace/Defense Sensing, Simul. and Controls, 1997.</p>
<p>[5] R. E. Kalman. A New Approach to Linear Filtering and Prediction Problems. Transactions of the ASME–Journal of Basic Engineering, 82(Series D):35–45, 1960.</p>
<p>[6] S. Mika, B. Sch¨ lkopf, A. J. Smola, K.-R. M¨ ller, M. Scholz, and G. R¨ tsch. Kernel PCA and o u a De-Noising in Feature Spaces. In NIPS. MIT Press, 1999.</p>
<p>[7] S. Mukherjee, E. Osuna, and F. Girosi. Nonlinear prediction of chaotic time series using support vector machines. In Proc. of IEEE NNSP’97, 1997.</p>
<p>[8] K. M¨ ller, A. Smola, G. R¨ tsch, B. Sch¨ lkopf, J. Kohlmorgen, and V. Vapnik. Predicting u a o Time Series with Support Vector Machines. In W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, editors, Artiﬁcial Neural Networks - ICANN’97, pages 999–1004. Springer, 1997.</p>
<p>[9] L. Ralaivola. Mod´ lisation et apprentissage de concepts et de syst` mes dynamiques. PhD thesis, e e Universit´Paris 6, France, 2003. e</p>
<p>[10] L. Ralaivola and F. d’Alch´ e-Buc. Filtrage de Kalman non lin´ a l’aide de noyaux. In Actes eaire ` du 19eme Symposium GRETSI sur le traitement du signal et des images, 2003.</p>
<p>[11] A-V.I. Rosti and M.J.F. Gales. Generalised linear Gaussian models. Technical Report CUED/FINFENG/TR.420, Cambridge University Engineering Department, 2001.</p>
<p>[12] S. Roweis and Z. Ghahramani. A unifying review of linear Gaussian models. Neural Computation, 11(2):305–345, 1997.</p>
<p>[13] B. Sch¨ lkopf and A. J. Smola. Learning with Kernels, Support Vector Machines, Regularizao tion, Optimization and Beyond. MIT University Press, 2002.</p>
<p>[14] A. Smola and B. Sch¨ lkopf. A Tutorial on Support Vector Regression. Technical Report NC2o TR-1998-030, NeuroCOLT2, 1998.</p>
<p>[15] V. Vapnik. Statistical Learning Theory. John Wiley and Sons, inc., 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
