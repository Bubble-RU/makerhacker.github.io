<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-97" href="../nips2003/nips-2003-Iterative_Scaled_Trust-Region_Learning_in_Krylov_Subspaces_via_Pearlmutter%27s_Implicit_Sparse_Hessian.html">nips2003-97</a> <a title="nips-2003-97-reference" href="#">nips2003-97-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>97 nips-2003-Iterative Scaled Trust-Region Learning in Krylov Subspaces via Pearlmutter's Implicit Sparse Hessian</h1>
<br/><p>Source: <a title="nips-2003-97-pdf" href="http://papers.nips.cc/paper/2376-iterative-scaled-trust-region-learning-in-krylov-subspaces-via-pearlmutters-implicit-sparse-hessian.pdf">pdf</a></p><p>Author: Eiji Mizutani, James Demmel</p><p>Abstract: The online incremental gradient (or backpropagation) algorithm is widely considered to be the fastest method for solving large-scale neural-network (NN) learning problems. In contrast, we show that an appropriately implemented iterative batch-mode (or block-mode) learning method can be much faster. For example, it is three times faster in the UCI letter classiﬁcation problem (26 outputs, 16,000 data items, 6,066 parameters with a two-hidden-layer multilayer perceptron) and 353 times faster in a nonlinear regression problem arising in color recipe prediction (10 outputs, 1,000 data items, 2,210 parameters with a neuro-fuzzy modular network). The three principal innovative ingredients in our algorithm are the following: First, we use scaled trust-region regularization with inner-outer iteration to solve the associated “overdetermined” nonlinear least squares problem, where the inner iteration performs a truncated (or inexact) Newton method. Second, we employ Pearlmutter’s implicit sparse Hessian matrix-vector multiply algorithm to construct the Krylov subspaces used to solve for the truncated Newton update. Third, we exploit sparsity (for preconditioning) in the matrices resulting from the NNs having many outputs. 1</p><br/>
<h2>reference text</h2><p>[1] Shun-ichi Amari. Natural gradient works eﬃciently in learning. In Neural Computation, 10, pp. 251–276, 1998.</p>
<p>[2] A. R. Conn, N. I. M. Gould, and P. L. Toint. Trust-Region Methods. SIAM, 2000.</p>
<p>[3] James W. Demmel. Applied Numerical Linear Algebra. SIAM, 1997.</p>
<p>[4] J. E. Dennis, D. M. Gay, and R. E. Welsch. “An Adaptive Nonlinear Least-Squares Algorithm.” In ACM Trans. on Mathematical Software, 7(3), pp. 348–368, 1981.</p>
<p>[5] R. A. Jacobs, M. I. Jordan, S. J. Nowlan and G. E. Hinton. “Adaptive Mixtures of Local Experts.” In Neural Computation, pp. 79–87, Vol. 3, No. 1, 1991.</p>
<p>[6] Eiji Mizutani and James W. Demmel. “On structure-exploiting trust-region regularized nonlinear least squares algorithms for neural-network learning.” In International Journal of Neural Networks. Elsevier Science, Vol. 16, pp. 745-753, 2003.</p>
<p>[7] Eiji Mizutani and James W. Demmel. “On separable nonlinear least squares algorithms for neuro-fuzzy modular network learning.” In Proceedings of the IEEE Int’l Joint Conf. on Neural Networks, Vol.3, pp. 2399–2404, Honolulu USA, May, 2002. (Available at http://www.cs.berkeley.edu/˜eiji/ijcnn02.pdf.)</p>
<p>[8] Eiji Mizutani and Stuart E. Dreyfus. “On complexity analysis of supervised MLPlearning for algorithmic comparisons.” In Proceedings of the INNS-IEEE Int’l Joint Conf. on Neural Networks, Vol. 1, pp. 347–352, Washington D.C., July, 2001.</p>
<p>[9] Jorge J. Mor´ and Danny C. Sorensen. “Computing A Trust Region Step.” In SIAM e J. Sci. Stat. Comp. 4(3), pp. 553–572, 1983.</p>
<p>[10] Trond Steihaug “The Conjugate Gradient Method and Trust Regions in Large Scale Optimization.” In SIAM J. Numer. Anal. pp. 626–637, vol. 20, no. 3. 1983.</p>
<p>[11] Barak A. Pearlmutter. “Fast exact multiplication by the Hessian.” In Neural Computation, pp. 147–160, Vol. 6, No. 1, 1994.</p>
<p>[12] Holger Schwenk and Yoshua Bengio. “Boosting neural networks.” In Neural Computation, pp. 1869–1887, Vol. 12, No. 8, 2000.</p>
<p>[13] Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The Elements of Statistical Learning. Springer-Verlag, 2001 (Corrected printing 2002).</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
