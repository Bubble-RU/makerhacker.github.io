<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>143 nips-2003-On the Dynamics of Boosting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-143" href="../nips2003/nips-2003-On_the_Dynamics_of_Boosting.html">nips2003-143</a> <a title="nips-2003-143-reference" href="#">nips2003-143-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>143 nips-2003-On the Dynamics of Boosting</h1>
<br/><p>Source: <a title="nips-2003-143-pdf" href="http://papers.nips.cc/paper/2535-on-the-dynamics-of-boosting.pdf">pdf</a></p><p>Author: Cynthia Rudin, Ingrid Daubechies, Robert E. Schapire</p><p>Abstract: In order to understand AdaBoost’s dynamics, especially its ability to maximize margins, we derive an associated simpliﬁed nonlinear iterated map and analyze its behavior in low-dimensional cases. We ﬁnd stable cycles for these cases, which can explicitly be used to solve for AdaBoost’s output. By considering AdaBoost as a dynamical system, we are able to prove R¨ tsch and Warmuth’s conjecture that AdaBoost may fail a to converge to a maximal-margin combined classiﬁer when given a ‘nonoptimal’ weak learning algorithm. AdaBoost is known to be a coordinate descent method, but other known algorithms that explicitly aim to maximize the margin (such as AdaBoost∗ and arc-gv) are not. We consider a differentiable function for which coordinate ascent will yield a maximum margin solution. We then make a simple approximation to derive a new boosting algorithm whose updates are slightly more aggressive than those of arc-gv. 1</p><br/>
<h2>reference text</h2><p>[1] Robert E. Schapire. A brief introduction to boosting. In Proceedings of the Sixteenth International Joint Conference on Artiﬁcial Intelligence, 1999.</p>
<p>[2] Robert E. Schapire, Yoav Freund, Peter Bartlett, and Wee Sun Lee. Boosting the margin: A new explanation for the effectiveness of voting methods. The Annals of Statistics, 26(5):1651–1686, October 1998.</p>
<p>[3] Gunnar R¨ atsch and Manfred Warmuth. Maximizing the margin with boosting. In Proceedings of the 15th Annual Conference on Computational Learning Theory, pages 334–350, 2002.</p>
<p>[4] Gunnar R¨ atsch and Manfred Warmuth. Efﬁcient margin maximizing with boosting. Journal of Machine Learning Research, submitted 2002.</p>
<p>[5] Leo Breiman. Prediction games and arcing classiﬁers. Neural Computation, 11(7):1493–1517, 1999.</p>
<p>[6] Michael Collins, Robert E. Schapire, and Yoram Singer. Logistic regression, AdaBoost and Bregman distances. Machine Learning, 48(1/2/3), 2002.</p>
<p>[7] Saharon Rosset, Ji Zhu, and Trevor Hastie. Boosting as a regularized path to a maximum margin classiﬁer. Technical report, Department of Statistics, Stanford University, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
