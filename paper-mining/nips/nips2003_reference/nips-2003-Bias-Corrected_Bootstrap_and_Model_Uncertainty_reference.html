<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>40 nips-2003-Bias-Corrected Bootstrap and Model Uncertainty</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-40" href="../nips2003/nips-2003-Bias-Corrected_Bootstrap_and_Model_Uncertainty.html">nips2003-40</a> <a title="nips-2003-40-reference" href="#">nips2003-40-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>40 nips-2003-Bias-Corrected Bootstrap and Model Uncertainty</h1>
<br/><p>Source: <a title="nips-2003-40-pdf" href="http://papers.nips.cc/paper/2427-bias-corrected-bootstrap-and-model-uncertainty.pdf">pdf</a></p><p>Author: Harald Steck, Tommi S. Jaakkola</p><p>Abstract: The bootstrap has become a popular method for exploring model (structure) uncertainty. Our experiments with artiﬁcial and realworld data demonstrate that the graphs learned from bootstrap samples can be severely biased towards too complex graphical models. Accounting for this bias is hence essential, e.g., when exploring model uncertainty. We ﬁnd that this bias is intimately tied to (well-known) spurious dependences induced by the bootstrap. The leading-order bias-correction equals one half of Akaike’s penalty for model complexity. We demonstrate the eﬀect of this simple bias-correction in our experiments. We also relate this bias to the bias of the plug-in estimator for entropy, as well as to the diﬀerence between the expected test and training errors of a graphical model, which asymptotically equals Akaike’s penalty (rather than one half). 1</p><br/>
<h2>reference text</h2><p>[1] H. Akaike. Information theory and an extension of the maximum likelihood principle. International Symposium on Information Theory, pp. 267–81. 1973.</p>
<p>[2] Carlton.On the bias of information estimates.Psych. Bulletin, 71:108–13, 1969.</p>
<p>[3] G. Cooper and E. Herskovits. A Bayesian method for constructing Bayesian belief networks from databases. UAI, pp. 86–94. 1991.</p>
<p>[4] A.C. Davison and D.V. Hinkley. Bootstrap methods and their application. 1997.</p>
<p>[5] B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. 1993.</p>
<p>[6] N. Friedman, M. Goldszmidt, and A. Wyner. Data analysis with Bayesian networks: A bootstrap approach. UAI, pp. 196–205. 1999.</p>
<p>[7] N. Friedman, M. Goldszmidt, and A. Wyner. On the application of the bootstrap for computing conﬁdence measures on features of induced Bayesian networks. AI & Stat., p.p 197–202. 1999.</p>
<p>[8] N. Friedman, M. Linial, I. Nachman, and D. Pe’er. Using Bayesian networks to analyze expression data. Journal of Computational Biology, 7:601–20, 2000.</p>
<p>[9] A. J. Hartemink, D. K. Giﬀord, T. S. Jaakkola, and R. A. Young. Combining location and expression data for principled discovery of genetic regulatory networks. In Paciﬁc Symposium on Biocomputing, 2002.</p>
<p>[10] D. Heckerman, D. Geiger, and D. M. Chickering. Learning Bayesian networks: The combination of knowledge and statistical data. Machine Learning, 20:197– 243, 1995.</p>
<p>[11] G. A. Miller. Note on the bias of information estimates. Information Theory in Psychology: Problems and Methods, pages 95–100, 1955.</p>
<p>[12] D. Pe’er, A. Regev, G. Elidan, and N. Friedman. Inferring subnetworks from perturbed expression proﬁles. Bioinformatics, 1:1–9, 2001.</p>
<p>[13] D. J. Spiegelhalter, N. G. Best, B. P. Carlin, and A. van der Linde. Bayesian measures of model complexity and ﬁt. J. R. Stat. Soc. B, 64:583–639, 2002.</p>
<p>[14] H. Steck and T. S. Jaakkola. (Semi-)predictive discretization during model selection. AI Memo 2003-002, MIT, 2003.</p>
<p>[15] M. Stone. An asymptotic equivalence of choice of model by cross-validation and Akaike’s criterion. J. R. Stat. Soc. B, 36:44–7, 1977.</p>
<p>[16] J. D. Victor. Asymptotic bias in information estimates and the exponential (Bell) polynomials. Neural Computation, 12:2797–804, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
