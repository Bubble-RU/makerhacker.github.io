<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>98 nips-2003-Kernel Dimensionality Reduction for Supervised Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-98" href="../nips2003/nips-2003-Kernel_Dimensionality_Reduction_for_Supervised_Learning.html">nips2003-98</a> <a title="nips-2003-98-reference" href="#">nips2003-98-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>98 nips-2003-Kernel Dimensionality Reduction for Supervised Learning</h1>
<br/><p>Source: <a title="nips-2003-98-pdf" href="http://papers.nips.cc/paper/2513-kernel-dimensionality-reduction-for-supervised-learning.pdf">pdf</a></p><p>Author: Kenji Fukumizu, Francis R. Bach, Michael I. Jordan</p><p>Abstract: We propose a novel method of dimensionality reduction for supervised learning. Given a regression or classiﬁcation problem in which we wish to predict a variable Y from an explanatory vector X, we treat the problem of dimensionality reduction as that of ﬁnding a low-dimensional “effective subspace” of X which retains the statistical relationship between X and Y . We show that this problem can be formulated in terms of conditional independence. To turn this formulation into an optimization problem, we characterize the notion of conditional independence using covariance operators on reproducing kernel Hilbert spaces; this allows us to derive a contrast function for estimation of the effective subspace. Unlike many conventional methods, the proposed method requires neither assumptions on the marginal distribution of X, nor a parametric model of the conditional distribution of Y . 1</p><br/>
<h2>reference text</h2><p>[1] Friedman, J.H. and Stuetzle, W. Projection pursuit regression. J. Amer. Stat. Assoc., 76:817– 823, 1981.</p>
<p>[2] Breiman, L. and Friedman, J.H. Estimating optimal transformations for multiple regression and correlation. J. Amer. Stat. Assoc., 80:580–598, 1985.</p>
<p>[3] Wold, H. Partial least squares. in S. Kotz and N.L. Johnson (Eds.), Encyclopedia of Statistical Sciences, Vol. 6, Wiley, New York. pp.581–591. 1985.</p>
<p>[4] Li, K.-C. Sliced inverse regression for dimension reduction (with discussion). J. Amer. Stat. Assoc., 86:316–342, 1991.</p>
<p>[5] Li, K.-C. On principal Hessian directions for data visualization and dimension reduction: Another application of Stein’s lemma. J. Amer. Stat. Assoc., 87:1025–1039, 1992.</p>
<p>[6] Aronszajn, N. Theory of reproducing kernels. Trans. Amer. Math. Soc., 69(3):337–404, 1950.</p>
<p>[7] Sch¨ lkopf, B., Burges, C.J.C., and Smola, A. (eds.) Advances in Kernel Methods: Support o Vector Learning. MIT Press. 1999.</p>
<p>[8] Sch¨ lkopf, B., Smola, A and M¨ ller, K.-R. Nonlinear component analysis as a kernel eigenvalue o u problem. Neural Computation, 10:1299–1319, 1998.</p>
<p>[9] Bach, F.R. and Jordan, M.I. Kernel independent component analysis. JMLR, 3:1–48, 2002.</p>
<p>[10] Baker, C.R. Joint measures and cross-covariance operators. Trans. Amer. Math. Soc., 186:273– 289, 1973.</p>
<p>[11] Fukumizu, K., Bach, F.R. and Jordan, M.I. Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces. JMLR, 5:73–99, 2004.</p>
<p>[12] Golub T.R. et al. Molecular classiﬁcation of cancer: Class discovery and class prediction by gene expression monitoring. Science, 286:531–537, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
