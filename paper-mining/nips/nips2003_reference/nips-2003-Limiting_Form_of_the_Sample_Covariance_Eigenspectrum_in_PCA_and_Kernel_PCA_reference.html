<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-114" href="../nips2003/nips-2003-Limiting_Form_of_the_Sample_Covariance_Eigenspectrum_in_PCA_and_Kernel_PCA.html">nips2003-114</a> <a title="nips-2003-114-reference" href="#">nips2003-114-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>114 nips-2003-Limiting Form of the Sample Covariance Eigenspectrum in PCA and Kernel PCA</h1>
<br/><p>Source: <a title="nips-2003-114-pdf" href="http://papers.nips.cc/paper/2501-limiting-form-of-the-sample-covariance-eigenspectrum-in-pca-and-kernel-pca.pdf">pdf</a></p><p>Author: David Hoyle, Magnus Rattray</p><p>Abstract: We derive the limiting form of the eigenvalue spectrum for sample covariance matrices produced from non-isotropic data. For the analysis of standard PCA we study the case where the data has increased variance along a small number of symmetry-breaking directions. The spectrum depends on the strength of the symmetry-breaking signals and on a parameter α which is the ratio of sample size to data dimension. Results are derived in the limit of large data dimension while keeping α ﬁxed. As α increases there are transitions in which delta functions emerge from the upper end of the bulk spectrum, corresponding to the symmetry-breaking directions in the data, and we calculate the bias in the corresponding eigenvalues. For kernel PCA the covariance matrix in feature space may contain symmetry-breaking structure even when the data components are independently distributed with equal variance. We show examples of phase-transition behaviour analogous to the PCA results in this case. 1</p><br/>
<h2>reference text</h2><p>[1]</p>
<p>[2]</p>
<p>[3]</p>
<p>[4]</p>
<p>[5]</p>
<p>[6]</p>
<p>[7]</p>
<p>[8]</p>
<p>[9]</p>
<p>[10]</p>
<p>[11]</p>
<p>[12]</p>
<p>[13]</p>
<p>[14]</p>
<p>[15]</p>
<p>[16]  I.T. Jolliffe. Principal Component Analysis. Springer-Verlag, New York, 1986. I.M. Johnstone. Ann. Stat., 29, 2001. P. Reimann, C. Van den Broeck, and G.J. Bex. J. Phys. A:Math. Gen., 29:3521, 1996. B. Scholk¨ pf, A. Smola, and K.-R. M¨ ller. Neural Computation, 10:1299–1319, o u 1998. T.P. Minka. Automatic choice of dimensionality for PCA. In T.K. Leen, T.G. Dietterich, and V. Tresp, editors, NIPS 13, pages 598–604. MIT Press, 2001. A. Engel and C. Van den Broeck. Statistical Mechanics of Learning. Cambridge University Press, 2001. D.C. Hoyle and M. Rattray. Phys. Rev. E, in press. V.A. Marˇ enko and L.A. Pastur. Math. USSR-Sb, 1:507, 1967. c A. Edelman. SIAM J. Matrix Anal. Appl., 9:543, 1988. K.W. Wachter. Ann. Probab., 6:1, 1978. A.M. Sengupta and P.P. Mitra. Phys. Rev. E, 60:3389, 1999. J.W. Silverstein and S. Choi. J. Multivariate Analysis, 54:295, 1995. J.W. Silverstein and P.L. Combettes. IEEE Trans. Signal Processing, 40:2100, 1992. Z.D. Bai. Ann. Probab., 21:649, 1993. P. Sollich. J. Phys. A, 27:7771, 1994. S. Halkjær and O. Winther. In M. Mozer, M. Jordan, and T. Petsche, editors, NIPS 9, page 169. MIT Press, 1997.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
