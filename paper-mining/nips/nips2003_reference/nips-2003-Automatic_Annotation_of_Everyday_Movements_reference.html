<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>37 nips-2003-Automatic Annotation of Everyday Movements</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-37" href="../nips2003/nips-2003-Automatic_Annotation_of_Everyday_Movements.html">nips2003-37</a> <a title="nips-2003-37-reference" href="#">nips2003-37-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>37 nips-2003-Automatic Annotation of Everyday Movements</h1>
<br/><p>Source: <a title="nips-2003-37-pdf" href="http://papers.nips.cc/paper/2370-automatic-annotation-of-everyday-movements.pdf">pdf</a></p><p>Author: Deva Ramanan, David A. Forsyth</p><p>Abstract: This paper describes a system that can annotate a video sequence with: a description of the appearance of each actor; when the actor is in view; and a representation of the actor’s activity while in view. The system does not require a ﬁxed background, and is automatic. The system works by (1) tracking people in 2D and then, using an annotated motion capture dataset, (2) synthesizing an annotated 3D motion sequence matching the 2D tracks. The 3D motion capture data is manually annotated off-line using a class structure that describes everyday motions and allows motion annotations to be composed — one may jump while running, for example. Descriptions computed from video of real motions show that the method is accurate.</p><br/>
<h2>reference text</h2><p>[1] J. K. Aggarwal and Q. Cai. Human motion analysis: A review. Computer Vision and Image Understanding: CVIU, 73(3):428–440, 1999.</p>
<p>[2] O. Arikan and D. Forsyth. Interactive motion generation from examples. In Proc. ACM SIGGRAPH, 2002.</p>
<p>[3] O. Arikan, D. Forsyth, and J. O’Brien. Motion synthesis from annotations. In Proc. ACM SIGGRAPH, 2003.</p>
<p>[4] A. Bobick. Movement, activity, and action: The role of knowledge in the perception of motion. Philosophical Transactions of Royal Society of London, B-352:1257–1265, 1997.</p>
<p>[5] A. F. Bobick and J. Davis. The recognition of human movement using temporal templates. IEEE T. Pattern Analysis and Machine Intelligence, 23(3):257–267, 2001.</p>
<p>[6] L. W. Campbell and A. F. Bobick. Recognition of human body motion using phase space constraints. In ICCV, pages 624–630, 1995.</p>
<p>[7] C. C. Chang and C. J. Lin. Libsvm: Introduction and benchmarks. Technical report, Department of Computer Science and Information Engineering, National Taiwan University, 2000.</p>
<p>[8] P. Felzenschwalb and D. Huttenlocher. Efﬁcient matching of pictorial structures. In Proc CVPR, 2000.</p>
<p>[9] D. M. Gavrila. The visual analysis of human movement: A survey. Computer Vision and Image Understanding: CVIU, 73(1):82–98, 1999.</p>
<p>[10] J. K. Hodgins and N. S. Pollard. Adapting simulated behaviors for new characters. In SIGGRAPH - 97, 1997.</p>
<p>[11] M. I. Jordan, editor. Learning in Graphical Models. MIT Press, Cambridge, MA, 1999.</p>
<p>[12] M. Leventon and W. Freeman. Bayesian estimation of 3D human motion from an image sequence. Technical Report TR-98-06, MERL, 1998.</p>
<p>[13] D. Ramanan and D. A. Forsyth. Automatic annotation of everyday movements. Technical report, UCB//CSD-03-1262, UC Berkeley, CA, 2003.</p>
<p>[14] D. Ramanan and D. A. Forsyth. Finding and tracking people from the bottom up. In Proc CVPR, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
