<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-104" href="../nips2003/nips-2003-Learning_Curves_for_Stochastic_Gradient_Descent_in_Linear_Feedforward_Networks.html">nips2003-104</a> <a title="nips-2003-104-reference" href="#">nips2003-104-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>104 nips-2003-Learning Curves for Stochastic Gradient Descent in Linear Feedforward Networks</h1>
<br/><p>Source: <a title="nips-2003-104-pdf" href="http://papers.nips.cc/paper/2511-learning-curves-for-stochastic-gradient-descent-in-linear-feedforward-networks.pdf">pdf</a></p><p>Author: Justin Werfel, Xiaohui Xie, H. S. Seung</p><p>Abstract: Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difﬁculties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the ﬁrst power of the dimensionality of the noise injected into the system; with sufﬁciently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures in which they will be effective. 1</p><br/>
<h2>reference text</h2><p>[1] Widrow, B. & Lehr, M. A. 30 years of adaptive neural networks: Perceptron, Madaline, and backpropagation. Proc. IEEE 78(9):1415–1442, 1990.</p>
<p>[2] Jabri, M. & Flower, B. Weight perturbation: an optimal architecture and learning technique for analog VLSI feedforward and recurrent multilayered networks. IEEE Transactions on Neural Networks 3(1):154–157, 1992.</p>
<p>[3] Flower, B. & Jabri, M. Summed weight neuron perturbation: an O(n) improvement over weight perturbation. In Advances in Neural Information Processing Systems 5, San Mateo, CA: Morgan Kaufman Publishers: 212–219, 1993.</p>
<p>[4] Cauwenberghs, G. A fast stochastic error-descent algorithm for supervised learning and optimization. In Advances in Neural Information Processing Systems 5, San Mateo, CA: Morgan Kaufman Publishers: 244–251, 1993.</p>
<p>[5] Cauwenberghs, G. An analog VLSI recurrent neural network learning a continuous-time trajectory. IEEE Transactions on Neural Networks 7(2):346–361, 1996.</p>
<p>[6] Fiete, I. Private communication.</p>
<p>[7] Bartlett, P. & Baxter, J. Hebbian synaptic modiﬁcations in spiking neurons that learn. Technical report, November 27 1999.</p>
<p>[8] Baldi, P. & Hornik, K. Learning in linear neural networks: a survey. IEEE Transactions on Neural Networks 6(4):837–858, 1995.</p>
<p>[9] Williams, R.J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning 8:229–256, 1992.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
