<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-167" href="../nips2003/nips-2003-Robustness_in_Markov_Decision_Problems_with_Uncertain_Transition_Matrices.html">nips2003-167</a> <a title="nips-2003-167-reference" href="#">nips2003-167-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>167 nips-2003-Robustness in Markov Decision Problems with Uncertain Transition Matrices</h1>
<br/><p>Source: <a title="nips-2003-167-pdf" href="http://papers.nips.cc/paper/2367-robustness-in-markov-decision-problems-with-uncertain-transition-matrices.pdf">pdf</a></p><p>Author: Arnab Nilim, Laurent El Ghaoui</p><p>Abstract: Optimal solutions to Markov Decision Problems (MDPs) are very sensitive with respect to the state transition probabilities. In many practical problems, the estimation of those probabilities is far from accurate. Hence, estimation errors are limiting factors in applying MDPs to realworld problems. We propose an algorithm for solving ﬁnite-state and ﬁnite-action MDPs, where the solution is guaranteed to be robust with respect to estimation errors on the state transition probabilities. Our algorithm involves a statistically accurate yet numerically efﬁcient representation of uncertainty, via Kullback-Leibler divergence bounds. The worst-case complexity of the robust algorithm is the same as the original Bellman recursion. Hence, robustness can be added at practically no extra computing cost.</p><br/>
<h2>reference text</h2><p>[1] J. Bagnell, A. Ng, and J. Schneider. Solving uncertain Markov decision problems. Technical Report CMU-RI-TR-01-25, Robotics Institute, Carnegie Mellon University, August 2001.</p>
<p>[2] L. El-Ghaoui and A. Nilim. Robust solution to Markov decision problems with uncertain transition matrices: proofs and complexity analysis. Technical Report UCB/ERL M04/07, Department of EECS, University of California, Berkeley, January 2004. A related version has been submitted to Operations Research in Dec. 2003.</p>
<p>[3] E. Feinberg and A. Shwartz. Handbook of Markov Decision Processes, Methods and Applications. Kluwer’s Academic Publishers, Boston, 2002.</p>
<p>[4] T. Ferguson. Prior distributions on space of probability measures. The Annal of Statistics, 2(4):615–629, 1974.</p>
<p>[5] R. Givan, S. Leach, and T. Dean. Bounded parameter Markov decision processes. In fourth European Conference on Planning, pages 234–246, 1997.</p>
<p>[6] E. Lehmann and G. Casella. Theory of point estimation. Springer-Verlag, New York, USA, 1998.</p>
<p>[7] M. Putterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. WileyInterscince, New York, 1994.</p>
<p>[8] J. K. Satia and R. L. Lave. Markov decision processes with uncertain transition probabilities. Operations Research, 21(3):728–740, 1973.</p>
<p>[9] A. Shapiro and A. J. Kleywegt. Minimax analysis of stochastic problems. Optimization Methods and Software, 2002. to appear.</p>
<p>[10] C. C. White and H. K. Eldeib. Markov decision processes with imprecise transition probabilities. Operations Research, 42(4):739–749, 1994.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
