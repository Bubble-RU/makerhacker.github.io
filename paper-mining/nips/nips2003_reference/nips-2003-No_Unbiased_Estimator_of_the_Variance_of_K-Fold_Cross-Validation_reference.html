<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>137 nips-2003-No Unbiased Estimator of the Variance of K-Fold Cross-Validation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-137" href="../nips2003/nips-2003-No_Unbiased_Estimator_of_the_Variance_of_K-Fold_Cross-Validation.html">nips2003-137</a> <a title="nips-2003-137-reference" href="#">nips2003-137-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>137 nips-2003-No Unbiased Estimator of the Variance of K-Fold Cross-Validation</h1>
<br/><p>Source: <a title="nips-2003-137-pdf" href="http://papers.nips.cc/paper/2468-no-unbiased-estimator-of-the-variance-of-k-fold-cross-validation.pdf">pdf</a></p><p>Author: Yoshua Bengio, Yves Grandvalet</p><p>Abstract: Most machine learning researchers perform quantitative experiments to estimate generalization error and compare algorithm performances. In order to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the estimation of uncertainty around the K-fold cross-validation estimator. The main theorem shows that there exists no universal unbiased estimator of the variance of K-fold cross-validation. An analysis based on the eigendecomposition of the covariance matrix of errors helps to better understand the nature of the problem and shows that naive estimators may grossly underestimate variance, as con£rmed by numerical experiments. 1</p><br/>
<h2>reference text</h2><p>[1] Y. Bengio and Y. Grandvalet. No unbiased estimator of the variance of K-fold cross-validation. Journal of Machine Learning Research, 2003.</p>
<p>[2] L. Breiman. Heuristics of instability and stabilization in model selection. The Annals of Statistics, 24(6):2350–2383, 1996.</p>
<p>[3] L. Devroye, L. Gy¨ r£, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. Springer, o 1996.</p>
<p>[4] T. G. Dietterich. Approximate statistical tests for comparing supervised classi£cation learning algorithms. Neural Computation, 10(7):1895–1924, 1999.</p>
<p>[5] B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap, volume 57 of Monographs on Statistics and Applied Probability. Chapman & Hall, 1993.</p>
<p>[6] M. Kearns and D. Ron. Algorithmic stability and sanity-check bounds for leave-one-out crossvalidation. Neural Computation, 11(6):1427–1453, 1996.</p>
<p>[7] R. Kohavi. A study of cross-validation and bootstrap for accuracy estimation and model selection. In Proceedings of the Fourteenth International Joint Conference on Arti£cial Intelligence, pages 1137–1143, 1995.</p>
<p>[8] C. Nadeau and Y. Bengio. Inference for the generalization error. Machine Learning, 52(3):239– 281, 2003.</p>
<p>[9] M. Stone. Cross-validatory choice and assessment of statistical predictions. Journal of the Royal Statistical Society, B, 36(1):111–147, 1974.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
