<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>175 nips-2003-Sensory Modality Segregation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-175" href="../nips2003/nips-2003-Sensory_Modality_Segregation.html">nips2003-175</a> <a title="nips-2003-175-reference" href="#">nips2003-175-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>175 nips-2003-Sensory Modality Segregation</h1>
<br/><p>Source: <a title="nips-2003-175-pdf" href="http://papers.nips.cc/paper/2524-sensory-modality-segregation.pdf">pdf</a></p><p>Author: Virginia Sa</p><p>Abstract: Why are sensory modalities segregated the way they are? In this paper we show that sensory modalities are well designed for self-supervised cross-modal learning. Using the Minimizing-Disagreement algorithm on an unsupervised speech categorization task with visual (moving lips) and auditory (sound signal) inputs, we show that very informative auditory dimensions actually harm performance when moved to the visual side of the network. It is better to throw them away than to consider them part of the “visual input”. We explain this ﬁnding in terms of the statistical structure in sensory inputs. 1</p><br/>
<h2>reference text</h2><p>[1] Virginia R. de Sa. Learning classiﬁcation with unlabeled data. In J.D. Cowan, G. Tesauro, and J. Alspector, editors, Advances in Neural Information Processing Systems 6, pages 112—119. Morgan Kaufmann, 1994.</p>
<p>[2] Virginia R. de Sa and Dana H. Ballard. Category learning through multimodality sensing. Neural Computation, 10(5):1097–1117, 1998.</p>
<p>[3] Teuvo Kohonen. Improved versions of learning vector quantization. In IJCNN International Joint Conference on Neural Networks, volume 1, pages I–545–I–550, 1990.</p>
<p>[4] Ramprasad Polana. Temporal Texture and Activity Recognition. PhD thesis, Department of Computer Science, University of Rochester, 1994.</p>
<p>[5] Virginia R. de Sa and Dana H. Ballard. Perceptual learning from cross-modal feedback. In R. L. Goldstone, P.G. Schyns, and D. L. Medin, editors, Psychology of Learning and Motivation, volume 36, pages 309–351. Academic Press, San Diego, CA, 1997.</p>
<p>[6] A. Blum and T. Mitchell. Combining labeled and unlabeled data with co-training. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory (COLT-98), pages 92–100, 1998.</p>
<p>[7] Ion Muslea, Steve Minton, and Craig Knoblock. Active + semi-supervised learning = robust multi-view learning. In Proceedings of the 19th International Conference on Machine Learning (ICML 2002), pages 435–442, 2002.</p>
<p>[8] C. McCollough. Color adaptation of edge-detectors in the human visual system. Science, 149:1115–1116, 1965.</p>
<p>[9] P.C. Dodwell and G.K. Humphrey. A functional theory of the mccollough effect. Psychological Review, 1990.</p>
<p>[10] F. H. Durgin and D.R. Profﬁtt. Combining recalibration and learning accounts of contingent aftereffects. In Proceedings of the annual meeting of the Psychonomic Society.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
