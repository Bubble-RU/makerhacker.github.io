<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-38" href="../nips2003/nips-2003-Autonomous_Helicopter_Flight_via_Reinforcement_Learning.html">nips2003-38</a> <a title="nips-2003-38-reference" href="#">nips2003-38-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>38 nips-2003-Autonomous Helicopter Flight via Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2003-38-pdf" href="http://papers.nips.cc/paper/2455-autonomous-helicopter-flight-via-reinforcement-learning.pdf">pdf</a></p><p>Author: H. J. Kim, Michael I. Jordan, Shankar Sastry, Andrew Y. Ng</p><p>Abstract: Autonomous helicopter ﬂight represents a challenging control problem, with complex, noisy, dynamics. In this paper, we describe a successful application of reinforcement learning to autonomous helicopter ﬂight. We ﬁrst ﬁt a stochastic, nonlinear model of the helicopter dynamics. We then use the model to learn to hover in place, and to ﬂy a number of maneuvers taken from an RC helicopter competition.</p><br/>
<h2>reference text</h2><p>[1] J. Bagnell and J. Schneider. Autonomous helicopter control using reinforcement learning policy search methods. In Int’l Conf. Robotics and Automation. IEEE, 2001.</p>
<p>[2] G. Balas, J. Doyle, K. Glover, A. Packard, and R. Smith. -analysis and synthesis toolbox user’s guide, 1995.</p>
<p>[3] W. Cleveland. Robust locally weighted regression and smoothing scatterplots. J. Amer. Stat. Assoc, 74, 1979.</p>
<p>[4] Gene F. Franklin, J. David Powell, and Abbas Emani-Naeini. Feedback Control of Dynamic Systems. Addison-Wesley, 1995.</p>
<p>[5] Y. Ho and X. Cao. Pertubation analysis of discrete event dynamic systems. Kluwer, 1991.</p>
<p>[6] J. Kiefer and J. Wolfowitz. Stochastic estimation of the maximum of a regression function. Annals of Mathematical Statistics, 23:462–466, 1952.</p>
<p>[7] J. Leishman. Principles of Helicopter Aerodynamics. Cambridge Univ. Press, 2000.</p>
<p>[8] A. Y. Ng, D. Harada, and S. Russell. Policy invariance under reward transformations: Theory and application to reward shaping. In Proc. 16th ICML, pages 278–287, 1999.</p>
<p>[9] Andrew Y. Ng. Shaping and policy search in reinforcement learning. PhD thesis, EECS, University of California, Berkeley, 2003.</p>
<p>[10] Andrew Y. Ng and Michael I. Jordan. P EGASUS: A policy search method for large MDPs and POMDPs. In Proc. 16th Conf. Uncertainty in Artiﬁcial Intelligence, 2000.</p>
<p>[11] C. Atkeson S. Schaal and A. Moore. Locally weighted learning. AI Review, 11, 1997.</p>
<p>[12] Hyunchul Shim. Hierarchical ﬂight control system synthesis for rotorcraft-based unmanned aerial vehicles. PhD thesis, Mech. Engr., U.C. Berkeley, 2000. S</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
