<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-191" href="../nips2003/nips-2003-Unsupervised_Context_Sensitive_Language_Acquisition_from_a_Large_Corpus.html">nips2003-191</a> <a title="nips-2003-191-reference" href="#">nips2003-191-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>191 nips-2003-Unsupervised Context Sensitive Language Acquisition from a Large Corpus</h1>
<br/><p>Source: <a title="nips-2003-191-pdf" href="http://papers.nips.cc/paper/2467-unsupervised-context-sensitive-language-acquisition-from-a-large-corpus.pdf">pdf</a></p><p>Author: Zach Solan, David Horn, Eytan Ruppin, Shimon Edelman</p><p>Abstract: We describe a pattern acquisition algorithm that learns, in an unsupervised fashion, a streamlined representation of linguistic structures from a plain natural-language corpus. This paper addresses the issues of learning structured knowledge from a large-scale natural language data set, and of generalization to unseen text. The implemented algorithm represents sentences as paths on a graph whose vertices are words (or parts of words). Signiﬁcant patterns, determined by recursive context-sensitive statistical inference, form new vertices. Linguistic constructions are represented by trees composed of signiﬁcant patterns and their associated equivalence classes. An input module allows the algorithm to be subjected to a standard test of English as a Second Language (ESL) proﬁciency. The results are encouraging: the model attains a level of performance considered to be “intermediate” for 9th-grade students, despite having been trained on a corpus (CHILDES) containing transcribed speech of parents directed to small children. 1</p><br/>
<h2>reference text</h2><p>[1] N. Chomsky. Knowledge of language: its nature, origin, and use. Praeger, New York, 1986.</p>
<p>[2] S. Pinker. The Language Instinct: How the Mind Creates Language. William Morro, New York, NY, 1994.</p>
<p>[3] P. J. Hopper. Emergent grammar. In M. Tomasello, editor, The new psychology of language, pp. 155–175. Erlbaum, Mahwah, NJ, 1998.</p>
<p>[4] W. Croft. Radical Construction Grammar: syntactic theory in typological perspective. Oxford University Press, Oxford, 2001.</p>
<p>[5] R. W. Langacker. Foundations of cognitive grammar, volume I: theoretical prerequisites. Stanford University Press, Stanford, CA, 1987.</p>
<p>[6] A. Wray. Formulaic language and the lexicon. Cambridge University Press, Cambridge, UK, 2002.</p>
<p>[7] K. Lari and S. J. Young. The estimation of stochastic context-free grammars using the Inside-Outside algorithm. Computer Speech and Language, 4:35–56, 1990.</p>
<p>[8] F. Pereira and Y. Schab` s. Inside-Outside reestimation from partially bracketed core pora. In Annual Meeting of the ACL, pp. 128–135, 1992.</p>
<p>[9] D. Klein and C. D. Manning. Natural language grammar induction using a constituent-context model. In T. G. Dietterich, S. Becker, and Z. Ghahramani, ed., Advances in Neural Information Processing Systems 14, Cambridge, MA, 2002. MIT Press.</p>
<p>[10] M. van Zaanen and P. Adriaans. Comparing two unsupervised grammar induction systems: Alignment-based learning vs. EMILE. Report 05, School of Computing, Leeds University, 2001.</p>
<p>[11] M. Gross. The construction of local grammars. In E. Roche and Y. Schab` s, ed., e Finite-State Language Processing, pp. 329–354. MIT Press, Cambridge, MA, 1997.</p>
<p>[12] J. G. Wolff. Learning syntax and meanings through optimization and distributional analysis. In Y. Levy, I. M. Schlesinger, and M. D. S. Braine, ed., Categories and Processes in Language Acquisition, pp. 179–215. Lawrence Erlbaum, Hillsdale, NJ, 1988.</p>
<p>[13] Z. Solan, E. Ruppin, D. Horn, and S. Edelman. Automatic acquisition and efﬁcient representation of syntactic structures. In S. Thrun, editor, Advances in Neural Information Processing, volume 15, Cambridge, MA, 2003. MIT Press.</p>
<p>[14] B. MacWhinney and C. Snow. The child language exchange system. Journal of Computational Lingustics, 12:271–296, 1985.</p>
<p>[15] S. Edelman. Constraining the neural representation of the visual world. Trends in Cognitive Sciences, 6:125–131, 2002.</p>
<p>[16] A. E. Goldberg. Constructions: A construction grammar approach to argument structure. University of Chicago Press, Chicago, 1995.</p>
<p>[17] M. C. MacDonald and M. H. Christiansen. Reassessing working memory: A comment on Just and Carpenter (1992) and Waters and Caplan (1996). Psychological Review, 109:35–54, 2002.</p>
<p>[18] A. Clark. Unsupervised Language Acquisition: Theory and Practice. PhD thesis, COGS, University of Sussex, 2001.</p>
<p>[19] I. A. Sag and T. Wasow. Syntactic theory: a formal introduction. CSLI Publications, Stanford, CA, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
