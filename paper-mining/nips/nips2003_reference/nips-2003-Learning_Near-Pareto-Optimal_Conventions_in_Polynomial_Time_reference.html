<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 nips-2003-Learning Near-Pareto-Optimal Conventions in Polynomial Time</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-105" href="../nips2003/nips-2003-Learning_Near-Pareto-Optimal_Conventions_in_Polynomial_Time.html">nips2003-105</a> <a title="nips-2003-105-reference" href="#">nips2003-105-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>105 nips-2003-Learning Near-Pareto-Optimal Conventions in Polynomial Time</h1>
<br/><p>Source: <a title="nips-2003-105-pdf" href="http://papers.nips.cc/paper/2390-learning-near-pareto-optimal-conventions-in-polynomial-time.pdf">pdf</a></p><p>Author: Xiaofeng Wang, Tuomas Sandholm</p><p>Abstract: We study how to learn to play a Pareto-optimal strict Nash equilibrium when there exist multiple equilibria and agents may have different preferences among the equilibria. We focus on repeated coordination games of non-identical interest where agents do not know the game structure up front and receive noisy payoffs. We design efﬁcient near-optimal algorithms for both the perfect monitoring and the imperfect monitoring setting(where the agents only observe their own payoffs and the joint actions). 1</p><br/>
<h2>reference text</h2><p>[1] Bellare and Rogaway. Random oracle are practical: A paradigm for designing efﬁcient protocols. In Proceedings of First ACM Annual Conference on Computer and Communication Security, 93.</p>
<p>[2] Boutilier. Planning, learning and coordination in multi-agent decision processes. In TARK, 96.</p>
<p>[3] Brafman and Tennenholtz. R-max: A general polynomial time algorithm for near-optimal reinforcement learning. In IJCAI, 01.</p>
<p>[4] Claus and Boutilier. The dynamics of reinforcement learning in cooperative multi-agent systems. In AAAI, 98.</p>
<p>[5] Fiechter. Efﬁcient reinforcement learning. In COLT, 94.</p>
<p>[6] Fudenberg and Levine. The theory of learning in games. MIT Press, 98.</p>
<p>[7] Greenwald and Hall. Correlated-q learning. In AAAI Spring Symposium, 02.</p>
<p>[8] Hu and Wellman. Multiagent reinforcement learning: theoretical framework and an algorithm. In ICML, 98.</p>
<p>[9] Kaelbling, Littman, and Moore. Reinforcement learning: A survey. JAIR, 96.</p>
<p>[10] Kearns and Singh. Near-optimal reinforcement learning in polynomial time. In ICML, 98.</p>
<p>[11] Littman. Value-function reinforcement learning in markov games. J. of Cognitive System Research, 2:55–66, 00.</p>
<p>[12] Littman. Friend-or-Foe Q-learning in general sum game. In ICML, 01.</p>
<p>[13] Lov´ and Winkler. Exact mixing in an unknown markov chain. Electronic Journal of Combinatorics, 95. asz</p>
<p>[14] Pivazyan and Shoham. Polynomial-time reinforcement learning of near-optimal policies. In AAAI, 02.</p>
<p>[15] Wang and Sandholm. Learning to play pareto-optimal equilibria: Convergence and efﬁciency. www.cs.cmu.edu/˜xiaofeng/LearnPOC.ps.</p>
<p>[16] Wang and Sandholm. Reinforcement learning to play an optimal Nash equilibrium in team markov game. In NIPS, 02.</p>
<p>[17] Young. The evolution of conventions. Econometrica, 61:57–84, 93.</p>
<p>[18] Young. An evolutionary model of bargaining. Journal of Economic Theory, 59, 93.  4  Recall that agents have established the same numbering of actions. This allows them to encode their joint actions for inputting into γ in the same way. 5 The pattern of the for-loops is from the Lov´ asz-Winkler algorithm [13].</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
