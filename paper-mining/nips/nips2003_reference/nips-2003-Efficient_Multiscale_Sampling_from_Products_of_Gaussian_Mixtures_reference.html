<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>58 nips-2003-Efficient Multiscale Sampling from Products of Gaussian Mixtures</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-58" href="../nips2003/nips-2003-Efficient_Multiscale_Sampling_from_Products_of_Gaussian_Mixtures.html">nips2003-58</a> <a title="nips-2003-58-reference" href="#">nips2003-58-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>58 nips-2003-Efficient Multiscale Sampling from Products of Gaussian Mixtures</h1>
<br/><p>Source: <a title="nips-2003-58-pdf" href="http://papers.nips.cc/paper/2435-efficient-multiscale-sampling-from-products-of-gaussian-mixtures.pdf">pdf</a></p><p>Author: Alexander T. Ihler, Erik B. Sudderth, William T. Freeman, Alan S. Willsky</p><p>Abstract: The problem of approximating the product of several Gaussian mixture distributions arises in a number of contexts, including the nonparametric belief propagation (NBP) inference algorithm and the training of product of experts models. This paper develops two multiscale algorithms for sampling from a product of Gaussian mixtures, and compares their performance to existing methods. The ﬁrst is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical guarantees but improved empirical convergence rates. The second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler, which is guaranteed to sample points to within a tunable parameter of their true probability. We compare both multiscale samplers on a set of computational examples motivated by NBP, demonstrating signiﬁcant improvements over existing methods. 1</p><br/>
<h2>reference text</h2><p>[1] B. W. Silverman. Density Estimation for Statistics and Data Analysis. Chapman & Hall, 1986.</p>
<p>[2] E. B. Sudderth, A. T. Ihler, W. T. Freeman, and A. S. Willsky. Nonparametric belief propagation. In CVPR, 2003.</p>
<p>[3] M. Isard. PAMPAS: Real–valued graphical models for computer vision. In CVPR, 2003.</p>
<p>[4] G. E. Hinton. Training products of experts by minimizing contrastive divergence. Technical Report 2000-004, Gatsby Computational Neuroscience Unit, 2000.</p>
<p>[5] K. Deng and A. W. Moore. Multiresolution instance-based learning. In IJCAI, 1995.</p>
<p>[6] A. G. Gray and A. W. Moore. Very fast multivariate kernel density estimation. In JSM, 2003.</p>
<p>[7] A. Doucet, N. de Freitas, and N. Gordon, editors. Sequential Monte Carlo Methods in Practice. Springer-Verlag, New York, 2001.</p>
<p>[8] S. Thrun, J. Langford, and D. Fox. Monte Carlo HMMs. In ICML, pages 415–424, 1999.</p>
<p>[9] S. Geman and D. Geman. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Trans. PAMI, 6(6):721–741, November 1984.</p>
<p>[10] J. S. Liu and C. Sabatti. Generalised Gibbs sampler and multigrid Monte Carlo for Bayesian computation. Biometrika, 87(2):353–369, 2000.</p>
<p>[11] J. Strain. The fast Gauss transform with variable scales. SIAM J. SSC, 12(5):1131–1139, 1991.  Exact MS ε−Exact MS Seq. Gibbs MS Par. Gibbs Seq. Gibbs Par. Gibbs Gaussian IS Mixture IS  (a)  Input Mixtures  KL Divergence  0.5  0.4  0.3  0.2  0.1  0 0  Product Mixture  0.1  0.2  0.3  0.4  0.5  0.6  Computation Time (sec) 2 Exact MS ε−Exact MS Seq. Gibbs MS Par. Gibbs Seq. Gibbs Par. Gibbs Gaussian IS Mixture IS  1.8  (b)  Input Mixtures  KL Divergence  1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 0  Product Mixture  0.5  1  1.5  2  2.5  Computation Time (sec) Exact MS ε−Exact MS Seq. Gibbs MS Par. Gibbs Seq. Gibbs Par. Gibbs Gaussian IS Mixture IS  (c)  Input Mixtures  KL Divergence  0.5  0.4  0.3  0.2  0.1  0 0  Product Mixture  0.1  0.2  0.3  0.4  0.5  0.6  Computation Time (sec)  Figure 4: Comparison of average sampling accuracy versus computation time for different algorithms (see text). (a) Product of 3 mixtures (exact requires 2.75 sec). (b) Product of 5 mixtures (exact requires 7.6 hours). (c) Product of 2 mixtures (exact requires 0.02 sec). Target Location Observations Exact NBP  (a)  Target Location ε−Exact NBP Particle Filter  (b)  Target Location MS Seq. Gibbs NBP Seq. Gibbs NBP  (c)  Figure 5: Object tracking using NBP. Plots show the posterior distributions two time steps after an observation containing only clutter. The particle ﬁlter and Gibbs samplers are allowed equal computation. (a) Latest observations, and exact sampling posterior. (b) –exact sampling is very accurate, while a particle ﬁlter loses track. (c) Multiscale Gibbs sampling leads to improved performance.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
