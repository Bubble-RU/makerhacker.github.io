<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>126 nips-2003-Measure Based Regularization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-126" href="../nips2003/nips-2003-Measure_Based_Regularization.html">nips2003-126</a> <a title="nips-2003-126-reference" href="#">nips2003-126-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>126 nips-2003-Measure Based Regularization</h1>
<br/><p>Source: <a title="nips-2003-126-pdf" href="http://papers.nips.cc/paper/2504-measure-based-regularization.pdf">pdf</a></p><p>Author: Olivier Bousquet, Olivier Chapelle, Matthias Hein</p><p>Abstract: We address in this paper the question of how the knowledge of the marginal distribution P (x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We also propose practical implementations. 1</p><br/>
<h2>reference text</h2><p>[1] M. Belkin and P. Niyogi. Laplacian eigenmaps for dimensionality reduction and data representation. Neural Computation, 15(6):1373–1396, 2003.</p>
<p>[2] M. Belkin and P. Niyogi. Semi-supervised learning on manifolds. Machine Learning journal, 2003. to appear.</p>
<p>[3] F. Girosi, M. Jones, and T. Poggio. Priors, stabilizers and basis functions: From regularization to radial, tensor and additive splines. Technical Report Artiﬁcial Intelligence Memo 1430, Massachusetts Institute of Technology, 1993.</p>
<p>[4] W. Hoeﬀding. Probability inequalities for sums of bounded random variables. Journal of the American Statistical Association, 58:13–30, 1963.</p>
<p>[5] G. Kimeldorf and G. Wahba. Some results on tchebychean spline functions. Journal of Mathematics Analysis and Applications, 33:82–95, 1971.</p>
<p>[6] Doudou LaLoudouana and Mambobo Bonouliqui Tarare. Data set selection. In Advances in Neural Information Processing Systems, volume 15, 2002.</p>
<p>[7] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: Analysis and an algorithm. In Advances in Neural Information Processing Systems, volume 14, 2001.</p>
<p>[8] B. Sch¨lkopf and A. Smola. Learning with kernels. MIT Press, Cambridge, MA, 2002. o</p>
<p>[9] A. Smola and B. Scholkopf. On a kernel-based method for pattern recognition, regression, approximation and operator inversion. Algorithmica, 22:211–231, 1998.</p>
<p>[10] M. Szummer and T. Jaakkola. Information regularization with partially labeled data. In Advances in Neural Information Processing Systems, volume 15. MIT Press, 2002.</p>
<p>[11] J. B. Tenenbaum, V. de Silva, and J. C. Langford. A global geometric framework for nonlinear dimensionality reduction. Science, 290(5500):2319–2323, 2000.</p>
<p>[12] P. Vincent and Y. Bengio. Density-sensitive metrics and kernels. Presented at the Snowbird Learning Workshop, 2003.</p>
<p>[13] U. von Luxburg and O. Bousquet. Distance-based classiﬁcation with lipschitz functions. In Proceedings of the 16th Annual Conference on Computational Learning Theory, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
