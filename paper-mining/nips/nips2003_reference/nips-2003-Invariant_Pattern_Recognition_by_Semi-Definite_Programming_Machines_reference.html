<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-96" href="../nips2003/nips-2003-Invariant_Pattern_Recognition_by_Semi-Definite_Programming_Machines.html">nips2003-96</a> <a title="nips-2003-96-reference" href="#">nips2003-96-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>96 nips-2003-Invariant Pattern Recognition by Semi-Definite Programming Machines</h1>
<br/><p>Source: <a title="nips-2003-96-pdf" href="http://papers.nips.cc/paper/2403-invariant-pattern-recognition-by-semi-definite-programming-machines.pdf">pdf</a></p><p>Author: Thore Graepel, Ralf Herbrich</p><p>Abstract: Knowledge about local invariances with respect to given pattern transformations can greatly improve the accuracy of classiﬁcation. Previous approaches are either based on regularisation or on the generation of virtual (transformed) examples. We develop a new framework for learning linear classiﬁers under known transformations based on semideﬁnite programming. We present a new learning algorithm— the Semideﬁnite Programming Machine (SDPM)—which is able to ﬁnd a maximum margin hyperplane when the training examples are polynomial trajectories instead of single points. The solution is found to be sparse in dual variables and allows to identify those points on the trajectory with minimal real-valued output as virtual support vectors. Extensions to segments of trajectories, to more than one transformation parameter, and to learning with kernels are discussed. In experiments we use a Taylor expansion to locally approximate rotational invariance in pixel images from USPS and ﬁnd improvements over known methods. 1</p><br/>
<h2>reference text</h2><p>[1] O. Chapelle and B. Sch¨lkopf. Incorporating invariances in non-linear support vector o machines. In T. G. Dietterich, S. Becker, and Z. Ghahramani, editors, Advances in Neural Information Processing Systems 14, pages 609–616, Cambridge, MA, 2002. MIT Press.</p>
<p>[2] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:273–297, 1995.</p>
<p>[3] T. Graepel, R. Herbrich, A. Kharechko, and J. Shawe-Taylor. Semideﬁnite programming by perceptron learning. In S. Thrun, L. Saul, and B. Sch¨lkopf, editors, Advances in o Neural Information Processing Systems 16. MIT Press, 2004.</p>
<p>[4] A. Nemirovski. Five lectures on modern convex optimization, 2002. Lecture notes of the C.O.R.E. Summer School on Modern Convex Optimization.</p>
<p>[5] Y. Nesterov. Squared functional systems and optimization problems. In H. Frenk, K. Roos, T. Terlaky, and S. Zhang, editors, High Performance Optimization, pages 405– 440. Kluwer Academic Press, 2000.</p>
<p>[6] F. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6):386–408, 1958.</p>
<p>[7] B. Sch¨lkopf. Support Vector Learning. R. Oldenbourg Verlag, M¨nchen, 1997. Doko u torarbeit, TU Berlin. Download: http://www.kernel-machines.org.</p>
<p>[8] P. Simard, Y. LeCun, J. Denker, and B. Victorri. Transformation invariance in pattern recognition, tangent distance and tangent propagation. In G. Orr and M. K., editors, Neural Networks: Tricks of the trade. Springer, 1998.</p>
<p>[9] L. Vandenberghe and S. Boyd. Semideﬁnite programming. SIAM Review, 38(1):49–95, 1996.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
