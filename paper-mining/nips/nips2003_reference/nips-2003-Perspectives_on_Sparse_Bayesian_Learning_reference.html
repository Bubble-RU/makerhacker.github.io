<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>155 nips-2003-Perspectives on Sparse Bayesian Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-155" href="../nips2003/nips-2003-Perspectives_on_Sparse_Bayesian_Learning.html">nips2003-155</a> <a title="nips-2003-155-reference" href="#">nips2003-155-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>155 nips-2003-Perspectives on Sparse Bayesian Learning</h1>
<br/><p>Source: <a title="nips-2003-155-pdf" href="http://papers.nips.cc/paper/2393-perspectives-on-sparse-bayesian-learning.pdf">pdf</a></p><p>Author: Jason Palmer, Bhaskar D. Rao, David P. Wipf</p><p>Abstract: Recently, relevance vector machines (RVM) have been fashioned from a sparse Bayesian learning (SBL) framework to perform supervised learning using a weight prior that encourages sparsity of representation. The methodology incorporates an additional set of hyperparameters governing the prior, one for each weight, and then adopts a speciﬁc approximation to the full marginalization over all weights and hyperparameters. Despite its empirical success however, no rigorous motivation for this particular approximation is currently available. To address this issue, we demonstrate that SBL can be recast as the application of a rigorous variational approximation to the full model by expressing the prior in a dual form. This formulation obviates the necessity of assuming any hyperpriors and leads to natural, intuitive explanations of why sparsity is achieved in practice. 1</p><br/>
<h2>reference text</h2><p>[1] C. Bishop and M. Tipping, “Variational relevance vector machines,” Proc. 16th Conf. Uncertainty in Artiﬁcial Intelligence, pp. 46–53, 2000.</p>
<p>[2] R. Duda, P. Hart, and D. Stork, Pattern Classiﬁcation, Wiley, Inc., New York, 2nd ed., 2001.</p>
<p>[3] A.C. Faul and M.E. Tipping, “Analysis of sparse Bayesian learning,” Advances in Neural Information Processing Systems 14, pp. 383–389, 2002.</p>
<p>[4] M. Girolami, “A variational method for learning sparse and overcomplete representations,” Neural Computation, vol. 13, no. 11, pp. 2517–2532, 2001.</p>
<p>[5] M.I. Jordan, Z. Ghahramani, T. Jaakkola, and L.K. Saul, “An introduction to variational methods for graphical models,” Machine Learning, vol. 37, no. 2, pp. 183–233, 1999.</p>
<p>[6] D.J.C. MacKay, “Bayesian interpolation,” Neural Comp., vol. 4, no. 3, pp. 415–447, 1992.</p>
<p>[7] M.E. Tipping, “Sparse Bayesian learning and the relevance vector machine,” Journal of Machine Learning, vol. 1, pp. 211–244, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
