<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>1 nips-2003-1-norm Support Vector Machines</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-1" href="../nips2003/nips-2003-1-norm_Support_Vector_Machines.html">nips2003-1</a> <a title="nips-2003-1-reference" href="#">nips2003-1-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>1 nips-2003-1-norm Support Vector Machines</h1>
<br/><p>Source: <a title="nips-2003-1-pdf" href="http://papers.nips.cc/paper/2450-1-norm-support-vector-machines.pdf">pdf</a></p><p>Author: Ji Zhu, Saharon Rosset, Robert Tibshirani, Trevor J. Hastie</p><p>Abstract: The standard 2-norm SVM is known for its good performance in twoclass classi£cation. In this paper, we consider the 1-norm SVM. We argue that the 1-norm SVM may have some advantage over the standard 2-norm SVM, especially when there are redundant noise features. We also propose an ef£cient algorithm that computes the whole solution path of the 1-norm SVM, hence facilitates adaptive selection of the tuning parameter for the 1-norm SVM. 1</p><br/>
<h2>reference text</h2><p>[1] Bradley, P. & Mangasarian, O. (1998) Feature selection via concave minimization and support vector machines. In J. Shavlik (eds), ICML’98. Morgan Kaufmann.</p>
<p>[2] Evgeniou, T., Pontil, M. & Poggio., T. (1999) Regularization networks and support vector machines. Advances in Large Margin Classi£ers. MIT Press.</p>
<p>[3] Friedman, J., Hastie, T, Rosset, S, Tibshirani, R. & Zhu, J. (2004) Discussion of “Consistency in boosting” by W. Jiang, G. Lugosi, N. Vayatis and T. Zhang. Annals of Statistics. To appear.</p>
<p>[4] Golub,T., Slonim,D., Tamayo,P., Huard,C., Gaasenbeek,M., Mesirov,J., Coller,H., Loh,M., Downing,J. & Caligiuri,M. (1999) Molecular classi£cation of cancer: class discovery and class prediction by gene expression monitoring. Science 286, 531-536.</p>
<p>[5] Guyon,I., Weston,J., Barnhill,S. & Vapnik,V. (2002) Gene selection for cancer classi£cation using support vector machines. Machine Learning 46, 389-422.</p>
<p>[6] Hastie, T., Tibshirani, R. & Friedman, J. (2001) The Elements of Statistical Learning. SpringerVerlag, New York.</p>
<p>[7] Mukherjee, S., Tamayo,P., Slonim,D., Verri,A., Golub,T., Mesirov,J. & Poggio, T. (1999) Support vector machine classi£cation of microarray data. Technical Report AI Memo 1677, MIT.</p>
<p>[8] Rosset, S., Zhu, J. & Hastie, T. (2003) Boosting as a regularized path to a maximum margin classi£er. Technical Report, Department of Statistics, Stanford University, CA.</p>
<p>[9] Song, M., Breneman, C., Bi, J., Sukumar, N., Bennett, K., Cramer, S. & Tugcu, N. (2002) Prediction of protein retention times in anion-exchange chromatography systems using support vector regression. Journal of Chemical Information and Computer Sciences, September.</p>
<p>[10] Tibshirani, R. (1996) Regression shrinkage and selection via the lasso. J.R.S.S.B. 58, 267-288.</p>
<p>[11] Vapnik, V. (1995) Tha Nature of Statistical Learning Theory. Springer-Verlag, New York.</p>
<p>[12] Wahba, G. (1999) Support vector machine, reproducing kernel Hilbert spaces and the randomized GACV. Advances in Kernel Methods - Support Vector Learning, 69-88, MIT Press.</p>
<p>[13] Zhu, J. (2003) Flexible statistical modeling. Ph.D. Thesis. Stanford University.</p>
<p>[14] Zhu, J. & Hastie, T. (2003) Classi£cation of gene microarrays by penalized logistic regression. Biostatistics. Accepted.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
