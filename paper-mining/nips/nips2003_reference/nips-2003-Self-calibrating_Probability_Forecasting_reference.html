<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>170 nips-2003-Self-calibrating Probability Forecasting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-170" href="../nips2003/nips-2003-Self-calibrating_Probability_Forecasting.html">nips2003-170</a> <a title="nips-2003-170-reference" href="#">nips2003-170-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>170 nips-2003-Self-calibrating Probability Forecasting</h1>
<br/><p>Source: <a title="nips-2003-170-pdf" href="http://papers.nips.cc/paper/2462-self-calibrating-probability-forecasting.pdf">pdf</a></p><p>Author: Vladimir Vovk, Glenn Shafer, Ilia Nouretdinov</p><p>Abstract: In the problem of probability forecasting the learner’s goal is to output, given a training set and a new object, a suitable probability measure on the possible values of the new object’s label. An on-line algorithm for probability forecasting is said to be well-calibrated if the probabilities it outputs agree with the observed frequencies. We give a natural nonasymptotic formalization of the notion of well-calibratedness, which we then study under the assumption of randomness (the object/label pairs are independent and identically distributed). It turns out that, although no probability forecasting algorithm is automatically well-calibrated in our sense, there exists a wide class of algorithms for “multiprobability forecasting” (such algorithms are allowed to output a set, ideally very narrow, of probability measures) which satisfy this property; we call the algorithms in this class “Venn probability machines”. Our experimental results demonstrate that a 1-Nearest Neighbor Venn probability machine performs reasonably well on a standard benchmark data set, and one of our theoretical results asserts that a simple Venn probability machine asymptotically approaches the true conditional probabilities regardless, and without knowledge, of the true probability measure generating the examples.</p><br/>
<h2>reference text</h2><p>[1] A. Philip Dawid. Probability forecasting. In Samuel Kotz, Norman L. Johnson, and Campbell B. Read, editors, Encyclopedia of Statistical Sciences, volume 7, pages 210–218. Wiley, New York, 1986.</p>
<p>[2] Vladimir Vovk. On-line Conﬁdence Machines are well-calibrated. In Proceedings of the Forty Third Annual Symposium on Foundations of Computer Science, pages 187–196, Los Alamitos, CA, 2002. IEEE Computer Society.</p>
<p>[3] Vladimir Vovk, Ilia Nouretdinov, and Alex Gammerman. Testing exchangeability on-line. In Tom Fawcett and Nina Mishra, editors, Proceedings of the Twentieth International Conference on Machine Learning, pages 768–775, Menlo Park, CA, 2003. AAAI Press.</p>
<p>[4] Vladimir Vovk. Universal well-calibrated algorithm for on-line classiﬁcation. In Bernhard Sch¨ lkopf and Manfred K. Warmuth, editors, Learning Theory and Kero nel Machines: Sixteenth Annual Conference on Learning Theory and Seventh Kernel Workshop, volume 2777 of Lecture Notes in Artiﬁcial Intelligence, pages 358–372, Berlin, 2003. Springer.</p>
<p>[5] James O. Berger and Mohan Delampady. Testing precise hypotheses (with discussion). Statistical Science, 2:317–352, 1987.</p>
<p>[6] Vladimir Vovk, Alex Gammerman, and Glenn Shafer. Algorithmic Learning in a Random World. Springer, New York, to appear.</p>
<p>[7] Glenn Shafer and Vladimir Vovk. Probability and Finance: It’s Only a Game! Wiley, New York, 2001.</p>
<p>[8] Craig Saunders, Alex Gammerman, and Vladimir Vovk. Transduction with conﬁdence and credibility. In Proceedings of the Sixteenth International Joint Conference on Artiﬁcial Intelligence, pages 722–726, 1999.</p>
<p>[9] Vladimir Vovk, Alex Gammerman, and Craig Saunders. Machine-learning applications of algorithmic randomness. In Proceedings of the Sixteenth International Conference on Machine Learning, pages 444–453, San Francisco, CA, 1999. Morgan Kaufmann.</p>
<p>[10] Berna E. Kılınc. The reception of John Venn’s philosophy of probability. In Vincent F. ¸ Hendricks, Stig Andur Pedersen, and Klaus Frovin Jørgensen, editors, Probability Theory: Philosophy, Recent History and Relations to Science, pages 97–121. Kluwer, Dordrecht, 2001.</p>
<p>[11] Luc Devroye, L´ szl´ Gy¨ rﬁ, and G´ bor Lugosi. A Probabilistic Theory of Pattern a o o a Recognition. Springer, New York, 1996.</p>
<p>[12] Nick Littlestone and Manfred K. Warmuth. Relating data compression and learnability. Technical report, University of California, Santa Cruz, 1986.</p>
<p>[13] John Shawe-Taylor, Peter L. Bartlett, Robert C. Williamson, and Martin Anthony. Structural risk minimization over data-dependent hierarchies. IEEE Transactions on Information Theory, 44:1926–1940, 1998.</p>
<p>[14] David A. McAllester. Some PAC-Bayesian theorems. In Proceedings of the Eleventh Annual Conference on Computational Learning Theory, pages 230–234, New York, 1998. Association for Computing Machinery.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
