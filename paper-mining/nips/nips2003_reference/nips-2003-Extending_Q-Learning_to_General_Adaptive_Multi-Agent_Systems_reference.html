<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-65" href="../nips2003/nips-2003-Extending_Q-Learning_to_General_Adaptive_Multi-Agent_Systems.html">nips2003-65</a> <a title="nips-2003-65-reference" href="#">nips2003-65-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>65 nips-2003-Extending Q-Learning to General Adaptive Multi-Agent Systems</h1>
<br/><p>Source: <a title="nips-2003-65-pdf" href="http://papers.nips.cc/paper/2503-extending-q-learning-to-general-adaptive-multi-agent-systems.pdf">pdf</a></p><p>Author: Gerald Tesauro</p><p>Abstract: Recent multi-agent extensions of Q-Learning require knowledge of other agents’ payoffs and Q-functions, and assume game-theoretic play at all times by all other agents. This paper proposes a fundamentally different approach, dubbed “Hyper-Q” Learning, in which values of mixed strategies rather than base actions are learned, and in which other agents’ strategies are estimated from observed actions via Bayesian inference. Hyper-Q may be effective against many different types of adaptive agents, even if they are persistently dynamic. Against certain broad categories of adaptation, it is argued that Hyper-Q may converge to exact optimal time-varying policies. In tests using Rock-Paper-Scissors, Hyper-Q learns to significantly exploit an Infinitesimal Gradient Ascent (IGA) player, as well as a Policy Hill Climber (PHC) player. Preliminary analysis of Hyper-Q against itself is also presented. 1</p><br/>
<h2>reference text</h2><p>[1] M. Bowling. Convergence problems of general-sum multiagent reinforcement learning. In Proceedings of ICML-00, pages 89–94, 2000.</p>
<p>[2] M. Bowling and M. Veloso. Multiagent learning using a variable learning rate. Artificial Intelligence, 136:215–250, 2002.</p>
<p>[3] Y.-H. Chang and L. P. Kaelbling. Playing is believing: the role of beliefs in multi-agent learning. In Proceedings of NIPS-2001. MIT Press, 2002.</p>
<p>[4] S. J. Hong, J. Hosking, and R. Natarajan. Multiplicative adjustment of class probability: educating naive Bayes. Technical Report RC-22393, IBM Research, 2002.</p>
<p>[5] J. Hu and M. P. Wellman. Multiagent reinforcement learning: theoretical framework and an algorithm. In Proceedings of ICML-98, pages 242–250. Morgan Kaufmann, 1998.</p>
<p>[6] M. Kearns and Y. Mansour. Efficient Nash computation in large population games with bounded influence. In Proceedings of UAI-02, pages 259–266, 2002.</p>
<p>[7] M. L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Proceedings of ICML-94, pages 157–163. Morgan Kaufmann, 1994.</p>
<p>[8] M. L. Littman. Friend-or-Foe Q-learning in general-sum games. In Proceedings of ICML-01. Morgan Kaufmann, 2001.</p>
<p>[9] R. Munos. A convergent reinforcement learning algorithm in the continuous case based on a finite difference method. In Proceedings of IJCAI-97, pages 826–831. Morgan Kaufman, 1997.</p>
<p>[10] S. Singh, M. Kearns, and Y. Mansour. Nash convergence of gradient dynamics in general-sum games. In Proceedings of UAI-2000, pages 541–548. Morgan Kaufman, 2000.</p>
<p>[11] W. D. Smart and L. P. Kaelbling. Practical reinforcement learning in continuous spaces. In Proceedings of ICML-00, pages 903–910, 2000.</p>
<p>[12] W. T. B. Uther and M. M. Veloso. Tree based discretization for continuous state space reinforcement learning. In Proceedings of AAAI-98, pages 769–774, 1998.</p>
<p>[13] C. Watkins. Learning from Delayed Rewards. PhD thesis, Cambridge University, 1989.</p>
<p>[14] J. W. Weibull. Evolutionary Game Theory. The MIT Press, 1995.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
