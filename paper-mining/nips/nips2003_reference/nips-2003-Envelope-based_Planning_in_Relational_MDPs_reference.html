<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>62 nips-2003-Envelope-based Planning in Relational MDPs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-62" href="../nips2003/nips-2003-Envelope-based_Planning_in_Relational_MDPs.html">nips2003-62</a> <a title="nips-2003-62-reference" href="#">nips2003-62-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>62 nips-2003-Envelope-based Planning in Relational MDPs</h1>
<br/><p>Source: <a title="nips-2003-62-pdf" href="http://papers.nips.cc/paper/2424-envelope-based-planning-in-relational-mdps.pdf">pdf</a></p><p>Author: Natalia H. Gardiol, Leslie P. Kaelbling</p><p>Abstract: A mobile robot acting in the world is faced with a large amount of sensory data and uncertainty in its action outcomes. Indeed, almost all interesting sequential decision-making domains involve large state spaces and large, stochastic action sets. We investigate a way to act intelligently as quickly as possible in domains where ﬁnding a complete policy would take a hopelessly long time. This approach, Relational Envelopebased Planning (REBP) tackles large, noisy problems along two axes. First, describing a domain as a relational MDP (instead of as an atomic or propositionally-factored MDP) allows problem structure and dynamics to be captured compactly with a small set of probabilistic, relational rules. Second, an envelope-based approach to planning lets an agent begin acting quickly within a restricted part of the full state space and to judiciously expand its envelope as resources permit. 1</p><br/>
<h2>reference text</h2><p>[1] Avrim L. Blum and Merrick L. Furst. Fast plannning through planning graph analysis. Artiﬁcial Intelligence, 90:281–300, 1997.</p>
<p>[2] Avrim L. Blum and John C. Langford. Probabilistic planning in the graphplan framework. In 5th European Conference on Planning, 1999.</p>
<p>[3] Craig Boutilier, Raymond Reiter, and Bob Price. Symbolic dynamic programming for ﬁrstorder MDPs. In IJCAI, 2001.</p>
<p>[4] Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. Planning under time constraints in stochastic domains. Artiﬁcial Intelligence, 76, 1995.</p>
<p>[5] Kurt Driessens, Jan Ramon, and Hendrik Blockeel. Speeding up relational reinforcement learning through the use of an incremental ﬁrst order decision tree learner. In European Conference on Machine Learning, 2001.</p>
<p>[6] B. Cenk Gazen and Craig A. Knoblock. Combining the expressivity of UCPOP with the efﬁciency of graphplan. In Proc. European Conference on Planning (ECP-97), 1997.</p>
<p>[7] H. Geffner and B. Bonet. High-level planning and control with incomplete information using POMDPs. In Fall AAAI Symposium on Cognitive Robotics, 1998.</p>
<p>[8] C. Guestrin, D. Koller, C. Gearhart, and N. Kanodia. Generalizing plans to new environments in relational MDPs. In International Joint Conference on Artiﬁcial Intelligence, 2003.</p>
<p>[9] Jesse Hoey, Robert St-Aubin, Alan Hu, and Craig Boutilier. Spudd: Stochastic planning using decision diagrams. In Fifteenth Conference on Uncertainty in Artiﬁcial Intelligence, 1999.</p>
<p>[10] J. Koehler, B. Nebel, J. Hoffmann, and Y. Dimopoulos. Extending planning graphs to an ADL subset. In Proc. European Conference on Planning (ECP-97), 1997.</p>
<p>[11] B. Nebel, J. Koehler, and Y. Dimopoulos. Ignoring irrelevant facts and operators in plan generation. In Proc. European Conference on Planning (ECP-97), 1997.</p>
<p>[12] Daniel S. Weld. Recent advances in AI planning. AI Magazine, 20(2):93–123, 1999.</p>
<p>[13] Daniel S. Weld, Corin R. Anderson, and David E. Smith. Extending graphplan to handle uncertainty and sensing actions. In Proceedings of AAAI ’98, 1998.</p>
<p>[14] SungWook Yoon, Alan Fern, and Robert Givan. Inductive policy selection for ﬁrst-order MDPs. In 18th International Conference on Uncertainty in Artiﬁcial Intelligence, 2002.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
