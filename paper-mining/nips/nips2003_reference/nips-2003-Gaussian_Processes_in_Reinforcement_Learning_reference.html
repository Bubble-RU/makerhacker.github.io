<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>78 nips-2003-Gaussian Processes in Reinforcement Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-78" href="../nips2003/nips-2003-Gaussian_Processes_in_Reinforcement_Learning.html">nips2003-78</a> <a title="nips-2003-78-reference" href="#">nips2003-78-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>78 nips-2003-Gaussian Processes in Reinforcement Learning</h1>
<br/><p>Source: <a title="nips-2003-78-pdf" href="http://papers.nips.cc/paper/2420-gaussian-processes-in-reinforcement-learning.pdf">pdf</a></p><p>Author: Malte Kuss, Carl E. Rasmussen</p><p>Abstract: We exploit some useful properties of Gaussian process (GP) regression models for reinforcement learning in continuous state spaces and discrete time. We demonstrate how the GP model allows evaluation of the value function in closed form. The resulting policy iteration algorithm is demonstrated on a simple problem with a two dimensional state space. Further, we speculate that the intrinsic ability of GP models to characterise distributions of functions would allow the method to capture entire distributions over future values instead of merely their expectation, which has traditionally been the focus of much of reinforcement learning.</p><br/>
<h2>reference text</h2><p>Attias, H. (2003). Planning by probabilistic inference. In Proceedings of the Ninth International Workshop on Artiﬁcial Intelligence and Statistics. Dearden, R., N. Friedman, and S. J. Russell (1998). Bayesian Q-learning. In Fifteenth National Conference on Artiﬁcial Intelligence (AAAI). Dietterich, T. G. and X. Wang (2002). Batch value function approximation via support vectors. In Advances in Neural Information Processing Systems 14, Cambridge, MA, pp. 1491–1498. MIT Press. Girard, A., C. E. Rasmussen, J. Qui˜ nonero-Candela, and R. Murray-Smith (2002). Multiple-step ahead prediction for non linear dynamic systems – a Gaussian process treatment with propagation of the uncertainty. In Advances in Neural Information Processing Systems 15. Moore, A. W. and C. G. Atkeson (1995). The parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. Machine Learning 21, 199–233. Qui˜ nonero-Candela, J., A. Girard, J. Larsen, and C. E. Rasmussen (2003). Propagation of uncertainty in Bayesian kernel models - application to multiple-step ahead forecasting. In Proceedings of the 2003 IEEE Conference on Acoustics, Speech, and Signal Processing. Sutton, R. S. and A. G. Barto (1998). Reinforcement Learning. Cambridge, Massachusetts: MIT Press. Williams, C. K. I. and C. E. Rasmussen (1996). Gaussian processes for regression. In Advances in Neural Information Processing Systems 8.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
