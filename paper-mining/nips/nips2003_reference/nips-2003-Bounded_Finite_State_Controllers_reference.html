<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>42 nips-2003-Bounded Finite State Controllers</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-42" href="../nips2003/nips-2003-Bounded_Finite_State_Controllers.html">nips2003-42</a> <a title="nips-2003-42-reference" href="#">nips2003-42-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>42 nips-2003-Bounded Finite State Controllers</h1>
<br/><p>Source: <a title="nips-2003-42-pdf" href="http://papers.nips.cc/paper/2372-bounded-finite-state-controllers.pdf">pdf</a></p><p>Author: Pascal Poupart, Craig Boutilier</p><p>Abstract: We describe a new approximation algorithm for solving partially observable MDPs. Our bounded policy iteration approach searches through the space of bounded-size, stochastic ﬁnite state controllers, combining several advantages of gradient ascent (efﬁciency, search through restricted controller space) and policy iteration (less vulnerability to local optima).</p><br/>
<h2>reference text</h2><p>[1] D. Aberdeen and J. Baxter. Scaling internal-state policy-gradient methods for POMDPs. Proc. ICML-02, pp.3–10, Sydney, Australia, 2002.</p>
<p>[2] C. Boutilier and D. Poole. Computing optimal policies for partially observable decision processes using compact representations. Proc. AAAI-96, pp.1168–1175, Portland, OR, 1996.</p>
<p>[3] D. Braziunas. Stochastic local search for POMDP controllers. Master’s thesis, University of Toronto, Toronto, 2003.</p>
<p>[4] A. R. Cassandra, M. L. Littman, and N. L. Zhang. Incremental pruning: A simple, fast, exact method for POMDPs. Proc.UAI-97, pp.54–61, Providence, RI, 1997.</p>
<p>[5] H.-T. Cheng. Algorithms for Partially Observable Markov Decision Processes. PhD thesis, University of British Columbia, Vancouver, 1988.</p>
<p>[6] Z. Feng and E. A. Hansen. Approximate planning for factored POMDPs. Proc. ECP-01, Toledo, Spain, 2001.</p>
<p>[7] E. A. Hansen. Solving POMDPs by searching in policy space. Proc. UAI-98, pp.211–219, Madison, Wisconsin, 1998.</p>
<p>[8] M. Hauskrecht. Value-function approximations for partially observable Markov decision processes. Journal of Artiﬁcial Intelligence Research, 13:33–94, 2000.</p>
<p>[9] L. P. Kaelbling, M. Littman, and A. R. Cassandra. Planning and acting in partially observable stochastic domains. Artiﬁcial Intelligence, 101:99–134, 1998.</p>
<p>[10] N. Meuleau, K.-E. Kim, L. P. Kaelbling, and A. R. Cassandra. Solving POMDPs by searching the space of ﬁnite policies. Proc. UAI-99, pp.417–426, Stockholm, 1999.</p>
<p>[11] N. Meuleau, L. Peshkin, K.-E. Kim, and L. P. Kaelbling. Learning ﬁnite-state controllers for partially observable environments. Proc. UAI-99, pp.427–436, Stockholm, 1999.</p>
<p>[12] J. Pineau, G. Gordon, and S. Thrun. Point-based value iteration: an anytime algorithm for POMDPs. In Proc. IJCAI-03, Acapulco, Mexico, 2003.</p>
<p>[13] P. Poupart and C. Boutilier. Value-directed compressions of POMDPs. Proc. NIPS-02, pp.1547– 1554, Vancouver, Canada, 2002.</p>
<p>[14] N. L. Zhang and W. Zhang. Speeding up the convergence of value-iteration in partially observable Markov decision processes. Journal of Artiﬁcial Intelligence Research, 14:29–51, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
