<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>172 nips-2003-Semi-Supervised Learning with Trees</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-172" href="../nips2003/nips-2003-Semi-Supervised_Learning_with_Trees.html">nips2003-172</a> <a title="nips-2003-172-reference" href="#">nips2003-172-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>172 nips-2003-Semi-Supervised Learning with Trees</h1>
<br/><p>Source: <a title="nips-2003-172-pdf" href="http://papers.nips.cc/paper/2464-semi-supervised-learning-with-trees.pdf">pdf</a></p><p>Author: Charles Kemp, Thomas L. Griffiths, Sean Stromsten, Joshua B. Tenenbaum</p><p>Abstract: We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain. The tree (or a distribution over trees) may be inferred using the unlabeled data. A prior over concepts generated by a mutation process on the inferred tree(s) allows efﬁcient computation of the optimal Bayesian classiﬁcation function from the labeled examples. We test our approach on eight real-world datasets. 1</p><br/>
<h2>reference text</h2><p>[1] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using Gaussian ﬁelds and harmonic functions. In ICML, volume 20, 2003.</p>
<p>[2] M. Szummer and T. Jaakkola. Partially labeled classiﬁcation with Markov random walks. In NIPS, volume 14, 2002.</p>
<p>[3] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In ICML, volume 18, 2001.</p>
<p>[4] M. Belkin and P. Niyogi. Semi-supervised learning on manifolds. 2003. To appear in Machine Learning, Special Issue on Theoretical Advances in Data Clustering. ¨</p>
<p>[5] O. Chapelle, J. Weston, and B. Scholkopf. Cluster kernels for semi-supervised learning. In NIPS, volume 15, 2003.</p>
<p>[6] T. M. Mitchell. Machine Learning. McGraw-Hill, 1997.</p>
<p>[7] D. Haussler, M. Kearns, and R. Schapire. Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. Machine Learning, 14(1), 1994.</p>
<p>[8] C. Kemp and J. B. Tenenbaum. Theory-based induction. In Proceedings of the 25th Annual Conference of the Cognitive Science Society, 2003.</p>
<p>[9] L. Shih and D. Karger. Learning classes correlated to a hierarchy. 2003. Unpublished manuscript.</p>
<p>[10] J.-P. Vert. A tree kernel to analyze phylogenetic proﬁles. Bioinformatics, 1(1):1–9, 2002.</p>
<p>[11] R. Neal. Deﬁning priors for distributions using Dirichlet diffusion trees. Technical Report 0108, University of Toronto, 2001.</p>
<p>[12] H. Jow, C. Hudelot, M. Rattray, and P. Higgs. Bayesian phylogenetics using an RNA substitution model applied to early mammalian evolution. Molecular Biology and Evolution, 19(9):1951–1601, 2002.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
