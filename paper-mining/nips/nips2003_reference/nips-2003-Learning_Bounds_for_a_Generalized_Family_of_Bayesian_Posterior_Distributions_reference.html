<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-103" href="../nips2003/nips-2003-Learning_Bounds_for_a_Generalized_Family_of_Bayesian_Posterior_Distributions.html">nips2003-103</a> <a title="nips-2003-103-reference" href="#">nips2003-103-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>103 nips-2003-Learning Bounds for a Generalized Family of Bayesian Posterior Distributions</h1>
<br/><p>Source: <a title="nips-2003-103-pdf" href="http://papers.nips.cc/paper/2439-learning-bounds-for-a-generalized-family-of-bayesian-posterior-distributions.pdf">pdf</a></p><p>Author: Tong Zhang</p><p>Abstract: In this paper we obtain convergence bounds for the concentration of Bayesian posterior distributions (around the true distribution) using a novel method that simpliﬁes and enhances previous results. Based on the analysis, we also introduce a generalized family of Bayesian posteriors, and show that the convergence behavior of these generalized posteriors is completely determined by the local prior structure around the true distribution. This important and surprising robustness property does not hold for the standard Bayesian posterior in that it may not concentrate when there exist “bad” prior structures even at places far away from the true distribution. 1</p><br/>
<h2>reference text</h2><p>[1] Andrew Barron, Mark J. Schervish, and Larry Wasserman. The consistency of posterior distributions in nonparametric problems. Ann. Statist., 27(2):536–561, 1999.</p>
<p>[2] Persi Diaconis and David Freedman. On the consistency of Bayes estimates. Ann. Statist., 14(1):1–67, 1986. With a discussion and a rejoinder by the authors.</p>
<p>[3] Subhashis Ghosal, Jayanta K. Ghosh, and Aad W. van der Vaart. Convergence rates of posterior distributions. Ann. Statist., 28(2):500–531, 2000.</p>
<p>[4] D. McAllester. PAC-Bayesian stochastic model selection. Machine Learning, 51(1):5– 21, 2003.</p>
<p>[5] Ron Meir and Tong Zhang. Generalization error bounds for Bayesian mixture algorithms. Journal of Machine Learning Research, 4:839–860, 2003.</p>
<p>[6] C. P. Robert. The Bayesian Choice: A Decision Theoretic Motivation. Springer Verlag, New York, 1994.</p>
<p>[7] M. Seeger. PAC-Bayesian generalization error bounds for Gaussian process classiﬁcation. JMLR, 3:233–269, 2002.</p>
<p>[8] Xiaotong Shen and Larry Wasserman. Rates of convergence of posterior distributions. Ann. Statist., 29(3):687–714, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
