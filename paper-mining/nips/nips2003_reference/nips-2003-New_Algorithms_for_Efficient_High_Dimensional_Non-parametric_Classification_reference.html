<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-136" href="../nips2003/nips-2003-New_Algorithms_for_Efficient_High_Dimensional_Non-parametric_Classification.html">nips2003-136</a> <a title="nips-2003-136-reference" href="#">nips2003-136-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>136 nips-2003-New Algorithms for Efficient High Dimensional Non-parametric Classification</h1>
<br/><p>Source: <a title="nips-2003-136-pdf" href="http://papers.nips.cc/paper/2469-new-algorithms-for-efficient-high-dimensional-non-parametric-classification.pdf">pdf</a></p><p>Author: Ting liu, Andrew W. Moore, Alexander Gray</p><p>Abstract: This paper is about non-approximate acceleration of high dimensional nonparametric operations such as k nearest neighbor classiﬁers and the prediction phase of Support Vector Machine classiﬁers. We attempt to exploit the fact that even if we want exact answers to nonparametric queries, we usually do not need to explicitly ﬁnd the datapoints close to the query, but merely need to ask questions about the properties about that set of datapoints. This offers a small amount of computational leeway, and we investigate how much that leeway can be exploited. For clarity, this paper concentrates on pure k-NN classiﬁcation and the prediction phase of SVMs. We introduce new ball tree algorithms that on real-world datasets give accelerations of 2-fold up to 100-fold compared against highly optimized traditional ball-tree-based k-NN. These results include datasets with up to 106 dimensions and 105 records, and show non-trivial speedups while giving exact answers. 1</p><br/>
<h2>reference text</h2><p>[1] S. Arya, D. Mount, N. Netanyahu, R. Silverman, and A. Wu. An optimal algorithm for approximate nearest neighbor searching ﬁxed dimensions. Journal of the ACM, 45(6):891–923, 1998.</p>
<p>[2] S. D. Bay. UCI KDD Archive [http://kdd.ics.uci.edu]. Irvine, CA: University of California, Dept of Information and Computer Science, 1999.</p>
<p>[3] C. Burges. A tutorial on Support Vector Machines for Pattern Recognition. Data Mining and Knowledge Discovery, 2(2):955–974, 1998.</p>
<p>[4] P. Ciaccia, M. Patella, and P. Zezula. M-tree: An efﬁcient access method for similarity search in metric spaces. In Proceedings of the 23rd VLDB International Conference, September 1997.</p>
<p>[5] K. Deng and A. W. Moore. Multiresolution Instance-based Learning. In Proceedings of the Twelfth International Joint Conference on Artiﬁcial Intelligence, pages 1233–1239, San Francisco, 1995. Morgan Kaufmann.</p>
<p>[6] J. H. Friedman, J. L. Bentley, and R. A. Finkel. An algorithm for ﬁnding best matches in logarithmic expected time. ACM Transactions on Mathematical Software, 3(3):209–226, September 1977.</p>
<p>[7] A. Gionis, P. Indyk, and R. Motwani. Similarity Search in High Dimensions via Hashing. In Proc 25th VLDB Conference, 1999.</p>
<p>[8] A. Gray and A. W. Moore. N-Body Problems in Statistical Learning. In Todd K. Leen, Thomas G. Dietterich, and Volker Tresp, editors, Advances in Neural Information Processing Systems 13 (December 2000). MIT Press, 2001.</p>
<p>[9] A. Guttman. R-trees: A dynamic index structure for spatial searching. In Proceedings of the Third ACM SIGACT-SIGMOD Symposium on Principles of Database Systems. Assn for Computing Machinery, April 1984.</p>
<p>[10] J. M. Hammersley. The Distribution of Distances in a Hypersphere. Annals of Mathematical Statistics, 21:447–452, 1950.</p>
<p>[11] CMU informedia digital video library project. The trec-2001 video trackorganized by nist shot boundary task, 2001.</p>
<p>[12] T. Joachims. Making large-scale support vector machine learning practical. In A. Smola B. Sch¨ lkopf, C. Burges, editor, Advances in Kernel Methods: Support Vector Machines. MIT o Press, Cambridge, MA, 1998.</p>
<p>[13] A. W. Moore. The Anchors Hierarchy: Using the Triangle Inequality to Survive HighDimensional Data. In Twelfth Conference on Uncertainty in Artiﬁcial Intelligence. AAAI Press, 2000.</p>
<p>[14] S. M. Omohundro. Efﬁcient Algorithms with Neural Network Behaviour. Journal of Complex Systems, 1(2):273–347, 1987.</p>
<p>[15] S. M. Omohundro. Bumptrees for Efﬁcient Function, Constraint, and Classiﬁcation Learning. In R. P. Lippmann, J. E. Moody, and D. S. Touretzky, editors, Advances in Neural Information Processing Systems 3. Morgan Kaufmann, 1991.</p>
<p>[16] D. Pelleg and A. W. Moore. Accelerating Exact k-means Algorithms with Geometric Reasoning. In Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining. ACM, 1999.</p>
<p>[17] F. P. Preparata and M. Shamos. Computational Geometry. Springer-Verlag, 1985.</p>
<p>[18] J. K. Uhlmann. Satisfying general proximity/similarity queries with metric trees. Information Processing Letters, 40:175–179, 1991.</p>
<p>[19] W. Zheng and A. Tropsha. A Novel Variable Selection QSAR Approach based on the K-Nearest Neighbor Principle. J. Chem. Inf.Comput. Sci., 40(1):185–194, 2000.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
