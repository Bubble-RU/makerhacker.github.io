<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>84 nips-2003-How to Combine Expert (and Novice) Advice when Actions Impact the Environment?</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-84" href="../nips2003/nips-2003-How_to_Combine_Expert_%28and_Novice%29_Advice_when_Actions_Impact_the_Environment%3F.html">nips2003-84</a> <a title="nips-2003-84-reference" href="#">nips2003-84-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>84 nips-2003-How to Combine Expert (and Novice) Advice when Actions Impact the Environment?</h1>
<br/><p>Source: <a title="nips-2003-84-pdf" href="http://papers.nips.cc/paper/2489-how-to-combine-expert-and-novice-advice-when-actions-impact-the-environment.pdf">pdf</a></p><p>Author: Daniela Pucci de Farias, Nimrod Megiddo</p><p>Abstract: The so-called “experts algorithms” constitute a methodology for choosing actions repeatedly, when the rewards depend both on the choice of action and on the unknown current state of the environment. An experts algorithm has access to a set of strategies (“experts”), each of which may recommend which action to choose. The algorithm learns how to combine the recommendations of individual experts so that, in the long run, for any ﬁxed sequence of states of the environment, it does as well as the best expert would have done relative to the same sequence. This methodology may not be suitable for situations where the evolution of states of the environment depends on past chosen actions, as is usually the case, for example, in a repeated non-zero-sum game. A new experts algorithm is presented and analyzed in the context of repeated games. It is shown that asymptotically, under certain conditions, it performs as well as the best available expert. This algorithm is quite different from previously proposed experts algorithms. It represents a shift from the paradigms of regret minimization and myopic optimization to consideration of the long-term effect of a player’s actions on the opponent’s actions or the environment. The importance of this shift is demonstrated by the fact that this algorithm is capable of inducing cooperation in the repeated Prisoner’s Dilemma game, whereas previous experts algorithms converge to the suboptimal non-cooperative play. 1</p><br/>
<h2>reference text</h2><p>[1] Auer, P., Cesa-Bianchi, N., Freund, Y. & Schapire, R.E. (1995) Gambling in a rigged casino: The adversarial multi-armed bandit problem. In Proc. 36th Annual IEEE Symp. on Foundations of Computer Science, pp. 322–331, Los Alamitos, CA: IEEE Computer Society Press.</p>
<p>[2] Freund, Y. & Schapire, R.E. (1999) Adaptive game playing using multiplicative weights. Games and Economic Behavior 29:79–103.</p>
<p>[3] Foster, D. & Vohra, R. (1999) Regret and the on-line decision problem. Games and Economic Behavior 29:7–35.</p>
<p>[4] Fudenberg, D. & Levine, D.K. (1997) The Theory of Learning in Games. Cambridge, MA: The MIT Press.</p>
<p>[5] Littlestone, N. & Warmuth, M.K. (1994) The weighted majority algorithm. Information and Computation 108 (2):212–261.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
