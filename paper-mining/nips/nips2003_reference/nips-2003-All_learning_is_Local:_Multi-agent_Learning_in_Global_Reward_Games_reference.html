<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-20" href="../nips2003/nips-2003-All_learning_is_Local%3A_Multi-agent_Learning_in_Global_Reward_Games.html">nips2003-20</a> <a title="nips-2003-20-reference" href="#">nips2003-20-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>20 nips-2003-All learning is Local: Multi-agent Learning in Global Reward Games</h1>
<br/><p>Source: <a title="nips-2003-20-pdf" href="http://papers.nips.cc/paper/2476-all-learning-is-local-multi-agent-learning-in-global-reward-games.pdf">pdf</a></p><p>Author: Yu-han Chang, Tracey Ho, Leslie P. Kaelbling</p><p>Abstract: In large multiagent games, partial observability, coordination, and credit assignment persistently plague attempts to design good learning algorithms. We provide a simple and efﬁcient algorithm that in part uses a linear system to model the world from a single agent’s limited perspective, and takes advantage of Kalman ﬁltering to allow an agent to construct a good training signal and learn an effective policy. 1</p><br/>
<h2>reference text</h2><p>[Auer et al., 1995] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. Schapire. Gambling in a rigged casino: the adversarial multi-armed bandit problem. In Proceedings of the 36th Annual Symposium on Foundations of Computer Science, 1995. [Chang et al., 2003] Y. Chang, T. Ho, and L. P. Kaelbling. Reinforcement learning in mobilized ad-hoc networks. MIT AI Lab Memo AIM-2003-025, 2003. [Choi et al., 1999] S. Choi, D. Yeung, and N. Zhang. Hidden-mode Markov decision processes. In IJCAI Workshop on Neural, Symbolic, and Reinforcement Methods for Sequence Learning, 1999. [Claus and Boutilier, 1998] Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative multiaent systems. In Proceedings of the 15th AAAI, 1998. [Kalman, 1960] R. E. Kalman. A new approach to linear ﬁltering and prediction problems. Transactions of the American Society of Mechanical Engineers, Journal of Basic Engineering, 1960. [McMahan et al., 2003] H. McMahan, G. Gordon, and A. Blum. Planning in the presence of cost functions controlled by an adversary. In Proceedings of the 20th ICML, 2003. [Ng et al., 1999] Andrew Y. Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations: theory and application to reward shaping. In Proc. 16th ICML, 1999. [Sutton and Barto, 1999] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, 1999. [Szita et al., 2002] Istvan Szita, Balimt Takacs, and Andras Lorincz. e-mdps: Learning in varying environments. Journal of Machine Learning Research, 2002. [Wolpert and Tumer, 1999] D. Wolpert and K. Tumer. An introduction to collective intelligence. Tech Report NASA-ARC-IC-99-63, 1999.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
