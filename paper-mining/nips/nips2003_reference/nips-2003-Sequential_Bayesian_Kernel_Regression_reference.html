<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>176 nips-2003-Sequential Bayesian Kernel Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-176" href="../nips2003/nips-2003-Sequential_Bayesian_Kernel_Regression.html">nips2003-176</a> <a title="nips-2003-176-reference" href="#">nips2003-176-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>176 nips-2003-Sequential Bayesian Kernel Regression</h1>
<br/><p>Source: <a title="nips-2003-176-pdf" href="http://papers.nips.cc/paper/2362-sequential-bayesian-kernel-regression.pdf">pdf</a></p><p>Author: Jaco Vermaak, Simon J. Godsill, Arnaud Doucet</p><p>Abstract: We propose a method for sequential Bayesian kernel regression. As is the case for the popular Relevance Vector Machine (RVM) [10, 11], the method automatically identiﬁes the number and locations of the kernels. Our algorithm overcomes some of the computational difﬁculties related to batch methods for kernel regression. It is non-iterative, and requires only a single pass over the data. It is thus applicable to truly sequential data sets and batch data sets alike. The algorithm is based on a generalisation of Importance Sampling, which allows the design of intuitively simple and efﬁcient proposal distributions for the model parameters. Comparative results on two standard data sets show our algorithm to compare favourably with existing batch estimation strategies.</p><br/>
<h2>reference text</h2><p>[1] C. M. Bishop and M. E. Tipping. Variational relevance vector machines. In C. Boutilier and M. Goldszmidt, editors, Proceedings of the 16th Conference on Uncertainty in Artiﬁcial Intelligence, pages 46–53. Morgan Kaufmann, 2000.</p>
<p>[2] D. Crisan. Particle ﬁlters – a theoretical perspective. In A. Doucet, J. F. G. de Freitas, and N. J. Gordon, editors, Sequential Monte Carlo Methods in Practice, pages 17–38. Springer-Verlag, 2001.</p>
<p>[3] A. Doucet, J. F. G. de Freitas, and N. J. Gordon, editors. Sequential Monte Carlo Methods in Practice. Springer-Verlag, New York, 2001.</p>
<p>[4] N. J. Gordon, D. J. Salmond, and A. F. M. Smith. Novel approach to nonlinear/non-Gaussian Bayesian state estimation. IEE Proceedings-F, 140(2):107–113, 1993.</p>
<p>[5] P. J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. Biometrika, 82(4):711–732, 1995.</p>
<p>[6] G. Kitagawa. Monte Carlo ﬁlter and smoother for non-Gaussian nonlinear state space models. Journal of Computational and Graphical Statistics, 5(1):1–25, 1996.</p>
<p>[7] P. Del Moral and A. Doucet. Sequential Monte Carlo samplers. Technical Report CUED/FINFENG/TR.443, Signal Processing Group, Cambridge University Engineering Department, 2002.</p>
<p>[8] R. M. Neal. Assessing relevance determination methods using DELVE. In C. M. Bishop, editor, Neural Networks and Machine Learning, pages 97–129. Springer-Verlag, 1998.</p>
<p>[9] S. S. Tham, A. Doucet, and R. Kotagiri. Sparse Bayesian learning for regression and classiﬁcation using Markov chain Monte Carlo. In Proceedings of the International Conference on Machine Learning, pages 634–643, 2002. ¨</p>
<p>[10] M. E. Tipping. The relevance vector machine. In S. A. Solla, T. K. Leen, and K. R. Muller, editors, Advances in Neural Information Processing Systems, volume 12, pages 652–658. MIT Press, 2000.</p>
<p>[11] M. E. Tipping. Sparse Bayesian learning and the relevance vector machine. Journal of Machine Learning Research, 1:211–244, 2001.</p>
<p>[12] M. E. Tipping and A. C. Faul. Fast marginal likelihood maximisation for sparse Bayesian models. In C. M. Bishop and B. J. Frey, editors, Proceedings of the Ninth International Workshop on Artiﬁcial Intelligence and Statistics, 2003.</p>
<p>[13] V. N. Vapnik. Statistical Learning Theory. John Wiley and Sons, New York, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
