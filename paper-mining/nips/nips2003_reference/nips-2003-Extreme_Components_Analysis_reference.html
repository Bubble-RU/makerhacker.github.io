<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>66 nips-2003-Extreme Components Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-66" href="../nips2003/nips-2003-Extreme_Components_Analysis.html">nips2003-66</a> <a title="nips-2003-66-reference" href="#">nips2003-66-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>66 nips-2003-Extreme Components Analysis</h1>
<br/><p>Source: <a title="nips-2003-66-pdf" href="http://papers.nips.cc/paper/2517-extreme-components-analysis.pdf">pdf</a></p><p>Author: Max Welling, Christopher Williams, Felix V. Agakov</p><p>Abstract: Principal components analysis (PCA) is one of the most widely used techniques in machine learning and data mining. Minor components analysis (MCA) is less well known, but can also play an important role in the presence of constraints on the data distribution. In this paper we present a probabilistic model for “extreme components analysis” (XCA) which at the maximum likelihood solution extracts an optimal combination of principal and minor components. For a given number of components, the log-likelihood of the XCA model is guaranteed to be larger or equal than that of the probabilistic models for PCA and MCA. We describe an efﬁcient algorithm to solve for the globally optimal solution. For log-convex spectra we prove that the solution consists of principal components only, while for log-concave spectra the solution consists of minor components. In general, the solution admits a combination of both. In experiments we explore the properties of XCA on some synthetic and real-world datasets.</p><br/>
<h2>reference text</h2><p>[1] G.E. Hinton. Products of experts. In Proceedings of the International Conference on Artiﬁcial Neural Networks, volume 1, pages 1–6, 1999.</p>
<p>[2] J.G. Proakis and D.G. Manolakis. Digital Signal Processing: Principles, Algorithms and Applications. Macmillan, 1992.</p>
<p>[3] S.T. Roweis. Em algorithms for pca and spca. In Advances in Neural Information Processing Systems, volume 10, pages 626–632, 1997.</p>
<p>[4] M.E. Tipping and C.M. Bishop. Probabilistic principal component analysis. Journal of the Royal Statistical Society, Series B, 21(3):611–622, 1999.</p>
<p>[5] M. Welling, R.S. Zemel, and G.E. Hinton. A tractable probabilistic model for projection pursuit. In Proceedings of the Conference on Uncertainty in Artiﬁcial Intelligence, 2003. accepted for publication.</p>
<p>[6] C.K.I. Williams and F.V. Agakov. Products of gaussians and probabilistic minor components analysis. Neural Computation, 14(5):1169–1182, 2002.</p>
<p>[7] H. Zhu, C. K. I. Williams, R. J. Rohwer, and M. Morciniec. Gaussian regression and optimal ﬁnite dimensional linear models. In C. M. Bishop, editor, Neural Networks and Machine Learning. Springer-Verlag, Berlin, 1998.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
