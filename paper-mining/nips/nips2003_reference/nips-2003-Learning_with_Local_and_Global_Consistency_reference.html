<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>113 nips-2003-Learning with Local and Global Consistency</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-113" href="../nips2003/nips-2003-Learning_with_Local_and_Global_Consistency.html">nips2003-113</a> <a title="nips-2003-113-reference" href="#">nips2003-113-reference</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>113 nips-2003-Learning with Local and Global Consistency</h1>
<br/><p>Source: <a title="nips-2003-113-pdf" href="http://papers.nips.cc/paper/2506-learning-with-local-and-global-consistency.pdf">pdf</a></p><p>Author: Dengyong Zhou, Olivier Bousquet, Thomas N. Lal, Jason Weston, Bernhard Schölkopf</p><p>Abstract: We consider the general problem of learning from labeled and unlabeled data, which is often called semi-supervised learning or transductive inference. A principled approach to semi-supervised learning is to design a classifying function which is sufﬁciently smooth with respect to the intrinsic structure collectively revealed by known labeled and unlabeled points. We present a simple algorithm to obtain such a smooth solution. Our method yields encouraging experimental results on a number of classiﬁcation problems and demonstrates effective use of unlabeled data. 1</p><br/>
<h2>reference text</h2><p>[1] J. R. Anderson. The architecture of cognition. Harvard Univ. press, Cambridge, MA, 1983.</p>
<p>[2] M. Belkin and P. Niyogi. Semi-supervised learning on manifolds. Machine Learning Journal, to appear.</p>
<p>[3] A. Blum and S. Chawla. Learning from labeled and unlabeled data using graph mincuts. In ICML, 2001.</p>
<p>[4] O. Chapelle, J. Weston, and B. Sch¨ lkopf. Cluster kernels for semi-supervised learno ing. In NIPS, 2002.</p>
<p>[5] D. DeCoste and B. Sch¨ lkopf. Training invariant support vector machines. Machine o Learning, 46:161–190, 2002.</p>
<p>[6] T. Joachims. Transductive learning via spectral graph partitioning. In ICML, 2003.</p>
<p>[7] J. Kandola, J. Shawe-Taylor, and N. Cristianini. Learning semantic similarity. In NIPS, 2002.</p>
<p>[8] R. I. Kondor and J. Lafferty. Diffusion kernels on graphs and other discrete input spaces. In ICML, 2002.</p>
<p>[9] A. Y. Ng, M. I. Jordan, and Y. Weiss. On spectral clustering: analysis and an algorithm. In NIPS, 2001.</p>
<p>[10] M. Seeger. Learning with labeled and unlabeled data. Technical report, The University of Edinburgh, 2000.</p>
<p>[11] J. Shrager, T. Hogg, and B. A. Huberman. Observation of phase transitions in spreading activation networks. Science, 236:1092–1094, 1987.</p>
<p>[12] A. Smola and R. I. Kondor. Kernels and regularization on graphs. In Learning Theory and Kernel Machines, Berlin - Heidelberg, Germany, 2003. Springer Verlag.</p>
<p>[13] M. Szummer and T. Jaakkola. Partially labeled classiﬁcation with markov random walks. In NIPS, 2001.</p>
<p>[14] V. N. Vapnik. Statistical learning theory. Wiley, NY, 1998.</p>
<p>[15] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervised learning using gaussian ﬁelds and harmonic functions. In ICML, 2003.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
