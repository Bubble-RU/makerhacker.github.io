<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>180 nips-2003-Sparseness of Support Vector Machines---Some Asymptotically Sharp Bounds</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-180" href="../nips2003/nips-2003-Sparseness_of_Support_Vector_Machines---Some_Asymptotically_Sharp_Bounds.html">nips2003-180</a> <a title="nips-2003-180-reference" href="#">nips2003-180-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>180 nips-2003-Sparseness of Support Vector Machines---Some Asymptotically Sharp Bounds</h1>
<br/><p>Source: <a title="nips-2003-180-pdf" href="http://papers.nips.cc/paper/2477-sparseness-of-support-vector-machines-some-asymptotically-sharp-bounds.pdf">pdf</a></p><p>Author: Ingo Steinwart</p><p>Abstract: The decision functions constructed by support vector machines (SVM’s) usually depend only on a subset of the training set—the so-called support vectors. We derive asymptotically sharp lower and upper bounds on the number of support vectors for several standard types of SVM’s. In particular, we show for the Gaussian RBF kernel that the fraction of support vectors tends to twice the Bayes risk for the L1-SVM, to the probability of noise for the L2-SVM, and to 1 for the LS-SVM. 1</p><br/>
<h2>reference text</h2><p>[1] C. Cortes and V. Vapnik. Support vector networks. Machine Learning, 20:1995, 273–297.</p>
<p>[2] J.A.K. Suykens and J. Vandewalle. Least squares support vector machine classiﬁers. Neural Processing Letters, 9:293–300, 1999.</p>
<p>[3] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68:337–404, 1950.</p>
<p>[4] N. Cristianini and J. Shawe-Taylor. An Introduction to Support Vector Machines. Cambridge University Press, 2000.</p>
<p>[5] B. Sch¨ lkopf, R. Herbrich, and A.J. Smola. A generalized representer theorem. o In Proceedings of the 14th Annual Conference on Computational Learning Theory, pages 416–426. Lecture Notes in Artiﬁcial Intelligence 2111, 2001.</p>
<p>[6] L. Devroye, L. Gy¨ rﬁ, and G. Lugosi. A Probabilistic Theory of Pattern Recognition. o Springer, New York, 1997.</p>
<p>[7] I. Steinwart. On the inﬂuence of the kernel on the consistency of support vector machines. Journal of Machine Learning Research, 2:67–93, 2001.</p>
<p>[8] I. Steinwart. Sparseness of support vector machines. Journal of Machine Learning Research, 4:1071–1105, 2003.</p>
<p>[9] I. Steinwart. Consistency of support vector machines and other regularized kernel machine. IEEE Transactions on Information Theory, to appear.</p>
<p>[10] R.M. Range. Holomorphic Functions and Integral Representations in Several Complex Variables. Springer, 1986.</p>
<p>[11] F. Girosi, M. Jones, and T. Poggio. Regularization theory and neural networks architectures. Neural Computation, 7:219–269, 1995.</p>
<p>[12] A. Kowalczyk. Sparsity of data representation of optimal kernel machine and leaveone-out estimator. In T.K. Leen, T.G. Dietterich, and V. Tresp, editors, Advances in Neural Information Processing Systems 13, pages 252–258. MIT Press, 2001.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
