<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2003" href="../home/nips2003_home.html">nips2003</a> <a title="nips-2003-4" href="../nips2003/nips-2003-A_Biologically_Plausible_Algorithm_for_Reinforcement-shaped_Representational_Learning.html">nips2003-4</a> <a title="nips-2003-4-reference" href="#">nips2003-4-reference</a> knowledge-graph by maker-knowledge-mining</p><h1>4 nips-2003-A Biologically Plausible Algorithm for Reinforcement-shaped Representational Learning</h1>
<br/><p>Source: <a title="nips-2003-4-pdf" href="http://papers.nips.cc/paper/2412-a-biologically-plausible-algorithm-for-reinforcement-shaped-representational-learning.pdf">pdf</a></p><p>Author: Maneesh Sahani</p><p>Abstract: Signiﬁcant plasticity in sensory cortical representations can be driven in mature animals either by behavioural tasks that pair sensory stimuli with reinforcement, or by electrophysiological experiments that pair sensory input with direct stimulation of neuromodulatory nuclei, but usually not by sensory stimuli presented alone. Biologically motivated theories of representational learning, however, have tended to focus on unsupervised mechanisms, which may play a signiﬁcant role on evolutionary or developmental timescales, but which neglect this essential role of reinforcement in adult plasticity. By contrast, theoretical reinforcement learning has generally dealt with the acquisition of optimal policies for action in an uncertain world, rather than with the concurrent shaping of sensory representations. This paper develops a framework for representational learning which builds on the relative success of unsupervised generativemodelling accounts of cortical encodings to incorporate the effects of reinforcement in a biologically plausible way. 1</p><br/>
<h2>reference text</h2><p>[1] R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 1998.</p>
<p>[2] M. I. Jordan, Z. Ghahramani, T. Jaakkola, and L. K. Saul. An introduction to variational methods for graphical models. Mach. Learning, 37(2):183–233, 1999.</p>
<p>[3] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive ﬁeld properties by learning a sparse code for natural images. Nature, 381(6583):607–9, 1996.</p>
<p>[4] A. J. Bell and T. J. Sejnowski. The ”independent components” of natural scenes are edge ﬁlters. Vision Res., 37(23):3327–3338, 1997.</p>
<p>[5] R. M. Neal and G. E. Hinton. A view of the EM algorithm that justiﬁes incremental, sparse, and other variants. In M. I. Jordan, ed., Learning in Graphical Models, pp. 355–370. Kluwer Academic Press, 1998.</p>
<p>[6] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and B. P. Flannery. Numerical Recipes in C: The Art of Scientiﬁc Computing. CUP, Cambridge, 2nd edition, 1993.</p>
<p>[7] B. S. Everitt. An Introduction to Latent Variable Models. Chapman and Hall, London, 1984.</p>
<br/>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
