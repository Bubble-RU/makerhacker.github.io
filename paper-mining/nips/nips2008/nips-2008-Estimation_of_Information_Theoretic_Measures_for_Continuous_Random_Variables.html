<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-76" href="#">nips2008-76</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</h1>
<br/><p>Source: <a title="nips-2008-76-pdf" href="http://papers.nips.cc/paper/3417-estimation-of-information-theoretic-measures-for-continuous-random-variables.pdf">pdf</a></p><p>Author: Fernando Pérez-Cruz</p><p>Abstract: We analyze the estimation of information theoretic measures of continuous random variables such as: differential entropy, mutual information or KullbackLeibler divergence. The objective of this paper is two-fold. First, we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with ﬁxed k converge almost surely, even though the k-nearest-neighbor density estimation with ﬁxed k does not converge to its true measure. Second, we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples. Nevertheless, these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent. We show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the Hilbert Schmidt independence criterion. 1</p><p>Reference: <a title="nips-2008-76-reference" href="../nips2008_reference/nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We analyze the estimation of information theoretic measures of continuous random variables such as: differential entropy, mutual information or KullbackLeibler divergence. [sent-2, score-0.94]
</p><p>2 First, we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with ﬁxed k converge almost surely, even though the k-nearest-neighbor density estimation with ﬁxed k does not converge to its true measure. [sent-4, score-0.975]
</p><p>3 Second, we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples. [sent-5, score-0.369]
</p><p>4 Nevertheless, these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent. [sent-6, score-0.293]
</p><p>5 We show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the Hilbert Schmidt independence criterion. [sent-7, score-0.378]
</p><p>6 1  Introduction  Kullback-Leibler divergence, mutual information and differential entropy are central to information theory [5]. [sent-8, score-0.796]
</p><p>7 The divergence [17] measures the ‘distance’ between two density distributions while mutual information measures the information one random variable contains about a related random variable [23]. [sent-9, score-1.152]
</p><p>8 For instance, the divergence is the error exponent in large deviation theory [5] and the divergence can be directly applied to solving the two-sample problem [1]. [sent-11, score-1.006]
</p><p>9 There are other relevant applications in different research areas in which divergence estimation is used to measure the difference between two density functions, such as multimedia [19] and text [13] classiﬁcation, among others. [sent-14, score-0.764]
</p><p>10 The estimation of information theoretic quantities can be traced back to the late ﬁfties [7], when Dobrushin estimated the differential entropy for one-dimensional random variables. [sent-15, score-0.519]
</p><p>11 [4] analyzes the different contributions of nonparametric differential entropy estimation for continuous random variables. [sent-17, score-0.489]
</p><p>12 The estimation of the divergence and mutual information for continuous random variables has been addressed by many different authors [25, 6, 26, 18, 20, 16], see also the references therein. [sent-18, score-1.141]
</p><p>13 For example, in [25], the authors propose to estimate the densities based on data-dependent histograms with a ﬁxed number of samples from q(x) in each bin. [sent-20, score-0.21]
</p><p>14 The authors of [6] compute relative frequencies on data-driven partitions achieving local independence for estimating mutual information. [sent-21, score-0.517]
</p><p>15 Also, in [20, 21], the authors compute the divergence using a variational approach, in which 1  See [22] for a detailed discussion on mutual information estimation in neuroscience. [sent-22, score-1.049]
</p><p>16 1  convergence is proven ensuring that the estimate for p(x)/q(x) or log p(x)/q(x) converges to the true measure ratio or its log ratio. [sent-23, score-0.408]
</p><p>17 There are only a handful of approaches that use k-nearest-neighbors (k-nn) density estimation [26, 18, 16] for estimating the divergence and mutual information for ﬁnite k. [sent-24, score-1.169]
</p><p>18 Although ﬁnite k-nn density estimation does not converge to the true measure, the authors are able to prove mean-square consistency of their divergence estimators imposing some regularity constraint over the densities. [sent-25, score-0.911]
</p><p>19 These proofs are based on the results reported in [15] for estimating the differential entropy with k-nn density estimation. [sent-26, score-0.464]
</p><p>20 First, we prove almost sure convergence of our divergence estimate based on k-nn density estimation with ﬁnite k. [sent-28, score-0.991]
</p><p>21 We can readily apply this result to the estimation of the differential entropy and mutual information. [sent-30, score-0.808]
</p><p>22 Second, we show that for k linearly growing with the number of samples, our estimates do not converge nor present known statistics. [sent-31, score-0.22]
</p><p>23 We show that for this choice of k, the estimates of the divergence or mutual information perform, respectively, as well as the maximum mean discrepancy (MMD) test in [9] and the Hilbert Schmidt independence criterion (HSIC) proposed in [10]. [sent-33, score-1.15]
</p><p>24 We prove in Section 2 the almost sure convergence of the divergence estimate based on k-nn density estimation with ﬁxed k. [sent-35, score-0.991]
</p><p>25 We extend this result for differential entropy and mutual information in Section 3. [sent-36, score-0.76]
</p><p>26 In Section 4 we present some examples to illustrate the convergence of our estimates and to show how can they be used to assess the independence of related random variables. [sent-37, score-0.196]
</p><p>27 2  Estimation of the Kullback-Leibler Divergence  If the densities P and Q exist with respect to a Lebesgue measure, the Kullback-Leibler divergence is given by: p(x) D(P ||Q) = p(x) log dx ≥ 0. [sent-39, score-0.596]
</p><p>28 (1) q(x) Rd This divergence is ﬁnite whenever P is absolutely continuous with respect to Q and it is zero only if P = Q. [sent-40, score-0.675]
</p><p>29 The idea of using k-nn density estimation to estimate the divergence was put forward in [26, 18], where they prove mean-square consistency of their estimator for ﬁnite k. [sent-41, score-0.878]
</p><p>30 In this paper, we prove the almost sure convergence of this divergence estimator, using waiting-times distributions without needing to impose additional conditions over the density models. [sent-42, score-0.819]
</p><p>31 Before proving (2) converges almost surely to D(P ||Q), let us show an intermediate necessary result. [sent-50, score-0.307]
</p><p>32 samples, X = {xi }n , from an absolutely continuous probability distrii=1 bution P , the limiting distribution of p(x)/p1 (x) is exponentially distributed with unit mean for any x in the support of p(x). [sent-55, score-0.426]
</p><p>33 The set Sx,R = {xi | xi − x 2 ≤ R, xi ∈ X } contains all the samples from X inside the ball centered in x of radius R. [sent-58, score-0.461]
</p><p>34 The samples in { xi − x d | xi ∈ Sx,R } are consequently uniformly distributed between 0 and Rd . [sent-60, score-0.455]
</p><p>35 2 Thereby, the limiting distribution of r1 (x)d = minxj ∈Sx,R ( xj − x d ) is exponentially distributed, 2 as it measures the waiting time between the origin and the ﬁrst event of a uniformly-spaced sample (see Theorem 2. [sent-61, score-0.334]
</p><p>36 Therefore, as n tends to inﬁnity p(arg minxj ∈Sx,R ( xj − x d )) → p(x) 2 and the limiting distribution of p(x)/p1 (x) is a unit-mean exponential distribution. [sent-65, score-0.269]
</p><p>37 samples, X = {xi }n , from an absolutely continuous probability distrii=1 bution P , the limiting distribution of p(x)/pk (x) is a unit-mean 1/k-variance gamma distribution for any x in the support of p(x). [sent-70, score-0.494]
</p><p>38 samples, X = {xi }n , from an absolutely continuous probability i=1 P  distribution P , then pk (x) → p(x) for any x in the support of p(x), if k → ∞ and k/n → 0, as n → ∞. [sent-78, score-0.357]
</p><p>39 Thereby the limiting distribution of p(x)/pk (x) is a unit-mean 1/k-variance gamma distribution. [sent-81, score-0.264]
</p><p>40 As k → ∞ the variance of the gamma distribution goes to zero and consequently pk (x) converges to p(x). [sent-82, score-0.358]
</p><p>41 The second corollary is the widely known result that k-nn density estimation converges to the true measure if k → ∞ and k/n → 0. [sent-83, score-0.349]
</p><p>42 For this growth on k, the divergence estimate does not converge to D(P ||Q). [sent-86, score-0.683]
</p><p>43 Now we can prove the almost surely convergence to (1) of the estimate in (2) based on the k-nn density estimation. [sent-87, score-0.52]
</p><p>44 Let P and Q be absolutely continuous probability measures and let P be absolutely continuous with respect to Q. [sent-89, score-0.41]
</p><p>45 The limiting distributions of p(xi )/pk (xi ) and q(xi )/qk (xi ) are unit-mean 1/k-variance gamma distributions, independent of i, p(x) and q(x) (see Corollary 1). [sent-97, score-0.294]
</p><p>46 In the large sample limit: 1 n  n  log i=1  p(xi ) pk (xi )  a. [sent-98, score-0.246]
</p><p>47 3  ∞  log(z)z k−1 e−kz dz 0  (7)  Finally, the sum of almost surely convergent terms also converges almost surely [11], which completes our proof. [sent-102, score-0.631]
</p><p>48 The k-nn based divergence estimator is biased, because the convergence rate of p(xi )/pk (xi ) and q(xi )/qk (xi ) to the unit-mean 1/k-variance gamma distribution depends on the density models and we should not expect them to be identical. [sent-103, score-0.782]
</p><p>49 If p(x) = q(x), the divergence is zero and our estimate is unbiased for any k (even if k/n does not tend to zero), since the statistics of the second and third term in (6) are identical and they cancel each other out for any n (their expected mean is the same). [sent-104, score-0.575]
</p><p>50 We use the Monte Carlo based test described in [9] with our divergence estimator to solve the two-sample problem and decide if the samples from X and X actually came from the same distribution. [sent-105, score-0.636]
</p><p>51 3  Differential Entropy and Mutual Information Estimation  The results obtained for the divergence can be readily applied to estimate the differential entropy of a random variable or the mutual information between two correlated random variables. [sent-106, score-1.335]
</p><p>52 The differential entropy for an absolutely continuous random variable P is given by: h(x) = −  p(x) log p(x)dx  (8)  We can estimate the differential entropy given a set with n i. [sent-107, score-0.979]
</p><p>53 samples from P , X = {xi }n , i=1 using k-nn density estimation as follows: hk (x) = −  1 n  log pk (xi )  (9)  i=1  where pk (xi ) is given by (3). [sent-110, score-0.724]
</p><p>54 Let P be an absolutely continuous probability measure and let X = {xi }n be i. [sent-112, score-0.215]
</p><p>55 We can rearrange hk (x) in (9) as follows: hk (x) = −  1 n  n  log pk (xi ) = − i=1  1 n  n  log p(xi ) + i=1  1 n  n  log i=1  p(xi ) pk (xi )  (12)  The ﬁrst term is the empirical estimate of (9) and, by the law of large numbers [11], it converges almost surely to its mean, h(x). [sent-121, score-1.162]
</p><p>56 The limiting distributions of p(xi )/pk (xi ) is a unit-mean 1/k-variance gamma distribution, independent of i and p(x) (see Corollary 1). [sent-122, score-0.294]
</p><p>57 In the large sample limit: 1 n  n  log i=1  p(xi ) pk (xi )  a. [sent-123, score-0.246]
</p><p>58 Finally, the sum of almost surely convergent terms also converges almost surely [11], which completes our proof. [sent-127, score-0.582]
</p><p>59 Now, we can use the expansion of the conditional differential entropy, mutual information and conditional mutual information to prove the convergence of their estimates based on k-nn density estimation to their values. [sent-128, score-1.443]
</p><p>60 4  • Conditional differential entropy: h(y|x) = − h(y|x) = −  p(x, y) log 1 n  n  log i=1  p(y, x) dxdy p(x)  p(yi , xi ) p(xi )  (14)  a. [sent-129, score-0.542]
</p><p>61 −→  h(y|x)  (15)  • Mutual Information: I(x; y) = − I(x; |y) =  1 n  p(x, y) log n  p(y, x) dxdy p(x)p(y)  p(yi , xi ) p(xi )p(yi )  log i=1  (16)  a. [sent-131, score-0.381]
</p><p>62 −→  I(x; y) + γk  (17)  • Conditional Mutual Information: I(x; y|z) = I(x; y|z) =  4  p(x, y, z) log 1 n  n  log i=1  p(y, x, z)p(z) dxdydz p(x, z)p(y, z)  p(yi , xi , zi )p(zi ) p(xi , zi )p(yi , zi )  a. [sent-133, score-0.435]
</p><p>63 In the ﬁrst one, we show the convergence of the divergence to their limiting value as the number of samples tends to inﬁnity and we compare the divergence estimation to the MMD test in [9] for MNIST dataset. [sent-136, score-1.425]
</p><p>64 We ﬁrst compare the divergence between a uniform distribution between 0 and 1 in d-dimension and a zero-mean Gaussian distribution with identity covariance matrix. [sent-138, score-0.487]
</p><p>65 √ plot the divergence estimates We for d = 1 and d = 5 in Figure 1 as a function of n, for k = 1, k = n and k = n/2 with m = n. [sent-139, score-0.613]
</p><p>66 7 2 10  4  n  10  (a)  3  10  4  n  10  (b)  Figure 1: We plot the divergence for d = 1 in (a) and d = 5 in (b). [sent-157, score-0.529]
</p><p>67 The solid line with ’ ’ represents the divergence estimate for k = 1, the solid line with ’∗’ represents the divergence estimate for √ k = n, the solid line with ’◦’ represents the divergence estimate for k = n/2 and the dasheddotted line represents the divergence. [sent-158, score-2.022]
</p><p>68 The dashed-lines represent ±3 standard deviation for each divergence estimate. [sent-159, score-0.487]
</p><p>69 As expected, the divergence estimate for k = n/2 does not converge to the true divergence as the limiting distributions of p(x)/pk (x) and q(x)/qk (x) are unknown and they depend on p(x) and 5  q(x), respectively. [sent-161, score-1.34]
</p><p>70 Nevertheless, this estimate converges faster to its limiting value and its variance √ is much smaller than that provided by the estimates of the divergence with k = n or k = 1. [sent-162, score-0.958]
</p><p>71 √ Both divergence estimates for k = 1 and k = n converge to the true divergence as the number of samples tends to inﬁnity. [sent-164, score-1.272]
</p><p>72 The convergence of the divergence estimate for k = 1 is signiﬁcantly √ faster than that with k = n, because p(x)/p1 (x) converges much faster to its limiting distribution √ (x). [sent-165, score-0.952]
</p><p>73 p(x)/p (x) converges faster because the nearest neighbor to x is much closer than p(x)/p n 1 √ than the n-nearest-neighbor and we need that the k-nn to be close enough to x for p(x)/pk (x) to be close to its limiting distribution. [sent-166, score-0.299]
</p><p>74 As d grows the divergence estimates need many more samples to converge and even for small dimensions the number of samples can be enormously large. [sent-167, score-0.797]
</p><p>75 Nevertheless, we can still use this divergence estimate to assess whether two sets of samples come from the same distribution, because the divergence estimate for p(x) = q(x) is unbiased for any k. [sent-168, score-1.239]
</p><p>76 In Figure 2(a) we plot the divergence estimate between the three’s and two’s handwritten digits in the MNIST dataset (http://yann. [sent-169, score-0.617]
</p><p>77 In Figure 2(a) we plot the divergence estimator for D1 (3, 2) (solid line) and D1 (3, 3) (dashed line) mean values for 100 experiments together with their 90% conﬁdence interval. [sent-172, score-0.572]
</p><p>78 But we can see that the conﬁdence interval for the MMD test decreases faster than the test based on the divergence estimate with k = 1 and we should expect better performance for larger n, similar to the divergence estimate with k = n/2. [sent-182, score-1.318]
</p><p>79 In the second example we compute the mutual information between y1 and y2 , which are given by: y1 cos(θ) = y2 − sin(θ)  sin(θ) cos(θ)  x1 x2  (20)  where x1 and x2 are independent and uniformly distributed between 0 and 1, and θ ∈ [0, π/4]. [sent-195, score-0.503]
</p><p>80 We compute the mutual information with k = 1, k = n and k = n/2 for our test, and compare it to the HSIC in [10]. [sent-201, score-0.447]
</p><p>81 The solid line uses the mutual information estimate with k = n/2 and the dash-dotted line uses√ HSIC. [sent-232, score-0.656]
</p><p>82 The dashed and the dotted lines, respectively, use the mutual information estimate with k = n and k = 1. [sent-233, score-0.586]
</p><p>83 The HSIC test and the mutual information estimate based test with k = n/2 perform equally well at predicting whether y1 and y2√ independent, while the test based on the mutual information are estimates with k = 1 and k = n clearly underperforms. [sent-234, score-1.207]
</p><p>84 This example shows that if our goal is to predict whether two random variables are independent we are better off using HSIC or a nonconvergent estimate of the mutual information rather than trying to compute the mutual information as accurately as possible. [sent-235, score-1.117]
</p><p>85 Furthermore, in our test, the computational complexity of computing HSIC for n = 5000 is over 10 times more computationally costly (running time) than computing the mutual information for k = n/22 . [sent-236, score-0.447]
</p><p>86 As we saw in the case of the divergence estimate in Figure 1, mutual information is more accurately estimated when k = 1, but at the cost of a higher variance. [sent-237, score-1.022]
</p><p>87 If our objective is to estimate the mutual information (or the divergence), we should use a small value of k, ideally k = 1. [sent-238, score-0.535]
</p><p>88 However, if we are interested in assessing whether two random variables are independent, it is better to use k = n/2, because the variance of the estimate is much lower, even though it does not converge to the mutual information (or the divergence). [sent-239, score-0.741]
</p><p>89 5  Conclusions  We have proved that the estimates of the differential entropy, mutual information and divergence based on k-nn density estimation for ﬁnite k converge almost surely, even though the density estimate does not converge. [sent-240, score-1.755]
</p><p>90 The previous literature could only prove mean-squared consistency and it required imposing some constraints over the density models. [sent-241, score-0.201]
</p><p>91 The proof in this paper relies on describing the limiting distribution of p(x)/pk (x). [sent-242, score-0.201]
</p><p>92 The divergence, mutual information and differential √ entropy estimates using k = 1 are much better than the estimates using k = n, even though for √ k = n we can prove that pk (x) converges to p(x) while for ﬁnite k this convergences does not occur. [sent-245, score-1.254]
</p><p>93 Finally, if we are interested in solving the two-sample problem or assessing if two random variables are independent, it is best to ﬁx k to a fraction of n (we have used k = n/2 in our experiments), although in this case the estimates do not converge to the true value. [sent-246, score-0.322]
</p><p>94 Two-sample test statistics for measuring discrepancies between two multivariate probability density functions using kernel-based density estimates. [sent-261, score-0.275]
</p><p>95 A simpliﬁed method for experimental estimate of the entropy of a stationary sequence. [sent-300, score-0.24]
</p><p>96 Sample estimate of the entropy of a random vector. [sent-361, score-0.24]
</p><p>97 A kullback-leibler divergence based kernel for svm classiﬁcation in multimedia applications. [sent-393, score-0.539]
</p><p>98 Nonparametric estimation of the likelihood ratio and divergence functionals. [sent-401, score-0.571]
</p><p>99 Estimating divergence functionals and the likelihood ratio by penalized convex risk minimization. [sent-411, score-0.487]
</p><p>100 A nearest-neighbor approach to estimating divergence between u continuous random vectors. [sent-445, score-0.59]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('divergence', 0.487), ('mutual', 0.411), ('mmd', 0.262), ('hsic', 0.185), ('xi', 0.185), ('limiting', 0.17), ('pk', 0.169), ('differential', 0.161), ('entropy', 0.152), ('surely', 0.144), ('absolutely', 0.122), ('density', 0.114), ('converge', 0.108), ('kld', 0.105), ('converges', 0.095), ('gamma', 0.094), ('estimate', 0.088), ('theoretic', 0.086), ('estimates', 0.084), ('kk', 0.084), ('estimation', 0.084), ('kz', 0.079), ('nonconvergent', 0.079), ('waiting', 0.078), ('log', 0.077), ('assessing', 0.072), ('almost', 0.068), ('continuous', 0.066), ('qk', 0.065), ('prove', 0.062), ('dk', 0.061), ('samples', 0.059), ('solid', 0.055), ('acept', 0.052), ('beirlant', 0.052), ('corolary', 0.052), ('distrii', 0.052), ('erlang', 0.052), ('minxj', 0.052), ('multimedia', 0.052), ('verd', 0.052), ('hk', 0.052), ('dence', 0.049), ('gretton', 0.049), ('dz', 0.049), ('law', 0.048), ('discrepancy', 0.047), ('test', 0.047), ('tends', 0.047), ('jmlr', 0.046), ('rearrange', 0.046), ('tests', 0.045), ('sure', 0.044), ('convergence', 0.044), ('estimator', 0.043), ('plot', 0.042), ('dxdy', 0.042), ('bution', 0.042), ('kulkarni', 0.042), ('sk', 0.041), ('interval', 0.04), ('princeton', 0.038), ('independence', 0.038), ('estimating', 0.037), ('information', 0.036), ('nevertheless', 0.036), ('measures', 0.034), ('faster', 0.034), ('platt', 0.034), ('nguyen', 0.034), ('fernando', 0.034), ('line', 0.033), ('completes', 0.033), ('solving', 0.032), ('densities', 0.032), ('zi', 0.032), ('ball', 0.032), ('schmidt', 0.032), ('authors', 0.031), ('describing', 0.031), ('acceptance', 0.031), ('cos', 0.031), ('independent', 0.03), ('assess', 0.03), ('convergent', 0.03), ('usa', 0.029), ('corollary', 0.029), ('mnist', 0.029), ('null', 0.028), ('growing', 0.028), ('measure', 0.027), ('variables', 0.026), ('lkopf', 0.026), ('sin', 0.026), ('dashed', 0.026), ('distributed', 0.026), ('nonparametric', 0.026), ('imposing', 0.025), ('wainwright', 0.025), ('dotted', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="76-tfidf-1" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>Author: Fernando Pérez-Cruz</p><p>Abstract: We analyze the estimation of information theoretic measures of continuous random variables such as: differential entropy, mutual information or KullbackLeibler divergence. The objective of this paper is two-fold. First, we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with ﬁxed k converge almost surely, even though the k-nearest-neighbor density estimation with ﬁxed k does not converge to its true measure. Second, we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples. Nevertheless, these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent. We show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the Hilbert Schmidt independence criterion. 1</p><p>2 0.20812751 <a title="76-tfidf-2" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>Author: Lev Faivishevsky, Jacob Goldberger</p><p>Abstract: In this paper we introduce the MeanNN approach for estimation of main information theoretic measures such as differential entropy, mutual information and divergence. As opposed to other nonparametric approaches the MeanNN results in smooth differentiable functions of the data samples with clear geometrical interpretation. Then we apply the proposed estimators to the ICA problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods. The improved performance of the proposed ICA algorithm is demonstrated on several test examples in comparison with state-ofthe-art techniques. 1</p><p>3 0.16095601 <a title="76-tfidf-3" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>Author: Novi Quadrianto, Le Song, Alex J. Smola</p><p>Abstract: Object matching is a fundamental operation in data analysis. It typically requires the deﬁnition of a similarity measure between the classes of objects to be matched. Instead, we develop an approach which is able to perform matching by requiring a similarity measure only within each of the classes. This is achieved by maximizing the dependency between matched pairs of observations by means of the Hilbert Schmidt Independence Criterion. This problem can be cast as one of maximizing a quadratic assignment problem with special structure and we present a simple algorithm for ﬁnding a locally optimal solution. 1</p><p>4 0.13690239 <a title="76-tfidf-4" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>Author: Xinhua Zhang, Le Song, Arthur Gretton, Alex J. Smola</p><p>Abstract: Many machine learning algorithms can be formulated in the framework of statistical independence such as the Hilbert Schmidt Independence Criterion. In this paper, we extend this criterion to deal with structured and interdependent observations. This is achieved by modeling the structures using undirected graphical models and comparing the Hilbert space embeddings of distributions. We apply this new criterion to independent component analysis and sequence clustering. 1</p><p>5 0.091632351 <a title="76-tfidf-5" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>Author: Koby Crammer, Mark Dredze, Fernando Pereira</p><p>Abstract: Conﬁdence-weighted (CW) learning [6], an online learning method for linear classiﬁers, maintains a Gaussian distributions over weight vectors, with a covariance matrix that represents uncertainty about weights and correlations. Conﬁdence constraints ensure that a weight vector drawn from the hypothesis distribution correctly classiﬁes examples with a speciﬁed probability. Within this framework, we derive a new convex form of the constraint and analyze it in the mistake bound model. Empirical evaluation with both synthetic and text data shows our version of CW learning achieves lower cumulative and out-of-sample errors than commonly used ﬁrst-order and second-order online methods. 1</p><p>6 0.09048906 <a title="76-tfidf-6" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>7 0.083074272 <a title="76-tfidf-7" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>8 0.081875078 <a title="76-tfidf-8" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>9 0.078888208 <a title="76-tfidf-9" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>10 0.076289698 <a title="76-tfidf-10" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>11 0.070693046 <a title="76-tfidf-11" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>12 0.070602946 <a title="76-tfidf-12" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>13 0.06802576 <a title="76-tfidf-13" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>14 0.061839357 <a title="76-tfidf-14" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>15 0.059492551 <a title="76-tfidf-15" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>16 0.057522908 <a title="76-tfidf-16" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>17 0.057398245 <a title="76-tfidf-17" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>18 0.056644943 <a title="76-tfidf-18" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>19 0.056312371 <a title="76-tfidf-19" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>20 0.05330053 <a title="76-tfidf-20" href="./nips-2008-Nonlinear_causal_discovery_with_additive_noise_models.html">153 nips-2008-Nonlinear causal discovery with additive noise models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.186), (1, -0.007), (2, -0.032), (3, 0.085), (4, 0.015), (5, 0.024), (6, 0.043), (7, 0.053), (8, 0.031), (9, 0.014), (10, 0.11), (11, -0.067), (12, 0.132), (13, -0.015), (14, -0.199), (15, 0.01), (16, 0.108), (17, 0.102), (18, 0.059), (19, 0.133), (20, -0.045), (21, 0.092), (22, -0.014), (23, 0.082), (24, 0.124), (25, -0.003), (26, 0.127), (27, -0.061), (28, 0.115), (29, -0.058), (30, -0.036), (31, -0.039), (32, 0.058), (33, -0.013), (34, -0.021), (35, 0.023), (36, -0.091), (37, 0.101), (38, 0.008), (39, -0.012), (40, 0.081), (41, 0.093), (42, 0.023), (43, 0.048), (44, -0.056), (45, -0.062), (46, -0.263), (47, 0.002), (48, -0.161), (49, -0.138)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96917897 <a title="76-lsi-1" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>Author: Fernando Pérez-Cruz</p><p>Abstract: We analyze the estimation of information theoretic measures of continuous random variables such as: differential entropy, mutual information or KullbackLeibler divergence. The objective of this paper is two-fold. First, we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with ﬁxed k converge almost surely, even though the k-nearest-neighbor density estimation with ﬁxed k does not converge to its true measure. Second, we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples. Nevertheless, these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent. We show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the Hilbert Schmidt independence criterion. 1</p><p>2 0.79532993 <a title="76-lsi-2" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>Author: Lev Faivishevsky, Jacob Goldberger</p><p>Abstract: In this paper we introduce the MeanNN approach for estimation of main information theoretic measures such as differential entropy, mutual information and divergence. As opposed to other nonparametric approaches the MeanNN results in smooth differentiable functions of the data samples with clear geometrical interpretation. Then we apply the proposed estimators to the ICA problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods. The improved performance of the proposed ICA algorithm is demonstrated on several test examples in comparison with state-ofthe-art techniques. 1</p><p>3 0.59048378 <a title="76-lsi-3" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>Author: Novi Quadrianto, Le Song, Alex J. Smola</p><p>Abstract: Object matching is a fundamental operation in data analysis. It typically requires the deﬁnition of a similarity measure between the classes of objects to be matched. Instead, we develop an approach which is able to perform matching by requiring a similarity measure only within each of the classes. This is achieved by maximizing the dependency between matched pairs of observations by means of the Hilbert Schmidt Independence Criterion. This problem can be cast as one of maximizing a quadratic assignment problem with special structure and we present a simple algorithm for ﬁnding a locally optimal solution. 1</p><p>4 0.54036087 <a title="76-lsi-4" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>Author: Jooseuk Kim, Clayton Scott</p><p>Abstract: We provide statistical performance guarantees for a recently introduced kernel classiﬁer that optimizes the L2 or integrated squared error (ISE) of a difference of densities. The classiﬁer is similar to a support vector machine (SVM) in that it is the solution of a quadratic program and yields a sparse classiﬁer. Unlike SVMs, however, the L2 kernel classiﬁer does not involve a regularization parameter. We prove a distribution free concentration inequality for a cross-validation based estimate of the ISE, and apply this result to deduce an oracle inequality and consistency of the classiﬁer on the sense of both ISE and probability of error. Our results also specialize to give performance guarantees for an existing method of L2 kernel density estimation. 1</p><p>5 0.46911848 <a title="76-lsi-5" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>Author: Iain Murray, Ruslan Salakhutdinov</p><p>Abstract: We present a simple new Monte Carlo algorithm for evaluating probabilities of observations in complex latent variable models, such as Deep Belief Networks. While the method is based on Markov chains, estimates based on short runs are formally unbiased. In expectation, the log probability of a test set will be underestimated, and this could form the basis of a probabilistic bound. The method is much cheaper than gold-standard annealing-based methods and only slightly more expensive than the cheapest Monte Carlo methods. We give examples of the new method substantially improving simple variational bounds at modest extra cost. 1</p><p>6 0.43221182 <a title="76-lsi-6" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>7 0.43083563 <a title="76-lsi-7" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>8 0.43055514 <a title="76-lsi-8" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>9 0.42059174 <a title="76-lsi-9" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>10 0.42017934 <a title="76-lsi-10" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>11 0.41539571 <a title="76-lsi-11" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>12 0.38020179 <a title="76-lsi-12" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>13 0.36582559 <a title="76-lsi-13" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>14 0.35789284 <a title="76-lsi-14" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>15 0.35299379 <a title="76-lsi-15" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>16 0.31172442 <a title="76-lsi-16" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>17 0.30304229 <a title="76-lsi-17" href="./nips-2008-Empirical_performance_maximization_for_linear_rank_statistics.html">72 nips-2008-Empirical performance maximization for linear rank statistics</a></p>
<p>18 0.29805234 <a title="76-lsi-18" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>19 0.29534379 <a title="76-lsi-19" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>20 0.29102632 <a title="76-lsi-20" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.013), (6, 0.091), (7, 0.081), (12, 0.018), (15, 0.012), (28, 0.194), (57, 0.055), (58, 0.268), (59, 0.014), (63, 0.032), (71, 0.032), (77, 0.065), (83, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80931729 <a title="76-lda-1" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>Author: Yang Wang, Greg Mori</p><p>Abstract: We present a discriminative part-based approach for human action recognition from video sequences using motion features. Our model is based on the recently proposed hidden conditional random ﬁeld (hCRF) for object recognition. Similar to hCRF for object recognition, we model a human action by a ﬂexible constellation of parts conditioned on image observations. Different from object recognition, our model combines both large-scale global features and local patch features to distinguish various actions. Our experimental results show that our model is comparable to other state-of-the-art approaches in action recognition. In particular, our experimental results demonstrate that combining large-scale global features and local patch features performs signiﬁcantly better than directly applying hCRF on local patches alone. 1</p><p>same-paper 2 0.79152769 <a title="76-lda-2" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>Author: Fernando Pérez-Cruz</p><p>Abstract: We analyze the estimation of information theoretic measures of continuous random variables such as: differential entropy, mutual information or KullbackLeibler divergence. The objective of this paper is two-fold. First, we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with ﬁxed k converge almost surely, even though the k-nearest-neighbor density estimation with ﬁxed k does not converge to its true measure. Second, we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples. Nevertheless, these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent. We show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the Hilbert Schmidt independence criterion. 1</p><p>3 0.67146719 <a title="76-lda-3" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>Author: Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</p><p>Abstract: Uncertainty is omnipresent when we perceive or interact with our environment, and the Bayesian framework provides computational methods for dealing with it. Mathematical models for Bayesian decision making typically require datastructures that are hard to implement in neural networks. This article shows that even the simplest and experimentally best supported type of synaptic plasticity, Hebbian learning, in combination with a sparse, redundant neural code, can in principle learn to infer optimal Bayesian decisions. We present a concrete Hebbian learning rule operating on log-probability ratios. Modulated by reward-signals, this Hebbian plasticity rule also provides a new perspective for understanding how Bayesian inference could support fast reinforcement learning in the brain. In particular we show that recent experimental results by Yang and Shadlen [1] on reinforcement learning of probabilistic inference in primates can be modeled in this way. 1</p><p>4 0.67138553 <a title="76-lda-4" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>5 0.67004275 <a title="76-lda-5" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>6 0.66953671 <a title="76-lda-6" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>7 0.66854078 <a title="76-lda-7" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>8 0.66832972 <a title="76-lda-8" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>9 0.66520774 <a title="76-lda-9" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>10 0.6651873 <a title="76-lda-10" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>11 0.66311753 <a title="76-lda-11" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>12 0.66308284 <a title="76-lda-12" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>13 0.66285104 <a title="76-lda-13" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>14 0.6627723 <a title="76-lda-14" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>15 0.66263843 <a title="76-lda-15" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>16 0.66205233 <a title="76-lda-16" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>17 0.66129154 <a title="76-lda-17" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>18 0.66119671 <a title="76-lda-18" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>19 0.66099697 <a title="76-lda-19" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>20 0.66020989 <a title="76-lda-20" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
