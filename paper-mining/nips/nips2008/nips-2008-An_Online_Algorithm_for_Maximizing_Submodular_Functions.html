<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>22 nips-2008-An Online Algorithm for Maximizing Submodular Functions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-22" href="#">nips2008-22</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>22 nips-2008-An Online Algorithm for Maximizing Submodular Functions</h1>
<br/><p>Source: <a title="nips-2008-22-pdf" href="http://papers.nips.cc/paper/3569-an-online-algorithm-for-maximizing-submodular-functions.pdf">pdf</a></p><p>Author: Matthew Streeter, Daniel Golovin</p><p>Abstract: We present an algorithm for solving a broad class of online resource allocation problems. Our online algorithm can be applied in environments where abstract jobs arrive one at a time, and one can complete the jobs by investing time in a number of abstract activities, according to some schedule. We assume that the fraction of jobs completed by a schedule is a monotone, submodular function of a set of pairs (v, τ ), where τ is the time invested in activity v. Under this assumption, our online algorithm performs near-optimally according to two natural metrics: (i) the fraction of jobs completed within time T , for some ﬁxed deadline T > 0, and (ii) the average time required to complete each job. We evaluate our algorithm experimentally by using it to learn, online, a schedule for allocating CPU time among solvers entered in the 2007 SAT solver competition. 1</p><p>Reference: <a title="nips-2008-22-reference" href="../nips2008_reference/nips-2008-An_Online_Algorithm_for_Maximizing_Submodular_Functions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract We present an algorithm for solving a broad class of online resource allocation problems. [sent-5, score-0.25]
</p><p>2 Our online algorithm can be applied in environments where abstract jobs arrive one at a time, and one can complete the jobs by investing time in a number of abstract activities, according to some schedule. [sent-6, score-0.725]
</p><p>3 We assume that the fraction of jobs completed by a schedule is a monotone, submodular function of a set of pairs (v, τ ), where τ is the time invested in activity v. [sent-7, score-1.041]
</p><p>4 Under this assumption, our online algorithm performs near-optimally according to two natural metrics: (i) the fraction of jobs completed within time T , for some ﬁxed deadline T > 0, and (ii) the average time required to complete each job. [sent-8, score-0.532]
</p><p>5 We evaluate our algorithm experimentally by using it to learn, online, a schedule for allocating CPU time among solvers entered in the 2007 SAT solver competition. [sent-9, score-0.807]
</p><p>6 1  Introduction  This paper presents an algorithm for solving the following class of online resource allocation problems. [sent-10, score-0.25]
</p><p>7 A job is a function f : S → [0, 1], where for any schedule S ∈ S, f (S) represents the proportion of some task that is accomplished by performing the sequence of actions S. [sent-15, score-0.89]
</p><p>8 (submodularity) for any schedules S1 , S2 ∈ S and any action a ∈ V × R>0 , fa (S1 ⊕ S2 ) ≤ fa (S1 ), where we deﬁne fa (S) ≡ f (S ⊕ a ) − f (S). [sent-18, score-0.57]
</p><p>9 The second objective is to minimize the cost of a schedule, which we deﬁne as ∞  1−f S  c (f, S) =  t  dt  t=0  where S t is the schedule that results from truncating schedule S at time t. [sent-22, score-1.22]
</p><p>10 Thus c (f, S) is the expected time we must wait before the desired event occurs if we execute actions according to the schedule S. [sent-33, score-0.73]
</p><p>11 Let each activity v represent a randomized algorithm for solving some decision problem, and let the action (v, τ ) represent running the algorithm (with a fresh random seed) for time τ . [sent-36, score-0.328]
</p><p>12 Fix some particular instance of the decision problem, and for any schedule S, let f (S) be the probability that one (or more) of the runs in the sequence S yields a solution to that instance. [sent-37, score-0.667]
</p><p>13 So f (S T ) is (by deﬁnition) the probability that performing the runs in schedule S yields a solution to the problem instance in time ≤ T , while c (f, S) is the expected time that elapses before a solution is obtained. [sent-38, score-0.711]
</p><p>14 The fact that f is submodular can be seen as follows. [sent-40, score-0.2]
</p><p>15 For any schedule S and action a, fa (S) equals the probability that action a succeeds after every action in S has failed, which can also be written as (1 − f (S)) · f ( a ). [sent-41, score-1.05]
</p><p>16 This, together with the monotonicity of f , implies that for any schedules S1 , S2 and any action a, we have fa (S1 ⊕ S2 ) = (1 − f (S1 ⊕ S2 )) · f ( a ) ≤ (1 − f (S1 )) · f ( a ) = fa (S1 ). [sent-42, score-0.454]
</p><p>17 In the online setting, an arbitrary sequence f (1) , f (2) , . [sent-43, score-0.256]
</p><p>18 , f (n) of jobs arrive one at a time, and we must ﬁnish each job (via some schedule) before moving on to the next job. [sent-46, score-0.363]
</p><p>19 When selecting a schedule S (i) to use to ﬁnish job f (i) , we have knowledge of the previous jobs f (1) , f (2) , . [sent-47, score-0.934]
</p><p>20 In this setting we aim to minimize regret, which measures the difference between the average cost (or average beneﬁt) of the schedules produced by our online algorithm and that of the best single schedule (in hindsight) for the given sequence of jobs. [sent-51, score-0.96]
</p><p>21 1  Problems that ﬁt into this framework  A number of previously-studied problems can be cast as the task of computing a schedule S that n 1 minimizes c (f, S), where f is of the form f (S) = n i=1 1 − (v,τ )∈S (1 − pi (v, τ )) . [sent-53, score-0.626]
</p><p>22 This expression can be interpreted as follows: the job f consists of n subtasks, and pi (v, τ ) is the probability that investing time τ in activity v completes the ith subtask. [sent-54, score-0.322]
</p><p>23 Assuming pi (v, τ ) is a non-decreasing function of τ for all i and v, it can be shown that any function f of this form is monotone and submodular. [sent-56, score-0.199]
</p><p>24 The problem of maximizing f (S T ) is a slight generalization of the problem of maximizing a monotone submodular set function subject to a knapsack constraint [14, 20] (which in turn generalizes B UDGETED M AXIMUM C OVERAGE [12], which generalizes M AX k-C OVERAGE [16]). [sent-60, score-0.625]
</p><p>25 The only difference between the two problems is that, in the latter problem, f (S) may only depend on the set of actions in the sequence S, and not on the order in which the actions appear. [sent-61, score-0.274]
</p><p>26 An algorithm portfolio [9] is a schedule for interleaving the execution of multiple (randomized) algorithms and periodically restarting them with a fresh random seed. [sent-66, score-0.745]
</p><p>27 In database query processing, one must extract all the records in a database that satisfy every predicate in a list of one or more predicates (the conjunction of predicates comprises the query). [sent-72, score-0.267]
</p><p>28 To process the query, each record is evaluated against the predicates one at a time until the record either fails to satisfy some predicate (in which case it does not match the query) or all predicates have been examined. [sent-73, score-0.271]
</p><p>29 Our work addresses the online version of this problem, which arises naturally in practice. [sent-78, score-0.218]
</p><p>30 Many sensor placement problems can be optimally solved by maximizing a monotone submodular set function subject to a knapsack constraint [13], a special case of our beneﬁt-maximization problem (see §1. [sent-84, score-0.514]
</p><p>31 Our online algorithms could be used to select sensor placements when the same set of sensors is repeatedly deployed in an unknown or adversarial environment. [sent-86, score-0.293]
</p><p>32 In the online setting we provide an online algorithm whose worst-case performance guarantees approach those of the ofﬂine greedy approximation algorithm asymptotically (as the number of jobs approaches inﬁnity). [sent-91, score-0.839]
</p><p>33 We then show how to modify our online algorithm for use in several different “bandit” feedback settings. [sent-92, score-0.34]
</p><p>34 Several of these problems have been considered in the online setting. [sent-97, score-0.218]
</p><p>35 [15] gave an online algorithm for P IPELINED S ET C OVER that is asymptotically O (log |V|)-competitive. [sent-99, score-0.323]
</p><p>36 Our ofﬂine beneﬁt-maximization problem generalizes the problem of maximizing a monotone submodular set function subject to a knapsack constraint. [sent-103, score-0.523]
</p><p>37 To our knowledge, none of these problems have previously been studied in an online setting. [sent-105, score-0.218]
</p><p>38 Note that our problem is quite different from online set covering problems (e. [sent-106, score-0.218]
</p><p>39 In this paper we convert a speciﬁc greedy approximation algorithm into an online algorithm. [sent-109, score-0.35]
</p><p>40 [10] gave a generic procedure for converting an α-approximation algorithm into an online algorithm that is asymptotically α-competitive. [sent-111, score-0.355]
</p><p>41 [17] developed a no-regret algorithm for the online version of M AX k-C OVERAGE, and applied it to online ranking. [sent-114, score-0.468]
</p><p>42 Our goal is to compute a schedule S that achieves one of two objectives, either minimizing the cost c (f S) or maximizing f (S) subject to the constraint (S) T . [sent-118, score-0.626]
</p><p>43 We now consider an arbitrary schedule G, whose j th action is gj = (vj f(v  j ). [sent-124, score-0.779]
</p><p>44 , greedily appending actions to the schedule so as to maximize the resulting increase in f per unit time). [sent-129, score-0.75]
</p><p>45 For any schedule S, any positive integer j, and any t > 0, f (S t )  f (Gj )+t (sj + j ). [sent-132, score-0.571]
</p><p>46 For any schedule L + j=1 Ej j , where Ej  4 c (f S ). [sent-138, score-0.571]
</p><p>47 Given a set of jobs f (1) f (2) f (n) , we can optimize the average schedule cost (or beneﬁt) simply P 1 by applying our ofﬂine algorithm to the job f = n n f (i) (since any convex combination of jobs is a job). [sent-140, score-1.166]
</p><p>48 i=1 2  4  4  Online Greedy Algorithm  In the online setting we are fed, one at a time, a sequence f (1) , f (2) , . [sent-141, score-0.256]
</p><p>49 Prior to receiving job f (i) , we must specify a schedule S (i) . [sent-145, score-0.734]
</p><p>50 T Here for any schedule S and job f , we deﬁne cT (S, f ) = t=0 1 − f S t dt to be the value of c (S, f ) when the integral is truncated at time T . [sent-149, score-0.775]
</p><p>51 Some form of truncation is necessary because c S (i) , f (i) could be inﬁnite, and without bounding it we could not prove any ﬁnite bound on regret (our regret bounds will be stated as a function of T ). [sent-150, score-0.257]
</p><p>52 Here we require e that for each i, E S (i) = T , where the expectation is over the online algorithm’s random bits. [sent-152, score-0.218]
</p><p>53 That is, we allow the online algorithm to treat T as a budget in expectation, rather than a hard budget. [sent-153, score-0.25]
</p><p>54 For simplicity, we consider the oblivious adversary model, in which the sequence of jobs is ﬁxed in advance and does not change in response to the decisions made by our online algorithm. [sent-155, score-0.456]
</p><p>55 We conﬁne our attention to schedules that consist of actions that come from some ﬁnite set A, and assume that the actions in A have integer durations (i. [sent-156, score-0.337]
</p><p>56 1  Unit-cost actions  In the special case in which each action takes unit time (i. [sent-160, score-0.28]
</p><p>57 , A ⊆ V × {1}), our online algorithm OGunit is very simple. [sent-162, score-0.25]
</p><p>58 , ET , where T is the number of time steps for which our schedule is deﬁned. [sent-166, score-0.612]
</p><p>59 The intent is that each action-selection algorithm is a no-regret algorithm such as randomized weighted majority (WMR) [4], which selects actions so as to maximize payoffs associated with the actions. [sent-167, score-0.243]
</p><p>60 Just before job f (i) arrives, each action-selection algorithm Et selects an action ai . [sent-168, score-0.354]
</p><p>61 The schedule used by OGunit on job f (i) is t (i) (i) (i) i i i S = a1 , a2 , . [sent-169, score-0.734]
</p><p>62 The payoff that Et associates with action a is fa S t−1 . [sent-173, score-0.295]
</p><p>63 Algorithm OGunit has E [Rbeneﬁt ] O T  T n  =  O  T n  ln |A|  and E [Rcost ]  =  ln |A| in the worst case, when WMR [4] is the subroutine action-selection algorithm. [sent-175, score-0.245]
</p><p>64 We will view OGunit as producing an approximate version of the ofﬂine greedy schedule for n 1 the job f = n i=1 f (i) . [sent-177, score-0.834]
</p><p>65 First, view the sequence of actions selected by Et as a single meta-action at , and extend the domain of each f (i) to include the meta-actions by deﬁning f (i) (S ⊕ at ) = ˜ ˜ f (i) (S ⊕ ai ) for all S ∈ S (note each f (i) remains monotone and submodular). [sent-178, score-0.338]
</p><p>66 Thus, the online t ˜ algorithm produces a single schedule S = a1 , a2 , . [sent-179, score-0.821]
</p><p>67 By construction, rt = maxa∈A fa S t−1 − fa S t−1 . [sent-184, score-0.288]
</p><p>68 ˜ t  Thus OGunit behaves exactly like the greedy schedule G for the function f , with t = rt . [sent-185, score-0.727]
</p><p>69 WMR has worst-case expected regret 1 O n Gmax ln |A| , where Gmax is the maximum sum of payoffs payoff for any single action. [sent-189, score-0.289]
</p><p>70 3 Because each payoff is at most 1 and there are n rounds, Gmax ≤ n, so a trivial bound is E [R] = O T  1 n  ln |A| . [sent-190, score-0.194]
</p><p>71 2  From unit-cost actions to arbitrary actions  In this section we generalize the online greedy algorithm presented in the previous section to accommodate actions with arbitrary durations. [sent-194, score-0.704]
</p><p>72 On each round i, OG constructs a schedule S (i) as follows: for t = 1, 2, . [sent-199, score-0.571]
</p><p>73 , L, it uses Et to choose an action (i) 1 ai = (v, τ ) ∈ A, and appends this action to S (i) with probability τ . [sent-202, score-0.28]
</p><p>74 Let St denote the schedule t (i) that results from the ﬁrst t steps of this process (so St contains between 0 and t actions). [sent-203, score-0.571]
</p><p>75 The (i) 1 payoff that Et associates with an action a = (v, τ ) equals τ fa (St−1 ) (i. [sent-204, score-0.295]
</p><p>76 Thus, the online ˜ t t ˜ algorithm produces a single schedule S = a1 , a2 , . [sent-209, score-0.821]
</p><p>77 For the purposes of analysis, we will imagine that each meta-action at always takes unit time ˜ (whereas in fact, at takes unit time per job in expectation). [sent-214, score-0.245]
</p><p>78 Thus S can be viewed as a version of the ˜ ˜ ˜ 1 ˜ ˜ − fat (St−1 ) , where we greedy schedule from §3, with t = max(v,τ )∈A τ f(v,τ ) (St−1 ) ˜ are using the assumption that at takes unit time. [sent-220, score-0.671]
</p><p>79 Because each f (i) is monotone and submodular, f is monotone and submodular as well, so the greedy schedule’s approximation guarantees apply to f . [sent-226, score-0.588]
</p><p>80 One additional complication is that S (i) is now a random variable, whereas in the deﬁnition of Rcost the cost of a schedule is always calculated up to time T . [sent-239, score-0.612]
</p><p>81 Algorithm OG, run with input L = T ln n, has E [Rcost ] = O(T ln n · E [R] + 3  In particular, E [Rcost ] = O (ln n) 2 T selection algorithm. [sent-244, score-0.202]
</p><p>82 3  Dealing with limited feedback  Thus far we have assumed that, after specifying a schedule S (i) , the online algorithm receives complete access to the job f (i) . [sent-247, score-1.074]
</p><p>83 The priced and partially transparent feedback models arise naturally in the case where action (v, τ ) represents running a deterministic algorithm v for τ time units, and f (S) = 1 if some action in S yields a solution to some particular problem instance, and f (S) = 0 otherwise. [sent-252, score-0.54]
</p><p>84 If we execute a schedule S and halt as soon as some action yields a solution, we obtain exactly the information that is revealed in the partially transparent model. [sent-253, score-0.742]
</p><p>85 Alternatively, running each algorithm v until it returns a solution would completely reveal the function f (i) , but incurs a computational cost, as reﬂected in the priced feedback model. [sent-254, score-0.207]
</p><p>86 This technique has been used in a number of online algorithms (e. [sent-257, score-0.218]
</p><p>87 Our proofs have the same high-level structure as that of the lower bound given in [4], in that we deﬁne a distribution over jobs that allows any online algorithm’s expected performance to be easily bounded, and then prove a bound on the expected performance of the best schedule in hindsight. [sent-262, score-1.059]
</p><p>88 Then any online algorithm has worst-case expected regret Ω (X) n T (resp. [sent-266, score-0.346]
</p><p>89 The competition consists of running each submitted solver on a number of benchmark instances, with a per-instance time limit. [sent-272, score-0.226]
</p><p>90 We evaluated the online algorithm OG by using it to combine solvers from the 2007 SAT solver competition. [sent-274, score-0.379]
</p><p>91 To do so, we used data available on the competition web site to construct a matrix X, where Xi,j is the time that the j th solver required on the ith benchmark instance. [sent-275, score-0.199]
</p><p>92 We used this data to determine whether or not a given schedule would solve an instance within the time limit T (schedule S solves instance i if and only if, for some j, S T contains an action (hj , τ ) with τ ≥ Xi,j ). [sent-276, score-0.789]
</p><p>93 As illustrated in Example 1, the task of maximizing the number of instances solved within the time limit, in an online setting in which a sequence of instances must be solved one at a time, is an instance of our online problem (under the beneﬁt-maximization objective). [sent-277, score-0.68]
</p><p>94 Within each instance category, we compared OG to the ofﬂine greedy schedule, to the individual solver that solved the most instances within the time limit, and to a schedule that ran each solver in parallel at equal strength. [sent-278, score-0.933]
</p><p>95 For these experiments, we ran OG in the full-information feedback model, after ﬁnding that the number of benchmark instances was too small for OG to be effective in the limited feedback models. [sent-279, score-0.25]
</p><p>96 In each category, the ofﬂine greedy schedule and the online greedy algorithm outperform all solvers entered in the competition as well as the na¨ve parallel schedule. [sent-281, score-1.161]
</p><p>97 Category Industrial Random Hand-crafted  Ofﬂine greedy 147 350 114  Online greedy 149 347 107  Parallel schedule 132 302 95  Top solver 139 257 98  References [1] Noga Alon, Baruch Awerbuch, and Yossi Azar. [sent-283, score-0.847]
</p><p>98 A note on the budgeted maximization of submodular functions. [sent-330, score-0.234]
</p><p>99 An analysis of approximations for maximizing submodular set functions. [sent-345, score-0.255]
</p><p>100 A note on maximizing a submodular set function subject to a knapsack constraint. [sent-358, score-0.332]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('schedule', 0.571), ('online', 0.218), ('rbene', 0.211), ('jobs', 0.2), ('submodular', 0.2), ('og', 0.192), ('rcost', 0.173), ('job', 0.163), ('ogunit', 0.154), ('monotone', 0.144), ('action', 0.121), ('ine', 0.119), ('actions', 0.118), ('fa', 0.116), ('predicates', 0.115), ('overage', 0.101), ('schedules', 0.101), ('ln', 0.101), ('greedy', 0.1), ('wmr', 0.096), ('regret', 0.096), ('gmax', 0.092), ('feedback', 0.09), ('gj', 0.087), ('ipelined', 0.077), ('knapsack', 0.077), ('solver', 0.076), ('munagala', 0.067), ('et', 0.062), ('portfolio', 0.062), ('babu', 0.058), ('priced', 0.058), ('streeter', 0.058), ('payoff', 0.058), ('rt', 0.056), ('pi', 0.055), ('maximizing', 0.055), ('sat', 0.054), ('solvers', 0.053), ('competition', 0.053), ('transparent', 0.05), ('ax', 0.048), ('generalizes', 0.047), ('fresh', 0.046), ('um', 0.046), ('bene', 0.045), ('subroutine', 0.043), ('instances', 0.041), ('time', 0.041), ('st', 0.04), ('asymptotically', 0.039), ('aximum', 0.038), ('carlos', 0.038), ('golovin', 0.038), ('haim', 0.038), ('kamesh', 0.038), ('kaplan', 0.038), ('rajeev', 0.038), ('shivnath', 0.038), ('udgeted', 0.038), ('uriel', 0.038), ('sensor', 0.038), ('ai', 0.038), ('sequence', 0.038), ('ct', 0.037), ('sensors', 0.037), ('query', 0.037), ('objective', 0.037), ('theorem', 0.036), ('daniel', 0.036), ('bound', 0.035), ('gave', 0.034), ('pipelined', 0.034), ('matthew', 0.034), ('entered', 0.034), ('appending', 0.034), ('budgeted', 0.034), ('investing', 0.034), ('krause', 0.034), ('mins', 0.034), ('motwani', 0.034), ('payoffs', 0.034), ('restarting', 0.034), ('stoc', 0.034), ('submodularity', 0.034), ('algorithm', 0.032), ('nish', 0.031), ('seed', 0.031), ('yoav', 0.031), ('runs', 0.03), ('bounds', 0.03), ('activity', 0.029), ('benchmark', 0.029), ('nicol', 0.029), ('subtasks', 0.029), ('instance', 0.028), ('running', 0.027), ('sj', 0.027), ('maximize', 0.027), ('industrial', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="22-tfidf-1" href="./nips-2008-An_Online_Algorithm_for_Maximizing_Submodular_Functions.html">22 nips-2008-An Online Algorithm for Maximizing Submodular Functions</a></p>
<p>Author: Matthew Streeter, Daniel Golovin</p><p>Abstract: We present an algorithm for solving a broad class of online resource allocation problems. Our online algorithm can be applied in environments where abstract jobs arrive one at a time, and one can complete the jobs by investing time in a number of abstract activities, according to some schedule. We assume that the fraction of jobs completed by a schedule is a monotone, submodular function of a set of pairs (v, τ ), where τ is the time invested in activity v. Under this assumption, our online algorithm performs near-optimally according to two natural metrics: (i) the fraction of jobs completed within time T , for some ﬁxed deadline T > 0, and (ii) the average time required to complete each job. We evaluate our algorithm experimentally by using it to learn, online, a schedule for allocating CPU time among solvers entered in the 2007 SAT solver competition. 1</p><p>2 0.11334088 <a title="22-tfidf-2" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>Author: Sham M. Kakade, Ambuj Tewari</p><p>Abstract: This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. As a corollary, we characterize the convergence rate of P EGASOS (with high probability), a recently proposed method for solving the SVM optimization problem. 1</p><p>3 0.10739115 <a title="22-tfidf-3" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>Author: Ofer Dekel</p><p>Abstract: We present cutoff averaging, a technique for converting any conservative online learning algorithm into a batch learning algorithm. Most online-to-batch conversion techniques work well with certain types of online learning algorithms and not with others, whereas cutoff averaging explicitly tries to adapt to the characteristics of the online algorithm being converted. An attractive property of our technique is that it preserves the efﬁciency of the original online algorithm, making it appropriate for large-scale learning problems. We provide a statistical analysis of our technique and back our theoretical claims with experimental results. 1</p><p>4 0.099834003 <a title="22-tfidf-4" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>Author: Giovanni Cavallanti, Nicolò Cesa-bianchi, Claudio Gentile</p><p>Abstract: We provide a new analysis of an efﬁcient margin-based algorithm for selective sampling in classiﬁcation problems. Using the so-called Tsybakov low noise condition to parametrize the instance distribution, we show bounds on the convergence rate to the Bayes risk of both the fully supervised and the selective sampling versions of the basic algorithm. Our analysis reveals that, excluding logarithmic factors, the average risk of the selective sampler converges to the Bayes risk at rate N −(1+α)(2+α)/2(3+α) where N denotes the number of √ queried labels, and α > 0 is the exponent in the low noise condition. For all α > 3 − 1 ≈ 0.73 this convergence rate is asymptotically faster than the rate N −(1+α)/(2+α) achieved by the fully supervised version of the same classiﬁer, which queries all labels, and for α → ∞ the two rates exhibit an exponential gap. Experiments on textual data reveal that simple variants of the proposed selective sampler perform much better than popular and similarly efﬁcient competitors. 1</p><p>5 0.090496756 <a title="22-tfidf-5" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and ﬁnd for datasets with large numbers of features, substantial sparsity is discoverable. 1</p><p>6 0.086412169 <a title="22-tfidf-6" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>7 0.083982036 <a title="22-tfidf-7" href="./nips-2008-Online_Optimization_in_X-Armed_Bandits.html">170 nips-2008-Online Optimization in X-Armed Bandits</a></p>
<p>8 0.07804177 <a title="22-tfidf-8" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>9 0.076976165 <a title="22-tfidf-9" href="./nips-2008-Online_Metric_Learning_and_Fast_Similarity_Search.html">168 nips-2008-Online Metric Learning and Fast Similarity Search</a></p>
<p>10 0.068606973 <a title="22-tfidf-10" href="./nips-2008-Mortal_Multi-Armed_Bandits.html">140 nips-2008-Mortal Multi-Armed Bandits</a></p>
<p>11 0.061982226 <a title="22-tfidf-11" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>12 0.060844917 <a title="22-tfidf-12" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>13 0.060528301 <a title="22-tfidf-13" href="./nips-2008-Posterior_Consistency_of_the_Silverman_g-prior_in_Bayesian_Model_Choice.html">182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</a></p>
<p>14 0.058169395 <a title="22-tfidf-14" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>15 0.055487398 <a title="22-tfidf-15" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>16 0.052693781 <a title="22-tfidf-16" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>17 0.052452996 <a title="22-tfidf-17" href="./nips-2008-Algorithms_for_Infinitely_Many-Armed_Bandits.html">17 nips-2008-Algorithms for Infinitely Many-Armed Bandits</a></p>
<p>18 0.051482767 <a title="22-tfidf-18" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<p>19 0.050717756 <a title="22-tfidf-19" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>20 0.046889476 <a title="22-tfidf-20" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.146), (1, 0.099), (2, -0.108), (3, -0.038), (4, -0.108), (5, -0.006), (6, -0.027), (7, -0.003), (8, 0.01), (9, -0.019), (10, -0.03), (11, 0.062), (12, 0.007), (13, 0.056), (14, 0.113), (15, -0.097), (16, -0.041), (17, 0.014), (18, -0.001), (19, 0.004), (20, -0.02), (21, 0.007), (22, 0.052), (23, 0.012), (24, -0.08), (25, -0.073), (26, 0.069), (27, -0.002), (28, 0.056), (29, 0.041), (30, -0.037), (31, 0.038), (32, 0.058), (33, -0.048), (34, -0.06), (35, 0.008), (36, 0.043), (37, 0.076), (38, 0.103), (39, 0.038), (40, -0.017), (41, 0.016), (42, -0.023), (43, 0.136), (44, -0.081), (45, -0.06), (46, -0.145), (47, 0.043), (48, 0.035), (49, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95074034 <a title="22-lsi-1" href="./nips-2008-An_Online_Algorithm_for_Maximizing_Submodular_Functions.html">22 nips-2008-An Online Algorithm for Maximizing Submodular Functions</a></p>
<p>Author: Matthew Streeter, Daniel Golovin</p><p>Abstract: We present an algorithm for solving a broad class of online resource allocation problems. Our online algorithm can be applied in environments where abstract jobs arrive one at a time, and one can complete the jobs by investing time in a number of abstract activities, according to some schedule. We assume that the fraction of jobs completed by a schedule is a monotone, submodular function of a set of pairs (v, τ ), where τ is the time invested in activity v. Under this assumption, our online algorithm performs near-optimally according to two natural metrics: (i) the fraction of jobs completed within time T , for some ﬁxed deadline T > 0, and (ii) the average time required to complete each job. We evaluate our algorithm experimentally by using it to learn, online, a schedule for allocating CPU time among solvers entered in the 2007 SAT solver competition. 1</p><p>2 0.64608055 <a title="22-lsi-2" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>Author: Ofer Dekel</p><p>Abstract: We present cutoff averaging, a technique for converting any conservative online learning algorithm into a batch learning algorithm. Most online-to-batch conversion techniques work well with certain types of online learning algorithms and not with others, whereas cutoff averaging explicitly tries to adapt to the characteristics of the online algorithm being converted. An attractive property of our technique is that it preserves the efﬁciency of the original online algorithm, making it appropriate for large-scale learning problems. We provide a statistical analysis of our technique and back our theoretical claims with experimental results. 1</p><p>3 0.57740432 <a title="22-lsi-3" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>Author: Sham M. Kakade, Ambuj Tewari</p><p>Abstract: This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. As a corollary, we characterize the convergence rate of P EGASOS (with high probability), a recently proposed method for solving the SVM optimization problem. 1</p><p>4 0.57641155 <a title="22-lsi-4" href="./nips-2008-Online_Metric_Learning_and_Fast_Similarity_Search.html">168 nips-2008-Online Metric Learning and Fast Similarity Search</a></p>
<p>Author: Prateek Jain, Brian Kulis, Inderjit S. Dhillon, Kristen Grauman</p><p>Abstract: Metric learning algorithms can provide useful distance functions for a variety of domains, and recent work has shown good accuracy for problems where the learner can access all distance constraints at once. However, in many real applications, constraints are only available incrementally, thus necessitating methods that can perform online updates to the learned metric. Existing online algorithms offer bounds on worst-case performance, but typically do not perform well in practice as compared to their ofﬂine counterparts. We present a new online metric learning algorithm that updates a learned Mahalanobis metric based on LogDet regularization and gradient descent. We prove theoretical worst-case performance bounds, and empirically compare the proposed method against existing online metric learning algorithms. To further boost the practicality of our approach, we develop an online locality-sensitive hashing scheme which leads to efﬁcient updates to data structures used for fast approximate similarity search. We demonstrate our algorithm on multiple datasets and show that it outperforms relevant baselines.</p><p>5 0.54084575 <a title="22-lsi-5" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and ﬁnd for datasets with large numbers of features, substantial sparsity is discoverable. 1</p><p>6 0.53056335 <a title="22-lsi-6" href="./nips-2008-Online_Models_for_Content_Optimization.html">169 nips-2008-Online Models for Content Optimization</a></p>
<p>7 0.50933623 <a title="22-lsi-7" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>8 0.49892452 <a title="22-lsi-8" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>9 0.47884002 <a title="22-lsi-9" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>10 0.45758858 <a title="22-lsi-10" href="./nips-2008-Online_Optimization_in_X-Armed_Bandits.html">170 nips-2008-Online Optimization in X-Armed Bandits</a></p>
<p>11 0.45306107 <a title="22-lsi-11" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>12 0.45273787 <a title="22-lsi-12" href="./nips-2008-Posterior_Consistency_of_the_Silverman_g-prior_in_Bayesian_Model_Choice.html">182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</a></p>
<p>13 0.34966493 <a title="22-lsi-13" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>14 0.34338996 <a title="22-lsi-14" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>15 0.340969 <a title="22-lsi-15" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>16 0.33126348 <a title="22-lsi-16" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>17 0.33121973 <a title="22-lsi-17" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>18 0.32938278 <a title="22-lsi-18" href="./nips-2008-Bounding_Performance_Loss_in_Approximate_MDP_Homomorphisms.html">39 nips-2008-Bounding Performance Loss in Approximate MDP Homomorphisms</a></p>
<p>19 0.3285982 <a title="22-lsi-19" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>20 0.32585442 <a title="22-lsi-20" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.026), (6, 0.105), (7, 0.041), (12, 0.043), (28, 0.15), (36, 0.342), (57, 0.052), (59, 0.011), (63, 0.037), (71, 0.025), (77, 0.037), (83, 0.028), (92, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74152792 <a title="22-lda-1" href="./nips-2008-An_Online_Algorithm_for_Maximizing_Submodular_Functions.html">22 nips-2008-An Online Algorithm for Maximizing Submodular Functions</a></p>
<p>Author: Matthew Streeter, Daniel Golovin</p><p>Abstract: We present an algorithm for solving a broad class of online resource allocation problems. Our online algorithm can be applied in environments where abstract jobs arrive one at a time, and one can complete the jobs by investing time in a number of abstract activities, according to some schedule. We assume that the fraction of jobs completed by a schedule is a monotone, submodular function of a set of pairs (v, τ ), where τ is the time invested in activity v. Under this assumption, our online algorithm performs near-optimally according to two natural metrics: (i) the fraction of jobs completed within time T , for some ﬁxed deadline T > 0, and (ii) the average time required to complete each job. We evaluate our algorithm experimentally by using it to learn, online, a schedule for allocating CPU time among solvers entered in the 2007 SAT solver competition. 1</p><p>2 0.5089376 <a title="22-lda-2" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>Author: Shai Shalev-shwartz, Sham M. Kakade</p><p>Abstract: We describe a primal-dual framework for the design and analysis of online strongly convex optimization algorithms. Our framework yields the tightest known logarithmic regret bounds for Follow-The-Leader and for the gradient descent algorithm proposed in Hazan et al. [2006]. We then show that one can interpolate between these two extreme cases. In particular, we derive a new algorithm that shares the computational simplicity of gradient descent but achieves lower regret in many practical situations. Finally, we further extend our framework for generalized strongly convex functions. 1</p><p>3 0.50420362 <a title="22-lda-3" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>Author: Sham M. Kakade, Ambuj Tewari</p><p>Abstract: This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. As a corollary, we characterize the convergence rate of P EGASOS (with high probability), a recently proposed method for solving the SVM optimization problem. 1</p><p>4 0.50228965 <a title="22-lda-4" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>5 0.50134593 <a title="22-lda-5" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>6 0.50091207 <a title="22-lda-6" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>7 0.50070131 <a title="22-lda-7" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>8 0.50015098 <a title="22-lda-8" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>9 0.49998283 <a title="22-lda-9" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>10 0.4979457 <a title="22-lda-10" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>11 0.49509156 <a title="22-lda-11" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>12 0.49167761 <a title="22-lda-12" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>13 0.49118799 <a title="22-lda-13" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>14 0.49113333 <a title="22-lda-14" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>15 0.49093065 <a title="22-lda-15" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>16 0.49084818 <a title="22-lda-16" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>17 0.48963696 <a title="22-lda-17" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>18 0.48943684 <a title="22-lda-18" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>19 0.48922339 <a title="22-lda-19" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>20 0.48913637 <a title="22-lda-20" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
