<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-183" href="#">nips2008-183</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</h1>
<br/><p>Source: <a title="nips-2008-183-pdf" href="http://papers.nips.cc/paper/3498-predicting-the-geometry-of-metal-binding-sites-from-protein-sequence.pdf">pdf</a></p><p>Author: Paolo Frasconi, Andrea Passerini</p><p>Abstract: Metal binding is important for the structural and functional characterization of proteins. Previous prediction efforts have only focused on bonding state, i.e. deciding which protein residues act as metal ligands in some binding site. Identifying the geometry of metal-binding sites, i.e. deciding which residues are jointly involved in the coordination of a metal ion is a new prediction problem that has been never attempted before from protein sequence alone. In this paper, we formulate it in the framework of learning with structured outputs. Our solution relies on the fact that, from a graph theoretical perspective, metal binding has the algebraic properties of a matroid, enabling the application of greedy algorithms for learning structured outputs. On a data set of 199 non-redundant metalloproteins, we obtained precision/recall levels of 75%/46% correct ligand-ion assignments, which improves to 88%/88% in the setting where the metal binding state is known. 1</p><p>Reference: <a title="nips-2008-183-reference" href="../nips2008_reference/nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 it  Abstract Metal binding is important for the structural and functional characterization of proteins. [sent-6, score-0.249]
</p><p>2 Previous prediction efforts have only focused on bonding state, i. [sent-7, score-0.238]
</p><p>3 deciding which protein residues act as metal ligands in some binding site. [sent-9, score-1.288]
</p><p>4 deciding which residues are jointly involved in the coordination of a metal ion is a new prediction problem that has been never attempted before from protein sequence alone. [sent-12, score-1.179]
</p><p>5 In this paper, we formulate it in the framework of learning with structured outputs. [sent-13, score-0.126]
</p><p>6 Our solution relies on the fact that, from a graph theoretical perspective, metal binding has the algebraic properties of a matroid, enabling the application of greedy algorithms for learning structured outputs. [sent-14, score-0.957]
</p><p>7 On a data set of 199 non-redundant metalloproteins, we obtained precision/recall levels of 75%/46% correct ligand-ion assignments, which improves to 88%/88% in the setting where the metal binding state is known. [sent-15, score-0.757]
</p><p>8 1  Introduction  Metal ions play important roles in protein function and structure and metalloproteins are involved in a number of diseases for which medicine is still seeking effective treatment, including cancer, Parkinson, dementia, and AIDS [10]. [sent-16, score-0.421]
</p><p>9 A metal binding site typically consists of an ion bound to one or more protein residues (called ligands). [sent-17, score-1.307]
</p><p>10 In some cases, the ion is embedded in a prosthetic group (e. [sent-18, score-0.216]
</p><p>11 Among the 20 amino acids, the four most common ligands are cysteine (C), histidine (H), aspartic acid (D), and glutamic acid (E). [sent-21, score-0.455]
</p><p>12 Highly conserved residues are more likely to be involved in the coordination of a metal ion, although in the case of cysteines, conservation is also often associated with the presence of a disulﬁde bridge (a covalent bond between the sulfur atoms of two cysteines) [8]. [sent-22, score-0.739]
</p><p>13 Predicting metal binding from sequence alone can be very useful in genomic annotation for characterizing the function and the structure of non determined proteins, but also during the experimental determination of new metalloproteins. [sent-23, score-0.761]
</p><p>14 Current high-throughput experimental technologies only annotate whole proteins as metal binding [13], but cannot determine the involved ligands. [sent-24, score-0.859]
</p><p>15 Most of the research for understanding metal binding has focused on ﬁnding sequence patterns that characterize binding sites [8]. [sent-25, score-1.129]
</p><p>16 The easiest task to formulate in this context is bonding state prediction, which is a binary classiﬁcation problem: either a residue is involved in the coordination of a metal ion or is free (in the case of cysteines, a third class can also be introduced for disulﬁde bridges). [sent-27, score-1.029]
</p><p>17 This prediction task has been addressed in a number of recent works in the case of cysteines only [6], in the case of transition metals (for C and H residues) [12] and for in the special but important case of zinc proteins (for C,H,D, and E residues) [11, 14]. [sent-28, score-0.347]
</p><p>18 Hovever, classiﬁcation of individual residues does not provide sufﬁcient information about a binding site. [sent-29, score-0.457]
</p><p>19 Many proteins bind to several ions in their holo form and a complete characterization requires us to identify the site geometry, i. [sent-30, score-0.361]
</p><p>20 This problem has been only studied assuming knowledge of the protein 3D structure (e. [sent-33, score-0.187]
</p><p>21 [5, 1]), limiting its applicability to structurally determined proteins or their  close homologs, but not from sequence alone. [sent-35, score-0.193]
</p><p>22 Abstracting away the biology, this is a structured output prediction problem where the input consists of a string of protein residues and the output is a labeling of each residue with the corresponding ion identiﬁer (speciﬁc details are given in the next section). [sent-36, score-0.987]
</p><p>23 The supervised learning problem with structured outputs has recently received a considerable amount of attention (see [2] for an overview). [sent-37, score-0.159]
</p><p>24 Different structured output learners deal with this issue by exploiting speciﬁc domain properties for the application at hand. [sent-40, score-0.186]
</p><p>25 Another solution is to construct the structured output in a suitable Hilbert space of features and seek the corresponding pre-image for obtaining the desired discrete structure [17]. [sent-47, score-0.222]
</p><p>26 We borrow ideas from [15] and [4] but speciﬁcally take advantage of the fact that, from a graph theoretical perspective, the metal binding problem has the algebraic structure of a matroid, enabling the application of greedy algorithms. [sent-50, score-0.867]
</p><p>27 2  A formalization of the metal binding sites prediction problem  A protein sequence s is a string in the alphabet of the 20 amino acids. [sent-51, score-1.235]
</p><p>28 Since only some of the 20 amino acids that exist in nature can act as ligands, we begin by extracting from s the subsequence x obtained by deleting characters corresponding to amino acids that never (or very rarely) act as ligands. [sent-52, score-0.422]
</p><p>29 By using T = {C, H, D, E} as the set of candidate ligands, we cover 92% ligands of structurally known proteins. [sent-53, score-0.292]
</p><p>30 We also introduce the set I of symbols associated with metal ion identiﬁers. [sent-57, score-0.665]
</p><p>31 The goal is to predict the coordination relation between amino acids in x and metal ions identiﬁers in I. [sent-59, score-0.85]
</p><p>32 Ideally, it would be also interesting to predict the chemical element of the bound metal ion. [sent-61, score-0.479]
</p><p>33 Hence, ion identiﬁers will have no chemical element attribute attached. [sent-63, score-0.246]
</p><p>34 In practice, we ﬁx a maximum number m of possible ions (m = 4 in the subsequent experiments, covering 93% of structurally known proteins) and let I = {nil , ι1 , . [sent-64, score-0.191]
</p><p>35 The number of admissible binding geometries for a given protein chain having n candidate ligands n! [sent-68, score-0.661]
</p><p>36 being m the number of ions and ki the number of ligands for ion ιi . [sent-73, score-0.581]
</p><p>37 In practice, each ion is coordinated by a variable number of ligands (typically ranging from 1 to 4, but occasionally more), and each protein chain binds a variable number of ions (typically ranging from 1 to 4). [sent-74, score-0.76]
</p><p>38 The number of candidate ligands n grows linearly with the protein chain. [sent-75, score-0.412]
</p><p>39 For example, in the case of PDB chain 1H0Hb (see Figure 1), there are n = 52 candidate ligands and m = 3 ions coordinated by 4 residues each, yielding a set of 7 · 1015 admissible conformations. [sent-76, score-0.657]
</p><p>40 In this view, the string x should be regarded as a set of vertices labeled with the corresponding amino acid in T . [sent-78, score-0.218]
</p><p>41 Let x and I be two sets of vertices (associated with candidate ligands and metal ion identiﬁers, respectively). [sent-82, score-0.952]
</p><p>42 We say that a bipartite edge set y ⊂ x × I satisﬁes the metal binding geometry (MBG) property if the degree of each vertex in x in the graph (x ∪ I, y) is at most 1. [sent-83, score-0.798]
</p><p>43 nil …  ι1  ι2  ι3  D C C C C H E H D H H E E D D D C H C C D E D H D D C D E D E C D E C D C D C C D E E E D C D D C H H E 1 1 2 3 4 5 0 0 0 0 0  Figure 1: Metal binding structure of PDB entry 1H0Hb. [sent-87, score-0.375]
</p><p>44 For readability, only a few connections from free residues to the nil symbol are shown. [sent-88, score-0.298]
</p><p>45 Note that the MBG problem is not a matching problem (such as those studied in [15]) since more than one edge can be incident to vertices belonging to I. [sent-89, score-0.101]
</p><p>46 As discussed above, we are not interested in distinguishing metal ions based on the element type. [sent-90, score-0.609]
</p><p>47 Hence, any two label-isomorphic bipartite graphs (obtained by exchanging two non-nil metal ion vertices) should be regarded as equivalent. [sent-91, score-0.706]
</p><p>48 In this view, the binding geometry consists of a very shallow “parse tree” for string x, as exampliﬁed in Figure 1. [sent-96, score-0.364]
</p><p>49 A difﬁculty that is immediately apparent is that the underlying grammar needs to be context sensitive in order to capture the crossing-dependencies between bound amino acids. [sent-97, score-0.102]
</p><p>50 In real data, when representing metal bonding state in this way, crossing edges are very common. [sent-98, score-0.697]
</p><p>51 This view enlightens a difﬁculty that would be encountered by attempting to solve the structured output problem with a generative model as in [16]. [sent-99, score-0.213]
</p><p>52 Weighted matroids can be seen as a kind of discrete counterparts of concave functions: thanks to the above theorem, if M is a weighted matroid, then the following greedy algorithm is guaranteed to ﬁnd the optimal structure, i. [sent-123, score-0.125]
</p><p>53 If F is a consistent objective function then, for each matroid on S, all greedy bases are optimal. [sent-136, score-0.329]
</p><p>54 3 is also necessary for a slighly more general class of algebraic structures that include matroids, called matroid embeddings [7]. [sent-138, score-0.26]
</p><p>55 We now show that the MBG problem is a suitable candidate for a greedy algorithmic solution. [sent-139, score-0.149]
</p><p>56 We can ﬁnally formulate the greedy algorithm for constructing the structured output in the MBG problem. [sent-149, score-0.279]
</p><p>57 Given the input x, we begin by forming the associated MBG matroid Mx and a corresponding objective function Fx : Yx → I + (in the next section we will show how to learn the R objective function from data). [sent-150, score-0.28]
</p><p>58 4  Learning the greedy objective function  A data set for the MBG problem consist of pairs D = {(xi , yi )} where xi is a string in T ∗ and yi a bipartite graph. [sent-158, score-0.393]
</p><p>59 For any input string x and (partial) output structure y ∈ Y, let Fx (y) = wT φx (y), being w a weight vector and φx (y) a feature vector for (x, y). [sent-161, score-0.152]
</p><p>60 , |D|, ∀y ⊂ yi , ∀e ∈ ext(y ) ∩ yi , ∀e ∈ ext(y ) \ yi , ∀y : y ⊂ y ⊂ Sx . [sent-165, score-0.189]
</p><p>61 edges that actually belong to the target output structure yi ) receive a higher weight than “wrong” extensions (i. [sent-169, score-0.189]
</p><p>62 For this purpose, we will use an online active learner that samples constraints chosen by the execution of the greedy construction algorithm. [sent-180, score-0.16]
</p><p>63 For each epoch, the algorithm maintains the current highest scoring partial correct output yi ⊆ yi for each example, initialized with the empty MBG structure, where the score is computed by the current objective function F . [sent-181, score-0.263]
</p><p>64 It also performs a predeﬁned number L of lookaheads by picking a random superset of y which is included in the target yi , evaluating it and updating the best MBG structure if needed, and adding a corresponding consistency constraint (see Eq. [sent-186, score-0.099]
</p><p>65 , L do randomly choose y : y ⊂ y ⊂ yi ∧ e, e ∈ Sx \ y F ORCE -C ONSTRAINT(Fxi (y ∪ {e}) − Fxi (y ∪ {e }) ≥ 1) if F (yi ) < F (y ∪ {e}) then yi ← y ∪ {e}  There are several suitable online learners implementing the interface required by the above procedure. [sent-197, score-0.126]
</p><p>66 Possible candidates include perceptron-like or ALMA-like update rules like those proposed in [4] for structured output learning (in our case the update would depend on the difference between feature vectors of correctly and incorrectly extended structures in the inner loop of G REEDY E POCH). [sent-198, score-0.241]
</p><p>67 We will therefore rewrite the objective function F using a kernel k(z, z ) = φx (y), φx (y ) between two structured instances z = (x, y) and z = (x , y ), so that Fx (y) = F (z) = i αi k(z, zi ). [sent-204, score-0.17]
</p><p>68 Let σi (z) denote the set of edges incident on ion ιi ∈ I \ nil and n(z) the number of non-nil ion identiﬁers that have at least one incident edge. [sent-205, score-0.642]
</p><p>69 kmbs measures the similarity between individual sites (two sites are orthogonal if have a different number of ligands, a choice that is supported by protein functional considerations). [sent-208, score-0.509]
</p><p>70 kglob ensures that two structures are orthogonal unless they have the same number of sites and down weights their similarity when their number of candidate ligands differs. [sent-209, score-0.492]
</p><p>71 5  Experiments  We tested the method on a dataset of non-redundant proteins previously used in [12] for metal bonding state prediction (http://www. [sent-210, score-0.848]
</p><p>72 Proteins that do not bind metal ions (used in [12] as negative examples) are of no interest in the present case and were removed, resulting in a set of 199 metalloproteins binding transition metals. [sent-215, score-0.938]
</p><p>73 The ﬁrst 220 attributes consist of multiple alignment proﬁles in the window of 11 amino acids centered around xi ( ) (the window was formed from the original protein sequence, not the substring xi of candidate ligands). [sent-221, score-0.458]
</p><p>74 Two prediction tasks were considered, from unknown and from known metal bonding state (a similar distinction is also customary for the related task of disulﬁde bonds prediction, see e. [sent-227, score-0.741]
</p><p>75 In the latter case, the input x only contains actual ligands and no nil symbol is needed. [sent-230, score-0.295]
</p><p>76 PS and RS are the metal binding site precision and recall, respectively (ratio of correctly predicted sites to the number of predicted/actual sites). [sent-236, score-0.939]
</p><p>77 Finally, PB and RB are precision and recall for metal bonding state prediction (as in binary classiﬁcation, being “bonded” the positive class). [sent-237, score-0.738]
</p><p>78 Table 2 reports the breakdown of these performance measures for proteins binding different numbers of metal ions (for L = 10). [sent-238, score-1.021]
</p><p>79 Results show that enforcing consistency constraints tends to improve recall, especially for the bonding state prediction, i. [sent-239, score-0.26]
</p><p>80 helps the predictor to assign a residue to a metal ion identiﬁer rather than to nil. [sent-241, score-0.729]
</p><p>81 Correct prediction of whole sites is very challenging and correct prediction of whole chains even more difﬁcult (given the enormous number of alternatives to be compared). [sent-243, score-0.336]
</p><p>82 Correct edge assignment, however, appears satisfactory and reasonably good when the bonding state is given. [sent-246, score-0.218]
</p><p>83 As in [15], our method is based on a large-margin approach for solving a structured output prediction problem. [sent-258, score-0.232]
</p><p>84 Disulﬁde connectivity is a (perfect) matching problem since each cysteine is bound to exactly one other cysteine (assuming known bonding state, yielding a perfect matching) or can be bound to another cysteine or free (unknown bonding state, yielding a non-perfect matching). [sent-260, score-0.654]
</p><p>85 The MBG problem is not a matching problem but has the structure of a matroid and our formulation allows us to control the number of effectively enforced constraints by taking advantage of a greedy algorithm. [sent-263, score-0.393]
</p><p>86 LaSO aims to solve a much broader class of structured output problems where good output structures can be generated by AI-style search algorithms such as beam search or A*. [sent-265, score-0.324]
</p><p>87 The generation of a fresh set of siblings in LaSO when the search is stuck with a frontier of wrong candidates (essentially a backtrack) is costly compared to our greedy selection procedure and (at least in principle) unnecessary when working on matroids. [sent-266, score-0.118]
</p><p>88 Another general way to deal with the exponential growth of the search space is to introduce a generative model so that arg maxy F (x, y) can be computed efﬁciently, e. [sent-267, score-0.097]
</p><p>89 Indeed, an alternative view of the MBG problem is supervised sequence labeling, where the output string consists of symbols in I. [sent-275, score-0.143]
</p><p>90 A (higher-order) hidden Markov model or chain-structured conditional random ﬁeld could be used as the underlying generative model for structured output learning. [sent-276, score-0.213]
</p><p>91 Unfortunately, these approaches are unlikely to be very accurate since models that are structured as linear chains of dependencies cannot easily capture long-ranged interactions such as those occurring in the example. [sent-277, score-0.182]
</p><p>92 In our preliminary experiments, SVMHMM [16] systematically assigned all bonded residues to the same ion, thus never correctly predicted the geometry except in trivial cases. [sent-278, score-0.326]
</p><p>93 7  Conclusions  We have reported about the ﬁrst successful solution to the challenging problem of predicting protein metal binding geometry from sequence alone. [sent-279, score-0.935]
</p><p>94 Learning with structured outputs is a fairly difﬁcult task and in spite of the fact that several methodologies have been proposed, no single general approach can effectively solve every possible application problem. [sent-281, score-0.159]
</p><p>95 The solution proposed in this paper draws on several previous ideas and speciﬁcally leverages the existence of a matroid for the metal binding problem. [sent-282, score-0.89]
</p><p>96 Other problems that formally exhibit a greedy structure might beneﬁt of similar solutions. [sent-283, score-0.129]
</p><p>97 Prediction of transition metal-binding sites from apo protein structures. [sent-291, score-0.306]
</p><p>98 Learning as search optimization: Approximate large margin methods for structured prediction. [sent-312, score-0.178]
</p><p>99 Identifying cysteines and histidines in transition-metal-binding sites using support vector machines and neural networks. [sent-384, score-0.283]
</p><p>100 Metalloproteomics: high-throughput structural and functional annotation of proteins in structural genomics. [sent-397, score-0.135]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('metal', 0.449), ('mbg', 0.304), ('binding', 0.249), ('ion', 0.216), ('residues', 0.208), ('ligands', 0.205), ('bonding', 0.192), ('matroid', 0.192), ('yx', 0.168), ('ions', 0.16), ('sites', 0.155), ('protein', 0.151), ('proteins', 0.135), ('ext', 0.128), ('structured', 0.126), ('fx', 0.119), ('amino', 0.102), ('cysteines', 0.096), ('disul', 0.096), ('greedy', 0.093), ('nil', 0.09), ('reedy', 0.084), ('acids', 0.083), ('cysteine', 0.08), ('ag', 0.076), ('pe', 0.076), ('ps', 0.073), ('residue', 0.064), ('lasvm', 0.064), ('onstraint', 0.064), ('orce', 0.064), ('passerini', 0.064), ('yi', 0.063), ('rs', 0.062), ('output', 0.06), ('geometry', 0.059), ('candidate', 0.056), ('sx', 0.056), ('fxi', 0.056), ('coordination', 0.056), ('string', 0.056), ('chains', 0.056), ('mx', 0.051), ('helman', 0.048), ('kglob', 0.048), ('kmbs', 0.048), ('laso', 0.048), ('metalloproteins', 0.048), ('onstruct', 0.048), ('poch', 0.048), ('prediction', 0.046), ('epoch', 0.045), ('incident', 0.045), ('maxy', 0.045), ('objective', 0.044), ('constraints', 0.042), ('lexicographically', 0.042), ('zinc', 0.042), ('bipartite', 0.041), ('algebraic', 0.04), ('structure', 0.036), ('site', 0.034), ('acid', 0.034), ('correct', 0.033), ('outputs', 0.033), ('xi', 0.033), ('bind', 0.032), ('bonded', 0.032), ('coordinations', 0.032), ('dianna', 0.032), ('firenze', 0.032), ('histidines', 0.032), ('kres', 0.032), ('matroids', 0.032), ('structurally', 0.031), ('matching', 0.03), ('edges', 0.03), ('chemical', 0.03), ('bs', 0.03), ('structures', 0.028), ('breakdown', 0.028), ('coordinated', 0.028), ('degli', 0.028), ('studi', 0.028), ('bonds', 0.028), ('metals', 0.028), ('pdb', 0.028), ('identi', 0.028), ('sequence', 0.027), ('generative', 0.027), ('margin', 0.027), ('correctly', 0.027), ('state', 0.026), ('vertices', 0.026), ('involved', 0.026), ('act', 0.026), ('search', 0.025), ('precision', 0.025), ('learner', 0.025), ('rb', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="183-tfidf-1" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>Author: Paolo Frasconi, Andrea Passerini</p><p>Abstract: Metal binding is important for the structural and functional characterization of proteins. Previous prediction efforts have only focused on bonding state, i.e. deciding which protein residues act as metal ligands in some binding site. Identifying the geometry of metal-binding sites, i.e. deciding which residues are jointly involved in the coordination of a metal ion is a new prediction problem that has been never attempted before from protein sequence alone. In this paper, we formulate it in the framework of learning with structured outputs. Our solution relies on the fact that, from a graph theoretical perspective, metal binding has the algebraic properties of a matroid, enabling the application of greedy algorithms for learning structured outputs. On a data set of 199 non-redundant metalloproteins, we obtained precision/recall levels of 75%/46% correct ligand-ion assignments, which improves to 88%/88% in the setting where the metal binding state is known. 1</p><p>2 0.10780049 <a title="183-tfidf-2" href="./nips-2008-Integrating_Locally_Learned_Causal_Structures_with_Overlapping_Variables.html">108 nips-2008-Integrating Locally Learned Causal Structures with Overlapping Variables</a></p>
<p>Author: David Danks, Clark Glymour, Robert E. Tillman</p><p>Abstract: In many domains, data are distributed among datasets that share only some variables; other recorded variables may occur in only one dataset. While there are asymptotically correct, informative algorithms for discovering causal relationships from a single dataset, even with missing values and hidden variables, there have been no such reliable procedures for distributed data with overlapping variables. We present a novel, asymptotically correct procedure that discovers a minimal equivalence class of causal DAG structures using local independence information from distributed data of this form and evaluate its performance using synthetic and real-world data against causal discovery algorithms for single datasets and applying Structural EM, a heuristic DAG structure learning procedure for data with missing values, to the concatenated data.</p><p>3 0.10004342 <a title="183-tfidf-3" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>Author: Olivier Chapelle, Chuong B. Do, Choon H. Teo, Quoc V. Le, Alex J. Smola</p><p>Abstract: Large-margin structured estimation methods minimize a convex upper bound of loss functions. While they allow for efﬁcient optimization algorithms, these convex formulations are not tight and sacriﬁce the ability to accurately model the true loss. We present tighter non-convex bounds based on generalizing the notion of a ramp loss from binary classiﬁcation to structured estimation. We show that a small modiﬁcation of existing optimization algorithms sufﬁces to solve this modiﬁed problem. On structured prediction tasks such as protein sequence alignment and web page ranking, our algorithm leads to improved accuracy. 1</p><p>4 0.099017948 <a title="183-tfidf-4" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>Author: Yoshihiro Yamanishi</p><p>Abstract: We formulate the problem of bipartite graph inference as a supervised learning problem, and propose a new method to solve it from the viewpoint of distance metric learning. The method involves the learning of two mappings of the heterogeneous objects to a uniﬁed Euclidean space representing the network topology of the bipartite graph, where the graph is easy to infer. The algorithm can be formulated as an optimization problem in a reproducing kernel Hilbert space. We report encouraging results on the problem of compound-protein interaction network reconstruction from chemical structure data and genomic sequence data. 1</p><p>5 0.075628988 <a title="183-tfidf-5" href="./nips-2008-Scalable_Algorithms_for_String_Kernels_with_Inexact_Matching.html">203 nips-2008-Scalable Algorithms for String Kernels with Inexact Matching</a></p>
<p>Author: Pavel P. Kuksa, Pai-hsi Huang, Vladimir Pavlovic</p><p>Abstract: We present a new family of linear time algorithms for string comparison with mismatches under the string kernels framework. Based on sufﬁcient statistics, our algorithms improve theoretical complexity bounds of existing approaches while scaling well in sequence alphabet size, the number of allowed mismatches and the size of the dataset. In particular, on large alphabets and under loose mismatch constraints our algorithms are several orders of magnitude faster than the existing algorithms for string comparison under the mismatch similarity measure. We evaluate our algorithms on synthetic data and real applications in music genre classiﬁcation, protein remote homology detection and protein fold prediction. The scalability of the algorithms allows us to consider complex sequence transformations, modeled using longer string features and larger numbers of mismatches, leading to a state-of-the-art performance with signiﬁcantly reduced running times. 1</p><p>6 0.075392343 <a title="183-tfidf-6" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>7 0.053436615 <a title="183-tfidf-7" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>8 0.051695284 <a title="183-tfidf-8" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>9 0.051588934 <a title="183-tfidf-9" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>10 0.051526766 <a title="183-tfidf-10" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>11 0.049235884 <a title="183-tfidf-11" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>12 0.046150148 <a title="183-tfidf-12" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>13 0.043760344 <a title="183-tfidf-13" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>14 0.043204878 <a title="183-tfidf-14" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>15 0.0425236 <a title="183-tfidf-15" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>16 0.042443786 <a title="183-tfidf-16" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>17 0.040947434 <a title="183-tfidf-17" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>18 0.039929725 <a title="183-tfidf-18" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>19 0.03891518 <a title="183-tfidf-19" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>20 0.038832609 <a title="183-tfidf-20" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.144), (1, -0.025), (2, -0.037), (3, 0.023), (4, 0.015), (5, -0.04), (6, 0.036), (7, -0.005), (8, 0.017), (9, -0.049), (10, 0.001), (11, 0.042), (12, -0.012), (13, -0.046), (14, 0.079), (15, 0.024), (16, 0.028), (17, -0.041), (18, -0.04), (19, 0.067), (20, 0.024), (21, 0.033), (22, -0.013), (23, -0.062), (24, 0.061), (25, -0.004), (26, -0.093), (27, 0.01), (28, -0.043), (29, 0.047), (30, -0.036), (31, 0.005), (32, 0.007), (33, -0.048), (34, 0.055), (35, 0.086), (36, -0.046), (37, 0.056), (38, -0.013), (39, 0.073), (40, -0.049), (41, 0.192), (42, 0.071), (43, -0.032), (44, 0.048), (45, 0.115), (46, -0.076), (47, -0.097), (48, 0.019), (49, -0.055)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.89907193 <a title="183-lsi-1" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>Author: Paolo Frasconi, Andrea Passerini</p><p>Abstract: Metal binding is important for the structural and functional characterization of proteins. Previous prediction efforts have only focused on bonding state, i.e. deciding which protein residues act as metal ligands in some binding site. Identifying the geometry of metal-binding sites, i.e. deciding which residues are jointly involved in the coordination of a metal ion is a new prediction problem that has been never attempted before from protein sequence alone. In this paper, we formulate it in the framework of learning with structured outputs. Our solution relies on the fact that, from a graph theoretical perspective, metal binding has the algebraic properties of a matroid, enabling the application of greedy algorithms for learning structured outputs. On a data set of 199 non-redundant metalloproteins, we obtained precision/recall levels of 75%/46% correct ligand-ion assignments, which improves to 88%/88% in the setting where the metal binding state is known. 1</p><p>2 0.61346167 <a title="183-lsi-2" href="./nips-2008-Scalable_Algorithms_for_String_Kernels_with_Inexact_Matching.html">203 nips-2008-Scalable Algorithms for String Kernels with Inexact Matching</a></p>
<p>Author: Pavel P. Kuksa, Pai-hsi Huang, Vladimir Pavlovic</p><p>Abstract: We present a new family of linear time algorithms for string comparison with mismatches under the string kernels framework. Based on sufﬁcient statistics, our algorithms improve theoretical complexity bounds of existing approaches while scaling well in sequence alphabet size, the number of allowed mismatches and the size of the dataset. In particular, on large alphabets and under loose mismatch constraints our algorithms are several orders of magnitude faster than the existing algorithms for string comparison under the mismatch similarity measure. We evaluate our algorithms on synthetic data and real applications in music genre classiﬁcation, protein remote homology detection and protein fold prediction. The scalability of the algorithms allows us to consider complex sequence transformations, modeled using longer string features and larger numbers of mismatches, leading to a state-of-the-art performance with signiﬁcantly reduced running times. 1</p><p>3 0.57942647 <a title="183-lsi-3" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>Author: Yoshihiro Yamanishi</p><p>Abstract: We formulate the problem of bipartite graph inference as a supervised learning problem, and propose a new method to solve it from the viewpoint of distance metric learning. The method involves the learning of two mappings of the heterogeneous objects to a uniﬁed Euclidean space representing the network topology of the bipartite graph, where the graph is easy to infer. The algorithm can be formulated as an optimization problem in a reproducing kernel Hilbert space. We report encouraging results on the problem of compound-protein interaction network reconstruction from chemical structure data and genomic sequence data. 1</p><p>4 0.55777472 <a title="183-lsi-4" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>Author: Olivier Chapelle, Chuong B. Do, Choon H. Teo, Quoc V. Le, Alex J. Smola</p><p>Abstract: Large-margin structured estimation methods minimize a convex upper bound of loss functions. While they allow for efﬁcient optimization algorithms, these convex formulations are not tight and sacriﬁce the ability to accurately model the true loss. We present tighter non-convex bounds based on generalizing the notion of a ramp loss from binary classiﬁcation to structured estimation. We show that a small modiﬁcation of existing optimization algorithms sufﬁces to solve this modiﬁed problem. On structured prediction tasks such as protein sequence alignment and web page ranking, our algorithm leads to improved accuracy. 1</p><p>5 0.54726493 <a title="183-lsi-5" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>Author: Alexandre Bouchard-côté, Dan Klein, Michael I. Jordan</p><p>Abstract: Accurate and efﬁcient inference in evolutionary trees is a central problem in computational biology. While classical treatments have made unrealistic site independence assumptions, ignoring insertions and deletions, realistic approaches require tracking insertions and deletions along the phylogenetic tree—a challenging and unsolved computational problem. We propose a new ancestry resampling procedure for inference in evolutionary trees. We evaluate our method in two problem domains—multiple sequence alignment and reconstruction of ancestral sequences—and show substantial improvement over the current state of the art. 1</p><p>6 0.5045585 <a title="183-lsi-6" href="./nips-2008-Integrating_Locally_Learned_Causal_Structures_with_Overlapping_Variables.html">108 nips-2008-Integrating Locally Learned Causal Structures with Overlapping Variables</a></p>
<p>7 0.456002 <a title="183-lsi-7" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>8 0.42879415 <a title="183-lsi-8" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>9 0.42083308 <a title="183-lsi-9" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>10 0.39956671 <a title="183-lsi-10" href="./nips-2008-Finding_Latent_Causes_in_Causal_Networks%3A_an_Efficient_Approach_Based_on_Markov_Blankets.html">86 nips-2008-Finding Latent Causes in Causal Networks: an Efficient Approach Based on Markov Blankets</a></p>
<p>11 0.39471546 <a title="183-lsi-11" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>12 0.38863206 <a title="183-lsi-12" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>13 0.3876242 <a title="183-lsi-13" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>14 0.37645349 <a title="183-lsi-14" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>15 0.36788955 <a title="183-lsi-15" href="./nips-2008-A_spatially_varying_two-sample_recombinant_coalescent%2C_with_applications_to_HIV_escape_response.html">11 nips-2008-A spatially varying two-sample recombinant coalescent, with applications to HIV escape response</a></p>
<p>16 0.36679664 <a title="183-lsi-16" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>17 0.36543548 <a title="183-lsi-17" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>18 0.34794781 <a title="183-lsi-18" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>19 0.34516209 <a title="183-lsi-19" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>20 0.34291688 <a title="183-lsi-20" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.039), (7, 0.042), (12, 0.023), (15, 0.01), (28, 0.116), (57, 0.029), (59, 0.012), (63, 0.018), (64, 0.01), (71, 0.017), (77, 0.024), (83, 0.57)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.93705362 <a title="183-lda-1" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>Author: Abhinav Gupta, Jianbo Shi, Larry S. Davis</p><p>Abstract: We present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufﬁcient for representing context. Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or “informative” background(context). We present a “shape-aware” model which utilizes contour information for efﬁcient and accurate labeling of features in the image. Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity. 1</p><p>2 0.93525124 <a title="183-lda-2" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>Author: Steffen Bickel, Christoph Sawade, Tobias Scheffer</p><p>Abstract: We address the problem of learning classiﬁers for several related tasks that may differ in their joint distribution of input and output variables. For each task, small – possibly even empty – labeled samples and large unlabeled samples are available. While the unlabeled samples reﬂect the target distribution, the labeled samples may be biased. This setting is motivated by the problem of predicting sociodemographic features for users of web portals, based on the content which they have accessed. Here, questionnaires offered to a portion of each portal’s users produce biased samples. We derive a transfer learning procedure that produces resampling weights which match the pool of all examples to the target distribution of any given task. Transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy. 1</p><p>3 0.9109292 <a title="183-lda-3" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>Author: Yoshihiro Yamanishi</p><p>Abstract: We formulate the problem of bipartite graph inference as a supervised learning problem, and propose a new method to solve it from the viewpoint of distance metric learning. The method involves the learning of two mappings of the heterogeneous objects to a uniﬁed Euclidean space representing the network topology of the bipartite graph, where the graph is easy to infer. The algorithm can be formulated as an optimization problem in a reproducing kernel Hilbert space. We report encouraging results on the problem of compound-protein interaction network reconstruction from chemical structure data and genomic sequence data. 1</p><p>same-paper 4 0.89440835 <a title="183-lda-4" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>Author: Paolo Frasconi, Andrea Passerini</p><p>Abstract: Metal binding is important for the structural and functional characterization of proteins. Previous prediction efforts have only focused on bonding state, i.e. deciding which protein residues act as metal ligands in some binding site. Identifying the geometry of metal-binding sites, i.e. deciding which residues are jointly involved in the coordination of a metal ion is a new prediction problem that has been never attempted before from protein sequence alone. In this paper, we formulate it in the framework of learning with structured outputs. Our solution relies on the fact that, from a graph theoretical perspective, metal binding has the algebraic properties of a matroid, enabling the application of greedy algorithms for learning structured outputs. On a data set of 199 non-redundant metalloproteins, we obtained precision/recall levels of 75%/46% correct ligand-ion assignments, which improves to 88%/88% in the setting where the metal binding state is known. 1</p><p>5 0.87797642 <a title="183-lda-5" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>Author: Takafumi Kanamori, Shohei Hido, Masashi Sugiyama</p><p>Abstract: We address the problem of estimating the ratio of two probability density functions (a.k.a. the importance). The importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally very efﬁcient and numerically stable. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efﬁcient than competing approaches. 1</p><p>6 0.75224286 <a title="183-lda-6" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>7 0.6114974 <a title="183-lda-7" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>8 0.59127063 <a title="183-lda-8" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>9 0.58660364 <a title="183-lda-9" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>10 0.57646918 <a title="183-lda-10" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>11 0.56838542 <a title="183-lda-11" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>12 0.56191528 <a title="183-lda-12" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>13 0.55714822 <a title="183-lda-13" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>14 0.55542076 <a title="183-lda-14" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>15 0.55453688 <a title="183-lda-15" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>16 0.54101568 <a title="183-lda-16" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>17 0.52864146 <a title="183-lda-17" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>18 0.52860701 <a title="183-lda-18" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>19 0.527538 <a title="183-lda-19" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>20 0.5172447 <a title="183-lda-20" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
