<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-182" href="#">nips2008-182</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</h1>
<br/><p>Source: <a title="nips-2008-182-pdf" href="http://papers.nips.cc/paper/3462-posterior-consistency-of-the-silverman-g-prior-in-bayesian-model-choice.pdf">pdf</a></p><p>Author: Zhihua Zhang, Michael I. Jordan, Dit-Yan Yeung</p><p>Abstract: Kernel supervised learning methods can be uniﬁed by utilizing the tools from regularization theory. The duality between regularization and prior leads to interpreting regularization methods in terms of maximum a posteriori estimation and has motivated Bayesian interpretations of kernel methods. In this paper we pursue a Bayesian interpretation of sparsity in the kernel setting by making use of a mixture of a point-mass distribution and prior that we refer to as “Silverman’s g-prior.” We provide a theoretical analysis of the posterior consistency of a Bayesian model choice procedure based on this prior. We also establish the asymptotic relationship between this procedure and the Bayesian information criterion. 1</p><p>Reference: <a title="nips-2008-182-reference" href="../nips2008_reference/nips-2008-Posterior_Consistency_of_the_Silverman_g-prior_in_Bayesian_Model_Choice_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Jordan Departments of EECS and Statistics University of California, Berkeley, CA, USA  Dit-Yan Yeung Department of Computer Science & Engineering HKUST, Hong Kong, China  Abstract Kernel supervised learning methods can be uniﬁed by utilizing the tools from regularization theory. [sent-2, score-0.032]
</p><p>2 The duality between regularization and prior leads to interpreting regularization methods in terms of maximum a posteriori estimation and has motivated Bayesian interpretations of kernel methods. [sent-3, score-0.141]
</p><p>3 In this paper we pursue a Bayesian interpretation of sparsity in the kernel setting by making use of a mixture of a point-mass distribution and prior that we refer to as “Silverman’s g-prior. [sent-4, score-0.077]
</p><p>4 ” We provide a theoretical analysis of the posterior consistency of a Bayesian model choice procedure based on this prior. [sent-5, score-0.119]
</p><p>5 We also establish the asymptotic relationship between this procedure and the Bayesian information criterion. [sent-6, score-0.043]
</p><p>6 1  Introduction  We address a supervised learning problem over a set of training data {xi , yi }n where xi ∈ X ⊂ Rp i=1 is a p-dimensional input vector and yi is a univariate response. [sent-7, score-0.069]
</p><p>7 Suppose f = u + h ∈ ({1} + HK ) where HK is a reproducing kernel Hilbert space (RKHS). [sent-9, score-0.043]
</p><p>8 The estimation of f (x) is then formulated as a regularization problem of the form min  f ∈HK  1 n  n  L(yi , f (xi )) + i=1  g h 2  2 HK  ,  (1)  where L(y, f (x)) is a loss function, h 2 K is the RKHS norm and g > 0 is the regularization H parameter. [sent-10, score-0.064]
</p><p>9 By the representer theorem [7], the solution for (1) is of the form n  f (x) = u +  βj K(x, xj ),  (2)  j=1 n  where K(·, ·) is the kernel function. [sent-11, score-0.106]
</p><p>10 ) the βi as min u,β  1 n  n  g L(yi , f (xi )) + β Kβ , 2 i=1  (3)  where K = [K(xi , xj )] is the n×n kernel matrix and β = (β1 , . [sent-15, score-0.043]
</p><p>11 From the Bayesian standpoint, the role of the regularization term g β Kβ can be captured by assign2 ing a design-dependent prior Nn (0, g −1 K−1 ) to the regression vector β. [sent-19, score-0.16]
</p><p>12 The prior Nn 0, K−1 for β was ﬁrst proposed by [5] in his Bayesian formulation of spline smoothing. [sent-20, score-0.089]
</p><p>13 Here we refer to the prior β ∼ Nn 0, g −1 K−1 as the Silverman g-prior by analogy to the Zellner g-prior [9]. [sent-21, score-0.074]
</p><p>14 When K is singular, by analogy to generalized singular g-prior (gsg-prior) [8], we call Nn 0, g −1 K−1 a generalized Silverman g-prior. [sent-22, score-0.042]
</p><p>15 For example, the number of support vectors in support vector machine (SVM) is equal to the number of nonzero components of β. [sent-24, score-0.045]
</p><p>16 That is, if βj = 0, the jth input vector is excluded from the basis expansion in (2); otherwise the jth input vector is a support vector. [sent-25, score-0.139]
</p><p>17 We are thus interested in a prior for β which allows some components of β to be zero. [sent-26, score-0.052]
</p><p>18 To specify such a prior we ﬁrst introduce an indicator vector γ = (γ1 , . [sent-27, score-0.067]
</p><p>19 , γn ) such that γj = 1 if xj is a support vector and γj = 0 if it is not. [sent-30, score-0.048]
</p><p>20 Let n nγ = j=1 γj be the number of support vectors, let Kγ be the n×nγ submatrix of K consisting of those columns of K for which γj = 1, and let β γ be the corresponding subvector of β. [sent-31, score-0.065]
</p><p>21 Accordingly, we let β γ ∼ Nnγ 0, g −1 K−1 where Kγγ is the nγ ×nγ submatrix of Kγ consisting of those rows γγ of Kγ for which γj = 1. [sent-32, score-0.031]
</p><p>22 We thus have a Bayesian model choice problem in which a family of models is indexed by an indicator vector γ. [sent-33, score-0.062]
</p><p>23 In this paper we provide a frequentist theoretical analysis of this Bayesian procedure. [sent-35, score-0.035]
</p><p>24 In particular, motivated by the work of [1] on the consistency of the Zellner g-prior, we investigate the consistency for model choice of the Silverman g-prior for sparse kernel-based regression. [sent-36, score-0.157]
</p><p>25 2  Main Results  Our analysis is based on the following regression model Mγ : y = u1n + Kγ β γ + 2  ∼ Nn (0, σ In ),  (4) 2  −1  β γ |σ ∼ Nnγ 0, σ (gγ Kγγ )  ,  where y = (y1 , . [sent-37, score-0.07]
</p><p>26 We compare each model Mγ with the null model M0 , formulating the model choice problem via the hypotheses H0 : β = 0 and Hγ : β γ ∈ Rnγ . [sent-42, score-0.153]
</p><p>27 The following condition is also assumed: 1 For a ﬁxed nγ < n, n Kγ Kγ is positive deﬁnite and converges to a positive deﬁnite matrix as n → ∞. [sent-45, score-0.08]
</p><p>28 (5)  Suppose that the sample y is generated by model Mν with parameter values u, β ν and σ. [sent-46, score-0.024]
</p><p>29 We formalize the problem of consistency for model choice as follows [1]: plim p(Mν |y) = 1 and plim p(Mγ |y) = 0 for all Mγ = Mν , (6) n→∞  n→∞  where “plim” denotes convergence in probability and the limit is taken w. [sent-47, score-1.031]
</p><p>30 the sampling distribution under the true model Mν . [sent-50, score-0.024]
</p><p>31 1  A Noninformative Prior for (u, σ 2 )  We ﬁrst consider the case when (u, σ 2 ) is assigned the following noninformative prior: (u, σ 2 ) ∝ 1/σ 2 . [sent-52, score-0.057]
</p><p>32 After some calculations the marginal likelihood is found to be p(y|Mγ ) =  Γ( n−1 ) 2 y − y 1n ¯ n−1 √ 2 π n  −n+1  1  2 |Qγ |− 2 (1 − Fγ )−  n−1 2  ,  (8)  where y = ¯  1 n  n i=1  yi , Qγ = In + gγ −1 Kγ K−1 Kγ and γγ y Kγ (gγ Kγγ + Kγ Kγ )−1 Kγ y . [sent-55, score-0.04]
</p><p>33 As a special case of (8), it is also immediate to obtain the marginal distribution of the null model as p(y|M0 ) =  Γ( n−1 ) 2 y − y 1n ¯ n−1 √ π 2 n  −n+1  . [sent-59, score-0.103]
</p><p>34 Then the Bayes factor for Mγ versus M0 is 1  2 BFγ0 = |Qγ |− 2 (1 − Fγ )−  n−1 2  . [sent-60, score-0.041]
</p><p>35 This implies that a large spread of the prior forces the Bayes factor to favor the null model. [sent-62, score-0.134]
</p><p>36 The Bayes factor for Mγ versus Mκ is given by 1  BFγκ  n−1  2 |Qγ |− 2 (1 − Fγ )− 2 BFγ0 = = 1 n−1 . [sent-64, score-0.041]
</p><p>37 2 BFκ0 |Qκ |− 2 (1 − Fκ )− 2  (9)  Based on the Bayes factor, we now explore the consistency of the Silverman g-prior. [sent-65, score-0.055]
</p><p>38 Suppose that the sample y is generated by model Mν with parameter values u, β ν and σ 2 . [sent-66, score-0.024]
</p><p>39 Then the consistency property (6) is equivalent to plim BFγν = 0, for all Mγ = Mν . [sent-67, score-0.512]
</p><p>40 n→∞  Assume that under any model Mγ that does not contain Mν , i. [sent-68, score-0.024]
</p><p>41 e, Mγ  lim  n→∞  Mν ,  β γ Kν (In − Hγ )Kν β γ = cγ ∈ (0, ∞), n  (10)  where β γ = (u, β γ ). [sent-69, score-0.139]
</p><p>42 Given that (In − Hγ )1n = 0 and 1n Kν = 0, condition (10) reduces to lim  n→∞  β ν Kν (In − Hγ )Kν β ν = cγ ∈ (0, ∞), n  where Hγ = Kγ (Kγ Kγ )−1 Kγ . [sent-71, score-0.161]
</p><p>43 We now have the following theorem whose proof is given in Sec. [sent-72, score-0.113]
</p><p>44 Theorem 1 Consider the regression model (4) with the noninformative prior for (u, σ 2 ) in (7). [sent-74, score-0.179]
</p><p>45 Assume that conditions (5) and (10) are satisﬁed and assume that gγ can be written in the form gγ =  w1 (nγ ) with w2 (n)  lim w2 (n) = ∞ and  n→∞  lim  n→∞  w2 (n) =0 w2 (n)  (11)  for particular choices of functions w1 and w2 , where w2 is differentiable and w2 (n) is the ﬁrst derivative w. [sent-75, score-0.278]
</p><p>46 , Mν = M0 , the posterior probabilities are consistent for model choice. [sent-81, score-0.041]
</p><p>47 It is interesting to consider the (asymptotic) relationship between the Bayes factor and Bayesian information (or Schwartz) criterion (BIC) in our setting. [sent-84, score-0.043]
</p><p>48 Given two models Mγ and Mκ , the difference between the BICs of these two models is given by Sγκ =  nκ − nγ n RSSκ ln + ln(n). [sent-85, score-0.508]
</p><p>49 2 RSSγ 2  We thus obtain the following asymptotic relationship (the proof is given in Sec. [sent-86, score-0.093]
</p><p>50 3): Theorem 2 Under the regression model and the conditions in Theorem 1, we have plim n→∞  ln BFγν Sγν +  nν −nγ 2  ln w2 (n)  = 1. [sent-87, score-1.543]
</p><p>51 ln BFγν Sγν  Furthermore, if Mν is not nested within Mγ , then plimn→∞ limits are taken w. [sent-88, score-0.7]
</p><p>52 (13)  The marginal likelihood of model Mγ is thus a /2  p(y|Mγ ) =  bσσ Γ( n+aσ ) 1 2 |Mγ |− 2 bσ + y M−1 y γ n/2 Γ( aσ ) π 2  +n − aσ2  ,  (14)  where Mγ = In + Kγ Σ−1 Kγ . [sent-97, score-0.045]
</p><p>53 The Bayes factor for Mγ versus Mκ is given by γ BFγκ  |Mκ | = |Mγ |  1 2  bσ + y M−1 y κ bσ + y M−1 y γ  aσ +n 2  . [sent-98, score-0.041]
</p><p>54 Theorem 3 Consider the regression model (4) with the conjugate prior for (u, σ 2 ) in (12). [sent-100, score-0.143]
</p><p>55 Assume that conditions (5) and (10) are satisﬁed and that gγ takes the form in (11) with w1 (nγ ) being a decreasing function. [sent-101, score-0.029]
</p><p>56 , Mν = M0 , the posterior probabilities are consistent for model choice. [sent-104, score-0.041]
</p><p>57 Note the difference between Theorem 1 and Theorem 3: in the latter theorem w1 (nγ ) is required to be a decreasing function of nγ . [sent-105, score-0.092]
</p><p>58 Thanks to the fact that gγ = w1 (nγ )/w2 (n), such a condition is equivalent to assuming that gγ is a decreasing function of nγ . [sent-106, score-0.051]
</p><p>59 Similarly with Theorem 2, we also have  Theorem 4 Under the regression model and the conditions in Theorem 3, we have ln BFγν = 1. [sent-108, score-0.578]
</p><p>60 plim nν −nγ n→∞ Sγν + ln w2 (n) 2 Furthermore, if Mν is not nested within Mγ , then plimn→∞ limits are taken w. [sent-109, score-1.157]
</p><p>61 A11 A12 be symmetric and positive deﬁnite, and let B = A21 A22 have the same size as A. [sent-115, score-0.029]
</p><p>62 Lemma 1 Let A =  A−1 11 0  0 0  Proof The proof follows readily once we express A−1 and B as A−1 =  I 0  −A−1 A12 11 I  A−1 11 0  0 A−1 22·1  B=  I 0  −A−1 A12 11 I  A−1 11 0  0 0  I −A21 A−1 11 I −A21 A−1 11  0 I  0 I  ,  ,  where A22·1 = A22 − A21 A−1 A12 is also positive deﬁnite. [sent-117, score-0.096]
</p><p>63 Lemma 2 Under the sampling model Mν : (i) if Mν is nested within or equal to a model Mγ , i. [sent-119, score-0.207]
</p><p>64 , Mν Mγ , then RSSγ = σ2 plim n n→∞ and (ii) for any model Mγ that does not contain Mν , if (10) satisﬁes, then RSSγ = σ 2 + cγ . [sent-121, score-0.481]
</p><p>65 plim n n→∞ Lemma 3 Under the sampling model Mν , if Mν is nested within a model Mγ , i. [sent-122, score-0.664]
</p><p>66 , Mν ⊂ Mγ , then n ln  RSSν RSSγ  d  d  −→ χ2 γ −nν as n → ∞ where −→ denotes convergence in distribution. [sent-124, score-0.508]
</p><p>67 n  Lemma 4 Under the regression model (4), if limn→∞ gγ (n) = 0 and condition (5) is satisﬁed, then 2 plim (1 − Fγ ) y − y 1n ¯  2  n→∞  − RSSγ = 0. [sent-125, score-0.549]
</p><p>68 σ2 σ2 Since both Kγ Kγ /n and Kγγ are positive deﬁnite, there exists an nγ ×nγ nonsingular matrix An and an nγ ×nγ positive diagonal matrix Λnγ such that Kγ Kγ /n = An Λnγ An and Kγγ = An An . [sent-127, score-0.118]
</p><p>69 nλj (n) + gγ (n) j  −1  z  2 Note that zj follows a noncentral chi-square distribution, χ2 (1, vj ), with vj = 2 2 nλj (n)(aj (n) β) /σ where λj (n) > 0 is the jth diagonal element of Λnγ and aj (n) is 2 2 the jth column of An . [sent-129, score-0.25]
</p><p>70 We thus have E(zj ) = 1 + vj and Var(zj ) = 2(1 + 2vj ). [sent-130, score-0.032]
</p><p>71 It follows from condition (5) that lim Kγ Kγ /n = lim An Λnγ An = A Λγ A, n→∞  n→∞  where A is nonsingular and Λγ is a diagonal matrix with positive diagonal elements, and both are independent of n. [sent-131, score-0.411]
</p><p>72 Hence, lim E  n→∞  gγ (n) z 2 = 0 and nλj (n) + gγ (n) j  lim Var  n→∞  gγ (n) z 2 = 0. [sent-132, score-0.278]
</p><p>73 Lemma 5 Assume that Mκ is nested within Mγ and gγ is a decreasing function of nγ . [sent-135, score-0.188]
</p><p>74 κ γ Proof Since Mκ is nested within Mγ , we express Kγ = [Kκ , K2 ] without loss of generality. [sent-137, score-0.176]
</p><p>75 0 0 0 (gκ −gγ )Kκκ  is positive semidef-  inite. [sent-140, score-0.029]
</p><p>76 Consequently, (Kκ Kκ +Σ11 )−1 − (Kκ Kκ +Σκ )−1 is positive semideﬁnite. [sent-141, score-0.029]
</p><p>77 It follows from γ (Kκ Kκ +Σκ )−1 0 is also positive semideﬁnite. [sent-142, score-0.029]
</p><p>78 Consider that ln BFγν =  2 1 |Qν | n−1 (1 − Fν ) ln + ln . [sent-146, score-1.524]
</p><p>79 2 2 |Qγ | 2 (1 − Fγ )  Because  nγ  |Qγ |  −1 2  gγ 2 |Kγγ |1/2 = , |gγ Kγγ + Kγ Kγ |1/2  we have |Qν | w1 (nγ )nγ |Kγγ | = ln + ln + ln ln |Qγ | w1 (nν )nν |Kνν |  w1 (nν ) nw2 (n) Kνν w1 (nγ ) nw2 (n) Kγγ  1 + n Kν Kν 1 + n Kγ Kγ  + (nν −nγ ) ln(nw2 (n)). [sent-147, score-2.032]
</p><p>80 Because α = lim ln n→∞  w1 (nν ) nw2 (n) Kνν w1 (nγ ) nw2 (n) Kγγ  1 + n Kν Kν 1 + n Kγ Kγ  = lim ln n→∞  1 | n Kν Kν | ∈ (−∞, ∞), 1 | n Kγ Kγ |  it is easily proven that ∞ nγ < n ν −∞ nγ > nν const nγ = nν ,  1 |Qν | ln = n→∞ 2 |Qγ | lim  where const =  α 2  +  1 2  ln  |Kγγ | |Kνν | . [sent-148, score-2.596]
</p><p>81 (15)  According to Lemma 4, we also have  2 2 y n−1 (1−Fν ) n−1 (1−Fν ) y−¯1n ln = plim ln 2) 2 ) y−¯1 (1−Fγ (1−Fγ y n n→∞ 2 n→∞ 2  plim  2 2  = plim  n→∞  n−1 RSSν ln . [sent-149, score-2.895]
</p><p>82 2 RSSγ  Now consider the following two cases: (a) Mν is not nested within Mγ : From Lemma 2, we obtain plim ln  n→∞  RSSν RSSν /n σ2 = plim ln = ln 2 . [sent-150, score-2.597]
</p><p>83 RSSγ RSSγ /n σ +cγ n→∞  Moreover, we have the following limit σ2 n−1 ln 2 n→∞ 2 σ +cγ due to limn→∞  nν −nγ n−1  ln(nw2 (n))  nν −nγ ln(nw2 (n)) = −∞ n−1  +  lim  =  limn→∞ (nν −nγ )  w2 (n)+nw2 (n) nw2 (n)  σ2 σ 2 +cγ  < 1. [sent-151, score-0.647]
</p><p>84 (b) Mν is nested within Mγ :  =  0 and  Thus we obtain  d  We always have nγ > nν . [sent-154, score-0.159]
</p><p>85 2  Proof of Theorem 2  Using the same notations as those in Theorem 1, we have  Cγν =  ln BFγν Sγν +  nν −nγ 2  ln w2 (n)  =  n−1 n  (1−F 2 )  ln (1−Fν ) + 2 γ  RSSν ln RSSγ  nν −nγ 2 ln(nw2 (n)) + n Const n . [sent-159, score-2.032]
</p><p>86 n −n + ν n γ ln(nw2 (n))  (a) Mν is not nested within Mγ : From Lemma 4, we obtain 2  plim Cγν = lim  ln σ2σ γ + +c  n→∞  n→∞  ln  σ2 σ 2 +c  γ  +  nν −nγ n nν −nγ n  ln(nw2 (n)) ln(nw2 (n))  = 1. [sent-160, score-1.771]
</p><p>87 In this case, we also have 2  n −n  ln σ2σ γ + ν n γ ln(nw2 (n)) ln BFγν +c = 1. [sent-161, score-1.016]
</p><p>88 plim = lim 2 n −n n→∞ Sγν n→∞ ln σ2σ γ + ν n γ ln n +c (b) Mν is nested within Mγ : We obtain (1−F 2 )  plim Cγν = plim  n→∞  n→∞  (n−1) ln (1−Fν ) + (nν −nγ ) ln(nw2 (n)) + 2 × Const 2 γ  RSSν n ln RSSγ + (nν −nγ ) ln(nw2 (n)) d  due to nγ > nν and n ln(RSSν /RSSγ ) −→ χ2 γ −nν . [sent-162, score-3.701]
</p><p>89 3  Proof of Theorem 3  We now sketch the proof of Theorem 3. [sent-164, score-0.05]
</p><p>90 For the case that Mν is not nested within Mγ , the proof is similar to that of Theorem 1. [sent-165, score-0.209]
</p><p>91 When Mν is nested within Mγ , Lemma 5 shows the following relationship ln  bσ + y In −Kν Θ−1 Kν y ν bσ + y In −Kγ Θ−1 Kγ y γ  ≤ ln  y In −Kν Θ−1 Kν y ν y In −Kγ Θ−1 Kγ y γ  . [sent-166, score-1.194]
</p><p>92 We thus have bσ + y In −Kν Θ−1 Kν y y In −Kν Θ−1 Kν y aσ +n aσ +n ν ν ≤ plim ln ln 2 2 n→∞ n→∞ bσ + y In −Kγ Θ−1 Kγ y y In −Kγ Θ−1 Kγ y γ γ plim  = plim  n→∞  y In −Hν y aσ +n ln ∈ (0, ∞). [sent-167, score-2.895]
</p><p>93 2 y In −Hγ y  From this result the proof follows readily. [sent-168, score-0.05]
</p><p>94 4  Conclusions  In this paper we have presented a frequentist analysis of a Bayesian model choice procedure for sparse regression. [sent-169, score-0.082]
</p><p>95 We have captured sparsity by a particular choice of prior distribution which we have referred to as a “Silverman g-prior. [sent-170, score-0.09]
</p><p>96 It is similar in spirit to the Zellner g-prior, which has been widely used for Bayesian variable selection and Bayesian model selection due to its computational tractability in the evaluation of marginal likelihoods [6, 2]. [sent-172, score-0.045]
</p><p>97 Our analysis provides a theoretical foundation for the Silverman g-prior and suggests that it can play a similarly wide-ranging role in the development of fully Bayesian kernel methods. [sent-173, score-0.025]
</p><p>98 Some aspects of the spline smoothing approach to non-parametric regression curve ﬁtting (with discussion). [sent-209, score-0.083]
</p><p>99 Bayesian factor regression models in the “large p, small n” paradigm. [sent-222, score-0.07]
</p><p>100 On assessing prior distributions and Bayesian regression analysis with g−prior distributions. [sent-240, score-0.098]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ln', 0.508), ('rss', 0.475), ('plim', 0.457), ('bf', 0.342), ('silverman', 0.147), ('lim', 0.139), ('nested', 0.133), ('nn', 0.114), ('zellner', 0.109), ('limn', 0.077), ('bayesian', 0.074), ('plimn', 0.065), ('const', 0.065), ('theorem', 0.063), ('lemma', 0.062), ('null', 0.058), ('noninformative', 0.057), ('consistency', 0.055), ('prior', 0.052), ('proof', 0.05), ('bayes', 0.048), ('jth', 0.047), ('regression', 0.046), ('rkhs', 0.044), ('plimg', 0.043), ('hk', 0.043), ('nonsingular', 0.038), ('spline', 0.037), ('frequentist', 0.035), ('econometrics', 0.033), ('semide', 0.032), ('regularization', 0.032), ('vj', 0.032), ('submatrix', 0.031), ('zj', 0.03), ('decreasing', 0.029), ('positive', 0.029), ('ga', 0.028), ('china', 0.026), ('within', 0.026), ('satis', 0.025), ('kernel', 0.025), ('factor', 0.024), ('model', 0.024), ('asymptotic', 0.024), ('choice', 0.023), ('var', 0.023), ('diagonal', 0.022), ('analogy', 0.022), ('condition', 0.022), ('conjugate', 0.021), ('marginal', 0.021), ('aj', 0.021), ('smith', 0.02), ('singular', 0.02), ('yi', 0.019), ('relationship', 0.019), ('noncentral', 0.019), ('bu', 0.019), ('honor', 0.019), ('kass', 0.019), ('departments', 0.019), ('paulo', 0.019), ('fern', 0.019), ('finetti', 0.019), ('goel', 0.019), ('hkust', 0.019), ('subvector', 0.019), ('yeung', 0.019), ('xj', 0.018), ('reproducing', 0.018), ('limits', 0.018), ('essays', 0.017), ('dawid', 0.017), ('ndez', 0.017), ('standpoint', 0.017), ('george', 0.017), ('express', 0.017), ('proven', 0.017), ('posterior', 0.017), ('versus', 0.017), ('noticing', 0.016), ('bernardo', 0.016), ('theme', 0.016), ('berger', 0.016), ('statistica', 0.016), ('association', 0.016), ('xi', 0.016), ('taken', 0.015), ('west', 0.015), ('heckerman', 0.015), ('amsterdam', 0.015), ('bic', 0.015), ('intercept', 0.015), ('american', 0.015), ('vector', 0.015), ('captured', 0.015), ('support', 0.015), ('kong', 0.015), ('eecs', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="182-tfidf-1" href="./nips-2008-Posterior_Consistency_of_the_Silverman_g-prior_in_Bayesian_Model_Choice.html">182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</a></p>
<p>Author: Zhihua Zhang, Michael I. Jordan, Dit-Yan Yeung</p><p>Abstract: Kernel supervised learning methods can be uniﬁed by utilizing the tools from regularization theory. The duality between regularization and prior leads to interpreting regularization methods in terms of maximum a posteriori estimation and has motivated Bayesian interpretations of kernel methods. In this paper we pursue a Bayesian interpretation of sparsity in the kernel setting by making use of a mixture of a point-mass distribution and prior that we refer to as “Silverman’s g-prior.” We provide a theoretical analysis of the posterior consistency of a Bayesian model choice procedure based on this prior. We also establish the asymptotic relationship between this procedure and the Bayesian information criterion. 1</p><p>2 0.22535114 <a title="182-tfidf-2" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>Author: Sham M. Kakade, Ambuj Tewari</p><p>Abstract: This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. As a corollary, we characterize the convergence rate of P EGASOS (with high probability), a recently proposed method for solving the SVM optimization problem. 1</p><p>3 0.13561347 <a title="182-tfidf-3" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>Author: Giovanni Cavallanti, Nicolò Cesa-bianchi, Claudio Gentile</p><p>Abstract: We provide a new analysis of an efﬁcient margin-based algorithm for selective sampling in classiﬁcation problems. Using the so-called Tsybakov low noise condition to parametrize the instance distribution, we show bounds on the convergence rate to the Bayes risk of both the fully supervised and the selective sampling versions of the basic algorithm. Our analysis reveals that, excluding logarithmic factors, the average risk of the selective sampler converges to the Bayes risk at rate N −(1+α)(2+α)/2(3+α) where N denotes the number of √ queried labels, and α > 0 is the exponent in the low noise condition. For all α > 3 − 1 ≈ 0.73 this convergence rate is asymptotically faster than the rate N −(1+α)/(2+α) achieved by the fully supervised version of the same classiﬁer, which queries all labels, and for α → ∞ the two rates exhibit an exponential gap. Experiments on textual data reveal that simple variants of the proposed selective sampler perform much better than popular and similarly efﬁcient competitors. 1</p><p>4 0.10533523 <a title="182-tfidf-4" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>Author: Mohak Shah</p><p>Abstract: We derive risk bounds for the randomized classiﬁers in Sample Compression setting where the classiﬁer-speciﬁcation utilizes two sources of information viz. the compression set and the message string. By extending the recently proposed Occam’s Hammer principle to the data-dependent settings, we derive point-wise versions of the bounds on the stochastic sample compressed classiﬁers and also recover the corresponding classical PAC-Bayes bound. We further show how these compare favorably to the existing results.</p><p>5 0.075759709 <a title="182-tfidf-5" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>Author: Steven J. Phillips, Miroslav Dudík</p><p>Abstract: We apply robust Bayesian decision theory to improve both generative and discriminative learners under bias in class proportions in labeled training data, when the true class proportions are unknown. For the generative case, we derive an entropybased weighting that maximizes expected log likelihood under the worst-case true class proportions. For the discriminative case, we derive a multinomial logistic model that minimizes worst-case conditional log loss. We apply our theory to the modeling of species geographic distributions from presence data, an extreme case of labeling bias since there is no absence data. On a benchmark dataset, we ﬁnd that entropy-based weighting offers an improvement over constant estimates of class proportions, consistently reducing log loss on unbiased test data. 1</p><p>6 0.072224498 <a title="182-tfidf-6" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>7 0.068299323 <a title="182-tfidf-7" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>8 0.068146378 <a title="182-tfidf-8" href="./nips-2008-Online_Optimization_in_X-Armed_Bandits.html">170 nips-2008-Online Optimization in X-Armed Bandits</a></p>
<p>9 0.067951694 <a title="182-tfidf-9" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>10 0.060528301 <a title="182-tfidf-10" href="./nips-2008-An_Online_Algorithm_for_Maximizing_Submodular_Functions.html">22 nips-2008-An Online Algorithm for Maximizing Submodular Functions</a></p>
<p>11 0.054125782 <a title="182-tfidf-11" href="./nips-2008-Bayesian_Exponential_Family_PCA.html">31 nips-2008-Bayesian Exponential Family PCA</a></p>
<p>12 0.050135314 <a title="182-tfidf-12" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>13 0.048052862 <a title="182-tfidf-13" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>14 0.044570245 <a title="182-tfidf-14" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>15 0.044123549 <a title="182-tfidf-15" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<p>16 0.038194645 <a title="182-tfidf-16" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>17 0.038126342 <a title="182-tfidf-17" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>18 0.037616204 <a title="182-tfidf-18" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>19 0.037222099 <a title="182-tfidf-19" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>20 0.034702685 <a title="182-tfidf-20" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.115), (1, -0.002), (2, -0.119), (3, 0.031), (4, -0.08), (5, 0.039), (6, -0.056), (7, 0.058), (8, -0.003), (9, -0.008), (10, -0.025), (11, 0.042), (12, 0.085), (13, -0.015), (14, 0.073), (15, -0.029), (16, -0.016), (17, 0.037), (18, -0.032), (19, -0.033), (20, -0.047), (21, -0.073), (22, 0.082), (23, 0.043), (24, -0.097), (25, -0.165), (26, 0.046), (27, -0.037), (28, 0.217), (29, 0.048), (30, 0.106), (31, -0.117), (32, 0.169), (33, 0.021), (34, -0.043), (35, 0.067), (36, 0.053), (37, 0.032), (38, 0.014), (39, 0.122), (40, 0.055), (41, -0.017), (42, -0.09), (43, -0.024), (44, 0.025), (45, -0.07), (46, 0.046), (47, 0.066), (48, 0.012), (49, -0.14)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95042396 <a title="182-lsi-1" href="./nips-2008-Posterior_Consistency_of_the_Silverman_g-prior_in_Bayesian_Model_Choice.html">182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</a></p>
<p>Author: Zhihua Zhang, Michael I. Jordan, Dit-Yan Yeung</p><p>Abstract: Kernel supervised learning methods can be uniﬁed by utilizing the tools from regularization theory. The duality between regularization and prior leads to interpreting regularization methods in terms of maximum a posteriori estimation and has motivated Bayesian interpretations of kernel methods. In this paper we pursue a Bayesian interpretation of sparsity in the kernel setting by making use of a mixture of a point-mass distribution and prior that we refer to as “Silverman’s g-prior.” We provide a theoretical analysis of the posterior consistency of a Bayesian model choice procedure based on this prior. We also establish the asymptotic relationship between this procedure and the Bayesian information criterion. 1</p><p>2 0.648435 <a title="182-lsi-2" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>Author: Sham M. Kakade, Ambuj Tewari</p><p>Abstract: This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. As a corollary, we characterize the convergence rate of P EGASOS (with high probability), a recently proposed method for solving the SVM optimization problem. 1</p><p>3 0.55966955 <a title="182-lsi-3" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>Author: Giovanni Cavallanti, Nicolò Cesa-bianchi, Claudio Gentile</p><p>Abstract: We provide a new analysis of an efﬁcient margin-based algorithm for selective sampling in classiﬁcation problems. Using the so-called Tsybakov low noise condition to parametrize the instance distribution, we show bounds on the convergence rate to the Bayes risk of both the fully supervised and the selective sampling versions of the basic algorithm. Our analysis reveals that, excluding logarithmic factors, the average risk of the selective sampler converges to the Bayes risk at rate N −(1+α)(2+α)/2(3+α) where N denotes the number of √ queried labels, and α > 0 is the exponent in the low noise condition. For all α > 3 − 1 ≈ 0.73 this convergence rate is asymptotically faster than the rate N −(1+α)/(2+α) achieved by the fully supervised version of the same classiﬁer, which queries all labels, and for α → ∞ the two rates exhibit an exponential gap. Experiments on textual data reveal that simple variants of the proposed selective sampler perform much better than popular and similarly efﬁcient competitors. 1</p><p>4 0.41625983 <a title="182-lsi-4" href="./nips-2008-An_Online_Algorithm_for_Maximizing_Submodular_Functions.html">22 nips-2008-An Online Algorithm for Maximizing Submodular Functions</a></p>
<p>Author: Matthew Streeter, Daniel Golovin</p><p>Abstract: We present an algorithm for solving a broad class of online resource allocation problems. Our online algorithm can be applied in environments where abstract jobs arrive one at a time, and one can complete the jobs by investing time in a number of abstract activities, according to some schedule. We assume that the fraction of jobs completed by a schedule is a monotone, submodular function of a set of pairs (v, τ ), where τ is the time invested in activity v. Under this assumption, our online algorithm performs near-optimally according to two natural metrics: (i) the fraction of jobs completed within time T , for some ﬁxed deadline T > 0, and (ii) the average time required to complete each job. We evaluate our algorithm experimentally by using it to learn, online, a schedule for allocating CPU time among solvers entered in the 2007 SAT solver competition. 1</p><p>5 0.38084114 <a title="182-lsi-5" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>Author: Cédric Archambeau, Francis R. Bach</p><p>Abstract: We present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a preprocessing tool for the construction of template attacks. 1</p><p>6 0.37166226 <a title="182-lsi-6" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>7 0.33870545 <a title="182-lsi-7" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>8 0.31846786 <a title="182-lsi-8" href="./nips-2008-Online_Optimization_in_X-Armed_Bandits.html">170 nips-2008-Online Optimization in X-Armed Bandits</a></p>
<p>9 0.31712109 <a title="182-lsi-9" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>10 0.310974 <a title="182-lsi-10" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>11 0.30978957 <a title="182-lsi-11" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>12 0.30569911 <a title="182-lsi-12" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>13 0.30292878 <a title="182-lsi-13" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>14 0.29218274 <a title="182-lsi-14" href="./nips-2008-Characteristic_Kernels_on_Groups_and_Semigroups.html">44 nips-2008-Characteristic Kernels on Groups and Semigroups</a></p>
<p>15 0.2645584 <a title="182-lsi-15" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>16 0.2550877 <a title="182-lsi-16" href="./nips-2008-Bayesian_Exponential_Family_PCA.html">31 nips-2008-Bayesian Exponential Family PCA</a></p>
<p>17 0.23952071 <a title="182-lsi-17" href="./nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</a></p>
<p>18 0.22657166 <a title="182-lsi-18" href="./nips-2008-Bayesian_Model_of_Behaviour_in_Economic_Games.html">33 nips-2008-Bayesian Model of Behaviour in Economic Games</a></p>
<p>19 0.22160363 <a title="182-lsi-19" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>20 0.21725534 <a title="182-lsi-20" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.126), (7, 0.105), (12, 0.04), (15, 0.028), (28, 0.127), (37, 0.3), (57, 0.046), (59, 0.016), (63, 0.018), (77, 0.039), (83, 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75075531 <a title="182-lda-1" href="./nips-2008-Posterior_Consistency_of_the_Silverman_g-prior_in_Bayesian_Model_Choice.html">182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</a></p>
<p>Author: Zhihua Zhang, Michael I. Jordan, Dit-Yan Yeung</p><p>Abstract: Kernel supervised learning methods can be uniﬁed by utilizing the tools from regularization theory. The duality between regularization and prior leads to interpreting regularization methods in terms of maximum a posteriori estimation and has motivated Bayesian interpretations of kernel methods. In this paper we pursue a Bayesian interpretation of sparsity in the kernel setting by making use of a mixture of a point-mass distribution and prior that we refer to as “Silverman’s g-prior.” We provide a theoretical analysis of the posterior consistency of a Bayesian model choice procedure based on this prior. We also establish the asymptotic relationship between this procedure and the Bayesian information criterion. 1</p><p>2 0.74363559 <a title="182-lda-2" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>Author: Fabian H. Sinz, Matthias Bethge</p><p>Abstract: Bandpass ﬁltering, orientation selectivity, and contrast gain control are prominent features of sensory coding at the level of V1 simple cells. While the effect of bandpass ﬁltering and orientation selectivity can be assessed within a linear model, contrast gain control is an inherently nonlinear computation. Here we employ the class of Lp elliptically contoured distributions to investigate the extent to which the two features—orientation selectivity and contrast gain control—are suited to model the statistics of natural images. Within this framework we ﬁnd that contrast gain control can play a signiﬁcant role for the removal of redundancies in natural images. Orientation selectivity, in contrast, has only a very limited potential for redundancy reduction. 1</p><p>3 0.56961095 <a title="182-lda-3" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>Author: Siwei Lyu, Eero P. Simoncelli</p><p>Abstract: We consider the problem of transforming a signal to a representation in which the components are statistically independent. When the signal is generated as a linear transformation of independent Gaussian or non-Gaussian sources, the solution may be computed using a linear transformation (PCA or ICA, respectively). Here, we consider a complementary case, in which the source is non-Gaussian but elliptically symmetric. Such a source cannot be decomposed into independent components using a linear transform, but we show that a simple nonlinear transformation, which we call radial Gaussianization (RG), is able to remove all dependencies. We apply this methodology to natural signals, demonstrating that the joint distributions of nearby bandpass ﬁlter responses, for both sounds and images, are closer to being elliptically symmetric than linearly transformed factorial sources. Consistent with this, we demonstrate that the reduction in dependency achieved by applying RG to either pairs or blocks of bandpass ﬁlter responses is signiﬁcantly greater than that achieved by PCA or ICA.</p><p>4 0.56290287 <a title="182-lda-4" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>5 0.55872136 <a title="182-lda-5" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>Author: Stefan Haufe, Vadim V. Nikulin, Andreas Ziehe, Klaus-Robert Müller, Guido Nolte</p><p>Abstract: We introduce a novel framework for estimating vector ﬁelds using sparse basis ﬁeld expansions (S-FLEX). The notion of basis ﬁelds, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector ﬁeld, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that signiﬁcantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art. 1</p><p>6 0.55634069 <a title="182-lda-6" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>7 0.55244851 <a title="182-lda-7" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>8 0.54935223 <a title="182-lda-8" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>9 0.54725355 <a title="182-lda-9" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>10 0.54676235 <a title="182-lda-10" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>11 0.54446834 <a title="182-lda-11" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>12 0.54445201 <a title="182-lda-12" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>13 0.54338694 <a title="182-lda-13" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>14 0.54270154 <a title="182-lda-14" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>15 0.54266185 <a title="182-lda-15" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>16 0.54202425 <a title="182-lda-16" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>17 0.54162848 <a title="182-lda-17" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>18 0.54065931 <a title="182-lda-18" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>19 0.53781968 <a title="182-lda-19" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>20 0.53774232 <a title="182-lda-20" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
