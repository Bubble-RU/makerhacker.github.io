<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>248 nips-2008-Using matrices to model symbolic relationship</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-248" href="#">nips2008-248</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>248 nips-2008-Using matrices to model symbolic relationship</h1>
<br/><p>Source: <a title="nips-2008-248-pdf" href="http://papers.nips.cc/paper/3482-using-matrices-to-model-symbolic-relationship.pdf">pdf</a></p><p>Author: Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn ﬁrst-order propositions such as (2, 5) ∈ +3 or (Christopher, Penelope) ∈ has wife, and higher-order propositions such as (3, +3) ∈ plus and (+3, −3) ∈ inverse or (has husband, has wife) ∈ higher oppsex. We further demonstrate that the system understands how higher-order propositions are related to ﬁrst-order ones by showing that it can correctly answer questions about ﬁrst-order propositions involving the relations +3 or has wife even though it has not been trained on any ﬁrst-order examples involving these relations. 1</p><p>Reference: <a title="nips-2008-248-reference" href="../nips2008_reference/nips-2008-Using_matrices_to_model_symbolic_relationship_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Using matrices to model symbolic relationships  Ilya Sutskever and Geoffrey Hinton University of Toronto {ilya, hinton}@cs. [sent-1, score-0.159]
</p><p>2 ca  Abstract We describe a way of learning matrix representations of objects and relationships. [sent-3, score-0.226]
</p><p>3 The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. [sent-4, score-0.451]
</p><p>4 We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. [sent-5, score-0.444]
</p><p>5 We show that the same system can learn ﬁrst-order propositions such as (2, 5) ∈ +3 or (Christopher, Penelope) ∈ has wife, and higher-order propositions such as (3, +3) ∈ plus and (+3, −3) ∈ inverse or (has husband, has wife) ∈ higher oppsex. [sent-6, score-0.49]
</p><p>6 1  Introduction  It is sometimes possible to ﬁnd a way of mapping objects in a “data” domain into objects in a “target” domain so that operations in the data domain can be modelled by operations in the target domain. [sent-8, score-0.308]
</p><p>7 When the objects in the data and target domains are more complicated than single numbers, it may be difﬁcult to ﬁnd good mappings using inspiration alone. [sent-10, score-0.188]
</p><p>8 Paccanaro and Hinton [10] introduced a method called “Linear Relational Embedding” (LRE) that uses multiplication of vectors by matrices in the target domain to model pairwise relations between objects in the data domain. [sent-12, score-0.686]
</p><p>9 LRE applies to a ﬁnite set of objects Ω and a ﬁnite set of relations R where every relation R ∈ R is a set of pairs of objects, so R ⊆ Ω × Ω. [sent-13, score-0.635]
</p><p>10 1 is “discriminative” because it compares the distance from RA to each correct answer with the distances from RA to all possible answers. [sent-16, score-0.131]
</p><p>11 The cost function then represents the sum of the negative log probabilities of picking the correct answers to questions of the form (A,? [sent-19, score-0.237]
</p><p>12 ) ∈ R if we pick answers stochastically in proportion to their probability densities under the spherical Gaussian centered at RA. [sent-20, score-0.164]
</p><p>13 We say that LRE accurately models a set of objects and relations if its answers to queries of the form (A, ? [sent-21, score-0.652]
</p><p>14 ) ∈ R are correct, which means that for each object A and relation R such that there are k objects X satisfying (A, X) ∈ R, each vector representation X of each such object X must be among the k closest vector representations to RA. [sent-22, score-0.442]
</p><p>15 ) ∈ R that has k correct answers (that is, (A, B) was removed from R during LRE’s learning), yet LRE answers the query (A, ? [sent-29, score-0.283]
</p><p>16 ) ∈ R correctly by placing B among the k closest object representations to RA, then we can claim that LRE’s representation generalizes. [sent-30, score-0.164]
</p><p>17 If the representation is high-dimensional, then LRE can easily represent any set of relations that is not too large, so its inductive bias ﬁnds all sets of relations plausible, which prevents generalization from being good. [sent-34, score-0.905]
</p><p>18 Paccanaro and Hinton [10] show that lowdimensional LRE exhibits excellent generalization on datasets such as the family relations task. [sent-36, score-0.479]
</p><p>19 A drawback of LRE is that the square matrices it uses to represent relations are quadratically more cumbersome than the vectors it uses to represent objects. [sent-39, score-0.567]
</p><p>20 More importantly, it also means that relations cannot themselves be treated as objects. [sent-41, score-0.372]
</p><p>21 In this paper we describe “Matrix Relational Embedding” (MRE), which is a version of LRE that uses matrices as the representation for objects as well as for relations. [sent-43, score-0.299]
</p><p>22 1 We have also experimented with a version of LRE that learns to generate a learned matrix representation of a relation from a learned vector representation of the relation. [sent-49, score-0.324]
</p><p>23 This too makes it possible to treat relations as objects because they both have vector representations. [sent-50, score-0.516]
</p><p>24 However, it is less straightforward than simply representing objects by matrices and it does not generalize quite as well. [sent-51, score-0.267]
</p><p>25 It can also represent relations involving an object and a relation, for instance (3, +3) ∈ plus. [sent-53, score-0.486]
</p><p>26 Formally, we are given a ﬁnite set of higher-order rela˜ ˜ ˜ tions R, where a higher-order relation R ∈ R is a relation whose arguments can be relations as well ˜ ⊆ R × R or R ⊆ Ω × R (R is the set of the basic relations). [sent-54, score-0.679]
</p><p>27 ˜ as objects, which we formalize as R The matrix representation of MRE allows it to treat relations in (almost) the same way it treats basic objects, so there is no difﬁculty representing relations whose arguments are also relations. [sent-55, score-0.92]
</p><p>28 ) ∈ +3 even though the training set contains no examples of the basic relation +3. [sent-57, score-0.215]
</p><p>29 It is told that (3, +3) ∈ plus and it ﬁgures out what plus means from higher-order examples of the form (2, +2) ∈ plus and basic examples of the form (3, 5) ∈ +2. [sent-59, score-0.316]
</p><p>30 This enables MRE to understand a relation from an “analogical deﬁnition”: if it is told that has f ather to has mother is like has brother to has sister, etc. [sent-60, score-0.335]
</p><p>31 , then MRE can answer queries involving has f ather based on this analogical information alone. [sent-61, score-0.232]
</p><p>32 Finally, we show that MRE can learn new relations after an initial set of objects and relations has already been learned and the learned matrices have been ﬁxed. [sent-62, score-1.069]
</p><p>33 This shows that MRE can add new knowledge to previously acquired propositions without the need to relearn the original propositions. [sent-63, score-0.185]
</p><p>34 We believe that MRE is the ﬁrst gradient-descent learning system that can learn new relations from deﬁnitions, including learning the meanings of the terms used in the deﬁnitions. [sent-64, score-0.394]
</p><p>35 Some of the existing connectionist models for representing and learning relations and analogies [2, 4] are able to detect new relations and to represent hierarchical relations of high complexity. [sent-66, score-1.175]
</p><p>36 They differ by using temporal synchrony for explicitly representing the binding of the relations to object, and, more importantly, do not use distributed representations for representing the relations themselves. [sent-67, score-0.854]
</p><p>37 2  The modular arithmetic task  Paccanaro and Hinton [10] describe a very simple modular arithmetic task in which the 10 objects are the numbers from 0 to 9 and the 9 relations are +0 to +4 and −1 to −4. [sent-68, score-1.274]
</p><p>38 Linear Relational Embedding easily learns this task using two-dimensional vectors for the numbers and 2 × 2 matrices for the relations. [sent-69, score-0.167]
</p><p>39 It arranges the numbers in a circle centered at the origin and uses rotation matrices to implement the relations. [sent-70, score-0.164]
</p><p>40 We used base 12 modular arithmetic, thus there are 12 objects, and made the task much more difﬁcult by using both the twelve relations +0 to +11 and the twelve relations ×0 to ×11. [sent-71, score-0.98]
</p><p>41 We did not include subtraction and division because in modular arithmetic every proposition involving subtraction or division is equivalent to one involving addition or multiplication. [sent-72, score-0.487]
</p><p>42 There are 288 propositions in the modular arithmetic ntask. [sent-73, score-0.522]
</p><p>43 We tried matrices of various sizes and discovered that 4 × 4 matrices gave the best generalization when some of the cases are held-out. [sent-74, score-0.259]
</p><p>44 We held-out 30, 60, or 90 test cases chosen at random and used the remaining cases to learn the realvalued entries of the 12 matrices that represent numbers and the 24 matrices that represent relations. [sent-75, score-0.414]
</p><p>45 We computed the gradient of the cost function on all of the training cases before updating the parameters, and initialized the parameters by a random sample from a spherical Gaussian with unit variance 2 on each dimension. [sent-81, score-0.169]
</p><p>46 01 i wi to the cost function, where i indexes all of the entries in the matrices for objects and relations. [sent-83, score-0.277]
</p><p>47 errors on 5 test sets mean test error (30) 0 0 0 0 0 0. [sent-92, score-0.175]
</p><p>48 0 Table 1: Test results on the basic modular arithmetic task. [sent-95, score-0.406]
</p><p>49 Each test query has 12 possible answers of which 1 is correct, so random guessing should be incorrect on at least 90% of the test cases. [sent-98, score-0.255]
</p><p>50 Separate experiments showed that 2 × 2 matrices were sufﬁcient for learning either the mod 3 or the mod 4 version of our modular arithmetic task, so the mod 12 version can clearly be done using a pair of 2 × 2 matrices for each number or relation. [sent-103, score-0.612]
</p><p>51 Notice that for the last four relations there are people in the families in ﬁgure 1(a) for whom there are two different correct answers to the question (A,? [sent-106, score-0.523]
</p><p>52 When there are N correct answers, the best way to maximize the sum of the log probabilities of picking the correct answer on each of the N cases is to produce an output matrix that is equidistant from the N correct answers and far from all other answers. [sent-108, score-0.436]
</p><p>53 If we count cases with two correct answers as two different cases the family trees task has 112 cases. [sent-110, score-0.422]
</p><p>54 We used precisely the same learning procedure and weight-decay as for the modular arithmetic task. [sent-111, score-0.337]
</p><p>55 We held-out 10, 20, or 30 randomly selected cases as test cases, and we repeated the random selection of the test cases ﬁve times. [sent-112, score-0.184]
</p><p>56 Table 2 shows the number of errors on the test cases when 4 × 4 matrices are learned for each person and for each relation. [sent-113, score-0.292]
</p><p>57 MRE generalizes much better than the  Test results for the basic family trees task. [sent-114, score-0.252]
</p><p>58 errors on 5 test sets mean test error (10) 0 0 0 0 2 0. [sent-115, score-0.175]
</p><p>59 0 Table 2: Test results on the basic family trees task. [sent-118, score-0.225]
</p><p>60 Each test query has 24 possible answers, of which at most 2 objects are considered correct. [sent-122, score-0.226]
</p><p>61 feedforward neural network used by [3] which typically gets one or two test cases wrong even when only four test cases are held-out. [sent-124, score-0.24]
</p><p>62 It also generalizes much better than all of the many variations of the learning algorithms used by [8] for the family trees task. [sent-125, score-0.183]
</p><p>63 These variations cannot achieve zero test errors even when only four test cases are held-out and the cases are chosen to facilitate generalization. [sent-126, score-0.259]
</p><p>64 5  The higher-order modular arithmetic task  We used a version of the modular arithmetic task in which the only basic relations were {+0, +2, . [sent-127, score-1.177]
</p><p>65 , +11}, but we also included the higher-order relations plus, minus, inverse consisting of 36 propositions, examples of which are (3, +3) ∈ plus; (3, +9) ∈ minus; (+3, +9) ∈ inverse. [sent-130, score-0.372]
</p><p>66 We then held-out all of the examples of one of the basic relations and trained 4 × 4 matrices on all of the other basic relations plus all of the higher-order relations. [sent-131, score-1.041]
</p><p>67 Our ﬁrst attempt to demonstrate that MRE could generalize from higher-order relations to basic relations failed: the generalization was only slightly better than chance. [sent-132, score-0.848]
</p><p>68 When learning the higher-order training case (3, +3) ∈ plus it is not necessary for the product of the matrix representing 3 and the matrix representing plus to be exactly equal to the matrix representing +3. [sent-135, score-0.367]
</p><p>69 In cases like the one shown in ﬁgure 1(b), the relative probability of the point B under a Gaussian centered at RA is increased by moving RA up, because this lowers the unnormalized probabilities of C and D by a greater proportion than it lowers the unnormalized probability of B. [sent-137, score-0.178]
</p><p>70 The discriminative objective function prevents all of the representations collapsing to the same point, but it does not force the matrix products to be exactly equal to the correct answer. [sent-138, score-0.183]
</p><p>71 Even when using this non-discriminative cost function for training the higher-order relations, the matrices could not all collapse to zero because the discriminative cost function was still being used for training the basic relations. [sent-141, score-0.325]
</p><p>72 errors on 5 test sets mean test error +1 (12) 5 0 0 0 0 1. [sent-144, score-0.175]
</p><p>73 6 Table 3: Test results on the higher-order arithmetic task. [sent-148, score-0.198]
</p><p>74 Each row shows the number of incorrectly answered queries involving a relation (i. [sent-149, score-0.205]
</p><p>75 , +1, +4, +6, or +10) all of whose basic examples were removed from MRE’s training data, so MRE’s knowledge of this relation was entirely from the other higher-order relations. [sent-151, score-0.215]
</p><p>76 errors on 5 test sets mean test error has father (12) 0 12 0 0 0 2. [sent-156, score-0.29]
</p><p>77 6 Table 4: Test results for the higher-order family trees task. [sent-160, score-0.156]
</p><p>78 In each row, all basic propositions involving a relation are held-out (i. [sent-161, score-0.423]
</p><p>79 Each row shows the number of errors MRE makes on these held-out propositions on 5 different learning runs from different initial random parameters. [sent-164, score-0.26]
</p><p>80 The only information MRE has on these relations is in the form of a single higher-order relation, higher oppsex. [sent-165, score-0.402]
</p><p>81 6  The higher-order family trees task  To demonstrate that similar performance is obtained on family trees task when higher-order relations are used, we included in addition to the 112 basic relations the higher-order relation higher oppsex. [sent-168, score-1.336]
</p><p>82 To deﬁne higher oppsex we observe that many relations have natural male and natural female versions, as in: mother-father, nephew-niece, uncle-aunt, brother-sister, husband-wife, and sondaughter. [sent-169, score-0.517]
</p><p>83 We say that (A, B) ∈ higher oppsex for relations A and B if A and B can be seen as natural counterparts in this sense. [sent-170, score-0.517]
</p><p>84 Four of the twelve examples of higher oppsex are given below: 1. [sent-171, score-0.178]
</p><p>85 (has sister, has brother) ∈ higher oppsex We performed an analogous test to that in the previous section on the higher order modular arithmetic task, using exactly the same learning procedure and learning parameters. [sent-175, score-0.562]
</p><p>86 The family trees task and its higher-order variant may appear difﬁcult for systems such as MRE or LRE because of the logical nature of the task, which is made apparent by hard rules such as (A, B) ∈ has father, (A, C) ∈ has brother ⇒ (C, B) ∈ has father. [sent-177, score-0.3]
</p><p>87 Instead, it “precomputes the answers” to all queries during training, by ﬁnding the matrix representation that models its training set. [sent-181, score-0.138]
</p><p>88 errors on 5 test sets mean test error +1 (12) 0 0 0 2 4 1. [sent-187, score-0.175]
</p><p>89 4  has has has has  The sequential higher-order family trees task. [sent-191, score-0.156]
</p><p>90 errors on 5 test sets mean test error father (12) 0 0 0 10 0 2. [sent-192, score-0.29]
</p><p>91 0  Table 5: Test results for the higher-order arithmetic task (top) and the higher-order family trees task (bottom) when a held-out basic relation is learned from higher-order propositions after the rest of the objects and relations have been learned and ﬁxed. [sent-196, score-1.373]
</p><p>92 Each entry shows the number of test errors, and the number of test cases is written in brackets. [sent-198, score-0.142]
</p><p>93 This does not mean that MRE can deal with general logical data of this kind, because MRE will fail when there are many relations that have many special cases. [sent-201, score-0.403]
</p><p>94 The special cases will prevent MRE from ﬁnding low dimensional matrices that ﬁt the data well and cause it to generalize much more poorly. [sent-202, score-0.133]
</p><p>95 7  Adding knowledge incrementally  The previous section shows that MRE can learn to apply a basic relation correctly even though the training set only contains higher-order propositions about the relation. [sent-203, score-0.444]
</p><p>96 After learning some objects, basic relations, and higher-order relations, we freeze the weights in all of the matrices and learn the matrix for a new relation from a few higherorder propositions. [sent-205, score-0.337]
</p><p>97 Table 5 shows that this works about as well as learning all of the propositions at the same time. [sent-206, score-0.185]
</p><p>98 The input vectors R and A represent a relation and an object using a one-of-N encoding. [sent-208, score-0.183]
</p><p>99 If the outgoing weights from the two active input units are set to R and A, these localist representations are converted into activity patterns in the ﬁrst hidden layer that represent the matrices R and A. [sent-209, score-0.24]
</p><p>100 The fact that MRE generalizes much better than a standard feedforward neural network on the family trees task is due to two features. [sent-217, score-0.27]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lre', 0.445), ('mre', 0.433), ('relations', 0.372), ('ra', 0.334), ('arithmetic', 0.198), ('propositions', 0.185), ('objects', 0.144), ('modular', 0.139), ('relation', 0.119), ('father', 0.115), ('oppsex', 0.115), ('sister', 0.101), ('answers', 0.1), ('matrices', 0.091), ('paccanaro', 0.087), ('relational', 0.087), ('trees', 0.084), ('brother', 0.082), ('answer', 0.08), ('errors', 0.075), ('family', 0.072), ('basic', 0.069), ('plus', 0.068), ('aunt', 0.066), ('nephew', 0.066), ('wife', 0.066), ('mother', 0.058), ('hinton', 0.057), ('correct', 0.051), ('involving', 0.05), ('test', 0.05), ('ij', 0.049), ('representations', 0.046), ('symbolic', 0.045), ('husband', 0.043), ('told', 0.043), ('cost', 0.042), ('cases', 0.042), ('representation', 0.039), ('spherical', 0.038), ('inductive', 0.037), ('bij', 0.037), ('object', 0.037), ('queries', 0.036), ('matrix', 0.036), ('generalization', 0.035), ('multiplication', 0.034), ('learned', 0.034), ('twelve', 0.033), ('analogical', 0.033), ('ather', 0.033), ('penelope', 0.033), ('feedforward', 0.032), ('logic', 0.032), ('query', 0.032), ('representing', 0.032), ('task', 0.031), ('mod', 0.031), ('logical', 0.031), ('higher', 0.03), ('alberto', 0.029), ('hummel', 0.029), ('lowers', 0.029), ('ilya', 0.029), ('layer', 0.028), ('units', 0.028), ('generalizes', 0.027), ('represent', 0.027), ('training', 0.027), ('discriminative', 0.027), ('embedding', 0.027), ('centered', 0.026), ('minus', 0.026), ('unnormalized', 0.026), ('subtraction', 0.025), ('picking', 0.025), ('uses', 0.025), ('mappings', 0.024), ('explicit', 0.024), ('network', 0.024), ('guessing', 0.023), ('prevents', 0.023), ('humans', 0.023), ('relationships', 0.023), ('learns', 0.023), ('numbers', 0.022), ('cij', 0.022), ('correctly', 0.022), ('learn', 0.022), ('table', 0.021), ('softmax', 0.021), ('causes', 0.021), ('outgoing', 0.02), ('gradient', 0.02), ('closest', 0.02), ('target', 0.02), ('questions', 0.019), ('representational', 0.019), ('christopher', 0.019), ('novelty', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="248-tfidf-1" href="./nips-2008-Using_matrices_to_model_symbolic_relationship.html">248 nips-2008-Using matrices to model symbolic relationship</a></p>
<p>Author: Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn ﬁrst-order propositions such as (2, 5) ∈ +3 or (Christopher, Penelope) ∈ has wife, and higher-order propositions such as (3, +3) ∈ plus and (+3, −3) ∈ inverse or (has husband, has wife) ∈ higher oppsex. We further demonstrate that the system understands how higher-order propositions are related to ﬁrst-order ones by showing that it can correctly answer questions about ﬁrst-order propositions involving the relations +3 or has wife even though it has not been trained on any ﬁrst-order examples involving these relations. 1</p><p>2 0.20657167 <a title="248-tfidf-2" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>Author: Ali Nouri, Michael L. Littman</p><p>Abstract: The essence of exploration is acting to try to decrease uncertainty. We propose a new methodology for representing uncertainty in continuous-state control problems. Our approach, multi-resolution exploration (MRE), uses a hierarchical mapping to identify regions of the state space that would beneﬁt from additional samples. We demonstrate MRE’s broad utility by using it to speed up learning in a prototypical model-based and value-based reinforcement-learning method. Empirical results show that MRE improves upon state-of-the-art exploration approaches. 1</p><p>3 0.12375181 <a title="248-tfidf-3" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>Author: Christian Walder, Bernhard Schölkopf</p><p>Abstract: This paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points. We argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a natural way of ensuring that pairwise distances are approximately preserved. Accordingly we develop an algorithm which diffeomorphically maps the data near to a lower dimensional subspace and then projects onto that subspace. The problem of solving for the mapping is transformed into one of solving for an Eulerian ﬂow ﬁeld which we compute using ideas from kernel methods. We demonstrate the efﬁcacy of our approach on various real world data sets. 1</p><p>4 0.087884165 <a title="248-tfidf-4" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>Author: Thomas L. Griffiths, Joseph L. Austerweil</p><p>Abstract: Almost all successful machine learning algorithms and cognitive models require powerful representations capturing the features that are relevant to a particular problem. We draw on recent work in nonparametric Bayesian statistics to deﬁne a rational model of human feature learning that forms a featural representation from raw sensory data without pre-specifying the number of features. By comparing how the human perceptual system and our rational model use distributional and category information to infer feature representations, we seek to identify some of the forces that govern the process by which people separate and combine sensory primitives to form features. 1</p><p>5 0.06986443 <a title="248-tfidf-5" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>Author: Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg, Eric P. Xing</p><p>Abstract: In many settings, such as protein interactions and gene regulatory networks, collections of author-recipient email, and social networks, the data consist of pairwise measurements, e.g., presence or absence of links between pairs of objects. Analyzing such data with probabilistic models requires non-standard assumptions, since the usual independence or exchangeability assumptions no longer hold. In this paper, we introduce a class of latent variable models for pairwise measurements: mixed membership stochastic blockmodels. Models in this class combine a global model of dense patches of connectivity (blockmodel) with a local model to instantiate node-speciﬁc variability in the connections (mixed membership). We develop a general variational inference algorithm for fast approximate posterior inference. We demonstrate the advantages of mixed membership stochastic blockmodel with applications to social networks and protein interaction networks. 1</p><p>6 0.06380178 <a title="248-tfidf-6" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<p>7 0.061709858 <a title="248-tfidf-7" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>8 0.053695578 <a title="248-tfidf-8" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>9 0.046837877 <a title="248-tfidf-9" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>10 0.041456297 <a title="248-tfidf-10" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>11 0.040946394 <a title="248-tfidf-11" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>12 0.038160264 <a title="248-tfidf-12" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>13 0.036114208 <a title="248-tfidf-13" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>14 0.036110204 <a title="248-tfidf-14" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>15 0.035733934 <a title="248-tfidf-15" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>16 0.035262927 <a title="248-tfidf-16" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>17 0.035213117 <a title="248-tfidf-17" href="./nips-2008-Implicit_Mixtures_of_Restricted_Boltzmann_Machines.html">103 nips-2008-Implicit Mixtures of Restricted Boltzmann Machines</a></p>
<p>18 0.035079949 <a title="248-tfidf-18" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>19 0.034885861 <a title="248-tfidf-19" href="./nips-2008-An_ideal_observer_model_of_infant_object_perception.html">23 nips-2008-An ideal observer model of infant object perception</a></p>
<p>20 0.034462661 <a title="248-tfidf-20" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.123), (1, -0.01), (2, 0.044), (3, -0.045), (4, 0.038), (5, -0.04), (6, 0.033), (7, 0.003), (8, -0.005), (9, -0.01), (10, -0.012), (11, 0.002), (12, -0.042), (13, 0.014), (14, 0.04), (15, 0.017), (16, 0.031), (17, -0.059), (18, 0.111), (19, 0.08), (20, -0.043), (21, -0.044), (22, 0.12), (23, 0.005), (24, -0.031), (25, -0.063), (26, -0.007), (27, 0.076), (28, -0.007), (29, 0.125), (30, -0.039), (31, 0.125), (32, -0.094), (33, 0.085), (34, 0.049), (35, 0.118), (36, 0.139), (37, 0.242), (38, -0.035), (39, -0.159), (40, 0.058), (41, -0.049), (42, 0.051), (43, -0.119), (44, 0.03), (45, 0.019), (46, -0.016), (47, -0.246), (48, -0.059), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9410516 <a title="248-lsi-1" href="./nips-2008-Using_matrices_to_model_symbolic_relationship.html">248 nips-2008-Using matrices to model symbolic relationship</a></p>
<p>Author: Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn ﬁrst-order propositions such as (2, 5) ∈ +3 or (Christopher, Penelope) ∈ has wife, and higher-order propositions such as (3, +3) ∈ plus and (+3, −3) ∈ inverse or (has husband, has wife) ∈ higher oppsex. We further demonstrate that the system understands how higher-order propositions are related to ﬁrst-order ones by showing that it can correctly answer questions about ﬁrst-order propositions involving the relations +3 or has wife even though it has not been trained on any ﬁrst-order examples involving these relations. 1</p><p>2 0.61534917 <a title="248-lsi-2" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>Author: Ali Nouri, Michael L. Littman</p><p>Abstract: The essence of exploration is acting to try to decrease uncertainty. We propose a new methodology for representing uncertainty in continuous-state control problems. Our approach, multi-resolution exploration (MRE), uses a hierarchical mapping to identify regions of the state space that would beneﬁt from additional samples. We demonstrate MRE’s broad utility by using it to speed up learning in a prototypical model-based and value-based reinforcement-learning method. Empirical results show that MRE improves upon state-of-the-art exploration approaches. 1</p><p>3 0.57452136 <a title="248-lsi-3" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>Author: Christian Walder, Bernhard Schölkopf</p><p>Abstract: This paper introduces a new approach to constructing meaningful lower dimensional representations of sets of data points. We argue that constraining the mapping between the high and low dimensional spaces to be a diffeomorphism is a natural way of ensuring that pairwise distances are approximately preserved. Accordingly we develop an algorithm which diffeomorphically maps the data near to a lower dimensional subspace and then projects onto that subspace. The problem of solving for the mapping is transformed into one of solving for an Eulerian ﬂow ﬁeld which we compute using ideas from kernel methods. We demonstrate the efﬁcacy of our approach on various real world data sets. 1</p><p>4 0.47522607 <a title="248-lsi-4" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>Author: Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg, Eric P. Xing</p><p>Abstract: In many settings, such as protein interactions and gene regulatory networks, collections of author-recipient email, and social networks, the data consist of pairwise measurements, e.g., presence or absence of links between pairs of objects. Analyzing such data with probabilistic models requires non-standard assumptions, since the usual independence or exchangeability assumptions no longer hold. In this paper, we introduce a class of latent variable models for pairwise measurements: mixed membership stochastic blockmodels. Models in this class combine a global model of dense patches of connectivity (blockmodel) with a local model to instantiate node-speciﬁc variability in the connections (mixed membership). We develop a general variational inference algorithm for fast approximate posterior inference. We demonstrate the advantages of mixed membership stochastic blockmodel with applications to social networks and protein interaction networks. 1</p><p>5 0.44519961 <a title="248-lsi-5" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<p>Author: Christopher G. Lucas, Thomas L. Griffiths, Fei Xu, Christine Fawcett</p><p>Abstract: Young children demonstrate the ability to make inferences about the preferences of other agents based on their choices. However, there exists no overarching account of what children are doing when they learn about preferences or how they use that knowledge. We use a rational model of preference learning, drawing on ideas from economics and computer science, to explain the behavior of children in several recent experiments. Speciﬁcally, we show how a simple econometric model can be extended to capture two- to four-year-olds’ use of statistical information in inferring preferences, and their generalization of these preferences. 1</p><p>6 0.42135811 <a title="248-lsi-6" href="./nips-2008-An_ideal_observer_model_of_infant_object_perception.html">23 nips-2008-An ideal observer model of infant object perception</a></p>
<p>7 0.42049673 <a title="248-lsi-7" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>8 0.39982945 <a title="248-lsi-8" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>9 0.34809718 <a title="248-lsi-9" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>10 0.31530538 <a title="248-lsi-10" href="./nips-2008-A_Massively_Parallel_Digital_Learning_Processor.html">3 nips-2008-A Massively Parallel Digital Learning Processor</a></p>
<p>11 0.30656189 <a title="248-lsi-11" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>12 0.30276418 <a title="248-lsi-12" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>13 0.29660201 <a title="248-lsi-13" href="./nips-2008-Bayesian_Exponential_Family_PCA.html">31 nips-2008-Bayesian Exponential Family PCA</a></p>
<p>14 0.27894485 <a title="248-lsi-14" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>15 0.27612501 <a title="248-lsi-15" href="./nips-2008-Fast_High-dimensional_Kernel_Summations_Using_the_Monte_Carlo_Multipole_Method.html">83 nips-2008-Fast High-dimensional Kernel Summations Using the Monte Carlo Multipole Method</a></p>
<p>16 0.26302209 <a title="248-lsi-16" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>17 0.25674042 <a title="248-lsi-17" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>18 0.25475743 <a title="248-lsi-18" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>19 0.2540153 <a title="248-lsi-19" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>20 0.25391686 <a title="248-lsi-20" href="./nips-2008-Breaking_Audio_CAPTCHAs.html">41 nips-2008-Breaking Audio CAPTCHAs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.037), (7, 0.103), (12, 0.015), (28, 0.122), (57, 0.1), (59, 0.013), (63, 0.03), (74, 0.335), (77, 0.045), (78, 0.017), (83, 0.064)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71388018 <a title="248-lda-1" href="./nips-2008-Using_matrices_to_model_symbolic_relationship.html">248 nips-2008-Using matrices to model symbolic relationship</a></p>
<p>Author: Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn ﬁrst-order propositions such as (2, 5) ∈ +3 or (Christopher, Penelope) ∈ has wife, and higher-order propositions such as (3, +3) ∈ plus and (+3, −3) ∈ inverse or (has husband, has wife) ∈ higher oppsex. We further demonstrate that the system understands how higher-order propositions are related to ﬁrst-order ones by showing that it can correctly answer questions about ﬁrst-order propositions involving the relations +3 or has wife even though it has not been trained on any ﬁrst-order examples involving these relations. 1</p><p>2 0.69816422 <a title="248-lda-2" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>Author: Matt Jones, Sachiko Kinoshita, Michael C. Mozer</p><p>Abstract: In most cognitive and motor tasks, speed-accuracy tradeoffs are observed: Individuals can respond slowly and accurately, or quickly yet be prone to errors. Control mechanisms governing the initiation of behavioral responses are sensitive not only to task instructions and the stimulus being processed, but also to the recent stimulus history. When stimuli can be characterized on an easy-hard dimension (e.g., word frequency in a naming task), items preceded by easy trials are responded to more quickly, and with more errors, than items preceded by hard trials. We propose a rationally motivated mathematical model of this sequential adaptation of control, based on a diffusion model of the decision process in which difﬁculty corresponds to the drift rate for the correct response. The model assumes that responding is based on the posterior distribution over which response is correct, conditioned on the accumulated evidence. We derive this posterior as a function of the drift rate, and show that higher estimates of the drift rate lead to (normatively) faster responding. Trial-by-trial tracking of difﬁculty thus leads to sequential effects in speed and accuracy. Simulations show the model explains a variety of phenomena in human speeded decision making. We argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theories based on elaborate control structures. 1</p><p>3 0.66537982 <a title="248-lda-3" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>Author: Kilian Q. Weinberger, Olivier Chapelle</p><p>Abstract: Applications of multi-class classiﬁcation, such as document categorization, often appear in cost-sensitive settings. Recent work has signiﬁcantly improved the state of the art by moving beyond “ﬂat” classiﬁcation through incorporation of class hierarchies [4]. We present a novel algorithm that goes beyond hierarchical classiﬁcation and estimates the latent semantic space that underlies the class hierarchy. In this space, each class is represented by a prototype and classiﬁcation is done with the simple nearest neighbor rule. The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. We show that our optimization is convex and can be solved efﬁciently for large data sets. Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization. 1</p><p>4 0.51233697 <a title="248-lda-4" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>Author: Indraneel Mukherjee, David M. Blei</p><p>Abstract: Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean ﬁeld variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k − 1) + log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation. 1</p><p>5 0.49841535 <a title="248-lda-5" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>Author: Minh H. Nguyen, Fernando Torre</p><p>Abstract: Kernel Principal Component Analysis (KPCA) is a popular generalization of linear PCA that allows non-linear feature extraction. In KPCA, data in the input space is mapped to higher (usually) dimensional feature space where the data can be linearly modeled. The feature space is typically induced implicitly by a kernel function, and linear PCA in the feature space is performed via the kernel trick. However, due to the implicitness of the feature space, some extensions of PCA such as robust PCA cannot be directly generalized to KPCA. This paper presents a technique to overcome this problem, and extends it to a uniﬁed framework for treating noise, missing data, and outliers in KPCA. Our method is based on a novel cost function to perform inference in KPCA. Extensive experiments, in both synthetic and real data, show that our algorithm outperforms existing methods. 1</p><p>6 0.495116 <a title="248-lda-6" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>7 0.49451944 <a title="248-lda-7" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>8 0.49319977 <a title="248-lda-8" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>9 0.49239314 <a title="248-lda-9" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>10 0.49234903 <a title="248-lda-10" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>11 0.49199986 <a title="248-lda-11" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>12 0.49098504 <a title="248-lda-12" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>13 0.49049184 <a title="248-lda-13" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>14 0.48980999 <a title="248-lda-14" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>15 0.48975435 <a title="248-lda-15" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<p>16 0.48894194 <a title="248-lda-16" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>17 0.48741305 <a title="248-lda-17" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>18 0.48737368 <a title="248-lda-18" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>19 0.48599547 <a title="248-lda-19" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>20 0.48512891 <a title="248-lda-20" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
