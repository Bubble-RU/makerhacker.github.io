<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>153 nips-2008-Nonlinear causal discovery with additive noise models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-153" href="#">nips2008-153</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>153 nips-2008-Nonlinear causal discovery with additive noise models</h1>
<br/><p>Source: <a title="nips-2008-153-pdf" href="http://papers.nips.cc/paper/3548-nonlinear-causal-discovery-with-additive-noise-models.pdf">pdf</a></p><p>Author: Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jan R. Peters, Bernhard Schölkopf</p><p>Abstract: The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models with additive noise are often used because these models are well understood and there are well-known methods to ﬁt them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that the basic linear framework can be generalized to nonlinear models. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identiﬁed. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identiﬁcation power provided by nonlinearities. 1</p><p>Reference: <a title="nips-2008-153-reference" href="../nips2008_reference/nips-2008-Nonlinear_causal_discovery_with_additive_noise_models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Nonlinear causal discovery with additive noise models  Patrik O. [sent-1, score-0.659]
</p><p>2 For continuous-valued data linear acyclic causal models with additive noise are often used because these models are well understood and there are well-known methods to ﬁt them to data. [sent-3, score-0.688]
</p><p>3 In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. [sent-4, score-0.495]
</p><p>4 In this contribution we show that the basic linear framework can be generalized to nonlinear models. [sent-5, score-0.137]
</p><p>5 In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identiﬁed. [sent-6, score-0.422]
</p><p>6 1  Introduction  Causal relationships are fundamental to science because they enable predictions of the consequences of actions [1]. [sent-8, score-0.091]
</p><p>7 While controlled randomized experiments constitute the primary tool for identifying causal relationships, such experiments are in many cases either unethical, too expensive, or technically impossible. [sent-9, score-0.371]
</p><p>8 The development of causal discovery methods to infer causal relationships from uncontrolled data constitutes an important current research topic [1, 2, 3, 4, 5, 6, 7, 8]. [sent-10, score-1.009]
</p><p>9 If the observed data is continuous-valued, methods based on linear causal models (aka structural equation models) are commonly applied [1, 2, 9]. [sent-11, score-0.469]
</p><p>10 This is not necessarily because the true causal relationships are really believed to be linear, but rather it reﬂects the fact that linear models are well understood and easy to work with. [sent-12, score-0.523]
</p><p>11 For continuous variables, the independence tests often assume linear models with additive Gaussian noise [2]. [sent-14, score-0.336]
</p><p>12 Recently, however, it has been shown that for linear models, non-Gaussianity in the data can actually aid in distinguishing the causal directions and allow one to uniquely identify the generating graph under favourable conditions [7]. [sent-15, score-0.486]
</p><p>13 Thus the practical case of non-Gaussian data which long was considered a nuisance turned out to be helpful in the causal discovery setting. [sent-16, score-0.462]
</p><p>14 In this contribution we show that nonlinearities can play a role quite similar to that of nonGaussianity: When causal relationships are nonlinear it typically helps break the symmetry between the observed variables and allows the identiﬁcation of causal directions. [sent-17, score-1.1]
</p><p>15 As Friedman and Nachman have pointed out [10], non-invertible functional relationships between the observed variables can provide clues to the generating causal model. [sent-18, score-0.638]
</p><p>16 However, we show that the phenomenon is much more general; for nonlinear models with additive noise almost any nonlinearities (invertible or not) will typically yield identiﬁable models. [sent-19, score-0.352]
</p><p>17 We describe a practical method for inferring the generating model from a sample of data vectors in Section 4, and show its utility in simulations and on real data (Section 5). [sent-22, score-0.091]
</p><p>18 2  Model deﬁnition  We assume that the observed data has been generated in the following way: Each observed variable xi is associated with a node i in a directed acyclic graph G, and the value of xi is obtained as a function of its parents in G, plus independent additive noise ni , i. [sent-23, score-0.431]
</p><p>19 Our data then consists of a number of vectors x sampled independently, each using G, the same functions fi , and the ni sampled independently from the same densities pni (ni ). [sent-26, score-0.378]
</p><p>20 Note that this model includes the special case when all the fi are linear and all the pni are Gaussian, yielding the standard linear–Gaussian model family [2, 3, 9]. [sent-27, score-0.286]
</p><p>21 When the functions are linear but the densities pni are non-Gaussian we obtain the linear–non-Gaussian models described in [7]. [sent-28, score-0.289]
</p><p>22 The goal of causal discovery is, given the data vectors, to infer as much as possible about the generating mechanism; in particular, we seek to infer the generating graph G. [sent-29, score-0.678]
</p><p>23 In the next section we discuss the prospects of this task in the theoretical case where the joint distribution px (x) of the observed data can be estimated exactly. [sent-30, score-0.303]
</p><p>24 Denoting the two variables x and y, we are considering the generative model y := f (x) + n where x and n are  c 0. [sent-35, score-0.084]
</p><p>25 020  0 0  1  2  x  33  p(x)  xvals[theinds]  e  noise  f(x) ! [sent-44, score-0.093]
</p><p>26 1  0 0  x  1  2  3 3  xvals[theinds]  Figure 1: Identiﬁcation of causal direction based on constancy of conditionals. [sent-65, score-0.371]
</p><p>27 (g) shows an example of a joint density p(x y) generated by a causal model x y with y := f (x) + n where f is nonlinear, the supports of the densities px (x) and pn (n) are compact regions, and the function f is constant on each connected component of the support of px . [sent-67, score-1.281]
</p><p>28 The support of the joint density is now given by the two gray squares. [sent-68, score-0.078]
</p><p>29 Note that the input distribution px , the noise distribution pn and f can in fact be chosen such that the joint density is symmetrical with respect to the two variables, i. [sent-69, score-0.64]
</p><p>30 p(x y) = p(y x), making it obvious that there will also be a valid backward model. [sent-71, score-0.332]
</p><p>31 In panel (a) we plot the joint density p(x, y) of the observed variables, for the linear case of f (x) = x. [sent-73, score-0.213]
</p><p>32 As a trivial consequence of the model, the conditional density p(y | x) has identical shape for all values of x and is simply shifted by the function f (x); this is illustrated in panel (b). [sent-74, score-0.107]
</p><p>33 In general, there is no reason to believe that this relationship would also hold for the conditionals p(x | y) for different values of y but, as is well known, for the linear–Gaussian model this is actually the case, as illustrated in panel (c). [sent-75, score-0.138]
</p><p>34 Panels (d-f) show the corresponding joint and conditional densities for the corresponding model with a nonlinear function f (x) = x + x3 . [sent-76, score-0.273]
</p><p>35 Notice how the conditionals p(x | y) look different for different values of y, indicating that a reverse causal model of the form x := g(y) + n (with y and n statistically ˜ ˜ independent) would not be able to ﬁt the joint density. [sent-77, score-0.642]
</p><p>36 To see the latter, we ﬁrst show that there exist models other than the linear–Gaussian and the independent case which admit both a forward x → y and a backward x ← y model. [sent-79, score-0.551]
</p><p>37 Panel (g) of Figure 1 presents a nonlinear functional model with additive non-Gaussian noise and non-Gaussian input distributions that nevertheless admits a backward model. [sent-80, score-0.677]
</p><p>38 Note that the example of panel (g) in Figure 1 is somewhat artiﬁcial: p has compact support, and x, y are independent inside the connected components of the support. [sent-82, score-0.065]
</p><p>39 Roughly speaking, the nonlinearity of f does not matter since it occurs where p is zero — an artiﬁcal situation which is avoided by the requirement that from now on, we will assume that all probability densities are strictly positive. [sent-83, score-0.146]
</p><p>40 In this case, the following theorem shows that for generic choices of f , px (x), and pn (n), there exists no backward model. [sent-85, score-0.835]
</p><p>41 Theorem 1 Let the joint probability density of x and y be given by p(x, y) = pn (y − f (x))px (x) ,  (2)  where pn , px are probability densities on R. [sent-86, score-0.883]
</p><p>42 If there is a backward model of the same form, i. [sent-87, score-0.368]
</p><p>43 Moreover, if for a ﬁxed pair (f, ν) there exists y ∈ R such that ν (y − f (x))f (x) = 0 for all but a countable set of points x ∈ R, the set of all px for which p has a backward model is contained in a 3-dimensional afﬁne space. [sent-90, score-0.598]
</p><p>44 Loosely speaking, the statement that the differential equation for ξ has a 3-dimensional space of solutions (while a priori, the space of all possible log-marginals ξ is inﬁnite dimensional) amounts to saying that in the generic case, our forward model cannot be inverted. [sent-91, score-0.212]
</p><p>45 A simple corollary is that if both the marginal density px (x) and the noise density pn (y − f (x)) are Gaussian then the existence of a backward model implies linearity of f : Corollary 1 Assume that ν  =ξ  = 0 everywhere. [sent-92, score-1.053]
</p><p>46 Finally, we note that even when f is linear and pn and px are non-Gaussian, although a linear backward model has previously been ruled out [7], there exist special cases where there is a nonlinear backward model with independent additive noise. [sent-95, score-1.451]
</p><p>47 One such case is when f (x) = −x and px and pn are Gumbel distributions: px (x) = exp(−x − exp(−x)) and pn (n) = exp(−n − exp(−n)). [sent-96, score-0.938]
</p><p>48 Then taking py (y) = exp(−y − 2 log(1 + exp(−y))), pn (˜ ) = exp(−2˜ − exp(−˜ )) and n n ˜ n g(y) = log(1 + exp(−y)) one obtains p(x, y) = pn (y − f (x))px (x) = pn (x − g(y))py (y). [sent-97, score-0.785]
</p><p>49 4  Model estimation  Section 3 established for the two-variable case that given knowledge of the exact densities, the true model is (in the generic case) identiﬁable. [sent-100, score-0.07]
</p><p>50 We now consider practical estimation methods which infer the generating graph from sample data. [sent-101, score-0.108]
</p><p>51 Again, we begin by considering the case of two observed scalar variables x and y. [sent-102, score-0.085]
</p><p>52 If they are not, we continue as described in the following manner: We test whether a model y := f (x) + n is consistent ˆ with the data, simply by doing a nonlinear regression of y on x (to get an estimate f of f ), calculating ˆ(x), and testing whether n is independent of x. [sent-104, score-0.236]
</p><p>53 If so, we the corresponding residuals n = y − f ˆ ˆ accept the model y := f (x) + n; if not, we reject it. [sent-105, score-0.432]
</p><p>54 We then similarly test whether the reverse model x := g(y) + n ﬁts the data. [sent-106, score-0.17]
</p><p>55 First, if x and y are deemed mutually independent we infer that there is no causal relationship between the two, and no further analysis is performed. [sent-108, score-0.452]
</p><p>56 On the other hand, if they are dependent but both directional models are accepted we conclude that either model may be correct but we cannot infer it from the data. [sent-109, score-0.209]
</p><p>57 A more positive result is when we are able to reject one of the directions and (tentatively) accept the other. [sent-110, score-0.105]
</p><p>58 Finally, it may be the case that neither direction is consistent with the data, in which case we conclude that the generating mechanism is more complex and cannot be described using this model. [sent-111, score-0.1]
</p><p>59 On the other hand, if none of the independence tests are rejected, Gi is consistent with the data. [sent-114, score-0.151]
</p><p>60 Furthermore, the above algorithm returns all DAGs consistent with the data, including all those for which consistent subgraphs exist. [sent-116, score-0.09]
</p><p>61 The selection of the nonlinear regressor and of the particular independence tests are not constrained. [sent-118, score-0.21]
</p><p>62 Any prior information on the types of functional relationships or the distributions of the noise should optimally be utilized here. [sent-119, score-0.22]
</p><p>63 In our implementation, we perform the regression using Gaussian Processes [12] and the independence tests using kernel methods [13]. [sent-120, score-0.157]
</p><p>64 Note that one must take care to avoid overﬁtting, as overﬁtting may lead one to falsely accept models which should be rejected. [sent-121, score-0.084]
</p><p>65 5  Experiments  To show the ability of our method to ﬁnd the correct model when all the assumptions hold we have applied our implementation to a variety of simulated and real data. [sent-122, score-0.101]
</p><p>66 1 In principle, any regression method can be used; we have veriﬁed that our results do not depend signiﬁcantly on the choice of the regression method by comparing with ν-SVR [15] and with thinplate spline kernel regression [16]. [sent-124, score-0.183]
</p><p>67 For the independence test, we implemented the HSIC [13] with a Gaussian kernel, where we used the gamma distribution as an approximation for the distribution of the HSIC under the null hypothesis of independence in order to calculate the p-value of the test result. [sent-125, score-0.15]
</p><p>68 We simulated data using the model y = x + bx3 + n; the random variables x and n were sampled from a Gaussian distribution and their absolute values were raised to the power q while keeping the original sign. [sent-128, score-0.084]
</p><p>69 q=1  b=0 1 correct reverse  paccept  paccept  1  0  0 0. [sent-131, score-0.357]
</p><p>70 The parameter b controls the strength of the nonlinearity of the function, b = 0 corresponding to the linear case. [sent-134, score-0.082]
</p><p>71 We used 300 (x, y) samples for each trial and used a signiﬁcance level of 2% for rejecting the null hypothesis of independence of residuals and cause. [sent-136, score-0.38]
</p><p>72 By plotting the acceptance probability of the correct and the reverse model as a function of non-Gaussianity we can see that when the distributions are sufﬁciently non-Gaussian the method is able to infer the correct causal direction. [sent-139, score-0.755]
</p><p>73 Then, in panel (b) we similarly demonstrate that we can identify the correct direction for the Gaussian marginal and Gaussian noise model when the functional relationship is sufﬁciently nonlinear. [sent-140, score-0.295]
</p><p>74 We also did experiments for 4 variables w, x, y and z with a diamond-like causal structure. [sent-142, score-0.419]
</p><p>75 We took w ∼ U (−3, 3), x = w2 + nx with nx ∼ U (−1, 1), y = 4 |w|+ny with ny ∼ U (−1, 1), z = 2 sin x+2 sin y+nz with nz ∼ U (−1, 1). [sent-143, score-0.13]
</p><p>76 The simplest DAG that was consistent with the data (with signiﬁcance level 2% for each test) turned out to be precisely the true causal structure. [sent-145, score-0.416]
</p><p>77 The ﬁrst dataset, the “Old Faithful” dataset [17] contains data about the duration of an eruption and the time interval between subsequent eruptions of the Old Faithful geyser in Yellowstone National Park, USA. [sent-150, score-0.384]
</p><p>78 5 for the (forward) model “current duration causes next interval length” and a p-value of 4. [sent-152, score-0.444]
</p><p>79 4 × 10−9 for the (backward) model “next interval length causes current duration”. [sent-153, score-0.396]
</p><p>80 Thus, we accept the model where the time interval between the current and the next eruption is a function of the duration of the current eruption, but reject the reverse model. [sent-154, score-0.67]
</p><p>81 Figure 3 illustrates the data, the forward and backward ﬁt and the residuals for both ﬁts. [sent-156, score-0.765]
</p><p>82 Note that for the forward model, the residuals seem to be independent of the duration, whereas for the backward model, the residuals are clearly dependent on the interval length. [sent-157, score-1.151]
</p><p>83 Time-shifting the data by one time step, we obtain for the (forward) model “current interval length causes next duration” a p-value smaller than 10−15 and for the (backward) model “next duration causes current interval length” we get a p-value of 1. [sent-158, score-0.84]
</p><p>84 Hence, our simple nonlinear model with independent additive noise is not consistent with the data in either direction. [sent-160, score-0.354]
</p><p>85 The second dataset, the “Abalone” dataset from the UCI ML repository [18], contains measurements of the number of rings in the shell of abalone (a group of shellﬁsh), which indicate their age, and the length of the shell. [sent-161, score-0.317]
</p><p>86 The correct model “age causes length” leads to a p-value of 0. [sent-163, score-0.257]
</p><p>87 2 0 0  10  20  0  10  20  20  0 0  30  rings  (b)  20  10  −0. [sent-167, score-0.138]
</p><p>88 5 length  1  Figure 4: Abalone data: (a) forward ﬁt corresponding to “age (rings) causes length”; (b) residuals for forward ﬁt; (c) backward ﬁt corresponding to “length causes age (rings)”; (d) residuals for backward ﬁt. [sent-175, score-1.996]
</p><p>89 0 1000 2000 altitude  0 −1 −2 0  3000  (b)  1000 2000 altitude  2000 1000 0 −10  3000  (c)  400 residuals of (c)  5  3000  1 altitude  10  −5 0  (a)  2 residuals of (a)  temperature  15  0 10 temperature  200 0 −200 −400 −10  20  (d)  0 10 temperature  20  Figure 5: Altitude–temperature data. [sent-176, score-1.593]
</p><p>90 (a) forward ﬁt corresponding to “altitude causes temperature”; (b) residuals for forward ﬁt; (c) backward ﬁt corresponding to “temperature causes altitude”; (d) residuals for backward ﬁt. [sent-177, score-1.842]
</p><p>91 Note that our method favors the correct direction although the assumption of independent additive noise is only approximately correct here; indeed, the variance of the length is dependent on age. [sent-180, score-0.376]
</p><p>92 Finally, we assay the method on a simple example involving two observed variables: The altitude above sea level (in meters) and the local yearly average outdoor temperature in centigrade, for 349 weather stations in Germany, collected over the time period of 1961–1990 [19]. [sent-181, score-0.374]
</p><p>93 The correct model “altitude causes temperature” leads to p = 0. [sent-182, score-0.257]
</p><p>94 017, while “temperature causes altitude” can clearly be rejected (p = 8 × 10−15 ), in agreement with common understanding of causality in this case. [sent-183, score-0.244]
</p><p>95 6  Conclusions  In this paper, we have shown that the linear–non-Gaussian causal discovery framework can be generalized to admit nonlinear functional dependencies as long as the noise on the variables remains additive. [sent-185, score-0.792]
</p><p>96 In this approach nonlinear relationships are in fact helpful rather than a hindrance, as they tend to break the symmetry between the variables and allow the correct causal directions to be identiﬁed. [sent-186, score-0.706]
</p><p>97 Although there exist special cases which admit reverse models we have shown that in the generic case the model is identiﬁable. [sent-187, score-0.281]
</p><p>98 A Proof of Theorem 1 Set π(x, y) := log p(x, y) = ν(y − f (x)) + ξ(x) ,  (5)  and ν := log pn , η := log py . [sent-195, score-0.307]
</p><p>99 Given ﬁxed f and ν, the set of all ξ admitting a backward model is contained in this subspace. [sent-212, score-0.368]
</p><p>100 A linear non-Gaussian acyclic model for a causal discovery. [sent-270, score-0.499]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('causal', 0.371), ('backward', 0.332), ('residuals', 0.291), ('pn', 0.239), ('px', 0.23), ('altitude', 0.207), ('duration', 0.157), ('causes', 0.156), ('forward', 0.142), ('rings', 0.138), ('reverse', 0.134), ('pni', 0.131), ('temperature', 0.13), ('theinds', 0.105), ('nonlinear', 0.104), ('ni', 0.1), ('densities', 0.097), ('interval', 0.095), ('noise', 0.093), ('relationships', 0.091), ('discovery', 0.091), ('dag', 0.089), ('glymour', 0.084), ('eruption', 0.079), ('janzing', 0.079), ('paccept', 0.079), ('length', 0.077), ('age', 0.077), ('additive', 0.076), ('identi', 0.074), ('faithful', 0.071), ('py', 0.068), ('panel', 0.065), ('correct', 0.065), ('causation', 0.063), ('independence', 0.061), ('mpi', 0.061), ('acyclic', 0.059), ('dags', 0.059), ('accept', 0.056), ('abalone', 0.056), ('generating', 0.055), ('old', 0.055), ('germany', 0.055), ('infer', 0.053), ('geyser', 0.053), ('hoyer', 0.053), ('xpa', 0.053), ('xvals', 0.053), ('yvals', 0.053), ('bingen', 0.051), ('nonlinearities', 0.051), ('regression', 0.051), ('fi', 0.05), ('admit', 0.049), ('nonlinearity', 0.049), ('rejected', 0.049), ('reject', 0.049), ('variables', 0.048), ('gaussian', 0.047), ('cybernetics', 0.046), ('gpml', 0.046), ('nz', 0.046), ('shell', 0.046), ('tests', 0.045), ('consistent', 0.045), ('gi', 0.042), ('finland', 0.042), ('nx', 0.042), ('density', 0.042), ('causality', 0.039), ('sch', 0.039), ('corollary', 0.039), ('conditionals', 0.037), ('cooper', 0.037), ('hsic', 0.037), ('observed', 0.037), ('model', 0.036), ('joint', 0.036), ('functional', 0.036), ('helsinki', 0.035), ('sun', 0.034), ('exp', 0.034), ('generic', 0.034), ('linear', 0.033), ('current', 0.032), ('principle', 0.032), ('acceptance', 0.031), ('invertible', 0.031), ('biological', 0.03), ('spline', 0.03), ('concern', 0.029), ('parents', 0.029), ('models', 0.028), ('mutually', 0.028), ('null', 0.028), ('statistically', 0.028), ('distinguishing', 0.027), ('accepted', 0.027), ('symmetry', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="153-tfidf-1" href="./nips-2008-Nonlinear_causal_discovery_with_additive_noise_models.html">153 nips-2008-Nonlinear causal discovery with additive noise models</a></p>
<p>Author: Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jan R. Peters, Bernhard Schölkopf</p><p>Abstract: The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models with additive noise are often used because these models are well understood and there are well-known methods to ﬁt them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that the basic linear framework can be generalized to nonlinear models. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identiﬁed. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identiﬁcation power provided by nonlinearities. 1</p><p>2 0.17304684 <a title="153-tfidf-2" href="./nips-2008-Integrating_Locally_Learned_Causal_Structures_with_Overlapping_Variables.html">108 nips-2008-Integrating Locally Learned Causal Structures with Overlapping Variables</a></p>
<p>Author: David Danks, Clark Glymour, Robert E. Tillman</p><p>Abstract: In many domains, data are distributed among datasets that share only some variables; other recorded variables may occur in only one dataset. While there are asymptotically correct, informative algorithms for discovering causal relationships from a single dataset, even with missing values and hidden variables, there have been no such reliable procedures for distributed data with overlapping variables. We present a novel, asymptotically correct procedure that discovers a minimal equivalence class of causal DAG structures using local independence information from distributed data of this form and evaluate its performance using synthetic and real-world data against causal discovery algorithms for single datasets and applying Structural EM, a heuristic DAG structure learning procedure for data with missing values, to the concatenated data.</p><p>3 0.17035225 <a title="153-tfidf-3" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>Author: Tong Zhang</p><p>Abstract: Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefﬁcients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneﬁcial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory. 1</p><p>4 0.13753437 <a title="153-tfidf-4" href="./nips-2008-Finding_Latent_Causes_in_Causal_Networks%3A_an_Efficient_Approach_Based_on_Markov_Blankets.html">86 nips-2008-Finding Latent Causes in Causal Networks: an Efficient Approach Based on Markov Blankets</a></p>
<p>Author: Jean-philippe Pellet, AndrÄ&sbquo;Ĺ  Elisseeff</p><p>Abstract: Causal structure-discovery techniques usually assume that all causes of more than one variable are observed. This is the so-called causal sufficiency assumption. In practice, it is untestable, and often violated. In this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data. Similar to algorithms such as IC* and FCI, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables. Assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditionalindependence tests w.r.t. the number of variables. We show with experiments that our algorithm is comparable to the state-of-the-art FCI algorithm in accuracy, while being several orders of magnitude faster on large problems. We conclude that MBCS* makes a new range of causally insufficient problems computationally tractable. Keywords: Graphical Models, Structure Learning, Causal Inference. 1 Introduction: Task Definition & Related Work The statistical definition of causality pioneered by Pearl (2000) and Spirtes et al. (2001) has shed new light on how to detect causation. Central in this approach is the automated detection of causeeffect relationships using observational (i.e., non-experimental) data. This can be a necessary task, as in many situations, performing randomized controlled experiments to unveil causation can be impossible, unethical , or too costly. When the analysis deals with variables that cannot be manipulated, being able to learn from data collected by observing the running system is the only possibility. It turns out that learning the full causal structure of a set of variables is, in its most general form , impossible. If we suppose that the</p><p>5 0.11154523 <a title="153-tfidf-5" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>Author: Steven J. Phillips, Miroslav Dudík</p><p>Abstract: We apply robust Bayesian decision theory to improve both generative and discriminative learners under bias in class proportions in labeled training data, when the true class proportions are unknown. For the generative case, we derive an entropybased weighting that maximizes expected log likelihood under the worst-case true class proportions. For the discriminative case, we derive a multinomial logistic model that minimizes worst-case conditional log loss. We apply our theory to the modeling of species geographic distributions from presence data, an extreme case of labeling bias since there is no absence data. On a benchmark dataset, we ﬁnd that entropy-based weighting offers an improvement over constant estimates of class proportions, consistently reducing log loss on unbiased test data. 1</p><p>6 0.097792819 <a title="153-tfidf-6" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>7 0.071139589 <a title="153-tfidf-7" href="./nips-2008-Offline_Handwriting_Recognition_with_Multidimensional_Recurrent_Neural_Networks.html">158 nips-2008-Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</a></p>
<p>8 0.068471678 <a title="153-tfidf-8" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>9 0.062259793 <a title="153-tfidf-9" href="./nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</a></p>
<p>10 0.061641362 <a title="153-tfidf-10" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>11 0.060346343 <a title="153-tfidf-11" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>12 0.057539929 <a title="153-tfidf-12" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>13 0.054714307 <a title="153-tfidf-13" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>14 0.054090839 <a title="153-tfidf-14" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>15 0.05330053 <a title="153-tfidf-15" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>16 0.052341942 <a title="153-tfidf-16" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>17 0.052236777 <a title="153-tfidf-17" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>18 0.051570799 <a title="153-tfidf-18" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>19 0.051311363 <a title="153-tfidf-19" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>20 0.050658241 <a title="153-tfidf-20" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.168), (1, 0.003), (2, 0.014), (3, 0.052), (4, 0.069), (5, -0.056), (6, -0.018), (7, 0.085), (8, 0.051), (9, 0.03), (10, -0.003), (11, 0.053), (12, 0.002), (13, -0.158), (14, 0.002), (15, -0.048), (16, 0.063), (17, -0.029), (18, -0.103), (19, 0.017), (20, -0.009), (21, 0.209), (22, 0.122), (23, -0.081), (24, 0.323), (25, -0.173), (26, -0.03), (27, -0.049), (28, -0.059), (29, 0.075), (30, 0.025), (31, -0.094), (32, 0.068), (33, 0.042), (34, 0.009), (35, 0.001), (36, 0.024), (37, -0.075), (38, 0.055), (39, 0.001), (40, 0.106), (41, -0.069), (42, 0.024), (43, 0.045), (44, -0.07), (45, -0.002), (46, -0.024), (47, -0.146), (48, 0.024), (49, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93288362 <a title="153-lsi-1" href="./nips-2008-Nonlinear_causal_discovery_with_additive_noise_models.html">153 nips-2008-Nonlinear causal discovery with additive noise models</a></p>
<p>Author: Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jan R. Peters, Bernhard Schölkopf</p><p>Abstract: The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models with additive noise are often used because these models are well understood and there are well-known methods to ﬁt them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that the basic linear framework can be generalized to nonlinear models. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identiﬁed. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identiﬁcation power provided by nonlinearities. 1</p><p>2 0.7738564 <a title="153-lsi-2" href="./nips-2008-Finding_Latent_Causes_in_Causal_Networks%3A_an_Efficient_Approach_Based_on_Markov_Blankets.html">86 nips-2008-Finding Latent Causes in Causal Networks: an Efficient Approach Based on Markov Blankets</a></p>
<p>Author: Jean-philippe Pellet, AndrÄ&sbquo;Ĺ  Elisseeff</p><p>Abstract: Causal structure-discovery techniques usually assume that all causes of more than one variable are observed. This is the so-called causal sufficiency assumption. In practice, it is untestable, and often violated. In this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data. Similar to algorithms such as IC* and FCI, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables. Assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditionalindependence tests w.r.t. the number of variables. We show with experiments that our algorithm is comparable to the state-of-the-art FCI algorithm in accuracy, while being several orders of magnitude faster on large problems. We conclude that MBCS* makes a new range of causally insufficient problems computationally tractable. Keywords: Graphical Models, Structure Learning, Causal Inference. 1 Introduction: Task Definition & Related Work The statistical definition of causality pioneered by Pearl (2000) and Spirtes et al. (2001) has shed new light on how to detect causation. Central in this approach is the automated detection of causeeffect relationships using observational (i.e., non-experimental) data. This can be a necessary task, as in many situations, performing randomized controlled experiments to unveil causation can be impossible, unethical , or too costly. When the analysis deals with variables that cannot be manipulated, being able to learn from data collected by observing the running system is the only possibility. It turns out that learning the full causal structure of a set of variables is, in its most general form , impossible. If we suppose that the</p><p>3 0.73113048 <a title="153-lsi-3" href="./nips-2008-Integrating_Locally_Learned_Causal_Structures_with_Overlapping_Variables.html">108 nips-2008-Integrating Locally Learned Causal Structures with Overlapping Variables</a></p>
<p>Author: David Danks, Clark Glymour, Robert E. Tillman</p><p>Abstract: In many domains, data are distributed among datasets that share only some variables; other recorded variables may occur in only one dataset. While there are asymptotically correct, informative algorithms for discovering causal relationships from a single dataset, even with missing values and hidden variables, there have been no such reliable procedures for distributed data with overlapping variables. We present a novel, asymptotically correct procedure that discovers a minimal equivalence class of causal DAG structures using local independence information from distributed data of this form and evaluate its performance using synthetic and real-world data against causal discovery algorithms for single datasets and applying Structural EM, a heuristic DAG structure learning procedure for data with missing values, to the concatenated data.</p><p>4 0.46138388 <a title="153-lsi-4" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>Author: Rama Natarajan, Iain Murray, Ladan Shams, Richard S. Zemel</p><p>Abstract: We explore a recently proposed mixture model approach to understanding interactions between conﬂicting sensory cues. Alternative model formulations, differing in their sensory noise models and inference methods, are compared based on their ﬁt to experimental data. Heavy-tailed sensory likelihoods yield a better description of the subjects’ response behavior than standard Gaussian noise models. We study the underlying cause for this result, and then present several testable predictions of these models. 1</p><p>5 0.45454961 <a title="153-lsi-5" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>Author: Tong Zhang</p><p>Abstract: Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefﬁcients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneﬁcial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory. 1</p><p>6 0.43349379 <a title="153-lsi-6" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>7 0.35970467 <a title="153-lsi-7" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>8 0.33867201 <a title="153-lsi-8" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>9 0.32911903 <a title="153-lsi-9" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>10 0.32897654 <a title="153-lsi-10" href="./nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</a></p>
<p>11 0.31526724 <a title="153-lsi-11" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>12 0.30878991 <a title="153-lsi-12" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>13 0.30157128 <a title="153-lsi-13" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>14 0.30147943 <a title="153-lsi-14" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>15 0.27873653 <a title="153-lsi-15" href="./nips-2008-Breaking_Audio_CAPTCHAs.html">41 nips-2008-Breaking Audio CAPTCHAs</a></p>
<p>16 0.26904383 <a title="153-lsi-16" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>17 0.26340565 <a title="153-lsi-17" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>18 0.26203462 <a title="153-lsi-18" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>19 0.26131138 <a title="153-lsi-19" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<p>20 0.2545929 <a title="153-lsi-20" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.067), (7, 0.081), (12, 0.021), (15, 0.02), (21, 0.309), (28, 0.159), (57, 0.075), (59, 0.02), (63, 0.02), (64, 0.02), (71, 0.02), (77, 0.059), (83, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.86884254 <a title="153-lda-1" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>Author: Rama Natarajan, Iain Murray, Ladan Shams, Richard S. Zemel</p><p>Abstract: We explore a recently proposed mixture model approach to understanding interactions between conﬂicting sensory cues. Alternative model formulations, differing in their sensory noise models and inference methods, are compared based on their ﬁt to experimental data. Heavy-tailed sensory likelihoods yield a better description of the subjects’ response behavior than standard Gaussian noise models. We study the underlying cause for this result, and then present several testable predictions of these models. 1</p><p>2 0.85534942 <a title="153-lda-2" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>Author: Sanmay Das, Malik Magdon-Ismail</p><p>Abstract: We study the proﬁt-maximization problem of a monopolistic market-maker who sets two-sided prices in an asset market. The sequential decision problem is hard to solve because the state space is a function. We demonstrate that the belief state is well approximated by a Gaussian distribution. We prove a key monotonicity property of the Gaussian state update which makes the problem tractable, yielding the ﬁrst optimal sequential market-making algorithm in an established model. The algorithm leads to a surprising insight: an optimal monopolist can provide more liquidity than perfectly competitive market-makers in periods of extreme uncertainty, because a monopolist is willing to absorb initial losses in order to learn a new valuation rapidly so she can extract higher proﬁts later. 1</p><p>same-paper 3 0.77566725 <a title="153-lda-3" href="./nips-2008-Nonlinear_causal_discovery_with_additive_noise_models.html">153 nips-2008-Nonlinear causal discovery with additive noise models</a></p>
<p>Author: Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jan R. Peters, Bernhard Schölkopf</p><p>Abstract: The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models with additive noise are often used because these models are well understood and there are well-known methods to ﬁt them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that the basic linear framework can be generalized to nonlinear models. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identiﬁed. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identiﬁcation power provided by nonlinearities. 1</p><p>4 0.73050272 <a title="153-lda-4" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>Author: Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg, Eric P. Xing</p><p>Abstract: In many settings, such as protein interactions and gene regulatory networks, collections of author-recipient email, and social networks, the data consist of pairwise measurements, e.g., presence or absence of links between pairs of objects. Analyzing such data with probabilistic models requires non-standard assumptions, since the usual independence or exchangeability assumptions no longer hold. In this paper, we introduce a class of latent variable models for pairwise measurements: mixed membership stochastic blockmodels. Models in this class combine a global model of dense patches of connectivity (blockmodel) with a local model to instantiate node-speciﬁc variability in the connections (mixed membership). We develop a general variational inference algorithm for fast approximate posterior inference. We demonstrate the advantages of mixed membership stochastic blockmodel with applications to social networks and protein interaction networks. 1</p><p>5 0.60084414 <a title="153-lda-5" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>Author: Nikos Komodakis, Nikos Paragios, Georgios Tziritas</p><p>Abstract: A novel center-based clustering algorithm is proposed in this paper. We ﬁrst formulate clustering as an NP-hard linear integer program and we then use linear programming and the duality theory to derive the solution of this optimization problem. This leads to an efﬁcient and very general algorithm, which works in the dual domain, and can cluster data based on an arbitrary set of distances. Despite its generality, it is independent of initialization (unlike EM-like methods such as K-means), has guaranteed convergence, can automatically determine the number of clusters, and can also provide online optimality bounds about the quality of the estimated clustering solutions. To deal with the most critical issue in a centerbased clustering algorithm (selection of cluster centers), we also introduce the notion of stability of a cluster center, which is a well deﬁned LP-based quantity that plays a key role to our algorithm’s success. Furthermore, we also introduce, what we call, the margins (another key ingredient in our algorithm), which can be roughly thought of as dual counterparts to stabilities and allow us to obtain computationally efﬁcient approximations to the latter. Promising experimental results demonstrate the potentials of our method.</p><p>6 0.57605267 <a title="153-lda-6" href="./nips-2008-Finding_Latent_Causes_in_Causal_Networks%3A_an_Efficient_Approach_Based_on_Markov_Blankets.html">86 nips-2008-Finding Latent Causes in Causal Networks: an Efficient Approach Based on Markov Blankets</a></p>
<p>7 0.57338834 <a title="153-lda-7" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>8 0.57245684 <a title="153-lda-8" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>9 0.57086629 <a title="153-lda-9" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>10 0.57030141 <a title="153-lda-10" href="./nips-2008-Integrating_Locally_Learned_Causal_Structures_with_Overlapping_Variables.html">108 nips-2008-Integrating Locally Learned Causal Structures with Overlapping Variables</a></p>
<p>11 0.57029724 <a title="153-lda-11" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>12 0.5679872 <a title="153-lda-12" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>13 0.56688172 <a title="153-lda-13" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>14 0.56597912 <a title="153-lda-14" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>15 0.56447524 <a title="153-lda-15" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>16 0.56440032 <a title="153-lda-16" href="./nips-2008-Offline_Handwriting_Recognition_with_Multidimensional_Recurrent_Neural_Networks.html">158 nips-2008-Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</a></p>
<p>17 0.56439567 <a title="153-lda-17" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>18 0.56404018 <a title="153-lda-18" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>19 0.56396067 <a title="153-lda-19" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>20 0.56350172 <a title="153-lda-20" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
