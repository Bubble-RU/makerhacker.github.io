<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>179 nips-2008-Phase transitions for high-dimensional joint support recovery</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-179" href="#">nips2008-179</a> knowledge-graph by maker-knowledge-mining</p><h1>179 nips-2008-Phase transitions for high-dimensional joint support recovery</h1>
<br/><p>Source: <a title="nips-2008-179-pdf" href="http://papers.nips.cc/paper/3392-phase-transitions-for-high-dimensional-joint-support-recovery.pdf">pdf</a></p><p>Author: Sahand Negahban, Martin J. Wainwright</p><p>Abstract: Given a collection of r ≥ 2 linear regression problems in p dimensions, suppose that the regression coefﬁcients share partially common supports. This set-up suggests the use of 1 / ∞ -regularized regression for joint estimation of the p × r matrix of regression coefﬁcients. We analyze the high-dimensional scaling of 1 / ∞ -regularized quadratic programming, considering both consistency rates in ∞ -norm, and also how the minimal sample size n required for performing variable selection grows as a function of the model dimension, sparsity, and overlap between the supports. We begin by establishing bounds on the ∞ error as well sufﬁcient conditions for exact variable selection for ﬁxed design matrices, as well as designs drawn randomly from general Gaussian matrices. These results show that the high-dimensional scaling of 1 / ∞ -regularization is qualitatively similar to that of ordinary 1 -regularization. Our second set of results applies to design matrices drawn from standard Gaussian ensembles, for which we provide a sharp set of necessary and sufﬁcient conditions: the 1 / ∞ -regularized method undergoes a phase transition characterized by the rescaled sample size θ1,∞ (n, p, s, α) = n/{(4 − 3α)s log(p − (2 − α) s)}. More precisely, for any δ > 0, the probability of successfully recovering both supports converges to 1 for scalings such that θ1,∞ ≥ 1 + δ, and converges to 0 for scalings for which θ1,∞ ≤ 1 − δ. An implication of this threshold is that use of 1,∞ -regularization yields improved statistical efﬁciency if the overlap parameter is large enough (α > 2/3), but performs worse than a naive Lasso-based approach for moderate to small overlap (α < 2/3). We illustrate the close agreement between these theoretical predictions, and the actual behavior in simulations. 1</p><p>Reference: <a title="nips-2008-179-reference" href="../nips2008_reference/nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('recovery', 0.345), ('bgap', 0.315), ('block', 0.239), ('lasso', 0.239), ('las', 0.223), ('regress', 0.22), ('overlap', 0.167), ('sdu', 0.157), ('rp', 0.153), ('stack', 0.144), ('ordin', 0.137), ('support', 0.136), ('recov', 0.122), ('bmin', 0.118), ('program', 0.116), ('design', 0.113), ('berkeley', 0.11), ('resc', 0.106), ('spars', 0.105), ('gap', 0.104)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="179-tfidf-1" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>2 0.37694228 <a title="179-tfidf-2" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>3 0.30177158 <a title="179-tfidf-3" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>4 0.24168593 <a title="179-tfidf-4" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>5 0.2053123 <a title="179-tfidf-5" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>6 0.17981723 <a title="179-tfidf-6" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>7 0.16928758 <a title="179-tfidf-7" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>8 0.14350271 <a title="179-tfidf-8" href="./nips-2008-Resolution_Limits_of_Sparse_Coding_in_High_Dimensions.html">198 nips-2008-Resolution Limits of Sparse Coding in High Dimensions</a></p>
<p>9 0.13133454 <a title="179-tfidf-9" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>10 0.12759016 <a title="179-tfidf-10" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>11 0.099494569 <a title="179-tfidf-11" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>12 0.092645876 <a title="179-tfidf-12" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>13 0.090062365 <a title="179-tfidf-13" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>14 0.08690764 <a title="179-tfidf-14" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>15 0.085699141 <a title="179-tfidf-15" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>16 0.084870569 <a title="179-tfidf-16" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>17 0.083959371 <a title="179-tfidf-17" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>18 0.083657309 <a title="179-tfidf-18" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>19 0.083096936 <a title="179-tfidf-19" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>20 0.078831859 <a title="179-tfidf-20" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.284), (1, 0.067), (2, 0.08), (3, -0.18), (4, 0.112), (5, 0.126), (6, 0.107), (7, 0.238), (8, -0.133), (9, -0.14), (10, -0.02), (11, -0.268), (12, -0.074), (13, 0.025), (14, -0.059), (15, 0.025), (16, -0.054), (17, 0.119), (18, 0.057), (19, 0.045), (20, 0.005), (21, -0.088), (22, -0.034), (23, -0.032), (24, 0.053), (25, -0.051), (26, -0.023), (27, 0.133), (28, 0.053), (29, 0.067), (30, 0.019), (31, -0.055), (32, -0.121), (33, -0.04), (34, 0.005), (35, -0.028), (36, 0.109), (37, -0.007), (38, 0.019), (39, -0.003), (40, -0.006), (41, 0.029), (42, 0.077), (43, -0.145), (44, -0.017), (45, -0.034), (46, -0.045), (47, 0.002), (48, 0.014), (49, -0.008)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95470512 <a title="179-lsi-1" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>2 0.95191604 <a title="179-lsi-2" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>3 0.79922706 <a title="179-lsi-3" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>4 0.74458212 <a title="179-lsi-4" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>5 0.65648884 <a title="179-lsi-5" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>6 0.63678157 <a title="179-lsi-6" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>7 0.58151609 <a title="179-lsi-7" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>8 0.56626987 <a title="179-lsi-8" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>9 0.54842836 <a title="179-lsi-9" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>10 0.53820771 <a title="179-lsi-10" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>11 0.50461292 <a title="179-lsi-11" href="./nips-2008-Resolution_Limits_of_Sparse_Coding_in_High_Dimensions.html">198 nips-2008-Resolution Limits of Sparse Coding in High Dimensions</a></p>
<p>12 0.47725713 <a title="179-lsi-12" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>13 0.47501174 <a title="179-lsi-13" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>14 0.46952969 <a title="179-lsi-14" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>15 0.45681307 <a title="179-lsi-15" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>16 0.43547872 <a title="179-lsi-16" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>17 0.42939419 <a title="179-lsi-17" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>18 0.42555684 <a title="179-lsi-18" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>19 0.41505522 <a title="179-lsi-19" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>20 0.41412488 <a title="179-lsi-20" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(30, 0.067), (38, 0.017), (40, 0.157), (45, 0.13), (60, 0.024), (63, 0.299), (64, 0.114), (71, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94293398 <a title="179-lda-1" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>2 0.91725463 <a title="179-lda-2" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>3 0.8988927 <a title="179-lda-3" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>4 0.89279759 <a title="179-lda-4" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>5 0.89209133 <a title="179-lda-5" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>6 0.88108331 <a title="179-lda-6" href="./nips-2008-Resolution_Limits_of_Sparse_Coding_in_High_Dimensions.html">198 nips-2008-Resolution Limits of Sparse Coding in High Dimensions</a></p>
<p>7 0.87054688 <a title="179-lda-7" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>8 0.86701661 <a title="179-lda-8" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>9 0.866925 <a title="179-lda-9" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>10 0.8651585 <a title="179-lda-10" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>11 0.8576045 <a title="179-lda-11" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>12 0.85158354 <a title="179-lda-12" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>13 0.84911919 <a title="179-lda-13" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>14 0.84534037 <a title="179-lda-14" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>15 0.84503192 <a title="179-lda-15" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>16 0.84404582 <a title="179-lda-16" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>17 0.84291232 <a title="179-lda-17" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>18 0.8378427 <a title="179-lda-18" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>19 0.83645165 <a title="179-lda-19" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>20 0.83574557 <a title="179-lda-20" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
