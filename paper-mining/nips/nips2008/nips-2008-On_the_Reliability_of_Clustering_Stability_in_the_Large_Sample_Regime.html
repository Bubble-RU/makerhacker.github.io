<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-165" href="#">nips2008-165</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</h1>
<br/><p>Source: <a title="nips-2008-165-pdf" href="http://papers.nips.cc/paper/3438-on-the-reliability-of-clustering-stability-in-the-large-sample-regime.pdf">pdf</a></p><p>Author: Ohad Shamir, Naftali Tishby</p><p>Abstract: unkown-abstract</p><p>Reference: <a title="nips-2008-165-reference" href="../nips2008_reference/nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Formally, deﬁne the random variable dm (Ak (S1 ), Ak (S2 )) := D  √  mdD (Ak (S1 ), Ak (S2 )) =  √  m Pr  x∼D  argmaxfθ,i (x) = argmaxfθ′ ,i (x) , ˆ ˆ i  i  where θ, θ′ ∈ Θ are the solutions returned by Ak (S1 ), Ak (S2 ), and S1 , S2 are random samples, each of size m, drawn i. [sent-7, score-0.421]
</p><p>2 The scaling by the square root of the sample size will allow us to analyze the non-trivial asymptotic behavior of these distance measures, which without scaling simply converge to zero in probability as m → ∞. [sent-10, score-0.122]
</p><p>3 We will also need to deﬁne the following variant of dm (Ak (S1 ), Ak (S2 )), where we restrict ourD selves to the mass in some subset of Rn . [sent-13, score-0.429]
</p><p>4 Formally, we deﬁne the restricted distance between two clusterings, with respect to a set B ∈ Rn , as √ dm (Ak (S1 ), Ak (S2 ), B) := m Pr argmaxfθ,i (x) = argmaxfθ′ ,i (x) ∧ x ∈ B . [sent-14, score-0.385]
</p><p>5 (1) ˆ ˆ D x∼D  i  i  In particular, dm (Ak (S1 ), Ak (S2 ), Br/√m (∪i,j Fθ0 ,i,j )) refers to the mass which switches clusters, D √ and is also inside an r/ m-neighborhood of the limit cluster boundaries (where the boundaries are deﬁned with respect to fθ0 (·)). [sent-15, score-0.889]
</p><p>6 Once again, when S1 , S2 are random samples, we can think of it as a random variable with respect to drawing and clustering S1 , S2 . [sent-16, score-0.139]
</p><p>7 Consistency Condition: θ converges in probability (over drawing and clustering a sample of size m, m → ∞) to some θ 0 ∈ Θ. [sent-19, score-0.223]
</p><p>8 Central Limit Condition: m(θ − θ 0 ) converges in distribution to a multivariate zero mean Gaussian random variable Z. [sent-25, score-0.098]
</p><p>9 Both fθ0 (x) and (∂/∂θ)fθ0 (x) are twice differentiable with respect to any x ∈ X , with a uniformly bounded second derivative. [sent-28, score-0.108]
</p><p>10 (d) Minimal Parametric Stability: It holds for some δ > 0 that  ` ´ Pr dm (Ak (S1 ), Ak (S2 )) = dm (Ak (S1 ), Ak (S2 ), Br/√m (∪i,j Fθ 0 ,i,j )) = O(r−3−δ ) + o(1), D D  where o(1) → 0 as m → ∞. [sent-32, score-0.77]
</p><p>11 Namely, the mass of D which switches between clusters is with high probability inside thin strips around the limit cluster boundaries, and this high probability increases at least polynomially as the width of the strips increase (see below for a further discussion of this). [sent-33, score-0.749]
</p><p>12 The regularity assumptions are relatively mild, and can usually be inferred based on the consistency and central limit conditions, as well as the the speciﬁc clustering framework that we are considering. [sent-34, score-0.208]
</p><p>13 For example, condition 3c and the assumptions on Fθ0 ,i,j in condition 3b are fulﬁlled in a clustering framework where the clusters are separated by hyperplanes. [sent-35, score-0.192]
</p><p>14 As to condition 3d, suppose our ˆ clustering framework is such that the cluster boundaries depend on θ in a smooth manner. [sent-36, score-0.328]
</p><p>15 Then the ˆ with variance O(1/m), and the compactness of X , will generally imply asymptotic normality of θ, that the cluster boundaries √ obtained from clustering a sample are contained with high probability inside strips of width O(1/ m) around the limit cluster boundaries. [sent-37, score-0.897]
</p><p>16 More speciﬁcally, the asymp√ totic probability of this happening for strips of width r/ m will be exponentially high in r, due ˆ to the asymptotic normality of θ. [sent-38, score-0.298]
</p><p>17 As a result, the mass which switches between clusters, when we compare two independent clusterings, will be in those strips with probability exponentially high in r. [sent-39, score-0.263]
</p><p>18 Therefore, condition 3d will hold by a large margin, since only polynomially high probability is required there. [sent-40, score-0.088]
</p><p>19 Throughout the proofs, we will sometimes use the stochastic order notation Op (·) and op (·) (cf. [sent-43, score-0.093]
</p><p>20 We write Xm = op (Ym ) to mean that Pr( Xm ≥ ǫ Ym ) → 0 for each ǫ > 0. [sent-47, score-0.093]
</p><p>21 For example, Xm = op (1) means that Xm → 0 in probability. [sent-49, score-0.093]
</p><p>22 When we write for example Xm = Ym + op (1), we mean that Xm − Ym = op (1). [sent-50, score-0.186]
</p><p>23 C  Proof of Proposition 1  ˆ By condition 3a, fθ (x) has a ﬁrst order Taylor expansion with respect to any θ close enough to θ 0 , with a remainder term uniformly bounded for any x: fθ (x) = fθ0 (x) + ˆ  ∂ fθ (x) ∂θ 0 2  ⊤  ˆ ˆ (θ − θ 0 ) + o( θ − θ 0 ). [sent-51, score-0.164]
</p><p>24 (2)  By the asymptotic normality assumption, Therefore, we get from Eq. [sent-52, score-0.139]
</p><p>25 ⊤ √ ∂ ˆ fθ0 (x) ( m(θ − θ 0 )) + op (1), (3) ∂θ where the remainder term op (1) does not depend on x. [sent-54, score-0.216]
</p><p>26 By regularity condition 3a and compactness of X , (∂/∂θ)fθ0 (·) is a uniformly bounded vector-valued function from X to the Euclidean space ˆ ˆ in which Θ resides. [sent-55, score-0.167]
</p><p>27 As a result, the mapping θ → ((∂/∂θ)fθ0 (·))⊤ θ is a mapping from Θ, with the metric induced by the Euclidean space in which it resides, to the space of all uniformly bounded Rk -valued functions on X . [sent-56, score-0.119]
</p><p>28 We also know that m(θ−θ 0 ) converges in distribution to a multivariate Gaussian random variable Z. [sent-59, score-0.098]
</p><p>29 √ The purpose of the stability estimator ηm,q , scaled by m, boils down to trying to assess the ˆk ”expected” value of the random variable dm (Ak (S1 ), Ak (S2 )): we estimate q instantiations of D dm (Ak (S1 ), Ak (S2 )), and take their average. [sent-67, score-0.832]
</p><p>30 The reason is that the convergence tools at our disposal deals with convergence in distribution of random variables, but convergence in distribution does not necessarily imply convergence of expectations. [sent-71, score-0.132]
</p><p>31 In other words, we can try and analyze the asymptotic distribution of dm (Ak (S1 ), Ak (S2 )), but the expected value of this asymptotic distribution is not necessarily the D same as limm→∞ Edm (Ak (S1 ), Ak (S2 )). [sent-72, score-0.549]
</p><p>32 D Here is the basic idea: instead of analyzing the asymptotic expectation of dm (Ak (S1 ), Ak (S2 )), we D analyze the asymptotic expectation of a different random variable, dm (Ak (S1 ), Ak (S2 ), B), which D was formally deﬁned in Eq. [sent-74, score-0.952]
</p><p>33 Informally, recall that dm (Ak (S1 ), Ak (S2 )) is the mass of the unD derlying distribution D which switches between clusters, when we draw and cluster two indepenm dent samples of size m. [sent-76, score-0.63]
</p><p>34 A, we will pick B to be dm (Ak (S1 ), Ak (S2 ), Br/√m (∪i,j Fθ0 ,i,j )) for some r > 0. [sent-79, score-0.385]
</p><p>35 In words, this constitutes strips of width D √ r/ m around the limit cluster boundaries. [sent-80, score-0.344]
</p><p>36 Writing the above expression for B as Br/√m , we have that if r be large enough, then dm (Ak (S1 ), Ak (S2 ), Br/√m ) is equal to dm (Ak (S1 ), Ak (S2 )) with D D very high probability over drawing and clustering a pair of samples, for any large enough sample size m. [sent-81, score-0.96]
</p><p>37 Basically, this is because the ﬂuctuations of the cluster boundaries, based on drawing and clustering a random sample of size m, cannot be too large, and therefore the mass which switches clusters is concentrated around the limit cluster boundaries, if m is large enough. [sent-82, score-0.639]
</p><p>38 The advantage of the ’surrogate’ random variable dm (Ak (S1 ), Ak (S2 ), Br/√m ) is that it is bounded D for any ﬁnite r, unlike dm (Ak (S1 ), Ak (S2 )). [sent-83, score-0.843]
</p><p>39 With bounded random variables, convergence in D distribution does imply convergence of expectations, and as a result we are able to calculate limm→∞ Edm (Ak (S1 ), Ak (S2 ), Br/√m ) explicitly. [sent-84, score-0.143]
</p><p>40 Our goal is to perform this calculation without going through an intermediate step of explicitly characterizing the distribution of dm (Ak (S1 ), Ak (S2 ), Br/√m ). [sent-98, score-0.385]
</p><p>41 This is because the distribution might be highly dependent on the speD ciﬁc clustering framework, and thus it is unsuitable for the level of generality which we aim at (in other words, we do not wish to assume a speciﬁc clustering framework). [sent-99, score-0.162]
</p><p>42 The idea is as follows: recall that dm (Ak (S1 ), Ak (S2 ), Br/√m ) is the mass of the underlying distribution D, inside strips of √ D width r/ m around the limit cluster boundaries, which switches clusters when we draw and cluster two independent samples of size m. [sent-100, score-1.058]
</p><p>43 Then we can write dm (Ak (S1 ), Ak (S2 ), Br/√m ), by Fubini’s theorem, as: D Edm (Ak (S1 ), Ak (S2 ), Br/√m ) = D  √ mE  √ m Pr(Ax )p(x)dx. [sent-102, score-0.385]
</p><p>44 5, which considers what happens to the integral above inside a single strip near one of the limit cluster boundaries Fθ0 ,i,j . [sent-104, score-0.37]
</p><p>45 5 can be combined to give the asymptotic value of Eq. [sent-106, score-0.082]
</p><p>46 The bottom line is that we can simply sum the contributions from each strip, because the intersection of these different strips is asymptotically negligible. [sent-108, score-0.127]
</p><p>47 (5), and transforms it to an expression composed of a constant value, and a remainder term which converges to 0 as m → ∞. [sent-115, score-0.131]
</p><p>48 The ﬁrst step is rewriting everything using the asymptotic Gaussian distribution of the cluster association function fθ (x) for each x, plus remainder terms (Eq. [sent-117, score-0.281]
</p><p>49 Since we are ˆ integrating over x, special care is given to show that the convergence to the asymptotic distribution is uniform for all x in the domain of integration. [sent-119, score-0.123]
</p><p>50 The second step is to rewrite the integral (which is over a strip around the cluster boundary) as a double integral along the cluster boundary itself, and along a normal segment at any point on the cluster boundary (Eq. [sent-120, score-0.584]
</p><p>51 Since the strips become arbitrarily small as m → ∞, the third step consists of rewriting everything in terms of a Taylor expansion around each point on the cluster boundary (Eq. [sent-122, score-0.357]
</p><p>52 1 below), characterizing the asymptotic expected value of dm (Ak (S1 ), Ak (S2 ), Br/√m (∪i,j Fθ0 ,i,j )). [sent-129, score-0.467]
</p><p>53 ˆ ˆ ˆ ˆ This is simply by deﬁnition of Ax : the probability that under one clustering, based on a random sample, x is more associated with cluster i, and that under a second clustering, based on another independent random sample, x is more associated with cluster j. [sent-135, score-0.319]
</p><p>54 However, notice that any point x in Br/√m (Fθ0 ,i,j ) (for some i, j) is much closer to Fθ0 ,i,j than to any other cluster boundary. [sent-137, score-0.15]
</p><p>55 Therefore, if x does switch clusters, then it is highly likely to switch between cluster i and cluster j. [sent-139, score-0.262]
</p><p>56 Formally, by regularity condition 3d (which ensure that the cluster boundaries experience √ at most O(1/ m) ﬂuctuations), we have that uniformly for any x,  Pr(Ax ) = 2 Pr(fθ,i (x) − fθ,j (x) < 0) Pr(fθ,i (x) − fθ,j > 0) + o(1), ˆ ˆ ˆ ˆ where o(1) converges to 0 as m → ∞. [sent-140, score-0.389]
</p><p>57 Therefore, the probability in the lemma above can be written as Pr  1 q  q  i=1  Zi − E[Zi ] ≥ ν  ,  where Zi are bounded in [0, r] with probability 1. [sent-163, score-0.13]
</p><p>58 Let Am be the event that for all subsample pairs {Si , Si }, r 1 2 1 2 dm (Ak (Si ), Ak (Si ), Br/√m(∪i,j Fθ0 ,i,j ) ) = dm (Ak (Si ), Ak (Si )). [sent-166, score-0.827]
</p><p>59 Namely, this is the event that for D D all subsample pairs, the mass which switches clusters when we compare the two resulting clusterings √ is always in an r/ m-neighborhood of the limit cluster boundaries. [sent-167, score-0.434]
</p><p>60 Since p(·) is bounded, we have that dm (r) is deterministically bounded by O(r), with implicit D constants depending only on D and θ 0 . [sent-168, score-0.44]
</p><p>61 As a result, for any 11  ǫ > 0, √ Pr m ηm,q − instab(Ak , D) > ǫ ˆk q  ≤ Pr  1 q  + Pr  √  i=1  1 2 dm (Ak (Si ), Ak (Si )) − instab(Ak , D) > D  m ηm,q − instab(Ak , D) > ǫ ˆk  1 q  ǫ 2  q  i=1  1 2 dm (Ak (Si ), Ak (Si )) − instab(Ak , D) ≤ D  ǫ . [sent-172, score-0.77]
</p><p>62 We start by analyzing the conditional probability, forming the second summand in Eq. [sent-178, score-0.112]
</p><p>63 Recall 1 2 that ηm,q , after clustering the q subsample pairs {Si , Si }q , uses an additional i. [sent-180, score-0.107]
</p><p>64 Thus, conditioned on the event appearing in the second summand of Eq. [sent-184, score-0.184]
</p><p>65 Using a relative entropy version of Hoeffding’s bound [5], we have that the second summand in Eq. [sent-192, score-0.131]
</p><p>66 (22) is upper bounded by: exp −mDkl  v + ǫ/2 v √ √ m m  + exp −mDkl max 0,  v − ǫ/2 √ m  v √ m  ,  (23)  where Dkl [p||q] := −p log(p/q) − (1 − p) log((1 − p)/(1 − q)) for any q ∈ (0, 1) and any p ∈ [0, 1]. [sent-193, score-0.133]
</p><p>67 (23) can be upper bounded by a quantity which converges to 0 as m → ∞. [sent-195, score-0.153]
</p><p>68 (22), using the triangle inequality and switching sides allows us to upper bound it by: Pr  1 q  q  i=1  1 2 dm (Ak (Si ), Ak (Si )) − E[dm (r)|Am ] D D r  ≥  ǫ − E[dm (r)|Am ] − E[dm (r)] − Edm (r) − instab(Ak , D) D r D D 2  (24)  By the deﬁnition of instab(Ak , D) as appearing in Thm. [sent-199, score-0.446]
</p><p>69 (24) by Pr  1 q  q  i=1  1 2 dm (Ak (Si ), Ak (Si )) − E[dm (r)|Am ] D D r  ≥  ǫ − (1 − Pr(Am ))O(r) − O(exp(−r2 )) − o(1) , r 2 12  (26)  where o(1) → 0 as m → ∞. [sent-205, score-0.385]
</p><p>70 6, we have that for any ν > 0, Pr  1 q  ≤ (1 −  q  i=1  1 2 dm (Ak (Si ), Ak (Si )) − E[dm (r)|Am ] > ν D D r  Pr(Am )) r  ∗1+  1 q  Pr(Am ) Pr r  q  i=1  2qν 2 ≤ (1 − Pr(Am )) + 2 Pr(Am ) exp − 2 r r r  1 2 dm (Ak (Si ), Ak (Si )) − E[dm (r)|Am ] > ν Am D D r r  . [sent-207, score-0.8]
</p><p>71 6 can be applied because dm (Ak (Si ), Ak (Si )) = dm (r) for any i, if Am occurs. [sent-209, score-0.77]
</p><p>72 (26) is upper bounded by (1 − Pr(Am )) + 2 Pr(Am ) exp − r r  ǫ 2  2q  − (1 − Pr(Am ))O(r) − O(exp(−r2 ))) − o(1) r r2  2  . [sent-212, score-0.103]
</p><p>73 (29)  Let gm (r) :=  Pr  S1 ,S2 ∼D m  (dm (r) = dm (Ak (S1 ), Ak (S2 ))) D D  ,  g(r) = lim gm (r) m→∞  −3−δ  By regularity condition 3d, g(r) = O(r ) for some δ > 0. [sent-213, score-0.528]
</p><p>74 (29) converges to q  (1 − (1 − g(r))) ) + 2(1 − g(r))q exp −  2q  ǫ 2  − (1 − (1 − g(r))q )O(r) − O(exp(−r2 )) r2  2  . [sent-216, score-0.11]
</p><p>75 (30), we get an upper bound 3+δ  1  O q 1− 2+δ/2 + exp −2q 1− 1+δ/4  δ ǫ − O q − 4+δ 2  1  − O exp(−q 1+δ/4 )  2  . [sent-223, score-0.088]
</p><p>76 Since δ > 0, it can be veriﬁed that the ﬁrst summand asymptotically dominates the second summand (as q → ∞), and can be bounded in turn by o(q −1/2 ). [sent-224, score-0.296]
</p><p>77 ∞, Summarizing, we have that the ﬁrst summand in Eq. [sent-225, score-0.112]
</p><p>78 (22) converges to o(q −1/2 ) as m →√ and the second summand in Eq. [sent-226, score-0.192]
</p><p>79 3 is the following general central limit theorem for Z-estimators (Thm. [sent-232, score-0.111]
</p><p>80 If Ψ(θ 0 ) = 0 and Ψm (θ)/ m → 0 √ ˆ ˆ in probability, and θ converges in probability to θ 0 , then m(θ − θ 0 ) converges in distribution to ˙ −1 Z. [sent-241, score-0.181]
</p><p>81 It is important to note that the theorem is stronger than what we actually need, since we only consider ﬁnite dimensional Euclidean spaces, while the theorem deals with possibly inﬁnite dimensional Banach spaces. [sent-247, score-0.14]
</p><p>82 In principle, it is possible to use this theorem to prove central limit theorems in inﬁnite dimensional settings, for example in kernel clustering where the associated reproducing kernel Hilbert space is inﬁnite dimensional. [sent-248, score-0.331]
</p><p>83 We will prove this in a slightly more complicated way than necessary, which also treats the case of kernel clustering where H is inﬁnite-dimensional. [sent-263, score-0.126]
</p><p>84 Intuitively, a set of real functions {f (·)} from X (with any probability distribution D) to R is called Donsker if it satisﬁes a uniform central limit theorem. [sent-267, score-0.125]
</p><p>85 Without getting too much into the details, 1 A linear operator is automatically continuous in ﬁnite dimensional spaces, not necessarily in inﬁnite dimensional spaces. [sent-268, score-0.088]
</p><p>86 + f (xm ))/ m converges in distribution (as m → ∞) to a Gaussian random variable, and the convergence is uniform over all f (·) in the set, in an appropriately deﬁned sense. [sent-274, score-0.139]
</p><p>87 (32) ˆ ˆ Notice that the ﬁrst class is a set of bounded constant functions, while the third class is a set of indicator functions for all possible clusters. [sent-281, score-0.091]
</p><p>88 1 in [3] (and its preceding discussion), the class is Donsker if any function in the class is differentiable to a sufﬁciently high order. [sent-288, score-0.087]
</p><p>89 For ﬁnite dimensional kernel clustering, it is enough to show that { ·, φ(h) }h∈X is Donsker (namely, the same class of functions after performing the transformation from X to φ(X )). [sent-293, score-0.116]
</p><p>90 In inﬁnite dimensional kernel clustering, our class of functions can be written as {k(·, h)}h∈X , where k(·, ·) is the kernel function, so it is Donsker if the kernel function is differentiable to a sufﬁciently high order. [sent-295, score-0.188]
</p><p>91 15 in [3] (and its preceding discussion), it sufﬁces that the boundary of each possible cluster is composed of a ﬁnite number of smooth surfaces (differentiable to a high enough order) in some Euclidean space. [sent-300, score-0.238]
</p><p>92 This will still be true for inﬁnite dimensional kernel clustering, if we can guarantee that any cluster in any solution close enough to θ0 in Θ will have smooth boundaries. [sent-303, score-0.229]
</p><p>93 For example, universal kernels (such as the Gaussian kernel) are capable of inducing cluster boundaries arbitrarily close in form to any continuous function, and thus our line of attack will not work in such cases. [sent-305, score-0.238]
</p><p>94 In a sense, this is not too surprising, since these kernels correspond to very ’rich’ hypothesis classes, and it is not clear if a precise characterization of their stability properties, via central limit theorems, is at all possible. [sent-306, score-0.11]
</p><p>95 √ As to the asymptotic distribution of m(Ψm − Ψ)(θ 0 ), since Ψ(θ 0 ) = 0 by assumption, we have that for any i ∈ {1, . [sent-313, score-0.082]
</p><p>96 As a result, by the √ standard central limit theorem, m(Ψi −Ψi )(θ 0 ) converges in distribution to a zero mean Gaussian m random vector Y , with covariance matrix Vi = Cθ0 ,i  p(x)(φ(x) − θ 0,i )(φ(x) − θ 0,i )⊤ dx. [sent-325, score-0.183]
</p><p>97 Therefore, √ m(Ψm − Ψ)(θ 0 ) converges in distribution to a zero mean Gaussian random vector, whose covariance matrix V is composed of k diagonal blocks (V1 , . [sent-327, score-0.119]
</p><p>98 1 to get that m(θ−θ 0 ) converges in distribution to a zero mean Gaussian ˙ random vector of the form −Ψ−1 Y , which is a Gaussian random vector with a covariance matrix of θ0 ˙ −1 V Ψ−1 . [sent-333, score-0.137]
</p><p>99 X ∂θ ˆ ˆ Under the assumptions we have made, the model θ returned by the algorithm satisﬁes Ψm (θ) = 0, ˆ converges in probability to some θ 0 for which Ψ(θ 0 ) = 0. [sent-341, score-0.101]
</p><p>100 The asymptotic normality of and θ √ ˆ m(θ − θ 0 ) is now an immediate consequence of central limit theorems for ’maximum likelihood’ Z-estimators, such as Thm. [sent-342, score-0.228]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ak', 0.689), ('dm', 0.385), ('pr', 0.262), ('instab', 0.224), ('edm', 0.154), ('cluster', 0.131), ('donsker', 0.126), ('limm', 0.126), ('si', 0.121), ('summand', 0.112), ('strips', 0.11), ('op', 0.093), ('boundaries', 0.086), ('asymptotic', 0.082), ('clustering', 0.081), ('converges', 0.08), ('xm', 0.074), ('switches', 0.07), ('ym', 0.065), ('bounded', 0.055), ('limit', 0.054), ('banach', 0.052), ('clusters', 0.051), ('argmaxf', 0.049), ('dimensional', 0.044), ('mass', 0.044), ('ax', 0.044), ('dd', 0.042), ('regularity', 0.042), ('euclidean', 0.04), ('boundary', 0.039), ('strip', 0.037), ('normality', 0.036), ('hoeffding', 0.035), ('bregman', 0.035), ('differentiable', 0.033), ('inside', 0.033), ('lemma', 0.033), ('width', 0.031), ('event', 0.031), ('central', 0.031), ('remainder', 0.03), ('condition', 0.03), ('exp', 0.03), ('proof', 0.03), ('integral', 0.029), ('enough', 0.029), ('namely', 0.029), ('mdkl', 0.028), ('vaart', 0.028), ('clusterings', 0.027), ('theorem', 0.026), ('nite', 0.026), ('gm', 0.026), ('subsample', 0.026), ('imply', 0.026), ('kernel', 0.025), ('stability', 0.025), ('theorems', 0.025), ('appearing', 0.024), ('proofs', 0.024), ('neighborhood', 0.023), ('proposition', 0.023), ('qg', 0.022), ('mapping', 0.022), ('drawing', 0.022), ('convergence', 0.022), ('zi', 0.021), ('probability', 0.021), ('composed', 0.021), ('arbitrarily', 0.021), ('everything', 0.021), ('get', 0.021), ('prove', 0.02), ('compactness', 0.02), ('dkl', 0.02), ('uniformly', 0.02), ('bound', 0.019), ('uniform', 0.019), ('law', 0.019), ('lim', 0.019), ('derivative', 0.019), ('polynomially', 0.019), ('instantiations', 0.019), ('sample', 0.019), ('notice', 0.019), ('xi', 0.018), ('high', 0.018), ('conditions', 0.018), ('fr', 0.018), ('rn', 0.018), ('class', 0.018), ('random', 0.018), ('upper', 0.018), ('around', 0.018), ('asymptotically', 0.017), ('veri', 0.017), ('conditioned', 0.017), ('summarizing', 0.017), ('rewriting', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="165-tfidf-1" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>Author: Ohad Shamir, Naftali Tishby</p><p>Abstract: unkown-abstract</p><p>2 0.11756263 <a title="165-tfidf-2" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>Author: Chunhua Shen, Alan Welsh, Lei Wang</p><p>Abstract: In this work, we consider the problem of learning a positive semideﬁnite matrix. The critical issue is how to preserve positive semideﬁniteness during the course of learning. Our algorithm is mainly inspired by LPBoost [1] and the general greedy convex optimization framework of Zhang [2]. We demonstrate the essence of the algorithm, termed PSDBoost (positive semideﬁnite Boosting), by focusing on a few different applications in machine learning. The proposed PSDBoost algorithm extends traditional Boosting algorithms in that its parameter is a positive semideﬁnite matrix with trace being one instead of a classiﬁer. PSDBoost is based on the observation that any trace-one positive semideﬁnite matrix can be decomposed into linear convex combinations of trace-one rank-one matrices, which serve as base learners of PSDBoost. Numerical experiments are presented. 1</p><p>3 0.11034346 <a title="165-tfidf-3" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>Author: Peter Carbonetto, Mark Schmidt, Nando D. Freitas</p><p>Abstract: The stochastic approximation method is behind the solution to many important, actively-studied problems in machine learning. Despite its farreaching application, there is almost no work on applying stochastic approximation to learning problems with general constraints. The reason for this, we hypothesize, is that no robust, widely-applicable stochastic approximation method exists for handling such problems. We propose that interior-point methods are a natural solution. We establish the stability of a stochastic interior-point approximation method both analytically and empirically, and demonstrate its utility by deriving an on-line learning algorithm that also performs feature selection via L1 regularization. 1</p><p>4 0.079195388 <a title="165-tfidf-4" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>Author: Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents the ﬁrst Rademacher complexity-based error bounds for noni.i.d. settings, a generalization of similar existing bounds derived for the i.i.d. case. Our bounds hold in the scenario of dependent samples generated by a stationary β-mixing process, which is commonly adopted in many previous studies of noni.i.d. settings. They beneﬁt from the crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from such ﬁnite samples and lead to tighter generalization bounds. We also present the ﬁrst margin bounds for kernel-based classiﬁcation in this non-i.i.d. setting and brieﬂy study their convergence.</p><p>5 0.07496994 <a title="165-tfidf-5" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>Author: Ralf M. Haefner, Bruce G. Cumming</p><p>Abstract: A crucial part of developing mathematical models of information processing in the brain is the quantiﬁcation of their success. One of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model. Unfortunately, this metric is biased due to the intrinsic variability in the data. We derive a simple analytical modiﬁcation of the traditional formula that signiﬁcantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying Variance Explained by the model class. Our estimator advances on previous work by a) accounting for overﬁtting due to free model parameters mitigating the need for a separate validation data set, b) adjusting for the uncertainty in the noise estimate and c) adding a conditioning term. We apply our new estimator to binocular disparity tuning curves of a set of macaque V1 neurons and ﬁnd that on a population level almost all of the variance unexplained by Gabor functions is attributable to noise. 1</p><p>6 0.074549273 <a title="165-tfidf-6" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>7 0.074243002 <a title="165-tfidf-7" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>8 0.07373628 <a title="165-tfidf-8" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>9 0.070315629 <a title="165-tfidf-9" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>10 0.069085933 <a title="165-tfidf-10" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>11 0.062137984 <a title="165-tfidf-11" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>12 0.061414771 <a title="165-tfidf-12" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>13 0.059745479 <a title="165-tfidf-13" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>14 0.059017681 <a title="165-tfidf-14" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>15 0.058887511 <a title="165-tfidf-15" href="./nips-2008-Reconciling_Real_Scores_with_Binary_Comparisons%3A_A_New_Logistic_Based_Model_for_Ranking.html">190 nips-2008-Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking</a></p>
<p>16 0.057633728 <a title="165-tfidf-16" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>17 0.055849425 <a title="165-tfidf-17" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>18 0.052813068 <a title="165-tfidf-18" href="./nips-2008-Measures_of_Clustering_Quality%3A_A_Working_Set_of_Axioms_for_Clustering.html">132 nips-2008-Measures of Clustering Quality: A Working Set of Axioms for Clustering</a></p>
<p>19 0.052685838 <a title="165-tfidf-19" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>20 0.049179703 <a title="165-tfidf-20" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.138), (1, -0.007), (2, -0.049), (3, 0.057), (4, 0.002), (5, -0.019), (6, 0.047), (7, -0.086), (8, 0.013), (9, -0.034), (10, 0.01), (11, -0.0), (12, 0.087), (13, 0.072), (14, -0.058), (15, 0.144), (16, -0.013), (17, 0.078), (18, 0.089), (19, -0.046), (20, 0.003), (21, 0.019), (22, -0.014), (23, 0.066), (24, 0.014), (25, 0.01), (26, -0.068), (27, -0.033), (28, 0.062), (29, 0.055), (30, 0.067), (31, -0.079), (32, -0.09), (33, -0.148), (34, -0.006), (35, -0.067), (36, 0.029), (37, -0.085), (38, 0.106), (39, 0.068), (40, -0.092), (41, -0.153), (42, -0.06), (43, -0.054), (44, -0.281), (45, 0.065), (46, -0.034), (47, -0.085), (48, 0.071), (49, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97158617 <a title="165-lsi-1" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>Author: Ohad Shamir, Naftali Tishby</p><p>Abstract: unkown-abstract</p><p>2 0.53942609 <a title="165-lsi-2" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>Author: Chunhua Shen, Alan Welsh, Lei Wang</p><p>Abstract: In this work, we consider the problem of learning a positive semideﬁnite matrix. The critical issue is how to preserve positive semideﬁniteness during the course of learning. Our algorithm is mainly inspired by LPBoost [1] and the general greedy convex optimization framework of Zhang [2]. We demonstrate the essence of the algorithm, termed PSDBoost (positive semideﬁnite Boosting), by focusing on a few different applications in machine learning. The proposed PSDBoost algorithm extends traditional Boosting algorithms in that its parameter is a positive semideﬁnite matrix with trace being one instead of a classiﬁer. PSDBoost is based on the observation that any trace-one positive semideﬁnite matrix can be decomposed into linear convex combinations of trace-one rank-one matrices, which serve as base learners of PSDBoost. Numerical experiments are presented. 1</p><p>3 0.43962052 <a title="165-lsi-3" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>Author: Peter Carbonetto, Mark Schmidt, Nando D. Freitas</p><p>Abstract: The stochastic approximation method is behind the solution to many important, actively-studied problems in machine learning. Despite its farreaching application, there is almost no work on applying stochastic approximation to learning problems with general constraints. The reason for this, we hypothesize, is that no robust, widely-applicable stochastic approximation method exists for handling such problems. We propose that interior-point methods are a natural solution. We establish the stability of a stochastic interior-point approximation method both analytically and empirically, and demonstrate its utility by deriving an on-line learning algorithm that also performs feature selection via L1 regularization. 1</p><p>4 0.4278309 <a title="165-lsi-4" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>Author: Nikos Komodakis, Nikos Paragios, Georgios Tziritas</p><p>Abstract: A novel center-based clustering algorithm is proposed in this paper. We ﬁrst formulate clustering as an NP-hard linear integer program and we then use linear programming and the duality theory to derive the solution of this optimization problem. This leads to an efﬁcient and very general algorithm, which works in the dual domain, and can cluster data based on an arbitrary set of distances. Despite its generality, it is independent of initialization (unlike EM-like methods such as K-means), has guaranteed convergence, can automatically determine the number of clusters, and can also provide online optimality bounds about the quality of the estimated clustering solutions. To deal with the most critical issue in a centerbased clustering algorithm (selection of cluster centers), we also introduce the notion of stability of a cluster center, which is a well deﬁned LP-based quantity that plays a key role to our algorithm’s success. Furthermore, we also introduce, what we call, the margins (another key ingredient in our algorithm), which can be roughly thought of as dual counterparts to stabilities and allow us to obtain computationally efﬁcient approximations to the latter. Promising experimental results demonstrate the potentials of our method.</p><p>5 0.41563034 <a title="165-lsi-5" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>Author: Vlad I. Morariu, Balaji V. Srinivasan, Vikas C. Raykar, Ramani Duraiswami, Larry S. Davis</p><p>Abstract: Many machine learning algorithms require the summation of Gaussian kernel functions, an expensive operation if implemented straightforwardly. Several methods have been proposed to reduce the computational complexity of evaluating such sums, including tree and analysis based methods. These achieve varying speedups depending on the bandwidth, dimension, and prescribed error, making the choice between methods difﬁcult for machine learning tasks. We provide an algorithm that combines tree methods with the Improved Fast Gauss Transform (IFGT). As originally proposed the IFGT suffers from two problems: (1) the Taylor series expansion does not perform well for very low bandwidths, and (2) parameter selection is not trivial and can drastically affect performance and ease of use. We address the ﬁrst problem by employing a tree data structure, resulting in four evaluation methods whose performance varies based on the distribution of sources and targets and input parameters such as desired accuracy and bandwidth. To solve the second problem, we present an online tuning approach that results in a black box method that automatically chooses the evaluation method and its parameters to yield the best performance for the input data, desired accuracy, and bandwidth. In addition, the new IFGT parameter selection approach allows for tighter error bounds. Our approach chooses the fastest method at negligible additional cost, and has superior performance in comparisons with previous approaches. 1</p><p>6 0.40605789 <a title="165-lsi-6" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>7 0.40271485 <a title="165-lsi-7" href="./nips-2008-Cyclizing_Clusters_via_Zeta_Function_of_a_Graph.html">55 nips-2008-Cyclizing Clusters via Zeta Function of a Graph</a></p>
<p>8 0.39995036 <a title="165-lsi-8" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>9 0.38804361 <a title="165-lsi-9" href="./nips-2008-Measures_of_Clustering_Quality%3A_A_Working_Set_of_Axioms_for_Clustering.html">132 nips-2008-Measures of Clustering Quality: A Working Set of Axioms for Clustering</a></p>
<p>10 0.37042513 <a title="165-lsi-10" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>11 0.35962909 <a title="165-lsi-11" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>12 0.34872395 <a title="165-lsi-12" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>13 0.34552959 <a title="165-lsi-13" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>14 0.34202677 <a title="165-lsi-14" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>15 0.33904073 <a title="165-lsi-15" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>16 0.33899003 <a title="165-lsi-16" href="./nips-2008-Reconciling_Real_Scores_with_Binary_Comparisons%3A_A_New_Logistic_Based_Model_for_Ranking.html">190 nips-2008-Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking</a></p>
<p>17 0.33410886 <a title="165-lsi-17" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>18 0.32789487 <a title="165-lsi-18" href="./nips-2008-Nonparametric_Bayesian_Learning_of_Switching_Linear_Dynamical_Systems.html">154 nips-2008-Nonparametric Bayesian Learning of Switching Linear Dynamical Systems</a></p>
<p>19 0.32172102 <a title="165-lsi-19" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>20 0.32032487 <a title="165-lsi-20" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.066), (7, 0.061), (12, 0.024), (28, 0.248), (57, 0.065), (59, 0.019), (63, 0.022), (71, 0.018), (77, 0.066), (80, 0.242), (83, 0.042)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87433994 <a title="165-lda-1" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>Author: Ohad Shamir, Naftali Tishby</p><p>Abstract: unkown-abstract</p><p>2 0.75238281 <a title="165-lda-2" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>3 0.7508657 <a title="165-lda-3" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>Author: David Sontag, Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: We propose a new class of consistency constraints for Linear Programming (LP) relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and signiﬁcantly faster than a method that does not use partitioning. 1</p><p>4 0.75058979 <a title="165-lda-4" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>Author: Jeremy Reynolds, Michael C. Mozer</p><p>Abstract: Cognitive control refers to the ﬂexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efﬁciently reﬁned with subsequent task experience. 1</p><p>5 0.75029194 <a title="165-lda-5" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>Author: Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</p><p>Abstract: Uncertainty is omnipresent when we perceive or interact with our environment, and the Bayesian framework provides computational methods for dealing with it. Mathematical models for Bayesian decision making typically require datastructures that are hard to implement in neural networks. This article shows that even the simplest and experimentally best supported type of synaptic plasticity, Hebbian learning, in combination with a sparse, redundant neural code, can in principle learn to infer optimal Bayesian decisions. We present a concrete Hebbian learning rule operating on log-probability ratios. Modulated by reward-signals, this Hebbian plasticity rule also provides a new perspective for understanding how Bayesian inference could support fast reinforcement learning in the brain. In particular we show that recent experimental results by Yang and Shadlen [1] on reinforcement learning of probabilistic inference in primates can be modeled in this way. 1</p><p>6 0.74973089 <a title="165-lda-6" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>7 0.74825805 <a title="165-lda-7" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>8 0.74789768 <a title="165-lda-8" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>9 0.74732989 <a title="165-lda-9" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>10 0.74723369 <a title="165-lda-10" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>11 0.7471683 <a title="165-lda-11" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>12 0.74677157 <a title="165-lda-12" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>13 0.74672067 <a title="165-lda-13" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>14 0.74552476 <a title="165-lda-14" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>15 0.74462366 <a title="165-lda-15" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>16 0.74442476 <a title="165-lda-16" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>17 0.74372435 <a title="165-lda-17" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>18 0.74343085 <a title="165-lda-18" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>19 0.74326718 <a title="165-lda-19" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>20 0.74276286 <a title="165-lda-20" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
