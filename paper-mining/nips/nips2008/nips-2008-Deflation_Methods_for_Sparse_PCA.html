<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>57 nips-2008-Deflation Methods for Sparse PCA</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-57" href="#">nips2008-57</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>57 nips-2008-Deflation Methods for Sparse PCA</h1>
<br/><p>Source: <a title="nips-2008-57-pdf" href="http://papers.nips.cc/paper/3575-deflation-methods-for-sparse-pca.pdf">pdf</a></p><p>Author: Lester W. Mackey</p><p>Abstract: In analogy to the PCA setting, the sparse PCA problem is often solved by iteratively alternating between two subtasks: cardinality-constrained rank-one variance maximization and matrix deﬂation. While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justiﬁcation from the PCA context. In this work, we demonstrate that the standard PCA deﬂation procedure is seldom appropriate for the sparse PCA setting. To rectify the situation, we ﬁrst develop several deﬂation alternatives better suited to the cardinality-constrained context. We then reformulate the sparse PCA optimization problem to explicitly reﬂect the maximum additional variance objective on each round. The result is a generalized deﬂation procedure that typically outperforms more standard techniques on real-world datasets. 1</p><p>Reference: <a title="nips-2008-57-reference" href="../nips2008_reference/nips-2008-Deflation_Methods_for_Sparse_PCA_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justiﬁcation from the PCA context. [sent-2, score-0.053]
</p><p>2 In this work, we demonstrate that the standard PCA deﬂation procedure is seldom appropriate for the sparse PCA setting. [sent-3, score-0.119]
</p><p>3 We then reformulate the sparse PCA optimization problem to explicitly reﬂect the maximum additional variance objective on each round. [sent-5, score-0.171]
</p><p>4 The goal of PCA is to extract several principal components, linear combinations of input variables that together best account for the variance in a data set. [sent-8, score-0.127]
</p><p>5 Often, PCA is formulated as an eigenvalue decomposition problem: each eigenvector of the sample covariance matrix of a data set corresponds to the loadings or coefﬁcients of a principal component. [sent-9, score-0.308]
</p><p>6 A common approach to solving this partial eigenvalue decomposition is to iteratively alternate between two subproblems: rank-one variance maximization and matrix deﬂation. [sent-10, score-0.156]
</p><p>7 The ﬁrst subproblem involves ﬁnding the maximum-variance loadings vector for a given sample covariance matrix or, equivalently, ﬁnding the leading eigenvector of the matrix. [sent-11, score-0.239]
</p><p>8 The second involves modifying the covariance matrix to eliminate the inﬂuence of that eigenvector. [sent-12, score-0.079]
</p><p>9 Each principal component is a linear combination of all variables, and the loadings are typically non-zero. [sent-14, score-0.161]
</p><p>10 Sparse PCA [8, 3, 16, 17, 6, 18, 1, 2, 9, 10, 12] injects sparsity into the PCA process by searching for “pseudoeigenvectors”, sparse loadings that explain a maximal amount variance in the data. [sent-16, score-0.256]
</p><p>11 In analogy to the PCA setting, many authors attempt to solve the sparse PCA problem by iteratively alternating between two subtasks: cardinality-constrained rank-one variance maximization and matrix deﬂation. [sent-17, score-0.197]
</p><p>12 In this work, we demonstrate that the standard PCA deﬂation procedure is seldom appropriate for the sparse PCA setting. [sent-20, score-0.119]
</p><p>13 To rectify the situation, we ﬁrst develop several heuristic deﬂation alternatives with more desirable properties. [sent-21, score-0.04]
</p><p>14 We then reformulate the sparse PCA optimization problem to explicitly reﬂect the maximum additional variance objective on each round. [sent-22, score-0.171]
</p><p>15 In Section 2 we discuss matrix deﬂation as it relates to PCA and sparse PCA. [sent-25, score-0.11]
</p><p>16 We examine the failings of typical PCA deﬂation in the sparse setting and develop several alternative deﬂation procedures. [sent-26, score-0.104]
</p><p>17 In Section 3, we present a reformulation of the standard iterative sparse PCA optimization problem and derive a generalized deﬂation procedure to solve the reformulation. [sent-27, score-0.124]
</p><p>18 2  Deﬂation methods  A matrix deﬂation modiﬁes a matrix to eliminate the inﬂuence of a given eigenvector, typically by setting the associated eigenvalue to zero (see [14] for a more detailed discussion). [sent-32, score-0.116]
</p><p>19 We will ﬁrst discuss deﬂation in the context of PCA and then consider its extension to sparse PCA. [sent-33, score-0.084]
</p><p>20 1  Hotelling’s deﬂation and PCA  In the PCA setting, the goal is to extract the r leading eigenvectors of the sample covariance matrix, A0 ∈ Sp , as its eigenvectors are equivalent to the loadings of the ﬁrst r principal components. [sent-35, score-0.279]
</p><p>21 On the t-th iteration of the deﬂation method, we ﬁrst extract the leading eigenvector of At−1 , xt = argmax xT At−1 x (1) x:xT x=1  and we then use Hotelling’s deﬂation to annihilate xt : At = At−1 − xt xT At−1 xt xT . [sent-37, score-1.898]
</p><p>22 t t  (2)  The deﬂation step ensures that the t + 1-st leading eigenvector of A0 is the leading eigenvector of At . [sent-38, score-0.164]
</p><p>23 ˆ Axj = Axj − xj xT Axj xT xj = Axj − xj xT Axj = λj xj − λj xj = 0xj . [sent-62, score-0.075]
</p><p>24 Ax j j Thus, Hotelling’s deﬂation preserves all eigenvectors of a matrix and annihilates a selected eigenvalue while maintaining all others. [sent-64, score-0.142]
</p><p>25 In the case of our iterative deﬂation method, annihilating the t-th leading eigenvector of A0 renders the t + 1-st leading eigenvector dominant in the next round. [sent-66, score-0.217]
</p><p>26 2  Hotelling’s deﬂation and sparse PCA  In the sparse PCA setting, we seek r sparse loadings which together capture the maximum amount of variance in the data. [sent-68, score-0.412]
</p><p>27 Most authors [1, 9, 16, 12] adopt the additional constraint that the loadings be produced in a sequential fashion. [sent-69, score-0.113]
</p><p>28 Typically, Hotelling’s deﬂation is utilized by substituting an extracted pseudo-eigenvector for a true eigenvector in the deﬂation step of Eq. [sent-74, score-0.077]
</p><p>29 Let x = (1, 0)T , a sparse pseudo-eigenvector, and C = C − xxT CxxT , the corresponding 0 1 ˆ ˆ ˆ deﬂated matrix. [sent-84, score-0.084]
</p><p>30 That Sp is not closed under pseudo-eigenvector Hotelling’s deﬂation is a serious failing, for most + iterative sparse PCA methods assume a positive-semideﬁnite matrix on each iteration. [sent-89, score-0.125]
</p><p>31 A second, related shortcoming of pseudo-eigenvector Hotelling’s deﬂation is its failure to render a pseudoeigenvector orthogonal to a deﬂated matrix. [sent-90, score-0.039]
</p><p>32 If A is our matrix of interest, x is our pseudo-eigenvector ˆ ˆ with variance λ = xT Ax, and A = A − xxT AxxT is our deﬂated matrix, then Ax = Ax − xxT AxxT x = Ax − λx is zero iff x is a true eigenvector. [sent-91, score-0.087]
</p><p>33 3  Alternative deﬂation techniques  In this section, we will attempt to rectify the failings of pseudo-eigenvector Hotelling’s deﬂation by considering several alternative deﬂation techniques better suited to the sparse PCA setting. [sent-98, score-0.157]
</p><p>34 1  Projection deﬂation  Given a data matrix Y ∈ Rn×p and an arbitrary unit vector in x ∈ Rp , an intuitive way to remove the contribution of x from Y is to project Y onto the orthocomplement of the space spanned by x: ˆ ˆ Y = Y (I − xxT ). [sent-104, score-0.1]
</p><p>35 t t However, in the general case, when xt is not a true eigenvector, projection deﬂation maintains the desirable properties that were lost to Hotelling’s deﬂation. [sent-106, score-0.527]
</p><p>36 For example, positive-semideﬁniteness is preserved: ∀y, y T At y = y T (I − xt xT )At−1 (I − xt xT )y = z T At−1 z t t where z = (I − xt xT )y. [sent-107, score-1.338]
</p><p>37 Moreover, At is rendered left and right t + orthogonal to xt , as (I −xt xT )xt = xt −xt = 0 and At is symmetric. [sent-109, score-0.931]
</p><p>38 Projection deﬂation therefore t annihilates all covariances with xt : ∀v, v T At xt = xT At v = 0. [sent-110, score-0.928]
</p><p>39 2  Schur complement deﬂation  Since our goal in matrix deﬂation is to eliminate the inﬂuence, as measured through variance and covariances, of a newly discovered pseudo-eigenvector, it is reasonable to consider the conditional variance of our data variables given a pseudo-principal component. [sent-113, score-0.289]
</p><p>40 While this conditional variance is non-trivial to compute in general, it takes on a simple closed form when the variables are normally distributed. [sent-114, score-0.061]
</p><p>41 If W has covariance matrix Σ, then (W, W x) has covariance T Σ Σx Σ matrix V = , and V ar(W |W x) = Σ − ΣxxΣx whenever xT Σx = 0 [15]. [sent-116, score-0.116]
</p><p>42 xT xT Σ xT Σx That is, the conditional variance is the Schur complement of the vector variance xT Σx in the full covariance matrix V . [sent-117, score-0.28]
</p><p>43 By substituting sample covariance matrices for their population counterparts, we arrive at a new deﬂation technique: Schur complement deﬂation At = At−1 −  At−1 xt xT At−1 t xT At−1 xt t  (5)  Schur complement deﬂation, like projection deﬂation, preserves positive-semideﬁniteness. [sent-118, score-1.215]
</p><p>44 To v T At−1 xt xT At−1 v t see this, suppose At−1 ∈ Sp . [sent-119, score-0.446]
</p><p>45 Then, ∀v, v T At v = v T At−1 v − ≥ 0 as + xT At−1 xt t  v T At−1 vxT At−1 xt − (v T At−1 xt )2 ≥ 0 by the Cauchy-Schwarz inequality and xT At−1 xt ≥ 0 t t as At−1 ∈ Sp . [sent-120, score-1.784]
</p><p>46 + Furthermore, Schur complement deﬂation renders xt left and right orthogonal to At , since At is A x xT At−1 x t symmetric and At xt = At−1 xt − t−1T t t−1 xt t = At−1 xt − At−1 xt = 0. [sent-121, score-2.833]
</p><p>47 x A t  Additionally, Schur complement deﬂation reduces to Hotelling’s deﬂation when xt is an eigenvector of At−1 with eigenvalue λt = 0: At−1 xt xT At−1 t xT At−1 xt t λt xt xT λt t = At−1 − λt = At−1 − xt xT At−1 xt xT . [sent-122, score-2.879]
</p><p>48 t t  At = At−1 −  While we motivated Schur complement deﬂation with a Gaussianity assumption, the technique admits a more general interpretation as a column projection of a data matrix. [sent-123, score-0.189]
</p><p>49 Suppose Y ∈ Rn×p xxT Y T ˆ is a mean-centered data matrix, x ∈ Rp has unit norm, and Y = (I − Y||Y x||2 )Y , the projection of the columns of Y onto the orthocomplement of the space spanned by the pseudo-principal comˆ ponent, Y x. [sent-124, score-0.142]
</p><p>50 If Y has sample covariance matrix A, then the sample covariance of Y is given by ˆ = 1 Y T (I − Y xxT Y2T )T (I − Y xxT Y2T )Y = 1 Y T (I − Y xxT Y2T )Y = A − AxxT A . [sent-125, score-0.09]
</p><p>51 Whenever we deal with a sequence of non-orthogonal vectors, we must take care to distinguish between the variance explained by a vector and the additional variance explained, given all previous vectors. [sent-129, score-0.167]
</p><p>52 These concepts are equivalent in the PCA setting, as true eigenvectors of a matrix are orthogonal, but, in general, the vectors extracted by sparse PCA will not be orthogonal. [sent-130, score-0.157]
</p><p>53 The additional variance explained by the t-th pseudo-eigenvector, xt , is equivalent to the variance explained by the component of xt orthogonal to the space spanned by all previous pseudo-eigenvectors, qt = xt − Pt−1 xt , where Pt−1 is the orthogonal projection onto the space spanned by x1 , . [sent-131, score-2.42]
</p><p>54 On each deﬂation step, therefore, we only want to eliminate the variance associated with qt . [sent-135, score-0.265]
</p><p>55 Annihilating the full vector xt will often lead to “double counting” and could re-introduce components parallel to previously annihilated vectors. [sent-136, score-0.446]
</p><p>56 (2)) for t = 1 and can be easily expressed in terms of a running Gram-Schmidt decomposition for t > 1: Orthogonalized Hotelling’s deﬂation (OHD) qt =  (I − Qt−1 QT )xt t−1 (I − Qt−1 QT )xt t−1  (6)  T T At = At−1 − qt qt At−1 qt qt  where q1 = x1 , and q1 , . [sent-149, score-0.915]
</p><p>57 , qt−1 form an orthonormal basis for the space spanned by x1 , . [sent-156, score-0.052]
</p><p>58 However, the same principles may be applied to projection deﬂation to generate an orthogonalized variant that inherits its desirable properties. [sent-162, score-0.206]
</p><p>59 Schur complement deﬂation is unique in that it preserves orthogonality in all subsequent rounds. [sent-163, score-0.123]
</p><p>60 A xt xT A v t That is, if a vector v is orthogonal to At−1 for any t, then At v = At−1 v − t−1T At−1 xt−1 = 0 as xt t At−1 v = 0. [sent-164, score-0.931]
</p><p>61 Orthogonalized Schur complement deﬂation is equivalent to Schur complement deﬂation. [sent-168, score-0.2]
</p><p>62 We may write xt = ot + pt , where pt is in the subspace spanned by all previously extracted pseudo-eigenvectors and ot is orthogonal to this subspace. [sent-171, score-1.089]
</p><p>63 Then we know that At−1 pt = 0, as pt is a linear combination of x1 , . [sent-172, score-0.26]
</p><p>64 Thus, xT At xt = pT At pt + oT At pt + pT At ot + oT At ot = oT At ot . [sent-176, score-1.135]
</p><p>65 t t t t t t Further, At−1 xt xT At−1 = At−1 pt pT At−1 +At−1 pt oT At−1 +At−1 ot pT At−1 +At−1 ot oT At−1 = t t t t t At−1 ot oT At−1 . [sent-177, score-1.135]
</p><p>66 Hence, At = At−1 − t  At−1 ot oT At−1 t oT At−1 ot t  = At−1 −  T At−1 qt qt At−1 T qt At−1 qt  as qt =  ot ||ot || . [sent-178, score-1.344]
</p><p>67 In this section, we explore a more principled alternative: reformulating the sparse PCA optimization problem to explicitly reﬂect our maximization objective on each round. [sent-184, score-0.119]
</p><p>68 Recall that the goal of sparse PCA is to ﬁnd r cardinality-constrained pseudo-eigenvectors which together explain the most variance in the data. [sent-185, score-0.157]
</p><p>69 If we additionally constrain the sparse loadings to 5  be generated sequentially, as in the PCA setting and the previous section, then a greedy approach of maximizing the additional variance of each new vector naturally suggests itself. [sent-186, score-0.258]
</p><p>70 T  A0 On round t, the additional variance of a vector x is given by q qT q q where A0 is the data covariance matrix, q = (I − Pt−1 )x, and Pt−1 is the projection onto the space spanned by previous pseudo-eigenvectors x1 , . [sent-187, score-0.247]
</p><p>71 As q T q = xT (I − Pt−1 )(I − Pt−1 )x = xT (I − Pt−1 )x, maximizing additional variance is equivalent to solving a cardinality-constrained maximum generalized eigenvalue problem,  max xT (I − Pt−1 )A0 (I − Pt−1 )x x  subject to xT (I − Pt−1 )x = 1 Card(x) ≤ kt . [sent-191, score-0.159]
</p><p>72 , qt−1 form an orthonormal basis for the space t−1 t−1 T T spanned by x1 , . [sent-195, score-0.052]
</p><p>73 Writing I − Pt−1 = I − s=1 qs qs = s=1 (I − qs qs ) suggests a generalized deﬂation technique that leads to the solution of Eq. [sent-199, score-0.182]
</p><p>74 We imbed the technique into the following algorithm for sparse PCA: Algorithm 1 Generalized Deﬂation Method for Sparse PCA p Given: A0 ∈ S+ , r ∈ N, {k1 , . [sent-201, score-0.105]
</p><p>75 , r • xt ←  argmax  xT At−1 x  x:xT Bt−1 x=1,Card(x)≤kt  • • • •  qt ← Bt−1 xt T T At ← (I − qt qt )At−1 (I − qt qt ) T Bt ← Bt−1 (I − qt qt ) xt ← xt / ||xt ||  Return: {x1 , . [sent-209, score-3.079]
</p><p>76 1  Pit props dataset  The pit props dataset [5] with 13 variables and 180 observations has become a de facto standard for benchmarking sparse PCA methods. [sent-216, score-0.244]
</p><p>77 To demonstrate the disparate behavior of differing deﬂation methods, we utilize each sparse PCA algorithm and deﬂation technique to successively extract six sparse loadings, each constrained to have cardinality less than or equal to kt = 4. [sent-217, score-0.242]
</p><p>78 We report the additional variances explained by each sparse vector in Table 2 and the cumulative percentage variance explained on each iteration in Table 3. [sent-218, score-0.26]
</p><p>79 For reference, the ﬁrst 6 true principal components of the pit props dataset capture 87% of the variance. [sent-219, score-0.143]
</p><p>80 908  Table 2: Additional variance explained by each of the ﬁrst 6 sparse loadings extracted from the Pit Props dataset. [sent-292, score-0.292]
</p><p>81 4% of the variance, while the best performing methods, Schur complement deﬂation and generalized deﬂation, explain approximately 79% of the variance each. [sent-294, score-0.198]
</p><p>82 Projection deﬂation and its orthogonalized variant also outperform Hotelling’s deﬂation, while orthogonalized Hotelling’s shows the worst performance with only 63. [sent-295, score-0.222]
</p><p>83 Generalized deﬂation and the two projection deﬂations dominate, with GD achieving the maximum cumulative variance explained on each round. [sent-298, score-0.182]
</p><p>84 In contrast, the more standard Hotelling’s and orthogonalized Hotelling’s underperform the remaining techniques. [sent-299, score-0.111]
</p><p>85 2%  Table 3: Cumulative percentage variance explained by the ﬁrst 6 sparse loadings extracted from the Pit Props dataset. [sent-372, score-0.309]
</p><p>86 2  Gene expression data  The Berkeley Drosophila Transcription Network Project (BDTNP) 3D gene expression data [4] contains gene expression levels measured in each nucleus of developing Drosophila embryos and averaged across many embryos and developmental stages. [sent-374, score-0.078]
</p><p>87 223  GSLDA additional variance explained PD SCD OHD OPD 1. [sent-386, score-0.106]
</p><p>88 1%  GSLDA cumulative percentage variance PD SCD OHD OPD 21. [sent-434, score-0.1]
</p><p>89 6%  Table 4: Additional variance and cumulative percentage variance explained by the ﬁrst 8 sparse loadings of GSLDA on the BDTNP VirtualEmbryo. [sent-474, score-0.375]
</p><p>90 The generalized deﬂation technique performs best, achieving the largest additional variance on every round and a ﬁnal cumulative variance of 79. [sent-476, score-0.222]
</p><p>91 Schur complement deﬂation, projection deﬂation, and orthogonalized projection deﬂation all perform comparably, explaining roughly 77% of the total variance after 8 rounds. [sent-478, score-0.408]
</p><p>92 In last place are the standard Hotelling’s and orthogonalized Hotelling’s deﬂations, both of which explain less than 76% of variance after 8 rounds. [sent-479, score-0.184]
</p><p>93 7  5  Conclusion  In this work, we have exposed the theoretical and empirical shortcomings of Hotelling’s deﬂation in the sparse PCA setting and developed several alternative methods more suitable for non-eigenvector deﬂation. [sent-480, score-0.084]
</p><p>94 Notably, the utility of these procedures is not limited to the sparse PCA setting. [sent-481, score-0.084]
</p><p>95 Indeed, the methods presented can be applied to any of a number of constrained eigendecomposition-based problems, including sparse canonical correlation analysis [13] and linear discriminant analysis [10]. [sent-482, score-0.084]
</p><p>96 Loadings and correlations in the interpretation of principal components. [sent-511, score-0.048]
</p><p>97 Two case studies in the application of principal components. [sent-535, score-0.048]
</p><p>98 Spectral bounds for sparse PCA: Exact and greedy algorithms. [sent-556, score-0.084]
</p><p>99 Simon, Low-rank approximations with sparse factors I: Basic algorithms and error analysis. [sent-607, score-0.084]
</p><p>100 Simon, Low-rank approximations with sparse factors II: Penalized methods with discrete Newton-like iterations. [sent-616, score-0.084]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ation', 0.638), ('xt', 0.446), ('hotelling', 0.324), ('pca', 0.188), ('qt', 0.183), ('ot', 0.143), ('schur', 0.138), ('pt', 0.13), ('orthogonalized', 0.111), ('ohd', 0.101), ('complement', 0.1), ('loadings', 0.099), ('xxt', 0.097), ('gslda', 0.091), ('sparse', 0.084), ('opd', 0.071), ('scd', 0.071), ('gd', 0.071), ('projection', 0.068), ('axj', 0.062), ('variance', 0.061), ('eigenvector', 0.06), ('ax', 0.057), ('hd', 0.057), ('sp', 0.052), ('dcpca', 0.051), ('props', 0.051), ('pd', 0.05), ('pc', 0.049), ('principal', 0.048), ('pit', 0.044), ('eigenvalue', 0.043), ('spanned', 0.041), ('axxt', 0.04), ('orthogonal', 0.039), ('seldom', 0.035), ('qs', 0.034), ('covariance', 0.032), ('explained', 0.031), ('eigenvectors', 0.03), ('rectify', 0.027), ('matrix', 0.026), ('generalized', 0.025), ('rp', 0.025), ('ated', 0.024), ('preserves', 0.023), ('leading', 0.022), ('cumulative', 0.022), ('bt', 0.022), ('technique', 0.021), ('eliminate', 0.021), ('gene', 0.021), ('niteness', 0.021), ('annihilates', 0.02), ('annihilating', 0.02), ('ations', 0.02), ('bdtnp', 0.02), ('failings', 0.02), ('moghaddam', 0.02), ('orthocomplement', 0.02), ('reformulating', 0.02), ('newly', 0.02), ('cardinality', 0.019), ('eigenvalues', 0.019), ('extract', 0.018), ('round', 0.018), ('renders', 0.018), ('borrowed', 0.018), ('zha', 0.018), ('aspremont', 0.018), ('embryos', 0.018), ('percentage', 0.017), ('extracted', 0.017), ('covariances', 0.016), ('jolliffe', 0.016), ('torres', 0.016), ('kt', 0.016), ('subtasks', 0.015), ('drosophila', 0.015), ('sriperumbudur', 0.015), ('xj', 0.015), ('iterative', 0.015), ('maximization', 0.015), ('argmax', 0.014), ('axi', 0.014), ('component', 0.014), ('additional', 0.014), ('card', 0.014), ('inherits', 0.014), ('simon', 0.014), ('de', 0.014), ('sequentially', 0.013), ('desirable', 0.013), ('onto', 0.013), ('techniques', 0.013), ('proposition', 0.012), ('explain', 0.012), ('reformulate', 0.012), ('iteratively', 0.011), ('orthonormal', 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="57-tfidf-1" href="./nips-2008-Deflation_Methods_for_Sparse_PCA.html">57 nips-2008-Deflation Methods for Sparse PCA</a></p>
<p>Author: Lester W. Mackey</p><p>Abstract: In analogy to the PCA setting, the sparse PCA problem is often solved by iteratively alternating between two subtasks: cardinality-constrained rank-one variance maximization and matrix deﬂation. While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justiﬁcation from the PCA context. In this work, we demonstrate that the standard PCA deﬂation procedure is seldom appropriate for the sparse PCA setting. To rectify the situation, we ﬁrst develop several deﬂation alternatives better suited to the cardinality-constrained context. We then reformulate the sparse PCA optimization problem to explicitly reﬂect the maximum additional variance objective on each round. The result is a generalized deﬂation procedure that typically outperforms more standard techniques on real-world datasets. 1</p><p>2 0.23068459 <a title="57-tfidf-2" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>Author: Angela J. Yu, Jonathan D. Cohen</p><p>Abstract: In a variety of behavioral tasks, subjects exhibit an automatic and apparently suboptimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no real predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reﬂect the inadvertent engagement of mechanisms critical for adapting to a changing environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential ﬁltering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential ﬁlter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that parameter-tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities. 1</p><p>3 0.20395653 <a title="57-tfidf-3" href="./nips-2008-Translated_Learning%3A_Transfer_Learning_across_Different_Feature_Spaces.html">242 nips-2008-Translated Learning: Transfer Learning across Different Feature Spaces</a></p>
<p>Author: Wenyuan Dai, Yuqiang Chen, Gui-rong Xue, Qiang Yang, Yong Yu</p><p>Abstract: This paper investigates a new machine learning strategy called translated learning. Unlike many previous learning tasks, we focus on how to use labeled data from one feature space to enhance the classiﬁcation of other entirely different learning spaces. For example, we might wish to use labeled text data to help learn a model for classifying image data, when the labeled images are difﬁcult to obtain. An important aspect of translated learning is to build a “bridge” to link one feature space (known as the “source space”) to another space (known as the “target space”) through a translator in order to migrate the knowledge from source to target. The translated learning solution uses a language model to link the class labels to the features in the source spaces, which in turn is translated to the features in the target spaces. Finally, this chain of linkages is completed by tracing back to the instances in the target spaces. We show that this path of linkage can be modeled using a Markov chain and risk minimization. Through experiments on the text-aided image classiﬁcation and cross-language classiﬁcation tasks, we demonstrate that our translated learning framework can greatly outperform many state-of-the-art baseline methods. 1</p><p>4 0.16031653 <a title="57-tfidf-4" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>Author: Xinhua Zhang, Le Song, Arthur Gretton, Alex J. Smola</p><p>Abstract: Many machine learning algorithms can be formulated in the framework of statistical independence such as the Hilbert Schmidt Independence Criterion. In this paper, we extend this criterion to deal with structured and interdependent observations. This is achieved by modeling the structures using undirected graphical models and comparing the Hilbert space embeddings of distributions. We apply this new criterion to independent component analysis and sequence clustering. 1</p><p>5 0.14886783 <a title="57-tfidf-5" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>Author: Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos</p><p>Abstract: Our setting is a Partially Observable Markov Decision Process with continuous state, observation and action spaces. Decisions are based on a Particle Filter for estimating the belief state given past observations. We consider a policy gradient approach for parameterized policy optimization. For that purpose, we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy, focusing on Finite Difference (FD) techniques. We show that the naive FD is subject to variance explosion because of the non-smoothness of the resampling procedure. We propose a more sophisticated FD method which overcomes this problem and establish its consistency. 1</p><p>6 0.11176921 <a title="57-tfidf-6" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>7 0.092825674 <a title="57-tfidf-7" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>8 0.078464977 <a title="57-tfidf-8" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>9 0.078255042 <a title="57-tfidf-9" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>10 0.076336749 <a title="57-tfidf-10" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>11 0.071404755 <a title="57-tfidf-11" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>12 0.070478305 <a title="57-tfidf-12" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>13 0.068794988 <a title="57-tfidf-13" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>14 0.066628873 <a title="57-tfidf-14" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>15 0.062834777 <a title="57-tfidf-15" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>16 0.058528014 <a title="57-tfidf-16" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>17 0.056892619 <a title="57-tfidf-17" href="./nips-2008-Nonparametric_Bayesian_Learning_of_Switching_Linear_Dynamical_Systems.html">154 nips-2008-Nonparametric Bayesian Learning of Switching Linear Dynamical Systems</a></p>
<p>18 0.053077329 <a title="57-tfidf-18" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>19 0.049399637 <a title="57-tfidf-19" href="./nips-2008-Bayesian_Exponential_Family_PCA.html">31 nips-2008-Bayesian Exponential Family PCA</a></p>
<p>20 0.045514788 <a title="57-tfidf-20" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.115), (1, 0.051), (2, -0.053), (3, -0.002), (4, -0.041), (5, 0.256), (6, -0.041), (7, 0.168), (8, 0.143), (9, -0.12), (10, -0.028), (11, -0.259), (12, 0.028), (13, -0.03), (14, -0.004), (15, 0.01), (16, 0.063), (17, -0.051), (18, 0.069), (19, -0.138), (20, 0.02), (21, -0.139), (22, -0.101), (23, -0.044), (24, -0.016), (25, -0.05), (26, -0.044), (27, -0.032), (28, -0.099), (29, 0.03), (30, 0.014), (31, -0.049), (32, -0.041), (33, 0.047), (34, 0.012), (35, -0.01), (36, 0.004), (37, -0.07), (38, -0.055), (39, -0.005), (40, -0.046), (41, -0.083), (42, 0.172), (43, 0.018), (44, -0.01), (45, -0.01), (46, 0.081), (47, -0.027), (48, -0.016), (49, 0.052)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9878844 <a title="57-lsi-1" href="./nips-2008-Deflation_Methods_for_Sparse_PCA.html">57 nips-2008-Deflation Methods for Sparse PCA</a></p>
<p>Author: Lester W. Mackey</p><p>Abstract: In analogy to the PCA setting, the sparse PCA problem is often solved by iteratively alternating between two subtasks: cardinality-constrained rank-one variance maximization and matrix deﬂation. While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justiﬁcation from the PCA context. In this work, we demonstrate that the standard PCA deﬂation procedure is seldom appropriate for the sparse PCA setting. To rectify the situation, we ﬁrst develop several deﬂation alternatives better suited to the cardinality-constrained context. We then reformulate the sparse PCA optimization problem to explicitly reﬂect the maximum additional variance objective on each round. The result is a generalized deﬂation procedure that typically outperforms more standard techniques on real-world datasets. 1</p><p>2 0.72219688 <a title="57-lsi-2" href="./nips-2008-Translated_Learning%3A_Transfer_Learning_across_Different_Feature_Spaces.html">242 nips-2008-Translated Learning: Transfer Learning across Different Feature Spaces</a></p>
<p>Author: Wenyuan Dai, Yuqiang Chen, Gui-rong Xue, Qiang Yang, Yong Yu</p><p>Abstract: This paper investigates a new machine learning strategy called translated learning. Unlike many previous learning tasks, we focus on how to use labeled data from one feature space to enhance the classiﬁcation of other entirely different learning spaces. For example, we might wish to use labeled text data to help learn a model for classifying image data, when the labeled images are difﬁcult to obtain. An important aspect of translated learning is to build a “bridge” to link one feature space (known as the “source space”) to another space (known as the “target space”) through a translator in order to migrate the knowledge from source to target. The translated learning solution uses a language model to link the class labels to the features in the source spaces, which in turn is translated to the features in the target spaces. Finally, this chain of linkages is completed by tracing back to the instances in the target spaces. We show that this path of linkage can be modeled using a Markov chain and risk minimization. Through experiments on the text-aided image classiﬁcation and cross-language classiﬁcation tasks, we demonstrate that our translated learning framework can greatly outperform many state-of-the-art baseline methods. 1</p><p>3 0.69860059 <a title="57-lsi-3" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>Author: Angela J. Yu, Jonathan D. Cohen</p><p>Abstract: In a variety of behavioral tasks, subjects exhibit an automatic and apparently suboptimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no real predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reﬂect the inadvertent engagement of mechanisms critical for adapting to a changing environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential ﬁltering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential ﬁlter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that parameter-tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities. 1</p><p>4 0.61344451 <a title="57-lsi-4" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>Author: Pierre-arnaud Coquelin, Romain Deguest, Rémi Munos</p><p>Abstract: Our setting is a Partially Observable Markov Decision Process with continuous state, observation and action spaces. Decisions are based on a Particle Filter for estimating the belief state given past observations. We consider a policy gradient approach for parameterized policy optimization. For that purpose, we investigate sensitivity analysis of the performance measure with respect to the parameters of the policy, focusing on Finite Difference (FD) techniques. We show that the naive FD is subject to variance explosion because of the non-smoothness of the resampling procedure. We propose a more sophisticated FD method which overcomes this problem and establish its consistency. 1</p><p>5 0.60601366 <a title="57-lsi-5" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>Author: Xinhua Zhang, Le Song, Arthur Gretton, Alex J. Smola</p><p>Abstract: Many machine learning algorithms can be formulated in the framework of statistical independence such as the Hilbert Schmidt Independence Criterion. In this paper, we extend this criterion to deal with structured and interdependent observations. This is achieved by modeling the structures using undirected graphical models and comparing the Hilbert space embeddings of distributions. We apply this new criterion to independent component analysis and sequence clustering. 1</p><p>6 0.55181336 <a title="57-lsi-6" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>7 0.38654825 <a title="57-lsi-7" href="./nips-2008-Nonparametric_Bayesian_Learning_of_Switching_Linear_Dynamical_Systems.html">154 nips-2008-Nonparametric Bayesian Learning of Switching Linear Dynamical Systems</a></p>
<p>8 0.37210828 <a title="57-lsi-8" href="./nips-2008-Bayesian_Exponential_Family_PCA.html">31 nips-2008-Bayesian Exponential Family PCA</a></p>
<p>9 0.36839724 <a title="57-lsi-9" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>10 0.36647639 <a title="57-lsi-10" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>11 0.34766057 <a title="57-lsi-11" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>12 0.34478897 <a title="57-lsi-12" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>13 0.31089076 <a title="57-lsi-13" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>14 0.30607352 <a title="57-lsi-14" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>15 0.29626772 <a title="57-lsi-15" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>16 0.28153616 <a title="57-lsi-16" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>17 0.28131559 <a title="57-lsi-17" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>18 0.26857466 <a title="57-lsi-18" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>19 0.26173922 <a title="57-lsi-19" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>20 0.23207243 <a title="57-lsi-20" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.047), (7, 0.069), (12, 0.022), (28, 0.197), (42, 0.383), (57, 0.022), (63, 0.022), (77, 0.062), (83, 0.037)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77295262 <a title="57-lda-1" href="./nips-2008-Deflation_Methods_for_Sparse_PCA.html">57 nips-2008-Deflation Methods for Sparse PCA</a></p>
<p>Author: Lester W. Mackey</p><p>Abstract: In analogy to the PCA setting, the sparse PCA problem is often solved by iteratively alternating between two subtasks: cardinality-constrained rank-one variance maximization and matrix deﬂation. While the former has received a great deal of attention in the literature, the latter is seldom analyzed and is typically borrowed without justiﬁcation from the PCA context. In this work, we demonstrate that the standard PCA deﬂation procedure is seldom appropriate for the sparse PCA setting. To rectify the situation, we ﬁrst develop several deﬂation alternatives better suited to the cardinality-constrained context. We then reformulate the sparse PCA optimization problem to explicitly reﬂect the maximum additional variance objective on each round. The result is a generalized deﬂation procedure that typically outperforms more standard techniques on real-world datasets. 1</p><p>2 0.58057725 <a title="57-lda-2" href="./nips-2008-Rademacher_Complexity_Bounds_for_Non-I.I.D._Processes.html">189 nips-2008-Rademacher Complexity Bounds for Non-I.I.D. Processes</a></p>
<p>Author: Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents the ﬁrst Rademacher complexity-based error bounds for noni.i.d. settings, a generalization of similar existing bounds derived for the i.i.d. case. Our bounds hold in the scenario of dependent samples generated by a stationary β-mixing process, which is commonly adopted in many previous studies of noni.i.d. settings. They beneﬁt from the crucial advantages of Rademacher complexity over other measures of the complexity of hypothesis classes. In particular, they are data-dependent and measure the complexity of a class of hypotheses based on the training sample. The empirical Rademacher complexity can be estimated from such ﬁnite samples and lead to tighter generalization bounds. We also present the ﬁrst margin bounds for kernel-based classiﬁcation in this non-i.i.d. setting and brieﬂy study their convergence.</p><p>3 0.50295126 <a title="57-lda-3" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>Author: David Sontag, Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: We propose a new class of consistency constraints for Linear Programming (LP) relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and signiﬁcantly faster than a method that does not use partitioning. 1</p><p>4 0.502886 <a title="57-lda-4" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>5 0.50115627 <a title="57-lda-5" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Recently, supervised dimensionality reduction has been gaining attention, owing to the realization that data labels are often available and indicate important underlying structure in the data. In this paper, we present a novel convex supervised dimensionality reduction approach based on exponential family PCA, which is able to avoid the local optima of typical EM learning. Moreover, by introducing a sample-based approximation to exponential family models, it overcomes the limitation of the prevailing Gaussian assumptions of standard PCA, and produces a kernelized formulation for nonlinear supervised dimensionality reduction. A training algorithm is then devised based on a subgradient bundle method, whose scalability can be gained using a coordinate descent procedure. The advantage of our global optimization approach is demonstrated by empirical results over both synthetic and real data. 1</p><p>6 0.49948582 <a title="57-lda-6" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>7 0.4992778 <a title="57-lda-7" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>8 0.49903333 <a title="57-lda-8" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>9 0.49902686 <a title="57-lda-9" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>10 0.49798471 <a title="57-lda-10" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>11 0.49761033 <a title="57-lda-11" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>12 0.49722868 <a title="57-lda-12" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>13 0.49683324 <a title="57-lda-13" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>14 0.49561739 <a title="57-lda-14" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>15 0.49546078 <a title="57-lda-15" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>16 0.49520597 <a title="57-lda-16" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>17 0.49514014 <a title="57-lda-17" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>18 0.49512357 <a title="57-lda-18" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>19 0.49493676 <a title="57-lda-19" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>20 0.49452335 <a title="57-lda-20" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
