<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-142" href="#">nips2008-142</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</h1>
<br/><p>Source: <a title="nips-2008-142-pdf" href="http://papers.nips.cc/paper/3598-multi-level-active-prediction-of-useful-image-annotations-for-recognition.pdf">pdf</a></p><p>Author: Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples. We propose to allow the categorylearner to strategically choose what annotations it receives—based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation. We construct a multiple-instance discriminative classiﬁer based on the initial training data. Then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next. After each request, the current classiﬁer is incrementally updated. Unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e.g., a full segmentation on some images and a present/absent ﬂag on others). As a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort. 1</p><p>Reference: <a title="nips-2008-142-reference" href="../nips2008_reference/nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples. [sent-3, score-0.595]
</p><p>2 We propose to allow the categorylearner to strategically choose what annotations it receives—based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation. [sent-4, score-0.333]
</p><p>3 Then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next. [sent-6, score-0.932]
</p><p>4 Unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e. [sent-8, score-0.612]
</p><p>5 As a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort. [sent-11, score-0.524]
</p><p>6 The extent of an image labeling can range from a ﬂag telling whether the object of interest is present or absent, to a full segmentation specifying the object boundary. [sent-17, score-0.387]
</p><p>7 Meanwhile, the learning algorithm must be able to accommodate the multiple levels of granularity that may occur in provided image annotations, and to compute which item at which of those levels appears to be most fruitful to have labeled next (see Figure 1). [sent-25, score-0.446]
</p><p>8 Useful image annotations can occur at multiple levels of granularity. [sent-28, score-0.421]
</p><p>9 Left: For example, a learner may only know whether the image contains a particular object or not (top row, dotted boxes denote object is present), or it may also have segmented foregrounds (middle row), or it may have detailed outlines of object parts (bottom row). [sent-29, score-0.428]
</p><p>10 The learner may only be given the noisy groups and told that each includes at least one instance of the speciﬁed class (top), or, for some groups, the individual example images may be labeled as positive or negative (bottom). [sent-31, score-0.498]
</p><p>11 We propose an active learning paradigm that directs manual annotation effort to the most informative examples and levels. [sent-32, score-0.793]
</p><p>12 To address this challenge, we propose a method that actively targets the learner’s requests for supervision so as to maximize the expected beneﬁt to the category models. [sent-33, score-0.35]
</p><p>13 Our method constructs an initial classiﬁer from limited labeled data, and then considers all remaining unlabeled and weakly labeled examples to determine what annotation seems most informative to obtain. [sent-34, score-0.836]
</p><p>14 Since the varying levels of annotation demand varying degrees of manual effort, our active selection process weighs the value of the information gain against the cost of actually obtaining any given annotation. [sent-35, score-0.965]
</p><p>15 Our approach accounts for the fact that image annotations can exist at multiple levels of granularity: both the classiﬁer and active selection objectives are formulated to accommodate dual-layer labels. [sent-37, score-0.773]
</p><p>16 To achieve this duality for the classiﬁer, we express the problem in the multiple instance learning (MIL) setting [9], where training examples are speciﬁed as bags of the ﬁner granularity instances, and positive bags may contain an arbitrary number of negatives. [sent-38, score-0.892]
</p><p>17 To achieve the duality for the active selection, we design a decision-theoretic criterion that balances the variable costs associated with each type of annotation with the expected gain in information. [sent-39, score-0.603]
</p><p>18 Essentially this allows the learner to automatically predict when the extra effort of a more precise annotation is warranted. [sent-40, score-0.423]
</p><p>19 The main contribution of this work is a uniﬁed framework to actively learn categories from a mixture of weakly and strongly labeled examples. [sent-41, score-0.454]
</p><p>20 We are the ﬁrst to identify and address the problem of active visual category learning with multi-level annotations. [sent-42, score-0.446]
</p><p>21 Not only does our active strategy learn more quickly than a random selection baseline, but for a ﬁxed amount of manual resources, it yields more accurate models than conventional single-layer active selection strategies. [sent-44, score-0.79]
</p><p>22 Recent methods have shown the possibility of learning visual patterns from unlabeled [3, 2] image collections, while other techniques aim to share or re-use knowledge across categories [10, 4]. [sent-46, score-0.458]
</p><p>23 Using weakly labeled images to learn categories was proposed in [1], and several researchers have shown that MIL can accommodate the weak or noisy supervision often available for image data [11–14]. [sent-48, score-0.595]
</p><p>24 Working in the other direction, some research seeks to facilitate the manual labor of image annotation, tempting users with games or nice datasets [7, 8]. [sent-49, score-0.322]
</p><p>25 However, when faced with a distribution of unlabeled images, almost all existing methods for visual category learning are essentially passive, selecting points at random to label. [sent-50, score-0.391]
</p><p>26 Our active selection procedure is in part inspired by this work, as it  also seeks to balance the cost and utility tradeoff. [sent-55, score-0.441]
</p><p>27 Recent work has considered active learning with Gaussian Process classiﬁers [19], and relevance feedback for video annotations [20]. [sent-56, score-0.493]
</p><p>28 In contrast, we show how to form active multiple-instance learners, where constraints or labels must be sought at multiple levels of granularity. [sent-57, score-0.442]
</p><p>29 Further, we introduce the notion of predicting when to “invest” the labor of more expensive image annotations so as to ultimately yield bigger beneﬁts to the classiﬁer. [sent-58, score-0.412]
</p><p>30 Unlike any previous work, our method continually guides the annotation process to the appropriate level of supervision. [sent-59, score-0.383]
</p><p>31 While an active criterion for instance-level queries is suggested in [21] and applied within an MI learner, it cannot actively select positive bags or unlabeled bags, and does not consider the cost of obtaining the labels requested. [sent-60, score-1.328]
</p><p>32 The key idea is to actively determine which annotations a user should be asked to provide, and in what order. [sent-64, score-0.379]
</p><p>33 We consider image collections consisting of a variety of supervisory information: some images are labeled as containing the category of interest (or not), some have both a class label and a foreground segmentation, while others have no annotations at all. [sent-65, score-0.821]
</p><p>34 We derive an active learning criterion function that predicts how informative further annotation on any particular unlabeled image or region would be, while accounting for the variable expense associated with different annotation types. [sent-66, score-1.146]
</p><p>35 As long as the information expected from further annotations outweighs the cost of obtaining them, our algorithm will request the next valuable label, re-train the classiﬁer, and repeat. [sent-67, score-0.529]
</p><p>36 However, the fact that image annotations can exist at multiple levels of granularity demands a learning algorithm that can encode any known labels at the levels they occur, and so MIL [9] is more applicable. [sent-73, score-0.619]
</p><p>37 In MIL, the learner is instead provided with sets (bags) of patterns rather than individual patterns, and is only told that at least one member of any positive bag is truly positive, while every member of any negative bag is guaranteed to be negative. [sent-74, score-0.615]
</p><p>38 MIL is well-suited for the following two image classiﬁcation scenarios: • Training images are labeled as to whether they contain the category of interest, but they also contain other objects and background clutter. [sent-76, score-0.524]
</p><p>39 Every image is represented by a bag of regions, each of which is characterized by its color, texture, shape, etc. [sent-77, score-0.318]
</p><p>40 The goal is to predict when new image regions contain the object—that is, to learn to label regions as foreground or background. [sent-80, score-0.348]
</p><p>41 We integrate our active selection method with the SVM-based MIL approach given in [22], which uses a Normalized Set Kernel (NSK) to describe bags based on the average representation of instances within them. [sent-87, score-0.766]
</p><p>42 Following [23], we use the NSK mapping for positive bags only; all instances in a negative bag are treated individually as negative. [sent-88, score-0.78]
</p><p>43 Whereas active selection criteria for traditional supervised classiﬁers need only identify the best instance to label next, in the MIL domain we have a more complex choice. [sent-95, score-0.485]
</p><p>44 There are three possible types of request: the system can ask for a label on an instance, a label on an unlabeled bag, or for a joint labeling of all instances within a positive bag. [sent-96, score-0.673]
</p><p>45 So, we must design a selection criterion that simultaneously determines which type of annotation to request, and for which example to request it. [sent-97, score-0.435]
</p><p>46 Adding to the challenge, the selection process must also account for the variable costs associated with each level of annotation (e. [sent-98, score-0.452]
</p><p>47 We extend the value of information (VOI) strategy proposed in [18] to enable active MIL selection, and derive a generalized value function that can accept both instances and bags. [sent-101, score-0.411]
</p><p>48 This allows us to predict the information gain in a joint labeling of multiple instances at once, and thereby actively choose when it is worthwhile to expend more or less manual effort in the training process. [sent-102, score-0.641]
</p><p>49 Our method continually re-evaluates the expected signiﬁcance of knowing more about any unlabeled or partially labeled example, as quantiﬁed by the predicted reduction in misclassiﬁcation risk plus the cost of obtaining the label. [sent-103, score-0.696]
</p><p>50 We consider a collection of unlabeled data XU , and labeled data XL composed of a set of positive ˜ bags Xp and a set of negative instances Xn . [sent-104, score-0.896]
</p><p>51 Recall that positively labeled bags contain instances whose labels are unknown, since they contain an unknown mix of positive and negative instances. [sent-105, score-0.841]
</p><p>52 Let rp denote the user-speciﬁed risk associated with misclassifying a positive example as negative, and rn denote the risk of misclassifying a negative. [sent-106, score-0.492]
</p><p>53 The risk associated with the labeled data is: Risk(XL ) =  rp (1 − p(Xi )) +  rn p(xi ),  (1)  ˜ xi ∈Xn  Xi ∈Xp  where xi denotes an instance and Xi denotes a bag. [sent-107, score-0.431]
</p><p>54 The corresponding risk for unlabeled data is: Risk(XU ) =  rp (1 − p(xi )) Pr(yi = +1|xi ) + rn p(xi )(1 − Pr(yi = +1|xi )),  (2)  xi ∈XU  where yi is the true label for unlabeled example xi . [sent-111, score-0.785]
</p><p>55 This simpliﬁes the risk for the unlabeled data to: Risk(XU ) = xi ∈XU (rp + rn )(1 − p(xi ))p(xi ), where again we transform unlabeled bags according to the NSK before computing the posterior. [sent-113, score-0.93]
</p><p>56 If the VOI is high for a  given input, then the total cost would be decreased by adding its annotation; similarly, low values indicate minor gains, and negative values indicate an annotation that costs more to obtain than it is worth. [sent-117, score-0.471]
</p><p>57 Thus at each iteration, the active learner surveys all remaining unlabeled and weakly labeled examples, computes their VOI, and requests the label for the example with the maximal value. [sent-118, score-0.867]
</p><p>58 Secondly, for active selection to proceed at multiple levels, the VOI must act as an overloaded function: we need to be able to evaluate the VOI when z is an unlabeled instance or an unlabeled bag or a weakly labeled example, i. [sent-121, score-1.24]
</p><p>59 , a positive bag containing an unknown number of negative instances. [sent-123, score-0.337]
</p><p>60 Similarly, if z is an unlabeled bag, the label assignment can only be positive or negative, and we compute the probability of either label via the NSK mapping. [sent-127, score-0.462]
</p><p>61 For positive bag z, the expected total risk is then the average risk computed over all S generated samples: 1 E= S  S  Risk({XL  (a1 )k  z} ∪ {z1  (a  )  , . [sent-139, score-0.609]
</p><p>62 To compute the risk on XL for each ﬁxed sample we simply remove the weakly labeled positive bag z, and insert its instances as labeled positives and negatives, as dictated by the sample’s label assignment. [sent-146, score-1.008]
</p><p>63 To complete our active selection function, we must deﬁne the cost function C(z), which maps an input to the amount of effort required to annotate it. [sent-148, score-0.532]
</p><p>64 We can now actively select which examples and what type of annotation to request, so as to maximize the expected beneﬁt to the category model relative to the manual effort expended. [sent-152, score-0.768]
</p><p>65 After each annotation is added and the classiﬁer is revised accordingly, the VOI is evaluated on the remaining unlabeled and weakly labeled data in order to choose the next annotation. [sent-153, score-0.671]
</p><p>66 This process repeats either until the available amount of manual resources is exhausted, or, alternatively, until the maximum VOI is negative, indicating further annotations are not worth the effort. [sent-154, score-0.34]
</p><p>67 We provide comparisons with single-level active learning (with both the method of [21], and where the same VOI function is used but is restricted to actively label only instances), as well as passive learning. [sent-158, score-0.567]
</p><p>68 2 To determine how much more labeling a positive bag costs relative to labeling an instance, we performed user studies for both of the scenarios evaluated. [sent-160, score-0.508]
</p><p>69 For the ﬁrst scenario, users were shown oversegmented images and had to click on all the segments belonging to the object of interest. [sent-161, score-0.357]
</p><p>70 In the second, users were shown a page of downloaded Web images and had to click on only those images containing the object of interest. [sent-162, score-0.426]
</p><p>71 For segmentation, obtaining labels on all positive segments took users on average four times as much time as setting a ﬂag. [sent-164, score-0.336]
</p><p>72 3 times as long to identify all positives within bags of 25 noisy images. [sent-166, score-0.33]
</p><p>73 Thus we set the cost of labeling a positive bag to 4 and 6. [sent-167, score-0.474]
</p><p>74 These values agree with the average sparsity of the two datasets: the Google set contains about 30% true positive images while the SIVAL set contains 10% positive segments per image. [sent-169, score-0.324]
</p><p>75 Thus each image is a bag containing both positive and negative instances (segments). [sent-175, score-0.59]
</p><p>76 Labels on the training data specify whether the object of interest is present or not, but the segments themselves are unlabeled (though the dataset does provide ground truth segment labels for evaluation purposes). [sent-176, score-0.434]
</p><p>77 Our active learning method must choose its queries from among 10 positive bags (complete segmentations), 300 unlabeled instances (individual segments), and about 150 unlabeled bags (present/absent ﬂag on the image). [sent-178, score-1.558]
</p><p>78 All methods are given a ﬁxed amount of manual effort (40 cost units) and are allowed to make a sequence of choices until that cost is used up. [sent-182, score-0.443]
</p><p>79 Recall that a cost of 40 could correspond, for example, to obtaining labels on 40 40 1 = 40 instances or 4 = 10 positive bags, or some mixture thereof. [sent-183, score-0.494]
</p><p>80 Figure 2(b) summarizes the learning curves for all categories, in terms of the average improvement at a ﬁxed point midway through the active learning phase. [sent-184, score-0.311]
</p><p>81 This is because single-level active selection can only make a sequence of greedy choices while our approach can jointly select bags of instances to query. [sent-190, score-0.766]
</p><p>82 (b) Summary of the average improvement over all categories after half of the annotation cost is used. [sent-200, score-0.454]
</p><p>83 For the same amount of annotation cost, our multi-level approach learns more quickly than both traditional single-level active selection as well as both forms of random selection. [sent-201, score-0.613]
</p><p>84 Our method tends to request complete segmentations or image labels early on, followed by queries on unlabeled segments later on. [sent-218, score-0.62]
</p><p>85 For both methods, the percent gains decrease with increasing cost; this makes sense, since eventually (for enough manual effort) a passive learner can begin to catch up to an active learner. [sent-228, score-0.566]
</p><p>86 2 Actively Learning Visual Categories from Web Images Next we evaluate the scenario where each positive bag is a collection of images, among which only a portion are actually positive instances for the class of interest. [sent-230, score-0.535]
</p><p>87 We show how to boost accuracy with these types of learners while leveraging minimal manual annotation effort. [sent-236, score-0.377]
</p><p>88 To re-use the publicly available dataset from [5], we randomly group Google images into bags of size 25 to simulate multiple searches as in [11], yielding about 30 bags per category. [sent-237, score-0.752]
</p><p>89 We randomly select 10 positive and 10 negative bags (from all other categories) to serve as the initial training data for each class. [sent-238, score-0.431]
</p><p>90 The rest of the positive bags of a class are used to construct the test sets. [sent-239, score-0.407]
</p><p>91 We represent each image as a bag of “visual words”, and compare examples with a linear kernel. [sent-241, score-0.344]
</p><p>92 Our method makes active queries among 10 positive bags (complete labels) and about 250 unlabeled instances (images). [sent-242, score-1.043]
</p><p>93 There are no unlabeled bags in this scenario, since every downloaded batch is associated with a keyword. [sent-243, score-0.555]
</p><p>94 Our multi-level active approach outperforms both random selection strategies and traditional single-level active selection. [sent-247, score-0.621]
</p><p>95 Figure 4 shows the learning curves and a summary of our active learner’s performance. [sent-248, score-0.311]
</p><p>96 On this dataset, random selection with multi-level annotations actually outperforms random selection on single-level annotations (see the boxplots). [sent-250, score-0.556]
</p><p>97 We attribute this to the distribution of bags/instances: on average more positive bags were randomly chosen, and each addition led to a larger increase in the AUROC. [sent-251, score-0.407]
</p><p>98 5  Conclusions and Future Work  Our approach addresses a new problem: how to actively choose not only which instance to label, but also what type of image annotation to acquire in a cost-effective way. [sent-252, score-0.576]
</p><p>99 Our method is general enough to accept other types of annotations or classiﬁers, as long as the cost and risk functions can be appropriately deﬁned. [sent-253, score-0.503]
</p><p>100 Comparisons with passive learning methods and single-level active learning show that our multi-level method is better-suited for building classiﬁers with minimal human intervention. [sent-254, score-0.328]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bags', 0.301), ('active', 0.269), ('mil', 0.266), ('annotation', 0.261), ('annotations', 0.224), ('unlabeled', 0.214), ('multi', 0.213), ('bag', 0.207), ('xl', 0.172), ('voi', 0.166), ('risk', 0.161), ('actively', 0.155), ('instances', 0.142), ('request', 0.12), ('category', 0.119), ('cost', 0.118), ('sival', 0.116), ('manual', 0.116), ('images', 0.113), ('image', 0.111), ('labeled', 0.109), ('xu', 0.103), ('level', 0.095), ('effort', 0.091), ('labels', 0.087), ('weakly', 0.087), ('label', 0.084), ('nsk', 0.083), ('object', 0.082), ('positive', 0.08), ('classi', 0.076), ('categories', 0.075), ('er', 0.072), ('learner', 0.071), ('labeling', 0.069), ('obtaining', 0.067), ('auroc', 0.066), ('granularity', 0.062), ('passive', 0.059), ('visual', 0.058), ('ag', 0.056), ('selection', 0.054), ('ought', 0.053), ('segments', 0.051), ('gains', 0.051), ('users', 0.051), ('negative', 0.05), ('vijayanarasimhan', 0.05), ('grauman', 0.05), ('google', 0.05), ('roc', 0.05), ('levels', 0.049), ('instance', 0.049), ('pr', 0.049), ('web', 0.048), ('labor', 0.044), ('segmentation', 0.043), ('supervision', 0.043), ('curves', 0.042), ('costs', 0.042), ('scenarios', 0.041), ('downloaded', 0.04), ('selections', 0.04), ('xi', 0.04), ('multiple', 0.037), ('queries', 0.037), ('contain', 0.036), ('sgn', 0.036), ('foreground', 0.035), ('ers', 0.034), ('expensive', 0.033), ('craven', 0.033), ('finer', 0.033), ('kapoor', 0.033), ('oversegmented', 0.033), ('requests', 0.033), ('zmm', 0.033), ('rp', 0.032), ('gain', 0.031), ('cluttered', 0.03), ('informative', 0.03), ('misclassifying', 0.029), ('miu', 0.029), ('traditional', 0.029), ('accommodate', 0.029), ('positives', 0.029), ('learn', 0.028), ('area', 0.027), ('regions', 0.027), ('boxplots', 0.027), ('requested', 0.027), ('click', 0.027), ('continually', 0.027), ('class', 0.026), ('attribute', 0.026), ('examples', 0.026), ('freeman', 0.025), ('selective', 0.025), ('ray', 0.025), ('austin', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999905 <a title="142-tfidf-1" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>Author: Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples. We propose to allow the categorylearner to strategically choose what annotations it receives—based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation. We construct a multiple-instance discriminative classiﬁer based on the initial training data. Then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next. After each request, the current classiﬁer is incrementally updated. Unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e.g., a full segmentation on some images and a present/absent ﬂag on others). As a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort. 1</p><p>2 0.21546483 <a title="142-tfidf-2" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>Author: Rui M. Castro, Charles Kalish, Robert Nowak, Ruichen Qian, Tim Rogers, Xiaojin Zhu</p><p>Abstract: We investigate a topic at the interface of machine learning and cognitive science. Human active learning, where learners can actively query the world for information, is contrasted with passive learning from random examples. Furthermore, we compare human active learning performance with predictions from statistical learning theory. We conduct a series of human category learning experiments inspired by a machine learning task for which active and passive learning error bounds are well understood, and dramatically distinct. Our results indicate that humans are capable of actively selecting informative queries, and in doing so learn better and faster than if they are given random training data, as predicted by learning theory. However, the improvement over passive learning is not as dramatic as that achieved by machine active learning algorithms. To the best of our knowledge, this is the ﬁrst quantitative study comparing human category learning in active versus passive settings. 1</p><p>3 0.15683903 <a title="142-tfidf-3" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>4 0.14808832 <a title="142-tfidf-4" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>5 0.14140566 <a title="142-tfidf-5" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>6 0.13571751 <a title="142-tfidf-6" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>7 0.13530883 <a title="142-tfidf-7" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>8 0.13158678 <a title="142-tfidf-8" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>9 0.12846683 <a title="142-tfidf-9" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>10 0.1283213 <a title="142-tfidf-10" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>11 0.12559442 <a title="142-tfidf-11" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>12 0.12023581 <a title="142-tfidf-12" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>13 0.11609368 <a title="142-tfidf-13" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>14 0.10671686 <a title="142-tfidf-14" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>15 0.094284914 <a title="142-tfidf-15" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>16 0.091500856 <a title="142-tfidf-16" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>17 0.087489888 <a title="142-tfidf-17" href="./nips-2008-Translated_Learning%3A_Transfer_Learning_across_Different_Feature_Spaces.html">242 nips-2008-Translated Learning: Transfer Learning across Different Feature Spaces</a></p>
<p>18 0.086499386 <a title="142-tfidf-18" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>19 0.083908305 <a title="142-tfidf-19" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>20 0.082332842 <a title="142-tfidf-20" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.241), (1, -0.168), (2, 0.038), (3, -0.19), (4, -0.105), (5, 0.069), (6, 0.007), (7, -0.103), (8, 0.034), (9, 0.102), (10, 0.064), (11, -0.001), (12, -0.071), (13, -0.273), (14, -0.009), (15, 0.007), (16, -0.084), (17, 0.074), (18, 0.069), (19, -0.02), (20, 0.05), (21, 0.007), (22, 0.084), (23, -0.032), (24, -0.03), (25, -0.09), (26, 0.007), (27, -0.08), (28, 0.02), (29, -0.056), (30, 0.013), (31, 0.116), (32, -0.154), (33, -0.135), (34, -0.017), (35, 0.062), (36, -0.046), (37, -0.006), (38, 0.072), (39, -0.057), (40, -0.066), (41, -0.016), (42, -0.062), (43, -0.018), (44, 0.123), (45, -0.022), (46, 0.03), (47, -0.057), (48, 0.034), (49, -0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97160852 <a title="142-lsi-1" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>Author: Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples. We propose to allow the categorylearner to strategically choose what annotations it receives—based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation. We construct a multiple-instance discriminative classiﬁer based on the initial training data. Then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next. After each request, the current classiﬁer is incrementally updated. Unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e.g., a full segmentation on some images and a present/absent ﬂag on others). As a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort. 1</p><p>2 0.70910406 <a title="142-lsi-2" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>Author: Rui M. Castro, Charles Kalish, Robert Nowak, Ruichen Qian, Tim Rogers, Xiaojin Zhu</p><p>Abstract: We investigate a topic at the interface of machine learning and cognitive science. Human active learning, where learners can actively query the world for information, is contrasted with passive learning from random examples. Furthermore, we compare human active learning performance with predictions from statistical learning theory. We conduct a series of human category learning experiments inspired by a machine learning task for which active and passive learning error bounds are well understood, and dramatically distinct. Our results indicate that humans are capable of actively selecting informative queries, and in doing so learn better and faster than if they are given random training data, as predicted by learning theory. However, the improvement over passive learning is not as dramatic as that achieved by machine active learning algorithms. To the best of our knowledge, this is the ﬁrst quantitative study comparing human category learning in active versus passive settings. 1</p><p>3 0.62882113 <a title="142-lsi-3" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>4 0.62397122 <a title="142-lsi-4" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>Author: Geremy Heitz, Stephen Gould, Ashutosh Saxena, Daphne Koller</p><p>Abstract: One of the original goals of computer vision was to fully understand a natural scene. This requires solving several sub-problems simultaneously, including object detection, region labeling, and geometric reasoning. The last few decades have seen great progress in tackling each of these problems in isolation. Only recently have researchers returned to the difﬁcult task of considering them jointly. In this work, we consider learning a set of related models in such that they both solve their own problem and help each other. We develop a framework called Cascaded Classiﬁcation Models (CCM), where repeated instantiations of these classiﬁers are coupled by their input/output variables in a cascade that improves performance at each level. Our method requires only a limited “black box” interface with the models, allowing us to use very sophisticated, state-of-the-art classiﬁers without having to look under the hood. We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d reconstruction. 1</p><p>5 0.6194762 <a title="142-lsi-5" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>Author: Tae-kyun Kim, Roberto Cipolla</p><p>Abstract: We present a new co-clustering problem of images and visual features. The problem involves a set of non-object images in addition to a set of object images and features to be co-clustered. Co-clustering is performed in a way that maximises discrimination of object images from non-object images, thus emphasizing discriminative features. This provides a way of obtaining perceptual joint-clusters of object images and features. We tackle the problem by simultaneously boosting multiple strong classiﬁers which compete for images by their expertise. Each boosting classiﬁer is an aggregation of weak-learners, i.e. simple visual features. The obtained classiﬁers are useful for object detection tasks which exhibit multimodalities, e.g. multi-category and multi-view object detection tasks. Experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classiﬁers in object detection tasks. 1</p><p>6 0.60902834 <a title="142-lsi-6" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>7 0.59982622 <a title="142-lsi-7" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>8 0.58844632 <a title="142-lsi-8" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>9 0.54705727 <a title="142-lsi-9" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>10 0.53745961 <a title="142-lsi-10" href="./nips-2008-Breaking_Audio_CAPTCHAs.html">41 nips-2008-Breaking Audio CAPTCHAs</a></p>
<p>11 0.50085765 <a title="142-lsi-11" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>12 0.49277985 <a title="142-lsi-12" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>13 0.48896018 <a title="142-lsi-13" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>14 0.48843724 <a title="142-lsi-14" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>15 0.46317396 <a title="142-lsi-15" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>16 0.41296881 <a title="142-lsi-16" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>17 0.41225669 <a title="142-lsi-17" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>18 0.4103905 <a title="142-lsi-18" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>19 0.39750585 <a title="142-lsi-19" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>20 0.39503649 <a title="142-lsi-20" href="./nips-2008-Translated_Learning%3A_Transfer_Learning_across_Different_Feature_Spaces.html">242 nips-2008-Translated Learning: Transfer Learning across Different Feature Spaces</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.087), (7, 0.063), (12, 0.05), (15, 0.01), (28, 0.137), (57, 0.077), (59, 0.017), (63, 0.019), (65, 0.238), (71, 0.03), (77, 0.039), (78, 0.021), (83, 0.119)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81502783 <a title="142-lda-1" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>Author: Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples. We propose to allow the categorylearner to strategically choose what annotations it receives—based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation. We construct a multiple-instance discriminative classiﬁer based on the initial training data. Then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next. After each request, the current classiﬁer is incrementally updated. Unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e.g., a full segmentation on some images and a present/absent ﬂag on others). As a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort. 1</p><p>2 0.77230918 <a title="142-lda-2" href="./nips-2008-Understanding_Brain_Connectivity_Patterns_during_Motor_Imagery_for_Brain-Computer_Interfacing.html">243 nips-2008-Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing</a></p>
<p>Author: Moritz Grosse-wentrup</p><p>Abstract: EEG connectivity measures could provide a new type of feature space for inferring a subject’s intention in Brain-Computer Interfaces (BCIs). However, very little is known on EEG connectivity patterns for BCIs. In this study, EEG connectivity during motor imagery (MI) of the left and right is investigated in a broad frequency range across the whole scalp by combining Beamforming with Transfer Entropy and taking into account possible volume conduction effects. Observed connectivity patterns indicate that modulation intentionally induced by MI is strongest in the γ-band, i.e., above 35 Hz. Furthermore, modulation between MI and rest is found to be more pronounced than between MI of different hands. This is in contrast to results on MI obtained with bandpower features, and might provide an explanation for the so far only moderate success of connectivity features in BCIs. It is concluded that future studies on connectivity based BCIs should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions. 1</p><p>3 0.66948444 <a title="142-lda-3" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>4 0.6689921 <a title="142-lda-4" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>5 0.6647144 <a title="142-lda-5" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>6 0.66341257 <a title="142-lda-6" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>7 0.66261297 <a title="142-lda-7" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>8 0.6625011 <a title="142-lda-8" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>9 0.66232097 <a title="142-lda-9" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>10 0.66163075 <a title="142-lda-10" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>11 0.6613512 <a title="142-lda-11" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>12 0.66054362 <a title="142-lda-12" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>13 0.65639079 <a title="142-lda-13" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>14 0.65441835 <a title="142-lda-14" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>15 0.65193117 <a title="142-lda-15" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>16 0.65156245 <a title="142-lda-16" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>17 0.65051192 <a title="142-lda-17" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>18 0.64999044 <a title="142-lda-18" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>19 0.64987427 <a title="142-lda-19" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>20 0.64902568 <a title="142-lda-20" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
