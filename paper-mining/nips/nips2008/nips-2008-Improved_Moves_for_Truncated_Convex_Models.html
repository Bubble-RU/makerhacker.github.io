<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>104 nips-2008-Improved Moves for Truncated Convex Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-104" href="#">nips2008-104</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>104 nips-2008-Improved Moves for Truncated Convex Models</h1>
<br/><p>Source: <a title="nips-2008-104-pdf" href="http://papers.nips.cc/paper/3618-improved-moves-for-truncated-convex-models.pdf">pdf</a></p><p>Author: Philip Torr, M. P. Kumar</p><p>Abstract: We consider the problem of obtaining the approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. For this problem, we propose an improved st-MINCUT based move making algorithm. Unlike previous move making approaches, which either provide a loose bound or no bound on the quality of the solution (in terms of the corresponding Gibbs energy), our algorithm achieves the same guarantees as the standard linear programming (LP) relaxation. Compared to previous approaches based on the LP relaxation, e.g. interior-point algorithms or treereweighted message passing (TRW), our method is faster as it uses only the efﬁcient st-MINCUT algorithm in its design. Furthermore, it directly provides us with a primal solution (unlike TRW and other related methods which solve the dual of the LP). We demonstrate the effectiveness of the proposed approach on both synthetic and standard real data problems. Our analysis also opens up an interesting question regarding the relationship between move making algorithms (such as α-expansion and the algorithms presented in this paper) and the randomized rounding schemes used with convex relaxations. We believe that further explorations in this direction would help design efﬁcient algorithms for more complex relaxations.</p><p>Reference: <a title="nips-2008-104-reference" href="../nips2008_reference/nips-2008-Improved_Moves_for_Truncated_Convex_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We consider the problem of obtaining the approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. [sent-12, score-0.734]
</p><p>2 Unlike previous move making approaches, which either provide a loose bound or no bound on the quality of the solution (in terms of the corresponding Gibbs energy), our algorithm achieves the same guarantees as the standard linear programming (LP) relaxation. [sent-14, score-0.275]
</p><p>3 Our analysis also opens up an interesting question regarding the relationship between move making algorithms (such as α-expansion and the algorithms presented in this paper) and the randomized rounding schemes used with convex relaxations. [sent-20, score-0.4]
</p><p>4 The word ‘discrete’ refers to the fact that each of the random variables va ∈ v = {v0 , · · · , vn−1 } can take one label from a discrete set l = {l0 , · · · , lh−1 }. [sent-28, score-0.257]
</p><p>5 An MRF deﬁnes a neighbourhood relationship (denoted by E) over the random variables such that (a, b) ∈ E if, and only if, va and vb are neighbouring random variables. [sent-30, score-0.418]
</p><p>6 Given an MRF, a labelling refers to a function f such that f : {0, · · · , n − 1} −→ {0, · · · , h − 1}. [sent-31, score-0.34]
</p><p>7 In other words, the function f assigns to each random variable va ∈ v, a label lf (a) ∈ l. [sent-32, score-0.236]
</p><p>8 The probability of the labelling is given by the following Gibbs distribution: Pr(f, D|θ) = exp(−Q(f, D; θ))/Z(θ), where θ is the parameter of the MRF and Z(θ) is the normalization constant (i. [sent-33, score-0.34]
</p><p>9 Assuming a pairwise MRF, the Gibbs energy is given by: 1 θa;f (a) +  Q(f, D; θ) = va ∈v  2 θab;f (a)f (b) ,  (1)  (a,b)∈E  1 2 where θa;f (a) and θab;f (a)f (b) are the unary and pairwise potentials respectively. [sent-36, score-0.799]
</p><p>10 The superscripts ‘1’ and ‘2’ indicate that the unary potential depends on the labelling of one random variable at a time, while the pairwise potential depends on the labelling of two neighbouring random variables. [sent-37, score-1.007]
</p><p>11 Clearly, the labelling f which maximizes the posterior Pr(f, D|θ) can be obtained by minimizing the Gibbs energy. [sent-38, score-0.34]
</p><p>12 The problem of obtaining such a labelling f is known as maximum a posteriori 1  (MAP) estimation. [sent-39, score-0.34]
</p><p>13 In this paper, we consider the problem of MAP estimation of random ﬁelds where the pairwise potentials are deﬁned by truncated convex models [4]. [sent-40, score-0.754]
</p><p>14 Formally speaking, the pairwise potentials are of the form 2 θab;f (a)f (b) = wab min{d(f (a) − f (b)), M }  (2)  where wab ≥ 0 for all (a, b) ∈ E, d(·) is a convex function and M > 0 is the truncation factor. [sent-41, score-0.825]
</p><p>15 Examples of pairwise potentials of this form include the truncated linear metric and the truncated quadratic semi-metric, i. [sent-45, score-1.008]
</p><p>16 2 2 θab;f (a)f (b) = wab min{|f (a) − f (b)|, M }, θab;f (a)f (b) = wab min{(f (a) − f (b))2 , M }. [sent-47, score-0.394]
</p><p>17 (3)  Before proceeding further, we would like to note here that the method presented in this paper can be trivially extended to truncated submodular models (a generalization of truncated convex models). [sent-48, score-0.712]
</p><p>18 However, we will restrict our discussion to truncated convex models for two reasons: (i) it makes the analysis of our approach easier; and (ii) truncated convex pairwise potentials are commonly used in several problems such as stereo reconstruction, image denoising and inpainting [22]. [sent-49, score-1.183]
</p><p>19 Given their widespread use, it is not surprising that several approximate MAP estimation algorithms have been proposed in the literature for the truncated convex model. [sent-54, score-0.443]
</p><p>20 1 Related Work Given a random ﬁeld with truncated convex pairwise potentials, Felzenszwalb and Huttenlocher [6] improved the efﬁciency of the popular max-product belief propagation (BP) algorithm [19] to obtain the MAP estimate. [sent-57, score-0.531]
</p><p>21 However, for a general MRF , BP provides no bounds on the quality of the approximate MAP labelling obtained. [sent-61, score-0.379]
</p><p>22 the labelling f ) is often obtained from the dual solution in an unprincipled manner1. [sent-69, score-0.373]
</p><p>23 [5] showed that when using certain randomized rounding schemes on the primal solution (to get the ﬁnal labelling f ), the following guarantees hold true: (i) for Potts model (i. [sent-77, score-0.484]
</p><p>24 d(f (a) − f (b)) = |f (a) − f (b)| and M = 1), we obtain a multiplicative bound2 of 2; (ii) for the truncated linear metric (i. [sent-79, score-0.431]
</p><p>25 2 Let f be the labelling obtained by an algorithm A (e. [sent-83, score-0.34]
</p><p>26 in this case when the pairwise potentials form a Potts model). [sent-87, score-0.311]
</p><p>27 The algorithm A is said to achieve a multiplicative bound of σ, if for every instance in the class of MAP estimation problems the following holds true: „ « Q(f, D; θ) E ≤ σ, Q(f ∗ , D; θ)  where E(·) denotes the expectation of its argument under the rounding scheme. [sent-89, score-0.241]
</p><p>28 2  Initialization - Initialize the labelling to some function f1 . [sent-90, score-0.34]
</p><p>29 Iteration - Choose an interval Im = [im + 1, jm ] where (jm − im ) = L such that d(L) ≥ M . [sent-92, score-0.425]
</p><p>30 - Move from current labelling fm to a new labelling fm+1 such that fm+1 (a) = fm (a) or fm+1 (a) ∈ Im , ∀va ∈ v. [sent-93, score-1.624]
</p><p>31 The new labelling is obtained by solving the st- MINCUT problem on a graph described in § 2. [sent-94, score-0.407]
</p><p>32 As is typical with move making methods, our approach iteratively goes from one labelling to the next by solving an st-MINCUT problem. [sent-98, score-0.501]
</p><p>33 d(f (a) − f (b)) = |f (a) − f (b)| and a general M > 0), we obtain a multiplicative bound of √ 2 + 2; and (iii) for the truncated quadratic semi-metric (i. [sent-100, score-0.494]
</p><p>34 Move making algorithms start with an initial labelling f0 and iteratively minimize the Gibbs energy by moving to a better labelling. [sent-104, score-0.486]
</p><p>35 The recently proposed range move algorithm [23] modiﬁes this approach such that any variable currently labelled li where i ∈ [α, β] can be assigned any label lj where j ∈ [α, β]. [sent-109, score-0.235]
</p><p>36 In contrast, the α-expansion algorithm [4] (where each variable can either retain its label or get assigned the label lα at an iteration) provides a multiplicative bound of 2 for the Potts model and 2M for the truncated linear metric. [sent-116, score-0.532]
</p><p>37 Gupta and Tardos [8] generalized the α-expansion algorithm for the truncated linear metric and obtained a multiplicative bound of 4. [sent-117, score-0.475]
</p><p>38 Komodakis and Tziritas [14] designed a primal-dual algorithm which provides a bound of 2M for the truncated quadratic semimetric. [sent-118, score-0.421]
</p><p>39 However, all the above move making algorithms use only a single st-MINCUT at each iteration and are hence, much faster than interior point algorithms, TRW, TRW- S and BP. [sent-120, score-0.265]
</p><p>40 The ﬁrst extension allows us to handle any truncated convex model (and not just truncated linear). [sent-123, score-0.712]
</p><p>41 In other words, if in the mth iteration ′ we move from label fm to fm+1 then it is possible that there exists another labelling fm+1 such ′ ′ ′ that fm+1 (a) = fm (a) or fm+1 (a) ∈ Im for all va ∈ v and Q(fm+1 , D; θ) < Q(fm+1 , D; θ). [sent-131, score-1.662]
</p><p>42 We now turn our attention to designing a method of moving from labelling fm to fm+1 . [sent-133, score-0.812]
</p><p>43 Our approach relies on constructing a graph such that every st-cut on the graph corresponds to a labelling f ′ of the random variables which satisﬁes: f ′ (a) = fm (a) or f ′ (a) ∈ Im , for all va ∈ v. [sent-134, score-1.171]
</p><p>44 The new labelling fm+1 is obtained in two steps: (i) we obtain a labelling f ′ which corresponds to the 3  st-MINCUT on our graph; and (ii) we choose the new labelling fm+1 as f′ if Q(f ′ , D; θ) ≤ Q(fm , D; θ), fm+1 = fm otherwise. [sent-135, score-1.492]
</p><p>45 We also have the current labelling fm for all the random variables. [sent-141, score-0.836]
</p><p>46 We construct a directed weighted graph (with non-negative weights) Gm = {Vm , Em , cm (·, ·)} such that for each va ∈ v, we deﬁne vertices {aim +1 , aim +2 , · · · , ajm } ∈ Vm . [sent-142, score-0.575]
</p><p>47 weight) cm (e) are of two types: (i) those that represent the unary potentials of a labelling corresponding to an st-cut in the graph and; (ii) those that represent the pairwise potentials of the labelling. [sent-146, score-1.166]
</p><p>48 Figure 1: Part of the graph Gm containing the terminals and the vertices corresponding to the variable va . [sent-147, score-0.304]
</p><p>49 The edges which represent the unary potential of the new labelling are also shown. [sent-148, score-0.593]
</p><p>50 1 shows the above edges together with their capacities for one random variable va . [sent-151, score-0.411]
</p><p>51 Any st-cut with ﬁnite cost3 contains only one of the ﬁnite capacity edges for each random variable va . [sent-153, score-0.455]
</p><p>52 This is because if an st-cut included more than one ﬁnite capacity edge, then by construction it must include at least one inﬁnite capacity edge thereby making its cost inﬁnite [9, 23]. [sent-154, score-0.319]
</p><p>53 We interpret a ﬁnite cost st-cut as a relabelling of the random variables as follows: k if st-cut includes edge (ak , ak+1 ) where k ∈ [im + 1, jm ), jm if st-cut includes edge (ajm , t), (5) f ′ (a) = fm (a) if st-cut includes edge (s, aim +1 ). [sent-155, score-0.953]
</p><p>54 Note that the sum of the unary potentials for the labelling f ′ is exactly equal to the cost of the st-cut over the edges deﬁned above. [sent-156, score-0.817]
</p><p>55 However, the Gibbs energy of the labelling also includes the sum of the pairwise potentials (as shown in equation (1)). [sent-157, score-0.744]
</p><p>56 Unlike the unary potentials we will not be able to model the sum of pairwise potentials exactly. [sent-158, score-0.621]
</p><p>57 Representing Pairwise Potentials For all neighbouring random variables va and vb , i. [sent-160, score-0.321]
</p><p>58 (a, b) ∈ E, we deﬁne edges (ak , bk′ ) ∈ Em where either one or both of k and k ′ belong to the set (im + 1, jm ] (i. [sent-162, score-0.274]
</p><p>59 The capacity of these edges is given by wab cm (ak , bk′ ) = (d(k − k ′ + 1) − 2d(k − k ′ ) + d(k − k ′ − 1)) . [sent-165, score-0.586]
</p><p>60 (6) 2 The above capacity is non-negative due to the fact that wab ≥ 0 and d(·) is convex. [sent-166, score-0.302]
</p><p>61 Furthermore, we also add the following edges: ab cm (ak , ak+1 ) = w2 (d(L − k + im ) + d(k − im )) , ∀(a, b) ∈ E, k ∈ [im + 1, jm ) wab cm (bk′ , bk′ +1 ) = 2 (d(L − k ′ + im ) + d(k ′ − im )) , ∀(a, b) ∈ E, k ′ ∈ [im + 1, jm ) ab cm (ajm , t) = cm (bjm , t) = w2 d(L), ∀(a, b) ∈ E. [sent-167, score-2.523]
</p><p>62 (7)  3 Recall that the cost of an st-cut is the sum of the capacities of the edges whose starting point lies in the set of vertices containing the source s and whose ending point lies in the set of vertices containing the sink t. [sent-168, score-0.314]
</p><p>63 4  (b)  (c)  (d) (a) Figure 2: (a) Edges that are used to represent the pairwise potentials of two neighbouring random variables va and vb are shown. [sent-169, score-0.632]
</p><p>64 Undirected edges indicate that there are opposing edges in both directions with equal capacity (as given by equation 6). [sent-170, score-0.419]
</p><p>65 Directed dashed edges, with capacities shown in equation (7), are added to ensure that the graph models the convex pairwise potentials correctly. [sent-171, score-0.525]
</p><p>66 (b) An additional edge is added when fm (a) ∈ Im and fm (b) ∈ Im . [sent-172, score-0.979]
</p><p>67 (c) A similar additional edge is added when fm (a) ∈ Im and fm (b) ∈ Im . [sent-174, score-0.979]
</p><p>68 (d) Five / edges, with capacities as shown in equation (8), are added when fm (a) ∈ Im and fm (b) ∈ Im . [sent-175, score-1.005]
</p><p>69 / / Undirected edges indicate the presence of opposing edges with equal capacity. [sent-176, score-0.314]
</p><p>70 Note that in [23] the graph obtained by the edges in equations (6) and (7) was used to ﬁnd the exact MAP estimate for convex pairwise potentials. [sent-177, score-0.407]
</p><p>71 A proof that the above edges exactly model convex pairwise potentials up to an additive constant κab = wab d(L) can be found in [17]. [sent-178, score-0.74]
</p><p>72 However, we are concerned with the NP-hard case where the pairwise potentials are truncated. [sent-179, score-0.311]
</p><p>73 • If fm (a) ∈ Im and fm (b) ∈ Im then we do not add any more edges in the graph (see Fig. [sent-182, score-1.157]
</p><p>74 • If fm (a) ∈ Im and fm (b) ∈ Im then we add an edge (aim +1 , bim +1 ) with capacity wab M +κab /2, / where κab = wab d(L) is a constant for a given pair of neighbouring random variables (a, b) ∈ E (see Fig. [sent-184, score-1.647]
</p><p>75 Similarly, if fm (a) ∈ Im and fm (b) ∈ Im then we add an edge (bim +1 , aim +1 ) with / capacity wab M + κab /2 (see Fig. [sent-186, score-1.335]
</p><p>76 • If fm (a) ∈ Im and fm (b) ∈ Im , we introduce a new vertex pab . [sent-188, score-1.049]
</p><p>77 Using this vertex pab , ﬁve edges / / are deﬁned with the following capacities (see Fig. [sent-189, score-0.312]
</p><p>78 2(d)): cm (aim +1 , pab ) = cm (pab , aim +1 ) = cm (bim +1 , pab ) = cm (pab , bim +1 ) = wab M + κab /2, 2 (8) cm (s, pab ) = θab;fm (a),fm (b) + κab . [sent-190, score-1.316]
</p><p>79 Given the graph Gm we solve the st-MINCUT problem which provides us with a labelling f ′ as described in equation (5). [sent-192, score-0.426]
</p><p>80 The new labelling fm+1 is obtained using equation (4). [sent-193, score-0.34]
</p><p>81 Note that our graph construction is similar to that of Gupta and Tardos [8] with two notable exceptions: (i) we can handle any general truncated convex model and not just truncated linear as in the case of [8]. [sent-194, score-0.803]
</p><p>82 The following properties provide such a value of L for both the truncated linear and the truncated quadratic models. [sent-197, score-0.671]
</p><p>83 2 Properties of the Algorithm For the above graph construction, the following properties hold true: • The cost of the st-MINCUT provides an upper bound on the Gibbs energy of the labelling f ′ and hence, on the Gibbs energy of fm+1 (see section 2. [sent-200, score-0.677]
</p><p>84 √ • For the truncated linear metric, our algorithm obtains a multiplicative bound of 2 + 2 using √ L = 2M (see section 3, Theorem 1, of [17]). [sent-202, score-0.472]
</p><p>85 √ • For the truncated quadratic semi-metric, our algorithm obtains a multiplicative bound of O( M ) √ using L = M (see section 3, Theorem 2, of [17]). [sent-208, score-0.517]
</p><p>86 1 Synthetic Data Experimental Setup We used 100 random ﬁelds for both the truncated linear and truncated quadratic models. [sent-214, score-0.695]
</p><p>87 1 Speciﬁcally, the unary potentials θa;i were sampled uniformly from the interval [0, 10] while the weights wab , which determine the pairwise potentials, were sampled uniformly from [0, 5]. [sent-221, score-0.615]
</p><p>88 For the above random ﬁeld formulation, the unary potentials were deﬁned as in [22] and were truncated at 15. [sent-235, score-0.647]
</p><p>89 We experimented using the following truncated convex potentials: 2 2 θab;ij = 50 min{|i − j|, 10}, θab;ij = 50 min{(i − j)2 , 100}. [sent-238, score-0.399]
</p><p>90 (9)  The above form of pairwise potentials encourage neighbouring pixels to take similar disparity values which corresponds to our expectations of ﬁnding smooth surfaces in natural images. [sent-239, score-0.396]
</p><p>91 Truncation of pairwise potentials is essential to avoid oversmoothing, as observed in [4, 23]. [sent-240, score-0.311]
</p><p>92 4 Discussion We have presented an st-MINCUT based algorithm for obtaining the approximate MAP estimate of discrete random ﬁelds with truncated convex pairwise potentials. [sent-270, score-0.531]
</p><p>93 Our method improves the multiplicative bound for the truncated linear metric compared to [4, 8] and provides the best known bound for the truncated quadratic semi-metric. [sent-271, score-0.896]
</p><p>94 In fact, its speed can be further improved by a large factor using clever techniques such as those described in [12] (for convex unary potentials) and/or [1] (for general unary potentials). [sent-273, score-0.322]
</p><p>95 2 shows that, for the truncated linear and truncated quadratic models, the bound achieved by our move making algorithm over intervals of any length L is equal to that of rounding the LP relaxation’s optimal solution using the same intervals [5]. [sent-277, score-0.961]
</p><p>96 A natural question would be to ask about the relationship between move making algorithms and the rounding schemes used in convex relaxations. [sent-279, score-0.376]
</p><p>97 Note that despite recent efforts [14] which analyze certain move making algorithms in the context of primal-dual approaches for the LP relaxation, not many results 7  are known about their connection with randomized rounding schemes. [sent-280, score-0.27]
</p><p>98 A linear programming formulation and approximation algorithms for the metric labelling problem. [sent-317, score-0.39]
</p><p>99 Conditional random ﬁelds: Probabilistic models for segmenting and labelling sequence data. [sent-392, score-0.364]
</p><p>100 Graph cut based optimization for MRFs with truncated convex priors. [sent-421, score-0.399]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('fm', 0.472), ('labelling', 0.34), ('truncated', 0.313), ('im', 0.297), ('potentials', 0.203), ('wab', 0.197), ('va', 0.18), ('ab', 0.165), ('edges', 0.146), ('cm', 0.138), ('move', 0.132), ('jm', 0.128), ('lp', 0.11), ('pairwise', 0.108), ('unary', 0.107), ('capacity', 0.105), ('ajm', 0.105), ('pab', 0.105), ('energy', 0.093), ('trw', 0.092), ('multiplicative', 0.092), ('ak', 0.091), ('convex', 0.086), ('rounding', 0.085), ('gibbs', 0.08), ('stereo', 0.074), ('mrf', 0.072), ('bp', 0.068), ('graph', 0.067), ('neighbouring', 0.064), ('relaxation', 0.063), ('map', 0.061), ('capacities', 0.061), ('elds', 0.061), ('bim', 0.06), ('gupta', 0.056), ('aim', 0.054), ('neighbourhood', 0.053), ('tardos', 0.048), ('potts', 0.045), ('quadratic', 0.045), ('bound', 0.044), ('pami', 0.044), ('kolmogorov', 0.043), ('komodakis', 0.039), ('bk', 0.037), ('passing', 0.036), ('message', 0.035), ('edge', 0.035), ('truncation', 0.034), ('veksler', 0.034), ('iteration', 0.034), ('dual', 0.033), ('primal', 0.033), ('vb', 0.032), ('label', 0.032), ('vertices', 0.031), ('chekuri', 0.03), ('ishikawa', 0.03), ('teddy', 0.03), ('wolfson', 0.03), ('synthetic', 0.03), ('making', 0.029), ('ii', 0.029), ('gm', 0.028), ('unlike', 0.028), ('range', 0.028), ('eld', 0.027), ('slower', 0.026), ('terminals', 0.026), ('pawan', 0.026), ('interior', 0.026), ('kumar', 0.026), ('reconstruction', 0.026), ('guarantees', 0.026), ('metric', 0.026), ('art', 0.025), ('algorithms', 0.024), ('labelled', 0.024), ('disparities', 0.024), ('sink', 0.024), ('random', 0.024), ('construction', 0.024), ('obtains', 0.023), ('opposing', 0.022), ('clever', 0.022), ('disparity', 0.021), ('vm', 0.021), ('variables', 0.021), ('cost', 0.021), ('felzenszwalb', 0.02), ('bounds', 0.02), ('relationship', 0.02), ('em', 0.02), ('labels', 0.02), ('faster', 0.02), ('estimation', 0.02), ('provides', 0.019), ('oxford', 0.019), ('lj', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="104-tfidf-1" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>Author: Philip Torr, M. P. Kumar</p><p>Abstract: We consider the problem of obtaining the approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. For this problem, we propose an improved st-MINCUT based move making algorithm. Unlike previous move making approaches, which either provide a loose bound or no bound on the quality of the solution (in terms of the corresponding Gibbs energy), our algorithm achieves the same guarantees as the standard linear programming (LP) relaxation. Compared to previous approaches based on the LP relaxation, e.g. interior-point algorithms or treereweighted message passing (TRW), our method is faster as it uses only the efﬁcient st-MINCUT algorithm in its design. Furthermore, it directly provides us with a primal solution (unlike TRW and other related methods which solve the dual of the LP). We demonstrate the effectiveness of the proposed approach on both synthetic and standard real data problems. Our analysis also opens up an interesting question regarding the relationship between move making algorithms (such as α-expansion and the algorithms presented in this paper) and the randomized rounding schemes used with convex relaxations. We believe that further explorations in this direction would help design efﬁcient algorithms for more complex relaxations.</p><p>2 0.2242361 <a title="104-tfidf-2" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<p>Author: Mark Herbster, Guy Lever, Massimiliano Pontil</p><p>Abstract: We continue our study of online prediction of the labelling of a graph. We show a fundamental limitation of Laplacian-based algorithms: if the graph has a large diameter then the number of mistakes made by such algorithms may be proportional to the square root of the number of vertices, even when tackling simple problems. We overcome this drawback by means of an efﬁcient algorithm which achieves a logarithmic mistake bound. It is based on the notion of a spine, a path graph which provides a linear embedding of the original graph. In practice, graphs may exhibit cluster structure; thus in the last part, we present a modiﬁed algorithm which achieves the “best of both worlds”: it performs well locally in the presence of cluster structure, and globally on large diameter graphs. 1</p><p>3 0.15833604 <a title="104-tfidf-3" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>Author: John Langford, Lihong Li, Tong Zhang</p><p>Abstract: We propose a general method called truncated gradient to induce sparsity in the weights of online-learning algorithms with convex loss. This method has several essential properties. First, the degree of sparsity is continuous—a parameter controls the rate of sparsiﬁcation from no sparsiﬁcation to total sparsiﬁcation. Second, the approach is theoretically motivated, and an instance of it can be regarded as an online counterpart of the popular L1 -regularization method in the batch setting. We prove small rates of sparsiﬁcation result in only small additional regret with respect to typical online-learning guarantees. Finally, the approach works well empirically. We apply it to several datasets and ﬁnd for datasets with large numbers of features, substantial sparsity is discoverable. 1</p><p>4 0.095943607 <a title="104-tfidf-4" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>Author: David Sontag, Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: We propose a new class of consistency constraints for Linear Programming (LP) relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and signiﬁcantly faster than a method that does not use partitioning. 1</p><p>5 0.081341736 <a title="104-tfidf-5" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables. The bound is obtained by propagating local bounds (convex sets of probability distributions) over a subtree of the factor graph, rooted in the variable of interest. By construction, the method not only bounds the exact marginal probability distribution of a variable, but also its approximate Belief Propagation marginal (“belief”). Thus, apart from providing a practical means to calculate bounds on marginals, our contribution also lies in providing a better understanding of the error made by Belief Propagation. We show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis. 1</p><p>6 0.080575563 <a title="104-tfidf-6" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>7 0.077399351 <a title="104-tfidf-7" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>8 0.074243002 <a title="104-tfidf-8" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>9 0.072974637 <a title="104-tfidf-9" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>10 0.072393805 <a title="104-tfidf-10" href="./nips-2008-Multiscale_Random_Fields_with_Application_to_Contour_Grouping.html">147 nips-2008-Multiscale Random Fields with Application to Contour Grouping</a></p>
<p>11 0.072074324 <a title="104-tfidf-11" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>12 0.069828108 <a title="104-tfidf-12" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>13 0.065640293 <a title="104-tfidf-13" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>14 0.064353488 <a title="104-tfidf-14" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>15 0.063029438 <a title="104-tfidf-15" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>16 0.059713371 <a title="104-tfidf-16" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>17 0.058515489 <a title="104-tfidf-17" href="./nips-2008-Fast_Prediction_on_a_Tree.html">84 nips-2008-Fast Prediction on a Tree</a></p>
<p>18 0.05599089 <a title="104-tfidf-18" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>19 0.055074226 <a title="104-tfidf-19" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>20 0.053602286 <a title="104-tfidf-20" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.156), (1, -0.026), (2, -0.045), (3, 0.038), (4, 0.032), (5, -0.077), (6, 0.018), (7, -0.098), (8, 0.006), (9, -0.181), (10, -0.146), (11, 0.063), (12, 0.033), (13, -0.032), (14, 0.015), (15, -0.06), (16, 0.0), (17, 0.048), (18, -0.117), (19, -0.028), (20, -0.008), (21, -0.067), (22, -0.023), (23, -0.014), (24, -0.066), (25, -0.006), (26, -0.0), (27, 0.056), (28, -0.099), (29, -0.044), (30, -0.102), (31, -0.033), (32, -0.07), (33, -0.07), (34, -0.09), (35, -0.012), (36, -0.142), (37, -0.018), (38, 0.084), (39, 0.077), (40, -0.057), (41, -0.056), (42, -0.081), (43, -0.107), (44, -0.086), (45, 0.047), (46, -0.074), (47, -0.031), (48, 0.043), (49, -0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94695139 <a title="104-lsi-1" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>Author: Philip Torr, M. P. Kumar</p><p>Abstract: We consider the problem of obtaining the approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. For this problem, we propose an improved st-MINCUT based move making algorithm. Unlike previous move making approaches, which either provide a loose bound or no bound on the quality of the solution (in terms of the corresponding Gibbs energy), our algorithm achieves the same guarantees as the standard linear programming (LP) relaxation. Compared to previous approaches based on the LP relaxation, e.g. interior-point algorithms or treereweighted message passing (TRW), our method is faster as it uses only the efﬁcient st-MINCUT algorithm in its design. Furthermore, it directly provides us with a primal solution (unlike TRW and other related methods which solve the dual of the LP). We demonstrate the effectiveness of the proposed approach on both synthetic and standard real data problems. Our analysis also opens up an interesting question regarding the relationship between move making algorithms (such as α-expansion and the algorithms presented in this paper) and the randomized rounding schemes used with convex relaxations. We believe that further explorations in this direction would help design efﬁcient algorithms for more complex relaxations.</p><p>2 0.60895091 <a title="104-lsi-2" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<p>Author: Mark Herbster, Guy Lever, Massimiliano Pontil</p><p>Abstract: We continue our study of online prediction of the labelling of a graph. We show a fundamental limitation of Laplacian-based algorithms: if the graph has a large diameter then the number of mistakes made by such algorithms may be proportional to the square root of the number of vertices, even when tackling simple problems. We overcome this drawback by means of an efﬁcient algorithm which achieves a logarithmic mistake bound. It is based on the notion of a spine, a path graph which provides a linear embedding of the original graph. In practice, graphs may exhibit cluster structure; thus in the last part, we present a modiﬁed algorithm which achieves the “best of both worlds”: it performs well locally in the presence of cluster structure, and globally on large diameter graphs. 1</p><p>3 0.54702568 <a title="104-lsi-3" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>Author: Nicol N. Schraudolph, Dmitry Kamenetsky</p><p>Abstract: We give polynomial-time algorithms for the exact computation of lowest-energy states, worst margin violators, partition functions, and marginals in certain binary undirected graphical models. Our approach provides an interesting alternative to the well-known graph cut paradigm in that it does not impose any submodularity constraints; instead we require planarity to establish a correspondence with perfect matchings in an expanded dual graph. Maximum-margin parameter estimation for a boundary detection task shows our approach to be efﬁcient and effective. A C++ implementation is available from http://nic.schraudolph.org/isinf/. 1</p><p>4 0.54242271 <a title="104-lsi-4" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>Author: Lukas Kroc, Ashish Sabharwal, Bart Selman</p><p>Abstract: We show that an important and computationally challenging solution space feature of the graph coloring problem (COL), namely the number of clusters of solutions, can be accurately estimated by a technique very similar to one for counting the number of solutions. This cluster counting approach can be naturally written in terms of a new factor graph derived from the factor graph representing the COL instance. Using a variant of the Belief Propagation inference framework, we can efﬁciently approximate cluster counts in random COL problems over a large range of graph densities. We illustrate the algorithm on instances with up to 100, 000 vertices. Moreover, we supply a methodology for computing the number of clusters exactly using advanced techniques from the knowledge compilation literature. This methodology scales up to several hundred variables. 1</p><p>5 0.5260939 <a title="104-lsi-5" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>Author: Gal Elidan, Stephen Gould</p><p>Abstract: With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufﬁciently expressive for generalization while also allowing for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overﬁtting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modiﬁcations and that is polynomial in the size of the graph and the treewidth bound. At the heart of our method is a triangulated graph that we dynamically update in a way that facilitates the addition of chain structures that increase the bound on the model’s treewidth by at most one. We demonstrate the effectiveness of our “treewidth-friendly” method on several real-life datasets. Importantly, we also show that by using global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth. 1</p><p>6 0.52352065 <a title="104-lsi-6" href="./nips-2008-Fast_Prediction_on_a_Tree.html">84 nips-2008-Fast Prediction on a Tree</a></p>
<p>7 0.43250781 <a title="104-lsi-7" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>8 0.42222339 <a title="104-lsi-8" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>9 0.40923485 <a title="104-lsi-9" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>10 0.39691934 <a title="104-lsi-10" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>11 0.39376974 <a title="104-lsi-11" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>12 0.38923746 <a title="104-lsi-12" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>13 0.38223103 <a title="104-lsi-13" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>14 0.36984488 <a title="104-lsi-14" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>15 0.36276311 <a title="104-lsi-15" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>16 0.35766384 <a title="104-lsi-16" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>17 0.35315323 <a title="104-lsi-17" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>18 0.34077471 <a title="104-lsi-18" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>19 0.33235627 <a title="104-lsi-19" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>20 0.32737902 <a title="104-lsi-20" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.058), (7, 0.046), (12, 0.029), (28, 0.157), (56, 0.279), (57, 0.129), (59, 0.032), (63, 0.029), (71, 0.054), (77, 0.027), (83, 0.044)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72277266 <a title="104-lda-1" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>Author: Philip Torr, M. P. Kumar</p><p>Abstract: We consider the problem of obtaining the approximate maximum a posteriori estimate of a discrete random ﬁeld characterized by pairwise potentials that form a truncated convex model. For this problem, we propose an improved st-MINCUT based move making algorithm. Unlike previous move making approaches, which either provide a loose bound or no bound on the quality of the solution (in terms of the corresponding Gibbs energy), our algorithm achieves the same guarantees as the standard linear programming (LP) relaxation. Compared to previous approaches based on the LP relaxation, e.g. interior-point algorithms or treereweighted message passing (TRW), our method is faster as it uses only the efﬁcient st-MINCUT algorithm in its design. Furthermore, it directly provides us with a primal solution (unlike TRW and other related methods which solve the dual of the LP). We demonstrate the effectiveness of the proposed approach on both synthetic and standard real data problems. Our analysis also opens up an interesting question regarding the relationship between move making algorithms (such as α-expansion and the algorithms presented in this paper) and the randomized rounding schemes used with convex relaxations. We believe that further explorations in this direction would help design efﬁcient algorithms for more complex relaxations.</p><p>2 0.61879754 <a title="104-lda-2" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>Author: Mehmet K. Muezzinoglu, Alexander Vergara, Ramon Huerta, Thomas Nowotny, Nikolai Rulkov, Henry Abarbanel, Allen Selverston, Mikhail Rabinovich</p><p>Abstract: The odor transduction process has a large time constant and is susceptible to various types of noise. Therefore, the olfactory code at the sensor/receptor level is in general a slow and highly variable indicator of the input odor in both natural and artiﬁcial situations. Insects overcome this problem by using a neuronal device in their Antennal Lobe (AL), which transforms the identity code of olfactory receptors to a spatio-temporal code. This transformation improves the decision of the Mushroom Bodies (MBs), the subsequent classiﬁer, in both speed and accuracy. Here we propose a rate model based on two intrinsic mechanisms in the insect AL, namely integration and inhibition. Then we present a MB classiﬁer model that resembles the sparse and random structure of insect MB. A local Hebbian learning procedure governs the plasticity in the model. These formulations not only help to understand the signal conditioning and classiﬁcation methods of insect olfactory systems, but also can be leveraged in synthetic problems. Among them, we consider here the discrimination of odor mixtures from pure odors. We show on a set of records from metal-oxide gas sensors that the cascade of these two new models facilitates fast and accurate discrimination of even highly imbalanced mixtures from pure odors. 1</p><p>3 0.6097663 <a title="104-lda-3" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<p>Author: Jing Xu, Thomas L. Griffiths</p><p>Abstract: Many human interactions involve pieces of information being passed from one person to another, raising the question of how this process of information transmission is affected by the capacities of the agents involved. In the 1930s, Sir Frederic Bartlett explored the inﬂuence of memory biases in “serial reproduction” of information, in which one person’s reconstruction of a stimulus from memory becomes the stimulus seen by the next person. These experiments were done using relatively uncontrolled stimuli such as pictures and stories, but suggested that serial reproduction would transform information in a way that reﬂected the biases inherent in memory. We formally analyze serial reproduction using a Bayesian model of reconstruction from memory, giving a general result characterizing the effect of memory biases on information transmission. We then test the predictions of this account in two experiments using simple one-dimensional stimuli. Our results provide theoretical and empirical justiﬁcation for the idea that serial reproduction reﬂects memory biases. 1</p><p>4 0.60808492 <a title="104-lda-4" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>Author: Erik B. Sudderth, Michael I. Jordan</p><p>Abstract: We develop a statistical framework for the simultaneous, unsupervised segmentation and discovery of visual object categories from image databases. Examining a large set of manually segmented scenes, we show that object frequencies and segment sizes both follow power law distributions, which are well modeled by the Pitman–Yor (PY) process. This nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects, and segmentation methods which automatically adapt their resolution to each image. Generalizing previous applications of PY processes, we use Gaussian processes to discover spatially contiguous segments which respect image boundaries. Using a novel family of variational approximations, our approach produces segmentations which compare favorably to state-of-the-art methods, while simultaneously discovering categories shared among natural scenes. 1</p><p>5 0.60186899 <a title="104-lda-5" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>6 0.59755224 <a title="104-lda-6" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>7 0.59618938 <a title="104-lda-7" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>8 0.59387577 <a title="104-lda-8" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>9 0.59382558 <a title="104-lda-9" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>10 0.59135664 <a title="104-lda-10" href="./nips-2008-Extended_Grassmann_Kernels_for_Subspace-Based_Learning.html">80 nips-2008-Extended Grassmann Kernels for Subspace-Based Learning</a></p>
<p>11 0.59103727 <a title="104-lda-11" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>12 0.59083682 <a title="104-lda-12" href="./nips-2008-Offline_Handwriting_Recognition_with_Multidimensional_Recurrent_Neural_Networks.html">158 nips-2008-Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</a></p>
<p>13 0.58993131 <a title="104-lda-13" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>14 0.58927894 <a title="104-lda-14" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>15 0.58918279 <a title="104-lda-15" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>16 0.58906019 <a title="104-lda-16" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>17 0.58835185 <a title="104-lda-17" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<p>18 0.58722264 <a title="104-lda-18" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>19 0.58682203 <a title="104-lda-19" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>20 0.58646858 <a title="104-lda-20" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
