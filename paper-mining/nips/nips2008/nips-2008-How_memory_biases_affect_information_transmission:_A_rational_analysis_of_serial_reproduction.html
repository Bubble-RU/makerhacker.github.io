<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-100" href="#">nips2008-100</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</h1>
<br/><p>Source: <a title="nips-2008-100-pdf" href="http://papers.nips.cc/paper/3588-how-memory-biases-affect-information-transmission-a-rational-analysis-of-serial-reproduction.pdf">pdf</a></p><p>Author: Jing Xu, Thomas L. Griffiths</p><p>Abstract: Many human interactions involve pieces of information being passed from one person to another, raising the question of how this process of information transmission is affected by the capacities of the agents involved. In the 1930s, Sir Frederic Bartlett explored the inﬂuence of memory biases in “serial reproduction” of information, in which one person’s reconstruction of a stimulus from memory becomes the stimulus seen by the next person. These experiments were done using relatively uncontrolled stimuli such as pictures and stories, but suggested that serial reproduction would transform information in a way that reﬂected the biases inherent in memory. We formally analyze serial reproduction using a Bayesian model of reconstruction from memory, giving a general result characterizing the effect of memory biases on information transmission. We then test the predictions of this account in two experiments using simple one-dimensional stimuli. Our results provide theoretical and empirical justiﬁcation for the idea that serial reproduction reﬂects memory biases. 1</p><p>Reference: <a title="nips-2008-100-reference" href="../nips2008_reference/nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 How memory biases affect information transmission: A rational analysis of serial reproduction  Jing Xu Thomas L. [sent-1, score-1.405]
</p><p>2 edu  Abstract Many human interactions involve pieces of information being passed from one person to another, raising the question of how this process of information transmission is affected by the capacities of the agents involved. [sent-4, score-0.197]
</p><p>3 In the 1930s, Sir Frederic Bartlett explored the inﬂuence of memory biases in “serial reproduction” of information, in which one person’s reconstruction of a stimulus from memory becomes the stimulus seen by the next person. [sent-5, score-0.668]
</p><p>4 These experiments were done using relatively uncontrolled stimuli such as pictures and stories, but suggested that serial reproduction would transform information in a way that reﬂected the biases inherent in memory. [sent-6, score-1.331]
</p><p>5 We formally analyze serial reproduction using a Bayesian model of reconstruction from memory, giving a general result characterizing the effect of memory biases on information transmission. [sent-7, score-1.476]
</p><p>6 Our results provide theoretical and empirical justiﬁcation for the idea that serial reproduction reﬂects memory biases. [sent-9, score-1.243]
</p><p>7 We might thus expect that these memory biases would affect the transmission of information, since such a process relies on each person remembering a fact accurately. [sent-13, score-0.423]
</p><p>8 The question of how memory biases affect information transmission was ﬁrst investigated in detail in Sir Frederic Bartlett’s “serial reproduction” experiments [2]. [sent-14, score-0.345]
</p><p>9 Bartlett interpreted these studies as showing that people were biased by their own culture when they reconstruct information from memory, and that this bias became exaggerated through serial reproduction. [sent-15, score-0.644]
</p><p>10 Serial reproduction has become one of the standard methods used to simulate the process of cultural transmission, and several subsequent studies have used this paradigm (e. [sent-16, score-0.68]
</p><p>11 In this paper, we formally analyze and empirically evaluate how information is changed by serial reproduction and how this process relates to memory biases. [sent-20, score-1.27]
</p><p>12 In particular, we provide a rational analysis of serial reproduction (in the spirit of [5]), considering how information should change when passed along a chain of rational agents. [sent-21, score-1.249]
</p><p>13 For example, people are biased by their knowledge of the structure of categories when they reconstruct simple stimuli from memory. [sent-23, score-0.258]
</p><p>14 One common  effect of this kind is that people judge stimuli that cross boundaries of two different categories to be further apart than those within the same category, although the distances between the stimuli are the same in the two situations [6]. [sent-24, score-0.341]
</p><p>15 If we assume that memory is solving the problem of extracting and storing information from the noisy signal presented to our senses, we can analyze the process of reconstruction from memory as a Bayesian inference. [sent-26, score-0.492]
</p><p>16 Under this view, reconstructions should combine prior knowledge about the world with the information provided by noisy stimuli. [sent-27, score-0.155]
</p><p>17 Use of prior knowledge will result in biases, but these biases ultimately make memory more accurate [7]. [sent-28, score-0.285]
</p><p>18 If this account of reconstruction from memory is true, we would expect the same inference process to occur at every step of serial reproduction. [sent-29, score-0.792]
</p><p>19 The effects of memory biases should thus be accumulated. [sent-30, score-0.254]
</p><p>20 Assuming all participants share the same prior knowledge about the world, serial reproduction should ultimately reveal the nature of this knowledge. [sent-31, score-1.352]
</p><p>21 Drawing on recent work exploring other processes of information transmission [8, 9], we show that a rational analysis of serial reproduction makes exactly this prediction. [sent-32, score-1.202]
</p><p>22 In this case we can precisely characterize behavior at every step of serial reproduction. [sent-34, score-0.479]
</p><p>23 We use these predictions to test the Bayesian models of serial reproduction in two laboratory experiments and show that the predictions hold serial reproduction both between- and within-subjects. [sent-36, score-2.238]
</p><p>24 Section 2 lays out the Bayesian account of serial reproduction. [sent-38, score-0.511]
</p><p>25 Sections 4 and 5 present two experiments testing the model’s prediction that serial reproduction reveals memory biases. [sent-40, score-1.259]
</p><p>26 2  A Bayesian view of serial reproduction  We will outline our Bayesian approach to serial reproduction by ﬁrst considering the problem of reconstruction from memory, and then asking what happens when the solution to this problem is repeated many times, as in serial reproduction. [sent-42, score-2.808]
</p><p>27 1  Reconstruction from memory  Our goal is to give a rational account of reconstruction from memory, considering the underlying computational problem and ﬁnding the optimal solution to that problem. [sent-44, score-0.324]
</p><p>28 We will formulate the problem of reconstruction from memory as a problem of inferring and storing accurate information about the world from noisy sensory data. [sent-45, score-0.341]
</p><p>29 Given a noisy stimulus x, we seek to recover the true state of the world µ that generated that stimulus, storing an estimate µ in memory. [sent-46, score-0.167]
</p><p>30 Huttenlocher and colleagues [7] have conducted several experiments testing this account of memory biases, showing that people’s reconstructions interpolate between observed stimuli and the mean of a trained distribution as predicted. [sent-59, score-0.467]
</p><p>31 2  Serial reproduction  With a model of how people might approach the problem of reconstruction from memory in hand, we are now in a position to analyze what happens in serial reproduction, where the stimuli that people receive on one trial are the results of a previous reconstruction. [sent-62, score-1.689]
</p><p>32 On the nth trial, a participant sees a stimulus xn . [sent-63, score-0.49]
</p><p>33 The participant then computes p(µ|xn ) as outlined in the previous section, and stores a sample µ from this distribution in memory. [sent-64, score-0.283]
</p><p>34 When asked to produce a reconstruction, the ˆ participant generates a new value xn+1 from a distribution that depends on µ. [sent-65, score-0.283]
</p><p>35 ˆ Viewed from this perspective, serial reproduction deﬁnes a stochastic process: a sequence of random variables evolving over time. [sent-68, score-1.093]
</p><p>36 In particular, it is a Markov chain, since the reconstruction produced on the current trial depends only on the value produced on the preceding trial (e. [sent-69, score-0.286]
</p><p>37 The transition probabilities of this Markov chain are p(xn+1 |xn ) =  p(xn+1 |µ)p(µ|xn ) dµ  (2)  being the probability that xn+1 is produced as a reconstruction for the stimulus xn . [sent-72, score-0.438]
</p><p>38 That is, after many reproductions, we should expect the probability of seeing a particular stimulus being produced as a reproduction to stabilize to a ﬁxed distribution. [sent-74, score-0.741]
</p><p>39 Identifying this distribution will help us understand the consequences of serial reproduction. [sent-75, score-0.519]
</p><p>40 The stationary distribution of this Markov chain is the prior predictive distribution π(x) =  p(x|µ)p(µ) dµ  (3)  being the probability of observing the stimulus x when µ is sampled from the prior. [sent-78, score-0.286]
</p><p>41 This gives a clear characterization of the consequences of serial reproduction: after many reproductions, the stimuli being produced will be sampled from the prior distribution assumed by the participants. [sent-80, score-0.731]
</p><p>42 Convergence to the prior predictive distribution provides a formal justiﬁcation for the traditional claims that serial reproduction reveals cultural biases, since those biases would be reﬂected in the prior. [sent-81, score-1.287]
</p><p>43 In the special case of reconstruction of stimuli that vary along a single dimension, we can also analytically compute the probability density functions for the transition probabilities and stationary distribution. [sent-82, score-0.313]
</p><p>44 When λ is close to 1, convergence is slow since µn is close to xn . [sent-86, score-0.163]
</p><p>45 More perceptual noise results in  faster convergence, since the speciﬁc value of xn is trusted less; while more uncertainty in the prior results in slower convergence, since xn is given greater weight. [sent-89, score-0.343]
</p><p>46 It is straightforward to show that the stochastic process deﬁned by serial reproduction where a sample from the posterior distribution on µ is stored in memory and a new value x is sampled from the likelihood is an AR(1) process. [sent-99, score-1.305]
</p><p>47 Identifying serial reproduction for single-dimensional stimuli as an AR(1) process allows us to relax our assumptions about the way that people are storing and reconstructing information. [sent-103, score-1.388]
</p><p>48 The AR(1) model can accommodate different assumptions about memory storage and reconstruction. [sent-104, score-0.171]
</p><p>49 1 All these ways of characterizing serial reproduction lead to the same basic prediction: that repeatedly reconstructing stimuli from memory will result in convergence to a distribution whose mean corresponds to the mean of the prior. [sent-105, score-1.511]
</p><p>50 In the following sections, we present two serial reproduction experiments conducted with stimuli that vary along only one dimension (width of ﬁsh). [sent-107, score-1.249]
</p><p>51 The ﬁrst experiment follows previous research in using a between-subjects design, with the reconstructions of one participant serving as the stimuli for the next. [sent-108, score-0.502]
</p><p>52 The second experiment uses a within-subjects design in which each person reconstructs stimuli that they themselves produced on a previous trial, testing the potential of this design to reveal the memory biases of individuals. [sent-109, score-0.538]
</p><p>53 4  Experiment 1: Between-subjects serial reproduction  This experiment directly tested the basic prediction that the outcome of serial reproduction will reﬂect people’s priors. [sent-110, score-2.247]
</p><p>54 In the reproduction phase, the participant’s reproduction xn+1 can be 1) a noisy reconstruction, which is a sample from the likelihood p(xn+1 |ˆ), as assumed above, or 2) a perfect µ reconstruction from memory, such that xn+1 = µ. [sent-112, score-1.354]
</p><p>55 This deﬁnes four different models of serial reproduction, ˆ 2 all of which correspond to AR(1) processes that differ only in the variance σǫ (although maximizing p(µ|xn ) 2 and then storing a perfect reconstruction is degenerate, with σǫ = 0). [sent-113, score-0.622]
</p><p>56 In all four cases serial reproduction thus converges to a Gaussian stationary distribution with mean µ0 , but with different variances. [sent-114, score-1.205]
</p><p>57 The two distributions differed in their means, allowing us to examine whether the mean of the distribution produced by serial reproduction is affected by the prior. [sent-116, score-1.241]
</p><p>58 All the ﬁsh stimuli varied only in one dimension, the width of the ﬁsh, ranging from 2. [sent-121, score-0.216]
</p><p>59 Two groups of participants were trained on one of the two distributions of ﬁsh-farm ﬁsh (prior distributions A and B), with different means and same standard deviations. [sent-129, score-0.303]
</p><p>60 In the training phase, participants ﬁrst received a block of 60 trials. [sent-135, score-0.228]
</p><p>61 On each trial, a stimulus was presented at the center of a computer monitor and participants tried to predict which type of ﬁsh it was by pressing one of the keys on the keyboard and they received feedback about the correctness of the prediction. [sent-136, score-0.343]
</p><p>62 The participants were then tested for 20 trials on their knowledge of the two types of ﬁsh. [sent-137, score-0.278]
</p><p>63 The trainingtesting loop was repeated until the participants reached 80% correct in using the optimal decision strategy. [sent-139, score-0.228]
</p><p>64 If a participant could not pass the test after ﬁve iterations, the experiment halted. [sent-140, score-0.292]
</p><p>65 In the reproduction phase, the participants were told that they were to record ﬁsh sizes for the ﬁsh farm. [sent-141, score-0.842]
</p><p>66 Another ﬁsh of random size appeared at one of four possible positions near the center of screen and the participants used the up and down arrow keys to adjust the width of the ﬁsh until they thought it matched the ﬁsh they just saw. [sent-143, score-0.347]
</p><p>67 The ﬁsh widths seen by the ﬁrst participant in each condition were 120 values randomly sampled from a uniform distribution from 2. [sent-144, score-0.36]
</p><p>68 The ﬁrst participant tried to memorize these random samples and then gave the reconstructions. [sent-147, score-0.278]
</p><p>69 Each subsequent participant in each condition was then presented with the data generated by the previous participant and they again tried to reconstruct those ﬁsh widths. [sent-148, score-0.603]
</p><p>70 Thus, each participant’s data constitute one slice of time in 120 serial reproduction chains. [sent-149, score-1.093]
</p><p>71 At the end of the experiment, the participants were given a ﬁnal 50-trial test to check if their prior distributions had drifted. [sent-150, score-0.288]
</p><p>72 2  Results and Discussion  There were 18 participants in each condition, resulting in 18 generations of serial reproduction. [sent-153, score-0.707]
</p><p>73 The mean reconstructed ﬁsh widths produced by the ﬁrst participants in conditions A and B were 4. [sent-155, score-0.364]
</p><p>74 For the ﬁnal participants in each chain, the mean reconstructed ﬁsh widths were 3. [sent-160, score-0.296]
</p><p>75 The difference in means matches the direction of the difference in the training provided in conditions A and B, although the overall size of the difference is reduced and the means of the stationary distributions were lower than those of the distributions used in training. [sent-165, score-0.188]
</p><p>76 The correlation between the stimulus xn and its reconstruction xn+1 is the correlation between the AR(1) model’s predictions and the data, and this correlation was high in both conditions, being 0. [sent-168, score-0.429]
</p><p>77 (a) The distribution of stimuli and Gaussian ﬁts to reconstructions for the ﬁrst participants in the two conditions. [sent-189, score-0.458]
</p><p>78 (b) Gaussian ﬁts to reconstructions generated by the 18th participants in each condition. [sent-190, score-0.304]
</p><p>79 5  Experiment 2: Within-subjects serial reproduction  The between-subjects design allows us to reproduce the process of information transmission, but our analysis suggests that serial reproduction might also have promise as a method for investigating the memory biases of individuals. [sent-197, score-2.481]
</p><p>80 To explore the potential of this method, we tested the model with a within-subjects design, in which a participant’s reproduction in the current trial became the stimulus for that same participant in a later trial. [sent-198, score-1.036]
</p><p>81 Each participant produced three such chains, starting from widely separated initial values. [sent-200, score-0.31]
</p><p>82 Control trials and careful instructions were used so that the participants would not realize that some of the stimuli were their own reproductions. [sent-201, score-0.397]
</p><p>83 The basic procedure was the same as Experiment 1, except in the reproduction phase. [sent-204, score-0.631]
</p><p>84 The chains started with three original stimuli with width values of 2. [sent-206, score-0.301]
</p><p>85 76cm, then in the following trials, the stimuli participants saw were their own reproductions in the previous trials in the same chain. [sent-209, score-0.431]
</p><p>86 To prevent participants from realizing this fact, chain order was randomized and the Markov chain trials were intermixed with 40 control trials in which widths were drawn from the prior distribution. [sent-210, score-0.5]
</p><p>87 It took most participants about 20 trials for the chains to converge, so only the second half of the chains (trials 21-40) were analyzed further. [sent-214, score-0.447]
</p><p>88 The locations of the stationary distributions were measured by computing the means of the reproduced ﬁsh widths for each participant. [sent-215, score-0.192]
</p><p>89 02  20 Iteration  xt  Figure 3: Chains and stationary distributions for individual participants from the two conditions. [sent-232, score-0.361]
</p><p>90 The basic prediction of the model was borne out: participants converged to distributions that differed signiﬁcantly in their means when they were exposed to data suggesting a different prior. [sent-238, score-0.324]
</p><p>91 2 Figure 3 shows the chains, training distributions, the Gaussian ﬁts and the autoregression for the second half of the Markov chains for two participants in the two conditions. [sent-243, score-0.391]
</p><p>92 The 2 Since both experiments produced stationary distributions with means lower than those of the training distributions, we conducted a separate experiment examining the reconstructions that people produced without training. [sent-247, score-0.415]
</p><p>93 The mean ﬁsh width produced by 20 participants was 3. [sent-248, score-0.374]
</p><p>94 This result suggested that people seem to have an a priori expectation that ﬁsh will have widths smaller than those used as our category means, suggesting that people in the experiments are using a prior that is a compromise between this expectation and the training data. [sent-253, score-0.266]
</p><p>95 6  Conclusion  We have presented a Bayesian account of serial reproduction, and tested the basic predictions of this account using two strictly controlled laboratory experiments. [sent-260, score-0.601]
</p><p>96 The results of these experiments are consistent with the predictions of our account, with serial reproduction converging to a distribution that is inﬂuenced by the prior distribution established through training. [sent-261, score-1.19]
</p><p>97 Our analysis connects the biases revealed by serial reproduction with the more general Bayesian strategy of combining prior knowledge with noisy data to achieve higher accuracy [7]. [sent-262, score-1.25]
</p><p>98 It also shows that serial reproduction can be analyzed using Markov chains and ﬁrst-order autoregressive models, providing the opportunity to draw on a rich body of work on the dynamics and asymptotic behavior of such processes. [sent-263, score-1.223]
</p><p>99 Transformation between scientiﬁc and social representations of conception: The method of serial reproduction. [sent-286, score-0.498]
</p><p>100 Spreading nonnatural concepts: The role of intuitive conceptual structures in memory and transmission of cultural materials. [sent-291, score-0.26]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('reproduction', 0.614), ('serial', 0.479), ('participant', 0.263), ('participants', 0.228), ('sh', 0.221), ('memory', 0.15), ('xn', 0.147), ('stimuli', 0.134), ('ar', 0.105), ('reconstruction', 0.104), ('biases', 0.104), ('chains', 0.085), ('width', 0.082), ('stimulus', 0.08), ('autoregression', 0.078), ('reconstructions', 0.076), ('stationary', 0.075), ('people', 0.073), ('transmission', 0.071), ('chain', 0.06), ('fish', 0.052), ('widths', 0.051), ('produced', 0.047), ('trial', 0.044), ('storing', 0.039), ('cultural', 0.039), ('markov', 0.038), ('rational', 0.038), ('reconstruct', 0.036), ('trials', 0.035), ('reproductions', 0.034), ('bayesian', 0.033), ('account', 0.032), ('autoregressive', 0.031), ('prior', 0.031), ('person', 0.03), ('experiment', 0.029), ('xt', 0.029), ('distributions', 0.029), ('process', 0.027), ('hemmer', 0.026), ('condition', 0.026), ('world', 0.026), ('predictions', 0.026), ('phase', 0.025), ('characterizing', 0.025), ('correlation', 0.024), ('compromise', 0.024), ('bartlett', 0.023), ('frederic', 0.023), ('ocean', 0.023), ('sir', 0.023), ('conducted', 0.022), ('reconstructing', 0.022), ('noisy', 0.022), ('conditions', 0.021), ('storage', 0.021), ('culture', 0.021), ('remembering', 0.021), ('asking', 0.021), ('psychophysics', 0.021), ('passed', 0.02), ('affect', 0.02), ('distribution', 0.02), ('huttenlocher', 0.02), ('became', 0.02), ('keys', 0.02), ('reproduced', 0.02), ('consequences', 0.02), ('excluded', 0.02), ('social', 0.019), ('differed', 0.019), ('perceptual', 0.018), ('capacities', 0.018), ('happens', 0.018), ('basic', 0.017), ('mean', 0.017), ('means', 0.017), ('experience', 0.017), ('screen', 0.017), ('ths', 0.017), ('correlations', 0.016), ('testing', 0.016), ('erlbaum', 0.016), ('affected', 0.016), ('convergence', 0.016), ('grif', 0.016), ('iterated', 0.016), ('ects', 0.016), ('psychology', 0.016), ('posterior', 0.015), ('gaussian', 0.015), ('uenced', 0.015), ('tried', 0.015), ('agents', 0.015), ('biased', 0.015), ('tested', 0.015), ('analyzed', 0.014), ('suggesting', 0.014), ('design', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999976 <a title="100-tfidf-1" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<p>Author: Jing Xu, Thomas L. Griffiths</p><p>Abstract: Many human interactions involve pieces of information being passed from one person to another, raising the question of how this process of information transmission is affected by the capacities of the agents involved. In the 1930s, Sir Frederic Bartlett explored the inﬂuence of memory biases in “serial reproduction” of information, in which one person’s reconstruction of a stimulus from memory becomes the stimulus seen by the next person. These experiments were done using relatively uncontrolled stimuli such as pictures and stories, but suggested that serial reproduction would transform information in a way that reﬂected the biases inherent in memory. We formally analyze serial reproduction using a Bayesian model of reconstruction from memory, giving a general result characterizing the effect of memory biases on information transmission. We then test the predictions of this account in two experiments using simple one-dimensional stimuli. Our results provide theoretical and empirical justiﬁcation for the idea that serial reproduction reﬂects memory biases. 1</p><p>2 0.12566832 <a title="100-tfidf-2" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>Author: Jeremy Reynolds, Michael C. Mozer</p><p>Abstract: Cognitive control refers to the ﬂexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efﬁciently reﬁned with subsequent task experience. 1</p><p>3 0.12013741 <a title="100-tfidf-3" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>Author: Thomas L. Griffiths, Chris Lucas, Joseph Williams, Michael L. Kalish</p><p>Abstract: Accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities: that people are estimating explicit functions, or that they are performing associative learning supported by similarity. We provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. Using the equivalence of Bayesian linear regression and Gaussian processes, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. We use this insight to deﬁne a Gaussian process model of human function learning that combines the strengths of both approaches. 1</p><p>4 0.084409527 <a title="100-tfidf-4" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>Author: Michael T. Todd, Yael Niv, Jonathan D. Cohen</p><p>Abstract: Working memory is a central topic of cognitive neuroscience because it is critical for solving real-world problems in which information from multiple temporally distant sources must be combined to generate appropriate behavior. However, an often neglected fact is that learning to use working memory effectively is itself a difficult problem. The Gating framework [14] is a collection of psychological models that show how dopamine can train the basal ganglia and prefrontal cortex to form useful working memory representations in certain types of problems. We unite Gating with machine learning theory concerning the general problem of memory-based optimal control [5-6]. We present a normative model that learns, by online temporal difference methods, to use working memory to maximize discounted future reward in partially observable settings. The model successfully solves a benchmark working memory problem, and exhibits limitations similar to those observed in humans. Our purpose is to introduce a concise, normative definition of high level cognitive concepts such as working memory and cognitive control in terms of maximizing discounted future rewards. 1 I n t ro d u c t i o n Working memory is loosely defined in cognitive neuroscience as information that is (1) internally maintained on a temporary or short term basis, and (2) required for tasks in which immediate observations cannot be mapped to correct actions. It is widely assumed that prefrontal cortex (PFC) plays a role in maintaining and updating working memory. However, relatively little is known about how PFC develops useful working memory representations for a new task. Furthermore, current work focuses on describing the structure and limitations of working memory, but does not ask why, or in what general class of tasks, is it necessary. Borrowing from the theory of optimal control in partially observable Markov decision problems (POMDPs), we frame the psychological concept of working memory as an internal state representation, developed and employed to maximize future reward in partially observable environments. We combine computational insights from POMDPs and neurobiologically plausible models from cognitive neuroscience to suggest a simple reinforcement learning (RL) model of working memory function that can be implemented through dopaminergic training of the basal ganglia and PFC. The Gating framework is a series of cognitive neuroscience models developed to explain how dopaminergic RL signals can shape useful working memory representations [1-4]. Computationally this framework models working memory as a collection of past observations, each of which can occasionally be replaced with the current observation, and addresses the problem of learning when to update each memory element versus maintaining it. In the original Gating model [1-2] the PFC contained a unitary working memory representation that was updated whenever a phasic dopamine (DA) burst occurred (e.g., due to unexpected reward or novelty). That model was the first to connect working memory and RL via the temporal difference (TD) model of DA firing [7-8], and thus to suggest how working memory might serve a normative purpose. However, that model had limited computational flexibility due to the unitary nature of the working memory (i.e., a singleobservation memory controlled by a scalar DA signal). More recent work [3-4] has partially repositioned the Gating framework within the Actor/Critic model of mesostriatal RL [9-10], positing memory updating as but another cortical action controlled by the dorsal striatal</p><p>5 0.081365675 <a title="100-tfidf-5" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>Author: Rui M. Castro, Charles Kalish, Robert Nowak, Ruichen Qian, Tim Rogers, Xiaojin Zhu</p><p>Abstract: We investigate a topic at the interface of machine learning and cognitive science. Human active learning, where learners can actively query the world for information, is contrasted with passive learning from random examples. Furthermore, we compare human active learning performance with predictions from statistical learning theory. We conduct a series of human category learning experiments inspired by a machine learning task for which active and passive learning error bounds are well understood, and dramatically distinct. Our results indicate that humans are capable of actively selecting informative queries, and in doing so learn better and faster than if they are given random training data, as predicted by learning theory. However, the improvement over passive learning is not as dramatic as that achieved by machine active learning algorithms. To the best of our knowledge, this is the ﬁrst quantitative study comparing human category learning in active versus passive settings. 1</p><p>6 0.075052552 <a title="100-tfidf-6" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>7 0.072393134 <a title="100-tfidf-7" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>8 0.07195583 <a title="100-tfidf-8" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>9 0.057845451 <a title="100-tfidf-9" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>10 0.055392798 <a title="100-tfidf-10" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>11 0.048603803 <a title="100-tfidf-11" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>12 0.044199854 <a title="100-tfidf-12" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<p>13 0.04280287 <a title="100-tfidf-13" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>14 0.042194076 <a title="100-tfidf-14" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>15 0.041520383 <a title="100-tfidf-15" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>16 0.040805727 <a title="100-tfidf-16" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>17 0.039542772 <a title="100-tfidf-17" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>18 0.03788517 <a title="100-tfidf-18" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>19 0.037628002 <a title="100-tfidf-19" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>20 0.034807064 <a title="100-tfidf-20" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.11), (1, 0.046), (2, 0.083), (3, 0.012), (4, 0.043), (5, 0.015), (6, -0.034), (7, 0.095), (8, 0.07), (9, 0.068), (10, 0.002), (11, 0.033), (12, -0.062), (13, -0.039), (14, 0.022), (15, 0.125), (16, 0.066), (17, -0.036), (18, 0.051), (19, -0.02), (20, -0.098), (21, -0.069), (22, 0.113), (23, 0.014), (24, -0.078), (25, -0.025), (26, 0.012), (27, 0.035), (28, -0.002), (29, -0.042), (30, -0.049), (31, 0.023), (32, -0.048), (33, -0.038), (34, -0.064), (35, 0.003), (36, -0.039), (37, -0.102), (38, 0.038), (39, 0.014), (40, -0.132), (41, 0.003), (42, -0.029), (43, 0.02), (44, 0.041), (45, -0.003), (46, -0.119), (47, -0.004), (48, -0.041), (49, 0.007)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94755185 <a title="100-lsi-1" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<p>Author: Jing Xu, Thomas L. Griffiths</p><p>Abstract: Many human interactions involve pieces of information being passed from one person to another, raising the question of how this process of information transmission is affected by the capacities of the agents involved. In the 1930s, Sir Frederic Bartlett explored the inﬂuence of memory biases in “serial reproduction” of information, in which one person’s reconstruction of a stimulus from memory becomes the stimulus seen by the next person. These experiments were done using relatively uncontrolled stimuli such as pictures and stories, but suggested that serial reproduction would transform information in a way that reﬂected the biases inherent in memory. We formally analyze serial reproduction using a Bayesian model of reconstruction from memory, giving a general result characterizing the effect of memory biases on information transmission. We then test the predictions of this account in two experiments using simple one-dimensional stimuli. Our results provide theoretical and empirical justiﬁcation for the idea that serial reproduction reﬂects memory biases. 1</p><p>2 0.64263016 <a title="100-lsi-2" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>Author: Jeremy Reynolds, Michael C. Mozer</p><p>Abstract: Cognitive control refers to the ﬂexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efﬁciently reﬁned with subsequent task experience. 1</p><p>3 0.59108126 <a title="100-lsi-3" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>Author: Thomas L. Griffiths, Chris Lucas, Joseph Williams, Michael L. Kalish</p><p>Abstract: Accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities: that people are estimating explicit functions, or that they are performing associative learning supported by similarity. We provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. Using the equivalence of Bayesian linear regression and Gaussian processes, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. We use this insight to deﬁne a Gaussian process model of human function learning that combines the strengths of both approaches. 1</p><p>4 0.58503705 <a title="100-lsi-4" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>Author: Rui M. Castro, Charles Kalish, Robert Nowak, Ruichen Qian, Tim Rogers, Xiaojin Zhu</p><p>Abstract: We investigate a topic at the interface of machine learning and cognitive science. Human active learning, where learners can actively query the world for information, is contrasted with passive learning from random examples. Furthermore, we compare human active learning performance with predictions from statistical learning theory. We conduct a series of human category learning experiments inspired by a machine learning task for which active and passive learning error bounds are well understood, and dramatically distinct. Our results indicate that humans are capable of actively selecting informative queries, and in doing so learn better and faster than if they are given random training data, as predicted by learning theory. However, the improvement over passive learning is not as dramatic as that achieved by machine active learning algorithms. To the best of our knowledge, this is the ﬁrst quantitative study comparing human category learning in active versus passive settings. 1</p><p>5 0.56902552 <a title="100-lsi-5" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>Author: Matt Jones, Sachiko Kinoshita, Michael C. Mozer</p><p>Abstract: In most cognitive and motor tasks, speed-accuracy tradeoffs are observed: Individuals can respond slowly and accurately, or quickly yet be prone to errors. Control mechanisms governing the initiation of behavioral responses are sensitive not only to task instructions and the stimulus being processed, but also to the recent stimulus history. When stimuli can be characterized on an easy-hard dimension (e.g., word frequency in a naming task), items preceded by easy trials are responded to more quickly, and with more errors, than items preceded by hard trials. We propose a rationally motivated mathematical model of this sequential adaptation of control, based on a diffusion model of the decision process in which difﬁculty corresponds to the drift rate for the correct response. The model assumes that responding is based on the posterior distribution over which response is correct, conditioned on the accumulated evidence. We derive this posterior as a function of the drift rate, and show that higher estimates of the drift rate lead to (normatively) faster responding. Trial-by-trial tracking of difﬁculty thus leads to sequential effects in speed and accuracy. Simulations show the model explains a variety of phenomena in human speeded decision making. We argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theories based on elaborate control structures. 1</p><p>6 0.53521895 <a title="100-lsi-6" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>7 0.52254057 <a title="100-lsi-7" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>8 0.50344372 <a title="100-lsi-8" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>9 0.50090766 <a title="100-lsi-9" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>10 0.47199404 <a title="100-lsi-10" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>11 0.44585055 <a title="100-lsi-11" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>12 0.40844247 <a title="100-lsi-12" href="./nips-2008-A_computational_model_of_hippocampal_function_in_trace_conditioning.html">7 nips-2008-A computational model of hippocampal function in trace conditioning</a></p>
<p>13 0.40829292 <a title="100-lsi-13" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>14 0.39523548 <a title="100-lsi-14" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>15 0.38987571 <a title="100-lsi-15" href="./nips-2008-Psychiatry%3A_Insights_into_depression_through_normative_decision-making_models.html">187 nips-2008-Psychiatry: Insights into depression through normative decision-making models</a></p>
<p>16 0.38705239 <a title="100-lsi-16" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>17 0.37517065 <a title="100-lsi-17" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<p>18 0.36064976 <a title="100-lsi-18" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>19 0.35049552 <a title="100-lsi-19" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>20 0.34705722 <a title="100-lsi-20" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.035), (7, 0.074), (12, 0.03), (15, 0.043), (28, 0.172), (57, 0.386), (59, 0.011), (63, 0.016), (77, 0.029), (83, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.98723197 <a title="100-lda-1" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>Author: Leo Zhu, Yuanhao Chen, Yuan Lin, Chenxi Lin, Alan L. Yuille</p><p>Abstract: Language and image understanding are two major goals of artiﬁcial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. Natural language researchers have made great progress by exploiting the 1D structure of language to design efﬁcient polynomialtime parsing algorithms. By contrast, the two-dimensional nature of images makes it much harder to design efﬁcient image parsers and the form of the hierarchical representations is also unclear. Attempts to adapt representations and algorithms from natural language have only been partially successful. In this paper, we propose a Hierarchical Image Model (HIM) for 2D image parsing which outputs image segmentation and object recognition. This HIM is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. Firstly, the HIM has a coarse-to-ﬁne representation which is capable of capturing long-range dependency and exploiting different levels of contextual information. Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dynamic programming, which enables us to parse the image rapidly in polynomial time. Thirdly, we can learn the HIM efﬁciently in a discriminative manner from a labeled dataset. We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena. 1</p><p>2 0.98131895 <a title="100-lda-2" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>Author: Iain Murray, David MacKay, Ryan P. Adams</p><p>Abstract: We present the Gaussian Process Density Sampler (GPDS), an exchangeable generative model for use in nonparametric Bayesian density estimation. Samples drawn from the GPDS are consistent with exact, independent samples from a ﬁxed density function that is a transformation of a function drawn from a Gaussian process prior. Our formulation allows us to infer an unknown density from data using Markov chain Monte Carlo, which gives samples from the posterior distribution over density functions and from the predictive distribution on data space. We can also infer the hyperparameters of the Gaussian process. We compare this density modeling technique to several existing techniques on a toy problem and a skullreconstruction task. 1</p><p>3 0.97914869 <a title="100-lda-3" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>Author: Viren Jain, Sebastian Seung</p><p>Abstract: We present an approach to low-level vision that combines two main ideas: the use of convolutional networks as an image processing architecture and an unsupervised learning procedure that synthesizes training samples from speciﬁc noise models. We demonstrate this approach on the challenging problem of natural image denoising. Using a test set with a hundred natural images, we ﬁnd that convolutional networks provide comparable and in some cases superior performance to state of the art wavelet and Markov random ﬁeld (MRF) methods. Moreover, we ﬁnd that a convolutional network offers similar performance in the blind denoising setting as compared to other techniques in the non-blind setting. We also show how convolutional networks are mathematically related to MRF approaches by presenting a mean ﬁeld theory for an MRF specially designed for image denoising. Although these approaches are related, convolutional networks avoid computational difﬁculties in MRF approaches that arise from probabilistic learning and inference. This makes it possible to learn image processing architectures that have a high degree of representational power (we train models with over 15,000 parameters), but whose computational expense is signiﬁcantly less than that associated with inference in MRF approaches with even hundreds of parameters. 1 Background Low-level image processing tasks include edge detection, interpolation, and deconvolution. These tasks are useful both in themselves, and as a front-end for high-level visual tasks like object recognition. This paper focuses on the task of denoising, deﬁned as the recovery of an underlying image from an observation that has been subjected to Gaussian noise. One approach to image denoising is to transform an image from pixel intensities into another representation where statistical regularities are more easily captured. For example, the Gaussian scale mixture (GSM) model introduced by Portilla and colleagues is based on a multiscale wavelet decomposition that provides an effective description of local image statistics [1, 2]. Another approach is to try and capture statistical regularities of pixel intensities directly using Markov random ﬁelds (MRFs) to deﬁne a prior over the image space. Initial work used handdesigned settings of the parameters, but recently there has been increasing success in learning the parameters of such models from databases of natural images [3, 4, 5, 6, 7, 8]. Prior models can be used for tasks such as image denoising by augmenting the prior with a noise model. Alternatively, an MRF can be used to model the probability distribution of the clean image conditioned on the noisy image. This conditional random ﬁeld (CRF) approach is said to be discriminative, in contrast to the generative MRF approach. Several researchers have shown that the CRF approach can outperform generative learning on various image restoration and labeling tasks [9, 10]. CRFs have recently been applied to the problem of image denoising as well [5]. 1 The present work is most closely related to the CRF approach. Indeed, certain special cases of convolutional networks can be seen as performing maximum likelihood inference on a CRF [11]. The advantage of the convolutional network approach is that it avoids a general difﬁculty with applying MRF-based methods to image analysis: the computational expense associated with both parameter estimation and inference in probabilistic models. For example, naive methods of learning MRFbased models involve calculation of the partition function, a normalization factor that is generally intractable for realistic models and image dimensions. As a result, a great deal of research has been devoted to approximate MRF learning and inference techniques that meliorate computational difﬁculties, generally at the cost of either representational power or theoretical guarantees [12, 13]. Convolutional networks largely avoid these difﬁculties by posing the computational task within the statistical framework of regression rather than density estimation. Regression is a more tractable computation and therefore permits models with greater representational power than methods based on density estimation. This claim will be argued for with empirical results on the denoising problem, as well as mathematical connections between MRF and convolutional network approaches. 2 Convolutional Networks Convolutional networks have been extensively applied to visual object recognition using architectures that accept an image as input and, through alternating layers of convolution and subsampling, produce one or more output values that are thresholded to yield binary predictions regarding object identity [14, 15]. In contrast, we study networks that accept an image as input and produce an entire image as output. Previous work has used such architectures to produce images with binary targets in image restoration problems for specialized microscopy data [11, 16]. Here we show that similar architectures can also be used to produce images with the analog ﬂuctuations found in the intensity distributions of natural images. Network Dynamics and Architecture A convolutional network is an alternating sequence of linear ﬁltering and nonlinear transformation operations. The input and output layers include one or more images, while intermediate layers contain “hidden</p><p>4 0.97270721 <a title="100-lda-4" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>Author: Daniel M. Roy, Yee W. Teh</p><p>Abstract: We describe a novel class of distributions, called Mondrian processes, which can be interpreted as probability distributions over kd-tree data structures. Mondrian processes are multidimensional generalizations of Poisson processes and this connection allows us to construct multidimensional generalizations of the stickbreaking process described by Sethuraman (1994), recovering the Dirichlet process in one dimension. After introducing the Aldous-Hoover representation for jointly and separately exchangeable arrays, we show how the process can be used as a nonparametric prior distribution in Bayesian models of relational data. 1</p><p>5 0.96762705 <a title="100-lda-5" href="./nips-2008-Extended_Grassmann_Kernels_for_Subspace-Based_Learning.html">80 nips-2008-Extended Grassmann Kernels for Subspace-Based Learning</a></p>
<p>Author: Jihun Hamm, Daniel D. Lee</p><p>Abstract: Subspace-based learning problems involve data whose elements are linear subspaces of a vector space. To handle such data structures, Grassmann kernels have been proposed and used previously. In this paper, we analyze the relationship between Grassmann kernels and probabilistic similarity measures. Firstly, we show that the KL distance in the limit yields the Projection kernel on the Grassmann manifold, whereas the Bhattacharyya kernel becomes trivial in the limit and is suboptimal for subspace-based problems. Secondly, based on our analysis of the KL distance, we propose extensions of the Projection kernel which can be extended to the set of afﬁne as well as scaled subspaces. We demonstrate the advantages of these extended kernels for classiﬁcation and recognition tasks with Support Vector Machines and Kernel Discriminant Analysis using synthetic and real image databases. 1</p><p>same-paper 6 0.91214567 <a title="100-lda-6" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<p>7 0.878739 <a title="100-lda-7" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>8 0.84316421 <a title="100-lda-8" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>9 0.79897565 <a title="100-lda-9" href="./nips-2008-Offline_Handwriting_Recognition_with_Multidimensional_Recurrent_Neural_Networks.html">158 nips-2008-Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks</a></p>
<p>10 0.78254235 <a title="100-lda-10" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>11 0.76793551 <a title="100-lda-11" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>12 0.75650853 <a title="100-lda-12" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>13 0.75514209 <a title="100-lda-13" href="./nips-2008-Bayesian_Synchronous_Grammar_Induction.html">35 nips-2008-Bayesian Synchronous Grammar Induction</a></p>
<p>14 0.7482537 <a title="100-lda-14" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>15 0.73989099 <a title="100-lda-15" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>16 0.73859 <a title="100-lda-16" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>17 0.7371949 <a title="100-lda-17" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>18 0.73688209 <a title="100-lda-18" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>19 0.72788429 <a title="100-lda-19" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>20 0.72656214 <a title="100-lda-20" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
