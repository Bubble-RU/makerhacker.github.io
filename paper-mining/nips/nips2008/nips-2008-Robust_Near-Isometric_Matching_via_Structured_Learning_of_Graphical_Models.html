<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-201" href="#">nips2008-201</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</h1>
<br/><p>Source: <a title="nips-2008-201-pdf" href="http://papers.nips.cc/paper/3464-robust-near-isometric-matching-via-structured-learning-of-graphical-models.pdf">pdf</a></p><p>Author: Alex J. Smola, Julian J. Mcauley, Tibério S. Caetano</p><p>Abstract: Models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the isometric assumption. However, real shapes from image datasets, even when expected to be related by “almost isometric” transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale. In this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction. The outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations. Our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times. 1</p><p>Reference: <a title="nips-2008-201-reference" href="../nips2008_reference/nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  Abstract Models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the isometric assumption. [sent-12, score-0.78]
</p><p>2 However, real shapes from image datasets, even when expected to be related by “almost isometric” transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale. [sent-13, score-0.289]
</p><p>3 In this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction. [sent-14, score-0.374]
</p><p>4 The outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations. [sent-15, score-0.614]
</p><p>5 Our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times. [sent-16, score-0.094]
</p><p>6 1  Introduction  Matching shapes in images has many applications, including image retrieval, alignment, and registration [1, 2, 3, 4]. [sent-17, score-0.184]
</p><p>7 Typically, matching is approached by selecting features for a set of landmark points in both images; a correspondence between the two is then chosen such that some distance measure between these features is minimised. [sent-18, score-0.718]
</p><p>8 A great deal of attention has been devoted to deﬁning complex features which are robust to changes in rotation, scale etc. [sent-19, score-0.236]
</p><p>9 1 An important class of matching problems is that of near-isometric shape matching. [sent-21, score-0.409]
</p><p>10 In this setting, it is assumed that shapes are deﬁned up to an isometric transformation (allowing for some noise), and therefore distance features are typically used to encode the shape. [sent-22, score-0.521]
</p><p>11 Recent work has shown how the isometric constraint can be exploited by a particular type of graphical model whose topology encodes the necessary properties for obtaining optimal matches in polynomial time [11]. [sent-23, score-0.551]
</p><p>12 Another line of work has focused on structured learning to optimize graph matching scores, however no explicit exploitation of the geometrical constraints involved in shape modeling are made [12]. [sent-24, score-0.579]
</p><p>13 We produce an exact, efﬁcient model to solve near-isometric shape matching problems using not only isometryinvariant features, but also appearance and scale-invariant features. [sent-26, score-0.53]
</p><p>14 By doing so we can learn the relative importances of variations in appearance and scale with regard to variations in shape per se. [sent-27, score-0.573]
</p><p>15 Therefore, even knowing that we are in a near-isometric setting, we will capture the eventual variations in appearance and scale into our matching criterion in order to produce a robust nearisometric matcher. [sent-28, score-0.447]
</p><p>16 In terms of learning, we introduce a two-stage structured learning approach to address the speed and memory efﬁciency of this model. [sent-29, score-0.137]
</p><p>17 1  1  Figure 1: The graphical model introduced in [11]. [sent-36, score-0.119]
</p><p>18 Here we study the case of identifying an instance of a template shape (S ⊆ T ) in a target scene (U) [1]. [sent-39, score-0.46]
</p><p>19 the points in the template that we want to query in the scene. [sent-42, score-0.227]
</p><p>20 For each point t ∈ T and u ∈ U, a certain set of unary features are extracted (here denoted by φ(t), φ(u)), which contain local information about the image at that point [5, 6]. [sent-44, score-0.472]
</p><p>21 (1) is a linear assignment problem, efﬁciently solvable in cubic time. [sent-50, score-0.122]
</p><p>22 In addition to unary or ﬁrst-order features, pairwise or second-order features can be induced from the locations of the unary features. [sent-51, score-0.741]
</p><p>23 (1) would be generalised to minimise an aggregate distance between pairwise features. [sent-53, score-0.177]
</p><p>24 Discriminative structured learning has recently been applied to models of both linear and quadratic assignment in [12]. [sent-55, score-0.365]
</p><p>25 2  Graphical Models  In isometric matching settings, one may suspect that it may not be necessary to include all pairwise relations in quadratic assignment. [sent-57, score-0.559]
</p><p>26 In fact a recent paper [11] has shown that if only the distances as encoded by the graphical model depicted in ﬁgure 1 are taken into account (nodes represent points in S and states represent points in U), exact probabilistic inference in such a model can solve the isometric problem optimally. [sent-58, score-0.613]
</p><p>27 (2)  i=1  In [11], it is shown that loopy belief propagation using this model converges to the optimal assignment, and that the number of iterations required before convergence is small in practice. [sent-60, score-0.095]
</p><p>28 We will extend this model by adding a unary term, c1 (si , y(si )) (as in (eq. [sent-61, score-0.305]
</p><p>29 2 Here T is the set of all points in the template scene, whereas S corresponds to those points in which we are interested. [sent-64, score-0.311]
</p><p>30 3  Discriminative Structured Learning  In practice, feature vectors may be very high-dimensional, and which components are ‘important’ will depend on the speciﬁc properties of the shapes being matched. [sent-70, score-0.104]
</p><p>31 Therefore, we introduce a parameter, θ, which controls the relative importances of the various feature components. [sent-71, score-0.094]
</p><p>32 Note that θ is parameterising the matching criterion itself. [sent-72, score-0.194]
</p><p>33 4 In order to measure the performance of a particular weight vector, we use a loss function, ∆(ˆ, y i ), which represents the cost incurred by choosing the assignment y when the correct y ˆ assignment is y i (our speciﬁc choice of loss function is described in section 4). [sent-75, score-0.435]
</p><p>34 Learning in this setting now becomes a matter of choosing θ such that the empirical risk (average loss on all training instances) is minimised, but which is also sufﬁciently ‘smooth’ (to prevent overﬁtting). [sent-78, score-0.153]
</p><p>35 y N , then we wish to minimise 1 N  N  ∆(f (S i , U i ; θ), y i ) + i=1  empirical risk  λ θ 2  2 2  . [sent-88, score-0.106]
</p><p>36 (5)  regulariser  Here λ (the regularisation constant) controls the relative importance of minimising the empirical risk against the regulariser. [sent-89, score-0.167]
</p><p>37 Here we capitalise on recent advances in large-margin structured estimation [15], which consist of obtaining convex relaxations of this problem. [sent-93, score-0.103]
</p><p>38 This means that we end up minimising an upper bound on the loss, instead of the loss itself. [sent-96, score-0.123]
</p><p>39 3  Figure 2: Left: the (ordered) set of points in our template shape (S). [sent-107, score-0.442]
</p><p>40 3  Our Model  Although the model of [11] solves isometric matching problems optimally, it provides no guarantees for near-isometric problems, as it only considers those compatibilities which form cliques in our graphical model. [sent-110, score-0.59]
</p><p>41 With this in mind, we introduce three new features (for brevity we use the shorthand yi = y(si )): Φ1 (s1 , s2 , y1 , y2 ) = (d1 (s1 , s2 ) − d1 (y1 , y2 ))2 , where d1 (a, b) is the Euclidean distance between a and b, scaled according to the width of the target scene. [sent-112, score-0.292]
</p><p>42 Φ2 (s1 , s2 , s3 , y1 , y2 , y3 ) = (d2 (s1 , s2 , s3 ) − d2 (y1 , y2 , y3 ))2 , where d2 (a, b, c) is the Euclidean distance between a and b scaled by the average of the distances between a, b, and c. [sent-113, score-0.147]
</p><p>43 5 We also include the unary features Φ0 (s1 , y1 ) = (φ(s1 ) − φ(y1 ))2 (i. [sent-118, score-0.427]
</p><p>44 Φ1 is exactly the feature used in [11], and is invariant to isometric transformations (rotation, reﬂection, and translation); Φ2 and Φ3 capture triangle similarity, and are thus also invariant to scale. [sent-121, score-0.409]
</p><p>45 (8) In practice, landmark detectors often identify several hundred points [6, 17], which is clearly impractical for an O(|S||U|3 ) method (|U| is the number of landmarks in the target scene). [sent-124, score-0.452]
</p><p>46 To address this, we adopt a two stage learning approach: in the ﬁrst stage, we learn only unary compatibilities, exactly as is done in [12]. [sent-125, score-0.523]
</p><p>47 During the second stage of learning, we collapse the ﬁrst-order feature vector into a single term, namely Φ0 (s1 , y1 ) = θ0 , Φ0 (s1 , y1 )  (9)  (θ0 is the weight vector learned during the ﬁrst stage). [sent-126, score-0.286]
</p><p>48 We now perform learning for the third-order model, but consider only the p ‘most likely’ matches for each node, where the likelihood is simply determined using Φ0 (s1 , y1 ). [sent-127, score-0.155]
</p><p>49 A consequence of using this approach is that we must now tune two regularisation constants; this is not an issue in practice, as learning can be performed quickly using this approach. [sent-129, score-0.103]
</p><p>50 6 5  Using features of such different scales can be an issue for regularisation – in practice we adjusted these features to have roughly the same scale. [sent-130, score-0.373]
</p><p>51 6 In fact, even in those cases where a single stage approach was tractable (such as the experiment in section 4. [sent-132, score-0.214]
</p><p>52 1), we found that the two stage approach worked better. [sent-133, score-0.251]
</p><p>53 Typically, we required much less regularity during the second stage, possibly because the higher order features are heterogeneous. [sent-134, score-0.152]
</p><p>54 4  Figure 3: Left: The adjacency structure of the graph (top); the boundary of our ‘shape’ (centre); the topology of our graphical model (bottom). [sent-135, score-0.257]
</p><p>55 Right: Example matches using linear assignment (top, 6/30 mismatches), quadratic assignment (centre, 4/30 mismatches), and the proposed model (bottom, no mismatches). [sent-136, score-0.501]
</p><p>56 The images shown are the 12th and 102nd frames in our sequence. [sent-137, score-0.118]
</p><p>57 Correct matches are shown in green, incorrect matches in red. [sent-138, score-0.242]
</p><p>58 Both papers report the performance of their methods on the CMU ‘house’ sequence – a sequence of 111 frames of a toy house, with 30 landmarks identiﬁed in each frame. [sent-142, score-0.225]
</p><p>59 7 As in [12], we compute the Shape Context features for each of the 30 points [5]. [sent-143, score-0.236]
</p><p>60 In addition to the unary model of [12], a model based on quadratic assignment is also presented, in which pairwise features are determined using the adjacency structure of the graphs. [sent-144, score-0.811]
</p><p>61 Speciﬁcally, if a pair of points (p1 , p2 ) in the template scene is to be matched to (q1 , q2 ) in the target, there is a feature which is 1 if there is an edge between p1 and p2 in the template, and an edge between q1 and q2 in the target (and 0 otherwise). [sent-145, score-0.426]
</p><p>62 We also use such a feature for this experiment, however our model only considers matchings for which (p1 , p2 ) forms an edge in our graphical model (see ﬁgure 3, bottom left). [sent-146, score-0.245]
</p><p>63 As in [11], we compare pairs of images with a ﬁxed baseline (separation between frames). [sent-148, score-0.14]
</p><p>64 For our loss function, ∆(ˆ, y i ), we used the normalised Hamming loss, i. [sent-149, score-0.205]
</p><p>65 In ﬁgure 5, we see that the running time of our method is similar to the quadratic assignment method of [12]. [sent-155, score-0.35]
</p><p>66 for each point in the template scene, we only consider the 10 ‘most likely’ matches, using the weights from the ﬁrst stage of learning. [sent-158, score-0.41]
</p><p>67 html Interestingly, the quadratic method of [12] performs worse than their unary method; this is likely because the relative scale of the unary and quadratic features is badly tuned before learning, and is indeed similar to what the authors report. [sent-163, score-0.99]
</p><p>68 Furthermore, the results we present for the method of [12] after learning are much better than what the authors report – in that paper, the unary features are scaled using a pointwise exponent (− exp(−|φa − φb |2 )), whereas we found that scaling the features linearly (|φa − φb |2 ) worked better. [sent-164, score-0.776]
</p><p>69 8  5  House data, learning Normalised Hamming loss on test set  Normalised Hamming loss on test set  House data, no learning 1 point matching linear quadratic higher order  0. [sent-165, score-0.516]
</p><p>70 3 linear (learning) quadratic (learning) higher order (learning, 10 points) higher order (learning)  0. [sent-170, score-0.106]
</p><p>71 25  linear (learning) quadratic (learning) higher order (learning, 10 points) higher order (learning)  0. [sent-181, score-0.106]
</p><p>72 1 Average running time (seconds, logarithmic scale)  1  Figure 5: The running time and performance of our method, compared to those of [12] (note that the method of [11] has running time identical to our method). [sent-189, score-0.216]
</p><p>73 magnitude, bringing it closer to that of linear assignment; even this model achieves approximately zero error up to a baseline of 50. [sent-191, score-0.106]
</p><p>74 Finally, ﬁgure 6 (left) shows the weight vector of our model, for a baseline of 60. [sent-192, score-0.119]
</p><p>75 It is much more difﬁcult to reason about the second stage of learning, as the features have different scales, and cannot be compared directly – however, it appears that all of the higher-order features are important to our model. [sent-196, score-0.518]
</p><p>76 2  Bikes Data  For our second experiment, we used images of bicycles from the Caltech 256 Dataset [18]. [sent-198, score-0.162]
</p><p>77 Bicycles are reasonably rigid objects, meaning that matching based on their shape is logical. [sent-199, score-0.445]
</p><p>78 For each image in the dataset, we detected landmarks automatically, and six points on the frame were hand-labelled (see ﬁgure 7). [sent-201, score-0.3]
</p><p>79 Only shapes in which these interest points were not occluded were used, and we only included images that had a background; in total, we labelled 44 6  House data first/higher order weight vector (baseline = 60) 2 0. [sent-202, score-0.301]
</p><p>80 The ﬁrst 60 weights are for the Shape Context features from the ﬁrst stage of of learning; the ﬁnal 5 weights are for the second stage of learning. [sent-211, score-0.686]
</p><p>81 Left: The template image (with the shape outlined in green, and landmark points marked in blue). [sent-215, score-0.579]
</p><p>82 Centre: The target image, and the match (in red) using unary features with the afﬁne invariant/SIFT model of [17] after learning (endpoint error = 0. [sent-216, score-0.589]
</p><p>83 Right: the match using our model after learning (endpoint error = 0. [sent-218, score-0.123]
</p><p>84 Thus we are learning to match bicycles similar to the chosen template. [sent-222, score-0.191]
</p><p>85 Initially, we used the SIFT landmarks and features as described in [6]. [sent-223, score-0.323]
</p><p>86 Since we cannot hope to get exact matches, we use the endpoint error instead of the normalised Hamming loss, i. [sent-227, score-0.262]
</p><p>87 This may be explained by the fact that although the SIFT features are invariant to scale and rotation, they are not invariant to reﬂection. [sent-231, score-0.326]
</p><p>88 In [17], the authors report that the SIFT features can provide good matches in such cases, as long as landmarks are chosen which are locally invariant to afﬁne transformations. [sent-232, score-0.508]
</p><p>89 They give a method for identifying afﬁne-invariant feature points, whose SIFT features are then computed. [sent-233, score-0.211]
</p><p>90 Figure 7 shows an example match using both the unary and higher-order techniques. [sent-235, score-0.334]
</p><p>91 Interestingly, the ﬁrst-order term during the second stage of learning has almost zero weight. [sent-237, score-0.248]
</p><p>92 This must not be misinterpreted: during the second stage, the response of each of the 20 candidate points is so similar that the ﬁrst-order features are simply unable to convey any new information – yet they are still very useful in determining the 20 candidate points. [sent-238, score-0.236]
</p><p>93 9 Here the endpoint error is just the average Euclidean distance from the correct label, scaled according to the width of the image. [sent-239, score-0.232]
</p><p>94 The endpoint error is reported, with standard errors in parentheses (note that the second-last column, ‘higher-order’ uses the weights from the ﬁrst stage of learning, but not the second). [sent-242, score-0.398]
</p><p>95 039)  Afﬁne invariant/SIFT [17]  5  unary Training: 0. [sent-261, score-0.275]
</p><p>96 034)  Conclusion  We have presented a model for near-isometric shape matching which is robust to typical additional variations of the shape. [sent-291, score-0.555]
</p><p>97 This is achieved by performing structured learning in a graphical model that encodes features with several different types of invariances, so that we can directly learn a “compound invariance” instead of taking for granted the exclusive assumption of isometric invariance. [sent-292, score-0.671]
</p><p>98 Our experiments revealed that structured learning with a principled graphical model that encodes both the rigid shape as well as non-isometric variations gives substantial improvements, while still maintaining competitive performance in terms of running time. [sent-293, score-0.722]
</p><p>99 : Learning methods for generic object recognition with invariance to pose and lighting. [sent-339, score-0.119]
</p><p>100 : Support vector machine learning for interdependent and structured output spaces. [sent-366, score-0.137]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('si', 0.354), ('unary', 0.275), ('house', 0.228), ('isometric', 0.22), ('shape', 0.215), ('stage', 0.214), ('matching', 0.194), ('landmarks', 0.171), ('features', 0.152), ('template', 0.143), ('normalised', 0.131), ('endpoint', 0.131), ('assignment', 0.122), ('matches', 0.121), ('bikes', 0.114), ('quadratic', 0.106), ('malik', 0.106), ('structured', 0.103), ('centre', 0.102), ('sift', 0.098), ('bicycles', 0.098), ('caetano', 0.098), ('landmark', 0.092), ('appearance', 0.091), ('graphical', 0.089), ('hamming', 0.087), ('mismatches', 0.086), ('points', 0.084), ('variations', 0.078), ('baseline', 0.076), ('shapes', 0.075), ('loss', 0.074), ('regularisation', 0.069), ('belongie', 0.069), ('barbosa', 0.065), ('importances', 0.065), ('mcauley', 0.065), ('bins', 0.065), ('invariant', 0.064), ('images', 0.064), ('af', 0.064), ('gure', 0.063), ('scene', 0.063), ('running', 0.062), ('match', 0.059), ('smola', 0.059), ('scaled', 0.057), ('compatibilities', 0.057), ('minimise', 0.057), ('adjacency', 0.057), ('pami', 0.057), ('object', 0.056), ('radial', 0.054), ('frames', 0.054), ('weights', 0.053), ('mori', 0.052), ('minimised', 0.052), ('risk', 0.049), ('minimising', 0.049), ('topology', 0.048), ('nicta', 0.046), ('distances', 0.046), ('scale', 0.046), ('image', 0.045), ('distance', 0.044), ('felzenszwalb', 0.044), ('weight', 0.043), ('encodes', 0.043), ('rotation', 0.04), ('pointwise', 0.039), ('australian', 0.039), ('pairwise', 0.039), ('target', 0.039), ('robust', 0.038), ('aggregate', 0.037), ('worked', 0.037), ('hundred', 0.036), ('rigid', 0.036), ('alexander', 0.036), ('propagation', 0.035), ('labelled', 0.035), ('euclidean', 0.035), ('learning', 0.034), ('edge', 0.034), ('graph', 0.033), ('invariance', 0.033), ('bottom', 0.033), ('transformations', 0.032), ('substantial', 0.032), ('argmax', 0.031), ('context', 0.03), ('validation', 0.03), ('typically', 0.03), ('model', 0.03), ('matter', 0.03), ('depicted', 0.03), ('recognition', 0.03), ('method', 0.03), ('belief', 0.03), ('feature', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="201-tfidf-1" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>Author: Alex J. Smola, Julian J. Mcauley, Tibério S. Caetano</p><p>Abstract: Models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the isometric assumption. However, real shapes from image datasets, even when expected to be related by “almost isometric” transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale. In this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction. The outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations. Our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times. 1</p><p>2 0.19421262 <a title="201-tfidf-2" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>Author: Geremy Heitz, Gal Elidan, Benjamin Packer, Daphne Koller</p><p>Abstract: Discriminative tasks, including object categorization and detection, are central components of high-level computer vision. Sometimes, however, we are interested in more reﬁned aspects of the object in an image, such as pose or particular regions. In this paper we develop a method (LOOPS) for learning a shape and image feature model that can be trained on a particular object class, and used to outline instances of the class in novel images. Furthermore, while the training data consists of uncorresponded outlines, the resulting LOOPS model contains a set of landmark points that appear consistently across instances, and can be accurately localized in an image. Our model achieves state-of-the-art results in precisely outlining objects that exhibit large deformations and articulations in cluttered natural images. These localizations can then be used to address a range of tasks, including descriptive classiﬁcation, search, and clustering. 1</p><p>3 0.18994172 <a title="201-tfidf-3" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>Author: Christoph Kolodziejski, Bernd Porr, Minija Tamosiunaite, Florentin Wörgötter</p><p>Abstract: In this theoretical contribution we provide mathematical proof that two of the most important classes of network learning - correlation-based differential Hebbian learning and reward-based temporal difference learning - are asymptotically equivalent when timing the learning with a local modulatory signal. This opens the opportunity to consistently reformulate most of the abstract reinforcement learning framework from a correlation based perspective that is more closely related to the biophysics of neurons. 1</p><p>4 0.15586901 <a title="201-tfidf-4" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>5 0.12967254 <a title="201-tfidf-5" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>Author: Abhinav Gupta, Jianbo Shi, Larry S. Davis</p><p>Abstract: We present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufﬁcient for representing context. Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or “informative” background(context). We present a “shape-aware” model which utilizes contour information for efﬁcient and accurate labeling of features in the image. Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity. 1</p><p>6 0.12410012 <a title="201-tfidf-6" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>7 0.11235752 <a title="201-tfidf-7" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>8 0.093238421 <a title="201-tfidf-8" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>9 0.091645613 <a title="201-tfidf-9" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>10 0.090835035 <a title="201-tfidf-10" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>11 0.090047367 <a title="201-tfidf-11" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>12 0.089981109 <a title="201-tfidf-12" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>13 0.087590076 <a title="201-tfidf-13" href="./nips-2008-Multiscale_Random_Fields_with_Application_to_Contour_Grouping.html">147 nips-2008-Multiscale Random Fields with Application to Contour Grouping</a></p>
<p>14 0.084723823 <a title="201-tfidf-14" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>15 0.081172526 <a title="201-tfidf-15" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>16 0.080997713 <a title="201-tfidf-16" href="./nips-2008-Nonrigid_Structure_from_Motion_in_Trajectory_Space.html">157 nips-2008-Nonrigid Structure from Motion in Trajectory Space</a></p>
<p>17 0.079458505 <a title="201-tfidf-17" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>18 0.077884383 <a title="201-tfidf-18" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>19 0.07540863 <a title="201-tfidf-19" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>20 0.075278036 <a title="201-tfidf-20" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.238), (1, -0.065), (2, 0.07), (3, -0.104), (4, 0.043), (5, -0.012), (6, -0.068), (7, -0.194), (8, 0.05), (9, -0.044), (10, -0.065), (11, -0.008), (12, -0.007), (13, -0.039), (14, 0.095), (15, 0.01), (16, 0.156), (17, 0.135), (18, 0.041), (19, 0.102), (20, -0.003), (21, -0.005), (22, -0.097), (23, 0.042), (24, 0.05), (25, 0.178), (26, -0.048), (27, -0.082), (28, -0.036), (29, 0.065), (30, -0.073), (31, -0.064), (32, 0.061), (33, -0.076), (34, -0.002), (35, 0.091), (36, 0.081), (37, -0.039), (38, -0.126), (39, 0.031), (40, 0.035), (41, -0.03), (42, -0.08), (43, -0.004), (44, -0.037), (45, 0.037), (46, -0.048), (47, -0.017), (48, -0.019), (49, 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9549402 <a title="201-lsi-1" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>Author: Alex J. Smola, Julian J. Mcauley, Tibério S. Caetano</p><p>Abstract: Models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the isometric assumption. However, real shapes from image datasets, even when expected to be related by “almost isometric” transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale. In this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction. The outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations. Our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times. 1</p><p>2 0.73573065 <a title="201-lsi-2" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>Author: Geremy Heitz, Gal Elidan, Benjamin Packer, Daphne Koller</p><p>Abstract: Discriminative tasks, including object categorization and detection, are central components of high-level computer vision. Sometimes, however, we are interested in more reﬁned aspects of the object in an image, such as pose or particular regions. In this paper we develop a method (LOOPS) for learning a shape and image feature model that can be trained on a particular object class, and used to outline instances of the class in novel images. Furthermore, while the training data consists of uncorresponded outlines, the resulting LOOPS model contains a set of landmark points that appear consistently across instances, and can be accurately localized in an image. Our model achieves state-of-the-art results in precisely outlining objects that exhibit large deformations and articulations in cluttered natural images. These localizations can then be used to address a range of tasks, including descriptive classiﬁcation, search, and clustering. 1</p><p>3 0.67977023 <a title="201-lsi-3" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>4 0.62482417 <a title="201-lsi-4" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>Author: Volkan Cevher, Marco F. Duarte, Chinmay Hegde, Richard Baraniuk</p><p>Abstract: Compressive Sensing (CS) combines sampling and compression into a single subNyquist linear measurement process for sparse and compressible signals. In this paper, we extend the theory of CS to include signals that are concisely represented in terms of a graphical model. In particular, we use Markov Random Fields (MRFs) to represent sparse signals whose nonzero coefﬁcients are clustered. Our new model-based recovery algorithm, dubbed Lattice Matching Pursuit (LaMP), stably recovers MRF-modeled signals using many fewer measurements and computations than the current state-of-the-art algorithms.</p><p>5 0.62254596 <a title="201-lsi-5" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>Author: Abhinav Gupta, Jianbo Shi, Larry S. Davis</p><p>Abstract: We present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufﬁcient for representing context. Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or “informative” background(context). We present a “shape-aware” model which utilizes contour information for efﬁcient and accurate labeling of features in the image. Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity. 1</p><p>6 0.60650319 <a title="201-lsi-6" href="./nips-2008-Multiscale_Random_Fields_with_Application_to_Contour_Grouping.html">147 nips-2008-Multiscale Random Fields with Application to Contour Grouping</a></p>
<p>7 0.60639668 <a title="201-lsi-7" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>8 0.44211811 <a title="201-lsi-8" href="./nips-2008-Nonrigid_Structure_from_Motion_in_Trajectory_Space.html">157 nips-2008-Nonrigid Structure from Motion in Trajectory Space</a></p>
<p>9 0.43948734 <a title="201-lsi-9" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>10 0.43722802 <a title="201-lsi-10" href="./nips-2008-An_ideal_observer_model_of_infant_object_perception.html">23 nips-2008-An ideal observer model of infant object perception</a></p>
<p>11 0.41896236 <a title="201-lsi-11" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>12 0.38667795 <a title="201-lsi-12" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>13 0.38439178 <a title="201-lsi-13" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>14 0.36907741 <a title="201-lsi-14" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>15 0.36656025 <a title="201-lsi-15" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>16 0.36470175 <a title="201-lsi-16" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>17 0.36254823 <a title="201-lsi-17" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>18 0.34930497 <a title="201-lsi-18" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>19 0.34912306 <a title="201-lsi-19" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>20 0.34711757 <a title="201-lsi-20" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.06), (7, 0.062), (12, 0.088), (28, 0.13), (57, 0.071), (59, 0.044), (63, 0.021), (71, 0.028), (77, 0.07), (81, 0.257), (83, 0.095)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84159958 <a title="201-lda-1" href="./nips-2008-Playing_Pinball_with_non-invasive_BCI.html">180 nips-2008-Playing Pinball with non-invasive BCI</a></p>
<p>Author: Matthias Krauledat, Konrad Grzeska, Max Sagebaum, Benjamin Blankertz, Carmen Vidaurre, Klaus-Robert Müller, Michael Schröder</p><p>Abstract: Compared to invasive Brain-Computer Interfaces (BCI), non-invasive BCI systems based on Electroencephalogram (EEG) signals have not been applied successfully for precisely timed control tasks. In the present study, however, we demonstrate and report on the interaction of subjects with a real device: a pinball machine. Results of this study clearly show that fast and well-timed control well beyond chance level is possible, even though the environment is extremely rich and requires precisely timed and complex predictive behavior. Using machine learning methods for mental state decoding, BCI-based pinball control is possible within the ﬁrst session without the necessity to employ lengthy subject training. The current study shows clearly that very compelling control with excellent timing and dynamics is possible for a non-invasive BCI. 1</p><p>same-paper 2 0.75039941 <a title="201-lda-2" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>Author: Alex J. Smola, Julian J. Mcauley, Tibério S. Caetano</p><p>Abstract: Models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the isometric assumption. However, real shapes from image datasets, even when expected to be related by “almost isometric” transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale. In this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction. The outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations. Our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times. 1</p><p>3 0.71316177 <a title="201-lda-3" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>Author: Julien Mairal, Jean Ponce, Guillermo Sapiro, Andrew Zisserman, Francis R. Bach</p><p>Abstract: It is now well established that sparse signal models are well suited for restoration tasks and can be effectively learned from audio, image, and video data. Recent research has been aimed at learning discriminative sparse models instead of purely reconstructive ones. This paper proposes a new step in that direction, with a novel sparse representation for signals belonging to different classes in terms of a shared dictionary and discriminative class models. The linear version of the proposed model admits a simple probabilistic interpretation, while its most general variant admits an interpretation in terms of kernels. An optimization framework for learning all the components of the proposed model is presented, along with experimental results on standard handwritten digit and texture classiﬁcation tasks. 1</p><p>4 0.64558095 <a title="201-lda-4" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>5 0.63276803 <a title="201-lda-5" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>Author: Kate Saenko, Trevor Darrell</p><p>Abstract: Polysemy is a problem for methods that exploit image search engines to build object category models. Existing unsupervised approaches do not take word sense into consideration. We propose a new method that uses a dictionary to learn models of visual word sense from a large collection of unlabeled web data. The use of LDA to discover a latent sense space makes the model robust despite the very limited nature of dictionary deﬁnitions. The deﬁnitions are used to learn a distribution in the latent space that best represents a sense. The algorithm then uses the text surrounding image links to retrieve images with high probability of a particular dictionary sense. An object classiﬁer is trained on the resulting sense-speciﬁc images. We evaluate our method on a dataset obtained by searching the web for polysemous words. Category classiﬁcation experiments show that our dictionarybased approach outperforms baseline methods. 1</p><p>6 0.63071054 <a title="201-lda-6" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>7 0.61615801 <a title="201-lda-7" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>8 0.61314219 <a title="201-lda-8" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>9 0.61189425 <a title="201-lda-9" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>10 0.61108601 <a title="201-lda-10" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>11 0.60936159 <a title="201-lda-11" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>12 0.60316753 <a title="201-lda-12" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>13 0.60117638 <a title="201-lda-13" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>14 0.59952104 <a title="201-lda-14" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>15 0.59835988 <a title="201-lda-15" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>16 0.59672755 <a title="201-lda-16" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>17 0.59484458 <a title="201-lda-17" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>18 0.59353024 <a title="201-lda-18" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>19 0.59086919 <a title="201-lda-19" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>20 0.59084219 <a title="201-lda-20" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
