<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-173" href="#">nips2008-173</a> knowledge-graph by maker-knowledge-mining</p><h1>173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</h1>
<br/><p>Source: <a title="nips-2008-173-pdf" href="http://papers.nips.cc/paper/3386-optimization-on-a-budget-a-reinforcement-learning-approach.pdf">pdf</a></p><p>Author: Paul L. Ruvolo, Ian Fasel, Javier R. Movellan</p><p>Abstract: Many popular optimization algorithms, like the Levenberg-Marquardt algorithm (LMA), use heuristic-based “controllers” that modulate the behavior of the optimizer during the optimization process. For example, in the LMA a damping parameter λ is dynamically modiﬁed based on a set of rules that were developed using heuristic arguments. Reinforcement learning (RL) is a machine learning approach to learn optimal controllers from examples and thus is an obvious candidate to improve the heuristic-based controllers implicit in the most popular and heavily used optimization algorithms. Improving the performance of off-the-shelf optimizers is particularly important for time-constrained optimization problems. For example the LMA algorithm has become popular for many real-time computer vision problems, including object tracking from video, where only a small amount of time can be allocated to the optimizer on each incoming video frame. Here we show that a popular modern reinforcement learning technique using a very simple state space can dramatically improve the performance of general purpose optimizers, like the LMA. Surprisingly the controllers learned for a particular domain also work well in very different optimization domains. For example we used RL methods to train a new controller for the damping parameter of the LMA. This controller was trained on a collection of classic, relatively small, non-linear regression problems. The modiﬁed LMA performed better than the standard LMA on these problems. This controller also dramatically outperformed the standard LMA on a difﬁcult computer vision problem for which it had not been trained. Thus the controller appeared to have extracted control rules that were not just domain speciﬁc but generalized across a range of optimization domains. 1</p><p>Reference: <a title="nips-2008-173-reference" href="../nips2008_reference/nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lma', 0.737), ('damp', 0.259), ('budget', 0.239), ('policy', 0.228), ('smil', 0.184), ('xk', 0.16), ('lspi', 0.155), ('control', 0.117), ('crossb', 0.092), ('rl', 0.088), ('loss', 0.08), ('window', 0.078), ('desc', 0.078), ('optim', 0.073), ('reinforc', 0.073), ('reward', 0.072), ('fac', 0.07), ('genk', 0.069), ('xopt', 0.069), ('lter', 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="173-tfidf-1" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>2 0.20153528 <a title="173-tfidf-2" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>3 0.19027448 <a title="173-tfidf-3" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>4 0.17492279 <a title="173-tfidf-4" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>5 0.11835305 <a title="173-tfidf-5" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>6 0.11628149 <a title="173-tfidf-6" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>7 0.10998322 <a title="173-tfidf-7" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>8 0.10585801 <a title="173-tfidf-8" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>9 0.10229324 <a title="173-tfidf-9" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>10 0.095028847 <a title="173-tfidf-10" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>11 0.088538624 <a title="173-tfidf-11" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>12 0.08814995 <a title="173-tfidf-12" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>13 0.082803972 <a title="173-tfidf-13" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>14 0.071530834 <a title="173-tfidf-14" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>15 0.07036531 <a title="173-tfidf-15" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>16 0.066443987 <a title="173-tfidf-16" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>17 0.063694842 <a title="173-tfidf-17" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>18 0.059876427 <a title="173-tfidf-18" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>19 0.059646495 <a title="173-tfidf-19" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>20 0.059266772 <a title="173-tfidf-20" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.177), (1, -0.204), (2, 0.091), (3, 0.109), (4, 0.121), (5, 0.024), (6, -0.088), (7, -0.013), (8, -0.057), (9, 0.033), (10, 0.004), (11, 0.025), (12, -0.038), (13, 0.018), (14, 0.043), (15, -0.022), (16, 0.031), (17, 0.071), (18, -0.041), (19, -0.001), (20, -0.039), (21, 0.009), (22, -0.007), (23, 0.019), (24, 0.007), (25, -0.026), (26, 0.032), (27, -0.047), (28, -0.03), (29, -0.075), (30, -0.003), (31, 0.058), (32, 0.01), (33, -0.008), (34, -0.053), (35, -0.037), (36, -0.036), (37, 0.019), (38, 0.016), (39, 0.084), (40, -0.026), (41, 0.017), (42, 0.047), (43, -0.049), (44, 0.018), (45, -0.054), (46, -0.031), (47, 0.017), (48, 0.019), (49, -0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88870287 <a title="173-lsi-1" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>2 0.85036749 <a title="173-lsi-2" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>3 0.8441813 <a title="173-lsi-3" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>4 0.7472741 <a title="173-lsi-4" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>5 0.73795211 <a title="173-lsi-5" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>6 0.70162213 <a title="173-lsi-6" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>7 0.70140386 <a title="173-lsi-7" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>8 0.68564487 <a title="173-lsi-8" href="./nips-2008-Goal-directed_decision_making_in_prefrontal_cortex%3A_a_computational_framework.html">94 nips-2008-Goal-directed decision making in prefrontal cortex: a computational framework</a></p>
<p>9 0.6441828 <a title="173-lsi-9" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>10 0.6015228 <a title="173-lsi-10" href="./nips-2008-Bounding_Performance_Loss_in_Approximate_MDP_Homomorphisms.html">39 nips-2008-Bounding Performance Loss in Approximate MDP Homomorphisms</a></p>
<p>11 0.56305665 <a title="173-lsi-11" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>12 0.55924422 <a title="173-lsi-12" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>13 0.46544269 <a title="173-lsi-13" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>14 0.46385226 <a title="173-lsi-14" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>15 0.4304527 <a title="173-lsi-15" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>16 0.41831055 <a title="173-lsi-16" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>17 0.41642731 <a title="173-lsi-17" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>18 0.39801648 <a title="173-lsi-18" href="./nips-2008-Multi-Agent_Filtering_with_Infinitely_Nested_Beliefs.html">141 nips-2008-Multi-Agent Filtering with Infinitely Nested Beliefs</a></p>
<p>19 0.39469004 <a title="173-lsi-19" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>20 0.39328015 <a title="173-lsi-20" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(25, 0.039), (27, 0.252), (30, 0.085), (40, 0.046), (46, 0.012), (60, 0.016), (63, 0.204), (64, 0.118), (71, 0.102), (90, 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77524245 <a title="173-lda-1" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>2 0.7102403 <a title="173-lda-2" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>3 0.70878273 <a title="173-lda-3" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>4 0.70605725 <a title="173-lda-4" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>5 0.70494926 <a title="173-lda-5" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>6 0.70312697 <a title="173-lda-6" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>7 0.70213991 <a title="173-lda-7" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>8 0.7017858 <a title="173-lda-8" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>9 0.7015546 <a title="173-lda-9" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>10 0.70136046 <a title="173-lda-10" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>11 0.70063341 <a title="173-lda-11" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>12 0.69995409 <a title="173-lda-12" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>13 0.6993438 <a title="173-lda-13" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>14 0.69742924 <a title="173-lda-14" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>15 0.69616652 <a title="173-lda-15" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>16 0.6952951 <a title="173-lda-16" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>17 0.69520366 <a title="173-lda-17" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>18 0.69376493 <a title="173-lda-18" href="./nips-2008-Model_selection_and_velocity_estimation_using_novel_priors_for_motion_patterns.html">136 nips-2008-Model selection and velocity estimation using novel priors for motion patterns</a></p>
<p>19 0.69188023 <a title="173-lda-19" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>20 0.69180906 <a title="173-lda-20" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
