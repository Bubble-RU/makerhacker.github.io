<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-127" href="#">nips2008-127</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</h1>
<br/><p>Source: <a title="nips-2008-127-pdf" href="http://papers.nips.cc/paper/3559-logistic-normal-priors-for-unsupervised-probabilistic-grammar-induction.pdf">pdf</a></p><p>Author: Shay B. Cohen, Kevin Gimpel, Noah A. Smith</p><p>Abstract: We explore a new Bayesian model for probabilistic grammars, a family of distributions over discrete structures that includes hidden Markov models and probabilistic context-free grammars. Our model extends the correlated topic model framework to probabilistic grammars, exploiting the logistic normal distribution as a prior over the grammar parameters. We derive a variational EM algorithm for that model, and then experiment with the task of unsupervised grammar induction for natural language dependency parsing. We show that our model achieves superior results over previous models that use diﬀerent priors. 1</p><p>Reference: <a title="nips-2008-127-reference" href="../nips2008_reference/nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We explore a new Bayesian model for probabilistic grammars, a family of distributions over discrete structures that includes hidden Markov models and probabilistic context-free grammars. [sent-5, score-0.3]
</p><p>2 Our model extends the correlated topic model framework to probabilistic grammars, exploiting the logistic normal distribution as a prior over the grammar parameters. [sent-6, score-0.982]
</p><p>3 We derive a variational EM algorithm for that model, and then experiment with the task of unsupervised grammar induction for natural language dependency parsing. [sent-7, score-0.938]
</p><p>4 In this paper, we consider learning probabilistic grammars, a class of structure models that includes Markov models, hidden Markov models (HMMs) and probabilistic context-free grammars (PCFGs). [sent-10, score-0.651]
</p><p>5 Central to natural language processing (NLP), probabilistic grammars are recursive generative models over discrete graphical structures, built out of conditional multinomial distributions, that make independence assumptions to permit eﬃcient exact probabilistic inference. [sent-11, score-0.748]
</p><p>6 There has been an increased interest in the use of Bayesian methods as applied to probabilistic grammars for NLP, including part-of-speech tagging [10, 20], phrase-structure parsing [7, 11, 16], and combinations of models [8]. [sent-12, score-0.607]
</p><p>7 In Bayesian-minded work with probabilistic grammars, a common thread is the use of a Dirichlet prior for the underlying multinomials, because as the conjugate prior for the multinomial, it bestows computational feasibility. [sent-13, score-0.296]
</p><p>8 The Dirichlet prior can also be used to encourage the desired property of sparsity in the learned grammar [11]. [sent-14, score-0.545]
</p><p>9 A related widely known example is the latent Dirichlet allocation (LDA) model for topic modeling in document collections [5], in which each document’s topic distribution is treated as a hidden variable, as is the topic distribution from which each word is drawn. [sent-15, score-0.354]
</p><p>10 1 Blei and Laﬀerty [4] showed empirical improvements over LDA using a logistic normal distribution that permits diﬀerent topics to correlate with each other, resulting in a correlated topic model (CTM). [sent-16, score-0.321]
</p><p>11 Here we aim to learn analogous correlations such as: a word that is likely to take one kind of argument (e. [sent-17, score-0.141]
</p><p>12 By permitting such correlations via the distribution over the 1 A certain variant of LDA can be seen as a Bayesian version of a zero-order HMM, where the unigram state (topic) distribution is sampled ﬁrst for each sequence (document). [sent-22, score-0.075]
</p><p>13 ∑k ηk μk  θk K  y  x  N  K  Figure 1: A graphical model for the logistic normal probabilistic grammar. [sent-23, score-0.354]
</p><p>14 In this paper, we present a model, in the Bayesian setting, which extends CTM for probabilistic grammars. [sent-26, score-0.15]
</p><p>15 We also derive an inference algorithm for that model, which is ultimately used to provide a point estimate for the grammar, permitting us to perform fast and exact inference. [sent-27, score-0.087]
</p><p>16 This is required if the learned grammar is to be used as a component in an application. [sent-28, score-0.489]
</p><p>17 §2 gives a general form for probabilistic grammars built out of multinomial distributions. [sent-30, score-0.554]
</p><p>18 §3 describes our model and an eﬃcient variational inference algorithm. [sent-31, score-0.203]
</p><p>19 §4 presents a probabilistic context-free dependency grammar often used in unsupervised natural language learning. [sent-32, score-0.865]
</p><p>20 Experimental results showing the competitiveness of our method for estimating that grammar are presented in §5. [sent-33, score-0.455]
</p><p>21 2  Probabilistic Grammars  A probabilistic grammar deﬁnes a probability distribution over a certain kind of structured object (a derivation of the underlying symbolic grammar) explained through step-by-step stochastic process. [sent-34, score-0.644]
</p><p>22 HMMs, for example, can be understood as a random walk through a probabilistic ﬁnite-state network, with an output symbol sampled at each state. [sent-35, score-0.204]
</p><p>23 PCFGs generate phrase-structure trees by recursively rewriting nonterminal symbols as sequences of “child” symbols (each itself either a nonterminal symbol or a terminal symbol analogous to the emissions of an HMM). [sent-36, score-0.301]
</p><p>24 The parameters θ are a collection of K multinomials θ 1 , . [sent-39, score-0.052]
</p><p>25 HMMs and vanilla PCFGs are the best known probabilistic grammars, but there are others. [sent-44, score-0.15]
</p><p>26 For example, in §5 we experiment with the “dependency model with valence,” a probabilistic grammar for dependency parsing ﬁrst proposed in [14]. [sent-45, score-0.864]
</p><p>27 3  Logistic Normal Prior on Probabilistic Grammars  A natural choice for a prior over the parameters of a probabilistic grammar is a Dirichlet prior. [sent-46, score-0.661]
</p><p>28 The Dirichlet family is conjugate to the multinomial family, which makes the inference more elegant and less computationally intensive. [sent-47, score-0.132]
</p><p>29 In addition, a Dirichlet prior can encourage sparse solutions, a property which is important with probabilistic grammars [11]. [sent-48, score-0.557]
</p><p>30 However, in [4], Blei and Laﬀerty noticed that the Dirichlet distribution is limited in its expressive power when modeling a corpus of documents, since it is less ﬂexible about capturing relationships between possible topics. [sent-49, score-0.045]
</p><p>31 To solve this modeling issue, they extended the LDA model to use a logistic normal distribution [2] yielding correlated topic models. [sent-50, score-0.321]
</p><p>32 The logistic normal distribution maps a d-dimensional multivariate Gaussian to a distribution d on the d-dimensional probability simplex, Sd = { z1 , . [sent-51, score-0.204]
</p><p>33 Our hierarchical generative model, which we call a logisticnormal probabilistic grammar, generates a sentence and derivation tree x, y as follows (see also Fig. [sent-56, score-0.323]
</p><p>34 We now turn to derive a variational inference algorithm for the model. [sent-74, score-0.203]
</p><p>35 4 is problematic, so we follow [4] in approximating it with ˜ ˜ a ﬁrst-order Taylor expansion, introducing K more variational parameters ζ1 , . [sent-79, score-0.192]
</p><p>36 3 With probabilistic grammars, this quantity can be computed using a summing dynamic programming algorithm like the forward-backward or inside-outside algorithm. [sent-83, score-0.15]
</p><p>37 ˜ ˜ The ﬁnal form of our bound is:4 K k=1  log p(x, y | µ, Σ) ≥  Eq [log p(η k | µk , Σk )] +  K k=1  Nk i=1  ˜ ˜ fk,i ψk,i + H(q)  (8)  Since, we are interested in EM-style algorithm, we will alternate between ﬁnding the maximizing q(η) and the maximizing q(y). [sent-85, score-0.113]
</p><p>38 Let r(y | x, eψ ) denote the conditional distribution over y given x deﬁned as: ˜  r(y | x, eψ ) =  1 ˜ Z(ψ)  K k=1  Nk i=1  ˜ exp ψk,i fk,i (x, y)  (9)  ˜ ˜ where Z(ψ) is a normalization constant. [sent-88, score-0.049]
</p><p>39 To see that, combine the deﬁnition of KL divergence ˜ K Nk ˜ ˜ ˜ with the fact that k=1 i=1 fk,i (x, y)ψk,i − log Z(ψ) = log r(y | x, eψ ) where log Z(ψ) does not depend on q(y). [sent-95, score-0.159]
</p><p>40 Interestingly, from the above lemma, the minimizing q(y) has the same form as the probabilistic grammar in discussion, only without having sum-to-one constraints on θ (leading to the required normalization constant). [sent-98, score-0.605]
</p><p>41 As in classic EM with probabilistic grammars, we never need to represent q(y) explicitly; we need only ˜, which can be calculated as expected f ˜ feature values under r(y | x, eψ ) using dynamic programming. [sent-99, score-0.15]
</p><p>42 To achieve this, we will use the above variational method within an EM algorithm that estimates µ and Σ in empirical Bayes fashion, then estimates θ as µ, the mean of the learned prior. [sent-101, score-0.192]
</p><p>43 In the E-step, we maximize the bound with respect to the variational parameters (˜ , σ , ζ, and ˜) using coordinate µ ˜ ˜ f ascent. [sent-102, score-0.158]
</p><p>44 We optimize each of these separately in turn, cycling through, using appropriate optimization algorithms for each (conjugate gradient for µ, Newton’s method for σ , a closed ˜ ˜ ˜ form for ζ, and dynamic programming to solve for ˜). [sent-103, score-0.047]
</p><p>45 In the M-step, we apply maximum f likelihood estimation with respect to µ and Σ given suﬃcient statistics gathered from the variational parameters in the E-step. [sent-104, score-0.19]
</p><p>46 4  Probabilistic Dependency Grammar Model  Dependency grammar [19] refers to linguistic theories that posit graphical representations of sentences in which words are vertices and the syntax is a tree. [sent-106, score-0.59]
</p><p>47 Such grammars can be context-free or context-sensitive in power, and they can be made probabilistic [9]. [sent-107, score-0.501]
</p><p>48 Dependency syntax is widely used in information extraction, machine translation, question 4  A tighter bound was proposed in [1], but we follow [4] for simplicity. [sent-108, score-0.069]
</p><p>49 x = NNP VBD JJ NNP ; y =  NNP Patrick  VBD spoke  JJ NNP little French  Figure 2: An example of a dependency tree (derivation y). [sent-109, score-0.196]
</p><p>50 Here, we are interested in unsupervised dependency parsing using the “dependency model with valence” [14]. [sent-112, score-0.322]
</p><p>51 The model is a probabilistic head automaton grammar [3] with a “split” form that renders inference cubic in the length of the sentence [6]. [sent-113, score-0.792]
</p><p>52 , xn be a sentence (here, as in prior work, represented as a sequence of part-of-speech tags). [sent-117, score-0.147]
</p><p>53 A tree y is deﬁned by a pair of functions yleft and yright (both {0, 1, 2, . [sent-119, score-0.147]
</p><p>54 ,n} ) that map each word to its sets of left and right dependents, respectively. [sent-125, score-0.108]
</p><p>55 Here, the graph is constrained to be a projective tree rooted at x0 = $: each word except $ has a single parent, and there are no cycles or crossing dependencies. [sent-126, score-0.181]
</p><p>56 yleft (0) is taken to be empty, and yright (0) contains the sentence’s single head. [sent-127, score-0.104]
</p><p>57 Let y(i) denote the subtree rooted at position i. [sent-128, score-0.065]
</p><p>58 The parameters θ are the multinomial distributions θs (· | ·, ·, ·) and θc (· | ·, ·). [sent-131, score-0.053]
</p><p>59 Figure 2 shows a dependency tree and its probability under this model. [sent-137, score-0.196]
</p><p>60 We follow standard parsing conventions and train on sections 2–21,5 tune on section 22, and report ﬁnal results on section 23. [sent-139, score-0.179]
</p><p>61 Evaluation After learning a point estimate θ, we predict y for unseen test data (by parsing with the probabilistic grammar) and report the fraction of words whose predicted parent matches the gold standard corpus, known as attachment accuracy. [sent-140, score-0.446]
</p><p>62 Two parsing methods were considered: the most probable “Viterbi” parse (argmaxy p(y | x, θ)) and the minimum Bayes risk (MBR) parse (argminy Ep(y |x,θ) [ (y; x, y )]) with dependency attachment error as the loss function. [sent-141, score-0.483]
</p><p>63 Settings Our experiment compares four methods for estimating the probabilistic grammar’s parameters: EM Maximum likelihood estimate of θ using the EM algorithm to optimize p(x | θ) [14]. [sent-142, score-0.229]
</p><p>64 EM-MAP Maximum a posteriori estimate of θ using the EM algorithm and a ﬁxed symmetric Dirichlet prior with α > 1 to optimize p(x, θ | α). [sent-143, score-0.103]
</p><p>65 Tune α to maximize the likelihood of an unannotated development dataset, using grid search over [1. [sent-144, score-0.13]
</p><p>66 5 Training in the unsupervised setting for this data set can be expensive, and requires running a cubic-time dynamic programming algorithm iteratively, so we follow common practice in restricting the training set (but not development or test sets) to sentences of length ten or fewer words. [sent-146, score-0.188]
</p><p>67 Short sentences are also less structurally ambiguous and may therefore be easier to learn from. [sent-147, score-0.061]
</p><p>68 VB-Dirichlet Use variational Bayes inference to estimate the posterior distribution p(θ | x, α), which is a Dirichlet. [sent-148, score-0.203]
</p><p>69 Tune the symmetric Dirichlet prior’s parameter α to maximize the likelihood of an unannotated development dataset, using grid search over [0. [sent-149, score-0.13]
</p><p>70 VB-EM-Dirichlet Use variational Bayes EM to optimize p(x | α) with respect to α. [sent-152, score-0.205]
</p><p>71 VB-EM-Log-Normal Use variational Bayes EM to optimize p(x | µ, Σ) with respect to µ and Σ. [sent-154, score-0.205]
</p><p>72 We used the successful initializer from [14], which estimates θ using soft counts on the training data where, in an n-length 1 sentence, (a) each word is counted as the sentence’s head n times, and (b) each word xi attaches to xj proportional to |i − j|, normalized to a single attachment per word. [sent-157, score-0.594]
</p><p>73 This initializer is used with EM, EM-MAP, VB-Dirichlet, and VB-EM-Dirichlet. [sent-158, score-0.078]
</p><p>74 In the case of VB-EM-Log-Normal, it is used as an initializer both for µ and inside the E-step. [sent-159, score-0.078]
</p><p>75 In all experiments reported here, we run the iterative estimation algorithm until the likelihood of a held-out, unannotated dataset stops increasing. [sent-160, score-0.1]
</p><p>76 For learning with the logistic normal prior, we consider two initializations of the covariance matrices Σk . [sent-161, score-0.281]
</p><p>77 We then tried to bias the solution by injecting prior knowledge about the part-of-speech tags. [sent-163, score-0.108]
</p><p>78 To do that, we mapped the tag set (34 tags) to twelve disjoint tag families. [sent-165, score-0.118]
</p><p>79 6 The covariance matrices for all dependency distributions were initialized with 1 on the diagonal, 0. [sent-166, score-0.23]
</p><p>80 5 between tags which belong to the same family, and 0 otherwise. [sent-167, score-0.129]
</p><p>81 We report attachment accuracy on three subsets of the corpus: sentences of length ≤ 10 (typically reported in prior work and most similar to the training dataset), length ≤ 20, and the full corpus. [sent-170, score-0.273]
</p><p>82 The Bayesian methods all outperform the common baseline (in which we attach each word to the word on its right), but the logistic normal prior performs considerably better than the other two methods as well. [sent-171, score-0.476]
</p><p>83 The learned covariance matrices were very sparse when using the identity matrix to initialize. [sent-172, score-0.111]
</p><p>84 When using the “tag families” initialization for the covariance, there were 151 elements across the covariance matrices which were not identically 0 (out of more than 1,000), pointing to a learned relationship between parameters. [sent-174, score-0.111]
</p><p>85 In this case, most covariance matrices for θc dependencies were diagonal, while many of the covariance matrices for the stopping probabilities (θs ) had signiﬁcant correlations. [sent-175, score-0.154]
</p><p>86 6  Conclusion  We have considered a Bayesian model for probabilistic grammars, which is based on the logistic normal prior. [sent-176, score-0.354]
</p><p>87 Experimentally, several diﬀerent approaches for grammar induction were compared based on diﬀerent priors. [sent-177, score-0.52]
</p><p>88 We found that a logistic normal prior outperforms earlier approaches, presumably because it can capitalize on similarity between part-of-speech tags, as diﬀerent tags tend to appear as arguments in similar syntactic contexts. [sent-178, score-0.421]
</p><p>89 The coarse tags were chosen manually to ﬁt seven treebanks in diﬀerent languages. [sent-181, score-0.129]
</p><p>90 25 VB-EM-Dirichlet (0) VB-EM-Log-Normal, Σk = I VB-EM-Log-Normal, families  attachment accuracy (%) Viterbi decoding MBR decoding |x| ≤ 10 |x| ≤ 20 all |x| ≤ 10 |x| ≤ 20 38. [sent-184, score-0.156]
</p><p>91 Attach-Right attaches each word to the word on its right and the last word to $. [sent-227, score-0.376]
</p><p>92 EM and EM-MAP with a Dirichlet prior (α > 1) are reproductions of earlier results [14, 18]. [sent-228, score-0.056]
</p><p>93 On tight approximate inference of the logistic normal topic admixture model. [sent-234, score-0.331]
</p><p>94 Solving the problem of cascading errors: Approximate Bayesian inference for linguistic annotation pipelines. [sent-280, score-0.114]
</p><p>95 Bayesian inference for PCFGs via Markov chain Monte Carlo. [sent-299, score-0.045]
</p><p>96 Corpus-based induction of syntactic structure: Models of dependency and constituency. [sent-323, score-0.25]
</p><p>97 A  VB-EM for Logistic-Normal Probabilistic Grammars  The algorithm for variational inference with probabilistic grammars using logistic normal ˜l,(t) prior follows. [sent-365, score-0.964]
</p><p>98 There are variational parameters for each training example, indexed by . [sent-367, score-0.158]
</p><p>99 Input: initial parameters µ(0) , Σ(0) , training data x, and development data x Output: learned parameters µ, Σ t←1; repeat E-step (for each training example = 1, . [sent-371, score-0.064]
</p><p>100 , K: use conjugate gradient descent ” with “ ˜ ” ´ ` PNk “ ˜ (t−1) −1 (t−1) ∂L ˜ ˜ ˜ ˜2 = − (Σk ) )(µk − µk ) − fk,i + i =1 fk,i /ζk exp µk,i + σk,i /2 ; ˜ ∂µ ˜ i  k,i  ,(t)  optimize σk ˜  , k = 1,“ K: use Newton’s method for each coordinate (with σk,i > 0) with . [sent-377, score-0.13]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('grammar', 0.455), ('grammars', 0.351), ('nk', 0.345), ('variational', 0.158), ('attachment', 0.156), ('dependency', 0.153), ('probabilistic', 0.15), ('dirichlet', 0.138), ('nnp', 0.13), ('erent', 0.13), ('tags', 0.129), ('eq', 0.115), ('word', 0.108), ('logistic', 0.107), ('parsing', 0.106), ('erty', 0.104), ('pnk', 0.104), ('em', 0.102), ('normal', 0.097), ('di', 0.096), ('sentence', 0.091), ('pcfgs', 0.091), ('topic', 0.082), ('initializer', 0.078), ('penn', 0.07), ('unannotated', 0.068), ('vbd', 0.068), ('la', 0.066), ('induction', 0.065), ('unsupervised', 0.063), ('acl', 0.063), ('treebank', 0.063), ('emnlp', 0.063), ('blei', 0.061), ('sentences', 0.061), ('tag', 0.059), ('hmm', 0.057), ('prior', 0.056), ('pcfg', 0.055), ('symbol', 0.054), ('log', 0.053), ('multinomial', 0.053), ('attaches', 0.052), ('ctm', 0.052), ('gri', 0.052), ('injecting', 0.052), ('mbr', 0.052), ('multinomials', 0.052), ('nonterminal', 0.052), ('rsty', 0.052), ('valence', 0.052), ('yleft', 0.052), ('yright', 0.052), ('lda', 0.052), ('head', 0.051), ('jj', 0.049), ('hmms', 0.049), ('exp', 0.049), ('optimize', 0.047), ('bayesian', 0.046), ('adjective', 0.046), ('corpus', 0.045), ('inference', 0.045), ('language', 0.044), ('covariance', 0.044), ('tree', 0.043), ('permitting', 0.042), ('counts', 0.041), ('derivation', 0.039), ('nlp', 0.039), ('linguistic', 0.039), ('yd', 0.039), ('tune', 0.039), ('nouns', 0.037), ('viterbi', 0.037), ('dkl', 0.037), ('bayes', 0.036), ('subtree', 0.035), ('syntax', 0.035), ('wall', 0.035), ('klein', 0.035), ('correlated', 0.035), ('learned', 0.034), ('conjugate', 0.034), ('parent', 0.034), ('parse', 0.034), ('noun', 0.034), ('follow', 0.034), ('matrices', 0.033), ('correlations', 0.033), ('kth', 0.032), ('syntactic', 0.032), ('newton', 0.032), ('likelihood', 0.032), ('rewriting', 0.031), ('development', 0.03), ('annotation', 0.03), ('rooted', 0.03), ('maximizing', 0.03), ('symbols', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="127-tfidf-1" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>Author: Shay B. Cohen, Kevin Gimpel, Noah A. Smith</p><p>Abstract: We explore a new Bayesian model for probabilistic grammars, a family of distributions over discrete structures that includes hidden Markov models and probabilistic context-free grammars. Our model extends the correlated topic model framework to probabilistic grammars, exploiting the logistic normal distribution as a prior over the grammar parameters. We derive a variational EM algorithm for that model, and then experiment with the task of unsupervised grammar induction for natural language dependency parsing. We show that our model achieves superior results over previous models that use diﬀerent priors. 1</p><p>2 0.29526073 <a title="127-tfidf-2" href="./nips-2008-Bayesian_Synchronous_Grammar_Induction.html">35 nips-2008-Bayesian Synchronous Grammar Induction</a></p>
<p>Author: Phil Blunsom, Trevor Cohn, Miles Osborne</p><p>Abstract: We present a novel method for inducing synchronous context free grammars (SCFGs) from a corpus of parallel string pairs. SCFGs can model equivalence between strings in terms of substitutions, insertions and deletions, and the reordering of sub-strings. We develop a non-parametric Bayesian model and apply it to a machine translation task, using priors to replace the various heuristics commonly used in this ﬁeld. Using a variational Bayes training procedure, we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations, showing improvements in translation performance over maximum likelihood models. 1</p><p>3 0.17620696 <a title="127-tfidf-3" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>Author: Jordan L. Boyd-graber, David M. Blei</p><p>Abstract: We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-speciﬁc topic weights and parse-tree-speciﬁc syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents. 1</p><p>4 0.16203858 <a title="127-tfidf-4" href="./nips-2008-Modeling_the_effects_of_memory_on_human_online_sentence_processing_with_particle_filters.html">139 nips-2008-Modeling the effects of memory on human online sentence processing with particle filters</a></p>
<p>Author: Roger P. Levy, Florencia Reali, Thomas L. Griffiths</p><p>Abstract: Language comprehension in humans is signiﬁcantly constrained by memory, yet rapid, highly incremental, and capable of utilizing a wide range of contextual information to resolve ambiguity and form expectations about future input. In contrast, most of the leading psycholinguistic models and ﬁelded algorithms for natural language parsing are non-incremental, have run time superlinear in input length, and/or enforce structural locality constraints on probabilistic dependencies between events. We present a new limited-memory model of sentence comprehension which involves an adaptation of the particle ﬁlter, a sequential Monte Carlo method, to the problem of incremental parsing. We show that this model can reproduce classic results in online sentence comprehension, and that it naturally provides the ﬁrst rational account of an outstanding problem in psycholinguistics, in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information. 1</p><p>5 0.11014877 <a title="127-tfidf-5" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>Author: Leo Zhu, Yuanhao Chen, Yuan Lin, Chenxi Lin, Alan L. Yuille</p><p>Abstract: Language and image understanding are two major goals of artiﬁcial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. Natural language researchers have made great progress by exploiting the 1D structure of language to design efﬁcient polynomialtime parsing algorithms. By contrast, the two-dimensional nature of images makes it much harder to design efﬁcient image parsers and the form of the hierarchical representations is also unclear. Attempts to adapt representations and algorithms from natural language have only been partially successful. In this paper, we propose a Hierarchical Image Model (HIM) for 2D image parsing which outputs image segmentation and object recognition. This HIM is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. Firstly, the HIM has a coarse-to-ﬁne representation which is capable of capturing long-range dependency and exploiting different levels of contextual information. Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dynamic programming, which enables us to parse the image rapidly in polynomial time. Thirdly, we can learn the HIM efﬁciently in a discriminative manner from a labeled dataset. We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena. 1</p><p>6 0.10901784 <a title="127-tfidf-6" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>7 0.106975 <a title="127-tfidf-7" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>8 0.098778695 <a title="127-tfidf-8" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>9 0.076433092 <a title="127-tfidf-9" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>10 0.069519967 <a title="127-tfidf-10" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>11 0.067971185 <a title="127-tfidf-11" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>12 0.066053331 <a title="127-tfidf-12" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>13 0.065658882 <a title="127-tfidf-13" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>14 0.06550914 <a title="127-tfidf-14" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>15 0.064937547 <a title="127-tfidf-15" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>16 0.06415201 <a title="127-tfidf-16" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>17 0.061666943 <a title="127-tfidf-17" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>18 0.061133415 <a title="127-tfidf-18" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>19 0.060837686 <a title="127-tfidf-19" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>20 0.059677489 <a title="127-tfidf-20" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.189), (1, -0.072), (2, 0.089), (3, -0.097), (4, -0.028), (5, -0.063), (6, 0.085), (7, 0.211), (8, -0.181), (9, -0.008), (10, -0.086), (11, 0.027), (12, 0.078), (13, 0.156), (14, -0.048), (15, 0.098), (16, 0.012), (17, -0.068), (18, -0.166), (19, 0.034), (20, 0.041), (21, 0.051), (22, 0.028), (23, -0.021), (24, -0.073), (25, 0.085), (26, 0.166), (27, -0.176), (28, -0.042), (29, 0.016), (30, -0.019), (31, -0.144), (32, -0.154), (33, 0.002), (34, 0.128), (35, -0.03), (36, 0.018), (37, -0.025), (38, -0.131), (39, 0.041), (40, -0.018), (41, 0.1), (42, -0.066), (43, 0.114), (44, 0.062), (45, 0.044), (46, 0.002), (47, -0.083), (48, 0.02), (49, -0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94587493 <a title="127-lsi-1" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>Author: Shay B. Cohen, Kevin Gimpel, Noah A. Smith</p><p>Abstract: We explore a new Bayesian model for probabilistic grammars, a family of distributions over discrete structures that includes hidden Markov models and probabilistic context-free grammars. Our model extends the correlated topic model framework to probabilistic grammars, exploiting the logistic normal distribution as a prior over the grammar parameters. We derive a variational EM algorithm for that model, and then experiment with the task of unsupervised grammar induction for natural language dependency parsing. We show that our model achieves superior results over previous models that use diﬀerent priors. 1</p><p>2 0.89589393 <a title="127-lsi-2" href="./nips-2008-Bayesian_Synchronous_Grammar_Induction.html">35 nips-2008-Bayesian Synchronous Grammar Induction</a></p>
<p>Author: Phil Blunsom, Trevor Cohn, Miles Osborne</p><p>Abstract: We present a novel method for inducing synchronous context free grammars (SCFGs) from a corpus of parallel string pairs. SCFGs can model equivalence between strings in terms of substitutions, insertions and deletions, and the reordering of sub-strings. We develop a non-parametric Bayesian model and apply it to a machine translation task, using priors to replace the various heuristics commonly used in this ﬁeld. Using a variational Bayes training procedure, we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations, showing improvements in translation performance over maximum likelihood models. 1</p><p>3 0.75729078 <a title="127-lsi-3" href="./nips-2008-Modeling_the_effects_of_memory_on_human_online_sentence_processing_with_particle_filters.html">139 nips-2008-Modeling the effects of memory on human online sentence processing with particle filters</a></p>
<p>Author: Roger P. Levy, Florencia Reali, Thomas L. Griffiths</p><p>Abstract: Language comprehension in humans is signiﬁcantly constrained by memory, yet rapid, highly incremental, and capable of utilizing a wide range of contextual information to resolve ambiguity and form expectations about future input. In contrast, most of the leading psycholinguistic models and ﬁelded algorithms for natural language parsing are non-incremental, have run time superlinear in input length, and/or enforce structural locality constraints on probabilistic dependencies between events. We present a new limited-memory model of sentence comprehension which involves an adaptation of the particle ﬁlter, a sequential Monte Carlo method, to the problem of incremental parsing. We show that this model can reproduce classic results in online sentence comprehension, and that it naturally provides the ﬁrst rational account of an outstanding problem in psycholinguistics, in which the preferred alternative in a syntactic ambiguity seems to grow more attractive over time even in the absence of strong disambiguating information. 1</p><p>4 0.60290474 <a title="127-lsi-4" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>Author: Jordan L. Boyd-graber, David M. Blei</p><p>Abstract: We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-speciﬁc topic weights and parse-tree-speciﬁc syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents. 1</p><p>5 0.56049371 <a title="127-lsi-5" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>Author: Andriy Mnih, Geoffrey E. Hinton</p><p>Abstract: Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the nonhierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models. 1</p><p>6 0.50434518 <a title="127-lsi-6" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>7 0.48390207 <a title="127-lsi-7" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>8 0.45849186 <a title="127-lsi-8" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>9 0.37176362 <a title="127-lsi-9" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>10 0.35183141 <a title="127-lsi-10" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>11 0.34064877 <a title="127-lsi-11" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>12 0.32895756 <a title="127-lsi-12" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>13 0.32589829 <a title="127-lsi-13" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>14 0.31535149 <a title="127-lsi-14" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>15 0.30896246 <a title="127-lsi-15" href="./nips-2008-Asynchronous_Distributed_Learning_of_Topic_Models.html">28 nips-2008-Asynchronous Distributed Learning of Topic Models</a></p>
<p>16 0.29986674 <a title="127-lsi-16" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>17 0.29742879 <a title="127-lsi-17" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>18 0.29717895 <a title="127-lsi-18" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>19 0.29692689 <a title="127-lsi-19" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>20 0.28868634 <a title="127-lsi-20" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.054), (7, 0.07), (12, 0.063), (28, 0.136), (45, 0.291), (52, 0.013), (57, 0.102), (59, 0.014), (63, 0.03), (71, 0.02), (77, 0.028), (78, 0.022), (83, 0.067)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77721614 <a title="127-lda-1" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>Author: Shay B. Cohen, Kevin Gimpel, Noah A. Smith</p><p>Abstract: We explore a new Bayesian model for probabilistic grammars, a family of distributions over discrete structures that includes hidden Markov models and probabilistic context-free grammars. Our model extends the correlated topic model framework to probabilistic grammars, exploiting the logistic normal distribution as a prior over the grammar parameters. We derive a variational EM algorithm for that model, and then experiment with the task of unsupervised grammar induction for natural language dependency parsing. We show that our model achieves superior results over previous models that use diﬀerent priors. 1</p><p>2 0.59211528 <a title="127-lda-2" href="./nips-2008-Bayesian_Synchronous_Grammar_Induction.html">35 nips-2008-Bayesian Synchronous Grammar Induction</a></p>
<p>Author: Phil Blunsom, Trevor Cohn, Miles Osborne</p><p>Abstract: We present a novel method for inducing synchronous context free grammars (SCFGs) from a corpus of parallel string pairs. SCFGs can model equivalence between strings in terms of substitutions, insertions and deletions, and the reordering of sub-strings. We develop a non-parametric Bayesian model and apply it to a machine translation task, using priors to replace the various heuristics commonly used in this ﬁeld. Using a variational Bayes training procedure, we learn the latent structure of translation equivalence through the induction of synchronous grammar categories for phrasal translations, showing improvements in translation performance over maximum likelihood models. 1</p><p>3 0.57554096 <a title="127-lda-3" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>4 0.56749332 <a title="127-lda-4" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>5 0.56469214 <a title="127-lda-5" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>Author: Erik B. Sudderth, Michael I. Jordan</p><p>Abstract: We develop a statistical framework for the simultaneous, unsupervised segmentation and discovery of visual object categories from image databases. Examining a large set of manually segmented scenes, we show that object frequencies and segment sizes both follow power law distributions, which are well modeled by the Pitman–Yor (PY) process. This nonparametric prior distribution leads to learning algorithms which discover an unknown set of objects, and segmentation methods which automatically adapt their resolution to each image. Generalizing previous applications of PY processes, we use Gaussian processes to discover spatially contiguous segments which respect image boundaries. Using a novel family of variational approximations, our approach produces segmentations which compare favorably to state-of-the-art methods, while simultaneously discovering categories shared among natural scenes. 1</p><p>6 0.56441718 <a title="127-lda-6" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>7 0.56188709 <a title="127-lda-7" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>8 0.56078672 <a title="127-lda-8" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>9 0.5607633 <a title="127-lda-9" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>10 0.56054288 <a title="127-lda-10" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>11 0.55997175 <a title="127-lda-11" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>12 0.55980313 <a title="127-lda-12" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>13 0.55916238 <a title="127-lda-13" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>14 0.55902535 <a title="127-lda-14" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>15 0.55893445 <a title="127-lda-15" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>16 0.5577988 <a title="127-lda-16" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>17 0.55768561 <a title="127-lda-17" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>18 0.55602235 <a title="127-lda-18" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>19 0.55478263 <a title="127-lda-19" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>20 0.55478233 <a title="127-lda-20" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
