<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-135" href="#">nips2008-135</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</h1>
<br/><p>Source: <a title="nips-2008-135-pdf" href="http://papers.nips.cc/paper/3436-model-selection-in-gaussian-graphical-models-high-dimensional-consistency-of-boldmathell_1-regularized-mle.pdf">pdf</a></p><p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>Reference: <a title="nips-2008-135-reference" href="../nips2008_reference/nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i. [sent-4, score-0.185]
</p><p>2 Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. [sent-9, score-0.441]
</p><p>3 Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. [sent-10, score-0.321]
</p><p>4 In this paper, we study the problem of estimating the graph structure of a Gauss Markov random ﬁeld (GMRF) in the high-dimensional setting. [sent-17, score-0.185]
</p><p>5 This graphical model selection problem can be reduced to the problem of estimating the zero-pattern of the inverse covariance or concentration matrix Θ∗ . [sent-18, score-0.633]
</p><p>6 A line of recent work [1, 2, 3, 4] has studied estimators based on minimizing Gaussian log-likelihood penalized by the ℓ1 norm of the entries (or the off-diagonal entries) of the concentration matrix. [sent-19, score-0.463]
</p><p>7 [1] have analyzed some aspects of high-dimensional behavior, in particular establishing consistency in Frobenius norm under certain conditions on the model covariance and under certain scalings of the sparsity, sample size, and ambient model dimension. [sent-22, score-0.665]
</p><p>8 The main contribution of this paper is to provide sufﬁcient conditions for model selection consistency of ℓ1 -regularized Gaussian maximum likelihood. [sent-23, score-0.34]
</p><p>9 It is worth noting that such a consistency result for structure learning of Gaussian graphical models cannot be derived from Frobenius norm consistency alone. [sent-24, score-0.491]
</p><p>10 For any concentration matrix Θ, denote the set of its non-zero off-diagonal entries 1  by E(Θ) = {s = t | Θst = 0}. [sent-25, score-0.352]
</p><p>11 (As will be clariﬁed below, the notation E alludes to the fact that this set corresponds to the edges in the graph deﬁning the GMRF. [sent-26, score-0.334]
</p><p>12 ) Under certain technical conditions to be speciﬁed, we prove that the ℓ1 -regularized (on off-diagonal entries of Θ) Gaussian MLE recovers this edge set with high probability, meaning that P[E(Θ) = E(Θ∗ )] → 1. [sent-27, score-0.339]
</p><p>13 Given a matrix U ∈ Rp×p and parameters a, b ∈ [1, ∞], we use |||U |||a,b to denote the induced matrix-operator norm max y a =1 U y b ; see [6] for background. [sent-38, score-0.223]
</p><p>14 Three cases of particular importance in this paper are the spectral norm |||U |||2 , corresponding to the maximal singular value of U ; the ℓ∞ /ℓ∞ -operator norm, given by p  |||U |||∞  :=  max  j=1,. [sent-39, score-0.181]
</p><p>15 Finally, we use U ∞ to denote the element-wise maximum maxi,j |Uij |; note that this is not a matrix norm, but rather a norm on the 2 vectorized form of the matrix. [sent-43, score-0.325]
</p><p>16 For any matrix U ∈ Rp×p , we use vec(U ) or equivalently U ∈ Rp to denote its vectorized form, obtained by stacking up the rows of U . [sent-44, score-0.142]
</p><p>17 Note that this inner product 2 induces the Frobenius norm |||U |||F := i,j Uij . [sent-46, score-0.181]
</p><p>18 1 Gaussian MRFs and ℓ1 penalized estimation Consider an undirected graph G = (V, E) with p = |V | vertices, and let X = (X1 , . [sent-52, score-0.237]
</p><p>19 , xp ; Θ∗ ) = (2) ∗ ))p/2 2 (2π det(Θ As illustrated in Figure 1, Markov structure is reﬂected in the sparsity pattern of the inverse covariance or concentration matrix Θ∗ , a p × p symmetric matrix. [sent-60, score-0.586]
</p><p>20 Consequently, the problem of / ij graphical model selection is equivalent to estimating the off-diagonal zero-pattern of the concentration matrix—that is, the set E(Θ∗ ) := {i, j ∈ V | i = j, Θ∗ = 0}. [sent-62, score-0.499]
</p><p>21 ij In this paper, we study the minimizer of the ℓ1 -penalized Gaussian negative log-likelihood. [sent-63, score-0.189]
</p><p>22 Letting A, B := i,j Aij Bij be the trace inner product on the space of symmetric matrices, this objective function takes the form Θ = arg min Θ 0  Θ, Σ − logdet(Θ) + λn Θ 2  1,oﬀ  = arg min g(Θ; Σ, λn ). [sent-64, score-0.17]
</p><p>23 Θ 0  (3)  Zero pattern of inverse covariance  1  2  1  2  3  3 5  4  4  5 1  2  (a)  3  4  5  (b)  Figure 1. [sent-65, score-0.241]
</p><p>24 This graph has p = 5 vertices, maximum degree d = 3 and s = 6 edges. [sent-68, score-0.334]
</p><p>25 (b) Zero pattern of the inverse covariance Θ∗ associated with the GMRF in (a). [sent-69, score-0.241]
</p><p>26 The set E(Θ∗ ) corresponds to the off-diagonal non-zeros (white blocks); the diagonal is also non-zero (grey squares), but these entries do not correspond to edges. [sent-70, score-0.223]
</p><p>27 Of interest in this paper is studying the probability P[E(Θ∗ ) = E(Θ)] as a function of the graph size p (which serves as the “model dimension” for the Gauss-Markov model), the sample size n, and the structural properties of Θ. [sent-80, score-0.417]
</p><p>28 ij  (4)  corresponding to the total number of edges, and the maximum degree or row cardinality d :=  max |{i | Θ∗ = 0}, ij  j=1,. [sent-82, score-0.527]
</p><p>29 ,p  (5)  corresponding to the maximum number of non-zeros in any row of Θ∗ , or equivalently the maximum degree in the graph G, where we include the diagonal in the degree count. [sent-85, score-0.541]
</p><p>30 Using standard results on matrix derivatives [5], it can be shown that this Hessian takes the form Γ∗  :=  ∇2 g(Θ) Θ  Θ=Θ∗  = Θ∗ −1 ⊗ Θ∗ −1 ,  (6)  where ⊗ denotes the Kronecker matrix product. [sent-88, score-0.164]
</p><p>31 By deﬁnition, Γ∗ is a p2 × p2 matrix indexed by ∂2 g vertex pairs, so that entry Γ∗ (j,k),(ℓ,m) corresponds to the second partial derivative ∂Θjk ∂Θℓm , evaluated at Θ = Θ∗ . [sent-89, score-0.172]
</p><p>32 For this reason, Γ can be viewed as an edge-based counterpart to the usual covariance matrix Σ∗ . [sent-91, score-0.318]
</p><p>33 We deﬁne the set of non-zero off-diagonal entries in the model concentration matrix Θ∗ : S(Θ∗ ) :=  {(i, j) ∈ V × V | i = j, Θ∗ = 0}, ij  (7)  and let S(Θ∗ ) = {S(Θ∗ ) ∪ {(1, 1), . [sent-92, score-0.541]
</p><p>34 We require the following conditions on the Fisher information matrix Γ∗ :  [A1] Incoherence condition: This condition captures the intuition that variable-pairs which are non-edges cannot exert an overtly strong effect on variable-pairs which form edges of the Gaussian graphical model. [sent-105, score-0.45]
</p><p>35 SS  (9)  These assumptions require that the covariance elements along any row of (Θ∗ )−1 and (Γ∗ )−1 have SS bounded ℓ1 norms. [sent-109, score-0.218]
</p><p>36 Note that similar assumptions are are also required for consistency in Frobenius norm [1]. [sent-110, score-0.296]
</p><p>37 Recall from equations (4) and (5) the deﬁnitions of the sparsity index s and maximum degree d, respectively. [sent-111, score-0.221]
</p><p>38 Consider a Gaussian distribution with concentration matrix Θ∗ that satisﬁes conditions (A1) and (A2). [sent-113, score-0.273]
</p><p>39 Suppose the penalty is set as λn = C1 min(i,j)∈S |Θ∗ | ij  Θ∗ min  scales as > C2 triple (n, d, p) satisﬁes the scaling  log p n  log p ∗ n , and the minimum edge-weight Θmin  :=  for some constants C1 , C2 > 0. [sent-114, score-0.403]
</p><p>40 Then the edge set E(Θ) speciﬁed by the estimator speciﬁes the true edge set w. [sent-116, score-0.251]
</p><p>41 [1] prove that the error of the estimator in Frobenius norm obeys the bound |||Θ − Θ∗ |||2 = O {((s + p) log p)/n}, with high probability. [sent-122, score-0.276]
</p><p>42 We note that model selection F consistency does not follow from this result, since an estimate may be close in Frobenius norm while differing substantially in terms of zero-pattern. [sent-123, score-0.353]
</p><p>43 In one sense, the model selection criterion is more demanding, since given knowledge of the edge set E(Θ∗ ), one could restrict estimation procedures to this subset, and so achieve faster rates. [sent-124, score-0.181]
</p><p>44 On the other hand, Theorem 1 requires incoherence conditions [A1] on the covariance matrix, which are not required for Frobenius norm consistency [1]. [sent-125, score-0.608]
</p><p>45 Wainwright [12] shows that the rate n ≍ d log p is a sharp threshold for the success/failure of neighborhood selection by Lasso. [sent-129, score-0.199]
</p><p>46 Below we show two cases where the Lasso irrepresentability condition holds, while the log-determinant requirement fails. [sent-133, score-0.339]
</p><p>47 However, in general, we do not know whether the log-determinant irrepresentability strictly dominates its analog for the Lasso. [sent-134, score-0.302]
</p><p>48 1 Illustration of irrepresentability: Diamond graph Consider the following Gaussian MRF example from [13]. [sent-137, score-0.185]
</p><p>49 Figure 2(a) shows a diamond-shaped graph G = (V, E), with vertex set V = {1, 2, 3, 4} and edge-set as the fully connected graph over V with the edge (1, 4) removed. [sent-138, score-0.548]
</p><p>50 The covariance matrix Σ∗ is parameterized by the correlation param2  2  1  3  1  4  4 (b)  3 (a)  Figure 2: (a) Graph of the example discussed by [13]. [sent-139, score-0.264]
</p><p>51 √ eter ρ ∈ [0, 1/ 2]: the diagonal entries are set to Σ∗ = 1, for all i ∈ V ; the entries corresponding ii to edges are set to Σ∗ = ρ for (i, j) ∈ E\{(2, 3)}, Σ∗ = 0; and ﬁnally the entry corresponding to 23 ij the non-edge is set as Σ∗ = 2ρ2 . [sent-141, score-0.672]
</p><p>52 For this model, [13] showed that the ℓ1 -regularized MLE Θ fails 14 to recover the graph structure for any sample size, if ρ > −1 + (3/2)1/2 ≈ 0. [sent-142, score-0.271]
</p><p>53 It is instructive to compare this necessary condition to the sufﬁcient condition provided in our analysis, namely the incoherence Assumption [A1] as applied to the Hessian Γ∗ . [sent-144, score-0.23]
</p><p>54 On the other hand, the irrepresentability condition for the Lasso requires only that 2|ρ| < 1, i. [sent-150, score-0.339]
</p><p>55 5), the Lasso irrepresentability condition holds while our log-determinant counterpart fails. [sent-157, score-0.443]
</p><p>56 2 Illustration of irrepresentability: Star graphs A second interesting example is the star-shaped graphical model, illustrated in Figure 2(b), which consists of a single hub node connected to the rest of the spoke nodes. [sent-160, score-0.372]
</p><p>57 We consider a four node graph, with vertex set V = {1, 2, 3, 4} and edge-set E = {(1, s) | s ∈ {2, 3, 4}}. [sent-161, score-0.168]
</p><p>58 The covariance matrix Σ∗ is parameterized the correlation parameter ρ ∈ [−1, 1]: the diagonal entries are set to Σ∗ = 1, for all i ∈ V ; the entries corresponding to edges are set to Σ∗ = ρ for (i, j) ∈ E; while ii ij the non-edge entries are set as Σ∗ = ρ2 for (i, j) ∈ E. [sent-162, score-1.101]
</p><p>59 Consequently, for this particular example, / ij Assumption [A1] reduces to the constraint |ρ|(|ρ|+2) < 1, which holds for all ρ ∈ (−0. [sent-163, score-0.239]
</p><p>60 The irrepresentability condition for the Lasso on the other hand allows the full range ρ ∈ (−1, 1). [sent-166, score-0.339]
</p><p>61 414, 1), where the Lasso irrepresentability condition holds while the log-determinant counterpart fails. [sent-168, score-0.443]
</p><p>62 There we consider the more general problem of estimation of the covariance matrix of a random vector (that need not necessarily be Gaussian) from i. [sent-170, score-0.264]
</p><p>63 samples; and where we relax Assumption [A2], and allow quantities KΣ∗ , KΓ∗ to grow with sample size n. [sent-173, score-0.159]
</p><p>64 Our proofs are based on a technique that we call a primal-dual witness method, used previously in analysis of the Lasso [12]. [sent-175, score-0.231]
</p><p>65 It involves following a speciﬁc sequence of steps to construct a pair (Θ, Z) of symmetric matrices that together satisfy the optimality conditions associated with the convex program (3) with high probability. [sent-176, score-0.371]
</p><p>66 In this way, the estimator Θ inherits from Θ various optimality properties in terms of its distance to the truth Θ∗ , and its recovery of the signed sparsity pattern. [sent-178, score-0.451]
</p><p>67 1 Primal-dual witness approach At the core of the primal-dual witness method are the standard convex optimality conditions that characterize the optimum Θ of the convex program (3). [sent-181, score-0.773]
</p><p>68 For future reference, we note that the subdifferential of the norm · 1,oﬀ evaluated at some Θ consists the set of all symmetric matrices Z ∈ Rp×p such that  if i = j 0 Zij = (12) sign(Θij ) if i = j and Θij = 0  ∈ [−1, +1] if i = j and Θij = 0. [sent-182, score-0.242]
</p><p>69 For any λn > 0 and sample covariance Σ with strictly positive diagonal, the ℓ1 regularized log-determinant problem (3) has a unique solution Θ ≻ 0 characterized by Σ − Θ−1 + λn Z where Z is an element of the subdifferential ∂ Θ  = 0,  (13)  1,oﬀ . [sent-184, score-0.359]
</p><p>70 Based on this lemma, we construct the primal-dual witness solution (Θ, Z) as follows: (a) We determine the matrix Θ by solving the restricted log-determinant problem Θ  := arg  min  Θ≻0, ΘS c =0  Θ, Σ − log det(Θ) + λn Θ  . [sent-185, score-0.414]
</p><p>71 (c) We set ZS c as ZS c  =  1 − ΣS c + [Θ−1 ]S c , λn  (15)  which ensures that constructed matrices (Θ, Z) satisfy the optimality condition (13). [sent-188, score-0.206]
</p><p>72 To clarify the nature of the construction, steps (a) through (c) sufﬁce to obtain a pair (Θ, Z) that satisfy the optimality conditions (13), but do not guarantee that Z is an element of sub-differential ∂ Θ 1,oﬀ . [sent-190, score-0.217]
</p><p>73 By construction, speciﬁcally step (b) of the construction ensures that the entries Z in S satisfy the sub-differential conditions, since ZS is a member of the sub-differential of ∂ ΘS 1,oﬀ . [sent-191, score-0.323]
</p><p>74 The purpose of step (d), then, is to verify that the remaining elements of Z satisfy the necessary conditions to belong to the sub-differential. [sent-192, score-0.196]
</p><p>75 If the primal-dual witness construction succeeds, then it acts as a witness to the fact that the solution Θ to the restricted problem (14) is equivalent to the solution Θ to the original (unrestricted) problem (3). [sent-193, score-0.511]
</p><p>76 We exploit this fact in our proof of Theorem 1: we ﬁrst show that the primal-dual witness technique succeeds with high-probability, from which we can conclude that the support of the optimal solution Θ is contained within the support of the true Θ∗ . [sent-194, score-0.304]
</p><p>77 The next step requires checking that none of the entries in ΘS constructed in Equation (14) are zero. [sent-195, score-0.165]
</p><p>78 These graphs allow us to vary both d and p, since the degree of the central hub can be varied between 1 and p − 1. [sent-200, score-0.289]
</p><p>79 We tested varying graph sizes p from p = 64 upwards to p = 375. [sent-202, score-0.29]
</p><p>80 The edge-weights were set as entries in the inverse of a covariance matrix Σ∗ with diagonal entries set as Σ∗ = 1 for all i = 1, . [sent-203, score-0.711]
</p><p>81 5/d for all (i, j) ∈ E, so that the ii ij quantities (KΣ∗ , KΓ∗ , α) remain constant. [sent-207, score-0.189]
</p><p>82 Dependence on graph size: Star graph  Star graph  0. [sent-208, score-0.555]
</p><p>83 Simulations for a star graph with varying number of nodes p, ﬁxed maximal degree d = 40, and edge covariances Σ∗ = 1/16 for all edges. [sent-219, score-0.769]
</p><p>84 Plots of probability of correct signed edge-set recovery ij versus the sample size n in panel (a), and versus the rescaled sample size n/ log p in panel (b). [sent-220, score-1.318]
</p><p>85 Panel (a) of Figure 3 plots the probability of correct signed edge-set recovery against the sample size n for a star-shaped graph of three different graph sizes p. [sent-222, score-0.893]
</p><p>86 For each curve, the probability of success starts at zero (for small sample sizes n), but then transitions to one as the sample size is increased. [sent-223, score-0.382]
</p><p>87 As would be expected, it is more difﬁcult to perform model selection for larger graph sizes, so that (for instance) the curve for p = 375 is shifted to the right relative to the curve for p = 64. [sent-224, score-0.394]
</p><p>88 Panel (b) of Figure 3 replots the same data, with the horizontal axis rescaled by (1/ log p). [sent-225, score-0.156]
</p><p>89 This scaling was chosen because our theory predicts that the sample size should scale logarithmically with p (see equation (10)). [sent-226, score-0.212]
</p><p>90 Consistent with this prediction, when plotted against the rescaled sample size n/ log p, the curves in panel (b) all stack up. [sent-227, score-0.525]
</p><p>91 Consequently, the ratio (n/ log p) acts as an effective sample size in controlling the success of model selection, consistent with the predictions of Theorem 1. [sent-228, score-0.3]
</p><p>92 Observe how the plots in panel (a) shift to the right as the maximum node degree d is increased, showing that star-shaped graphs with higher degrees are more difﬁcult. [sent-231, score-0.55]
</p><p>93 In panel (b) of Figure 4, we plot the same data versus the rescaled sample size n/d. [sent-232, score-0.464]
</p><p>94 Recall that if all the curves were to stack up under this rescaling, then it means the required sample size n scales linearly with d. [sent-233, score-0.208]
</p><p>95 In particular, observe that the curve d (right-most in panel (a)) remains a bit to the right in panel (b), which suggests that a somewhat more aggressive rescaling—perhaps n/dγ for some γ ∈ (1, 2)—is appropriate. [sent-235, score-0.38]
</p><p>96 Simulations for star graphs with ﬁxed number of nodes p = 200, varying maximal (hub) degree d, edge covariances Σ∗ = 2. [sent-247, score-0.675]
</p><p>97 Plots of probability of correct signed edge-set recovery ij versus the sample size n in panel (a), and versus the rescaled sample size n/d in panel (b). [sent-249, score-1.258]
</p><p>98 Model selection and estimation in the Gaussian graphical model. [sent-267, score-0.205]
</p><p>99 Sharp thresholds for high-dimensional and noisy recovery of sparsity using the Lasso. [sent-329, score-0.182]
</p><p>100 High-dimensional covariance estimation by minimizing ℓ1 -penalized log-determinant divergence. [sent-344, score-0.182]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('irrepresentability', 0.264), ('witness', 0.231), ('star', 0.204), ('ij', 0.189), ('graph', 0.185), ('covariance', 0.182), ('lasso', 0.176), ('frobenius', 0.176), ('entries', 0.165), ('panel', 0.161), ('norm', 0.141), ('signed', 0.127), ('mle', 0.122), ('consistency', 0.119), ('gmrf', 0.113), ('rothman', 0.113), ('graphical', 0.112), ('recovery', 0.11), ('degree', 0.107), ('concentration', 0.105), ('zs', 0.102), ('meinshausen', 0.099), ('uij', 0.099), ('rescaled', 0.096), ('edges', 0.095), ('selection', 0.093), ('graphs', 0.091), ('hub', 0.091), ('vertex', 0.09), ('edge', 0.088), ('sample', 0.086), ('conditions', 0.086), ('matrix', 0.082), ('success', 0.081), ('incoherence', 0.08), ('node', 0.078), ('raskutti', 0.075), ('condition', 0.075), ('estimator', 0.075), ('rp', 0.073), ('succeeds', 0.073), ('size', 0.073), ('sparsity', 0.072), ('plots', 0.071), ('ss', 0.071), ('ravikumar', 0.068), ('optimality', 0.067), ('hlmann', 0.066), ('gaussian', 0.065), ('hessian', 0.064), ('satisfy', 0.064), ('vectorized', 0.06), ('theorem', 0.06), ('log', 0.06), ('inverse', 0.059), ('diagonal', 0.058), ('curve', 0.058), ('zij', 0.056), ('gauss', 0.056), ('nodes', 0.056), ('sizes', 0.056), ('wainwright', 0.055), ('notation', 0.054), ('program', 0.054), ('counterpart', 0.054), ('outline', 0.054), ('subdifferential', 0.053), ('scaling', 0.053), ('convex', 0.052), ('penalized', 0.052), ('mrfs', 0.051), ('ambient', 0.051), ('holds', 0.05), ('construction', 0.049), ('varying', 0.049), ('stack', 0.049), ('versus', 0.048), ('symmetric', 0.048), ('neighborhood', 0.046), ('simulations', 0.046), ('verify', 0.046), ('member', 0.045), ('rescaling', 0.043), ('maximum', 0.042), ('min', 0.041), ('covariances', 0.04), ('mrf', 0.04), ('maximal', 0.04), ('inner', 0.04), ('berkeley', 0.04), ('det', 0.039), ('truncated', 0.039), ('eld', 0.039), ('strictly', 0.038), ('xp', 0.038), ('uc', 0.037), ('regularizer', 0.037), ('consequently', 0.037), ('assumptions', 0.036), ('fisher', 0.036)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="135-tfidf-1" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>2 0.30428487 <a title="135-tfidf-2" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>Author: Sahand Negahban, Martin J. Wainwright</p><p>Abstract: Given a collection of r ≥ 2 linear regression problems in p dimensions, suppose that the regression coefﬁcients share partially common supports. This set-up suggests the use of 1 / ∞ -regularized regression for joint estimation of the p × r matrix of regression coefﬁcients. We analyze the high-dimensional scaling of 1 / ∞ -regularized quadratic programming, considering both consistency rates in ∞ -norm, and also how the minimal sample size n required for performing variable selection grows as a function of the model dimension, sparsity, and overlap between the supports. We begin by establishing bounds on the ∞ error as well sufﬁcient conditions for exact variable selection for ﬁxed design matrices, as well as designs drawn randomly from general Gaussian matrices. These results show that the high-dimensional scaling of 1 / ∞ -regularization is qualitatively similar to that of ordinary 1 -regularization. Our second set of results applies to design matrices drawn from standard Gaussian ensembles, for which we provide a sharp set of necessary and sufﬁcient conditions: the 1 / ∞ -regularized method undergoes a phase transition characterized by the rescaled sample size θ1,∞ (n, p, s, α) = n/{(4 − 3α)s log(p − (2 − α) s)}. More precisely, for any δ > 0, the probability of successfully recovering both supports converges to 1 for scalings such that θ1,∞ ≥ 1 + δ, and converges to 0 for scalings for which θ1,∞ ≤ 1 − δ. An implication of this threshold is that use of 1,∞ -regularization yields improved statistical efﬁciency if the overlap parameter is large enough (α > 2/3), but performs worse than a naive Lasso-based approach for moderate to small overlap (α < 2/3). We illustrate the close agreement between these theoretical predictions, and the actual behavior in simulations. 1</p><p>3 0.25711033 <a title="135-tfidf-3" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>Author: Guillaume R. Obozinski, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: We study the behavior of block 1 / 2 regularization for multivariate regression, where a K-dimensional response vector is regressed upon a ﬁxed set of p covariates. The problem of support union recovery is to recover the subset of covariates that are active in at least one of the regression problems. Studying this problem under high-dimensional scaling (where the problem parameters as well as sample size n tend to inﬁnity simultaneously), our main result is to show that exact recovery is possible once the order parameter given by θ 1 / 2 (n, p, s) : = n/[2ψ(B ∗ ) log(p − s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the size of the union of supports, and ψ(B ∗ ) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefﬁcient vectors that constitute the model. This sparsity-overlap function reveals that block 1 / 2 regularization for multivariate regression never harms performance relative to a naive 1 -approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal relative to the design. We complement our theoretical results with simulations that demonstrate the sharpness of the result, even for relatively small problems. 1</p><p>4 0.18135171 <a title="135-tfidf-4" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>5 0.15194163 <a title="135-tfidf-5" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>Author: David Sontag, Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: We propose a new class of consistency constraints for Linear Programming (LP) relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and signiﬁcantly faster than a method that does not use partitioning. 1</p><p>6 0.14195277 <a title="135-tfidf-6" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>7 0.13944435 <a title="135-tfidf-7" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>8 0.13453288 <a title="135-tfidf-8" href="./nips-2008-Fast_Prediction_on_a_Tree.html">84 nips-2008-Fast Prediction on a Tree</a></p>
<p>9 0.12912245 <a title="135-tfidf-9" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>10 0.12292062 <a title="135-tfidf-10" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>11 0.11620709 <a title="135-tfidf-11" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>12 0.11540395 <a title="135-tfidf-12" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<p>13 0.11335996 <a title="135-tfidf-13" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>14 0.10791982 <a title="135-tfidf-14" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>15 0.10593773 <a title="135-tfidf-15" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>16 0.10476612 <a title="135-tfidf-16" href="./nips-2008-Resolution_Limits_of_Sparse_Coding_in_High_Dimensions.html">198 nips-2008-Resolution Limits of Sparse Coding in High Dimensions</a></p>
<p>17 0.097896509 <a title="135-tfidf-17" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>18 0.092455953 <a title="135-tfidf-18" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>19 0.089822322 <a title="135-tfidf-19" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>20 0.088405609 <a title="135-tfidf-20" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.294), (1, -0.039), (2, -0.138), (3, 0.212), (4, 0.19), (5, -0.098), (6, -0.08), (7, -0.004), (8, -0.039), (9, -0.055), (10, -0.26), (11, -0.094), (12, -0.081), (13, -0.083), (14, -0.087), (15, 0.052), (16, -0.004), (17, -0.013), (18, -0.081), (19, -0.036), (20, 0.012), (21, -0.058), (22, 0.039), (23, -0.015), (24, 0.021), (25, 0.009), (26, 0.03), (27, 0.066), (28, 0.075), (29, 0.019), (30, 0.102), (31, 0.019), (32, -0.079), (33, -0.103), (34, 0.054), (35, 0.016), (36, -0.066), (37, 0.085), (38, -0.034), (39, -0.03), (40, 0.108), (41, 0.144), (42, 0.027), (43, -0.027), (44, 0.025), (45, -0.082), (46, 0.104), (47, 0.039), (48, 0.021), (49, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9681226 <a title="135-lsi-1" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>2 0.86510766 <a title="135-lsi-2" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>Author: Guillaume R. Obozinski, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: We study the behavior of block 1 / 2 regularization for multivariate regression, where a K-dimensional response vector is regressed upon a ﬁxed set of p covariates. The problem of support union recovery is to recover the subset of covariates that are active in at least one of the regression problems. Studying this problem under high-dimensional scaling (where the problem parameters as well as sample size n tend to inﬁnity simultaneously), our main result is to show that exact recovery is possible once the order parameter given by θ 1 / 2 (n, p, s) : = n/[2ψ(B ∗ ) log(p − s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the size of the union of supports, and ψ(B ∗ ) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefﬁcient vectors that constitute the model. This sparsity-overlap function reveals that block 1 / 2 regularization for multivariate regression never harms performance relative to a naive 1 -approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal relative to the design. We complement our theoretical results with simulations that demonstrate the sharpness of the result, even for relatively small problems. 1</p><p>3 0.84413409 <a title="135-lsi-3" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>Author: Sahand Negahban, Martin J. Wainwright</p><p>Abstract: Given a collection of r ≥ 2 linear regression problems in p dimensions, suppose that the regression coefﬁcients share partially common supports. This set-up suggests the use of 1 / ∞ -regularized regression for joint estimation of the p × r matrix of regression coefﬁcients. We analyze the high-dimensional scaling of 1 / ∞ -regularized quadratic programming, considering both consistency rates in ∞ -norm, and also how the minimal sample size n required for performing variable selection grows as a function of the model dimension, sparsity, and overlap between the supports. We begin by establishing bounds on the ∞ error as well sufﬁcient conditions for exact variable selection for ﬁxed design matrices, as well as designs drawn randomly from general Gaussian matrices. These results show that the high-dimensional scaling of 1 / ∞ -regularization is qualitatively similar to that of ordinary 1 -regularization. Our second set of results applies to design matrices drawn from standard Gaussian ensembles, for which we provide a sharp set of necessary and sufﬁcient conditions: the 1 / ∞ -regularized method undergoes a phase transition characterized by the rescaled sample size θ1,∞ (n, p, s, α) = n/{(4 − 3α)s log(p − (2 − α) s)}. More precisely, for any δ > 0, the probability of successfully recovering both supports converges to 1 for scalings such that θ1,∞ ≥ 1 + δ, and converges to 0 for scalings for which θ1,∞ ≤ 1 − δ. An implication of this threshold is that use of 1,∞ -regularization yields improved statistical efﬁciency if the overlap parameter is large enough (α > 2/3), but performs worse than a naive Lasso-based approach for moderate to small overlap (α < 2/3). We illustrate the close agreement between these theoretical predictions, and the actual behavior in simulations. 1</p><p>4 0.64154899 <a title="135-lsi-4" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>5 0.57405198 <a title="135-lsi-5" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>Author: Guangzhi Cao, Charles Bouman</p><p>Abstract: Covariance estimation for high dimensional vectors is a classically difﬁcult problem in statistical analysis and machine learning. In this paper, we propose a maximum likelihood (ML) approach to covariance estimation, which employs a novel sparsity constraint. More speciﬁcally, the covariance is constrained to have an eigen decomposition which can be represented as a sparse matrix transform (SMT). The SMT is formed by a product of pairwise coordinate rotations known as Givens rotations. Using this framework, the covariance can be efﬁciently estimated using greedy minimization of the log likelihood function, and the number of Givens rotations can be efﬁciently computed using a cross-validation procedure. The resulting estimator is positive deﬁnite and well-conditioned even when the sample size is limited. Experiments on standard hyperspectral data sets show that the SMT covariance estimate is consistently more accurate than both traditional shrinkage estimates and recently proposed graphical lasso estimates for a variety of different classes and sample sizes. 1</p><p>6 0.50889486 <a title="135-lsi-6" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>7 0.50826526 <a title="135-lsi-7" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>8 0.50503719 <a title="135-lsi-8" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>9 0.50149375 <a title="135-lsi-9" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>10 0.4927426 <a title="135-lsi-10" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>11 0.48422939 <a title="135-lsi-11" href="./nips-2008-Fast_Prediction_on_a_Tree.html">84 nips-2008-Fast Prediction on a Tree</a></p>
<p>12 0.43957648 <a title="135-lsi-12" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>13 0.43716729 <a title="135-lsi-13" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>14 0.43715093 <a title="135-lsi-14" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>15 0.43539488 <a title="135-lsi-15" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<p>16 0.42999658 <a title="135-lsi-16" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>17 0.42352846 <a title="135-lsi-17" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>18 0.41788992 <a title="135-lsi-18" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>19 0.41547811 <a title="135-lsi-19" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>20 0.41282043 <a title="135-lsi-20" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.01), (6, 0.093), (7, 0.094), (12, 0.038), (16, 0.011), (24, 0.032), (27, 0.017), (28, 0.215), (43, 0.142), (57, 0.062), (59, 0.037), (63, 0.04), (71, 0.028), (77, 0.059), (83, 0.063)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91615713 <a title="135-lda-1" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>2 0.88289911 <a title="135-lda-2" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>Author: Sahand Negahban, Martin J. Wainwright</p><p>Abstract: Given a collection of r ≥ 2 linear regression problems in p dimensions, suppose that the regression coefﬁcients share partially common supports. This set-up suggests the use of 1 / ∞ -regularized regression for joint estimation of the p × r matrix of regression coefﬁcients. We analyze the high-dimensional scaling of 1 / ∞ -regularized quadratic programming, considering both consistency rates in ∞ -norm, and also how the minimal sample size n required for performing variable selection grows as a function of the model dimension, sparsity, and overlap between the supports. We begin by establishing bounds on the ∞ error as well sufﬁcient conditions for exact variable selection for ﬁxed design matrices, as well as designs drawn randomly from general Gaussian matrices. These results show that the high-dimensional scaling of 1 / ∞ -regularization is qualitatively similar to that of ordinary 1 -regularization. Our second set of results applies to design matrices drawn from standard Gaussian ensembles, for which we provide a sharp set of necessary and sufﬁcient conditions: the 1 / ∞ -regularized method undergoes a phase transition characterized by the rescaled sample size θ1,∞ (n, p, s, α) = n/{(4 − 3α)s log(p − (2 − α) s)}. More precisely, for any δ > 0, the probability of successfully recovering both supports converges to 1 for scalings such that θ1,∞ ≥ 1 + δ, and converges to 0 for scalings for which θ1,∞ ≤ 1 − δ. An implication of this threshold is that use of 1,∞ -regularization yields improved statistical efﬁciency if the overlap parameter is large enough (α > 2/3), but performs worse than a naive Lasso-based approach for moderate to small overlap (α < 2/3). We illustrate the close agreement between these theoretical predictions, and the actual behavior in simulations. 1</p><p>3 0.86965454 <a title="135-lda-3" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>Author: Guillaume R. Obozinski, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: We study the behavior of block 1 / 2 regularization for multivariate regression, where a K-dimensional response vector is regressed upon a ﬁxed set of p covariates. The problem of support union recovery is to recover the subset of covariates that are active in at least one of the regression problems. Studying this problem under high-dimensional scaling (where the problem parameters as well as sample size n tend to inﬁnity simultaneously), our main result is to show that exact recovery is possible once the order parameter given by θ 1 / 2 (n, p, s) : = n/[2ψ(B ∗ ) log(p − s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the size of the union of supports, and ψ(B ∗ ) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefﬁcient vectors that constitute the model. This sparsity-overlap function reveals that block 1 / 2 regularization for multivariate regression never harms performance relative to a naive 1 -approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal relative to the design. We complement our theoretical results with simulations that demonstrate the sharpness of the result, even for relatively small problems. 1</p><p>4 0.86962706 <a title="135-lda-4" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>5 0.8677842 <a title="135-lda-5" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>6 0.86751711 <a title="135-lda-6" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>7 0.86362296 <a title="135-lda-7" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>8 0.86357695 <a title="135-lda-8" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>9 0.86264718 <a title="135-lda-9" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>10 0.86215281 <a title="135-lda-10" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>11 0.8606922 <a title="135-lda-11" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>12 0.85934675 <a title="135-lda-12" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>13 0.85868138 <a title="135-lda-13" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>14 0.85847563 <a title="135-lda-14" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>15 0.85834247 <a title="135-lda-15" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>16 0.85816801 <a title="135-lda-16" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>17 0.8580808 <a title="135-lda-17" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>18 0.85717273 <a title="135-lda-18" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>19 0.85580021 <a title="135-lda-19" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>20 0.85527664 <a title="135-lda-20" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
