<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>218 nips-2008-Spectral Clustering with Perturbed Data</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-218" href="#">nips2008-218</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>218 nips-2008-Spectral Clustering with Perturbed Data</h1>
<br/><p>Source: <a title="nips-2008-218-pdf" href="http://papers.nips.cc/paper/3480-spectral-clustering-with-perturbed-data.pdf">pdf</a></p><p>Author: Ling Huang, Donghui Yan, Nina Taft, Michael I. Jordan</p><p>Abstract: Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a speciﬁcation of permitted loss in clustering performance. 1</p><p>Reference: <a title="nips-2008-218-reference" href="../nips2008_reference/nips-2008-Spectral_Clustering_with_Perturbed_Data_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. [sent-10, score-0.387]
</p><p>2 However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. [sent-11, score-0.433]
</p><p>3 In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. [sent-12, score-1.169]
</p><p>4 We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. [sent-13, score-1.53]
</p><p>5 From this result we derive approximate upper bounds on the clustering error. [sent-14, score-0.453]
</p><p>6 We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a speciﬁcation of permitted loss in clustering performance. [sent-15, score-0.592]
</p><p>7 One general tool for approaching large-scale problems is that of clustering or partitioning, in essence an appeal to the principle of divide-and-conquer. [sent-17, score-0.313]
</p><p>8 Such preprocessing steps also arise in the distributed sensing and distributed computing setting, where communication and storage limitations may preclude transmitting the original data to centralized processors. [sent-19, score-0.172]
</p><p>9 A number of recent works have begun to tackle the issue of determining the tradeoffs that arise under various “perturbations” of data, including quantization and downsampling [2, 3, 4]. [sent-20, score-0.431]
</p><p>10 In this paper we focus on spectral clustering, a class of clustering methods that are based on eigendecompositions of afﬁnity, dissimilarity or kernel matrices [5, 6, 7, 8]. [sent-23, score-0.559]
</p><p>11 These algorithms often outperform traditional clustering algorithms such as the K-means algorithm or hierarchical clustering. [sent-24, score-0.313]
</p><p>12 To date, however, their impact on real-world, large-scale problems has been limited; in particular, a distributed or “in-network” version of spectral clustering has not yet appeared. [sent-25, score-0.6]
</p><p>13 Moreover, there has been little work on the statistical analysis of spectral clustering, and thus there is little theory to guide the design of distributed algorithms. [sent-26, score-0.329]
</p><p>14 Data error Error propagation  Figure 1: A spectral bipartitioning algorithm. [sent-42, score-0.399]
</p><p>15 σ  Assumption A  Perturbation analysis  Figure 2: Perturbation analysis: from clustering error to data perturbation error. [sent-43, score-0.843]
</p><p>16 scaling spectral clustering (including downsampling [9, 10] and the relaxation of precision requirements for the eigenvector computation [7]), but this literature does not provide end-to-end, practical bounds on error rates as a function of data perturbations. [sent-44, score-0.913]
</p><p>17 In this paper we present the ﬁrst end-to-end analysis of the effect of data perturbations on spectral clustering. [sent-45, score-0.347]
</p><p>18 Indeed, given that our approach is based on treating perturbations as random variables, we believe that our methods will also prove useful in developing statistical analyses of spectral clustering (although that is not our focus in this paper). [sent-47, score-0.655]
</p><p>19 In Section 2, we provide a brief introduction to spectral clustering. [sent-49, score-0.246]
</p><p>20 Section 3 contains the main results of the paper; speciﬁcally we introduce the mis-clustering rate η, and present upper bounds on η due to data perturbations. [sent-50, score-0.242]
</p><p>21 The point of departure of a spectral clustering algorithm is a weighted similarity graph G(V, E), where the vertices correspond to data points and the weights correspond to the pairwise similarities. [sent-55, score-0.65]
</p><p>22 Based on this weighted graph, spectral clustering algorithms form the graph Laplacian and compute an eigendecomposition of this Laplacian [5, 6, 7]. [sent-56, score-0.595]
</p><p>23 While some algorithms use multiple eigenvectors and ﬁnd a k-way clustering directly, the most widely studied algorithms form a bipartitioning of the data by thresholding the second eigenvector of the Laplacian (the eigenvector with the second smallest eigenvalue). [sent-57, score-0.694]
</p><p>24 Larger numbers of clusters are found by applying the bipartitioning algorithm recursively. [sent-58, score-0.128]
</p><p>25 We present a speciﬁc example of a spectral bipartitioning algorithm in Fig. [sent-59, score-0.349]
</p><p>26 2  Input data perturbation  Let the data matrix X ∈ Rn×d be formed by stacking n data samples in rows. [sent-62, score-0.615]
</p><p>27 To this data matrix we ˜ assume that perturbation W is applied, such that we obtain a perturbed version X of the original data ˜ and we wish to compare the results X. [sent-63, score-0.671]
</p><p>28 We assume that a spectral clustering algorithm is applied to X of this clustering with respect to the spectral clustering of X. [sent-64, score-1.431]
</p><p>29 This analysis captures a number of data perturbation methods, including data ﬁltering, quantization, lossy compression and synopsis-based data approximation [11]. [sent-65, score-0.575]
</p><p>30 The multi-scale clustering algorithms that use “representative” samples to approximate the original data can be treated using our analysis as well [12]. [sent-66, score-0.371]
</p><p>31 2  3  Mis-clustering rate and effects of data perturbation  ˜ ˜ Let K and L be the similarity and Laplacian matrix on the original data X, and let K and L be those on the perturbed data. [sent-67, score-0.79]
</p><p>32 ˜ − X, which we now We wish to bound η in terms of the “magnitude” of the error matrix W = X deﬁne. [sent-69, score-0.222]
</p><p>33 We make the following general stochastic assumption on the error matrix W : A. [sent-70, score-0.111]
</p><p>34 (ii) This assumption is distribution free, and captures a wide variety of practical data collection and quantization schemes. [sent-80, score-0.372]
</p><p>35 (iii) Certain data perturbation schemes may not satisfy the independence assumption. [sent-81, score-0.48]
</p><p>36 We have not yet conducted an analysis of the robustness of our bounds to lack of independence, but in our empirical work we have found that the bounds are robust to relatively small amounts of correlation. [sent-82, score-0.1]
</p><p>37 We aim to produce practically useful bounds on η in terms of σ and the data matrix X. [sent-83, score-0.172]
</p><p>38 The bounds should be reasonably tight so that in practice they could be used to determine the degree of perturbation σ given a desired level of clustering performance, or to provide a clustering error guarantee on the original data even though we have access only to its approximate version. [sent-84, score-1.306]
</p><p>39 , by ﬁltering, quantization or compression), we introduce a perturbation W to the data which is quan˜ tiﬁed by σ 2 . [sent-89, score-0.773]
</p><p>40 This induces an error dK := K − K in the similarity matrix, and in turn an error ˜ − L in the Laplacian matrix. [sent-90, score-0.154]
</p><p>41 This further yields an error in the second eigenvector of dL := L the Laplacian matrix, which results in mis-clustering error. [sent-91, score-0.153]
</p><p>42 Overall, we establish an analytical relationship between the mis-clustering rate η and the data perturbation error σ 2 , where η is usually monotonically increasing with σ 2 . [sent-92, score-0.627]
</p><p>43 Our goal is to allow practitioners to specify a mis-clustering rate η ∗ , and by inverting this relationship, to determine the right magnitude of the perturbation σ ∗ allowed. [sent-93, score-0.609]
</p><p>44 That is, our work can provide a practical method to determine the tradeoff between data ˜ perturbation and the loss of clustering accuracy due to the use of X instead of X. [sent-94, score-0.859]
</p><p>45 When the data perturbation can be related to computational or communications savings, then our analysis yields a practical characterization of the overall resource/accuracy tradeoff. [sent-95, score-0.522]
</p><p>46 Practical Applications Consider in particular a clustering task in a distributed networking system that allows an application to specify a desired clustering error C ∗ on the distributed data (which is not available to the coordinator). [sent-96, score-0.795]
</p><p>47 , network operation center) gets access to the perturbed data X for spectral clustering. [sent-99, score-0.378]
</p><p>48 The coordinator can compute a clustering error bound C using our method. [sent-100, score-0.515]
</p><p>49 By setting C ≤ C ∗ , it determines the tolerable data perturbation error σ ∗ and instructs distributed devices to use appropriate numbers of bits to quantize their data. [sent-101, score-0.7]
</p><p>50 Thus we can provide guarantees on the achieved error, C ≤ C ∗ , with respect to the original distributed data even with access only to the perturbed data. [sent-102, score-0.194]
</p><p>51 1  Upper bounding the mis-clustering rate  Little is currently known about the connection between clustering error and perturbations to the Laplacian matrix in the spectral clustering setting. [sent-104, score-1.112]
</p><p>52 [5] presented an upper bound for the clustering error, however this bound is usually quite loose and is not viable for practical applications. [sent-105, score-0.667]
</p><p>53 If v2 is changed to v2 due to the perturbation, an incorrect clustering happens whenever a component of v2 in set a jumps to set b′ , denoted as a → b′ , or a component in set b jumps to set a′ , denoted as b → a′ . [sent-112, score-0.363]
</p><p>54 005  Figure 3: The second eigenvector v2 and its per˜ turbed counterpart v2 (denoted by dashed lines). [sent-121, score-0.103]
</p><p>55 035  Figure 4: An example of the tightness of the upper bound for η in Eq. [sent-128, score-0.201]
</p><p>56 , missed clusterings) is therefore constrained, and this translates into an upper bound for η. [sent-133, score-0.201]
</p><p>57 The components of v2 form two clusters (with respect to the spectral bipartitioning algorithm in Fig. [sent-135, score-0.4]
</p><p>58 The perturbation is small with the total number of mis-clusterings m < min(k1 , k2 ), and ˜ the components of v2 form two clusters. [sent-139, score-0.469]
</p><p>59 The perturbation of individual components of v2 in each set of a → a′ , a → b′ , b → a′ and b → b′ have identical (not necessary independent) distributions with bounded second moments, respectively, and they are uncorrelated with the components in v2 . [sent-142, score-0.518]
</p><p>60 Our perturbation bound can now be stated as follows: Proposition 1. [sent-143, score-0.554]
</p><p>61 Under assumptions B1, B2 and B3, the mis-clustering rate η of the spectral biparti˜ tioning algorithm under the perturbation satisﬁes η ≤ δ 2 = v2 − v2 2 . [sent-144, score-0.754]
</p><p>62 (iii) Our bound has a different ﬂavor than that obtained in [5]. [sent-152, score-0.111]
</p><p>63 3 in [5] works for k-way clustering, it assumes a block-diagonal Laplacian matrix and requires the gap between the k th and (k + 1)th eigenvalues to be greater than 1/2, which is unrealistic in many data sets. [sent-154, score-0.098]
</p><p>64 In the setting of 2-way spectral clustering and a small perturbation, our bound is much tighter than that derived in [5]; see Fig. [sent-155, score-0.67]
</p><p>65 2  Perturbation on the second eigenvector of Laplacian matrix  We now turn to the relationship between the perturbation of eigenvectors with that of its matrix. [sent-158, score-0.674]
</p><p>66 One approach is to simply draw on the classical domain of matrix perturbation theory; in particular, applying Theorem V. [sent-159, score-0.504]
</p><p>67 8 from [13], we have the following bound on the (small) perturbation of the second eigenvector: ˜ v2 − v2 ≤  4dL √ F ν − 2 dL  ,  (2)  F  where ν is the gap between the second and the third eigenvalue. [sent-161, score-0.554]
</p><p>68 zero mean Gaussian noise to the input data with different σ, and we see that the right-hand side (RHS) of (5) approximately upper bounds the left-hand side (LHS). [sent-204, score-0.202]
</p><p>69 Thus the bound given by (2) is often not useful in practical applications. [sent-206, score-0.153]
</p><p>70 (5) and (6), we can bound the squared norm of the perturbation on the second eigenvector in expectation, which in turn bounds the mis-clustering rate. [sent-220, score-0.707]
</p><p>71 3  Perturbation on the Laplacian matrix  Let D be the diagonal matrix with Di = j Kij . [sent-223, score-0.122]
</p><p>72 If perturbation dK is small compared to K, then dL = (1 + o(1)) ∆D−2 K − D−1 dK. [sent-226, score-0.443]
</p><p>73 The quantities needed to estimate E(dL) and E(dL ) can ˜ be obtained from moments and correlations among the elements of the similarity matrix Kij . [sent-228, score-0.211]
</p><p>74 For simplicity, we could set ˜ K ijq ρk to 1 in Eq. [sent-231, score-0.165]
</p><p>75 This bound could ijq optionally be tightened by using a simulation method to estimate the values of ρk . [sent-234, score-0.276]
</p><p>76 However, in our ijq experimental work we have found that our results are insensitive to the values of ρk , and setting ijq ρk = 0. [sent-235, score-0.33]
</p><p>77 , to upper bound) the ﬁrst two moments of dL using those of dK, which are computed using Eq. [sent-241, score-0.186]
</p><p>78 4  Perturbation on the similarity matrix  ˜ ˜ The similarity matrix K on perturbed data X is 2  ||xi − xj + ǫi − ǫj || ˜ Kij = exp − 2 2σk  ,  (14)  ˜ where σk is the kernel bandwidth. [sent-245, score-0.361]
</p><p>79 Then, given data X, the ﬁrst two moments of dKij = Kij − Kij , the error in the similarity matrix, can be determined by one of the following lemmas. [sent-246, score-0.237]
</p><p>80 6  (15)  (a) Gaussian data  (b) Sin−sep data  5 4  2  3  (c) Concentric data  3  1  2  10 5  0  0  −1  −5  1 0  −2  −1  −10  −3  −2 −2  0  2  4  −2  −1  0  1  2  −15  −10  −5  0  5  10  Figure 6: Synthetic data sets illustrated in two dimensions. [sent-252, score-0.148]
</p><p>81 Under Assumption A, given X and for large values of the dimension d, the ﬁrst two ˜ moments of Kij can be computed approximately as follows: 1 ˜ E Kij = Mij − 2 2σk where Mij (t) = exp  ,  1 ˜2 E Kij = Mij − 2 σk  ,  (16)  λij + 2dσ 2 t + dµ4 + dσ 4 + 4σ 2 λ2 t2 , and λij = ||xi − xj ||2 . [sent-254, score-0.118]
</p><p>82 (i) Given data perturbation error σ, kernel bandwidth σk and data X, the ﬁrst two moments of dKij can be estimated directly using (15) or (16). [sent-256, score-0.685]
</p><p>83 (1)–(16), we have established a relationship between the mis-clustering rate η and the data perturbation magnitude σ. [sent-258, score-0.599]
</p><p>84 , and the different data sets impose different difﬁculty levels on the underlying spectral clustering algorithm, demonstrating the wide applicability of our analysis. [sent-265, score-0.596]
</p><p>85 In the experiments, we use data quantization as the perturbation scheme to evaluate the upper bound provided by our analysis on the clustering error. [sent-266, score-1.287]
</p><p>86 7 plots the mis-clustering rate and the upper bound for data sets subject to varying degrees of quantization. [sent-268, score-0.303]
</p><p>87 As expected, the mis-clustering rate increases as one decreases the number of quantization bits. [sent-269, score-0.358]
</p><p>88 We ﬁnd that the error bounds are remarkably tight, which validate the assumptions we make in the analysis. [sent-270, score-0.1]
</p><p>89 It is also interesting to note that even when using as few as 3-4 bits, the clustering degrades very little in both real error and as assessed by our bound. [sent-271, score-0.384]
</p><p>90 The effectiveness of our bound should allow the practitioner to determine the right amount of quantization given a permitted loss in clustering performance. [sent-272, score-0.774]
</p><p>91 5  Conclusion  In this paper, we proposed a theoretical analysis of the clustering error for spectral clustering in the face of stochastic perturbations. [sent-273, score-0.922]
</p><p>92 Our experimental evaluation has provided support for the assumptions made in the analysis, showing that the bound is tight under conditions of practical interest. [sent-274, score-0.185]
</p><p>93 02  3 4 5 6 7 Number of quantization bits  8  0  0. [sent-358, score-0.422]
</p><p>94 018  4 5 6 7 Number of quantization bits (i) Waveform Data  0. [sent-369, score-0.422]
</p><p>95 001  8  Figure 7: Upper bounds of clustering error on approximate data obtained from quantization as a function of the number of bits. [sent-406, score-0.743]
</p><p>96 (h) Wisconsin breast cancer data (569 sample size, 30 features); (i) Waveform data (5000 sample size, 21 features). [sent-408, score-0.182]
</p><p>97 The x-axis shows the number of quantization bits and (above the axis) the corresponding data perturbation error σ. [sent-409, score-0.952]
</p><p>98 Weiss, “On spectral clustering: Analysis and an algorithm,” in Advances in Neural Information Processing Systems (NIPS), 2002. [sent-433, score-0.246]
</p><p>99 Bousquet, “Consistency of spectral clustering,” Annals of Statistics, vol. [sent-443, score-0.246]
</p><p>100 Brandt, “Fast multiscale clustering and manifold identiﬁcation,” Pattern Recognition, vol. [sent-466, score-0.313]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('perturbation', 0.443), ('kij', 0.336), ('clustering', 0.313), ('quantization', 0.293), ('spectral', 0.246), ('mis', 0.207), ('ekij', 0.165), ('ijq', 0.165), ('laplacian', 0.158), ('dl', 0.155), ('dk', 0.145), ('bits', 0.129), ('bound', 0.111), ('bipartitioning', 0.103), ('dlpq', 0.103), ('edi', 0.103), ('eigenvector', 0.103), ('moments', 0.096), ('upper', 0.09), ('mij', 0.088), ('dkij', 0.083), ('di', 0.076), ('downsampling', 0.072), ('perturbed', 0.072), ('lhs', 0.066), ('rate', 0.065), ('perturbations', 0.064), ('ij', 0.061), ('matrix', 0.061), ('breast', 0.056), ('wisconsin', 0.056), ('similarity', 0.054), ('rhs', 0.054), ('cancer', 0.052), ('bounds', 0.05), ('error', 0.05), ('waveform', 0.044), ('tradeoffs', 0.044), ('practical', 0.042), ('coordinator', 0.041), ('delity', 0.041), ('dlpi', 0.041), ('ekiq', 0.041), ('garofalakis', 0.041), ('kiq', 0.041), ('misclustering', 0.041), ('taft', 0.041), ('vpj', 0.041), ('distributed', 0.041), ('vj', 0.04), ('data', 0.037), ('concentric', 0.036), ('eigendecomposition', 0.036), ('iris', 0.036), ('wine', 0.036), ('eigenvectors', 0.035), ('perturb', 0.033), ('permitted', 0.033), ('pen', 0.033), ('vldb', 0.033), ('communication', 0.032), ('tight', 0.032), ('analyses', 0.032), ('jordan', 0.032), ('relationship', 0.032), ('iq', 0.031), ('digits', 0.031), ('kannan', 0.029), ('etc', 0.029), ('sep', 0.029), ('inverting', 0.028), ('nystr', 0.028), ('segmentation', 0.027), ('practitioners', 0.027), ('clusterings', 0.027), ('malik', 0.027), ('nguyen', 0.027), ('lemma', 0.027), ('components', 0.026), ('noise', 0.025), ('clusters', 0.025), ('cluster', 0.025), ('proposition', 0.025), ('jumps', 0.025), ('practically', 0.024), ('intel', 0.024), ('determine', 0.024), ('bousquet', 0.023), ('access', 0.023), ('uj', 0.023), ('uncorrelated', 0.023), ('xi', 0.022), ('magnitude', 0.022), ('xj', 0.022), ('bandwidth', 0.022), ('pj', 0.022), ('tackle', 0.022), ('original', 0.021), ('little', 0.021), ('compression', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000006 <a title="218-tfidf-1" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>Author: Ling Huang, Donghui Yan, Nina Taft, Michael I. Jordan</p><p>Abstract: Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a speciﬁcation of permitted loss in clustering performance. 1</p><p>2 0.16292191 <a title="218-tfidf-2" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>Author: Yair Weiss, Antonio Torralba, Rob Fergus</p><p>Abstract: Semantic hashing[1] seeks compact binary codes of data-points so that the Hamming distance between codewords correlates with semantic similarity. In this paper, we show that the problem of ﬁnding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresholded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds, we show how to eﬃciently calculate the code of a novel datapoint. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes outperform the state-of-the art. 1</p><p>3 0.14992157 <a title="218-tfidf-3" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>Author: Matthew Blaschko, Arthur Gretton</p><p>Abstract: We introduce a family of unsupervised algorithms, numerical taxonomy clustering, to simultaneously cluster data, and to learn a taxonomy that encodes the relationship between the clusters. The algorithms work by maximizing the dependence between the taxonomy and the original data. The resulting taxonomy is a more informative visualization of complex data than simple clustering; in addition, taking into account the relations between different clusters is shown to substantially improve the quality of the clustering, when compared with state-ofthe-art algorithms in the literature (both spectral clustering and a previous dependence maximization approach). We demonstrate our algorithm on image and text data. 1</p><p>4 0.13451444 <a title="218-tfidf-4" href="./nips-2008-Measures_of_Clustering_Quality%3A_A_Working_Set_of_Axioms_for_Clustering.html">132 nips-2008-Measures of Clustering Quality: A Working Set of Axioms for Clustering</a></p>
<p>Author: Shai Ben-David, Margareta Ackerman</p><p>Abstract: Aiming towards the development of a general clustering theory, we discuss abstract axiomatization for clustering. In this respect, we follow up on the work of Kleinberg, ([1]) that showed an impossibility result for such axiomatization. We argue that an impossibility result is not an inherent feature of clustering, but rather, to a large extent, it is an artifact of the speciﬁc formalism used in [1]. As opposed to previous work focusing on clustering functions, we propose to address clustering quality measures as the object to be axiomatized. We show that principles like those formulated in Kleinberg’s axioms can be readily expressed in the latter framework without leading to inconsistency. A clustering-quality measure (CQM) is a function that, given a data set and its partition into clusters, returns a non-negative real number representing how strong or conclusive the clustering is. We analyze what clustering-quality measures should look like and introduce a set of requirements (axioms) for such measures. Our axioms capture the principles expressed by Kleinberg’s axioms while retaining consistency. We propose several natural clustering quality measures, all satisfying the proposed axioms. In addition, we analyze the computational complexity of evaluating the quality of a given clustering and show that, for the proposed CQMs, it can be computed in polynomial time. 1</p><p>5 0.1242158 <a title="218-tfidf-5" href="./nips-2008-On_Computational_Power_and_the_Order-Chaos_Phase_Transition_in_Reservoir_Computing.html">160 nips-2008-On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing</a></p>
<p>Author: Benjamin Schrauwen, Lars Buesing, Robert A. Legenstein</p><p>Abstract: Randomly connected recurrent neural circuits have proven to be very powerful models for online computations when a trained memoryless readout function is appended. Such Reservoir Computing (RC) systems are commonly used in two ﬂavors: with analog or binary (spiking) neurons in the recurrent circuits. Previous work showed a fundamental difference between these two incarnations of the RC idea. The performance of a RC system built from binary neurons seems to depend strongly on the network connectivity structure. In networks of analog neurons such dependency has not been observed. In this article we investigate this apparent dichotomy in terms of the in-degree of the circuit nodes. Our analyses based amongst others on the Lyapunov exponent reveal that the phase transition between ordered and chaotic network behavior of binary circuits qualitatively differs from the one in analog circuits. This explains the observed decreased computational performance of binary circuits of high node in-degree. Furthermore, a novel mean-ﬁeld predictor for computational performance is introduced and shown to accurately predict the numerically obtained results. 1</p><p>6 0.11032368 <a title="218-tfidf-6" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>7 0.10046745 <a title="218-tfidf-7" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>8 0.098282665 <a title="218-tfidf-8" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>9 0.084405668 <a title="218-tfidf-9" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>10 0.083911307 <a title="218-tfidf-10" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>11 0.078997366 <a title="218-tfidf-11" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>12 0.078723386 <a title="218-tfidf-12" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>13 0.07571099 <a title="218-tfidf-13" href="./nips-2008-Cyclizing_Clusters_via_Zeta_Function_of_a_Graph.html">55 nips-2008-Cyclizing Clusters via Zeta Function of a Graph</a></p>
<p>14 0.06923987 <a title="218-tfidf-14" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>15 0.062344514 <a title="218-tfidf-15" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>16 0.062129091 <a title="218-tfidf-16" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>17 0.060887475 <a title="218-tfidf-17" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>18 0.060536902 <a title="218-tfidf-18" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>19 0.058582224 <a title="218-tfidf-19" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>20 0.056472387 <a title="218-tfidf-20" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.175), (1, -0.04), (2, -0.04), (3, 0.073), (4, 0.022), (5, -0.045), (6, 0.054), (7, -0.059), (8, 0.017), (9, -0.166), (10, 0.05), (11, -0.002), (12, -0.041), (13, 0.112), (14, -0.148), (15, 0.104), (16, -0.145), (17, 0.058), (18, 0.139), (19, -0.123), (20, -0.042), (21, 0.101), (22, 0.056), (23, 0.131), (24, 0.033), (25, 0.03), (26, -0.038), (27, -0.025), (28, -0.011), (29, 0.084), (30, -0.029), (31, -0.042), (32, 0.012), (33, 0.073), (34, -0.009), (35, -0.064), (36, 0.091), (37, -0.203), (38, 0.008), (39, -0.059), (40, -0.038), (41, 0.114), (42, 0.068), (43, 0.044), (44, 0.134), (45, -0.109), (46, 0.004), (47, 0.029), (48, -0.04), (49, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95427328 <a title="218-lsi-1" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>Author: Ling Huang, Donghui Yan, Nina Taft, Michael I. Jordan</p><p>Abstract: Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a speciﬁcation of permitted loss in clustering performance. 1</p><p>2 0.70949596 <a title="218-lsi-2" href="./nips-2008-Measures_of_Clustering_Quality%3A_A_Working_Set_of_Axioms_for_Clustering.html">132 nips-2008-Measures of Clustering Quality: A Working Set of Axioms for Clustering</a></p>
<p>Author: Shai Ben-David, Margareta Ackerman</p><p>Abstract: Aiming towards the development of a general clustering theory, we discuss abstract axiomatization for clustering. In this respect, we follow up on the work of Kleinberg, ([1]) that showed an impossibility result for such axiomatization. We argue that an impossibility result is not an inherent feature of clustering, but rather, to a large extent, it is an artifact of the speciﬁc formalism used in [1]. As opposed to previous work focusing on clustering functions, we propose to address clustering quality measures as the object to be axiomatized. We show that principles like those formulated in Kleinberg’s axioms can be readily expressed in the latter framework without leading to inconsistency. A clustering-quality measure (CQM) is a function that, given a data set and its partition into clusters, returns a non-negative real number representing how strong or conclusive the clustering is. We analyze what clustering-quality measures should look like and introduce a set of requirements (axioms) for such measures. Our axioms capture the principles expressed by Kleinberg’s axioms while retaining consistency. We propose several natural clustering quality measures, all satisfying the proposed axioms. In addition, we analyze the computational complexity of evaluating the quality of a given clustering and show that, for the proposed CQMs, it can be computed in polynomial time. 1</p><p>3 0.69590491 <a title="218-lsi-3" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>Author: Yair Weiss, Antonio Torralba, Rob Fergus</p><p>Abstract: Semantic hashing[1] seeks compact binary codes of data-points so that the Hamming distance between codewords correlates with semantic similarity. In this paper, we show that the problem of ﬁnding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresholded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds, we show how to eﬃciently calculate the code of a novel datapoint. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes outperform the state-of-the art. 1</p><p>4 0.69471347 <a title="218-lsi-4" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>Author: Matthew Blaschko, Arthur Gretton</p><p>Abstract: We introduce a family of unsupervised algorithms, numerical taxonomy clustering, to simultaneously cluster data, and to learn a taxonomy that encodes the relationship between the clusters. The algorithms work by maximizing the dependence between the taxonomy and the original data. The resulting taxonomy is a more informative visualization of complex data than simple clustering; in addition, taking into account the relations between different clusters is shown to substantially improve the quality of the clustering, when compared with state-ofthe-art algorithms in the literature (both spectral clustering and a previous dependence maximization approach). We demonstrate our algorithm on image and text data. 1</p><p>5 0.53508443 <a title="218-lsi-5" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>Author: Markus Maier, Ulrike V. Luxburg, Matthias Hein</p><p>Abstract: Graph clustering methods such as spectral clustering are deﬁned for general weighted graphs. In machine learning, however, data often is not given in form of a graph, but in terms of similarity (or distance) values between points. In this case, ﬁrst a neighborhood graph is constructed using the similarities between the points and then a graph clustering algorithm is applied to this graph. In this paper we investigate the inﬂuence of the construction of the similarity graph on the clustering results. We ﬁrst study the convergence of graph clustering criteria such as the normalized cut (Ncut) as the sample size tends to inﬁnity. We ﬁnd that the limit expressions are different for different types of graph, for example the r-neighborhood graph or the k-nearest neighbor graph. In plain words: Ncut on a kNN graph does something systematically different than Ncut on an r-neighborhood graph! This ﬁnding shows that graph clustering criteria cannot be studied independently of the kind of graph they are applied to. We also provide examples which show that these differences can be observed for toy and real data already for rather small sample sizes. 1</p><p>6 0.50697517 <a title="218-lsi-6" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>7 0.49981183 <a title="218-lsi-7" href="./nips-2008-Cyclizing_Clusters_via_Zeta_Function_of_a_Graph.html">55 nips-2008-Cyclizing Clusters via Zeta Function of a Graph</a></p>
<p>8 0.4639574 <a title="218-lsi-8" href="./nips-2008-On_Computational_Power_and_the_Order-Chaos_Phase_Transition_in_Reservoir_Computing.html">160 nips-2008-On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing</a></p>
<p>9 0.46198526 <a title="218-lsi-9" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>10 0.42299819 <a title="218-lsi-10" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>11 0.40328848 <a title="218-lsi-11" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>12 0.37927461 <a title="218-lsi-12" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>13 0.37707925 <a title="218-lsi-13" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>14 0.33993202 <a title="218-lsi-14" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>15 0.33802426 <a title="218-lsi-15" href="./nips-2008-Breaking_Audio_CAPTCHAs.html">41 nips-2008-Breaking Audio CAPTCHAs</a></p>
<p>16 0.32446969 <a title="218-lsi-16" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>17 0.31334421 <a title="218-lsi-17" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>18 0.30941328 <a title="218-lsi-18" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>19 0.30087885 <a title="218-lsi-19" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>20 0.294983 <a title="218-lsi-20" href="./nips-2008-Online_Metric_Learning_and_Fast_Similarity_Search.html">168 nips-2008-Online Metric Learning and Fast Similarity Search</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.071), (7, 0.079), (10, 0.075), (12, 0.046), (28, 0.184), (57, 0.044), (59, 0.029), (63, 0.048), (66, 0.011), (71, 0.026), (77, 0.036), (83, 0.037), (90, 0.201)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81549001 <a title="218-lda-1" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>Author: Ling Huang, Donghui Yan, Nina Taft, Michael I. Jordan</p><p>Abstract: Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a speciﬁcation of permitted loss in clustering performance. 1</p><p>2 0.75903231 <a title="218-lda-2" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>Author: Tong Zhang</p><p>Abstract: Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefﬁcients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneﬁcial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory. 1</p><p>3 0.74598926 <a title="218-lda-3" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>Author: Hannes Nickisch, Rolf Pohmann, Bernhard Schölkopf, Matthias Seeger</p><p>Abstract: We show how improved sequences for magnetic resonance imaging can be found through optimization of Bayesian design scores. Combining approximate Bayesian inference and natural image statistics with high-performance numerical computation, we propose the ﬁrst Bayesian experimental design framework for this problem of high relevance to clinical and brain research. Our solution requires large-scale approximate inference for dense, non-Gaussian models. We propose a novel scalable variational inference algorithm, and show how powerful methods of numerical mathematics can be modiﬁed to compute primitives in our framework. Our approach is evaluated on raw data from a 3T MR scanner. 1</p><p>4 0.71846938 <a title="218-lda-4" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>5 0.71519721 <a title="218-lda-5" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>Author: Tony Jebara, Pannagadatta K. Shivaswamy</p><p>Abstract: In classiﬁcation problems, Support Vector Machines maximize the margin of separation between two classes. While the paradigm has been successful, the solution obtained by SVMs is dominated by the directions with large data spread and biased to separate the classes by cutting along large spread directions. This article proposes a novel formulation to overcome such sensitivity and maximizes the margin relative to the spread of the data. The proposed formulation can be eﬃciently solved and experiments on digit datasets show drastic performance improvements over SVMs. 1</p><p>6 0.71415257 <a title="218-lda-6" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>7 0.71310467 <a title="218-lda-7" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>8 0.71208328 <a title="218-lda-8" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>9 0.71157938 <a title="218-lda-9" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>10 0.71133167 <a title="218-lda-10" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>11 0.71108031 <a title="218-lda-11" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>12 0.71085811 <a title="218-lda-12" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>13 0.70992625 <a title="218-lda-13" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>14 0.70931506 <a title="218-lda-14" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>15 0.70867503 <a title="218-lda-15" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>16 0.70852286 <a title="218-lda-16" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>17 0.70823431 <a title="218-lda-17" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>18 0.70789665 <a title="218-lda-18" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>19 0.70768255 <a title="218-lda-19" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>20 0.70746124 <a title="218-lda-20" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
