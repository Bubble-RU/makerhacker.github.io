<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-125" href="#">nips2008-125</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</h1>
<br/><p>Source: <a title="nips-2008-125-pdf" href="http://papers.nips.cc/paper/3403-local-gaussian-process-regression-for-real-time-online-model-learning.pdf">pdf</a></p><p>Author: Duy Nguyen-tuong, Jan R. Peters, Matthias Seeger</p><p>Abstract: Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up standard Gaussian process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Comparisons with other non-parametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and ν-SVR. 1</p><p>Reference: <a title="nips-2008-125-reference" href="../nips2008_reference/nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 , online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. [sent-7, score-0.638]
</p><p>2 Inspired by local learning, we propose a method to speed up standard Gaussian process regression (GPR) with local GP models (LGP). [sent-8, score-0.421]
</p><p>3 The training data is partitioned in local regions, for each an individual GP model is trained. [sent-9, score-0.234]
</p><p>4 The prediction for a query point is performed by weighted estimation using nearby local models. [sent-10, score-0.39]
</p><p>5 The proposed method achieves online learning and prediction in real-time. [sent-12, score-0.226]
</p><p>6 Especially in robot tracking control, only a well-estimated inverse dynamics model can allow both high accuracy and compliant control. [sent-15, score-0.437]
</p><p>7 For most real-time applications, online model learning poses a difﬁcult regression problem due to three constraints, i. [sent-17, score-0.219]
</p><p>8 Here, the true function is approximated with local linear functions covering the relevant state-space and online learning became computationally feasible due to low computational demands of the local projection regression which can be performed in real-time. [sent-27, score-0.554]
</p><p>9 , online learning of inverse dynamics model for model-based 1  robot control. [sent-35, score-0.424]
</p><p>10 The computational cost is then signiﬁcantly reduced due to much smaller number of training examples within a local model. [sent-42, score-0.244]
</p><p>11 The ME performance depends largely on the way of partitioning the training data and the choice of an optimal number of local models for a particular data set [4]. [sent-43, score-0.329]
</p><p>12 , LWPR and GPR, attempting to get as close as possible to the speed of local learning while having a comparable accuracy to Gaussian process regression. [sent-46, score-0.216]
</p><p>13 This results in an approach inspired by [6, 8] using many local GPs in order to obtain a signiﬁcant reduction of the computational cost during both prediction and learning step allowing the application to online learning. [sent-47, score-0.399]
</p><p>14 For partitioning the training data, we use a distance based measure, where the corresponding hyperparameters are optimized by maximizing the marginal likelihood. [sent-48, score-0.175]
</p><p>15 Subsequently, we describe our local Gaussian process models (LGP) approach in Section 3 and discuss how it inherits the advantages of both GPR and LWPR. [sent-50, score-0.205]
</p><p>16 , LWPR [8], standard GPR [1], sparse online Gaussian process regression (OGP) [5] and ν-support vector regression (ν-SVR) [11], respectively. [sent-53, score-0.272]
</p><p>17 Finally, our LGP method is evaluated for an online learning of the inverse dynamics models of real robots for accurate tracking control in Section 5. [sent-54, score-0.494]
</p><p>18 Here, the online learning is demonstrated by rank-one update of the local GP models [9]. [sent-55, score-0.347]
</p><p>19 The tracking task is performed in real-time using model-based control [10]. [sent-56, score-0.179]
</p><p>20 To our best knowledge, it is the ﬁrst time that GPR is successfully used for high-speed online model learning in real time control on a physical robot. [sent-57, score-0.228]
</p><p>21 We present the results on a version of the Barrett WAM showing that with the online learned model using LGP the tracking accuracy is superior compared to state-of-the art model-based methods [10] while remaining fully compliant. [sent-58, score-0.328]
</p><p>22 for k = 1 to M do Compute distance to the k-th local model: wk = exp(−0. [sent-69, score-0.307]
</p><p>23 5(x − ck )T W(x − ck )) Compute local mean using the k-th local model: yk = kT αk ¯ k end for Compute weighted prediction using M local models: M M y = k=1 wk yk / k=1 wk . [sent-70, score-1.149]
</p><p>24 for k = 1 to number of local models do Compute distance to the k-th local model: wk = exp(−0. [sent-72, score-0.488]
</p><p>25 (a) SARCOS arm  Algorithm 1: Partitioning of training data and model learning. [sent-75, score-0.168]
</p><p>26 Reducing this computational cost, we cluster the training data in local regions and, subsequently, train the corresponding GP models on these local clusters. [sent-78, score-0.391]
</p><p>27 The mean prediction for a query point is then made by weighted prediction using the nearby local models in the neighborhood. [sent-79, score-0.491]
</p><p>28 , allocation of new input points and learning of corresponding local models, (ii) prediction for a query point. [sent-82, score-0.384]
</p><p>29 1  Partitioning and Training of Local Models  Clustering input data is efﬁciently performed by considering a distance measure of the input point x to the centers of all local models. [sent-84, score-0.273]
</p><p>30 The distance measure wk is given by the kernel used to learn the local GP models, e. [sent-85, score-0.328]
</p><p>31 , Gaussian kernel 1 T wk = exp − (x − ck ) W (x − ck ) , (4) 2 where ck denotes the center of the k-th local model and W a diagonal matrix represented the kernel width. [sent-87, score-0.669]
</p><p>32 It should be noted, that we use the same kernel width for computing wk as well as for training of all local GP models as given in Section 2. [sent-88, score-0.392]
</p><p>33 During the localization process, a new model with center ck+1 is created, if all distance measures wk fall below a limit value wgen . [sent-91, score-0.32]
</p><p>34 Thus, the number of local models is allowed to increase as the trajectories become more complex. [sent-93, score-0.181]
</p><p>35 Otherwise, if a new point is assigned to a particular k-th model, the center ck is updated as mean of corresponding local 3  data points. [sent-94, score-0.308]
</p><p>36 With the new assigned input point, the inverse covariance matrix of the corresponding local model can be updated. [sent-95, score-0.279]
</p><p>37 The main computational cost of this algorithm is O(N 3 ) for inverting the local covariance matrix, where N presents the number of data points in a local model. [sent-97, score-0.428]
</p><p>38 Furthermore, we can control the complexity by limiting the number of data points in a local model. [sent-98, score-0.268]
</p><p>39 Since the number of local data points increases continuously over time, we can adhere to comply with this limit by deleting old data point as new ones are included. [sent-99, score-0.284]
</p><p>40 The cost for inverting the local covariance matrix can be further reduced, as we need only to update the full inverse matrix once it is computed. [sent-101, score-0.313]
</p><p>41 2  Prediction using Local Models  The prediction for a mean value y is performed using weighted averaging over M local predicˆ tions yk for a query point x [8]. [sent-104, score-0.46]
</p><p>42 The weighted prediction y is then given by y = E{¯k |x} = ¯ ˆ ˆ y M yk p(k|x). [sent-105, score-0.18]
</p><p>43 According to the Bayesian theorem, the probability of the model k given x can be ¯ k=1 M M expressed as p(k|x) = p(k, x)/ k=1 p(k, x) = wk / k=1 wk . [sent-106, score-0.302]
</p><p>44 (5)  The probability p(k|x) can be interpreted as a normalized distance of the query point x to the local model k where the measure metric wk is used as given in Equation (4). [sent-108, score-0.447]
</p><p>45 Thus, each local prediction yk , determined using Equation (3), is additionally weighted by the distance wk between ¯ the corresponding center ck and the query point x. [sent-109, score-0.728]
</p><p>46 The search for M local models can be quickly done by evaluating the distances between the query point x and all model centers ck . [sent-110, score-0.42]
</p><p>47 4  Learning Inverse Dynamics  We have evaluated our algorithm using high-dimensional robot data taken from real robots, e. [sent-112, score-0.152]
</p><p>48 , the 7 degree-of-freedom (DoF) anthropomorphic SARCOS master arm and 7-DoF Barrett whole arm manipulator shown in Figure 1, as well as a physically realistic SL simulation [12]. [sent-114, score-0.2]
</p><p>49 , LWPR, ν-SVR, OGP and standard GPR in the context of approximating inverse robot dynamics. [sent-117, score-0.201]
</p><p>50 For the considered 7 degrees of freedom robot arms, we, thus, have data with 21 input dimensions (for each joint, we have an angle, a velocity and an acceleration) and 7 targets (a torque for each joint). [sent-122, score-0.285]
</p><p>51 We learn the robot dynamics model in this 21-dim space for each DoF separately employing LWPR, ν-SVR, GPR, OGP and LGP, respectively. [sent-123, score-0.215]
</p><p>52 Partitioning of the training examples for LGP can be performed either in the same input space (where the model is learned) or in another space which has to be physically consistent with the approximated function. [sent-124, score-0.165]
</p><p>53 Thus, the partitioning of training data is performed in a 7-dim space (7 joint angles). [sent-126, score-0.172]
</p><p>54 After determining wk for all k local models in the partitioning space, the input point will be assigned to the nearest local model, i. [sent-127, score-0.566]
</p><p>55 , the local model with the maximal value of distance measure wk . [sent-129, score-0.331]
</p><p>56 The error is computed after prediction on the test sets with simulated data from SL Sarcos-model, real robot data from Barrett and SARCOS master arm, respectively. [sent-145, score-0.302]
</p><p>57 , the simulated SARCOS arm in (a), the real SARCOS arm in (b) and the Barrett arm in (c). [sent-151, score-0.225]
</p><p>58 During the prediction on the test set using LGP, we take the most activated local models, i. [sent-153, score-0.225]
</p><p>59 If wgen is too GPR small, a lot of local models will be generated LGP with small number of training points. [sent-157, score-0.312]
</p><p>60 It turns 2 out that these small local models do not perform well in generalization for unknown data. [sent-158, score-0.181]
</p><p>61 1 If wgen is large, the local models become also large which increase the computational complexity. [sent-159, score-0.261]
</p><p>62 Here, the training data are clustered in about 30 local regions ensuring that each local model has a sufﬁcient amount of data points 0 5000 10000 15000 for high accuracy (in practice, roughly a hunNr. [sent-160, score-0.469]
</p><p>63 of Training Points Figure 3: Average time in millisecond needed for dred data points for each local model sufﬁce) prediction of 1 query point. [sent-161, score-0.406]
</p><p>64 This small number of training inputs methods such as standard GPR and ν-SVR, local enables a fast training for each local model, i. [sent-167, score-0.403]
</p><p>65 05  7 6 5 4 3  Considering the approximation error on the test set shown in Figure 2(a-c), it can be seen that LGP generalizes well using only few local models for prediction. [sent-176, score-0.202]
</p><p>66 The operation of mean-prediction has then the order of O(n) for standard GPR (similarly, for ν-SVR) and O(N M ) for LGP, where n denotes the total number of training points, M number of local models and N number of data points in a local model. [sent-180, score-0.438]
</p><p>67 , matrix inversion), the prediction time is also reduced signiﬁcantly compared to GPR and ν-SVR due to the fact that only a small amount of local models in the vicinity of the query point are needed during prediction for LGP. [sent-184, score-0.507]
</p><p>68 Thus, the prediction time can be controlled by the number of local models. [sent-185, score-0.225]
</p><p>69 A large number of local models may provide a smooth prediction but on the other hand increases the time complexity. [sent-186, score-0.265]
</p><p>70 Subsequently, using the trained models we compute the average time needed to make a prediction for a query point for all 7 DoF. [sent-189, score-0.24]
</p><p>71 For LGP, we take a limited number of local models in the vicinity for prediction, e. [sent-190, score-0.203]
</p><p>72 Since our control system requires a minimal prediction rate at 100 Hz (10 ms) in order to ensure system stability, data sets with more than 15000 points are not applicable for standard GPR or ν-SVR due to high computation demands for prediction. [sent-193, score-0.26]
</p><p>73 As approach to deleting and inserting data points, we can use the information gain of the corresponding local model as a principled measure. [sent-198, score-0.219]
</p><p>74 , more than 5000 training examples), LGP reduces the prediction cost considerably while keeping a good learning performance. [sent-202, score-0.167]
</p><p>75 5  qd ˙ qd ¨ qd  Application in Model-based Robot Control  Local GP  u  +  Robot  +  + Kp  ˙ q  q  Kv +  −  In this section, ﬁrst, we use the inverse dynamics models learned in Section 4. [sent-203, score-0.473]
</p><p>76 1 for a modelbased tracking control task [10] in the setting shown in Figure 4. [sent-204, score-0.156]
</p><p>77 Here, the learned model of the robot is applied for an online prediction of the feedforward torques uFF given the desired trajectory [qd , qd , qd ]. [sent-205, score-0.71]
</p><p>78 Subsequently, ˙ ¨ the model approximated by LGP is used for an online learning performance. [sent-206, score-0.19]
</p><p>79 Demonstrating the online learning, the local GP models are adapted in real-time using rank-one update. [sent-207, score-0.323]
</p><p>80 As shown in Figure 4, the controller command u consists of the feedforward part uFF and the feedback part uFB = Kp e + Kv e, where e = ˙ qd − q denotes the tracking error and Kp , Kv Figure 4: Schematic showing model-based robot position-gain and velocity-gain, respectively. [sent-208, score-0.37]
</p><p>81 The learned dynamics model can be upDuring the control experiment we set the gains dated online using LGP. [sent-210, score-0.324]
</p><p>82 As a result, the learned model has a stronger effect on computing the predicted torque uFF and, hence, a better learning performance of each method results in a lower tracking error. [sent-212, score-0.205]
</p><p>83 +  −  For comparison with the learned models, we also compute the feedforward torque using rigid-body (RB) formulation which is a common approach in robot control [10]. [sent-213, score-0.314]
</p><p>84 01  0  1  2 3 4 5 6 Degree of Freedom  0  7  (a) Tracking Error on Barrett without online learning  1  2 3 4 5 6 Degree of Freedom  7  (b) Tracking Error after LGP online learning on Barrett  Figure 5: (a) Tracking error as RMSE on test trajectory for each DoF with Barrett WAM. [sent-227, score-0.339]
</p><p>85 The model uncertainty is reduced with online learning using LGP. [sent-229, score-0.186]
</p><p>86 With online learning, LGP is able to outperform ofﬂine learned models using standard GPR for test trajectories. [sent-230, score-0.221]
</p><p>87 As desired trajectory, we generate a test trajectory which is similar to the one used for learning the inverse dynamics models in Section 4. [sent-232, score-0.198]
</p><p>88 Figure 5 (a) shows the tracking errors on test trajectory for 7 DoFs, where the error is computed as root mean squared error (RMSE). [sent-234, score-0.17]
</p><p>89 Since it is not possible to learn the complete state space using a single data set, online learning is necessary. [sent-244, score-0.16]
</p><p>90 1  Online Learning of Inverse Dynamics Models with LGP  The ability of online adaptation of the learned inverse dynamics models with LGP is shown by the rank-one update of the local models which has a complexity of O(n2 ) [9]. [sent-246, score-0.55]
</p><p>91 Since the number of training examples in each local model is limited (500 points in average), the update procedure is fast enough for real-time application. [sent-247, score-0.306]
</p><p>92 For online learning the models are updated as shown in Figure 4. [sent-248, score-0.182]
</p><p>93 For doing so, we regularly sample the joint torques u and the corresponding robot trajectories [q, q, q] online. [sent-249, score-0.195]
</p><p>94 For the time being, as a new point is inserted we randomly delete another data ˙ ¨ point from the local model if the maximal number of data point is reached. [sent-250, score-0.273]
</p><p>95 Figure 5 (b) shows the tracking error after online learning with LGP. [sent-252, score-0.257]
</p><p>96 It can be seen that the errors for each DoF are signiﬁcantly reduced with online LGP compared to the ones with ofﬂine learned models. [sent-253, score-0.201]
</p><p>97 6  Conclusion  We combine with LGP the fast computation of local regression with more accurate regression methods while having little tuning efforts. [sent-255, score-0.285]
</p><p>98 The reducing cost allows LGP for model online learning which is necessary in oder to generalize the model for all trajectories. [sent-257, score-0.222]
</p><p>99 Model-based tracking control using online learned model achieves superior control performance compared to the state-of-the-art method as well as ofﬂine learned model for unknown trajectories. [sent-258, score-0.486]
</p><p>100 Seeger, “Computed torque control with nonparametric regression models,” Proceedings of the 2008 American Control Conference (ACC 2008), 2008. [sent-335, score-0.163]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lgp', 0.609), ('gpr', 0.477), ('lwpr', 0.238), ('barrett', 0.154), ('online', 0.142), ('local', 0.141), ('wk', 0.139), ('robot', 0.134), ('ogp', 0.128), ('sarcos', 0.126), ('ck', 0.099), ('nmse', 0.096), ('tracking', 0.094), ('query', 0.092), ('gp', 0.092), ('qd', 0.09), ('prediction', 0.084), ('wgen', 0.08), ('arm', 0.075), ('dof', 0.07), ('yk', 0.07), ('inverse', 0.067), ('svr', 0.064), ('wam', 0.064), ('xnew', 0.064), ('control', 0.062), ('partitioning', 0.061), ('dynamics', 0.057), ('regression', 0.053), ('sl', 0.052), ('training', 0.051), ('dofs', 0.048), ('uff', 0.048), ('torque', 0.048), ('points', 0.047), ('rmse', 0.045), ('sgp', 0.042), ('torques', 0.042), ('kt', 0.042), ('freedom', 0.042), ('models', 0.04), ('learned', 0.039), ('subsequently', 0.037), ('deleting', 0.036), ('kp', 0.036), ('kv', 0.036), ('seeger', 0.035), ('trajectory', 0.034), ('arms', 0.032), ('robots', 0.032), ('compliant', 0.032), ('ynew', 0.032), ('cost', 0.032), ('schaal', 0.031), ('feedforward', 0.031), ('ine', 0.031), ('gaussian', 0.03), ('demands', 0.03), ('xq', 0.03), ('accuracy', 0.029), ('master', 0.027), ('distance', 0.027), ('covariance', 0.027), ('center', 0.026), ('offline', 0.026), ('knew', 0.026), ('weighted', 0.026), ('update', 0.024), ('insertion', 0.024), ('localization', 0.024), ('xp', 0.024), ('model', 0.024), ('process', 0.024), ('approximated', 0.024), ('point', 0.024), ('performed', 0.023), ('targets', 0.023), ('physically', 0.023), ('experts', 0.022), ('rasmussen', 0.022), ('speed', 0.022), ('vijayakumar', 0.022), ('vicinity', 0.022), ('inverting', 0.022), ('kernel', 0.021), ('deletion', 0.021), ('error', 0.021), ('acceleration', 0.02), ('subsample', 0.02), ('input', 0.02), ('reduced', 0.02), ('joint', 0.019), ('degree', 0.019), ('fast', 0.019), ('computation', 0.019), ('competitive', 0.019), ('hyperparameters', 0.018), ('data', 0.018), ('maximizing', 0.018), ('mixtures', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="125-tfidf-1" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>Author: Duy Nguyen-tuong, Jan R. Peters, Matthias Seeger</p><p>Abstract: Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up standard Gaussian process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Comparisons with other non-parametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and ν-SVR. 1</p><p>2 0.12898441 <a title="125-tfidf-2" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>Author: Christopher Williams, Stefan Klanke, Sethu Vijayakumar, Kian M. Chai</p><p>Abstract: The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneﬁcial to be able to learn this function for adaptive control. A robotic manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem. By placing independent Gaussian process priors over the latent functions of the inverse dynamics, we obtain a multi-task Gaussian process prior for handling multiple loads, where the inter-task similarity depends on the underlying inertial parameters. Experiments demonstrate that this multi-task formulation is effective in sharing information among the various loads, and generally improves performance over either learning only on single tasks or pooling the data over all tasks. 1</p><p>3 0.11000883 <a title="125-tfidf-3" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>Author: Jo-anne Ting, Mrinal Kalakrishnan, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise varies spatially. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. We introduce a Bayesian formulation of nonparametric regression that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efﬁcient, requires no sampling, automatically rejects outliers and has only one prior to be speciﬁed. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law. 1</p><p>4 0.07534188 <a title="125-tfidf-4" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>Author: Kevyn Collins-thompson</p><p>Abstract: Query expansion is a long-studied approach for improving retrieval effectiveness by enhancing the user’s original query with additional related words. Current algorithms for automatic query expansion can often improve retrieval accuracy on average, but are not robust: that is, they are highly unstable and have poor worst-case performance for individual queries. To address this problem, we introduce a novel formulation of query expansion as a convex optimization problem over a word graph. The model combines initial weights from a baseline feedback algorithm with edge weights based on word similarity, and integrates simple constraints to enforce set-based criteria such as aspect balance, aspect coverage, and term centrality. Results across multiple standard test collections show consistent and signiﬁcant reductions in the number and magnitude of expansion failures, while retaining the strong positive gains of the baseline algorithm. Our approach does not assume a particular retrieval model, making it applicable to a broad class of existing expansion algorithms. 1</p><p>5 0.070146449 <a title="125-tfidf-5" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>Author: Ben Calderhead, Mark Girolami, Neil D. Lawrence</p><p>Abstract: Identiﬁcation and comparison of nonlinear dynamical system models using noisy and sparse experimental data is a vital task in many ﬁelds, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced. We present an accelerated sampling procedure which enables Bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of Gaussian processes (GP). Our method involves GP regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time. We demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods. 1</p><p>6 0.069376074 <a title="125-tfidf-6" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>7 0.066610232 <a title="125-tfidf-7" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>8 0.064995848 <a title="125-tfidf-8" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>9 0.06491711 <a title="125-tfidf-9" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>10 0.058043461 <a title="125-tfidf-10" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>11 0.056548361 <a title="125-tfidf-11" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>12 0.056117464 <a title="125-tfidf-12" href="./nips-2008-Algorithms_for_Infinitely_Many-Armed_Bandits.html">17 nips-2008-Algorithms for Infinitely Many-Armed Bandits</a></p>
<p>13 0.055002175 <a title="125-tfidf-13" href="./nips-2008-Mortal_Multi-Armed_Bandits.html">140 nips-2008-Mortal Multi-Armed Bandits</a></p>
<p>14 0.053546693 <a title="125-tfidf-14" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>15 0.052075546 <a title="125-tfidf-15" href="./nips-2008-Online_Metric_Learning_and_Fast_Similarity_Search.html">168 nips-2008-Online Metric Learning and Fast Similarity Search</a></p>
<p>16 0.048076738 <a title="125-tfidf-16" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>17 0.047321174 <a title="125-tfidf-17" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>18 0.04713301 <a title="125-tfidf-18" href="./nips-2008-Predictive_Indexing_for_Fast_Search.html">184 nips-2008-Predictive Indexing for Fast Search</a></p>
<p>19 0.046889476 <a title="125-tfidf-19" href="./nips-2008-An_Online_Algorithm_for_Maximizing_Submodular_Functions.html">22 nips-2008-An Online Algorithm for Maximizing Submodular Functions</a></p>
<p>20 0.044727325 <a title="125-tfidf-20" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.139), (1, 0.044), (2, -0.005), (3, 0.005), (4, 0.002), (5, -0.067), (6, -0.007), (7, 0.074), (8, 0.025), (9, 0.031), (10, 0.035), (11, 0.042), (12, 0.021), (13, 0.051), (14, 0.136), (15, -0.076), (16, -0.149), (17, 0.107), (18, 0.06), (19, 0.013), (20, 0.085), (21, -0.059), (22, 0.041), (23, -0.015), (24, 0.098), (25, 0.053), (26, 0.089), (27, 0.037), (28, -0.048), (29, -0.042), (30, -0.027), (31, 0.068), (32, 0.025), (33, 0.025), (34, 0.051), (35, -0.066), (36, 0.046), (37, 0.07), (38, 0.069), (39, 0.044), (40, 0.0), (41, 0.043), (42, -0.01), (43, 0.038), (44, -0.127), (45, -0.042), (46, -0.053), (47, -0.098), (48, -0.032), (49, 0.122)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90732777 <a title="125-lsi-1" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>Author: Duy Nguyen-tuong, Jan R. Peters, Matthias Seeger</p><p>Abstract: Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up standard Gaussian process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Comparisons with other non-parametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and ν-SVR. 1</p><p>2 0.6697697 <a title="125-lsi-2" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>Author: Christopher Williams, Stefan Klanke, Sethu Vijayakumar, Kian M. Chai</p><p>Abstract: The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneﬁcial to be able to learn this function for adaptive control. A robotic manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem. By placing independent Gaussian process priors over the latent functions of the inverse dynamics, we obtain a multi-task Gaussian process prior for handling multiple loads, where the inter-task similarity depends on the underlying inertial parameters. Experiments demonstrate that this multi-task formulation is effective in sharing information among the various loads, and generally improves performance over either learning only on single tasks or pooling the data over all tasks. 1</p><p>3 0.66456199 <a title="125-lsi-3" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>Author: Jo-anne Ting, Mrinal Kalakrishnan, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise varies spatially. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. We introduce a Bayesian formulation of nonparametric regression that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efﬁcient, requires no sampling, automatically rejects outliers and has only one prior to be speciﬁed. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law. 1</p><p>4 0.55685759 <a title="125-lsi-4" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>Author: Mauricio Alvarez, Neil D. Lawrence</p><p>Abstract: We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network. 1</p><p>5 0.5451318 <a title="125-lsi-5" href="./nips-2008-Online_Models_for_Content_Optimization.html">169 nips-2008-Online Models for Content Optimization</a></p>
<p>Author: Deepak Agarwal, Bee-chung Chen, Pradheep Elango, Nitin Motgi, Seung-taek Park, Raghu Ramakrishnan, Scott Roy, Joe Zachariah</p><p>Abstract: We describe a new content publishing system that selects articles to serve to a user, choosing from an editorially programmed pool that is frequently refreshed. It is now deployed on a major Yahoo! portal, and selects articles to serve to hundreds of millions of user visits per day, signiﬁcantly increasing the number of user clicks over the original manual approach, in which editors periodically selected articles to display. Some of the challenges we face include a dynamic content pool, short article lifetimes, non-stationary click-through rates, and extremely high trafﬁc volumes. The fundamental problem we must solve is to quickly identify which items are popular (perhaps within different user segments), and to exploit them while they remain current. We must also explore the underlying pool constantly to identify promising alternatives, quickly discarding poor performers. Our approach is based on tracking per article performance in near real time through online models. We describe the characteristics and constraints of our application setting, discuss our design choices, and show the importance and effectiveness of coupling online models with a randomization procedure. We discuss the challenges encountered in a production online content-publishing environment and highlight issues that deserve careful attention. Our analysis of this application also suggests a number of future research avenues. 1</p><p>6 0.4953562 <a title="125-lsi-6" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>7 0.48657066 <a title="125-lsi-7" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>8 0.46670145 <a title="125-lsi-8" href="./nips-2008-An_Online_Algorithm_for_Maximizing_Submodular_Functions.html">22 nips-2008-An Online Algorithm for Maximizing Submodular Functions</a></p>
<p>9 0.45958641 <a title="125-lsi-9" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>10 0.45110014 <a title="125-lsi-10" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>11 0.44334126 <a title="125-lsi-11" href="./nips-2008-Online_Metric_Learning_and_Fast_Similarity_Search.html">168 nips-2008-Online Metric Learning and Fast Similarity Search</a></p>
<p>12 0.43107647 <a title="125-lsi-12" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>13 0.42737031 <a title="125-lsi-13" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>14 0.40069941 <a title="125-lsi-14" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>15 0.38978913 <a title="125-lsi-15" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>16 0.3783195 <a title="125-lsi-16" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>17 0.36580914 <a title="125-lsi-17" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>18 0.3547416 <a title="125-lsi-18" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>19 0.34580925 <a title="125-lsi-19" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>20 0.34178147 <a title="125-lsi-20" href="./nips-2008-Fast_High-dimensional_Kernel_Summations_Using_the_Monte_Carlo_Multipole_Method.html">83 nips-2008-Fast High-dimensional Kernel Summations Using the Monte Carlo Multipole Method</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.063), (7, 0.078), (12, 0.035), (15, 0.013), (28, 0.156), (34, 0.314), (47, 0.021), (57, 0.046), (63, 0.032), (71, 0.013), (77, 0.05), (83, 0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.69112641 <a title="125-lda-1" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>Author: Duy Nguyen-tuong, Jan R. Peters, Matthias Seeger</p><p>Abstract: Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up standard Gaussian process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Comparisons with other non-parametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and ν-SVR. 1</p><p>2 0.66039699 <a title="125-lda-2" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>Author: David Sontag, Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: We propose a new class of consistency constraints for Linear Programming (LP) relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and signiﬁcantly faster than a method that does not use partitioning. 1</p><p>3 0.64144927 <a title="125-lda-3" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>4 0.54950982 <a title="125-lda-4" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>5 0.54740101 <a title="125-lda-5" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>6 0.547001 <a title="125-lda-6" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>7 0.5462541 <a title="125-lda-7" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>8 0.54423279 <a title="125-lda-8" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>9 0.54262829 <a title="125-lda-9" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>10 0.54221785 <a title="125-lda-10" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>11 0.54198527 <a title="125-lda-11" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>12 0.54187608 <a title="125-lda-12" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>13 0.54164636 <a title="125-lda-13" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>14 0.54122567 <a title="125-lda-14" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>15 0.54059768 <a title="125-lda-15" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>16 0.53904831 <a title="125-lda-16" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>17 0.53879333 <a title="125-lda-17" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>18 0.53860956 <a title="125-lda-18" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>19 0.53834498 <a title="125-lda-19" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>20 0.53827536 <a title="125-lda-20" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
