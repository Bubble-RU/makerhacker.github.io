<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>141 nips-2008-Multi-Agent Filtering with Infinitely Nested Beliefs</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-141" href="#">nips2008-141</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>141 nips-2008-Multi-Agent Filtering with Infinitely Nested Beliefs</h1>
<br/><p>Source: <a title="nips-2008-141-pdf" href="http://papers.nips.cc/paper/3459-multi-agent-filtering-with-infinitely-nested-beliefs.pdf">pdf</a></p><p>Author: Luke Zettlemoyer, Brian Milch, Leslie P. Kaelbling</p><p>Abstract: In partially observable worlds with many agents, nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents. The multi-agent ﬁltering problem is to efﬁciently represent and update these beliefs through time as the agents act in the world. In this paper, we formally deﬁne an inﬁnite sequence of nested beliefs about the state of the world at the current time t, and present a ﬁltering algorithm that maintains a ﬁnite representation which can be used to generate these beliefs. In some cases, this representation can be updated exactly in constant time; we also present a simple approximation scheme to compact beliefs if they become too complex. In experiments, we demonstrate efﬁcient ﬁltering in a range of multi-agent domains. 1</p><p>Reference: <a title="nips-2008-141-reference" href="../nips2008_reference/nips-2008-Multi-Agent_Filtering_with_Infinitely_Nested_Beliefs_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In partially observable worlds with many agents, nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents. [sent-8, score-1.48]
</p><p>2 The multi-agent ﬁltering problem is to efﬁciently represent and update these beliefs through time as the agents act in the world. [sent-9, score-0.721]
</p><p>3 In this paper, we formally deﬁne an inﬁnite sequence of nested beliefs about the state of the world at the current time t, and present a ﬁltering algorithm that maintains a ﬁnite representation which can be used to generate these beliefs. [sent-10, score-0.847]
</p><p>4 1  Introduction  The existence of nested beliefs is one of the deﬁning characteristics of a multi-agent world. [sent-13, score-0.541]
</p><p>5 As an agent acts, it often needs to reason about what other agents believe. [sent-14, score-0.745]
</p><p>6 A poker agent must think about what cards other players might have — and what cards they might think it has — in order to bet effectively. [sent-16, score-0.498]
</p><p>7 In this paper, we assume a cooperative setting where all the agents have predetermined, commonly-known policies expressed as functions of their beliefs; we focus on the problem of efﬁcient belief update, or ﬁltering. [sent-17, score-0.552]
</p><p>8 Since each agent does not get to see the others’ observations and actions, there is a natural notion of nested beliefs. [sent-20, score-0.628]
</p><p>9 Given its observations and actions, an agent can reason not only about the state of the external world, but also about the other agents’ observations and actions. [sent-21, score-0.521]
</p><p>10 The multi-agent ﬁltering problem is to efﬁciently represent and update these nested beliefs through time. [sent-24, score-0.541]
</p><p>11 In general, an agent’s beliefs depend on its entire history of actions and observations. [sent-25, score-0.538]
</p><p>12 Instead, we maintain a belief state that is sufﬁcient for predicting future beliefs and can be approximated to achieve constant-time belief updates. [sent-28, score-0.62]
</p><p>13 We begin by deﬁning an inﬁnite sequence of nested beliefs about the current state st , and showing that it is sufﬁcient for predicting future beliefs. [sent-29, score-0.718]
</p><p>14 That approach removes the need for the agents to perform any kind of ﬁltering, but requires the speciﬁcation of some particular class of policies that return actions for arbitrarily long histories. [sent-36, score-0.559]
</p><p>15 Their approach maintains ﬁnitely nested beliefs that are derived from a world model as well as hand-speciﬁed models of how each agent reasons about the other agents. [sent-39, score-1.13]
</p><p>16 There has been signiﬁcant work on inﬁnitely nested beliefs in game theory, where Brandenburger and Dekel [2] introduced the notion of an inﬁnite sequence of ﬁnitely nested beliefs. [sent-42, score-0.779]
</p><p>17 However, they do not describe any method for computing these beliefs from a world model or updating them over time. [sent-43, score-0.476]
</p><p>18 However, these algorithms have not addressed the fact that as agents interact with the world over time, the set of observation sequences they could have received (and possibly the set of beliefs they could arrive at) grows exponentially. [sent-47, score-0.869]
</p><p>19 1  Partially observable worlds with many agents  We will perform ﬁltering given a multi-agent, decision-theoretic model for acting in a partially observable world. [sent-51, score-0.473]
</p><p>20 Each agent j has a ﬁnite set of observations Oj that it can receive and a ﬁnite set of actions Aj that it can execute. [sent-54, score-0.568]
</p><p>21 For example, aj ∈ Aj is the action for agent j at time t; at = ai , . [sent-56, score-0.569]
</p><p>22 , aj ) is a sequence of actions for agent j at time steps 0 . [sent-62, score-0.667]
</p><p>23 For each agent j, observations are generated from a distribution p(oj |st , at−1 ) conditioned on the current t state and the previous joint action. [sent-67, score-0.485]
</p><p>24 Each agent j sees only its own actions and observations. [sent-68, score-0.515]
</p><p>25 To j record this information, it is useful to deﬁne a history hj = (aj 0:t−1 , o1:t ) for agent j at time t. [sent-69, score-0.765]
</p><p>26 A 0:t policy is a distribution π j (aj |hj ) over the actions agent j will take given this history. [sent-70, score-0.515]
</p><p>27 2  The nested ﬁltering problem  In this section, we describe how to compute inﬁnitely nested beliefs about the state at time t. [sent-75, score-0.799]
</p><p>28 Finally, we show that the current nested belief for an agent i contains all of the information required to compute future beliefs. [sent-77, score-0.676]
</p><p>29 For example, h−i and π −i are tuples of histories and policies for all agents k = i. [sent-79, score-0.614]
</p><p>30 0:t We deﬁne inﬁnitely nested beliefs by presenting an inﬁnite sequence of ﬁnitely nested beliefs. [sent-80, score-0.779]
</p><p>31 For each agent i and nesting level n, the belief function B i,n : hi → bi,n maps the agent’s history to its t 0:t nth-level beliefs at time t. [sent-81, score-1.071]
</p><p>32 Agent i’s ﬁrst-level belief function B i,1 (hi ) returns a joint distribution on st and the zeroth-level 0:t beliefs of all the other agents (what the other agents believe about the state of the world). [sent-84, score-1.287]
</p><p>33 Again, these beliefs are computed by summing over histories for the other agents that lead to the appropriate level n − 1 beliefs: B i,n (hi ) 0:t  =  p(st , b−i,n−1 |hi ) 0:t t  ∝  P  s0:t−1 ,h−i 0:t  p(s0:t , h0:t )δ(b−i,n−1 , B −i,n−1 (h−i )). [sent-89, score-0.859]
</p><p>34 There are only ﬁnitely many 0:t beliefs each agent k could hold at time t — each arising from one of the possible histories hk . [sent-91, score-1.017]
</p><p>35 0:t Deﬁne bi,∗ = B i,∗ (hi ) to be the inﬁnite sequence of nested beliefs generated by computing t 0:t B i,n (hi ) for n = 0, 1, . [sent-92, score-0.592]
</p><p>36 We can think of bi,∗ as a belief state for agent i, although not one t 0:t that can be used directly by a ﬁltering algorithm. [sent-96, score-0.55]
</p><p>37 Under this assumption, bi,∗ is a sufﬁcient statistic t for predicting future beliefs in the following sense: Proposition 1 In a model with policies π j (aj |bj,∗ ) for each agent j, there exists a belief estimation t t i i i i,∗ i i i i i i,∗ i function BE s. [sent-99, score-0.952]
</p><p>38 Agent l (the tiger listener) can hear the tiger roar, which is a noisy indication of its current location, but cannot open the doors. [sent-110, score-0.811]
</p><p>39 To facilitate communication, agent l has two actions, signal left and signal right, which each produce a unique observation for agent d. [sent-112, score-0.874]
</p><p>40 When a door is opened, the world resets and the tiger is placed behind a randomly chosen door. [sent-113, score-0.606]
</p><p>41 To act optimally, agent l must listen to the tiger’s roars until it is conﬁdent about the tiger’s location and then send the appropriate signal to agent d. [sent-114, score-0.952]
</p><p>42 0  Figure 1: Deterministic policies for the tiger world that depend on each agent’s beliefs about the physical state, where the tiger can be on the left (T L) or the right (T R). [sent-125, score-1.359]
</p><p>43 The tiger listener, agent l, will signal left (SL) or right (SR) if it conﬁdent of the tiger’s location. [sent-126, score-0.806]
</p><p>44 The door opener, agent d, will open the appropriate door when it is conﬁdent about the tiger’s location. [sent-127, score-0.561]
</p><p>45 Otherwise both agents listen (to the tiger or for a signal). [sent-128, score-0.751]
</p><p>46 1 shows a pair of policies that achieve this desired interaction and depend only on each agent’s level-zero beliefs about the state of the world. [sent-131, score-0.507]
</p><p>47 However, as we will see, the agents cannot maintain their level-zero beliefs in isolation. [sent-132, score-0.748]
</p><p>48 To correctly update these beliefs, each agent must reason about the unseen actions and observations of the other agent. [sent-133, score-0.572]
</p><p>49 Consider the beliefs that each agent must maintain to execute its policies during a typical scenario. [sent-134, score-0.965]
</p><p>50 Initially, both agents have uniform beliefs about the location of the tiger. [sent-136, score-0.715]
</p><p>51 However, it maintains a representation of the possible beliefs for agent l and knows that l is receiving observations that correlate with the state of the tiger. [sent-138, score-0.925]
</p><p>52 In this case, the most likely outcome is that agent l will hear enough roars on the left to do a “signal left” action. [sent-139, score-0.482]
</p><p>53 Because agent d has maintained the correspondence between the true state and agent l’s beliefs, it can now infer that the tiger is more likely to be on the left (it is unlikely that l could have come to believe the tiger was on the left if that were not true). [sent-141, score-1.628]
</p><p>54 This inference makes agent d conﬁdent enough about the tiger’s location to open the right door and reset the world. [sent-142, score-0.544]
</p><p>55 Agent l must also represent agent d’s beliefs, because it never receives any observations that indicate what actions agent d is taking. [sent-143, score-0.977]
</p><p>56 It must track agent d’s belief updates to know that d will wait for a signal and then immediately open a door. [sent-144, score-0.594]
</p><p>57 Even in this simple tiger world, we see a complicated reasoning pattern: the agents must track each others’ beliefs. [sent-146, score-0.792]
</p><p>58 To update its belief about the external world, each agent must infer what actions the other agent has taken, which requires maintaining that agent’s beliefs about the world. [sent-147, score-1.402]
</p><p>59 Continuing this reasoning to deeper levels leads to the inﬁnitely nested beliefs deﬁned in Sec. [sent-149, score-0.572]
</p><p>60 This algorithm is applicable in the cooperative setting where there are commonly known policies π j (aj |bj,∗ ) for each agent j. [sent-155, score-0.533]
</p><p>61 The SDS ﬁlter deals with two kinds of sequences: histories hj = 0:t j (aj 0:t−1 , o1:t ) and trajectories x0:t = (s0:t , a0:t−1 ). [sent-158, score-0.514]
</p><p>62 A history represents what agent j knows before acting at time t; a trajectory is a trace of the states and joint actions through time t. [sent-159, score-0.758]
</p><p>63 The ﬁlter for agent i maintains the following sequence sets: a set X of trajectories that might have occurred so far, and for each agent j (including i itself), a set H j of possible histories. [sent-160, score-1.013]
</p><p>64 The SDS ﬁlter maintains belief information in the form of sequence distributions αj (x0:t |hj ) = p(x0:t |hj ) and 0:t 0:t β j (hj |x0:t ) = p(hj |x0:t ) for all agents j, histories hj ∈ H j , and trajectories x0:t ∈ X. [sent-162, score-1.077]
</p><p>65 2 The 0:t 0:t 0:t αj distributions represent what agent j would believe about the possible sequences of states and other agents’ actions given hj . [sent-163, score-0.893]
</p><p>66 To compute beliefs about current and future states, it sufﬁces to maintain the sequence distributions αj and β j deﬁned above, along with the ﬁnal state st in each trajectory. [sent-175, score-0.611]
</p><p>67 In the rest of this section, we ﬁrst show how the sequence distributions can be used to compute nested beliefs of arbitrary depth. [sent-178, score-0.618]
</p><p>68 At level n, we perform the same outer sum, but for each trajectory we sum the probabilities of the histories for agents k = j that would lead to the beliefs we are interested in. [sent-184, score-0.892]
</p><p>69 Thus, the sequence distributions at time t are sufﬁcient for computing any desired element of the inﬁnite belief sequence B j,∗ (hj ) for any agent j and history hj . [sent-185, score-0.977]
</p><p>70 In practice, only some of the entries in B k,∗ (hk ) will be needed to compute k’s action; for example, in the tiger world, the policies are 0:t−1 functions of the zero-level beliefs. [sent-190, score-0.487]
</p><p>71 Returning to the example tiger world, we can see that maintaining these sequence distributions will allow us to achieve the desired interactions described in Sec. [sent-194, score-0.478]
</p><p>72 For example, when the door opener receives a “signal left” observation, it will infer that the tiger is on the left because it has done the reasoning in Eq. [sent-197, score-0.509]
</p><p>73 6 and determined that, with high probability, the trajectories that would have led the tiger listener to take this action are the ones where the tiger is actually on the left. [sent-198, score-0.923]
</p><p>74 0:t 0:t (c) Reset when marginal of st is common knowledge: • If ∀j, k, hj ∈ H j , hk , ∈ H k , st . [sent-226, score-0.489]
</p><p>75 At all times t, the ﬁlter maintains sequence sets X and H j , for all agents j, along with the sequence distributions αj and β j for all agents j. [sent-231, score-0.87]
</p><p>76 Agent i’s actual observed history is marked as a distinguished element hi ∈ H i and used to compute its beliefs B i,∗ (hi ). [sent-232, score-0.536]
</p><p>77 The ﬁlter is initialized with empty histories for each agent and trajectories with single states that are distributed according to the prior. [sent-237, score-0.7]
</p><p>78 At each time t, Step 1 extends the sequence sets, computes the sequence distributions, and records agent i’s history. [sent-238, score-0.534]
</p><p>79 2 and 3 still produce the correct nested beliefs at time t. [sent-241, score-0.568]
</p><p>80 Step 2(a) removes trajectories and histories when all the agents agree that they are impossible; there is no reason to track them. [sent-242, score-0.617]
</p><p>81 For example, in the tiger communication world, the policies are such that for the ﬁrst few time steps each agent will always listen (to the tiger or for signals). [sent-243, score-1.365]
</p><p>82 Step 2(b) merges histories for an agent j that lead to the same beliefs. [sent-245, score-0.57]
</p><p>83 For example, as the tiger listener hears roars, any two observation sequences with the same numbers of roars on the left and right provide the same information about the tiger and can be merged. [sent-247, score-0.897]
</p><p>84 For example, when both agents know that a door has been opened, this implies that the world has reset and all previous trajectories and histories can be discarded. [sent-249, score-0.813]
</p><p>85 5) that they enable the SDS ﬁlter to exactly track the tiger communication world extremely efﬁciently. [sent-254, score-0.557]
</p><p>86 The tiger communication world was described in detail in Sec. [sent-270, score-0.535]
</p><p>87 In this scenario, two agents interact in a 3x4 grid world where they must coordinate their actions to move a large box and then independently push two small boxes. [sent-282, score-0.638]
</p><p>88 We implemented policies for each agent that consist of a set of 20 rules specifying actions given its zeroth-level beliefs about the world state. [sent-289, score-1.1]
</p><p>89 3(c) shows the error incurred with various degrees of pruning, in terms of the difference between the estimated zeroth-level beliefs for the agents and the true posterior over physical states given their observations. [sent-299, score-0.752]
</p><p>90 5 Note that in order to accurately maintain each agent’s beliefs about the physical state—which includes the position of the other robot—the ﬁlter must assign accurate probabilities to unobserved actions by the other agent , which depend on its beliefs. [sent-300, score-0.962]
</p><p>91 This is the same reasoning pattern we saw in the tiger world where we are required to maintain inﬁnitely nested beliefs. [sent-301, score-0.772]
</p><p>92 We also ﬁnd that the problem is most challenging around time step ten and becomes easier in the limit, as the world moves towards the absorbing state where both agents have ﬁnished their tasks. [sent-303, score-0.557]
</p><p>93 Initially, it is commonly known that at least one agent has a muddy forehead. [sent-309, score-0.527]
</p><p>94 Because the box-pushing problem is too large for beliefs to be computed exactly, we compare the ﬁlter’s performance to empirical distributions obtained by generating 10,000 sequences of trajectories and histories. [sent-312, score-0.505]
</p><p>95 We group the runs by the history hi ; for all histories that appear at least ten times, we compare the empirical 0:t distribution ˆt of states occurring after that history to the ﬁlter’s computed beliefs ˜i,0 , using the variational b bt P ˆ bt distance V D(ˆt , ˜i,0 ) = s |bt (s) − ˜i,0 (s)|. [sent-313, score-0.88]
</p><p>96 Observations about the muddiness of the other agents are only correct with probability ν, and each agent raises its hand if it assigns probability at least 0. [sent-318, score-0.763]
</p><p>97 With m ≤ n muddy agents, everyone waits m time steps and then all of the muddy agents simultaneously raise their hands. [sent-322, score-0.665]
</p><p>98 The observed behavior is similar to the deterministic case: eventually, all of the m muddy agents raise their hands. [sent-327, score-0.494]
</p><p>99 We introduced the SDS algorithm, which maintains a ﬁnite belief representation that can be used to compute an inﬁnite sequence of nested beliefs about the physical world and the beliefs of other agents. [sent-331, score-1.232]
</p><p>100 At time two, they will both see only one other muddy agent and infer that they are muddy. [sent-410, score-0.554]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('agent', 0.405), ('tiger', 0.378), ('beliefs', 0.354), ('agents', 0.34), ('sds', 0.321), ('hj', 0.259), ('nested', 0.187), ('histories', 0.165), ('muddy', 0.122), ('world', 0.122), ('actions', 0.11), ('policies', 0.109), ('hi', 0.108), ('ltering', 0.101), ('trajectories', 0.09), ('belief', 0.084), ('lter', 0.083), ('st', 0.082), ('history', 0.074), ('aj', 0.074), ('door', 0.067), ('hk', 0.066), ('nitely', 0.065), ('maintains', 0.062), ('pruning', 0.06), ('maintain', 0.054), ('sequence', 0.051), ('listener', 0.044), ('roars', 0.044), ('state', 0.044), ('states', 0.04), ('dent', 0.039), ('ot', 0.039), ('observations', 0.036), ('observable', 0.036), ('filtering', 0.036), ('sequences', 0.035), ('communication', 0.035), ('bi', 0.034), ('hear', 0.033), ('kripke', 0.033), ('listen', 0.033), ('opener', 0.033), ('action', 0.033), ('trajectory', 0.033), ('raise', 0.032), ('reasoning', 0.031), ('ai', 0.03), ('oj', 0.029), ('reset', 0.029), ('time', 0.027), ('hansen', 0.027), ('decentralized', 0.027), ('fagin', 0.027), ('planning', 0.026), ('distributions', 0.026), ('returns', 0.025), ('bt', 0.024), ('worlds', 0.024), ('knows', 0.024), ('step', 0.024), ('nite', 0.023), ('move', 0.023), ('signal', 0.023), ('maintaining', 0.023), ('oi', 0.023), ('open', 0.022), ('box', 0.022), ('brandenburger', 0.022), ('doors', 0.022), ('epistemic', 0.022), ('halpern', 0.022), ('posgs', 0.022), ('waits', 0.022), ('track', 0.022), ('execute', 0.022), ('location', 0.021), ('must', 0.021), ('behind', 0.02), ('cooperative', 0.019), ('gmytrasiewicz', 0.019), ('milch', 0.019), ('nesting', 0.019), ('opened', 0.019), ('cards', 0.019), ('forehead', 0.019), ('puzzle', 0.019), ('resets', 0.019), ('partially', 0.019), ('acting', 0.018), ('physical', 0.018), ('believe', 0.018), ('drop', 0.018), ('raises', 0.018), ('observation', 0.018), ('runs', 0.017), ('receive', 0.017), ('think', 0.017), ('wait', 0.017), ('bd', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="141-tfidf-1" href="./nips-2008-Multi-Agent_Filtering_with_Infinitely_Nested_Beliefs.html">141 nips-2008-Multi-Agent Filtering with Infinitely Nested Beliefs</a></p>
<p>Author: Luke Zettlemoyer, Brian Milch, Leslie P. Kaelbling</p><p>Abstract: In partially observable worlds with many agents, nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents. The multi-agent ﬁltering problem is to efﬁciently represent and update these beliefs through time as the agents act in the world. In this paper, we formally deﬁne an inﬁnite sequence of nested beliefs about the state of the world at the current time t, and present a ﬁltering algorithm that maintains a ﬁnite representation which can be used to generate these beliefs. In some cases, this representation can be updated exactly in constant time; we also present a simple approximation scheme to compact beliefs if they become too complex. In experiments, we demonstrate efﬁcient ﬁltering in a range of multi-agent domains. 1</p><p>2 0.16817157 <a title="141-tfidf-2" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>Author: Yang Wang, Greg Mori</p><p>Abstract: We present a discriminative part-based approach for human action recognition from video sequences using motion features. Our model is based on the recently proposed hidden conditional random ﬁeld (hCRF) for object recognition. Similar to hCRF for object recognition, we model a human action by a ﬂexible constellation of parts conditioned on image observations. Different from object recognition, our model combines both large-scale global features and local patch features to distinguish various actions. Our experimental results show that our model is comparable to other state-of-the-art approaches in action recognition. In particular, our experimental results demonstrate that combining large-scale global features and local patch features performs signiﬁcantly better than directly applying hCRF on local patches alone. 1</p><p>3 0.12240104 <a title="141-tfidf-3" href="./nips-2008-Skill_Characterization_Based_on_Betweenness.html">212 nips-2008-Skill Characterization Based on Betweenness</a></p>
<p>Author: Ozgur Simsek, Andre S. Barreto</p><p>Abstract: We present a characterization of a useful class of skills based on a graphical representation of an agent’s interaction with its environment. Our characterization uses betweenness, a measure of centrality on graphs. It captures and generalizes (at least intuitively) the bottleneck concept, which has inspired many of the existing skill-discovery algorithms. Our characterization may be used directly to form a set of skills suitable for a given task. More importantly, it serves as a useful guide for developing incremental skill-discovery algorithms that do not rely on knowing or representing the interaction graph in its entirety. 1</p><p>4 0.11887159 <a title="141-tfidf-4" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>Author: Erik Talvitie, Satinder P. Singh</p><p>Abstract: We present a novel mathematical formalism for the idea of a “local model” of an uncontrolled dynamical system, a model that makes only certain predictions in only certain situations. As a result of its restricted responsibilities, a local model may be far simpler than a complete model of the system. We then show how one might combine several local models to produce a more detailed model. We demonstrate our ability to learn a collection of local models on a large-scale example and do a preliminary empirical comparison of learning a collection of local models and some other model learning methods. 1</p><p>5 0.10714529 <a title="141-tfidf-5" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>Author: Mahdi M. Fard, Joelle Pineau</p><p>Abstract: Markov Decision Processes (MDPs) have been extensively studied and used in the context of planning and decision-making, and many methods exist to ﬁnd the optimal policy for problems modelled as MDPs. Although ﬁnding the optimal policy is sufﬁcient in many domains, in certain applications such as decision support systems where the policy is executed by a human (rather than a machine), ﬁnding all possible near-optimal policies might be useful as it provides more ﬂexibility to the person executing the policy. In this paper we introduce the new concept of non-deterministic MDP policies, and address the question of ﬁnding near-optimal non-deterministic policies. We propose two solutions to this problem, one based on a Mixed Integer Program and the other one based on a search algorithm. We include experimental results obtained from applying this framework to optimize treatment choices in the context of a medical decision support system. 1</p><p>6 0.10627561 <a title="141-tfidf-6" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<p>7 0.10592581 <a title="141-tfidf-7" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>8 0.097933196 <a title="141-tfidf-8" href="./nips-2008-Bayesian_Model_of_Behaviour_in_Economic_Games.html">33 nips-2008-Bayesian Model of Behaviour in Economic Games</a></p>
<p>9 0.096607931 <a title="141-tfidf-9" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>10 0.092819385 <a title="141-tfidf-10" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>11 0.073388271 <a title="141-tfidf-11" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>12 0.071716011 <a title="141-tfidf-12" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>13 0.070590638 <a title="141-tfidf-13" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>14 0.068162553 <a title="141-tfidf-14" href="./nips-2008-Psychiatry%3A_Insights_into_depression_through_normative_decision-making_models.html">187 nips-2008-Psychiatry: Insights into depression through normative decision-making models</a></p>
<p>15 0.064835861 <a title="141-tfidf-15" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>16 0.051404685 <a title="141-tfidf-16" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>17 0.045830835 <a title="141-tfidf-17" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>18 0.045317441 <a title="141-tfidf-18" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<p>19 0.045257267 <a title="141-tfidf-19" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>20 0.043961577 <a title="141-tfidf-20" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.121), (1, 0.16), (2, 0.036), (3, -0.08), (4, 0.073), (5, 0.005), (6, 0.015), (7, -0.031), (8, 0.008), (9, -0.017), (10, 0.015), (11, 0.038), (12, 0.034), (13, -0.016), (14, -0.007), (15, -0.008), (16, 0.018), (17, -0.116), (18, -0.026), (19, 0.114), (20, 0.023), (21, -0.12), (22, -0.097), (23, 0.032), (24, 0.093), (25, -0.134), (26, 0.029), (27, -0.046), (28, 0.169), (29, 0.005), (30, 0.032), (31, 0.13), (32, -0.012), (33, -0.002), (34, 0.014), (35, -0.217), (36, 0.259), (37, 0.097), (38, -0.079), (39, 0.171), (40, -0.072), (41, -0.053), (42, 0.246), (43, -0.086), (44, 0.039), (45, 0.06), (46, -0.028), (47, 0.097), (48, 0.053), (49, -0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97748691 <a title="141-lsi-1" href="./nips-2008-Multi-Agent_Filtering_with_Infinitely_Nested_Beliefs.html">141 nips-2008-Multi-Agent Filtering with Infinitely Nested Beliefs</a></p>
<p>Author: Luke Zettlemoyer, Brian Milch, Leslie P. Kaelbling</p><p>Abstract: In partially observable worlds with many agents, nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents. The multi-agent ﬁltering problem is to efﬁciently represent and update these beliefs through time as the agents act in the world. In this paper, we formally deﬁne an inﬁnite sequence of nested beliefs about the state of the world at the current time t, and present a ﬁltering algorithm that maintains a ﬁnite representation which can be used to generate these beliefs. In some cases, this representation can be updated exactly in constant time; we also present a simple approximation scheme to compact beliefs if they become too complex. In experiments, we demonstrate efﬁcient ﬁltering in a range of multi-agent domains. 1</p><p>2 0.70048374 <a title="141-lsi-2" href="./nips-2008-Bayesian_Model_of_Behaviour_in_Economic_Games.html">33 nips-2008-Bayesian Model of Behaviour in Economic Games</a></p>
<p>Author: Debajyoti Ray, Brooks King-casas, P. R. Montague, Peter Dayan</p><p>Abstract: Classical game theoretic approaches that make strong rationality assumptions have difﬁculty modeling human behaviour in economic games. We investigate the role of ﬁnite levels of iterated reasoning and non-selﬁsh utility functions in a Partially Observable Markov Decision Process model that incorporates game theoretic notions of interactivity. Our generative model captures a broad class of characteristic behaviours in a multi-round Investor-Trustee game. We invert the generative process for a recognition model that is used to classify 200 subjects playing this game against randomly matched opponents. 1</p><p>3 0.63376933 <a title="141-lsi-3" href="./nips-2008-Skill_Characterization_Based_on_Betweenness.html">212 nips-2008-Skill Characterization Based on Betweenness</a></p>
<p>Author: Ozgur Simsek, Andre S. Barreto</p><p>Abstract: We present a characterization of a useful class of skills based on a graphical representation of an agent’s interaction with its environment. Our characterization uses betweenness, a measure of centrality on graphs. It captures and generalizes (at least intuitively) the bottleneck concept, which has inspired many of the existing skill-discovery algorithms. Our characterization may be used directly to form a set of skills suitable for a given task. More importantly, it serves as a useful guide for developing incremental skill-discovery algorithms that do not rely on knowing or representing the interaction graph in its entirety. 1</p><p>4 0.60052705 <a title="141-lsi-4" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>Author: Erik Talvitie, Satinder P. Singh</p><p>Abstract: We present a novel mathematical formalism for the idea of a “local model” of an uncontrolled dynamical system, a model that makes only certain predictions in only certain situations. As a result of its restricted responsibilities, a local model may be far simpler than a complete model of the system. We then show how one might combine several local models to produce a more detailed model. We demonstrate our ability to learn a collection of local models on a large-scale example and do a preliminary empirical comparison of learning a collection of local models and some other model learning methods. 1</p><p>5 0.4697679 <a title="141-lsi-5" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>Author: Yang Wang, Greg Mori</p><p>Abstract: We present a discriminative part-based approach for human action recognition from video sequences using motion features. Our model is based on the recently proposed hidden conditional random ﬁeld (hCRF) for object recognition. Similar to hCRF for object recognition, we model a human action by a ﬂexible constellation of parts conditioned on image observations. Different from object recognition, our model combines both large-scale global features and local patch features to distinguish various actions. Our experimental results show that our model is comparable to other state-of-the-art approaches in action recognition. In particular, our experimental results demonstrate that combining large-scale global features and local patch features performs signiﬁcantly better than directly applying hCRF on local patches alone. 1</p><p>6 0.44176859 <a title="141-lsi-6" href="./nips-2008-Psychiatry%3A_Insights_into_depression_through_normative_decision-making_models.html">187 nips-2008-Psychiatry: Insights into depression through normative decision-making models</a></p>
<p>7 0.43741867 <a title="141-lsi-7" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<p>8 0.43252388 <a title="141-lsi-8" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>9 0.38356641 <a title="141-lsi-9" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>10 0.35208678 <a title="141-lsi-10" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>11 0.34013361 <a title="141-lsi-11" href="./nips-2008-Multi-resolution_Exploration_in_Continuous_Spaces.html">144 nips-2008-Multi-resolution Exploration in Continuous Spaces</a></p>
<p>12 0.32711598 <a title="141-lsi-12" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>13 0.30522895 <a title="141-lsi-13" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>14 0.30123609 <a title="141-lsi-14" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>15 0.2883248 <a title="141-lsi-15" href="./nips-2008-Goal-directed_decision_making_in_prefrontal_cortex%3A_a_computational_framework.html">94 nips-2008-Goal-directed decision making in prefrontal cortex: a computational framework</a></p>
<p>16 0.26914424 <a title="141-lsi-16" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>17 0.25074077 <a title="141-lsi-17" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>18 0.24402951 <a title="141-lsi-18" href="./nips-2008-Stress%2C_noradrenaline%2C_and_realistic_prediction_of_mouse_behaviour_using_reinforcement_learning.html">222 nips-2008-Stress, noradrenaline, and realistic prediction of mouse behaviour using reinforcement learning</a></p>
<p>19 0.24091536 <a title="141-lsi-19" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<p>20 0.23353919 <a title="141-lsi-20" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.026), (6, 0.048), (7, 0.034), (12, 0.041), (28, 0.145), (41, 0.337), (57, 0.054), (59, 0.025), (63, 0.046), (71, 0.036), (77, 0.062), (78, 0.012), (83, 0.035)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76388174 <a title="141-lda-1" href="./nips-2008-Multi-Agent_Filtering_with_Infinitely_Nested_Beliefs.html">141 nips-2008-Multi-Agent Filtering with Infinitely Nested Beliefs</a></p>
<p>Author: Luke Zettlemoyer, Brian Milch, Leslie P. Kaelbling</p><p>Abstract: In partially observable worlds with many agents, nested beliefs are formed when agents simultaneously reason about the unknown state of the world and the beliefs of the other agents. The multi-agent ﬁltering problem is to efﬁciently represent and update these beliefs through time as the agents act in the world. In this paper, we formally deﬁne an inﬁnite sequence of nested beliefs about the state of the world at the current time t, and present a ﬁltering algorithm that maintains a ﬁnite representation which can be used to generate these beliefs. In some cases, this representation can be updated exactly in constant time; we also present a simple approximation scheme to compact beliefs if they become too complex. In experiments, we demonstrate efﬁcient ﬁltering in a range of multi-agent domains. 1</p><p>2 0.64030325 <a title="141-lda-2" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>Author: Adrian Haith, Carl P. Jackson, R. C. Miall, Sethu Vijayakumar</p><p>Abstract: Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a uniﬁed model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereﬀects. This uniﬁed model also makes the surprising prediction that force ﬁeld adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We conﬁrm this prediction with an experiment. 1</p><p>3 0.61650091 <a title="141-lda-3" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>Author: Silvia Chiappa, Jens Kober, Jan R. Peters</p><p>Abstract: Motor primitives or motion templates have become an important concept for both modeling human motor control as well as generating robot behaviors using imitation learning. Recent impressive results range from humanoid robot movement generation to timing models of human motions. The automatic generation of skill libraries containing multiple motion templates is an important step in robot learning. Such a skill learning system needs to cluster similar movements together and represent each resulting motion template as a generative model which is subsequently used for the execution of the behavior by a robot system. In this paper, we show how human trajectories captured as multi-dimensional time-series can be clustered using Bayesian mixtures of linear Gaussian state-space models based on the similarity of their dynamics. The appropriate number of templates is automatically determined by enforcing a parsimonious parametrization. As the resulting model is intractable, we introduce a novel approximation method based on variational Bayes, which is especially designed to enable the use of efﬁcient inference algorithms. On recorded human Balero movements, this method is not only capable of ﬁnding reasonable motion templates but also yields a generative model which works well in the execution of this complex task on a simulated anthropomorphic SARCOS arm.</p><p>4 0.47737548 <a title="141-lda-4" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>Author: Amir M. Farahmand, Mohammad Ghavamzadeh, Shie Mannor, Csaba Szepesvári</p><p>Abstract: In this paper we consider approximate policy-iteration-based reinforcement learning algorithms. In order to implement a ﬂexible function approximation scheme we propose the use of non-parametric methods with regularization, providing a convenient way to control the complexity of the function approximator. We propose two novel regularized policy iteration algorithms by adding L2 -regularization to two widely-used policy evaluation methods: Bellman residual minimization (BRM) and least-squares temporal difference learning (LSTD). We derive efﬁcient implementation for our algorithms when the approximate value-functions belong to a reproducing kernel Hilbert space. We also provide ﬁnite-sample performance bounds for our algorithms and show that they are able to achieve optimal rates of convergence under the studied conditions. 1</p><p>5 0.47321367 <a title="141-lda-5" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>Author: Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</p><p>Abstract: Uncertainty is omnipresent when we perceive or interact with our environment, and the Bayesian framework provides computational methods for dealing with it. Mathematical models for Bayesian decision making typically require datastructures that are hard to implement in neural networks. This article shows that even the simplest and experimentally best supported type of synaptic plasticity, Hebbian learning, in combination with a sparse, redundant neural code, can in principle learn to infer optimal Bayesian decisions. We present a concrete Hebbian learning rule operating on log-probability ratios. Modulated by reward-signals, this Hebbian plasticity rule also provides a new perspective for understanding how Bayesian inference could support fast reinforcement learning in the brain. In particular we show that recent experimental results by Yang and Shadlen [1] on reinforcement learning of probabilistic inference in primates can be modeled in this way. 1</p><p>6 0.47271144 <a title="141-lda-6" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<p>7 0.47184682 <a title="141-lda-7" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>8 0.47175822 <a title="141-lda-8" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>9 0.47073722 <a title="141-lda-9" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>10 0.46948838 <a title="141-lda-10" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>11 0.46889174 <a title="141-lda-11" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>12 0.46860856 <a title="141-lda-12" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>13 0.46807629 <a title="141-lda-13" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>14 0.46760085 <a title="141-lda-14" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>15 0.46543759 <a title="141-lda-15" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>16 0.46527302 <a title="141-lda-16" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>17 0.46498993 <a title="141-lda-17" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>18 0.46481296 <a title="141-lda-18" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>19 0.46458611 <a title="141-lda-19" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>20 0.46442631 <a title="141-lda-20" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
