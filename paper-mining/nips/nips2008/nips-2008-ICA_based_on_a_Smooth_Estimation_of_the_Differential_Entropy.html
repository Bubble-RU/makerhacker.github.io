<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-102" href="#">nips2008-102</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</h1>
<br/><p>Source: <a title="nips-2008-102-pdf" href="http://papers.nips.cc/paper/3500-ica-based-on-a-smooth-estimation-of-the-differential-entropy.pdf">pdf</a></p><p>Author: Lev Faivishevsky, Jacob Goldberger</p><p>Abstract: In this paper we introduce the MeanNN approach for estimation of main information theoretic measures such as differential entropy, mutual information and divergence. As opposed to other nonparametric approaches the MeanNN results in smooth differentiable functions of the data samples with clear geometrical interpretation. Then we apply the proposed estimators to the ICA problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods. The improved performance of the proposed ICA algorithm is demonstrated on several test examples in comparison with state-ofthe-art techniques. 1</p><p>Reference: <a title="nips-2008-102-reference" href="../nips2008_reference/nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('meann', 0.588), ('gst', 0.27), ('entrop', 0.263), ('ic', 0.26), ('milc', 0.221), ('knn', 0.184), ('st', 0.172), ('pik', 0.172), ('rot', 0.171), ('guv', 0.147), ('mut', 0.115), ('hme', 0.107), ('kernelic', 0.098), ('wqr', 0.098), ('neighb', 0.093), ('rad', 0.092), ('estim', 0.089), ('grady', 0.089), ('yd', 0.087), ('angl', 0.086)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="102-tfidf-1" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>Author: Lev Faivishevsky, Jacob Goldberger</p><p>Abstract: In this paper we introduce the MeanNN approach for estimation of main information theoretic measures such as differential entropy, mutual information and divergence. As opposed to other nonparametric approaches the MeanNN results in smooth differentiable functions of the data samples with clear geometrical interpretation. Then we apply the proposed estimators to the ICA problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods. The improved performance of the proposed ICA algorithm is demonstrated on several test examples in comparison with state-ofthe-art techniques. 1</p><p>2 0.18654524 <a title="102-tfidf-2" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>Author: Fernando Pérez-Cruz</p><p>Abstract: We analyze the estimation of information theoretic measures of continuous random variables such as: differential entropy, mutual information or KullbackLeibler divergence. The objective of this paper is two-fold. First, we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with ﬁxed k converge almost surely, even though the k-nearest-neighbor density estimation with ﬁxed k does not converge to its true measure. Second, we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples. Nevertheless, these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent. We show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the Hilbert Schmidt independence criterion. 1</p><p>3 0.14514773 <a title="102-tfidf-3" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>Author: Fabian H. Sinz, Matthias Bethge</p><p>Abstract: Bandpass ﬁltering, orientation selectivity, and contrast gain control are prominent features of sensory coding at the level of V1 simple cells. While the effect of bandpass ﬁltering and orientation selectivity can be assessed within a linear model, contrast gain control is an inherently nonlinear computation. Here we employ the class of Lp elliptically contoured distributions to investigate the extent to which the two features—orientation selectivity and contrast gain control—are suited to model the statistics of natural images. Within this framework we ﬁnd that contrast gain control can play a signiﬁcant role for the removal of redundancies in natural images. Orientation selectivity, in contrast, has only a very limited potential for redundancy reduction. 1</p><p>4 0.12091269 <a title="102-tfidf-4" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>Author: Siwei Lyu, Eero P. Simoncelli</p><p>Abstract: We consider the problem of transforming a signal to a representation in which the components are statistically independent. When the signal is generated as a linear transformation of independent Gaussian or non-Gaussian sources, the solution may be computed using a linear transformation (PCA or ICA, respectively). Here, we consider a complementary case, in which the source is non-Gaussian but elliptically symmetric. Such a source cannot be decomposed into independent components using a linear transform, but we show that a simple nonlinear transformation, which we call radial Gaussianization (RG), is able to remove all dependencies. We apply this methodology to natural signals, demonstrating that the joint distributions of nearby bandpass ﬁlter responses, for both sounds and images, are closer to being elliptically symmetric than linearly transformed factorial sources. Consistent with this, we demonstrate that the reduction in dependency achieved by applying RG to either pairs or blocks of bandpass ﬁlter responses is signiﬁcantly greater than that achieved by PCA or ICA.</p><p>5 0.11605589 <a title="102-tfidf-5" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>Author: Markus Maier, Ulrike V. Luxburg, Matthias Hein</p><p>Abstract: Graph clustering methods such as spectral clustering are deﬁned for general weighted graphs. In machine learning, however, data often is not given in form of a graph, but in terms of similarity (or distance) values between points. In this case, ﬁrst a neighborhood graph is constructed using the similarities between the points and then a graph clustering algorithm is applied to this graph. In this paper we investigate the inﬂuence of the construction of the similarity graph on the clustering results. We ﬁrst study the convergence of graph clustering criteria such as the normalized cut (Ncut) as the sample size tends to inﬁnity. We ﬁnd that the limit expressions are different for different types of graph, for example the r-neighborhood graph or the k-nearest neighbor graph. In plain words: Ncut on a kNN graph does something systematically different than Ncut on an r-neighborhood graph! This ﬁnding shows that graph clustering criteria cannot be studied independently of the kind of graph they are applied to. We also provide examples which show that these differences can be observed for toy and real data already for rather small sample sizes. 1</p><p>6 0.10482426 <a title="102-tfidf-6" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>7 0.087343849 <a title="102-tfidf-7" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>8 0.065637298 <a title="102-tfidf-8" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>9 0.065148428 <a title="102-tfidf-9" href="./nips-2008-Model_selection_and_velocity_estimation_using_novel_priors_for_motion_patterns.html">136 nips-2008-Model selection and velocity estimation using novel priors for motion patterns</a></p>
<p>10 0.063066691 <a title="102-tfidf-10" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>11 0.058457352 <a title="102-tfidf-11" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>12 0.055980131 <a title="102-tfidf-12" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>13 0.055747841 <a title="102-tfidf-13" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>14 0.055682849 <a title="102-tfidf-14" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>15 0.055473264 <a title="102-tfidf-15" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>16 0.055297371 <a title="102-tfidf-16" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>17 0.051851261 <a title="102-tfidf-17" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>18 0.050077531 <a title="102-tfidf-18" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>19 0.049005672 <a title="102-tfidf-19" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>20 0.048497465 <a title="102-tfidf-20" href="./nips-2008-A_Convergent_%24O%28n%29%24_Temporal-difference_Algorithm_for_Off-policy_Learning_with_Linear_Function_Approximation.html">1 nips-2008-A Convergent $O(n)$ Temporal-difference Algorithm for Off-policy Learning with Linear Function Approximation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.16), (1, 0.016), (2, -0.006), (3, -0.028), (4, 0.048), (5, 0.036), (6, 0.033), (7, -0.01), (8, 0.027), (9, 0.05), (10, 0.025), (11, 0.032), (12, -0.017), (13, 0.029), (14, 0.057), (15, 0.061), (16, 0.001), (17, -0.107), (18, 0.137), (19, 0.182), (20, -0.122), (21, 0.016), (22, 0.026), (23, -0.113), (24, 0.142), (25, 0.035), (26, 0.261), (27, -0.004), (28, -0.053), (29, -0.03), (30, -0.086), (31, -0.054), (32, -0.023), (33, 0.012), (34, 0.062), (35, 0.061), (36, 0.16), (37, -0.006), (38, -0.021), (39, 0.053), (40, -0.032), (41, 0.041), (42, 0.162), (43, 0.044), (44, -0.029), (45, 0.011), (46, 0.038), (47, -0.041), (48, -0.037), (49, -0.065)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91942739 <a title="102-lsi-1" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>Author: Lev Faivishevsky, Jacob Goldberger</p><p>Abstract: In this paper we introduce the MeanNN approach for estimation of main information theoretic measures such as differential entropy, mutual information and divergence. As opposed to other nonparametric approaches the MeanNN results in smooth differentiable functions of the data samples with clear geometrical interpretation. Then we apply the proposed estimators to the ICA problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods. The improved performance of the proposed ICA algorithm is demonstrated on several test examples in comparison with state-ofthe-art techniques. 1</p><p>2 0.71948099 <a title="102-lsi-2" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>Author: Siwei Lyu, Eero P. Simoncelli</p><p>Abstract: We consider the problem of transforming a signal to a representation in which the components are statistically independent. When the signal is generated as a linear transformation of independent Gaussian or non-Gaussian sources, the solution may be computed using a linear transformation (PCA or ICA, respectively). Here, we consider a complementary case, in which the source is non-Gaussian but elliptically symmetric. Such a source cannot be decomposed into independent components using a linear transform, but we show that a simple nonlinear transformation, which we call radial Gaussianization (RG), is able to remove all dependencies. We apply this methodology to natural signals, demonstrating that the joint distributions of nearby bandpass ﬁlter responses, for both sounds and images, are closer to being elliptically symmetric than linearly transformed factorial sources. Consistent with this, we demonstrate that the reduction in dependency achieved by applying RG to either pairs or blocks of bandpass ﬁlter responses is signiﬁcantly greater than that achieved by PCA or ICA.</p><p>3 0.71842277 <a title="102-lsi-3" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>Author: Fernando Pérez-Cruz</p><p>Abstract: We analyze the estimation of information theoretic measures of continuous random variables such as: differential entropy, mutual information or KullbackLeibler divergence. The objective of this paper is two-fold. First, we prove that the information theoretic measure estimates using the k-nearest-neighbor density estimation with ﬁxed k converge almost surely, even though the k-nearest-neighbor density estimation with ﬁxed k does not converge to its true measure. Second, we show that the information theoretic measure estimates do not converge for k growing linearly with the number of samples. Nevertheless, these nonconvergent estimates can be used for solving the two-sample problem and assessing if two random variables are independent. We show that the two-sample and independence tests based on these nonconvergent estimates compare favorably with the maximum mean discrepancy test and the Hilbert Schmidt independence criterion. 1</p><p>4 0.71403456 <a title="102-lsi-4" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>Author: Fabian H. Sinz, Matthias Bethge</p><p>Abstract: Bandpass ﬁltering, orientation selectivity, and contrast gain control are prominent features of sensory coding at the level of V1 simple cells. While the effect of bandpass ﬁltering and orientation selectivity can be assessed within a linear model, contrast gain control is an inherently nonlinear computation. Here we employ the class of Lp elliptically contoured distributions to investigate the extent to which the two features—orientation selectivity and contrast gain control—are suited to model the statistics of natural images. Within this framework we ﬁnd that contrast gain control can play a signiﬁcant role for the removal of redundancies in natural images. Orientation selectivity, in contrast, has only a very limited potential for redundancy reduction. 1</p><p>5 0.48546904 <a title="102-lsi-5" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>Author: Guangzhi Cao, Charles Bouman</p><p>Abstract: Covariance estimation for high dimensional vectors is a classically difﬁcult problem in statistical analysis and machine learning. In this paper, we propose a maximum likelihood (ML) approach to covariance estimation, which employs a novel sparsity constraint. More speciﬁcally, the covariance is constrained to have an eigen decomposition which can be represented as a sparse matrix transform (SMT). The SMT is formed by a product of pairwise coordinate rotations known as Givens rotations. Using this framework, the covariance can be efﬁciently estimated using greedy minimization of the log likelihood function, and the number of Givens rotations can be efﬁciently computed using a cross-validation procedure. The resulting estimator is positive deﬁnite and well-conditioned even when the sample size is limited. Experiments on standard hyperspectral data sets show that the SMT covariance estimate is consistently more accurate than both traditional shrinkage estimates and recently proposed graphical lasso estimates for a variety of different classes and sample sizes. 1</p><p>6 0.4844628 <a title="102-lsi-6" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>7 0.41495597 <a title="102-lsi-7" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>8 0.39406341 <a title="102-lsi-8" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>9 0.38292381 <a title="102-lsi-9" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>10 0.37491557 <a title="102-lsi-10" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>11 0.36888793 <a title="102-lsi-11" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>12 0.36709863 <a title="102-lsi-12" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>13 0.36064705 <a title="102-lsi-13" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>14 0.32953674 <a title="102-lsi-14" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>15 0.32486656 <a title="102-lsi-15" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>16 0.32197171 <a title="102-lsi-16" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>17 0.3187806 <a title="102-lsi-17" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>18 0.31716645 <a title="102-lsi-18" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>19 0.31496677 <a title="102-lsi-19" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>20 0.30585706 <a title="102-lsi-20" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.016), (6, 0.291), (30, 0.043), (38, 0.016), (40, 0.079), (51, 0.011), (60, 0.024), (63, 0.185), (64, 0.077), (71, 0.133)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80250716 <a title="102-lda-1" href="./nips-2008-Model_selection_and_velocity_estimation_using_novel_priors_for_motion_patterns.html">136 nips-2008-Model selection and velocity estimation using novel priors for motion patterns</a></p>
<p>Author: Shuang Wu, Hongjing Lu, Alan L. Yuille</p><p>Abstract: Psychophysical experiments show that humans are better at perceiving rotation and expansion than translation. These ﬁndings are inconsistent with standard models of motion integration which predict best performance for translation [6]. To explain this discrepancy, our theory formulates motion perception at two levels of inference: we ﬁrst perform model selection between the competing models (e.g. translation, rotation, and expansion) and then estimate the velocity using the selected model. We deﬁne novel prior models for smooth rotation and expansion using techniques similar to those in the slow-and-smooth model [17] (e.g. Green functions of differential operators). The theory gives good agreement with the trends observed in human experiments. 1</p><p>same-paper 2 0.74702549 <a title="102-lda-2" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>Author: Lev Faivishevsky, Jacob Goldberger</p><p>Abstract: In this paper we introduce the MeanNN approach for estimation of main information theoretic measures such as differential entropy, mutual information and divergence. As opposed to other nonparametric approaches the MeanNN results in smooth differentiable functions of the data samples with clear geometrical interpretation. Then we apply the proposed estimators to the ICA problem and obtain a smooth expression for the mutual information that can be analytically optimized by gradient descent methods. The improved performance of the proposed ICA algorithm is demonstrated on several test examples in comparison with state-ofthe-art techniques. 1</p><p>3 0.69934547 <a title="102-lda-3" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>Author: Daphna Weinshall, Hynek Hermansky, Alon Zweig, Jie Luo, Holly Jimison, Frank Ohl, Misha Pavel</p><p>Abstract: Unexpected stimuli are a challenge to any machine learning algorithm. Here we identify distinct types of unexpected events, focusing on ’incongruent events’ when ’general level’ and ’speciﬁc level’ classiﬁers give conﬂicting predictions. We deﬁne a formal framework for the representation and processing of incongruent events: starting from the notion of label hierarchy, we show how partial order on labels can be deduced from such hierarchies. For each event, we compute its probability in different ways, based on adjacent levels (according to the partial order) in the label hierarchy. An incongruent event is an event where the probability computed based on some more speciﬁc level (in accordance with the partial order) is much smaller than the probability computed based on some more general level, leading to conﬂicting predictions. We derive algorithms to detect incongruent events from different types of hierarchies, corresponding to class membership or part membership. Respectively, we show promising results with real data on two speciﬁc problems: Out Of Vocabulary words in speech recognition, and the identiﬁcation of a new sub-class (e.g., the face of a new individual) in audio-visual facial object recognition.</p><p>4 0.64936829 <a title="102-lda-4" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>Author: Volkan Cevher, Marco F. Duarte, Chinmay Hegde, Richard Baraniuk</p><p>Abstract: Compressive Sensing (CS) combines sampling and compression into a single subNyquist linear measurement process for sparse and compressible signals. In this paper, we extend the theory of CS to include signals that are concisely represented in terms of a graphical model. In particular, we use Markov Random Fields (MRFs) to represent sparse signals whose nonzero coefﬁcients are clustered. Our new model-based recovery algorithm, dubbed Lattice Matching Pursuit (LaMP), stably recovers MRF-modeled signals using many fewer measurements and computations than the current state-of-the-art algorithms.</p><p>5 0.64634442 <a title="102-lda-5" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>Author: Guillaume R. Obozinski, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: We study the behavior of block 1 / 2 regularization for multivariate regression, where a K-dimensional response vector is regressed upon a ﬁxed set of p covariates. The problem of support union recovery is to recover the subset of covariates that are active in at least one of the regression problems. Studying this problem under high-dimensional scaling (where the problem parameters as well as sample size n tend to inﬁnity simultaneously), our main result is to show that exact recovery is possible once the order parameter given by θ 1 / 2 (n, p, s) : = n/[2ψ(B ∗ ) log(p − s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the size of the union of supports, and ψ(B ∗ ) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefﬁcient vectors that constitute the model. This sparsity-overlap function reveals that block 1 / 2 regularization for multivariate regression never harms performance relative to a naive 1 -approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal relative to the design. We complement our theoretical results with simulations that demonstrate the sharpness of the result, even for relatively small problems. 1</p><p>6 0.64495146 <a title="102-lda-6" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>7 0.63989997 <a title="102-lda-7" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>8 0.63851416 <a title="102-lda-8" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>9 0.63687551 <a title="102-lda-9" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>10 0.63670069 <a title="102-lda-10" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>11 0.63667953 <a title="102-lda-11" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>12 0.6365034 <a title="102-lda-12" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>13 0.63216472 <a title="102-lda-13" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>14 0.63193959 <a title="102-lda-14" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>15 0.63106292 <a title="102-lda-15" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>16 0.63066447 <a title="102-lda-16" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>17 0.63026416 <a title="102-lda-17" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>18 0.63006938 <a title="102-lda-18" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>19 0.62975502 <a title="102-lda-19" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>20 0.62883621 <a title="102-lda-20" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
