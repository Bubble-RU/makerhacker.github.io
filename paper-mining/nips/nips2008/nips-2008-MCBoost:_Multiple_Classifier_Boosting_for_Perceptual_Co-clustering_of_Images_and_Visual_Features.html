<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-130" href="#">nips2008-130</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</h1>
<br/><p>Source: <a title="nips-2008-130-pdf" href="http://papers.nips.cc/paper/3483-mcboost-multiple-classifier-boosting-for-perceptual-co-clustering-of-images-and-visual-features.pdf">pdf</a></p><p>Author: Tae-kyun Kim, Roberto Cipolla</p><p>Abstract: We present a new co-clustering problem of images and visual features. The problem involves a set of non-object images in addition to a set of object images and features to be co-clustered. Co-clustering is performed in a way that maximises discrimination of object images from non-object images, thus emphasizing discriminative features. This provides a way of obtaining perceptual joint-clusters of object images and features. We tackle the problem by simultaneously boosting multiple strong classiﬁers which compete for images by their expertise. Each boosting classiﬁer is an aggregation of weak-learners, i.e. simple visual features. The obtained classiﬁers are useful for object detection tasks which exhibit multimodalities, e.g. multi-category and multi-view object detection tasks. Experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classiﬁers in object detection tasks. 1</p><p>Reference: <a title="nips-2008-130-reference" href="../nips2008_reference/nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract We present a new co-clustering problem of images and visual features. [sent-5, score-0.332]
</p><p>2 The problem involves a set of non-object images in addition to a set of object images and features to be co-clustered. [sent-6, score-0.669]
</p><p>3 Co-clustering is performed in a way that maximises discrimination of object images from non-object images, thus emphasizing discriminative features. [sent-7, score-0.418]
</p><p>4 This provides a way of obtaining perceptual joint-clusters of object images and features. [sent-8, score-0.439]
</p><p>5 We tackle the problem by simultaneously boosting multiple strong classiﬁers which compete for images by their expertise. [sent-9, score-0.563]
</p><p>6 Each boosting classiﬁer is an aggregation of weak-learners, i. [sent-10, score-0.247]
</p><p>7 The obtained classiﬁers are useful for object detection tasks which exhibit multimodalities, e. [sent-13, score-0.241]
</p><p>8 Experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classiﬁers in object detection tasks. [sent-16, score-1.727]
</p><p>9 Learning process may be associated with co-clusters of visual features and imagery data in a way of facilitating image data perception. [sent-18, score-0.268]
</p><p>10 We formulate this in the context of boosting classiﬁers with simple visual features for object detection task [3]. [sent-19, score-0.662]
</p><p>11 There are two sets of images: a set of object images and a set of non-object images, labelled as positive and negative class members respectively. [sent-20, score-0.494]
</p><p>12 There are also a huge number of simple image features, only a small fraction of which are selected to discriminate the positive class from the negative class by H(x) = t αt ht (x) where x is an input vector, αt , ht are the weight and the score of t-th weak-learner using a single feature. [sent-21, score-0.294]
</p><p>13 As object images typically exhibit multi-modalities, a single aggregation of simple features often does not dichotomise all object images from non-object images. [sent-22, score-0.839]
</p><p>14 Note that image clusters to be obtained are coupled with selected features and likewise features to be selected are dependent on image clusters, requiring a concurrent clustering of images and features. [sent-24, score-0.809]
</p><p>15 See Figure 1 for an example where subsets of face images are pose-wise obtained with associated features by the proposed method (Section 3). [sent-25, score-0.59]
</p><p>16 As such facial features are distributed differently mainly according to face pose, the obtained pose-wise face clusters are, therefore, intuitive and desirable in perception. [sent-28, score-0.838]
</p><p>17 Note the challenges in achieving this: The input set of face images are mixed up by different faces, lighting conditions as well as pose. [sent-29, score-0.533]
</p><p>18 Desired image clusters are not observable in input space. [sent-31, score-0.308]
</p><p>19 Random image set  Figure 1: Perceptual co-clusters of images and visual features. [sent-42, score-0.398]
</p><p>20 For given a set of face and random images and simple visual features, the proposed method ﬁnds perceptual joint-clusters of face images and features, which facilitates classiﬁcation of face images from random images. [sent-43, score-1.64]
</p><p>21 for the result of the traditional unsupervised method (k-means clustering) applied to the face images. [sent-45, score-0.282]
</p><p>22 Images of the obtained clusters are almost random with respect to pose. [sent-46, score-0.242]
</p><p>23 To obtain perceptual face clusters, a method requires a discriminative process and part-based representations (like the simple features used). [sent-47, score-0.462]
</p><p>24 Technically, we must be able to cope with an arbitrary initialisation of image clusters (as target clusters are hidden) and feature selection among a huge number of simple visual features. [sent-48, score-0.821]
</p><p>25 The Face cluster-2 method is also useful for object detection tasks. [sent-57, score-0.282]
</p><p>26 Boosting a classiﬁer with simple features [3] is a state-of-the-art in object detection tasks. [sent-58, score-0.316]
</p><p>27 Conventionally, multiple boosting classiﬁers are separately learnt for multiple categories and/or multiple views of object images [6]. [sent-60, score-0.849]
</p><p>28 Would there be a better partitioning for learning multiple boosting classiﬁers? [sent-63, score-0.334]
</p><p>29 It simultaneously boosts multiple strong classiﬁers, each of which has expertise on a particular set of object images by a set of weak-learners. [sent-65, score-0.517]
</p><p>30 It simultaneously clusters rows and columns of a co-occurrence table by e. [sent-71, score-0.242]
</p><p>31 While MoE makes a decision by dynamically selected local experts, all weak-learners contribute to a decision with learnt weights in boosting classiﬁer. [sent-86, score-0.437]
</p><p>32 two boosting classiﬁers, are required to conquer each half of data by a set of weak-learners. [sent-91, score-0.247]
</p><p>33 have addressed joint-learning of multiple boosting classiﬁers for multiple category and multiple view object detection [4]. [sent-93, score-0.644]
</p><p>34 Each classiﬁer in their method is based on each of category-wise or pose-wise clusters of object images, which requires manual labels for cateogry/pose, whereas we optimise image clusters and boosting classiﬁers simultaneously. [sent-95, score-1.007]
</p><p>35 3  MCBoost: multiple strong classiﬁer boosting  Our formulation considers K strong classiﬁers, each of which is represented by a linear combination of weak-learners as Hk (x) = αkt hkt (x), k = 1, . [sent-96, score-0.589]
</p><p>36 K, (1) t  where αkt and hkt are the weight and the score of t-th weak-learner of k-th strong classiﬁer. [sent-99, score-0.283]
</p><p>37 It assigns samples to a positive class if any of classiﬁers does and assigns samples to a negative class if every classiﬁer does. [sent-102, score-0.332]
</p><p>38 Conventional design in object detection study [6] also favours OR decision as it does not require classiﬁer selection. [sent-103, score-0.267]
</p><p>39 An individual classiﬁer is learnt from a subset of positive samples and all negative samples, enforcing a positive sample to be accepted by one of the classiﬁers and a negative sample to be rejected by all. [sent-104, score-0.39]
</p><p>40 wki = 1 if xi ∈ k and wki = 0 otherwise, where i and k denote i-th sample and k-th classiﬁer respectively. [sent-108, score-0.474]
</p><p>41 We set wki = 1/K for all k’s for negative samples. [sent-109, score-0.241]
</p><p>42 MCBoost Input: A data set (xi , yi ) and a set of pre-deﬁned weak-learners T Output: Multiple boosting classiﬁers Hk (x) = t=1 αkt hkt (x), k = 1. [sent-111, score-0.501]
</p><p>43 Compute a reduced set of weak-learners H by risk map (4) and randomly initialise the weights wki 2. [sent-115, score-0.278]
</p><p>44 Find weak-learners hkt that maximise i wki · hkt (xi ), hkt ∈ H. [sent-124, score-0.98]
</p><p>45 Find the weak-learner weights αkt that maximise J(H + αkt hkt ). [sent-126, score-0.39]
</p><p>46 End Figure 4: Pseudocode of MCBoost algorithm at t-th round of boosting, to maximise wki · hkt (xi ),  hkt ∈ H,  (3)  i  where hkt ∈ {−1, +1} and H is a reduced set of weak-learners for speeding up the proposed multiple classiﬁer boosting. [sent-131, score-1.105]
</p><p>47 By limiting xo to the data points that have high risk to be misclassiﬁed, the complexity of searching weak-learners at each round of boosting is greatly reduced. [sent-134, score-0.369]
</p><p>48 The risk is deﬁned as xi − xj 2 B j∈Ni R(xi ) = exp{− } (4) 1 + j∈N W xi − xj 2 i  where NiB and NiW are the set of predeﬁned number of nearest neighbors of xi in the opposite class and the same class of xi (See Figure 3). [sent-135, score-0.396]
</p><p>49 , K are then found to maximise J(H + αkt hkt ) by a line search. [sent-139, score-0.337]
</p><p>50 For the cost function J = log i P (xi )yi (1 − P (xi ))(1−yi ) , where yi ∈ {0, 1} is the label of i-th sample, the weight of k-th classiﬁer over i-th sample is updated by yi − P (xi ) ∂J wki = = · Pk (xi ). [sent-141, score-0.293]
</p><p>51 1  Data clustering  We propose a new data clustering method which assigns a positive sample xi to a classiﬁer (or cluster) that has the highest Pk (xi ). [sent-144, score-0.308]
</p><p>52 States in the ﬁgure represent the sam4  classifier 1  classifier 2  classifier 3  1. [sent-158, score-0.279]
</p><p>53 4 10  20  30  10  20  30  boosting round  Figure 5: Example of learning on XOR classiﬁcation problem. [sent-187, score-0.284]
</p><p>54 For a given random initialisation (three different color blobs in the left), the method learns three classiﬁers that nicely settle into desired clusters and decision boundaries (middle). [sent-188, score-0.584]
</p><p>55 Assume that a positive class has samples of three target clusters denoted by A, B and C. [sent-192, score-0.378]
</p><p>56 Samples of more than two target clusters are initially assigned to every classiﬁer. [sent-193, score-0.242]
</p><p>57 Similarly, samples A, C are moved into the respective most experts (step 4) and all re-allocated samples are correctly classiﬁed by weak-learners (step 5). [sent-198, score-0.243]
</p><p>58 Any single or double boosting classiﬁers, therefore, cannot successfully dichotomise the classes. [sent-201, score-0.289]
</p><p>59 The ﬁnal decision boundaries and the tracks of data cluster centres of the three boosting classiﬁers are shown in the middle. [sent-204, score-0.384]
</p><p>60 Despite the mixed-up initialisation, the method learns the three classiﬁers that nicely settle into the target clusters after a bit of jittering in the ﬁrst few rounds. [sent-205, score-0.376]
</p><p>61 Note that the method does not exploit any distance information between input data points, by which conventional clustering methods can apparently yield the same data clusters in this example. [sent-207, score-0.393]
</p><p>62 As exempliﬁed in Figure 2, obtaining desired data clusters by conventional ways are, however, difﬁcult in practice. [sent-208, score-0.3]
</p><p>63 3  Discussion on mixture of experts and future work  The existing local optimisation method, MoE, suffers from the absence of a good initialisation solution, but has nice properties once a good initialisation exists. [sent-211, score-0.348]
</p><p>64 By taking the derivative of the cost function, the sample weight of k-th classiﬁer is given as wki = (yi − P (xi )) · Qk (xi ). [sent-215, score-0.241]
</p><p>65 An EM-like algorithm iterates each round of boosting and the update of Qk (xi ). [sent-216, score-0.307]
</p><p>66 At the moment, we ﬁrst try a large K and decide the right number as the number of visually heterogeneous clusters obtained (See Section 4). [sent-219, score-0.242]
</p><p>67 When the classiﬁers start from wrong initial clusters and oscillate between clusters until settling down, some initial weak5  Random images and simple visual features  Pedestrian images  Image cluster centres  K=5  Face images  K=3  K=9  Figure 6: Perceptual clusters of pedestrian and face images. [sent-221, score-2.264]
</p><p>68 Clusters are found to maximise discrimination power of pedestrian and face images from random images by simple visual features. [sent-222, score-1.228]
</p><p>69 4  Experiments  We performed experiments using a set of INRIA pedestrian data [10] and PIE face data [9]. [sent-225, score-0.524]
</p><p>70 The INRIA set contains 618 pedestrian images as a positive class and 2436 random images as a negative class in training and 589 pedestrian and 9030 random images in testing. [sent-226, score-1.437]
</p><p>71 The pedestrian images show wide-variations in background, human pose and shapes, clothes and illuminations (Figure 6). [sent-227, score-0.627]
</p><p>72 The PIE data set involves 900 face images as a positive class (20 persons, 9 poses and 5 lighting conditions) and 2436 random images as a negative class in training and 900 face and 12180 random images in testing. [sent-228, score-1.435]
</p><p>73 The 9 poses are distributed form left proﬁle to right proﬁle of face, and the 5 lighting conditions make sharp changes on face appearance as shown in Figure 6. [sent-229, score-0.323]
</p><p>74 Avoiding the case that any of the k-means clusters is too small (or zero) in size has helped quick convergence in the proposed method. [sent-234, score-0.273]
</p><p>75 For all cases, every classiﬁer converged within 50 boosting rounds. [sent-237, score-0.247]
</p><p>76 The object images were partitioned into K clusters (or classiﬁers) by assigning them to the classiﬁer that has the highest Pk (x). [sent-239, score-0.603]
</p><p>77 For the given pedestrian images, the ﬁrst three cluster centres look unique and the last two are rather redundant. [sent-240, score-0.394]
</p><p>78 They emphasise the direction of intensity changes at contours of the human body as discriminating cues of pedestrian images from random images. [sent-242, score-0.543]
</p><p>79 For the PIE data set, the obtained face clusters reﬂect both pose and illumination changes, which is somewhat different from our initial expectation of getting purely pose-wise clusters as the case in Figure 1. [sent-244, score-0.883]
</p><p>80 This result is, however, also reasonable when considering the strong illumination conditions that cause shadowing of face parts. [sent-245, score-0.319]
</p><p>81 All 9 face clusters seem to capture unique characteristics of the face images. [sent-250, score-0.724]
</p><p>82 5  Figure 7: ROC curves for the pedestrian data (top four) and face data (bottom four). [sent-333, score-0.524]
</p><p>83 MCBoost is also much superior to AdaBoost method learnt with manual pose label (bottom right). [sent-335, score-0.278]
</p><p>84 Boosting classiﬁers were individually learnt by the positive samples of each cluster and all negative samples in AdaBoost method. [sent-340, score-0.35]
</p><p>85 The clusters obtained by the k-means method were exploited as the initialisation in MCBoost method. [sent-341, score-0.423]
</p><p>86 For the PIE data set, we also performed data partitioning by the manual pose label and learnt boosting classiﬁers separately for each pose in AdaBoost method. [sent-342, score-0.63]
</p><p>87 For both pedestrian and face experiments and all different number of classiﬁers K, MCBoost signiﬁcantly outperformed AdaBoost method by ﬁnding optimal data clusters and associated feature sets. [sent-343, score-0.807]
</p><p>88 Our method is also much superior to the Adaboost learnt with manual pose labels (bottom right). [sent-344, score-0.278]
</p><p>89 In the AdaBoost method, increasing number of clusters deteriorated the accuracy for the pedestrian data, whereas it increased the performance for the face data. [sent-345, score-0.766]
</p><p>90 We observed in Figure 6 that there are only three heterogenous pedestrian clusters while there are more than nine face clusters. [sent-347, score-0.766]
</p><p>91 a larger K) causes perFigure 8: Example pedestrian detection result. [sent-350, score-0.396]
</p><p>92 2  Figure 8 shows an example pedestrian detection result. [sent-357, score-0.396]
</p><p>93 5  Conclusions  We have introduced a discriminative co-clustering problem of images and visual features and have proposed a method of multiple classiﬁer boosting called MCBoost. [sent-361, score-0.774]
</p><p>94 It simultaneously learns image clusters and boosting classiﬁers, each of which has expertise on an image cluster. [sent-362, score-0.729]
</p><p>95 The method works well with either random initialisation or initialisation by conventional unsupervised clustering 7  methods. [sent-363, score-0.431]
</p><p>96 We have shown in the experiments that the proposed method yields perceptual co-clusters of images and features. [sent-364, score-0.352]
</p><p>97 In object detection tasks, it signiﬁcantly outperforms two conventional designs that individually learn multiple boosting classiﬁers by the clusters obtained by the k-means clustering method and pose-labels. [sent-365, score-0.933]
</p><p>98 Learning with a more exhaustive training set would improve the performance of the method in object detection tasks. [sent-369, score-0.282]
</p><p>99 Freeman, Sharing visual features for multiclass and multiview object detection, IEEE Trans. [sent-402, score-0.302]
</p><p>100 Zhang, Floatboost learning and statistical face detection, IEEE Trans. [sent-413, score-0.241]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mcboost', 0.353), ('pedestrian', 0.283), ('boosting', 0.247), ('clusters', 0.242), ('face', 0.241), ('images', 0.233), ('hkt', 0.228), ('classi', 0.224), ('ers', 0.215), ('wki', 0.187), ('adaboost', 0.155), ('er', 0.154), ('moe', 0.145), ('initialisation', 0.14), ('object', 0.128), ('detection', 0.113), ('pose', 0.111), ('maximise', 0.109), ('visual', 0.099), ('classifier', 0.093), ('learnt', 0.085), ('pk', 0.083), ('pie', 0.078), ('perceptual', 0.078), ('kt', 0.076), ('features', 0.075), ('expertise', 0.073), ('xi', 0.07), ('experts', 0.068), ('image', 0.066), ('qk', 0.065), ('bbb', 0.062), ('xor', 0.062), ('lighting', 0.059), ('conventional', 0.058), ('cluster', 0.057), ('samples', 0.057), ('centres', 0.054), ('negative', 0.054), ('weights', 0.053), ('clustering', 0.052), ('hk', 0.052), ('multiple', 0.052), ('faces', 0.048), ('xo', 0.047), ('illumination', 0.047), ('misclassi', 0.043), ('anyboost', 0.042), ('blobs', 0.042), ('cipolla', 0.042), ('dichotomise', 0.042), ('manual', 0.041), ('method', 0.041), ('viola', 0.04), ('positive', 0.04), ('class', 0.039), ('facial', 0.039), ('risk', 0.038), ('round', 0.037), ('sidney', 0.036), ('speeding', 0.036), ('learns', 0.035), ('partitioning', 0.035), ('inria', 0.035), ('correctly', 0.034), ('false', 0.033), ('sussex', 0.033), ('frontal', 0.033), ('expert', 0.032), ('huge', 0.032), ('strong', 0.031), ('mouth', 0.031), ('int', 0.031), ('nicely', 0.031), ('helped', 0.031), ('maximising', 0.031), ('wrong', 0.03), ('discrimination', 0.03), ('sample', 0.03), ('aggregating', 0.029), ('negatives', 0.028), ('imagery', 0.028), ('discriminating', 0.027), ('settle', 0.027), ('moved', 0.027), ('discriminative', 0.027), ('yi', 0.026), ('le', 0.026), ('decision', 0.026), ('eye', 0.025), ('nds', 0.025), ('pro', 0.024), ('torralba', 0.024), ('positives', 0.024), ('weight', 0.024), ('iterates', 0.023), ('poses', 0.023), ('rounds', 0.023), ('circle', 0.023), ('assigns', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="130-tfidf-1" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>Author: Tae-kyun Kim, Roberto Cipolla</p><p>Abstract: We present a new co-clustering problem of images and visual features. The problem involves a set of non-object images in addition to a set of object images and features to be co-clustered. Co-clustering is performed in a way that maximises discrimination of object images from non-object images, thus emphasizing discriminative features. This provides a way of obtaining perceptual joint-clusters of object images and features. We tackle the problem by simultaneously boosting multiple strong classiﬁers which compete for images by their expertise. Each boosting classiﬁer is an aggregation of weak-learners, i.e. simple visual features. The obtained classiﬁers are useful for object detection tasks which exhibit multimodalities, e.g. multi-category and multi-view object detection tasks. Experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classiﬁers in object detection tasks. 1</p><p>2 0.17756696 <a title="130-tfidf-2" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>Author: Geremy Heitz, Stephen Gould, Ashutosh Saxena, Daphne Koller</p><p>Abstract: One of the original goals of computer vision was to fully understand a natural scene. This requires solving several sub-problems simultaneously, including object detection, region labeling, and geometric reasoning. The last few decades have seen great progress in tackling each of these problems in isolation. Only recently have researchers returned to the difﬁcult task of considering them jointly. In this work, we consider learning a set of related models in such that they both solve their own problem and help each other. We develop a framework called Cascaded Classiﬁcation Models (CCM), where repeated instantiations of these classiﬁers are coupled by their input/output variables in a cascade that improves performance at each level. Our method requires only a limited “black box” interface with the models, allowing us to use very sophisticated, state-of-the-art classiﬁers without having to look under the hood. We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d reconstruction. 1</p><p>3 0.14222376 <a title="130-tfidf-3" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>Author: Kate Saenko, Trevor Darrell</p><p>Abstract: Polysemy is a problem for methods that exploit image search engines to build object category models. Existing unsupervised approaches do not take word sense into consideration. We propose a new method that uses a dictionary to learn models of visual word sense from a large collection of unlabeled web data. The use of LDA to discover a latent sense space makes the model robust despite the very limited nature of dictionary deﬁnitions. The deﬁnitions are used to learn a distribution in the latent space that best represents a sense. The algorithm then uses the text surrounding image links to retrieve images with high probability of a particular dictionary sense. An object classiﬁer is trained on the resulting sense-speciﬁc images. We evaluate our method on a dataset obtained by searching the web for polysemous words. Category classiﬁcation experiments show that our dictionarybased approach outperforms baseline methods. 1</p><p>4 0.13530883 <a title="130-tfidf-4" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>Author: Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples. We propose to allow the categorylearner to strategically choose what annotations it receives—based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation. We construct a multiple-instance discriminative classiﬁer based on the initial training data. Then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next. After each request, the current classiﬁer is incrementally updated. Unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e.g., a full segmentation on some images and a present/absent ﬂag on others). As a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort. 1</p><p>5 0.13301592 <a title="130-tfidf-5" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>Author: Phil Long, Rocco Servedio</p><p>Abstract: In recent work Long and Servedio [LS05] presented a “martingale boosting” algorithm that works by constructing a branching program over weak classiﬁers and has a simple analysis based on elementary properties of random walks. [LS05] showed that this martingale booster can tolerate random classiﬁcation noise when it is run with a noise-tolerant weak learner; however, a drawback of the algorithm is that it is not adaptive, i.e. it cannot effectively take advantage of variation in the quality of the weak classiﬁers it receives. We present an adaptive variant of the martingale boosting algorithm. This adaptiveness is achieved by modifying the original algorithm so that the random walks that arise in its analysis have different step size depending on the quality of the weak learner at each stage. The new algorithm inherits the desirable properties of the original [LS05] algorithm, such as random classiﬁcation noise tolerance, and has other advantages besides adaptiveness: it requires polynomially fewer calls to the weak learner than the original algorithm, and it can be used with conﬁdencerated weak hypotheses that output real values rather than Boolean predictions.</p><p>6 0.12763135 <a title="130-tfidf-6" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>7 0.12061167 <a title="130-tfidf-7" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>8 0.11915867 <a title="130-tfidf-8" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>9 0.11132906 <a title="130-tfidf-9" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>10 0.10723001 <a title="130-tfidf-10" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>11 0.10417192 <a title="130-tfidf-11" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>12 0.097714439 <a title="130-tfidf-12" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>13 0.096752428 <a title="130-tfidf-13" href="./nips-2008-Cyclizing_Clusters_via_Zeta_Function_of_a_Graph.html">55 nips-2008-Cyclizing Clusters via Zeta Function of a Graph</a></p>
<p>14 0.096009776 <a title="130-tfidf-14" href="./nips-2008-Implicit_Mixtures_of_Restricted_Boltzmann_Machines.html">103 nips-2008-Implicit Mixtures of Restricted Boltzmann Machines</a></p>
<p>15 0.095442623 <a title="130-tfidf-15" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>16 0.095115036 <a title="130-tfidf-16" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>17 0.093663126 <a title="130-tfidf-17" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>18 0.086338572 <a title="130-tfidf-18" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>19 0.082300439 <a title="130-tfidf-19" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>20 0.082204819 <a title="130-tfidf-20" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.223), (1, -0.149), (2, 0.058), (3, -0.136), (4, -0.065), (5, 0.056), (6, -0.04), (7, -0.212), (8, 0.068), (9, -0.003), (10, 0.117), (11, 0.035), (12, 0.018), (13, -0.089), (14, -0.122), (15, 0.028), (16, -0.109), (17, -0.115), (18, 0.114), (19, -0.037), (20, 0.076), (21, -0.038), (22, 0.042), (23, 0.097), (24, -0.031), (25, 0.019), (26, -0.031), (27, -0.115), (28, -0.076), (29, -0.033), (30, 0.15), (31, 0.046), (32, 0.09), (33, -0.057), (34, 0.107), (35, -0.026), (36, -0.133), (37, 0.08), (38, 0.021), (39, -0.009), (40, 0.069), (41, -0.039), (42, -0.102), (43, -0.062), (44, 0.04), (45, -0.024), (46, -0.009), (47, -0.032), (48, 0.04), (49, 0.009)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96919709 <a title="130-lsi-1" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>Author: Tae-kyun Kim, Roberto Cipolla</p><p>Abstract: We present a new co-clustering problem of images and visual features. The problem involves a set of non-object images in addition to a set of object images and features to be co-clustered. Co-clustering is performed in a way that maximises discrimination of object images from non-object images, thus emphasizing discriminative features. This provides a way of obtaining perceptual joint-clusters of object images and features. We tackle the problem by simultaneously boosting multiple strong classiﬁers which compete for images by their expertise. Each boosting classiﬁer is an aggregation of weak-learners, i.e. simple visual features. The obtained classiﬁers are useful for object detection tasks which exhibit multimodalities, e.g. multi-category and multi-view object detection tasks. Experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classiﬁers in object detection tasks. 1</p><p>2 0.73653084 <a title="130-lsi-2" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>Author: Daphna Weinshall, Hynek Hermansky, Alon Zweig, Jie Luo, Holly Jimison, Frank Ohl, Misha Pavel</p><p>Abstract: Unexpected stimuli are a challenge to any machine learning algorithm. Here we identify distinct types of unexpected events, focusing on ’incongruent events’ when ’general level’ and ’speciﬁc level’ classiﬁers give conﬂicting predictions. We deﬁne a formal framework for the representation and processing of incongruent events: starting from the notion of label hierarchy, we show how partial order on labels can be deduced from such hierarchies. For each event, we compute its probability in different ways, based on adjacent levels (according to the partial order) in the label hierarchy. An incongruent event is an event where the probability computed based on some more speciﬁc level (in accordance with the partial order) is much smaller than the probability computed based on some more general level, leading to conﬂicting predictions. We derive algorithms to detect incongruent events from different types of hierarchies, corresponding to class membership or part membership. Respectively, we show promising results with real data on two speciﬁc problems: Out Of Vocabulary words in speech recognition, and the identiﬁcation of a new sub-class (e.g., the face of a new individual) in audio-visual facial object recognition.</p><p>3 0.71687269 <a title="130-lsi-3" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>Author: Geremy Heitz, Stephen Gould, Ashutosh Saxena, Daphne Koller</p><p>Abstract: One of the original goals of computer vision was to fully understand a natural scene. This requires solving several sub-problems simultaneously, including object detection, region labeling, and geometric reasoning. The last few decades have seen great progress in tackling each of these problems in isolation. Only recently have researchers returned to the difﬁcult task of considering them jointly. In this work, we consider learning a set of related models in such that they both solve their own problem and help each other. We develop a framework called Cascaded Classiﬁcation Models (CCM), where repeated instantiations of these classiﬁers are coupled by their input/output variables in a cascade that improves performance at each level. Our method requires only a limited “black box” interface with the models, allowing us to use very sophisticated, state-of-the-art classiﬁers without having to look under the hood. We demonstrate the effectiveness of our method on a large set of natural images by combining the subtasks of scene categorization, object detection, multiclass image segmentation, and 3d reconstruction. 1</p><p>4 0.64401197 <a title="130-lsi-4" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>Author: Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples. We propose to allow the categorylearner to strategically choose what annotations it receives—based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation. We construct a multiple-instance discriminative classiﬁer based on the initial training data. Then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next. After each request, the current classiﬁer is incrementally updated. Unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e.g., a full segmentation on some images and a present/absent ﬂag on others). As a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort. 1</p><p>5 0.6354273 <a title="130-lsi-5" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>Author: Phil Long, Rocco Servedio</p><p>Abstract: In recent work Long and Servedio [LS05] presented a “martingale boosting” algorithm that works by constructing a branching program over weak classiﬁers and has a simple analysis based on elementary properties of random walks. [LS05] showed that this martingale booster can tolerate random classiﬁcation noise when it is run with a noise-tolerant weak learner; however, a drawback of the algorithm is that it is not adaptive, i.e. it cannot effectively take advantage of variation in the quality of the weak classiﬁers it receives. We present an adaptive variant of the martingale boosting algorithm. This adaptiveness is achieved by modifying the original algorithm so that the random walks that arise in its analysis have different step size depending on the quality of the weak learner at each stage. The new algorithm inherits the desirable properties of the original [LS05] algorithm, such as random classiﬁcation noise tolerance, and has other advantages besides adaptiveness: it requires polynomially fewer calls to the weak learner than the original algorithm, and it can be used with conﬁdencerated weak hypotheses that output real values rather than Boolean predictions.</p><p>6 0.60562974 <a title="130-lsi-6" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>7 0.59077501 <a title="130-lsi-7" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>8 0.57612264 <a title="130-lsi-8" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>9 0.57581353 <a title="130-lsi-9" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>10 0.53996861 <a title="130-lsi-10" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>11 0.50260788 <a title="130-lsi-11" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>12 0.4980934 <a title="130-lsi-12" href="./nips-2008-Cyclizing_Clusters_via_Zeta_Function_of_a_Graph.html">55 nips-2008-Cyclizing Clusters via Zeta Function of a Graph</a></p>
<p>13 0.4887926 <a title="130-lsi-13" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>14 0.4791753 <a title="130-lsi-14" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>15 0.4660103 <a title="130-lsi-15" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>16 0.45616016 <a title="130-lsi-16" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>17 0.41737673 <a title="130-lsi-17" href="./nips-2008-Implicit_Mixtures_of_Restricted_Boltzmann_Machines.html">103 nips-2008-Implicit Mixtures of Restricted Boltzmann Machines</a></p>
<p>18 0.41424233 <a title="130-lsi-18" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>19 0.40672851 <a title="130-lsi-19" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<p>20 0.4006817 <a title="130-lsi-20" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.073), (7, 0.081), (12, 0.054), (28, 0.135), (57, 0.099), (59, 0.011), (63, 0.021), (71, 0.02), (77, 0.05), (78, 0.022), (83, 0.093), (93, 0.237)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.76814008 <a title="130-lda-1" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>Author: Tae-kyun Kim, Roberto Cipolla</p><p>Abstract: We present a new co-clustering problem of images and visual features. The problem involves a set of non-object images in addition to a set of object images and features to be co-clustered. Co-clustering is performed in a way that maximises discrimination of object images from non-object images, thus emphasizing discriminative features. This provides a way of obtaining perceptual joint-clusters of object images and features. We tackle the problem by simultaneously boosting multiple strong classiﬁers which compete for images by their expertise. Each boosting classiﬁer is an aggregation of weak-learners, i.e. simple visual features. The obtained classiﬁers are useful for object detection tasks which exhibit multimodalities, e.g. multi-category and multi-view object detection tasks. Experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classiﬁers in object detection tasks. 1</p><p>2 0.75769192 <a title="130-lda-2" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>Author: Thomas L. Griffiths, Joseph L. Austerweil</p><p>Abstract: Almost all successful machine learning algorithms and cognitive models require powerful representations capturing the features that are relevant to a particular problem. We draw on recent work in nonparametric Bayesian statistics to deﬁne a rational model of human feature learning that forms a featural representation from raw sensory data without pre-specifying the number of features. By comparing how the human perceptual system and our rational model use distributional and category information to infer feature representations, we seek to identify some of the forces that govern the process by which people separate and combine sensory primitives to form features. 1</p><p>3 0.67398554 <a title="130-lda-3" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>4 0.66662478 <a title="130-lda-4" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>5 0.66532224 <a title="130-lda-5" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>6 0.66262311 <a title="130-lda-6" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>7 0.66229534 <a title="130-lda-7" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>8 0.66192949 <a title="130-lda-8" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>9 0.66176814 <a title="130-lda-9" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>10 0.65756297 <a title="130-lda-10" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>11 0.65673667 <a title="130-lda-11" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>12 0.65627766 <a title="130-lda-12" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>13 0.65453827 <a title="130-lda-13" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>14 0.65325224 <a title="130-lda-14" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>15 0.65270489 <a title="130-lda-15" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>16 0.65267152 <a title="130-lda-16" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>17 0.6501019 <a title="130-lda-17" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>18 0.64979059 <a title="130-lda-18" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>19 0.64958841 <a title="130-lda-19" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>20 0.64947927 <a title="130-lda-20" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
