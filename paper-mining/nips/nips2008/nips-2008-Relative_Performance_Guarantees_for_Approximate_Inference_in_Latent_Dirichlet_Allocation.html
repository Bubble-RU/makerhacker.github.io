<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-197" href="#">nips2008-197</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</h1>
<br/><p>Source: <a title="nips-2008-197-pdf" href="http://papers.nips.cc/paper/3455-relative-performance-guarantees-for-approximate-inference-in-latent-dirichlet-allocation.pdf">pdf</a></p><p>Author: Indraneel Mukherjee, David M. Blei</p><p>Abstract: Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean ﬁeld variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k − 1) + log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation. 1</p><p>Reference: <a title="nips-2008-197-reference" href="../nips2008_reference/nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. [sent-5, score-0.414]
</p><p>2 We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean ﬁeld variational inference (VB) in latent Dirichlet allocation. [sent-8, score-0.7]
</p><p>3 We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k − 1) + log m/m, where k is the number of topics in the model and m is the number of words in a document. [sent-9, score-0.522]
</p><p>4 As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. [sent-10, score-0.177]
</p><p>5 A topic model posits a generative probabilistic process of a document collection using a small number of distributions over words, which are called topics. [sent-16, score-0.314]
</p><p>6 Conditioned on the observed documents, the distribution of the underlying latent variables is inferred to probabilistically partition the data according to their hidden themes. [sent-17, score-0.107]
</p><p>7 Research in topic models has involved tailoring the latent structure to new kinds of data and designing new posterior inference algortihms to infer that latent structure. [sent-18, score-0.504]
</p><p>8 In generative models, such as latent Dirichlet allocation (LDA) and its extensions, inferring the posterior of the latent variables is intractable [3, 4]. [sent-19, score-0.315]
</p><p>9 (Some topic models, such as LSI and pLSI, are not fully generative. [sent-20, score-0.151]
</p><p>10 ) Several algorithms have emerged in recent years to approximate such posteriors, including mean-ﬁeld variational inference [3], expectation propagation [20], collapsed Gibbs sampling [19] and, most recently, collapsed variational inference [21]. [sent-21, score-0.698]
</p><p>11 There has been some empirical comparison in the topic modeling literature [4, 19], but little theoretical guidance. [sent-23, score-0.165]
</p><p>12 We analyze two variational inference algorithms for topic models, mean ﬁeld variational inference (VB) [3] and collapsed variational inference (CVB) [21]. [sent-25, score-0.979]
</p><p>13 “Collapsing,” or marginalizing out, a latent variable is a known technique for speeding up the convergence of Gibbs samplers, and CVB brought this idea to the world of variational algorithms. [sent-26, score-0.304]
</p><p>14 Variational algorithms minimize the distance between a simple distribution of the latent variables and the true posterior. [sent-29, score-0.107]
</p><p>15 We prove that the uncollapsed variational bound on the log probability of a document approaches the collapsed variational bound as the number of words in the document increases. [sent-31, score-0.896]
</p><p>16 This supports the empirical improvement observed for LDA, where documents are relatively short, and the smaller improvement observed in the DP mixture, which is akin to inference in a single long document. [sent-32, score-0.177]
</p><p>17 We also show how the number of topics and the sparsity of those topics affects the performance of the two algorithms. [sent-33, score-0.407]
</p><p>18 We prove that the difference between the two bounds decreases as O(k − 1) + log m/m, where k is the number of topics in the model, and m is the number of words in the document. [sent-34, score-0.371]
</p><p>19 We examine the consequences of the theory on both simulated and real text data, exploring the relative advantage of CVB under different document lengths, topic sparsities, and numbers of topics. [sent-36, score-0.433]
</p><p>20 The consequences of our theory lead to practical guidelines for choosing an appropriate variational algorithm. [sent-37, score-0.234]
</p><p>21 2  Posterior inference for latent Dirichlet allocation  Latent Dirichlet allocation (LDA) is a model of an observed corpus of documents. [sent-38, score-0.253]
</p><p>22 Each document is a collection of m words x1:m , where each word is from a ﬁxed vocabulary χ of size N . [sent-39, score-0.288]
</p><p>23 The topic matrix β denotes the N × k matrix whose columns are the topic distributions. [sent-44, score-0.302]
</p><p>24 Given the topic matrix and Dirichlet parameters, LDA assumes that each document arises from the following process. [sent-45, score-0.295]
</p><p>25 Then, for each word choose a topic assignment zi ∼ θ. [sent-47, score-0.246]
</p><p>26 This describes a joint probability distribution of the observed and latent variables p(x, z, θ|α, β). [sent-49, score-0.107]
</p><p>27 In parameter estimation, we ﬁnd the topics and Dirichlet parameters that maximize the likelihood of an observed corpus. [sent-51, score-0.195]
</p><p>28 In posterior inference, we ﬁx the model and compute the posterior distribution of the latent structure that underlies a particular document. [sent-52, score-0.215]
</p><p>29 (Parameter estimation crucially depends on posterior inference via the expectation-maximization algorithm. [sent-54, score-0.109]
</p><p>30 ) Given a document x, the posterior distribution of the latent variables is p(θ, z|x) = p(θ,z,x) . [sent-55, score-0.305]
</p><p>31 Variational methods approximate an intractable posterior by ﬁnding the member of a simpler family of distributions that is closest to it, where closeness is measured by relative entropy. [sent-60, score-0.107]
</p><p>32 Variational inference for LDA In the variational inference algorithm for LDA introduced in [3] (VB), the posterior p(θ, z|x) is approximated by a fully-factorized variational distribution q(θ, z|γ, φ1:m ) = q(θ|γ) 2  i  q(zi |φi ). [sent-63, score-0.524]
</p><p>33 Here q(θ|γ) is a Dirichlet distribution with parameters γ, and each q(zi |φi ) is a multinomial distribution on the set of K topic indices. [sent-64, score-0.151]
</p><p>34 In the true posterior, the latent variables are dependent; in this family of distributions, they are independent [3]. [sent-66, score-0.126]
</p><p>35 The algorithm seeks to ﬁnd the variational parameters that minimize the relative entropy between the true posterior and the approximation, RE(q(θ, z|γ, φ1:m ) p(θ, z|x)). [sent-67, score-0.264]
</p><p>36 The expression minimized by γ∗ , φ∗ is also known as the variational free energy of (γ, φ1:m ) and 1:m will be denoted by F(x, γ, φ1:m ). [sent-69, score-0.466]
</p><p>37 (1)  γ,φ1:m  Collapsed variational inference for LDA The collapsed variational inference algorithm (CVB) reformulates the LDA model by marginalizing out the topic proportions θ. [sent-73, score-0.753]
</p><p>38 This yields a formulation where the topic assignments z are fully dependent, but where the dimensionality of the latent space has been reduced. [sent-74, score-0.258]
</p><p>39 The variational family in CVB is a fully-factorized product of multinomial distributions, q(zi |φi ). [sent-75, score-0.199]
</p><p>40 q(z) = i  CVB ﬁnds the optimal variational parameters φ∗ as follows: 1:m φ∗ = arg min Eq(z|φ1:m ) log 1:m φ1:m  q(z|φ1:m ) p(z, x)  . [sent-76, score-0.216]
</p><p>41 It approximates the negative log probability of x with the collapsed variational free energy F(x, γ), which is the expression that φ∗ minimizes. [sent-77, score-0.601]
</p><p>42 Efﬁciency of the algorithms Both VB and CVB proceed by coordinate ascent to reach a local minimum of their respective free energies. [sent-81, score-0.171]
</p><p>43 Each coordinate update for VB requires in O(mk) time, where m is the length of a document and k is the number of topics. [sent-83, score-0.209]
</p><p>44 1 It is thus inappropriate for computing held out probability, a typical measure of quality of a topic model. [sent-88, score-0.151]
</p><p>45 Our main result states that for sufﬁciently large documents, the difference in approximation quality decreases with document length and converges to a constant that depends on the number of topics. [sent-91, score-0.247]
</p><p>46 Consider any LDA model with k topics, and a document consisting of m words x1 , . [sent-95, score-0.2]
</p><p>47 Recall that VB(x) and CVB(x), deﬁned in (1) and (2), are the free energies measured by VB and CVB respectively. [sent-99, score-0.216]
</p><p>48 In previous work on analyzing mean-ﬁeld variational inference, [24] analyze the performance of VB for posterior inference in a Gaussian mixture model. [sent-103, score-0.329]
</p><p>49 Concerning this quantity, the following corollary is immediate because the total free energy scales with the length of the document. [sent-106, score-0.375]
</p><p>50 The per word free energy change, as well as the percentage free energy change, between VB and CVB goes to zero with the length of the document. [sent-108, score-0.722]
</p><p>51 The bounds on the difference in free energy is equivalent to a bound on the ratio of probability obtained by VB and CVB. [sent-110, score-0.352]
</p><p>52 Since the probability of a document falls exponentially fast with the number of words, the additive difference in the probability estimates of VB and CVB is again negligible for large documents. [sent-111, score-0.189]
</p><p>53 When all topics are uniform distributions, the difference in the free energy estimates is Ω(k) for long documents. [sent-115, score-0.545]
</p><p>54 Then, VB(x) − CVB(x) ≤  Eq(z) [log Γ(mj + αj )] − log Γ(γj + αj )  (4)  z  where γj =  i qi (Zi  = j), ∀j ∈ [k], and mj is the number of occurrences of the topic j in z. [sent-134, score-0.291]
</p><p>55 Note that to analyze the term Eq(z) [log Γ(mj + αj )] corresponding to a particular topic j, we need consider only those positions i where qi (Zi = j) = 0; we denote the number of such positions by Nz . [sent-135, score-0.227]
</p><p>56 The difﬁculty in analyzing arbitrary documents lay in working with the right hand side of (4) without any prior knowledge about the qi ’s. [sent-136, score-0.171]
</p><p>57 As an immediate corollary of the previous two lemmas and the fact that log Γ is convex, we get VB(x) − CVB(x) ≤  E[log Γ(mj + αj )] − log Γ(γj + αj ). [sent-153, score-0.113]
</p><p>58 025  100  free energy change  50  40 30  0  1000  2000  3000  4000  5000  0  0  0. [sent-156, score-0.312]
</p><p>59 01  70  βparam = 1e−04  0  1000  2000  # words  3000  4000  5000  0  1000  2000  # words  3000  4000  5000  # words  (a) Difference in total free energy estimates βparam = 0. [sent-162, score-0.487]
</p><p>60 0015  βparam = 1e−04  0  1000  2000  # words  3000  4000  5000  0  # words  1000  2000  3000  4000  # words  (b) Percentage difference in free energy estimates  Figure 1: Results on synthetic text data. [sent-178, score-0.565]
</p><p>61 We sample k topics from a symmetric Dirichlet distribution with parameter βparam . [sent-179, score-0.195]
</p><p>62 We sample 10 documents from LDA models with these topics. [sent-180, score-0.118]
</p><p>63 For each preﬁx length, the VB and CVB free energies are averaged over the 10 documents. [sent-182, score-0.216]
</p><p>64 The curves obtained show how the advantage of CVB over VB changes with the length of a document, number of topics and sparsity of topics. [sent-183, score-0.299]
</p><p>65 The requirement of m > 1/q 2+o(1) is necessary, and translates to the condition that document 2+o(1) lengths be greater than (Nj /γj ) for Theorem 1 to hold. [sent-191, score-0.181]
</p><p>66 This gives an implicit lower bound on the required length of a document which depends on the sparsity of the topics. [sent-192, score-0.23]
</p><p>67 , low entropy, and dense topics spread their mass on more words, i. [sent-195, score-0.219]
</p><p>68 When the vocabulary is large, dense topics require long documents for the theory to take effect. [sent-198, score-0.381]
</p><p>69 We then sampled a corpus of 10 documents, each of length 5000 from an LDA model with these topics and Dirichlet hyper-parameter 1/k. [sent-208, score-0.276]
</p><p>70 For every subdocument length, the average converged values of the free energy was recorded for both algorithms. [sent-212, score-0.286]
</p><p>71 Moreover, we investigated the dependence of the advantage on topic sparsity. [sent-215, score-0.19]
</p><p>72 We repeat the above experiment, with three different values of the Dirichlet parameter βparam for the topic matrix. [sent-216, score-0.151]
</p><p>73 The advantage decreases with document length m and increases with the number of topics k. [sent-220, score-0.45]
</p><p>74 The theory predicts that the difference in free energy converges to a constant, implying that the percentage advantage decays as O(1)/m. [sent-221, score-0.404]
</p><p>75 Finally, for denser topic models the performances of CVB and VB converge only for very long documents, as was discussed at the end of Section 3. [sent-225, score-0.185]
</p><p>76 The Yale Law test corpus contained 200 documents of lengths between a thousand and 10, 000 words. [sent-236, score-0.173]
</p><p>77 For each data set, we ﬁt LDA models of different numbers of topics to the training corpus (k = 5, 10, 25, 50), and then evaluated the model on the held-out test set. [sent-237, score-0.243]
</p><p>78 In Figure 2, we plot the percentage difference of the per-word variational free energies achieved by CVB and VB as a function of document length and number of topics. [sent-238, score-0.665]
</p><p>79 We also plot the difference in the total free energy. [sent-239, score-0.218]
</p><p>80 As for the simulated data, the graphs match our theory; the percent decrease in per word free energy goes to zero with increasing document length, and the absolute difference approaches a constant. [sent-240, score-0.55]
</p><p>81 The difference is more pronounced as the number of topics increases. [sent-241, score-0.226]
</p><p>82 The predicted trends occur even for short documents containing around a hundred words. [sent-242, score-0.117]
</p><p>83 The issues seen with dense topics on simulated data are not relevant for real-world applications. [sent-244, score-0.238]
</p><p>84 5  Conclusion  We have provided a theoretical analysis of the relative performance of the two variational inference algorithms for LDA. [sent-245, score-0.265]
</p><p>85 We showed that the advantage of CVB decreases as document length increases, and increases with the number of topics and density of the topic distributions. [sent-246, score-0.601]
</p><p>86 Unlike previous analyses of variational methods, our theorem does not require that the observed data arise from the assumed model. [sent-248, score-0.196]
</p><p>87 Since the approximation to the likelihood based on CVB is more expensive to compute than for VB, this theory can inform our choice of a good variational approximation. [sent-249, score-0.18]
</p><p>88 Shorter documents and models with more topics lend themselves to analysis with CVB. [sent-250, score-0.338]
</p><p>89 Longer documents and models with fewer topics lend themselves to VB. [sent-251, score-0.338]
</p><p>90 We ﬁt LDA models with numbers of topics equal to 5, 10, 25, 50, and evaluated the models on a held-out corpus. [sent-254, score-0.225]
</p><p>91 We plot the percentage difference of the per-word variational free energies achieved by CVB and VB as a function of document length. [sent-255, score-0.617]
</p><p>92 We also plot the difference in the total free energy. [sent-256, score-0.218]
</p><p>93 The %-age decrease in per word free energy goes to zero with increasing document length, and the absolute difference approaches a constant. [sent-257, score-0.531]
</p><p>94 )  0  20  40  60  80  100  120  140  0  20  40  60  #words  80  100  120  140  #words  (a) ArXiv data-set VB − CVB: total free energies (1000 mov. [sent-269, score-0.235]
</p><p>95 )  2000  4000  6000  8000  10000  #words  2000  4000  6000  8000  10000  #words  (b) Yale Law data-set  In one strain of future work, we will analyze the consequences of the approximate posterior inference algorithm on parameter estimation. [sent-280, score-0.156]
</p><p>96 Our results regarding the sparsity of topics indicate that CVB is a better algorithm early in the EM algorithm, when topics are dense, and that VB will be more efﬁcient as the ﬁtted topics become more sparse. [sent-281, score-0.602]
</p><p>97 The author-recipient-topic model for topic and role discovery in social networks: Experiments with Enron and academic email. [sent-328, score-0.166]
</p><p>98 Modeling general and speciﬁc aspects of documents with a probabilistic topic model. [sent-383, score-0.273]
</p><p>99 A collapsed variational bayesian inference algorithm for latent dirichlet allocation. [sent-411, score-0.539]
</p><p>100 Stochastic complexities of gaussian mixtures in variational bayesian approximation. [sent-429, score-0.197]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('cvb', 0.704), ('vb', 0.412), ('topics', 0.195), ('variational', 0.18), ('param', 0.168), ('free', 0.154), ('topic', 0.151), ('document', 0.144), ('energy', 0.132), ('lda', 0.122), ('latent', 0.107), ('documents', 0.103), ('collapsed', 0.099), ('dirichlet', 0.098), ('energies', 0.062), ('words', 0.056), ('inference', 0.055), ('posterior', 0.054), ('qi', 0.052), ('mj', 0.052), ('length', 0.048), ('word', 0.048), ('zi', 0.047), ('text', 0.041), ('vocabulary', 0.04), ('advantage', 0.039), ('lengths', 0.037), ('log', 0.036), ('arxiv', 0.035), ('yale', 0.034), ('eq', 0.034), ('semantic', 0.033), ('corpus', 0.033), ('percentage', 0.032), ('difference', 0.031), ('diff', 0.031), ('guidelines', 0.031), ('plsi', 0.031), ('emerged', 0.03), ('indexing', 0.03), ('allocation', 0.029), ('lsi', 0.027), ('change', 0.026), ('synthetic', 0.025), ('lend', 0.025), ('landauer', 0.025), ('analyze', 0.024), ('dense', 0.024), ('decreases', 0.024), ('girolami', 0.023), ('consequences', 0.023), ('goes', 0.022), ('corollary', 0.022), ('jensen', 0.022), ('blei', 0.021), ('bound', 0.021), ('law', 0.021), ('lemmas', 0.019), ('probabilistic', 0.019), ('long', 0.019), ('family', 0.019), ('total', 0.019), ('simulated', 0.019), ('intractable', 0.018), ('pre', 0.018), ('digital', 0.018), ('age', 0.018), ('mixtures', 0.017), ('coordinate', 0.017), ('marginalizing', 0.017), ('sparsity', 0.017), ('proportions', 0.016), ('relative', 0.016), ('hierarchical', 0.016), ('lost', 0.016), ('analyzing', 0.016), ('eld', 0.016), ('implying', 0.016), ('theorem', 0.016), ('models', 0.015), ('designing', 0.015), ('gibbs', 0.015), ('princeton', 0.015), ('social', 0.015), ('prove', 0.015), ('vs', 0.015), ('estimates', 0.014), ('conference', 0.014), ('plot', 0.014), ('entropy', 0.014), ('normalizing', 0.014), ('trends', 0.014), ('theoretical', 0.014), ('bounds', 0.014), ('xm', 0.013), ('debugging', 0.013), ('enron', 0.013), ('faceted', 0.013), ('oca', 0.013), ('buntine', 0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="197-tfidf-1" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>Author: Indraneel Mukherjee, David M. Blei</p><p>Abstract: Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean ﬁeld variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k − 1) + log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation. 1</p><p>2 0.21088028 <a title="197-tfidf-2" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>Author: Jordan L. Boyd-graber, David M. Blei</p><p>Abstract: We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-speciﬁc topic weights and parse-tree-speciﬁc syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents. 1</p><p>3 0.17890713 <a title="197-tfidf-3" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>Author: Simon Lacoste-julien, Fei Sha, Michael I. Jordan</p><p>Abstract: Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in ﬁnding a reduced dimensionality representation. Speciﬁcally, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classiﬁcation. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classiﬁcation task and show how our model can identify shared topics across classes as well as class-dependent topics.</p><p>4 0.1607286 <a title="197-tfidf-4" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>5 0.13394433 <a title="197-tfidf-5" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>Author: Yi Zhang, Artur Dubrawski, Jeff G. Schneider</p><p>Abstract: In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any speciﬁc task in the same space through regularization. In an empirical study, we construct 190 different text classiﬁcation tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classiﬁcation tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the prediction model used.</p><p>6 0.13017423 <a title="197-tfidf-6" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>7 0.106975 <a title="197-tfidf-7" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>8 0.10012809 <a title="197-tfidf-8" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>9 0.097428732 <a title="197-tfidf-9" href="./nips-2008-Asynchronous_Distributed_Learning_of_Topic_Models.html">28 nips-2008-Asynchronous Distributed Learning of Topic Models</a></p>
<p>10 0.092647314 <a title="197-tfidf-10" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>11 0.083566077 <a title="197-tfidf-11" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>12 0.075758599 <a title="197-tfidf-12" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>13 0.070362054 <a title="197-tfidf-13" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>14 0.069805071 <a title="197-tfidf-14" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>15 0.069290809 <a title="197-tfidf-15" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>16 0.064277329 <a title="197-tfidf-16" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>17 0.058473628 <a title="197-tfidf-17" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>18 0.055637855 <a title="197-tfidf-18" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>19 0.055544812 <a title="197-tfidf-19" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>20 0.049477186 <a title="197-tfidf-20" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.146), (1, -0.091), (2, 0.08), (3, -0.107), (4, -0.075), (5, -0.04), (6, 0.118), (7, 0.188), (8, -0.28), (9, 0.004), (10, -0.088), (11, -0.054), (12, 0.017), (13, 0.122), (14, -0.045), (15, 0.045), (16, 0.095), (17, -0.016), (18, -0.085), (19, -0.076), (20, -0.002), (21, -0.064), (22, -0.0), (23, 0.003), (24, 0.11), (25, 0.02), (26, 0.008), (27, 0.065), (28, 0.039), (29, -0.028), (30, 0.001), (31, 0.033), (32, 0.108), (33, -0.045), (34, -0.052), (35, 0.062), (36, -0.048), (37, -0.014), (38, 0.006), (39, -0.026), (40, 0.115), (41, -0.121), (42, 0.009), (43, -0.038), (44, -0.019), (45, 0.078), (46, -0.096), (47, -0.002), (48, 0.02), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96010178 <a title="197-lsi-1" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>Author: Indraneel Mukherjee, David M. Blei</p><p>Abstract: Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean ﬁeld variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k − 1) + log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation. 1</p><p>2 0.8445791 <a title="197-lsi-2" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>Author: Jordan L. Boyd-graber, David M. Blei</p><p>Abstract: We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-speciﬁc topic weights and parse-tree-speciﬁc syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents. 1</p><p>3 0.80560225 <a title="197-lsi-3" href="./nips-2008-Asynchronous_Distributed_Learning_of_Topic_Models.html">28 nips-2008-Asynchronous Distributed Learning of Topic Models</a></p>
<p>Author: Padhraic Smyth, Max Welling, Arthur U. Asuncion</p><p>Abstract: Distributed learning is a problem of fundamental interest in machine learning and cognitive science. In this paper, we present asynchronous distributed learning algorithms for two well-known unsupervised learning frameworks: Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Processes (HDP). In the proposed approach, the data are distributed across P processors, and processors independently perform Gibbs sampling on their local data and communicate their information in a local asynchronous manner with other processors. We demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard LDA and HDP samplers, but with signiﬁcant improvements in computation time and memory. We show speedup results on a 730-million-word text corpus using 32 processors, and we provide perplexity results for up to 1500 virtual processors. As a stepping stone in the development of asynchronous HDP, a parallel HDP sampler is also introduced. 1</p><p>4 0.78817672 <a title="197-lsi-4" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>Author: Simon Lacoste-julien, Fei Sha, Michael I. Jordan</p><p>Abstract: Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in ﬁnding a reduced dimensionality representation. Speciﬁcally, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classiﬁcation. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classiﬁcation task and show how our model can identify shared topics across classes as well as class-dependent topics.</p><p>5 0.69864362 <a title="197-lsi-5" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>Author: Yik-cheung Tam, Tanja Schultz</p><p>Abstract: We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. The model is trained using efﬁcient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. We address the scalability issue to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%–8% relative perplexity reduction and 2.5% relative character error rate reduction which is statistically signiﬁcant compared to applying only unigram LSA. On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved which is also statistically signiﬁcant. 1</p><p>6 0.61035883 <a title="197-lsi-6" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>7 0.46328908 <a title="197-lsi-7" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>8 0.45155245 <a title="197-lsi-8" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>9 0.43538484 <a title="197-lsi-9" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>10 0.37346944 <a title="197-lsi-10" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>11 0.36899284 <a title="197-lsi-11" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>12 0.34766474 <a title="197-lsi-12" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>13 0.32729799 <a title="197-lsi-13" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>14 0.32626596 <a title="197-lsi-14" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>15 0.32496014 <a title="197-lsi-15" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>16 0.31155503 <a title="197-lsi-16" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>17 0.31020036 <a title="197-lsi-17" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>18 0.3072449 <a title="197-lsi-18" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>19 0.29682422 <a title="197-lsi-19" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>20 0.29002777 <a title="197-lsi-20" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.011), (6, 0.052), (7, 0.077), (12, 0.043), (15, 0.013), (28, 0.156), (52, 0.012), (57, 0.114), (59, 0.018), (63, 0.024), (71, 0.011), (74, 0.022), (75, 0.222), (77, 0.034), (78, 0.013), (83, 0.069)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.81249142 <a title="197-lda-1" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>Author: Indraneel Mukherjee, David M. Blei</p><p>Abstract: Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean ﬁeld variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k − 1) + log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation. 1</p><p>2 0.81172109 <a title="197-lda-2" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>Author: Maxim Raginsky, Svetlana Lazebnik, Rebecca Willett, Jorge Silva</p><p>Abstract: This paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions. For d covariates, there are 2d basis coefﬁcients to estimate, which renders conventional approaches computationally prohibitive when d is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error, and (2) the computational complexity is lower for sparser densities. Our method also allows for ﬂexible control of the trade-off between mean-squared error and computational complexity.</p><p>3 0.76525456 <a title="197-lda-3" href="./nips-2008-Fast_Prediction_on_a_Tree.html">84 nips-2008-Fast Prediction on a Tree</a></p>
<p>Author: Mark Herbster, Massimiliano Pontil, Sergio R. Galeano</p><p>Abstract: Given an n-vertex weighted tree with structural diameter S and a subset of m vertices, we present a technique to compute a corresponding m × m Gram matrix of the pseudoinverse of the graph Laplacian in O(n + m2 + mS) time. We discuss the application of this technique to fast label prediction on a generic graph. We approximate the graph with a spanning tree and then we predict with the kernel perceptron. We address the approximation of the graph with either a minimum spanning tree or a shortest path tree. The fast computation of the pseudoinverse enables us to address prediction problems on large graphs. We present experiments on two web-spam classiﬁcation tasks, one of which includes a graph with 400,000 vertices and more than 10,000,000 edges. The results indicate that the accuracy of our technique is competitive with previous methods using the full graph information. 1</p><p>4 0.70743775 <a title="197-lda-4" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>5 0.69783396 <a title="197-lda-5" href="./nips-2008-Using_matrices_to_model_symbolic_relationship.html">248 nips-2008-Using matrices to model symbolic relationship</a></p>
<p>Author: Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn ﬁrst-order propositions such as (2, 5) ∈ +3 or (Christopher, Penelope) ∈ has wife, and higher-order propositions such as (3, +3) ∈ plus and (+3, −3) ∈ inverse or (has husband, has wife) ∈ higher oppsex. We further demonstrate that the system understands how higher-order propositions are related to ﬁrst-order ones by showing that it can correctly answer questions about ﬁrst-order propositions involving the relations +3 or has wife even though it has not been trained on any ﬁrst-order examples involving these relations. 1</p><p>6 0.6964075 <a title="197-lda-6" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>7 0.69463211 <a title="197-lda-7" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>8 0.69445467 <a title="197-lda-8" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>9 0.69352412 <a title="197-lda-9" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>10 0.69302106 <a title="197-lda-10" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<p>11 0.69225359 <a title="197-lda-11" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>12 0.69115829 <a title="197-lda-12" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>13 0.68944407 <a title="197-lda-13" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>14 0.68787724 <a title="197-lda-14" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>15 0.68742371 <a title="197-lda-15" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>16 0.68652302 <a title="197-lda-16" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>17 0.68620586 <a title="197-lda-17" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>18 0.68512309 <a title="197-lda-18" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>19 0.68473303 <a title="197-lda-19" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>20 0.68409556 <a title="197-lda-20" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
