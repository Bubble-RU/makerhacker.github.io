<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-81" href="#">nips2008-81</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</h1>
<br/><p>Source: <a title="nips-2008-81-pdf" href="http://papers.nips.cc/paper/3522-extracting-state-transition-dynamics-from-multiple-spike-trains-with-correlated-poisson-hmm.pdf">pdf</a></p><p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>Reference: <a title="nips-2008-81-reference" href="../nips2008_reference/nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. [sent-6, score-0.151]
</p><p>2 Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. [sent-7, score-0.383]
</p><p>3 To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. [sent-8, score-0.403]
</p><p>4 The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. [sent-10, score-0.435]
</p><p>5 We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. [sent-11, score-0.128]
</p><p>6 We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. [sent-12, score-0.561]
</p><p>7 1  Introduction  Neural activities are highly non-stationary and vary from time to time according to stimuli and internal state changes. [sent-13, score-0.178]
</p><p>8 Hidden Markov Models (HMMs) have been used for segmenting spike trains into quasi-stationary states, in which the spike train is regarded as stationary, hence the statistics (e. [sent-14, score-1.231]
</p><p>9 Moreover, the PSTH approach cannot be applied to cases where we cannot align spike data to stimuli or the behaviors of animals. [sent-22, score-0.491]
</p><p>10 Previous studies using HMMs have assumed that all neural activities were independent of one another given the hidden states; hence, the models could not discriminate states whose ﬁring rates were almost the same but whose correlations among neurons were different. [sent-24, score-0.509]
</p><p>11 However, there has been reports that shows the correlation between neurons changes within a fraction of a second without modulating the ﬁring rate (e. [sent-25, score-0.311]
</p><p>12 We developed a method that enabled us to segment spike trains based on differences in neuronal correlation as well as the ﬁring rate. [sent-28, score-0.946]
</p><p>13 Treating neuronal correlations (including higher-order, and not only pairwise correlations) among multiple spike trains has been one of the central challenges in computational neuroscience. [sent-29, score-0.809]
</p><p>14 There have been approaches to calculating correlations by binarizing spike trains with small bin sizes [5, 6]. [sent-30, score-0.869]
</p><p>15 These approaches are limited to treating correlations of short bin length that includes at most one spike. [sent-31, score-0.186]
</p><p>16 Here, we introduce a multivariate Poisson distribution with a higher-order correlation structure (simply abbreviated as a correlated Poisson distribution) as the output distribution for HMMs. [sent-32, score-0.53]
</p><p>17 The correlated Poisson distribution can incorporate correlation at arbitrary time intervals. [sent-33, score-0.349]
</p><p>18 In our model, model complexity corresponds to the number of hidden states and types of correlations (we have a choice as to whether to include pairwise correlation, third-order correlation, or higher order correlation). [sent-35, score-0.332]
</p><p>19 , Akaike’s information criteria (AIC), minimum description length (MDL), and Bayesian information criteria (BIC) are based on the asymptotic assumption that only holds when a large number of data is obtained. [sent-39, score-0.124]
</p><p>20 In this study, we applied the variational Bayes (VB) method [10, 11] to HMMs whose output distribution is a correlated Poisson distribution. [sent-41, score-0.3]
</p><p>21 An optimal model structure can be determined based on tractable variational free energy, which is the upper bound of the negative marginal log-likelihood. [sent-43, score-0.192]
</p><p>22 Since the variational free energy does not need the asymptotic assumption, VB works well even when the sample size is small in practice [12]. [sent-44, score-0.273]
</p><p>23 The computation of posteriors for a correlated Poisson distribution imposes serious computational burdens. [sent-45, score-0.183]
</p><p>24 We developed an efﬁcient algorithm to calculate these by using the recurrence relationship of a multivariate Poisson distribution [13]. [sent-46, score-0.236]
</p><p>25 To the best of our knowledge, this is the ﬁrst report that has introduced VB method for a correlated Poisson distribution. [sent-47, score-0.105]
</p><p>26 Although Markov chain Monte Carlo (MCMC) methods has been applied to inferring posteriors for a correlated Poisson distribution [14], MCMC schemes are computationally demanding. [sent-48, score-0.183]
</p><p>27 We demonstrate the performance of the method on multiple spike data both on a synthesized spike train and real spike data recorded from the forebrain nucleus for the vocal control (HVC) of an anesthetized songbird. [sent-49, score-1.56]
</p><p>28 1  Method HMM with multivariate Poisson distribution  Suppose that we obtain spike trains of C neurons by using simultaneous recordings. [sent-51, score-0.889]
</p><p>29 As preprocessing, we ﬁrst discretize the spike trains with a non-overlapping window whose length is ∆ to obtain spike-count data. [sent-52, score-0.8]
</p><p>30 The number of spikes of neurons c in the tth window of the nth trial is denoted by xn,t . [sent-53, score-0.306]
</p><p>31 Hidden states yt are represented by a binary variable yk such that if the hidden state n,t at the tth window of the nth trial is k, then yk = 1; otherwise 0. [sent-56, score-0.999]
</p><p>32 At state k, the spike count is assumed to be generated according to p(xn,t |λk ), whose speciﬁc form is given in the following. [sent-57, score-0.632]
</p><p>33 We will introduce an auxiliary hidden variable, sl , l ∈ Φ ≡ {1, 2, 3, 12, 13, 23, 123} which satisﬁes x1 =s1 + s12 + s13 + s123 , x2 =s2 + s12 + s23 + s123 , x3 =s3 + s13 + s23 + s123 . [sent-61, score-0.279]
</p><p>34 x  Each sl obeys P(sl |λl ), where P(x|λ) denotes a univariate Poisson distribution: P(x|λ) = λ e−λ . [sent-62, score-0.158]
</p><p>35 λ13 + λ123 λ23 + λ123 λ3 + λ13 + λ23 + λ123 The general deﬁnition of the multivariate Poisson distribution is given using the vector, S = (s1 , s2 , . [sent-66, score-0.13]
</p><p>36 , xC )T deﬁned as x = BS follows a multivariate Poisson distribution. [sent-79, score-0.09]
</p><p>37 1 1  (1)  We can also consider only the second-order correlation model by setting B = [B1 , B2 ] and S = (s1 , s2 , s3 , s12 , s13 , s23 )T , or only the third-order correlation model by setting B = [B1 , B3 ] and S = (s1 , s2 , s3 , s123 )T . [sent-81, score-0.408]
</p><p>38 However, the computational burden can be alleviated by using recurrence relations for a multivariate Poisson distribution [13]. [sent-84, score-0.266]
</p><p>39 When we assume that the spike counts for all neurons are independent, (i. [sent-87, score-0.568]
</p><p>40 We use conjugate prior distributions for all parameters of CPHMMs, which enabled the posterior distribution to have the same form as the prior distribution. [sent-95, score-0.133]
</p><p>41 The prior distribution for initial probability distribution π and state transition matrix a is the Dirichlet distribution: K ∏ (π) (A) φ(π) = D({πk }K |{uk }K ), φ(a) = D({aik }K |{uk }K ). [sent-96, score-0.231]
</p><p>42 1, ∀j, and  The Bayesian method calculates p(θ, Z|X, M ), which is a posterior of unknown parameters and hidden variable set Z = (Y, S) given the data and model structure, M (in our case, this indicates the number of hidden states, and correlation structure). [sent-103, score-0.503]
</p><p>43 However, the calculation of the posterior involves a difﬁcult integral. [sent-104, score-0.101]
</p><p>44 Since the log marginal likelihood log p(X|M ) is independent of r(θ) and Q(Z), minimizing KL divergence is equivalent to minimizing variational free energy F ≡ −⟨log p(X, Z, θ|M )⟩r(θ)Q(Z) − Hr (θ) − HQ (Z). [sent-107, score-0.374]
</p><p>45 The subnormarized quantity aij is deﬁned as aij = exp(⟨log aij ⟩r(a) ) and p(X n,t |λk ) is ˜ ˜ ˜ ∑ ∏ ˜ p(X n,t |λk ) = ˜ P(sn,t |λk,l ), (9) k,l n,t Sk ∈G(X n,t ) l∈Φ  ˜ where P(sl |λk,l ) is a sub-normalized distribution: { } ˜ ¯ ˜ P(sl |λk,l ) = exp sl log λk,l − log(sl ! [sent-112, score-0.523]
</p><p>46 These quantities can be calculated by using the recurrence relations of the multivariate Poisson distribution (See the Appendix). [sent-114, score-0.351]
</p><p>47 The calculation of the posterior for S is given as: ∑ n,t ∏ ˜ n,t n,t l∈Φ P(sk,l |λk,l ) Sk ∈G(X n,t ) sk,l n,t n,t ⟨sk,l ⟩Q(Z) = ⟨yk ⟩Q(Z) ∑ . [sent-115, score-0.101]
</p><p>48 (11) ∏ ˜ n,t P(sn,t |λk,l ) n,t Sk ∈G(X  )  l∈Φ  k,l  This is also calculated by using the recurrence relations of the multivariate Poisson distribution. [sent-116, score-0.286]
</p><p>49 A: From top, 1) spike train of three neurons, 2) the probability t of state k staying at window t denoted by ⟨yk ⟩Q(Z) , 3) spike count data xt , and 4) posterior mean i t for hidden variables sk,l . [sent-121, score-1.403]
</p><p>50 n=1 t=2  The VB computes the VB-E and VB-M steps alternatively until the variational free energy converges to a local minimum. [sent-126, score-0.246]
</p><p>51 3  Demonstration on synthetic spike train  By using the synthetic spike train of three neurons, let us ﬁrst demonstrate how to apply our method to a spike train. [sent-128, score-1.523]
</p><p>52 In the case of three neurons, we have four choices for the correlation types that have (1) no correlation term, (2) only a second-order correlation term, (3) only a third-order correlation term, and (4) both of these. [sent-129, score-0.816]
</p><p>53 We generated spike trains by using a multivariate Poisson distribution with only a thirdorder correlation whose Poisson mean depends on periods as: (a) λ1 = λ2 = λ3 = 0. [sent-131, score-1.128]
</p><p>54 The periods (b) and (c) have the same mean ﬁring rate (the mean spike count in one window is λi + λ123 = 1. [sent-140, score-0.689]
</p><p>55 2  Figure 2: Typical examples of estimates of VB for spike train from HVC with (A) bird’s own song, (B) its reversed song, and (C) no stimuli presented. [sent-147, score-0.581]
</p><p>56 Selected model based on variational free energy was used for each condition (see Table 1). [sent-148, score-0.246]
</p><p>57 Background ¯ color indicates most probable state at each time window. [sent-150, score-0.107]
</p><p>58 spike trains for 10 trials, but only one trial is shown. [sent-152, score-0.73]
</p><p>59 The periods (b) and (c) are segmented into states 1 and 2, whose Poisson means are different (Fig. [sent-153, score-0.172]
</p><p>60 1A plot the posterior mean for {st }l∈Φ (Here, we omitted the index of trial n). [sent-156, score-0.126]
</p><p>61 These plots separately visualize k,l the contribution of the independent factor and correlation factor on spike counts xt , c ∈ {1, 2, 3}. [sent-157, score-0.732]
</p><p>62 c The spike counts in period (b) can be viewed as independent ﬁring. [sent-158, score-0.528]
</p><p>63 Even if the spikes are in the same window, this can be regarded as just a coincidence predicted by the assumption of independent ﬁring. [sent-159, score-0.133]
</p><p>64 In contrast, the spike counts in period (c) can be regarded as having been contributed by t common factor st 2,123 , as well as independent factors s2,i , i ∈ {1, 2, 3}. [sent-160, score-0.564]
</p><p>65 Here, we used a 3CP-HMM having three hidden states. [sent-161, score-0.121]
</p><p>66 Because periods (a) and (d) have identical statistics, it is clear that the model with three states (K = 3) is sufﬁcient for modeling this spike train. [sent-162, score-0.599]
</p><p>67 The 3CP-HMM with three hidden states yields the lowest F, implying that it is optimal. [sent-165, score-0.208]
</p><p>68 The 3CP-HMMs with fewer hidden states, IP-HMMs, or 2CP-HMMs cannot represent the statistical structure of the data, and hence yield higher F. [sent-166, score-0.143]
</p><p>69 The 3CP-HMMs with more hidden states (K > 3) or full-CP-HMMs (K ≥ 3) can include an optimal model, but by being penalized by a Bayesian Occam’s razor, yield higher F. [sent-167, score-0.208]
</p><p>70 4  Application to spike trains from HVC in songbird  We applied our method to data collected from the nucleus HVC of songbird. [sent-169, score-0.734]
</p><p>71 HVC is an important nucleus that integrates auditory information and motor information of song sequences [15]. [sent-170, score-0.108]
</p><p>72 We obtained spike trains of three single units by using a silicon probe from one anesthetized Bengalese ﬁnch. [sent-171, score-0.749]
</p><p>73 The bird’s own song (BOS) and reversed song (REV) were presented 50 times for each 6  Table 3: Log-likelihood on test data (REV). [sent-172, score-0.153]
</p><p>74 Method Independent & stationary assumption (K = 1) Stationary assumption (K = 1, correlation type is selected) Independent assumption (IP-HMM) (K is selected) CP-HMM (all selected) full-CP-HMM (K is selected)  Log-likelihood (mean ± s. [sent-173, score-0.347]
</p><p>75 We modeled spike trains for all stimuli using IP-HMMs and CP-HMMs by varying the number of states K and various correlation structure. [sent-188, score-1.012]
</p><p>76 We then selected the model that yielded the lowest free energy. [sent-189, score-0.133]
</p><p>77 Figure 2 shows a typical example of spike trains and the segmentation results for the selected models. [sent-192, score-0.746]
</p><p>78 The CP-HMMs were only selected for spike trains when REV was presented. [sent-193, score-0.746]
</p><p>79 If we assume that the spike statistics did not change over the trials (in our case, this corresponds to the model with only one hidden state, K = 1), CP-HMMs were selected under all experimental conditions. [sent-194, score-0.637]
</p><p>80 These results reﬂect the fact that neurons in anesthetized animals simultaneously transit between high-ﬁring and low-ﬁring states [17], which can be captured by a Poisson distribution with correlation terms. [sent-195, score-0.469]
</p><p>81 Timestationary assumptions have often been employed to obtain a sufﬁcient sample size for estimating correlation (e. [sent-196, score-0.204]
</p><p>82 Our results suggest that we should be careful when interpreting such results; even when the spike trains seem to have a correlation, if we take state transition into account, spike trains may be better captured by using an independent Poisson model. [sent-199, score-1.555]
</p><p>83 We measured predictive performance on test data to verify how well our model capture the statistical properties of the spike train. [sent-200, score-0.455]
</p><p>84 Here, we used spike trains for REV where 3CP-HMM was selected. [sent-201, score-0.685]
</p><p>85 We ﬁrst divided spike trains into 20 training and 20 test trials. [sent-202, score-0.685]
</p><p>86 In the prediction phase, we calculated the log-likelihood on test data under the posterior mean ⟨θ⟩r(θ) of selected models. [sent-204, score-0.202]
</p><p>87 We can see that the log-likelihood on the test data improved by taking both the state transition and correlation structure into consideration. [sent-207, score-0.377]
</p><p>88 These results imply that CP-HMMs can characterize the spike train better than classical Poisson-HMMs. [sent-208, score-0.51]
</p><p>89 The full-CP-HMM include 2nd-order CP-HMM, but shows lower predictive performance than the model in which correlation type were selected. [sent-209, score-0.204]
</p><p>90 5  Discussion  We constructed HMMs whose output is a correlated multivariate Poisson distribution for extracting state-transition dynamics from multiple spike trains. [sent-212, score-0.747]
</p><p>91 We applied the VB method for inferring the posterior over the parameter and hidden variables of the models. [sent-213, score-0.178]
</p><p>92 We have seen that VB can be used to select an appropriate model (the number of hidden states and correlation structure), which gives a better prediction. [sent-214, score-0.412]
</p><p>93 Our method incorporated the correlated Poisson distribution for treating pairwise and higher-order correlations. [sent-215, score-0.24]
</p><p>94 There have been approaches that have calculated correlations by binarizing spike data with log-linear [5] or maximum-entropy models [6]. [sent-216, score-0.666]
</p><p>95 These approaches are limited to treating correlations in short bin lengths, which include at most one spike. [sent-217, score-0.186]
</p><p>96 In contrast, our approach can incorporate correlations in an arbitrary time window from exact synchronization to ﬁring-rate correlations on a modest time scale. [sent-218, score-0.291]
</p><p>97 It can be incorporated by employing a mixture of multivariatePoisson distributions for the output distribution of HMMs. [sent-220, score-0.091]
</p><p>98 7  Appendix: Calculation of correlated Poisson distribution in VB-E step The sub-normalized distribution (Eq. [sent-222, score-0.185]
</p><p>99 9) can be calculated by using the recurrence relation of multivariate Poisson distribution [13]. [sent-223, score-0.296]
</p><p>100 Let us consider the tri-variate (C = 3) with the second-order correlation case, where B = [B1 , B2 ]. [sent-224, score-0.204]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spike', 0.455), ('poisson', 0.429), ('vb', 0.259), ('yk', 0.243), ('trains', 0.23), ('correlation', 0.204), ('sl', 0.158), ('hvc', 0.146), ('hmms', 0.137), ('rev', 0.128), ('hidden', 0.121), ('state', 0.107), ('recurrence', 0.106), ('aij', 0.106), ('correlated', 0.105), ('correlations', 0.102), ('variational', 0.098), ('bos', 0.097), ('multivariate', 0.09), ('states', 0.087), ('window', 0.087), ('silent', 0.085), ('hmm', 0.08), ('energy', 0.076), ('neurons', 0.074), ('ring', 0.072), ('free', 0.072), ('anesthetized', 0.064), ('selected', 0.061), ('calculated', 0.06), ('song', 0.059), ('posterior', 0.057), ('periods', 0.057), ('stationary', 0.056), ('train', 0.055), ('uj', 0.054), ('treating', 0.051), ('nucleus', 0.049), ('aik', 0.049), ('binarizing', 0.049), ('cq', 0.049), ('psth', 0.049), ('trivariate', 0.049), ('log', 0.047), ('trial', 0.045), ('transition', 0.044), ('calculation', 0.044), ('incapable', 0.043), ('multiplier', 0.043), ('count', 0.042), ('uk', 0.04), ('distribution', 0.04), ('bird', 0.039), ('hr', 0.039), ('counts', 0.039), ('sk', 0.038), ('posteriors', 0.038), ('wk', 0.037), ('enabled', 0.036), ('hq', 0.036), ('tth', 0.036), ('regarded', 0.036), ('stimuli', 0.036), ('activities', 0.035), ('reversed', 0.035), ('independent', 0.034), ('criteria', 0.034), ('spikes', 0.034), ('modulating', 0.033), ('bin', 0.033), ('cr', 0.032), ('japan', 0.032), ('neurophysiological', 0.032), ('nth', 0.03), ('lagrange', 0.03), ('relations', 0.03), ('xc', 0.029), ('assumption', 0.029), ('output', 0.029), ('kl', 0.029), ('stimulus', 0.028), ('restrictions', 0.028), ('whose', 0.028), ('bayes', 0.027), ('recorded', 0.027), ('asymptotic', 0.027), ('yj', 0.027), ('bj', 0.026), ('quantities', 0.025), ('mean', 0.024), ('synthetic', 0.024), ('cj', 0.024), ('summarized', 0.023), ('avoiding', 0.023), ('structure', 0.022), ('incorporated', 0.022), ('pairwise', 0.022), ('ij', 0.022), ('segment', 0.021), ('chiba', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="81-tfidf-1" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>2 0.33910808 <a title="81-tfidf-2" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Matthias Munk, Klaus Obermayer</p><p>Abstract: Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three ﬁndings: (1) copula-based distributions provide signiﬁcantly better ﬁts than discretized multivariate normal distributions; (2) negative binomial margins ﬁt the data signiﬁcantly better than Poisson margins; and (3) the dependence structure carries 12% of the mutual information between stimuli and responses. 1</p><p>3 0.31730786 <a title="81-tfidf-3" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>Author: Zhi Yang, Qi Zhao, Wentai Liu</p><p>Abstract: This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer. 1</p><p>4 0.22457224 <a title="81-tfidf-4" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>Author: Jan Gasthaus, Frank Wood, Dilan Gorur, Yee W. Teh</p><p>Abstract: In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle “appearance” and “disappearance” of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of inﬁnite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known. 1</p><p>5 0.19489355 <a title="81-tfidf-5" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>Author: Peng Xu, Timothy K. Horiuchi, Pamela A. Abshire</p><p>Abstract: We report a compact realization of short-term depression (STD) in a VLSI stochastic synapse. The behavior of the circuit is based on a subtractive single release model of STD. Experimental results agree well with simulation and exhibit expected STD behavior: the transmitted spike train has negative autocorrelation and lower power spectral density at low frequencies which can remove redundancy in the input spike train, and the mean transmission probability is inversely proportional to the input spike rate which has been suggested as an automatic gain control mechanism in neural systems. The dynamic stochastic synapse could potentially be a powerful addition to existing deterministic VLSI spiking neural systems. 1</p><p>6 0.1607286 <a title="81-tfidf-6" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>7 0.14438948 <a title="81-tfidf-7" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>8 0.11902721 <a title="81-tfidf-8" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>9 0.11357943 <a title="81-tfidf-9" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>10 0.10343286 <a title="81-tfidf-10" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>11 0.095926411 <a title="81-tfidf-11" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>12 0.083422795 <a title="81-tfidf-12" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>13 0.080721155 <a title="81-tfidf-13" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>14 0.075498022 <a title="81-tfidf-14" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>15 0.067350581 <a title="81-tfidf-15" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>16 0.066392869 <a title="81-tfidf-16" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>17 0.065964065 <a title="81-tfidf-17" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>18 0.062945768 <a title="81-tfidf-18" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>19 0.060837686 <a title="81-tfidf-19" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<p>20 0.060158629 <a title="81-tfidf-20" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.185), (1, 0.108), (2, 0.303), (3, 0.291), (4, -0.258), (5, 0.031), (6, 0.067), (7, -0.01), (8, -0.196), (9, -0.046), (10, -0.041), (11, -0.091), (12, 0.079), (13, -0.061), (14, -0.013), (15, -0.002), (16, -0.043), (17, -0.052), (18, -0.024), (19, 0.056), (20, -0.023), (21, -0.08), (22, 0.04), (23, -0.075), (24, 0.004), (25, 0.033), (26, 0.02), (27, 0.077), (28, 0.002), (29, 0.034), (30, 0.004), (31, 0.009), (32, -0.007), (33, -0.076), (34, -0.06), (35, 0.012), (36, -0.002), (37, -0.01), (38, -0.002), (39, -0.041), (40, 0.023), (41, -0.042), (42, -0.012), (43, 0.009), (44, -0.021), (45, 0.038), (46, -0.089), (47, -0.021), (48, -0.022), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96644121 <a title="81-lsi-1" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>2 0.86672091 <a title="81-lsi-2" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>Author: Jan Gasthaus, Frank Wood, Dilan Gorur, Yee W. Teh</p><p>Abstract: In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle “appearance” and “disappearance” of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of inﬁnite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known. 1</p><p>3 0.86627537 <a title="81-lsi-3" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>Author: Zhi Yang, Qi Zhao, Wentai Liu</p><p>Abstract: This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer. 1</p><p>4 0.79420459 <a title="81-lsi-4" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Matthias Munk, Klaus Obermayer</p><p>Abstract: Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three ﬁndings: (1) copula-based distributions provide signiﬁcantly better ﬁts than discretized multivariate normal distributions; (2) negative binomial margins ﬁt the data signiﬁcantly better than Poisson margins; and (3) the dependence structure carries 12% of the mutual information between stimuli and responses. 1</p><p>5 0.78241593 <a title="81-lsi-5" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>Author: Peng Xu, Timothy K. Horiuchi, Pamela A. Abshire</p><p>Abstract: We report a compact realization of short-term depression (STD) in a VLSI stochastic synapse. The behavior of the circuit is based on a subtractive single release model of STD. Experimental results agree well with simulation and exhibit expected STD behavior: the transmitted spike train has negative autocorrelation and lower power spectral density at low frequencies which can remove redundancy in the input spike train, and the mean transmission probability is inversely proportional to the input spike rate which has been suggested as an automatic gain control mechanism in neural systems. The dynamic stochastic synapse could potentially be a powerful addition to existing deterministic VLSI spiking neural systems. 1</p><p>6 0.6442399 <a title="81-lsi-6" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>7 0.56636482 <a title="81-lsi-7" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>8 0.50554168 <a title="81-lsi-8" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>9 0.49989909 <a title="81-lsi-9" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>10 0.46032655 <a title="81-lsi-10" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>11 0.43227649 <a title="81-lsi-11" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>12 0.40028122 <a title="81-lsi-12" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>13 0.35878846 <a title="81-lsi-13" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>14 0.35704282 <a title="81-lsi-14" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>15 0.30114686 <a title="81-lsi-15" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>16 0.28717706 <a title="81-lsi-16" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>17 0.28484583 <a title="81-lsi-17" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>18 0.2844781 <a title="81-lsi-18" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>19 0.28372025 <a title="81-lsi-19" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>20 0.27939382 <a title="81-lsi-20" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.013), (6, 0.08), (7, 0.085), (12, 0.031), (15, 0.013), (22, 0.292), (28, 0.155), (57, 0.064), (59, 0.01), (63, 0.025), (71, 0.012), (77, 0.071), (78, 0.015), (83, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81695843 <a title="81-lda-1" href="./nips-2008-Online_Models_for_Content_Optimization.html">169 nips-2008-Online Models for Content Optimization</a></p>
<p>Author: Deepak Agarwal, Bee-chung Chen, Pradheep Elango, Nitin Motgi, Seung-taek Park, Raghu Ramakrishnan, Scott Roy, Joe Zachariah</p><p>Abstract: We describe a new content publishing system that selects articles to serve to a user, choosing from an editorially programmed pool that is frequently refreshed. It is now deployed on a major Yahoo! portal, and selects articles to serve to hundreds of millions of user visits per day, signiﬁcantly increasing the number of user clicks over the original manual approach, in which editors periodically selected articles to display. Some of the challenges we face include a dynamic content pool, short article lifetimes, non-stationary click-through rates, and extremely high trafﬁc volumes. The fundamental problem we must solve is to quickly identify which items are popular (perhaps within different user segments), and to exploit them while they remain current. We must also explore the underlying pool constantly to identify promising alternatives, quickly discarding poor performers. Our approach is based on tracking per article performance in near real time through online models. We describe the characteristics and constraints of our application setting, discuss our design choices, and show the importance and effectiveness of coupling online models with a randomization procedure. We discuss the challenges encountered in a production online content-publishing environment and highlight issues that deserve careful attention. Our analysis of this application also suggests a number of future research avenues. 1</p><p>same-paper 2 0.74480605 <a title="81-lda-2" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>3 0.59760123 <a title="81-lda-3" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>4 0.59546286 <a title="81-lda-4" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>5 0.59300721 <a title="81-lda-5" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>Author: Cédric Archambeau, Francis R. Bach</p><p>Abstract: We present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a preprocessing tool for the construction of template attacks. 1</p><p>6 0.58788085 <a title="81-lda-6" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>7 0.58722818 <a title="81-lda-7" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>8 0.58678818 <a title="81-lda-8" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>9 0.58669984 <a title="81-lda-9" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>10 0.58608496 <a title="81-lda-10" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>11 0.58597839 <a title="81-lda-11" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>12 0.58536285 <a title="81-lda-12" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>13 0.58533931 <a title="81-lda-13" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>14 0.58531272 <a title="81-lda-14" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>15 0.58513957 <a title="81-lda-15" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>16 0.58357805 <a title="81-lda-16" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>17 0.58325958 <a title="81-lda-17" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>18 0.58100355 <a title="81-lda-18" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>19 0.58082658 <a title="81-lda-19" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>20 0.58049864 <a title="81-lda-20" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
