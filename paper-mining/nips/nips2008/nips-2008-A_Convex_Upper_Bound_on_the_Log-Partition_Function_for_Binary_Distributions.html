<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-2" href="#">nips2008-2</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</h1>
<br/><p>Source: <a title="nips-2008-2-pdf" href="http://papers.nips.cc/paper/3422-a-convex-upper-bound-on-the-log-partition-function-for-binary-distributions.pdf">pdf</a></p><p>Author: Laurent E. Ghaoui, Assane Gueye</p><p>Abstract: We consider the problem of bounding from above the log-partition function corresponding to second-order Ising models for binary distributions. We introduce a new bound, the cardinality bound, which can be computed via convex optimization. The corresponding error on the logpartition function is bounded above by twice the distance, in model parameter space, to a class of “standard” Ising models, for which variable inter-dependence is described via a simple mean ﬁeld term. In the context of maximum-likelihood, using the new bound instead of the exact log-partition function, while constraining the distance to the class of standard Ising models, leads not only to a good approximation to the log-partition function, but also to a model that is parsimonious, and easily interpretable. We compare our bound with the log-determinant bound introduced by Wainwright and Jordan (2006), and show that when the l1 -norm of the model parameter vector is small enough, the latter is outperformed by the new bound. 1 1.1</p><p>Reference: <a title="nips-2008-2-reference" href="../nips2008_reference/nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We introduce a new bound, the cardinality bound, which can be computed via convex optimization. [sent-6, score-0.198]
</p><p>2 The corresponding error on the logpartition function is bounded above by twice the distance, in model parameter space, to a class of “standard” Ising models, for which variable inter-dependence is described via a simple mean ﬁeld term. [sent-7, score-0.104]
</p><p>3 We compare our bound with the log-determinant bound introduced by Wainwright and Jordan (2006), and show that when the l1 -norm of the model parameter vector is small enough, the latter is outperformed by the new bound. [sent-9, score-0.544]
</p><p>4 Noting that xT Qx + q T x = xT (Q + D(q))x for every x ∈ {0, 1}n , we will without loss of generality assume that q = 0, and denote by Z(Q) the corresponding log-partition function   Z(Q) := log   exp[xT Qx] . [sent-13, score-0.164]
</p><p>5 (1)  x∈{0,1}n  In the Ising model, the maximum-likelihood approach to ﬁtting data leads to the problem min Z(Q) − TrQS, Q∈Q  (2)  n where Q is a subset of the set S n of symmetric matrices, and S ∈ S+ is the empirical second-moment matrix. [sent-14, score-0.14]
</p><p>6 When n Q = S , the dual to (2) is the maximum entropy problem  p(x)xxT ,  max H(p) : p ∈ P, S = p  (3)  x∈{0,1}n  where P is the set of distributions with support in {0, 1}n , and H is the entropy H(p) = −  p(x) log p(x). [sent-15, score-0.446]
</p><p>7 In this regard, convex, upper bounds on the log-partition function are of particular interest, and our focus here: convexity usually brings about computational tractability, while using upper bounds yields a parameter Q that is suboptimal for the exact problem. [sent-20, score-0.319]
</p><p>8 Using an upper bound in lieu of Z(Q) in (2), leads to a problem we will generically refer to as the pseudo maximumlikelihood problem. [sent-21, score-0.422]
</p><p>9 Such relaxations may involve two ingredients: an upper bound on the entropy, and an outer approximation to the marginal polytope. [sent-23, score-0.432]
</p><p>10 The so-called log-determinant bound has been recently introduced, for a large class of Markov random ﬁelds, by Wainwright and Jordan [2]. [sent-26, score-0.298]
</p><p>11 ) The log-determinant bound is based on an upper bound on the differential entropy of continuous random variable, that is attained for a Gaussian distribution. [sent-28, score-0.719]
</p><p>12 The log-determinant bound enjoys good tractability properties, both for the computation of the log-partition function, and in the context of the maximum-likelihood problem (2). [sent-29, score-0.319]
</p><p>13 A recent paper by Ravikumar and Lafferty [1] discusses using bounds on the log-partition function to estimate marginal probabilities for a large class of graphical models, which adds extra motivation for the present study. [sent-30, score-0.144]
</p><p>14 3  Main results and outline  The main purpose of this note is to introduce a new upper bound on the log-partition function that is computationally tractable. [sent-32, score-0.36]
</p><p>15 The new bound is convex in Q, and leads to a restriction to the maximum-likelihood problem that is also tractable. [sent-33, score-0.349]
</p><p>16 Our bound is constructed so as to be exact in the case of standard Ising models. [sent-38, score-0.313]
</p><p>17 In fact, the error between our bound and the true value of the log-partition function is bounded above by twice the l1 -norm distance from the model parameters (Q) to the class of standard Ising models. [sent-39, score-0.402]
</p><p>18 The outline of the note reﬂects our main results: in section 2, we introduce our bound, and show that the approximation error is bounded above by the distance to the class of standard Ising models. [sent-40, score-0.158]
</p><p>19 We discuss in section 3 the use of our bound in the context of the maximum-likelihood problem (2) and its dual (3). [sent-41, score-0.375]
</p><p>20 In particular, we discuss how imposing a bound on the distance to the class of standard Ising models may be desirable, not only to obtain an accurate approximation to the log-partition function, but also to ﬁnd a parsimonious model, having good interpretability properties. [sent-42, score-0.461]
</p><p>21 We then compare the new bound with the log-determinant bound of Wainwright and Jordan in section 4. [sent-43, score-0.544]
</p><p>22 We show  that our new bound outperforms the log-determinant bound when the norm Q 1 is small enough (less than 0. [sent-44, score-0.544]
</p><p>23 08n), and provide numerical experiments supporting the claim that our comparison analysis is quite conservative: our bound appears to be better over a wide range of values of Q 1 . [sent-45, score-0.297]
</p><p>24 Let ck = |∆k | denote the cardinal of ∆k , and πk := 2−n ck the probability of ∆k under the uniform distribution. [sent-52, score-0.312]
</p><p>25 1  The Cardinality Bound The maximum bound  To ease our derivation, we begin with a simple bound based on replacing each term in the log-partition function by its maximum over {0, 1}n . [sent-62, score-0.619]
</p><p>26 This leads to an upper bound on the log-partition function: Z(Q) ≤ n log 2 + φmax (Q), where φmax (Q) :=  max  x∈{0,1}n  xT Qx. [sent-63, score-0.565]
</p><p>27 For later reference, we note the dual form:  ψmax (Q) = min t : t,ν  = min ν  D(ν) − Q 1 T 2ν  1 2ν  t  0  1 T ν (D(ν) − Q)−1 ν : D(ν) 4  (6) Q. [sent-66, score-0.251]
</p><p>28 (7)  The corresponding bound on the log-partition function, referred to as the maximum bound, is Z(Q) ≤ Zmax (Q) := n log 2 + ψmax (Q). [sent-67, score-0.404]
</p><p>29 The complexity of this bound (using interior-point methods) is roughly O(n3 ). [sent-68, score-0.272]
</p><p>30 Second, we have Zmax (Q) ≤ n log 2 + Q 1 , which follows from (5), together with the fact that any matrix X that is feasible for that problem satisﬁes X ∞ ≤ 1. [sent-71, score-0.129]
</p><p>31 Indeed, for every symmetric matrices Q, R we have the sub-gradient inequality Zmax (R) ≥ Zmax (Q) + TrX opt (R − Q), where X opt is any optimal variable for the dual problem (5). [sent-75, score-0.261]
</p><p>32 Since any feasible X satisﬁes X ∞ ≤ 1, we can bound the term TrX opt (Q − R) from below by − Q − R 1 , and after exchanging the roles of Q, R, obtain the desired result. [sent-76, score-0.339]
</p><p>33 , n}, consider the subset of variables with cardinality k, ∆k := {x ∈ {0, 1}n : Card(x) = k}. [sent-81, score-0.163]
</p><p>34 Z(Q) = log k=0 x∈∆k  We can reﬁne the maximum bound by replacing the terms in the log-partition by their maximum over ∆k , leading to n  Z(Q) ≤ log  ck exp[φk (Q)] , k=0  where, for k ∈ {0, . [sent-83, score-0.715]
</p><p>35 Based on the identity φk (Q) =  max  (X,x)∈X+  TrQX : xT x = k, 1T X1 = k 2 , rankX = 1,  (8)  and using rank relaxation as before, we obtain the bound φk (Q) ≤ ψk (Q), where ψk (Q) =  max  (X,x)∈X+  TrQX : xT x = k, 1T X1 = k 2 . [sent-91, score-0.568]
</p><p>36 (9)  We deﬁne the cardinality bound, as n  ck exp[ψk (Q)] . [sent-92, score-0.319]
</p><p>37 Zcard (Q) := log k=0  The complexity of computing ψk (Q) (using interior-point methods) is roughly O(n3 ). [sent-93, score-0.106]
</p><p>38 The upper bound Zcard (Q) is computed via n semideﬁnite programs of the form (9). [sent-94, score-0.339]
</p><p>39 Problem (9) admits the dual form ψk (Q)  :=  min t + kµ + λk 2 :  t,µ,ν,λ  D(ν) + µI + λ11T − Q 1 T 2ν  1 2ν  t  0. [sent-96, score-0.167]
</p><p>40 It can be shown (proof which we omit due to space limitation) that, in the case of standard Ising models, that is if Q has the form µI + λ11T for some scalars µ, λ, then the bound ψk (Q) is exact. [sent-100, score-0.272]
</p><p>41 Since the values of xT Qx when x ranges ∆k are constant, the cardinality bound is also exact. [sent-101, score-0.435]
</p><p>42 By construction, Zcard (Q) is guaranteed to be better (lower) than Zmax (Q), since the latter is obtained upon replacing ψk (Q) by its upper bound ψ(Q) for every k. [sent-102, score-0.396]
</p><p>43 The cardinality bound thus satisﬁes Z(Q) ≤ Zcard (Q) ≤ Zmax (Q) ≤ n log 2 + Q 1 . [sent-103, score-0.541]
</p><p>44 We begin by establishing an upper bound on the difference between maximal and minimal values of xT Qx when x ∈ ∆k . [sent-109, score-0.339]
</p><p>45 We have the bound min xT Qx ≥ ηk (Q) :=  x∈∆k  min  (X,x)∈X+  TrQX : xT x = k, 1T X1 = k 2 . [sent-110, score-0.44]
</p><p>46 In the same fashion as for the quantity ψk (Q), we can express ηk (Q) as ηk (Q) = max kµ + k 2 λ + ψmin (Q − µI − λ11T ), µ,λ  where ψmin (Q) :=  min  (X,x)∈X+  TrQX. [sent-111, score-0.181]
</p><p>47 Note that this measure is easily computed, in O(n2 log n) time, by ﬁrst setting λ to be the median of the values Qij , 1 ≤ i < j ≤ n, and then setting µ to be the median of the values Qii − λ, i = 1, . [sent-114, score-0.164]
</p><p>48 We summarize our ﬁndings so far with the following theorem:  Theorem 1 (Cardinality bound) The cardinality bound is n  Zcard (Q) := log  ck exp[ψk (Q)] . [sent-118, score-0.715]
</p><p>49 The approximation error is bounded above by twice the distance (in l1 -norm) to the class of standard Ising models: 0 ≤ Zcard (Q) − Z(Q) ≤ 2 min Q − µI − λ11T λ,µ  3 3. [sent-123, score-0.249]
</p><p>50 The Pseudo Maximum-Likelihood Problem Tractable formulation  Using the bound Zcard (Q) in lieu of Z(Q) in the maximum-likelihood problem (2) leads to a convex restriction of that problem, referred to as the pseudo-maximum likelihood problem. [sent-125, score-0.384]
</p><p>51 This problem can be cast as n  min  t,µ,ν,Q  ck exp[tk + kµk + k 2 λk ]  log  − TrQS  k=0  s. [sent-126, score-0.346]
</p><p>52 For numerical reasons, and without loss of generality, it is advisable to scale the ck ’s and replace them by πk := 2−n ck ∈ [0, 1]. [sent-133, score-0.337]
</p><p>53 2  Dual and interpretation  When Q = S n , the dual to the above problem is n  max n  (Yk ,yk ,qk )k=0  −D(q||π) :  Yk , q ≥ 0, q T 1 = 1,  S= k=0  Yk T yk  yk qk  0, d(Yk ) = yk ,  1T yk = kqk , 1T Yk 1 = k 2 qk , k = 0 . [sent-135, score-1.352]
</p><p>54 , n}, with πk = Probu ∆k = 2−n ck , and D(q||π) is the relative entropy (Kullback-Leibler divergence) between the distributions q, π: n  D(q||π) :=  qk log k=0  qk . [sent-142, score-0.901]
</p><p>55 πk  −1 −1 To interpret this dual, we assume without loss of generality q > 0, and use the variables Xk := qk Yk , xk := qk yk . [sent-143, score-0.898]
</p><p>56 We obtain the equivalent (non-convex) formulation n  max  (Xk ,xk ,qk )n k=0  −D(q||π)  qk Xk , q ≥ 0, q T 1 = 1,  : S=  (14)  k=0  (Xk , xk ) ∈ X+ , 1T xk = k, 1T Xk 1 = k 2 , k = 0 . [sent-144, score-0.687]
</p><p>57 The above problem can be obtained as a relaxation to the dual of the exact maximum-likelihood problem (2), which is the maximum entropy problem (3). [sent-148, score-0.299]
</p><p>58 The relaxation involves two steps: one is to form an outer approximation to the marginal polytope, the other is to ﬁnd an upper bound on the entropy function (4). [sent-149, score-0.581]
</p><p>59 First observe that we can express any distribution on {0, 1}n as n  p(x) =  qk pk (x),  (15)  k=0  where qk = Probp ∆k =  −1 qk p(x) 0  p(x), pk (x) = x∈∆k  if x ∈ ∆k , otherwise. [sent-150, score-1.054]
</p><p>60 Note that the functions pk are valid distributions on {0, 1}n as well as ∆k . [sent-151, score-0.124]
</p><p>61 To obtain an outer approximation to the marginal polytope, we then write the moment-matching equality constraint in problem (3) as n  S = Ep xxT =  qk Xk , k=0  where Xk ’s are the second-order moment matrices with respect to pk : −1 Xk = Epk xxT = qk  p(x)xxT . [sent-152, score-0.814]
</p><p>62 x∈∆k  To relax the constraints in the maximum-entropy problem (3), we simply use the valid constraints Xk d(Xk ) = xk , 1T xk = k, 1T Xk 1 = k 2 , where xk is the mean under pk : −1 xk = Epk x = qk  xk xT , k  p(x)x. [sent-153, score-1.231]
</p><p>63 To ﬁnalize our relaxation, we now form an upper bound on the entropy function (4). [sent-155, score-0.406]
</p><p>64 3  Ensuring quality via bounds on Q  We consider the (exact) maximum-likelihood problem (2), with Q = {Q = QT : min Z(Q) − TrQS :  Q=QT  Q  1  ≤ ,  Q  1  ≤ }: (16)  and its convex relaxation: min Zcard (Q) − TrQS :  Q=QT  Q  1  ≤ . [sent-158, score-0.275]
</p><p>65 Thus, any -suboptimal solution of the relaxation (17) is guaranteed to by 3 -suboptimal for the exact problem, (16). [sent-160, score-0.123]
</p><p>66 As seen above, the constraint also implies a better approximation to the exact problem (16). [sent-163, score-0.104]
</p><p>67 A more accurate bound on the approximation error can be obtained by imposing the following constraint on Q and two new variables λ, µ: Q − µI − λ11T  1  ≤ . [sent-166, score-0.374]
</p><p>68 However, it will still be quite interpretable, as the bound above will encourage the number of off-diagonal elements in Q that differ from their median, to be small. [sent-169, score-0.272]
</p><p>69 A yet more accurate control on the approximation error can be induced by the constraints ψk (Q) ≤ + ηk (Q) for every k, each of which can be expressed as an LMI constraint. [sent-170, score-0.108]
</p><p>70 The corresponding constrained relaxation to the maximum-likelihood problem has the form n  min  t,µ± ,ν ± ,Q  s. [sent-171, score-0.166]
</p><p>71 ck exp[t+ + kµ+ + k 2 λ+ ] k k k  log  − TrQS  k=0 + diag(νk ) + µ+ I + λ+ 11T − Q k k 1 + 2 νk − Q − diag(νk ) − µ− I − λ− 11T k k 1 − 2 νk  1 + 2 νk t+ k 1 − νk 2 t− k  0, k = 0, . [sent-173, score-0.262]
</p><p>72 1  Links with the Log-Determinant Bound The log-determinant bounds  The bound in Wainwright and Jordan [2] is based on an upper bound on the (differential) entropy of a continuous random variable, which is attained for a Gaussian distribution. [sent-185, score-0.773]
</p><p>73 It has the form Z(Q) ≤ Zld (Q), with Zld (Q) := αn +  max  (X,x)∈X+  TrQX +  1 1 log det(X − xxT + I) 2 12  (18)  where α := (1/2) log(2πe) ≈ 1. [sent-186, score-0.203]
</p><p>74 Wainwright and Jordan suggest to further relax this bound to one which is easier to compute: Zld (Q) ≤ Zrld (Q) := αn + max  (X,x)∈X  TrQX +  1 1 log det(X − xxT + I). [sent-188, score-0.494]
</p><p>75 2 12  (19)  Like Z and the bounds examined previously, the bound Zld and Zrld are Lipschitz-continuous, with constant 1 with respect to the l1 norm. [sent-189, score-0.362]
</p><p>76 The proof starts with the representations above, and exploits the fact that Q 1 is an upper bound on TrQX when (X, x) ∈ X+ . [sent-190, score-0.361]
</p><p>77 The dual of the log-determinant bound has the form (see appendix (? [sent-191, score-0.355]
</p><p>78 )) Zld (Q) =  1 n log π − log 2+ 2 2 1 1 min t + Tr(D(ν) − Q − F ) − log det t,ν,F,g,h 12 2 s. [sent-193, score-0.459]
</p><p>79 (20)  The relaxed counterpart Zrld (Q) is obtained upon setting F, g, h to zero in the dual above: Zrld (Q) =  1 n 1 1 log π − log 2 + min t + Tr(D(ν) − Q) − log det t,ν 2 2 12 2  1 D(ν) − Q − 2 ν 1 T t −2ν  . [sent-196, score-0.601]
</p><p>80 Using Schur complements to eliminate the variable t, we further obtain 1 n log π + + 2 2 1 1 T 1 min ν (D(ν) − Q)−1 ν + Tr(D(ν) − Q) − log det(D(ν) − Q). [sent-197, score-0.315]
</p><p>81 2  (21)  Comparison with the maximum bound  We ﬁrst note the similarity in structure between the dual problem (5) deﬁning Zmax (Q) and that of the relaxed logdeterminant bound. [sent-199, score-0.44]
</p><p>82 Despite these connections, the log-determinant bound is neither better nor worse than the cardinality or maximum bounds. [sent-200, score-0.461]
</p><p>83 when Q is diagonal), the cardinality bound is exact, while the log-determinant one is not. [sent-203, score-0.435]
</p><p>84 Conversely, one can choose Q so that Zcard (Q) > Zld (Q), so no bound dominates the other. [sent-204, score-0.272]
</p><p>85 However, when we impose an extra condition on Q, namely a bound on its l1 norm, more can be said. [sent-207, score-0.272]
</p><p>86 The analysis is based on the case Q = 0, and exploits the Lipschitz continuity of the bounds with respect to the l1 -norm. [sent-208, score-0.142]
</p><p>87 First notice (although not shown in this paper because of space limitation) that, for Q = 0, the relaxed log-determinant bound writes 2πe 1 n + Zrld (0) = log 2 3 2 πe 1 n + . [sent-209, score-0.437]
</p><p>88 = Zmax (0) + log 2 6 2 Now invoke the Lipschitz continuity properties of the bounds Zrld (Q) and Zmax (Q), and obtain that Zrld (Q) − Zmax (Q) = (Zrld (Q) − Zrld (0)) + (Zrld (0) − Zmax (0)) + (Zmax (0) − Zmax (Q)) ≥ −2 Q 1 + (Zrld (0) − Zmax (0)) πe 1 n + . [sent-210, score-0.226]
</p><p>89 = −2 Q 1 + + log 2 6 2 1 This proves that if Q 1 ≤ n log πe + 4 , then the relaxed log-determinant bound Zrld (Q) is worse (larger) than the 4 6 maximum bound Zmax (Q). [sent-211, score-0.841]
</p><p>90 3  Summary of comparison results  To summarize our ﬁndings: Theorem 2 (Comparison) We have for every Q: Z(Q) ≤ Zcard (Q) ≤ Zmax (Q) ≤ n log 2 + Q 1 . [sent-215, score-0.158]
</p><p>91 4  A numerical experiment  We now illustrate our ﬁndings on the comparison between the log-determinant bounds and the cardinality and maximum bounds. [sent-219, score-0.286]
</p><p>92 Clearly, the new bound outperforms the log-determinant bounds for a wide range of values of ρ. [sent-224, score-0.344]
</p><p>93 Our predicted threshold value of Q 1 for which the new bound becomes worse, namely ρ = 0. [sent-225, score-0.272]
</p><p>94 Across the range of ρ, we note that the log-determinant bound is indistinguishable from its relaxed counterpart. [sent-229, score-0.331]
</p><p>95 5  Conclusion and Remarks  We have introduced a new upper bound (the cardinality bound) for the log-partition function corresponding to secondorder Ising models for binary distribution. [sent-230, score-0.522]
</p><p>96 We have shown that such a bound can be computed via convex optimization, and, when compared to the log-determinant bound introduced by Wainwright and Jordan (2006), the cardinality bound performs better when the l1 -norm of the model parameter vector is small enough. [sent-231, score-1.014]
</p><p>97 Although not shown in the paper, the cardinality bound becomes exact in the case of standard Ising model, while the maximum bound (for example) is not exact for such model. [sent-232, score-0.815]
</p><p>98 As was shown in section 2, the cardinality bound was computed by deﬁning a partition of {0, 1}. [sent-233, score-0.435]
</p><p>99 It turns out that partitions bound are closely linked to the more general class bounds that are based on worst-case probability analysis. [sent-235, score-0.37]
</p><p>100 We acknowledge the importance of applying our bound to real-word data. [sent-236, score-0.272]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('zmax', 0.415), ('zcard', 0.35), ('zrld', 0.306), ('qk', 0.286), ('ising', 0.28), ('bound', 0.272), ('trqx', 0.175), ('cardinality', 0.163), ('ck', 0.156), ('xk', 0.152), ('yk', 0.15), ('qx', 0.134), ('xxt', 0.134), ('zld', 0.131), ('trqs', 0.109), ('log', 0.106), ('pk', 0.098), ('max', 0.097), ('xt', 0.096), ('min', 0.084), ('dual', 0.083), ('relaxation', 0.082), ('wainwright', 0.074), ('bounds', 0.072), ('upper', 0.067), ('entropy', 0.067), ('relaxed', 0.059), ('det', 0.057), ('lipschitz', 0.057), ('qt', 0.053), ('continuity', 0.048), ('opt', 0.044), ('polytope', 0.044), ('epk', 0.044), ('probp', 0.044), ('trr', 0.044), ('trx', 0.044), ('semide', 0.044), ('exact', 0.041), ('jordan', 0.041), ('limitation', 0.038), ('dst', 0.038), ('exp', 0.038), ('lieu', 0.035), ('approximation', 0.035), ('convex', 0.035), ('every', 0.034), ('rn', 0.034), ('symmetric', 0.033), ('interpretability', 0.033), ('outer', 0.032), ('bounded', 0.032), ('berkeley', 0.031), ('card', 0.03), ('median', 0.029), ('twice', 0.028), ('parsimonious', 0.028), ('constraint', 0.028), ('tractability', 0.027), ('ndings', 0.026), ('ravikumar', 0.026), ('maximum', 0.026), ('marginal', 0.026), ('distance', 0.026), ('class', 0.026), ('valid', 0.026), ('pseudo', 0.025), ('numerical', 0.025), ('tk', 0.025), ('generality', 0.024), ('conservative', 0.023), ('physics', 0.023), ('tr', 0.023), ('matrices', 0.023), ('feasible', 0.023), ('tractable', 0.023), ('attained', 0.023), ('ep', 0.023), ('leads', 0.023), ('replacing', 0.023), ('exploits', 0.022), ('expense', 0.021), ('imposing', 0.021), ('outline', 0.021), ('constraints', 0.021), ('context', 0.02), ('rank', 0.02), ('statement', 0.02), ('graphical', 0.02), ('models', 0.02), ('complements', 0.019), ('elghaoui', 0.019), ('diag', 0.019), ('relax', 0.019), ('restriction', 0.019), ('summarize', 0.018), ('differential', 0.018), ('constant', 0.018), ('error', 0.018), ('electrical', 0.018)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="2-tfidf-1" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>Author: Laurent E. Ghaoui, Assane Gueye</p><p>Abstract: We consider the problem of bounding from above the log-partition function corresponding to second-order Ising models for binary distributions. We introduce a new bound, the cardinality bound, which can be computed via convex optimization. The corresponding error on the logpartition function is bounded above by twice the distance, in model parameter space, to a class of “standard” Ising models, for which variable inter-dependence is described via a simple mean ﬁeld term. In the context of maximum-likelihood, using the new bound instead of the exact log-partition function, while constraining the distance to the class of standard Ising models, leads not only to a good approximation to the log-partition function, but also to a model that is parsimonious, and easily interpretable. We compare our bound with the log-determinant bound introduced by Wainwright and Jordan (2006), and show that when the l1 -norm of the model parameter vector is small enough, the latter is outperformed by the new bound. 1 1.1</p><p>2 0.12014078 <a title="2-tfidf-2" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>Author: Nicol N. Schraudolph, Dmitry Kamenetsky</p><p>Abstract: We give polynomial-time algorithms for the exact computation of lowest-energy states, worst margin violators, partition functions, and marginals in certain binary undirected graphical models. Our approach provides an interesting alternative to the well-known graph cut paradigm in that it does not impose any submodularity constraints; instead we require planarity to establish a correspondence with perfect matchings in an expanded dual graph. Maximum-margin parameter estimation for a boundary detection task shows our approach to be efﬁcient and effective. A C++ implementation is available from http://nic.schraudolph.org/isinf/. 1</p><p>3 0.10199085 <a title="2-tfidf-3" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables. The bound is obtained by propagating local bounds (convex sets of probability distributions) over a subtree of the factor graph, rooted in the variable of interest. By construction, the method not only bounds the exact marginal probability distribution of a variable, but also its approximate Belief Propagation marginal (“belief”). Thus, apart from providing a practical means to calculate bounds on marginals, our contribution also lies in providing a better understanding of the error made by Belief Propagation. We show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis. 1</p><p>4 0.10175269 <a title="2-tfidf-4" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>Author: Sham M. Kakade, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. This derivation provides simpliﬁed proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). In addition to providing a uniﬁed analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2. 1</p><p>5 0.096287057 <a title="2-tfidf-5" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>Author: Ydo Wexler, Christopher Meek</p><p>Abstract: We propose a multiplicative approximation scheme (MAS) for inference problems in graphical models, which can be applied to various inference algorithms. The method uses -decompositions which decompose functions used throughout the inference procedure into functions over smaller sets of variables with a known error . MAS translates these local approximations into bounds on the accuracy of the results. We show how to optimize -decompositions and provide a fast closed-form solution for an L2 approximation. Applying MAS to the Variable Elimination inference algorithm, we introduce an algorithm we call DynaDecomp which is extremely fast in practice and provides guaranteed error bounds on the result. The superior accuracy and efﬁciency of DynaDecomp is demonstrated. 1</p><p>6 0.081484646 <a title="2-tfidf-6" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>7 0.079545833 <a title="2-tfidf-7" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>8 0.078464977 <a title="2-tfidf-8" href="./nips-2008-Deflation_Methods_for_Sparse_PCA.html">57 nips-2008-Deflation Methods for Sparse PCA</a></p>
<p>9 0.077087782 <a title="2-tfidf-9" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>10 0.074962065 <a title="2-tfidf-10" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>11 0.07256411 <a title="2-tfidf-11" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>12 0.070602946 <a title="2-tfidf-12" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>13 0.066955708 <a title="2-tfidf-13" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>14 0.066410884 <a title="2-tfidf-14" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>15 0.065505877 <a title="2-tfidf-15" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<p>16 0.061851416 <a title="2-tfidf-16" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>17 0.060552038 <a title="2-tfidf-17" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>18 0.058284551 <a title="2-tfidf-18" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>19 0.058099721 <a title="2-tfidf-19" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>20 0.057027794 <a title="2-tfidf-20" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.162), (1, 0.023), (2, -0.121), (3, 0.054), (4, -0.008), (5, 0.033), (6, -0.005), (7, -0.019), (8, 0.009), (9, -0.072), (10, -0.065), (11, -0.024), (12, 0.118), (13, 0.026), (14, -0.073), (15, 0.064), (16, 0.089), (17, -0.018), (18, 0.061), (19, -0.024), (20, -0.01), (21, -0.048), (22, -0.104), (23, -0.083), (24, 0.031), (25, -0.003), (26, 0.029), (27, 0.035), (28, -0.046), (29, -0.048), (30, -0.083), (31, -0.031), (32, -0.09), (33, -0.057), (34, 0.002), (35, 0.016), (36, 0.016), (37, 0.063), (38, 0.007), (39, -0.079), (40, -0.018), (41, -0.16), (42, 0.014), (43, 0.119), (44, 0.039), (45, -0.053), (46, -0.097), (47, 0.038), (48, -0.111), (49, 0.031)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93680763 <a title="2-lsi-1" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>Author: Laurent E. Ghaoui, Assane Gueye</p><p>Abstract: We consider the problem of bounding from above the log-partition function corresponding to second-order Ising models for binary distributions. We introduce a new bound, the cardinality bound, which can be computed via convex optimization. The corresponding error on the logpartition function is bounded above by twice the distance, in model parameter space, to a class of “standard” Ising models, for which variable inter-dependence is described via a simple mean ﬁeld term. In the context of maximum-likelihood, using the new bound instead of the exact log-partition function, while constraining the distance to the class of standard Ising models, leads not only to a good approximation to the log-partition function, but also to a model that is parsimonious, and easily interpretable. We compare our bound with the log-determinant bound introduced by Wainwright and Jordan (2006), and show that when the l1 -norm of the model parameter vector is small enough, the latter is outperformed by the new bound. 1 1.1</p><p>2 0.63371402 <a title="2-lsi-2" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>Author: Ydo Wexler, Christopher Meek</p><p>Abstract: We propose a multiplicative approximation scheme (MAS) for inference problems in graphical models, which can be applied to various inference algorithms. The method uses -decompositions which decompose functions used throughout the inference procedure into functions over smaller sets of variables with a known error . MAS translates these local approximations into bounds on the accuracy of the results. We show how to optimize -decompositions and provide a fast closed-form solution for an L2 approximation. Applying MAS to the Variable Elimination inference algorithm, we introduce an algorithm we call DynaDecomp which is extremely fast in practice and provides guaranteed error bounds on the result. The superior accuracy and efﬁciency of DynaDecomp is demonstrated. 1</p><p>3 0.55932826 <a title="2-lsi-3" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables. The bound is obtained by propagating local bounds (convex sets of probability distributions) over a subtree of the factor graph, rooted in the variable of interest. By construction, the method not only bounds the exact marginal probability distribution of a variable, but also its approximate Belief Propagation marginal (“belief”). Thus, apart from providing a practical means to calculate bounds on marginals, our contribution also lies in providing a better understanding of the error made by Belief Propagation. We show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis. 1</p><p>4 0.51826328 <a title="2-lsi-4" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>Author: Sham M. Kakade, Karthik Sridharan, Ambuj Tewari</p><p>Abstract: This work characterizes the generalization ability of algorithms whose predictions are linear in the input vector. To this end, we provide sharp bounds for Rademacher and Gaussian complexities of (constrained) linear classes, which directly lead to a number of generalization bounds. This derivation provides simpliﬁed proofs of a number of corollaries including: risk bounds for linear prediction (including settings where the weight vectors are constrained by either L2 or L1 constraints), margin bounds (including both L2 and L1 margins, along with more general notions based on relative entropy), a proof of the PAC-Bayes theorem, and upper bounds on L2 covering numbers (with Lp norm constraints and relative entropy constraints). In addition to providing a uniﬁed analysis, the results herein provide some of the sharpest risk and margin bounds. Interestingly, our results show that the uniform convergence rates of empirical risk minimization algorithms tightly match the regret bounds of online learning algorithms for linear prediction, up to a constant factor of 2. 1</p><p>5 0.48426166 <a title="2-lsi-5" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>Author: Volkan Cevher, Marco F. Duarte, Chinmay Hegde, Richard Baraniuk</p><p>Abstract: Compressive Sensing (CS) combines sampling and compression into a single subNyquist linear measurement process for sparse and compressible signals. In this paper, we extend the theory of CS to include signals that are concisely represented in terms of a graphical model. In particular, we use Markov Random Fields (MRFs) to represent sparse signals whose nonzero coefﬁcients are clustered. Our new model-based recovery algorithm, dubbed Lattice Matching Pursuit (LaMP), stably recovers MRF-modeled signals using many fewer measurements and computations than the current state-of-the-art algorithms.</p><p>6 0.4818846 <a title="2-lsi-6" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>7 0.48001584 <a title="2-lsi-7" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>8 0.46876112 <a title="2-lsi-8" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>9 0.44972876 <a title="2-lsi-9" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>10 0.44386891 <a title="2-lsi-10" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>11 0.41835397 <a title="2-lsi-11" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>12 0.4046599 <a title="2-lsi-12" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>13 0.39674559 <a title="2-lsi-13" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>14 0.39510933 <a title="2-lsi-14" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>15 0.39409795 <a title="2-lsi-15" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>16 0.38869548 <a title="2-lsi-16" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>17 0.38214323 <a title="2-lsi-17" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>18 0.37585467 <a title="2-lsi-18" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>19 0.37560207 <a title="2-lsi-19" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>20 0.37282342 <a title="2-lsi-20" href="./nips-2008-Deflation_Methods_for_Sparse_PCA.html">57 nips-2008-Deflation Methods for Sparse PCA</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.048), (7, 0.078), (12, 0.037), (28, 0.184), (57, 0.042), (59, 0.031), (63, 0.055), (71, 0.025), (77, 0.035), (83, 0.036), (92, 0.307)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83887267 <a title="2-lda-1" href="./nips-2008-Mortal_Multi-Armed_Bandits.html">140 nips-2008-Mortal Multi-Armed Bandits</a></p>
<p>Author: Deepayan Chakrabarti, Ravi Kumar, Filip Radlinski, Eli Upfal</p><p>Abstract: We formulate and study a new variant of the k-armed bandit problem, motivated by e-commerce applications. In our model, arms have (stochastic) lifetime after which they expire. In this setting an algorithm needs to continuously explore new arms, in contrast to the standard k-armed bandit model in which arms are available indeﬁnitely and exploration is reduced once an optimal arm is identiﬁed with nearcertainty. The main motivation for our setting is online-advertising, where ads have limited lifetime due to, for example, the nature of their content and their campaign budgets. An algorithm needs to choose among a large collection of ads, more than can be fully explored within the typical ad lifetime. We present an optimal algorithm for the state-aware (deterministic reward function) case, and build on this technique to obtain an algorithm for the state-oblivious (stochastic reward function) case. Empirical studies on various reward distributions, including one derived from a real-world ad serving application, show that the proposed algorithms signiﬁcantly outperform the standard multi-armed bandit approaches applied to these settings. 1</p><p>same-paper 2 0.7383424 <a title="2-lda-2" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>Author: Laurent E. Ghaoui, Assane Gueye</p><p>Abstract: We consider the problem of bounding from above the log-partition function corresponding to second-order Ising models for binary distributions. We introduce a new bound, the cardinality bound, which can be computed via convex optimization. The corresponding error on the logpartition function is bounded above by twice the distance, in model parameter space, to a class of “standard” Ising models, for which variable inter-dependence is described via a simple mean ﬁeld term. In the context of maximum-likelihood, using the new bound instead of the exact log-partition function, while constraining the distance to the class of standard Ising models, leads not only to a good approximation to the log-partition function, but also to a model that is parsimonious, and easily interpretable. We compare our bound with the log-determinant bound introduced by Wainwright and Jordan (2006), and show that when the l1 -norm of the model parameter vector is small enough, the latter is outperformed by the new bound. 1 1.1</p><p>3 0.58021635 <a title="2-lda-3" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>4 0.57673645 <a title="2-lda-4" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>Author: Ling Huang, Donghui Yan, Nina Taft, Michael I. Jordan</p><p>Abstract: Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a speciﬁcation of permitted loss in clustering performance. 1</p><p>5 0.5756098 <a title="2-lda-5" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>Author: Andriy Mnih, Geoffrey E. Hinton</p><p>Abstract: Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the nonhierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models. 1</p><p>6 0.57474869 <a title="2-lda-6" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>7 0.57319552 <a title="2-lda-7" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>8 0.57289028 <a title="2-lda-8" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>9 0.57227558 <a title="2-lda-9" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>10 0.57203454 <a title="2-lda-10" href="./nips-2008-Fast_Prediction_on_a_Tree.html">84 nips-2008-Fast Prediction on a Tree</a></p>
<p>11 0.57188982 <a title="2-lda-11" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>12 0.5715974 <a title="2-lda-12" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>13 0.57126087 <a title="2-lda-13" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>14 0.57113332 <a title="2-lda-14" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>15 0.57109636 <a title="2-lda-15" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>16 0.57082695 <a title="2-lda-16" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>17 0.57056057 <a title="2-lda-17" href="./nips-2008-Regularized_Policy_Iteration.html">195 nips-2008-Regularized Policy Iteration</a></p>
<p>18 0.57031542 <a title="2-lda-18" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>19 0.56990987 <a title="2-lda-19" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>20 0.56989324 <a title="2-lda-20" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
