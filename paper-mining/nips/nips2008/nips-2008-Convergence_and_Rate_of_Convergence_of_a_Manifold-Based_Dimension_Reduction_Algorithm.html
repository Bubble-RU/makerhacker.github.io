<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-51" href="#">nips2008-51</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</h1>
<br/><p>Source: <a title="nips-2008-51-pdf" href="http://papers.nips.cc/paper/3471-convergence-and-rate-of-convergence-of-a-manifold-based-dimension-reduction-algorithm.pdf">pdf</a></p><p>Author: Andrew Smith, Hongyuan Zha, Xiao-ming Wu</p><p>Abstract: We study the convergence and the rate of convergence of a local manifold learning algorithm: LTSA [13]. The main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of LTSA. We derive a worst-case upper bound of errors for LTSA which naturally leads to a convergence result. We then derive the rate of convergence for LTSA in a special case. 1</p><p>Reference: <a title="nips-2008-51-reference" href="../nips2008_reference/nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We study the convergence and the rate of convergence of a local manifold learning algorithm: LTSA [13]. [sent-6, score-0.445]
</p><p>2 The main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of LTSA. [sent-7, score-0.41]
</p><p>3 We derive a worst-case upper bound of errors for LTSA which naturally leads to a convergence result. [sent-8, score-0.276]
</p><p>4 We then derive the rate of convergence for LTSA in a special case. [sent-9, score-0.163]
</p><p>5 The main contribution of this paper is to establish some asymptotic properties of a local manifold learning algorithm: LTSA [13], as well as a demonstration of some of its limitations. [sent-12, score-0.243]
</p><p>6 The key idea in the analysis is to treat the solutions computed by LTSA as invariant subspaces of certain matrices, and then carry out a matrix perturbation analysis. [sent-13, score-0.506]
</p><p>7 Many efﬁcient ML algorithms have been developed including locally linear embedding (LLE) [6], ISOMAP [9], charting [2], local tangent space alignment (LTSA) [13], Laplacian eigenmaps [1], and Hessian eigenmaps [3]. [sent-14, score-0.653]
</p><p>8 A common feature of many of these manifold learning algorithms is that their solutions correspond to invariant subspaces, typically the eigenspace associated with the smallest eigenvalues of a kernel or alignment matrix. [sent-15, score-0.547]
</p><p>9 Second, the solution to each step of the LTSA algorithm is an invariant subspace, which makes analysis of its performance more tractable. [sent-21, score-0.188]
</p><p>10 , LLE, Laplacian eigenmaps and Hessian eigenmaps) suggests that our results may generalize. [sent-24, score-0.167]
</p><p>11 Rate of convergence under a special case is derived in Section 4. [sent-30, score-0.076]
</p><p>12 1  2  Manifold Learning and LTSA  We formulate the manifold learning problem as follows. [sent-33, score-0.191]
</p><p>13 For a positive integer n, let yi ∈ I D , i = R 1, 2, . [sent-34, score-0.075]
</p><p>14 We assume that there is a mapping f : I d → I D which satisﬁes R R a set of regularity conditions (detailed in the next subsection). [sent-38, score-0.094]
</p><p>15 In addition, we require another set of (possibly multivariate) values xi ∈ I d , d < D, i = 1, 2, . [sent-39, score-0.084]
</p><p>16 , n, such that R yi = f (xi ) + εi ,  i = 1, 2, . [sent-42, score-0.075]
</p><p>17 The central questions of manifold learning are: 1) Can we ﬁnd a set of low-dimensional vectors such that equation (1) holds? [sent-49, score-0.258]
</p><p>18 8  1  Figure 1: An illustrative example of LTSA in nonparametric dimension reduction. [sent-99, score-0.097]
</p><p>19 An illustrative example of dimension reduction that makes our formulation more concrete is given in Figure 1. [sent-101, score-0.158]
</p><p>20 For example, if the mapping f (·) and point set {xi } satisfy (1), so do f (A−1 (· − b)) and {Axi + b}, where A is an invertible d by d matrix and b is a d-dimensional vector. [sent-110, score-0.088]
</p><p>21 As being common in the manifold-learning literature, we adopt the following condition on f . [sent-111, score-0.091]
</p><p>22 1 (Local Isometry) The mapping f is locally isometric: For any ε > 0 and x in the domain of f , let Nε (x) = {z : z − x 2 < ε} denote an ε-neighborhood of x using Euclidean distance. [sent-113, score-0.071]
</p><p>23 The above condition indicates that in a local sense, f preserves Euclidean distance. [sent-115, score-0.143]
</p><p>24 2 The matrix J(f ; x0 ) is orthonormal for any x0 , i. [sent-122, score-0.101]
</p><p>25 For example, for any d by d orthogonal matrix O and any d-dimensional vector b, if f (·) and {xi } satisfy (1) and Condition 2. [sent-126, score-0.104]
</p><p>26 We can force b to be 0 by imposing the condition that i xi = 0. [sent-128, score-0.175]
</p><p>27 3 (Local Linear Independence Condition) Let Yi ∈ I D×k , 1 ≤ i ≤ n, denote a R matrix whose columns are made by the ith observation yi and its k − 1 nearest neighbors. [sent-132, score-0.243]
</p><p>28 We choose k − 1 neighbors so that the matrix Yi has k columns. [sent-133, score-0.093]
</p><p>29 For any 1 ≤ i ≤ n, the rank of Yi P k is at least d; in other words, the dth largest singular value of matrix Yi P k is greater than 0. [sent-135, score-0.166]
</p><p>30 1 In the above, we use the projection matrix P k = Ik − k ·1k 1T , where Ik is the k by k identity matrix k and 1k is a k-dimensional column vector of ones. [sent-136, score-0.118]
</p><p>31 The regularity of the manifold can be determined by the Hessians of the mapping. [sent-137, score-0.256]
</p><p>32 The following condition ensures that f is locally smooth. [sent-147, score-0.133]
</p><p>33 We impose a bound on all the components of the Hessians. [sent-148, score-0.088]
</p><p>34 3  Solutions as Invariant Subspaces and a Related Metric  We now give a more detailed discussion of invariant subspaces. [sent-152, score-0.188]
</p><p>35 Let R(X) denote the subspace spanned by the columns of X. [sent-153, score-0.291]
</p><p>36 If the set {Oxi }, where O is a d by d orthogonal square matrix, forms another solution to the dimension reduction problem, we have (Ox1 , Ox2 , · · · , Oxn )T = XOT . [sent-162, score-0.168]
</p><p>37 The goal of our performance analysis is to answer the following question: Letting tan(·, ·) 2 denote the Euclidean norm of the vector of canonical angles between two invariant subspaces ([8, Section I. [sent-165, score-0.317]
</p><p>38 Then the row vectors of Vi are the d right singular vectors of Yi P k . [sent-176, score-0.191]
</p><p>39 The solution to LTSA corresponds to the invariant subspace which is spanned and determined by the eigenvectors associated with the 2nd to the (d + 1)st smallest eigenvalues of the matrix T (S1 , . [sent-178, score-0.558]
</p><p>40 R 3  As mentioned earlier, the subspace spanned by the eigenvectors associated with the 2nd to the (d + 1)st smallest eigenvalues of the matrix in 2 is an invariant subspace, which will be analyzed using matrix perturbation techniques. [sent-192, score-0.718]
</p><p>41 3  Perturbation Analysis  We now carry out a perturbation analysis on the reformulated version of LTSA. [sent-194, score-0.132]
</p><p>42 2), we derive the variation of the null space under global alignment. [sent-201, score-0.12]
</p><p>43 1  Local Coordinates  Let X be the matrix of true parameters. [sent-203, score-0.087]
</p><p>44 , the columns of Xi are made by xi and those xj ’s that correspond to the k − 1 nearest neighbors of yi . [sent-206, score-0.302]
</p><p>45 We require a bound on the size of the local neighborhoods deﬁned by the Xi ’s. [sent-207, score-0.195]
</p><p>46 1 (Universal Bound on the Sizes of Neighborhoods) For all i, 1 ≤ i ≤ n, we have τi < τ , where τ is a prescribed constant and τi is an upper bound on the distance between two columns of Xi : τi = maxxj ,xk xj − xk , where the maximum is taken over all columns of Xi . [sent-209, score-0.269]
</p><p>47 We will need conditions on the local tangent spaces. [sent-211, score-0.124]
</p><p>48 Let dmin,i (respectively, dmax,i ) denote the minimum (respectively, maximum) singular values of Xi P k . [sent-212, score-0.107]
</p><p>49 1≤i≤n  1≤i≤n  √  We can bound dmax as dmin ≤ dmax ≤ τ k [5]. [sent-214, score-0.282]
</p><p>50 2 (Local Tangent Space) There exists a constant C2 > 0, such that C2 · τ ≤ dmin . [sent-216, score-0.068]
</p><p>51 (3)  The above can roughly be thought of as requiring that the local dimension of the manifold remain constant (i. [sent-217, score-0.305]
</p><p>52 ) The following condition deﬁnes a global bound on the errors (εi ). [sent-220, score-0.253]
</p><p>53 3 (Universal Error Bound) There exists σ > 0, such that ∀i, 1 ≤ i ≤ n, we have yi − f (xi ) ∞ < σ. [sent-222, score-0.075]
</p><p>54 τ It is reasonable to require that the error bound (σ) be smaller than the size of the neighborhood (τ ), which is reﬂected in the above condition. [sent-226, score-0.131]
</p><p>55 Within each neighborhood, we give a perturbation bound between an invariant subspace spanned by the true parametrization and the invariant subspace spanned by the singular vectors of the matrix of noisy observations. [sent-227, score-1.297]
</p><p>56 Let Xi P k = Ai Di Bi be the singular value decomposition of the matrix Xi P k ; here Ai ∈ I d×d is orthogonal (Ai AT = Id ), Di ∈ I d×d is diagonal, and the rows of Bi ∈ I d×k R R R i T are the right singular vectors corresponding to the largest singular values (Bi Bi = Id ). [sent-228, score-0.451]
</p><p>57 (4) Let Yi P k = Ai Di Bi be the singular value decomposition of Yi P k , and assume that this is the (0) “thin” decomposition of rank d. [sent-230, score-0.107]
</p><p>58 We may think of this as the perturbed version of J(f ; xi )Xi P k . [sent-231, score-0.084]
</p><p>59 T T Let R(Bi ) (respectively, R(Bi )) denote the invariant subspace that is spanned by the columns of T T matrix Bi (respectively, Bi ). [sent-233, score-0.538]
</p><p>60 4 Given invariant subspaces R(Bi ) and R(Bi )) as deﬁned above, we have T T lim sin(R(Bi ), R(Bi ))  2  τ →0  σ + C1 τ , τ  ≤ C3  where C3 is a constant that depends on k, D and C2 . [sent-235, score-0.4]
</p><p>61 The above gives an upper bound on the deviation of the local invariant subspace in step 1 of the modiﬁed LTSA. [sent-237, score-0.491]
</p><p>62 It will be used later to prove a global upper bound. [sent-238, score-0.083]
</p><p>63 We will derive an upper bound on the angle between the invariant subspace spanned by the result of LTSA and the space spanned by the true parameters. [sent-247, score-0.742]
</p><p>64 It is not hard to verify that the row vectors of (1n , X) span the (d + 1)-dimensional null space of the matrix: T T (S1 , . [sent-253, score-0.131]
</p><p>65 Although in our original R n problem formulation, we made no assumption about the xi ’s, we can still assume that the columns of X are orthonormal because we can transform any set of xi ’s into an orthonormal set by rescaling the columns and multiplying by an orthogonal matrix. [sent-264, score-0.399]
</p><p>66 , as τ → 0, we have τ 2 σ τ  n i=1  1 + 2 C1 τ ·  Si  min  ∞  > 0 and  min  goes to 0 at a slower  → 0. [sent-282, score-0.128]
</p><p>67 min  As discussed in [12, 11], this condition is actually related to the amount of overlap between the nearest neighbor sets. [sent-283, score-0.213]
</p><p>68 7 (Main Theorem) lim tan(R(X), R(X))  τ →0  2  ≤  n i=1  C3 ( σ + C1 τ ) · τ  Si  ∞  . [sent-285, score-0.083]
</p><p>69 (7)  min  As mentioned in the Introduction, the above theorem gives a worst-case bound on the performance of LTSA. [sent-286, score-0.179]
</p><p>70 4  A Preliminary Result on the Rate of Convergence  We discuss the rate of convergence for LTSA (to the true underlying manifold structure) in the aforementioned framework. [sent-292, score-0.345]
</p><p>71 We modify the LTSA (mainly on how to choose the size of the nearest neighborhood) for a reason that will become evident later. [sent-293, score-0.087]
</p><p>72 However, allowing k to be a function of the sample size n, say k = nα , where α ∈ [0, 1) allows us to control the asymptotic behavior of min along with the convergence of the estimated alignment matrix to the true alignment matrix. [sent-297, score-0.429]
</p><p>73 Consider our original bound on the angle between the true coordinates and the estimated coordinates: lim tan(R(X), R(X))  τ →0  2  ≤  n i=1  C3 ( σ + C1 τ ) · τ  Si  ∞  . [sent-298, score-0.242]
</p><p>74 Plugging all this into the original equation and dropping the constants, we get lim tan(R(X), R(X))  τ →0  ≤  2  n  α−1 d  ·n  3α 2  · Constant. [sent-309, score-0.083]
</p><p>75 , the generating coordinates can follow a more general distribution rather than only lying in a uniform grid), then we have lim tan(R(X), R(X))  n  α−1 d  α  · n 2 · nα  · Constant. [sent-312, score-0.126]
</p><p>76 α−1 n5α · n4· d Now the exponent is a function only of α and the constant d. [sent-313, score-0.066]
</p><p>77 We can try to solve for α such that the convergence is as fast as possible. [sent-314, score-0.076]
</p><p>78 Simplifying the exponents, we get τ →0  lim tan(R(X), R(X))  τ →0  2  ≤  2  ≤n  −7α α−1 2 −3( d )  · Constant. [sent-315, score-0.083]
</p><p>79 6  However, in the proof of the convergence of LTSA, it is assumed that the errors in the local step converge to 0. [sent-317, score-0.161]
</p><p>80 2 d d d+2 Note that this bound is strictly less than 1 for all positive integers d, so our possible choices of α are restricted further. [sent-322, score-0.088]
</p><p>81 Further, it is easy to see 2 that for all d, choosing an exponent roughly equal to d+2 will always yield a bound converging to 0. [sent-324, score-0.154]
</p><p>82 The following table gives the optimal exponents for selected values of d along with the convergence rate of limτ →0 tan(R(X), R(X)) 2 . [sent-325, score-0.165]
</p><p>83 In general, using the optimal value of α, the convergence −4 rate will be roughly n d+2 . [sent-326, score-0.126]
</p><p>84 5  Discussion  To the best of our knowledge, the performance analysis that is based on invariant subspaces is new. [sent-341, score-0.317]
</p><p>85 Consequently the worst-case upper bound is the ﬁrst of its kind. [sent-342, score-0.13]
</p><p>86 There are still open questions to be addressed (Section 5. [sent-343, score-0.067]
</p><p>87 In addition to a discussion on the relation of LTSA to existing dimension reduction methodologies, we will also address relation with known results as well (Section 5. [sent-345, score-0.123]
</p><p>88 1  Open Questions  The rate of convergence of min is determined by the topological structure of f . [sent-348, score-0.19]
</p><p>89 One can imagine that it is true when the error bound (σ) goes to 0 and when the xi ’s are sampled with a sufﬁcient density in the support of f . [sent-353, score-0.2]
</p><p>90 An open problem is how to derive the rate of convergence of τ → 0 as a function of the topology of f and the sampling scheme. [sent-354, score-0.189]
</p><p>91 However, Zhang and Zha [13] do not interpret their solutions as invariant subspaces, and hence their analysis does not yield a worst case bound as we have derived here. [sent-358, score-0.305]
</p><p>92 7  Reviewing the original papers on LLE [6], Laplacian eigenmaps [1], and Hessian eigenmaps [3] reveals that their solutions are subspaces spanned by a speciﬁc set of eigenvectors. [sent-359, score-0.611]
</p><p>93 ISOMAP, another popular manifold learning algorithm, is an exception. [sent-363, score-0.191]
</p><p>94 Its solution cannot immediately be rendered as an invariant subspace. [sent-364, score-0.188]
</p><p>95 However, ISOMAP calls for MDS, which can be associated with an invariant subspace; one may derive an analytical result through this route. [sent-365, score-0.225]
</p><p>96 6  Conclusion  We derive an upper bound of the distance between two invariant subspaces that are associated with the numerical output of LTSA and an assumed intrinsic parametrization. [sent-366, score-0.484]
</p><p>97 Such a bound describes the performance of LTSA with errors in the observations, and thus creates a theoretical foundation for its use in real-world applications in which we would naturally expect such errors to be present. [sent-367, score-0.154]
</p><p>98 Performance analysis of a manifold learning algorithm in dimension reduction. [sent-400, score-0.253]
</p><p>99 Spectral properties of the alignment matrices in manifold learning. [sent-450, score-0.292]
</p><p>100 Principal manifolds and nonlinear dimension reduction via local tangent space alignment. [sent-460, score-0.247]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ltsa', 0.651), ('bi', 0.225), ('manifold', 0.191), ('invariant', 0.188), ('eigenmaps', 0.167), ('tan', 0.156), ('subspaces', 0.129), ('subspace', 0.121), ('spanned', 0.119), ('zha', 0.114), ('si', 0.113), ('singular', 0.107), ('perturbation', 0.101), ('alignment', 0.101), ('huo', 0.091), ('condition', 0.091), ('bound', 0.088), ('xi', 0.084), ('lim', 0.083), ('sn', 0.082), ('id', 0.081), ('oxi', 0.078), ('convergence', 0.076), ('yi', 0.075), ('hessian', 0.074), ('tangent', 0.072), ('dmin', 0.068), ('sub', 0.067), ('exponent', 0.066), ('regularity', 0.065), ('min', 0.064), ('bn', 0.063), ('dmax', 0.063), ('isomap', 0.063), ('kd', 0.063), ('dimension', 0.062), ('reduction', 0.061), ('matrix', 0.059), ('georgia', 0.058), ('lle', 0.058), ('nearest', 0.058), ('ml', 0.057), ('neighborhoods', 0.055), ('local', 0.052), ('charting', 0.052), ('derivable', 0.052), ('disregarding', 0.052), ('xot', 0.052), ('columns', 0.051), ('rate', 0.05), ('ik', 0.049), ('laplacian', 0.047), ('orthogonal', 0.045), ('coordinates', 0.043), ('neighborhood', 0.043), ('upper', 0.042), ('locally', 0.042), ('null', 0.042), ('orthonormal', 0.042), ('mn', 0.041), ('global', 0.041), ('questions', 0.041), ('chapter', 0.04), ('atlanta', 0.039), ('exponents', 0.039), ('eigenvalues', 0.038), ('constants', 0.038), ('prescribed', 0.037), ('derive', 0.037), ('illustrative', 0.035), ('ai', 0.035), ('neighbors', 0.034), ('book', 0.034), ('ga', 0.034), ('diag', 0.033), ('eigenvectors', 0.033), ('errors', 0.033), ('parametrization', 0.032), ('row', 0.032), ('verify', 0.031), ('reformulated', 0.031), ('vi', 0.03), ('jk', 0.029), ('straight', 0.029), ('mapping', 0.029), ('solutions', 0.029), ('di', 0.029), ('evident', 0.029), ('true', 0.028), ('vn', 0.028), ('march', 0.028), ('universal', 0.028), ('euclidean', 0.028), ('letting', 0.027), ('invariance', 0.027), ('hi', 0.027), ('theorem', 0.027), ('open', 0.026), ('sin', 0.026), ('vectors', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="51-tfidf-1" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>Author: Andrew Smith, Hongyuan Zha, Xiao-ming Wu</p><p>Abstract: We study the convergence and the rate of convergence of a local manifold learning algorithm: LTSA [13]. The main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of LTSA. We derive a worst-case upper bound of errors for LTSA which naturally leads to a convergence result. We then derive the rate of convergence for LTSA in a special case. 1</p><p>2 0.14672862 <a title="51-tfidf-2" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>Author: Jeremy Lewi, Robert Butera, David M. Schneider, Sarah Woolley, Liam Paninski</p><p>Abstract: Sequential optimal design methods hold great promise for improving the efﬁciency of neurophysiology experiments. However, previous methods for optimal experimental design have incorporated only weak prior information about the underlying neural system (e.g., the sparseness or smoothness of the receptive ﬁeld). Here we describe how to use stronger prior information, in the form of parametric models of the receptive ﬁeld, in order to construct optimal stimuli and further improve the efﬁciency of our experiments. For example, if we believe that the receptive ﬁeld is well-approximated by a Gabor function, then our method constructs stimuli that optimally constrain the Gabor parameters (orientation, spatial frequency, etc.) using as few experimental trials as possible. More generally, we may believe a priori that the receptive ﬁeld lies near a known sub-manifold of the full parameter space; in this case, our method chooses stimuli in order to reduce the uncertainty along the tangent space of this sub-manifold as rapidly as possible. Applications to simulated and real data indicate that these methods may in many cases improve the experimental efﬁciency. 1</p><p>3 0.11563656 <a title="51-tfidf-3" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>Author: Florian Steinke, Matthias Hein</p><p>Abstract: This paper discusses non-parametric regression between Riemannian manifolds. This learning problem arises frequently in many application areas ranging from signal processing, computer vision, over robotics to computer graphics. We present a new algorithmic scheme for the solution of this general learning problem based on regularized empirical risk minimization. The regularization functional takes into account the geometry of input and output manifold, and we show that it implements a prior which is particularly natural. Moreover, we demonstrate that our algorithm performs well in a difﬁcult surface registration problem. 1</p><p>4 0.10046745 <a title="51-tfidf-4" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>Author: Ling Huang, Donghui Yan, Nina Taft, Michael I. Jordan</p><p>Abstract: Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a speciﬁcation of permitted loss in clustering performance. 1</p><p>5 0.096165031 <a title="51-tfidf-5" href="./nips-2008-Extended_Grassmann_Kernels_for_Subspace-Based_Learning.html">80 nips-2008-Extended Grassmann Kernels for Subspace-Based Learning</a></p>
<p>Author: Jihun Hamm, Daniel D. Lee</p><p>Abstract: Subspace-based learning problems involve data whose elements are linear subspaces of a vector space. To handle such data structures, Grassmann kernels have been proposed and used previously. In this paper, we analyze the relationship between Grassmann kernels and probabilistic similarity measures. Firstly, we show that the KL distance in the limit yields the Projection kernel on the Grassmann manifold, whereas the Bhattacharyya kernel becomes trivial in the limit and is suboptimal for subspace-based problems. Secondly, based on our analysis of the KL distance, we propose extensions of the Projection kernel which can be extended to the set of afﬁne as well as scaled subspaces. We demonstrate the advantages of these extended kernels for classiﬁcation and recognition tasks with Support Vector Machines and Kernel Discriminant Analysis using synthetic and real image databases. 1</p><p>6 0.089114897 <a title="51-tfidf-6" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>7 0.081172526 <a title="51-tfidf-7" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>8 0.080498546 <a title="51-tfidf-8" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>9 0.078754514 <a title="51-tfidf-9" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>10 0.078715354 <a title="51-tfidf-10" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>11 0.07813476 <a title="51-tfidf-11" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>12 0.075460143 <a title="51-tfidf-12" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>13 0.071878612 <a title="51-tfidf-13" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>14 0.06834159 <a title="51-tfidf-14" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>15 0.068276778 <a title="51-tfidf-15" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>16 0.067310125 <a title="51-tfidf-16" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>17 0.06696777 <a title="51-tfidf-17" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>18 0.064079672 <a title="51-tfidf-18" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>19 0.063703015 <a title="51-tfidf-19" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>20 0.06167867 <a title="51-tfidf-20" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.191), (1, -0.019), (2, -0.076), (3, 0.098), (4, 0.07), (5, -0.039), (6, 0.034), (7, -0.043), (8, 0.025), (9, -0.02), (10, 0.068), (11, -0.039), (12, -0.042), (13, 0.083), (14, -0.063), (15, 0.041), (16, 0.096), (17, 0.108), (18, 0.039), (19, -0.034), (20, -0.021), (21, -0.097), (22, -0.045), (23, -0.064), (24, -0.025), (25, 0.091), (26, -0.015), (27, -0.062), (28, 0.15), (29, 0.167), (30, 0.005), (31, -0.052), (32, 0.161), (33, 0.039), (34, 0.009), (35, -0.227), (36, 0.023), (37, 0.008), (38, -0.025), (39, -0.157), (40, -0.052), (41, -0.065), (42, -0.05), (43, 0.086), (44, 0.101), (45, 0.076), (46, 0.079), (47, -0.059), (48, -0.003), (49, -0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9375056 <a title="51-lsi-1" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>Author: Andrew Smith, Hongyuan Zha, Xiao-ming Wu</p><p>Abstract: We study the convergence and the rate of convergence of a local manifold learning algorithm: LTSA [13]. The main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of LTSA. We derive a worst-case upper bound of errors for LTSA which naturally leads to a convergence result. We then derive the rate of convergence for LTSA in a special case. 1</p><p>2 0.76523 <a title="51-lsi-2" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>Author: Florian Steinke, Matthias Hein</p><p>Abstract: This paper discusses non-parametric regression between Riemannian manifolds. This learning problem arises frequently in many application areas ranging from signal processing, computer vision, over robotics to computer graphics. We present a new algorithmic scheme for the solution of this general learning problem based on regularized empirical risk minimization. The regularization functional takes into account the geometry of input and output manifold, and we show that it implements a prior which is particularly natural. Moreover, we demonstrate that our algorithm performs well in a difﬁcult surface registration problem. 1</p><p>3 0.65504581 <a title="51-lsi-3" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>Author: Jeremy Lewi, Robert Butera, David M. Schneider, Sarah Woolley, Liam Paninski</p><p>Abstract: Sequential optimal design methods hold great promise for improving the efﬁciency of neurophysiology experiments. However, previous methods for optimal experimental design have incorporated only weak prior information about the underlying neural system (e.g., the sparseness or smoothness of the receptive ﬁeld). Here we describe how to use stronger prior information, in the form of parametric models of the receptive ﬁeld, in order to construct optimal stimuli and further improve the efﬁciency of our experiments. For example, if we believe that the receptive ﬁeld is well-approximated by a Gabor function, then our method constructs stimuli that optimally constrain the Gabor parameters (orientation, spatial frequency, etc.) using as few experimental trials as possible. More generally, we may believe a priori that the receptive ﬁeld lies near a known sub-manifold of the full parameter space; in this case, our method chooses stimuli in order to reduce the uncertainty along the tangent space of this sub-manifold as rapidly as possible. Applications to simulated and real data indicate that these methods may in many cases improve the experimental efﬁciency. 1</p><p>4 0.62375736 <a title="51-lsi-4" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>Author: Michael P. Holmes, Jr. Isbell, Charles Lee, Alexander G. Gray</p><p>Abstract: The Singular Value Decomposition is a key operation in many machine learning methods. Its computational cost, however, makes it unscalable and impractical for applications involving large datasets or real-time responsiveness, which are becoming increasingly common. We present a new method, QUIC-SVD, for fast approximation of the whole-matrix SVD based on a new sampling mechanism called the cosine tree. Our empirical tests show speedups of several orders of magnitude over exact SVD. Such scalability should enable QUIC-SVD to accelerate and enable a wide array of SVD-based methods and applications. 1</p><p>5 0.59587514 <a title="51-lsi-5" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>Author: Qiang Wu, Sayan Mukherjee, Feng Liang</p><p>Abstract: We developed localized sliced inverse regression for supervised dimension reduction. It has the advantages of preventing degeneracy, increasing estimation accuracy, and automatic subclass discovery in classiﬁcation problems. A semisupervised version is proposed for the use of unlabeled data. The utility is illustrated on simulated as well as real data sets.</p><p>6 0.54228956 <a title="51-lsi-6" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>7 0.42483267 <a title="51-lsi-7" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>8 0.40464023 <a title="51-lsi-8" href="./nips-2008-Extended_Grassmann_Kernels_for_Subspace-Based_Learning.html">80 nips-2008-Extended Grassmann Kernels for Subspace-Based Learning</a></p>
<p>9 0.38542664 <a title="51-lsi-9" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>10 0.37675756 <a title="51-lsi-10" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>11 0.37293833 <a title="51-lsi-11" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>12 0.36839631 <a title="51-lsi-12" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>13 0.35541266 <a title="51-lsi-13" href="./nips-2008-Fast_High-dimensional_Kernel_Summations_Using_the_Monte_Carlo_Multipole_Method.html">83 nips-2008-Fast High-dimensional Kernel Summations Using the Monte Carlo Multipole Method</a></p>
<p>14 0.35480961 <a title="51-lsi-14" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>15 0.35411164 <a title="51-lsi-15" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>16 0.33900964 <a title="51-lsi-16" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>17 0.33000332 <a title="51-lsi-17" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>18 0.32957548 <a title="51-lsi-18" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>19 0.32516766 <a title="51-lsi-19" href="./nips-2008-Characteristic_Kernels_on_Groups_and_Semigroups.html">44 nips-2008-Characteristic Kernels on Groups and Semigroups</a></p>
<p>20 0.32512861 <a title="51-lsi-20" href="./nips-2008-Empirical_performance_maximization_for_linear_rank_statistics.html">72 nips-2008-Empirical performance maximization for linear rank statistics</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.043), (7, 0.546), (12, 0.027), (28, 0.15), (57, 0.037), (59, 0.012), (63, 0.025), (77, 0.043), (78, 0.011), (83, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.96106458 <a title="51-lda-1" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>Author: Dominik Endres, Peter Foldiak</p><p>Abstract: We propose a novel application of Formal Concept Analysis (FCA) to neural decoding: instead of just trying to ﬁgure out which stimulus was presented, we demonstrate how to explore the semantic relationships in the neural representation of large sets of stimuli. FCA provides a way of displaying and interpreting such relationships via concept lattices. We explore the effects of neural code sparsity on the lattice. We then analyze neurophysiological data from high-level visual cortical area STSa, using an exact Bayesian approach to construct the formal context needed by FCA. Prominent features of the resulting concept lattices are discussed, including hierarchical face representation and indications for a product-of-experts code in real neurons. 1</p><p>2 0.9538331 <a title="51-lda-2" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>Author: Ben Calderhead, Mark Girolami, Neil D. Lawrence</p><p>Abstract: Identiﬁcation and comparison of nonlinear dynamical system models using noisy and sparse experimental data is a vital task in many ﬁelds, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced. We present an accelerated sampling procedure which enables Bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of Gaussian processes (GP). Our method involves GP regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time. We demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods. 1</p><p>3 0.93424678 <a title="51-lda-3" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>Author: Kai Yu, Wei Xu, Yihong Gong</p><p>Abstract: In this paper we aim to train deep neural networks for rapid visual recognition. The task is highly challenging, largely due to the lack of a meaningful regularizer on the functions realized by the networks. We propose a novel regularization method that takes advantage of kernel methods, where an oracle kernel function represents prior knowledge about the recognition task of interest. We derive an efﬁcient algorithm using stochastic gradient descent, and demonstrate encouraging results on a wide range of recognition tasks, in terms of both accuracy and speed. 1</p><p>same-paper 4 0.9230445 <a title="51-lda-4" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>Author: Andrew Smith, Hongyuan Zha, Xiao-ming Wu</p><p>Abstract: We study the convergence and the rate of convergence of a local manifold learning algorithm: LTSA [13]. The main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of LTSA. We derive a worst-case upper bound of errors for LTSA which naturally leads to a convergence result. We then derive the rate of convergence for LTSA in a special case. 1</p><p>5 0.88458204 <a title="51-lda-5" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>Author: Pietro Berkes, Frank Wood, Jonathan W. Pillow</p><p>Abstract: The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. However, there is no simple model that can simultaneously account for (1) marginal distributions over single-neuron spike counts that are discrete and non-negative; and (2) joint distributions over the responses of multiple neurons that are often strongly dependent. Here, we show that both marginal and joint properties of neural responses can be captured using copula models. Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefﬁcients. We explore a variety of copula models for joint neural response distributions, and derive an efﬁcient maximum likelihood procedure for estimating them. We apply these models to neuronal data collected in macaque pre-motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons. We ﬁnd that more than one third of neuron pairs shows dependency concentrated in the lower or upper tails for their ﬁring rate distribution. 1</p><p>6 0.69101638 <a title="51-lda-6" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>7 0.67983729 <a title="51-lda-7" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>8 0.66297084 <a title="51-lda-8" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>9 0.65292871 <a title="51-lda-9" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>10 0.64895761 <a title="51-lda-10" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>11 0.64561772 <a title="51-lda-11" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>12 0.63567209 <a title="51-lda-12" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>13 0.62822437 <a title="51-lda-13" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>14 0.62231809 <a title="51-lda-14" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>15 0.61970437 <a title="51-lda-15" href="./nips-2008-Fast_High-dimensional_Kernel_Summations_Using_the_Monte_Carlo_Multipole_Method.html">83 nips-2008-Fast High-dimensional Kernel Summations Using the Monte Carlo Multipole Method</a></p>
<p>16 0.61915022 <a title="51-lda-16" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>17 0.61340213 <a title="51-lda-17" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>18 0.60966223 <a title="51-lda-18" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>19 0.60800737 <a title="51-lda-19" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>20 0.60646772 <a title="51-lda-20" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
