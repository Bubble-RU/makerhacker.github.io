<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>105 nips-2008-Improving on Expectation Propagation</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-105" href="#">nips2008-105</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>105 nips-2008-Improving on Expectation Propagation</h1>
<br/><p>Source: <a title="nips-2008-105-pdf" href="http://papers.nips.cc/paper/3613-improving-on-expectation-propagation.pdf">pdf</a></p><p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: A series of corrections is developed for the ﬁxed points of Expectation Propagation (EP), which is one of the most popular methods for approximate probabilistic inference. These corrections can lead to improvements of the inference approximation or serve as a sanity check, indicating when EP yields unrealiable results.</p><p>Reference: <a title="nips-2008-105-reference" href="../nips2008_reference/nips-2008-Improving_on_Expectation_Propagation_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 dk  Abstract A series of corrections is developed for the ﬁxed points of Expectation Propagation (EP), which is one of the most popular methods for approximate probabilistic inference. [sent-6, score-0.605]
</p><p>2 These corrections can lead to improvements of the inference approximation or serve as a sanity check, indicating when EP yields unrealiable results. [sent-7, score-0.736]
</p><p>3 1 Introduction The expectation propagation (EP) message passing algorithm is often considered as the method of choice for approximate Bayesian inference when both good accuracy and computational efﬁciency are required [5]. [sent-8, score-0.247]
</p><p>4 However, while such empirical studies hold great value, they can not guarantee the same performance on other data sets or when completely different types of Bayesian models are considered. [sent-10, score-0.022]
</p><p>5 In this paper methods are developed to assess the quality of the EP approximation. [sent-11, score-0.044]
</p><p>6 We compute explicit expressions for the remainder terms of the approximation. [sent-12, score-0.067]
</p><p>7 This leads to various corrections for partition functions and posterior distributions. [sent-13, score-0.612]
</p><p>8 Under the hypothesis that the EP approximation works well, we identify quantities which can be assumed to be small and can be used in a series expansion of the corrections with increasing complexity. [sent-14, score-0.769]
</p><p>9 The computation of low order corrections in this expansion is often feasible, typically require only moderate computational efforts, and can lead to an improvement to the EP approximation or to the indication that the approximation cannot be trusted. [sent-15, score-0.84]
</p><p>10 2 Expectation Propagation in a Nutshell Since it is the goal of this paper to compute corrections to the EP approximation, we will not discuss details of EP algorithms but rather characterise the ﬁxed points which are reached when such algorithms converge. [sent-16, score-0.526]
</p><p>11 EP is applied to probabilistic models with an unobserved latent variable x having an intractable distribution p(x). [sent-17, score-0.098]
</p><p>12 In applications p(x) is usually the Bayesian posterior distribution conditioned on a set of observations. [sent-18, score-0.054]
</p><p>13 Since the dependency on the latter variables is not important for the subsequent theory, we will skip them in our notation. [sent-19, score-0.071]
</p><p>14 1  It is assumed that p(x) factorizes into a product of terms fn such that p(x) =  1 Z  where the normalising partition function Z = an approximation to p(x) in the form  fn (x) ,  (1)  n  dx  q(x) =  n  fn (x) is also intractable. [sent-20, score-1.29]
</p><p>15 We then assume  gn (x)  (2)  n  where the terms gn (x) belong to a tractable, e. [sent-21, score-0.535]
</p><p>16 To compute the optimal parameters of the gn term approximation a set of auxiliary tilted distributions is deﬁned via 1 q(x)fn (x) qn (x) = . [sent-24, score-0.775]
</p><p>17 (3) Zn gn (x) Here a single approximating term gn is replaced by an original term fn . [sent-25, score-0.825]
</p><p>18 Assuming that this replacement leaves qn still tractable, the parameters in gn are determined by the condition that q(x) and all qn (x) should be made as similar as possible. [sent-26, score-1.039]
</p><p>19 This is usually achieved by requiring that these distributions share a set of generalised moments (which usually coincide with the sufﬁcient statistics of the exponential family). [sent-27, score-0.267]
</p><p>20 Note, that we will not assume that this expectation consistency [8] for the moments is derived by minimising a Kullback–Leibler divergence, as was done in the original derivations of EP [5]. [sent-28, score-0.195]
</p><p>21 Such an assumption would limit the applicability of the approximate inference and exclude e. [sent-29, score-0.114]
</p><p>22 the approximation of models with binary, Ising variables by a Gaussian model as in one of the applications in the last section. [sent-31, score-0.091]
</p><p>23 The corresponding approximation to the normalising partition function in (1) was given in [8] and [7] and reads in our present notation1 ZEP =  Zn . [sent-32, score-0.338]
</p><p>24 (4)  n  3 Corrections to EP An expression for the remainder terms which are neglected by the EP approximation can be obtained by solving for fn in (3), and taking the product to get fn (x) = n  Hence Z =  dx R=  n n  Zn qn (x)gn (x) q(x)  qn (x) q(x)  = ZEP q(x) n  . [sent-33, score-1.562]
</p><p>25 (5)  fn (x) = ZEP R, with dx q(x) n  qn (x) q(x)  and p(x) =  1 q(x) R  n  qn (x) q(x)  . [sent-34, score-1.094]
</p><p>26 (6)  This shows that corrections to EP are small when all distributions qn are indeed close to q, justifying the optimality criterion of EP. [sent-35, score-0.95]
</p><p>27 Exact probabilistic inference with the corrections described here again leads to intractable computations. [sent-37, score-0.603]
</p><p>28 However, we can derive exact perturbation expansions involving a series of corrections with increasing computational complexity. [sent-38, score-0.756]
</p><p>29 Assuming that EP already yields a good approximation, the computation of a small number of these terms maybe sufﬁcient to obtain the most dominant corrections. [sent-39, score-0.089]
</p><p>30 On the other hand, when the leading corrections come out large or do not sufﬁciently decrease with order, this may indicate that the EP approximation is inaccurate. [sent-40, score-0.612]
</p><p>31 Two such perturbation expansions are be presented in this section. [sent-41, score-0.158]
</p><p>32 1  The deﬁnition of partition functions Zn is slightly different from previous works. [sent-42, score-0.09]
</p><p>33 1 Expansion I: Clusters n (x) The most basic expansion is based on the variables εn (x) = qq(x) − 1 which we can assume to be typically small, when the EP approximation is good. [sent-44, score-0.194]
</p><p>34 Expanding the products in (6) we obtain the correction to the partition function  R=  dx q(x)  (1 + εn (x))  (7)  n  =1+  εn1 (x)εn2 (x)  q  +  εn1 (x)εn2 (x)εn3 (x)  q  + . [sent-45, score-0.235]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('ep', 0.504), ('corrections', 0.5), ('qn', 0.364), ('fn', 0.273), ('gn', 0.257), ('zep', 0.17), ('zn', 0.128), ('normalising', 0.114), ('ulrich', 0.114), ('expansions', 0.103), ('dx', 0.093), ('approximation', 0.091), ('partition', 0.09), ('propagation', 0.082), ('expansion', 0.08), ('moments', 0.066), ('expectation', 0.057), ('perturbation', 0.055), ('sanity', 0.05), ('skip', 0.05), ('denmark', 0.045), ('manfred', 0.045), ('opper', 0.045), ('kullback', 0.045), ('justifying', 0.045), ('leibler', 0.045), ('qq', 0.045), ('intractable', 0.044), ('reads', 0.043), ('neglected', 0.043), ('factorizes', 0.043), ('minimising', 0.043), ('remainder', 0.041), ('maybe', 0.04), ('tractable', 0.04), ('ising', 0.038), ('generalised', 0.038), ('series', 0.035), ('coincide', 0.034), ('exclude', 0.034), ('tu', 0.034), ('inference', 0.033), ('informatics', 0.032), ('usually', 0.032), ('remarkably', 0.031), ('replacement', 0.031), ('correction', 0.031), ('efforts', 0.03), ('expanding', 0.029), ('moderate', 0.029), ('derivations', 0.029), ('unobserved', 0.028), ('family', 0.028), ('dominant', 0.028), ('berlin', 0.028), ('gp', 0.027), ('passing', 0.027), ('message', 0.026), ('probabilistic', 0.026), ('reached', 0.026), ('expressions', 0.026), ('indication', 0.026), ('applicability', 0.025), ('bayesian', 0.025), ('extensive', 0.025), ('harder', 0.025), ('exponential', 0.024), ('increasing', 0.023), ('laboratory', 0.023), ('typically', 0.023), ('leaves', 0.023), ('mcmc', 0.023), ('auxiliary', 0.023), ('developed', 0.022), ('assuming', 0.022), ('posterior', 0.022), ('great', 0.022), ('modelling', 0.022), ('approximate', 0.022), ('serve', 0.022), ('assess', 0.022), ('exact', 0.021), ('check', 0.021), ('yields', 0.021), ('come', 0.021), ('dependency', 0.021), ('distributions', 0.021), ('products', 0.021), ('belong', 0.021), ('product', 0.02), ('requiring', 0.02), ('optimality', 0.02), ('feasible', 0.02), ('assumed', 0.02), ('quantities', 0.02), ('improvements', 0.019), ('involving', 0.019), ('term', 0.019), ('divergence', 0.019), ('improving', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="105-tfidf-1" href="./nips-2008-Improving_on_Expectation_Propagation.html">105 nips-2008-Improving on Expectation Propagation</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: A series of corrections is developed for the ﬁxed points of Expectation Propagation (EP), which is one of the most popular methods for approximate probabilistic inference. These corrections can lead to improvements of the inference approximation or serve as a sanity check, indicating when EP yields unrealiable results.</p><p>2 0.10231685 <a title="105-tfidf-2" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>Author: Cédric Archambeau, Francis R. Bach</p><p>Abstract: We present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a preprocessing tool for the construction of template attacks. 1</p><p>3 0.079444595 <a title="105-tfidf-3" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>Author: Ben Calderhead, Mark Girolami, Neil D. Lawrence</p><p>Abstract: Identiﬁcation and comparison of nonlinear dynamical system models using noisy and sparse experimental data is a vital task in many ﬁelds, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced. We present an accelerated sampling procedure which enables Bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of Gaussian processes (GP). Our method involves GP regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time. We demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods. 1</p><p>4 0.070874311 <a title="105-tfidf-4" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<p>Author: Norm Aleks, Stuart Russell, Michael G. Madden, Diane Morabito, Kristan Staudenmayer, Mitchell Cohen, Geoffrey T. Manley</p><p>Abstract: We describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit (ICU). In particular, we consider the arterial-line blood pressure sensor, which is subject to frequent data artifacts that cause false alarms in the ICU and make the raw data almost useless for automated decision making. The problem is complicated by the fact that the sensor data are averaged over ﬁxed intervals whereas the events causing data artifacts may occur at any time and often have durations signiﬁcantly shorter than the data collection interval. We show that careful modeling of the sensor, combined with a general technique for detecting sub-interval events and estimating their duration, enables detection of artifacts and accurate estimation of the underlying blood pressure values. Our model’s performance identifying artifacts is superior to two other classiﬁers’ and about as good as a physician’s. 1</p><p>5 0.069981053 <a title="105-tfidf-5" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>Author: Iain Murray, David MacKay, Ryan P. Adams</p><p>Abstract: We present the Gaussian Process Density Sampler (GPDS), an exchangeable generative model for use in nonparametric Bayesian density estimation. Samples drawn from the GPDS are consistent with exact, independent samples from a ﬁxed density function that is a transformation of a function drawn from a Gaussian process prior. Our formulation allows us to infer an unknown density from data using Markov chain Monte Carlo, which gives samples from the posterior distribution over density functions and from the predictive distribution on data space. We can also infer the hyperparameters of the Gaussian process. We compare this density modeling technique to several existing techniques on a toy problem and a skullreconstruction task. 1</p><p>6 0.064279221 <a title="105-tfidf-6" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>7 0.054758083 <a title="105-tfidf-7" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>8 0.049449503 <a title="105-tfidf-8" href="./nips-2008-Gates.html">89 nips-2008-Gates</a></p>
<p>9 0.04677945 <a title="105-tfidf-9" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>10 0.041755054 <a title="105-tfidf-10" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>11 0.039492182 <a title="105-tfidf-11" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>12 0.038967725 <a title="105-tfidf-12" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>13 0.038389377 <a title="105-tfidf-13" href="./nips-2008-Nonparametric_sparse_hierarchical_models_describe_V1_fMRI_responses_to_natural_images.html">156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</a></p>
<p>14 0.037596282 <a title="105-tfidf-14" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>15 0.036051553 <a title="105-tfidf-15" href="./nips-2008-Predictive_Indexing_for_Fast_Search.html">184 nips-2008-Predictive Indexing for Fast Search</a></p>
<p>16 0.035894666 <a title="105-tfidf-16" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>17 0.035805684 <a title="105-tfidf-17" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>18 0.034741212 <a title="105-tfidf-18" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>19 0.032236107 <a title="105-tfidf-19" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>20 0.032235228 <a title="105-tfidf-20" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.084), (1, -0.008), (2, 0.016), (3, 0.022), (4, 0.039), (5, -0.033), (6, 0.015), (7, 0.079), (8, 0.013), (9, 0.0), (10, -0.008), (11, 0.021), (12, 0.109), (13, -0.009), (14, 0.012), (15, -0.021), (16, -0.023), (17, 0.053), (18, 0.097), (19, -0.022), (20, 0.041), (21, -0.059), (22, -0.008), (23, -0.03), (24, -0.009), (25, -0.025), (26, 0.091), (27, -0.02), (28, -0.023), (29, -0.012), (30, -0.05), (31, -0.089), (32, -0.009), (33, 0.037), (34, 0.008), (35, 0.015), (36, -0.012), (37, 0.061), (38, 0.022), (39, 0.077), (40, 0.024), (41, 0.007), (42, -0.052), (43, -0.006), (44, 0.16), (45, 0.044), (46, 0.101), (47, 0.219), (48, -0.089), (49, 0.139)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96001595 <a title="105-lsi-1" href="./nips-2008-Improving_on_Expectation_Propagation.html">105 nips-2008-Improving on Expectation Propagation</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: A series of corrections is developed for the ﬁxed points of Expectation Propagation (EP), which is one of the most popular methods for approximate probabilistic inference. These corrections can lead to improvements of the inference approximation or serve as a sanity check, indicating when EP yields unrealiable results.</p><p>2 0.59226424 <a title="105-lsi-2" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<p>Author: Norm Aleks, Stuart Russell, Michael G. Madden, Diane Morabito, Kristan Staudenmayer, Mitchell Cohen, Geoffrey T. Manley</p><p>Abstract: We describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit (ICU). In particular, we consider the arterial-line blood pressure sensor, which is subject to frequent data artifacts that cause false alarms in the ICU and make the raw data almost useless for automated decision making. The problem is complicated by the fact that the sensor data are averaged over ﬁxed intervals whereas the events causing data artifacts may occur at any time and often have durations signiﬁcantly shorter than the data collection interval. We show that careful modeling of the sensor, combined with a general technique for detecting sub-interval events and estimating their duration, enables detection of artifacts and accurate estimation of the underlying blood pressure values. Our model’s performance identifying artifacts is superior to two other classiﬁers’ and about as good as a physician’s. 1</p><p>3 0.52750993 <a title="105-lsi-3" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>Author: Cédric Archambeau, Francis R. Bach</p><p>Abstract: We present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a preprocessing tool for the construction of template attacks. 1</p><p>4 0.51779431 <a title="105-lsi-4" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>Author: Iain Murray, David MacKay, Ryan P. Adams</p><p>Abstract: We present the Gaussian Process Density Sampler (GPDS), an exchangeable generative model for use in nonparametric Bayesian density estimation. Samples drawn from the GPDS are consistent with exact, independent samples from a ﬁxed density function that is a transformation of a function drawn from a Gaussian process prior. Our formulation allows us to infer an unknown density from data using Markov chain Monte Carlo, which gives samples from the posterior distribution over density functions and from the predictive distribution on data space. We can also infer the hyperparameters of the Gaussian process. We compare this density modeling technique to several existing techniques on a toy problem and a skullreconstruction task. 1</p><p>5 0.46466038 <a title="105-lsi-5" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>Author: Ydo Wexler, Christopher Meek</p><p>Abstract: We propose a multiplicative approximation scheme (MAS) for inference problems in graphical models, which can be applied to various inference algorithms. The method uses -decompositions which decompose functions used throughout the inference procedure into functions over smaller sets of variables with a known error . MAS translates these local approximations into bounds on the accuracy of the results. We show how to optimize -decompositions and provide a fast closed-form solution for an L2 approximation. Applying MAS to the Variable Elimination inference algorithm, we introduce an algorithm we call DynaDecomp which is extremely fast in practice and provides guaranteed error bounds on the result. The superior accuracy and efﬁciency of DynaDecomp is demonstrated. 1</p><p>6 0.42391008 <a title="105-lsi-6" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>7 0.42234099 <a title="105-lsi-7" href="./nips-2008-Bayesian_Exponential_Family_PCA.html">31 nips-2008-Bayesian Exponential Family PCA</a></p>
<p>8 0.38090679 <a title="105-lsi-8" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>9 0.3693237 <a title="105-lsi-9" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>10 0.36346829 <a title="105-lsi-10" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>11 0.35471886 <a title="105-lsi-11" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>12 0.34422439 <a title="105-lsi-12" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>13 0.33267319 <a title="105-lsi-13" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>14 0.32566768 <a title="105-lsi-14" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>15 0.31647086 <a title="105-lsi-15" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>16 0.30470514 <a title="105-lsi-16" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>17 0.30448854 <a title="105-lsi-17" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>18 0.28848463 <a title="105-lsi-18" href="./nips-2008-Fast_High-dimensional_Kernel_Summations_Using_the_Monte_Carlo_Multipole_Method.html">83 nips-2008-Fast High-dimensional Kernel Summations Using the Monte Carlo Multipole Method</a></p>
<p>19 0.2809163 <a title="105-lsi-19" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>20 0.27817798 <a title="105-lsi-20" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.046), (7, 0.055), (12, 0.029), (28, 0.152), (57, 0.063), (63, 0.028), (64, 0.436), (77, 0.049), (78, 0.011), (83, 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.71955907 <a title="105-lda-1" href="./nips-2008-Improving_on_Expectation_Propagation.html">105 nips-2008-Improving on Expectation Propagation</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: A series of corrections is developed for the ﬁxed points of Expectation Propagation (EP), which is one of the most popular methods for approximate probabilistic inference. These corrections can lead to improvements of the inference approximation or serve as a sanity check, indicating when EP yields unrealiable results.</p><p>2 0.63764983 <a title="105-lda-2" href="./nips-2008-Integrating_Locally_Learned_Causal_Structures_with_Overlapping_Variables.html">108 nips-2008-Integrating Locally Learned Causal Structures with Overlapping Variables</a></p>
<p>Author: David Danks, Clark Glymour, Robert E. Tillman</p><p>Abstract: In many domains, data are distributed among datasets that share only some variables; other recorded variables may occur in only one dataset. While there are asymptotically correct, informative algorithms for discovering causal relationships from a single dataset, even with missing values and hidden variables, there have been no such reliable procedures for distributed data with overlapping variables. We present a novel, asymptotically correct procedure that discovers a minimal equivalence class of causal DAG structures using local independence information from distributed data of this form and evaluate its performance using synthetic and real-world data against causal discovery algorithms for single datasets and applying Structural EM, a heuristic DAG structure learning procedure for data with missing values, to the concatenated data.</p><p>3 0.59961575 <a title="105-lda-3" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>Author: Giovanni Cavallanti, Nicolò Cesa-bianchi, Claudio Gentile</p><p>Abstract: We provide a new analysis of an efﬁcient margin-based algorithm for selective sampling in classiﬁcation problems. Using the so-called Tsybakov low noise condition to parametrize the instance distribution, we show bounds on the convergence rate to the Bayes risk of both the fully supervised and the selective sampling versions of the basic algorithm. Our analysis reveals that, excluding logarithmic factors, the average risk of the selective sampler converges to the Bayes risk at rate N −(1+α)(2+α)/2(3+α) where N denotes the number of √ queried labels, and α > 0 is the exponent in the low noise condition. For all α > 3 − 1 ≈ 0.73 this convergence rate is asymptotically faster than the rate N −(1+α)/(2+α) achieved by the fully supervised version of the same classiﬁer, which queries all labels, and for α → ∞ the two rates exhibit an exponential gap. Experiments on textual data reveal that simple variants of the proposed selective sampler perform much better than popular and similarly efﬁcient competitors. 1</p><p>4 0.58881652 <a title="105-lda-4" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>Author: Florian Steinke, Matthias Hein</p><p>Abstract: This paper discusses non-parametric regression between Riemannian manifolds. This learning problem arises frequently in many application areas ranging from signal processing, computer vision, over robotics to computer graphics. We present a new algorithmic scheme for the solution of this general learning problem based on regularized empirical risk minimization. The regularization functional takes into account the geometry of input and output manifold, and we show that it implements a prior which is particularly natural. Moreover, we demonstrate that our algorithm performs well in a difﬁcult surface registration problem. 1</p><p>5 0.40496713 <a title="105-lda-5" href="./nips-2008-Finding_Latent_Causes_in_Causal_Networks%3A_an_Efficient_Approach_Based_on_Markov_Blankets.html">86 nips-2008-Finding Latent Causes in Causal Networks: an Efficient Approach Based on Markov Blankets</a></p>
<p>Author: Jean-philippe Pellet, AndrÄ&sbquo;Ĺ  Elisseeff</p><p>Abstract: Causal structure-discovery techniques usually assume that all causes of more than one variable are observed. This is the so-called causal sufficiency assumption. In practice, it is untestable, and often violated. In this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data. Similar to algorithms such as IC* and FCI, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables. Assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditionalindependence tests w.r.t. the number of variables. We show with experiments that our algorithm is comparable to the state-of-the-art FCI algorithm in accuracy, while being several orders of magnitude faster on large problems. We conclude that MBCS* makes a new range of causally insufficient problems computationally tractable. Keywords: Graphical Models, Structure Learning, Causal Inference. 1 Introduction: Task Definition & Related Work The statistical definition of causality pioneered by Pearl (2000) and Spirtes et al. (2001) has shed new light on how to detect causation. Central in this approach is the automated detection of causeeffect relationships using observational (i.e., non-experimental) data. This can be a necessary task, as in many situations, performing randomized controlled experiments to unveil causation can be impossible, unethical , or too costly. When the analysis deals with variables that cannot be manipulated, being able to learn from data collected by observing the running system is the only possibility. It turns out that learning the full causal structure of a set of variables is, in its most general form , impossible. If we suppose that the</p><p>6 0.39241999 <a title="105-lda-6" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>7 0.39187101 <a title="105-lda-7" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>8 0.39048997 <a title="105-lda-8" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>9 0.38902313 <a title="105-lda-9" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>10 0.38902038 <a title="105-lda-10" href="./nips-2008-Bayesian_Exponential_Family_PCA.html">31 nips-2008-Bayesian Exponential Family PCA</a></p>
<p>11 0.38891923 <a title="105-lda-11" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>12 0.38790137 <a title="105-lda-12" href="./nips-2008-Predictive_Indexing_for_Fast_Search.html">184 nips-2008-Predictive Indexing for Fast Search</a></p>
<p>13 0.38773793 <a title="105-lda-13" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>14 0.3874073 <a title="105-lda-14" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>15 0.38706627 <a title="105-lda-15" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>16 0.3869822 <a title="105-lda-16" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>17 0.38663438 <a title="105-lda-17" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>18 0.38592368 <a title="105-lda-18" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>19 0.38586733 <a title="105-lda-19" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>20 0.38455546 <a title="105-lda-20" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
