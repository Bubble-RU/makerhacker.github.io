<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-186" href="#">nips2008-186</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</h1>
<br/><p>Source: <a title="nips-2008-186-pdf" href="http://papers.nips.cc/paper/3434-probabilistic-detection-of-short-events-with-application-to-critical-care-monitoring.pdf">pdf</a></p><p>Author: Norm Aleks, Stuart Russell, Michael G. Madden, Diane Morabito, Kristan Staudenmayer, Mitchell Cohen, Geoffrey T. Manley</p><p>Abstract: We describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit (ICU). In particular, we consider the arterial-line blood pressure sensor, which is subject to frequent data artifacts that cause false alarms in the ICU and make the raw data almost useless for automated decision making. The problem is complicated by the fact that the sensor data are averaged over ﬁxed intervals whereas the events causing data artifacts may occur at any time and often have durations signiﬁcantly shorter than the data collection interval. We show that careful modeling of the sensor, combined with a general technique for detecting sub-interval events and estimating their duration, enables detection of artifacts and accurate estimation of the underlying blood pressure values. Our model’s performance identifying artifacts is superior to two other classiﬁers’ and about as good as a physician’s. 1</p><p>Reference: <a title="nips-2008-186-reference" href="../nips2008_reference/nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Probabilistic detection of short events, with application to critical care monitoring  Norm Aleks U. [sent-1, score-0.164]
</p><p>2 edu  Abstract We describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit (ICU). [sent-26, score-0.244]
</p><p>3 In particular, we consider the arterial-line blood pressure sensor, which is subject to frequent data artifacts that cause false alarms in the ICU and make the raw data almost useless for automated decision making. [sent-27, score-1.44]
</p><p>4 The problem is complicated by the fact that the sensor data are averaged over ﬁxed intervals whereas the events causing data artifacts may occur at any time and often have durations signiﬁcantly shorter than the data collection interval. [sent-28, score-0.393]
</p><p>5 We show that careful modeling of the sensor, combined with a general technique for detecting sub-interval events and estimating their duration, enables detection of artifacts and accurate estimation of the underlying blood pressure values. [sent-29, score-1.422]
</p><p>6 Our model’s performance identifying artifacts is superior to two other classiﬁers’ and about as good as a physician’s. [sent-30, score-0.159]
</p><p>7 1  Introduction  The work we report here falls under the general heading of state estimation, i. [sent-31, score-0.053]
</p><p>8 , computing the posterior distribution P(Xt |e1:t ) for the state variables X of a partially observable stochastic system, given a sequence of observations e1:t . [sent-33, score-0.06]
</p><p>9 The speciﬁc setting for our work at the Center for Biomedical Informatics in Critical Care (C-BICC) is an intensive care unit (ICU) at San Francisco General Hospital (SFGH) specializing in traumatic brain injury, part of a major regional trauma center. [sent-34, score-0.108]
</p><p>10 A section of data from these sensors is shown in Figure 1(a). [sent-36, score-0.039]
</p><p>11 The artifacts cannot be modeled simply as “noise” in the sensor model; many are extended over time (some for as long as 45 minutes) and most exhibit complex patterns of their own. [sent-38, score-0.265]
</p><p>12 Instead, we follow the general approach suggested by Russell and Norvig (2003), which involves careful generative modeling of sensor state using dynamic Bayesian networks (DBNs). [sent-40, score-0.165]
</p><p>13 This paper focuses on the arterial-line blood pressure sensor (Figure 1(b)), a key element of the monitoring system. [sent-41, score-1.285]
</p><p>14 artiﬁcially low or high values due to zeroing, line ﬂushes, or the drawing of blood samples. [sent-44, score-0.5]
</p><p>15 These artifacts not only complicate the state estimation and diagnosis task; they also corrupt recorded data and cause a large number of false alarms in the ICU, which lead in turn to true alarms being ignored and alarms being turned off (Tsien & Fackler, 1997). [sent-45, score-0.523]
</p><p>16 By modeling the artifact-generating processes, we hope to be able to infer the true underlying blood pressure even when artifacts occur. [sent-46, score-1.299]
</p><p>17 Thus, the natural time step for modeling the sensor state transitions might be one second, whereas the measurement interval is much larger. [sent-49, score-0.286]
</p><p>18 This is an instance of a very important issue studied in the dynamical systems and chemical kinetics literatures under the heading of separation of time scales (see, e. [sent-51, score-0.076]
</p><p>19 Section 4 describes the complete model for blood pressure estimation, including artifact models, and Section 5 then evaluates the model on real patient data. [sent-56, score-1.326]
</p><p>20 Our results show very high precision and recall rates for event detection; we are able to eliminate over 90% of false alarms for blood pressure while missing fewer than 1% of the true alarms. [sent-59, score-1.383]
</p><p>21 Our work is not the ﬁrst to consider the probabilistic analysis of intensive care data. [sent-60, score-0.108]
</p><p>22 Indeed, one of the best known early Bayes net applications was the A LARM model for patient monitoring under ventilation (Beinlich et al. [sent-61, score-0.124]
</p><p>23 The work most closely related to ours is that of Williams, Quinn, and McIntosh (2005), who apply factorial switching Kalman ﬁlters—a particular class of DBNs—to artifact detection in neonatal ICU data. [sent-63, score-0.235]
</p><p>24 Their (one-second) model is roughly analogous to the models described by Russell and Norvig, using Boolean state variables to represent events that block normal sensor readings. [sent-64, score-0.245]
</p><p>25 One the left, a blood draw and line ﬂush in quick succession. [sent-66, score-0.526]
</p><p>26 2  Blood Pressure Monitoring  Blood pressure provides informs much of medical thinking and is typically measured continuously in the ICU. [sent-71, score-0.61]
</p><p>27 The most common ICU blood pressure measurement device is the arterial line, illustrated in Figure 1(b); a catheter placed into one of the patient’s small arteries is connected to a pressure transducer whose output is displayed on a bedside monitor. [sent-72, score-1.948]
</p><p>28 Because blood ﬂow varies during the cardiac cycle, blood pressure is pulsatile. [sent-73, score-1.634]
</p><p>29 In medical records, including our data set, blood pressure measurements are summarized in two or three values: systolic blood pressure, which is the maximum reached during the cardiac cycle, diastolic, which is the corresponding minimum, and sometimes the mean. [sent-74, score-1.699]
</p><p>30 We refer to blood draws and line ﬂushes collectively as “bag events. [sent-76, score-0.523]
</p><p>31 ” Figure 2(top) shows the artifacts using data collected at one-second intervals. [sent-77, score-0.159]
</p><p>32 Using systolic pressure s as an example, for an artifact of length p (as a fraction of the averaging interval) and mean artifact pressure x, the apparent pressure s = px + (1 p)s. [sent-80, score-2.188]
</p><p>33 Our DBN model in Section 4 includes summary variables and equations relating the one-minute readings to the true underlying pressures, artifacts’ durations, bag and atmospheric pressure, etc. [sent-81, score-0.13]
</p><p>34 ; it can therefore estimate the duration and other characteristics of artifacts that have corrupted the data. [sent-82, score-0.159]
</p><p>35 Patterns produced by artifacts in the one-minute data are highly varied, but it turns out (see Section 5) that the detailed modeling pays off in revealing the characteristic relationships that follow from the nature of the corrupting events. [sent-83, score-0.189]
</p><p>36 3  Modeling Sub-Interval Events  The data we work with are generated by a combination of physiological processes that vary over timescales of several minutes and artifactual events lasting perhaps only a few seconds. [sent-84, score-0.154]
</p><p>37 A natural 3  Figure 3: (left) DBN model showing relationships among the fast event variables fi , interval count variables GN j , and measurement variables EN j . [sent-85, score-0.561]
</p><p>38 , 1 second: on this timescale, the sensor state variables indicate whether or not an artifactual event is currently in progress. [sent-89, score-0.341]
</p><p>39 The transition model for these variables indicates the probability at each second that a new event begins and the probability that an event already in progress continues. [sent-90, score-0.283]
</p><p>40 Assuming for now that there is only one event type, and given memoryless (geometric) distribution of durations such as we see in Section 5, only two parameters are necessary: p = P( fi = 1 fi 1 = 1) and q = P( fi = 1 fi 1 = 0). [sent-91, score-0.723]
</p><p>41 Both can be estimated simply by measuring event frequencies and durations. [sent-92, score-0.126]
</p><p>42 The main drawback of using a fast time step is computational: inference must be carried out over 60 time steps for every one measurement that arrives. [sent-93, score-0.086]
</p><p>43 First, to explain the evidence, we’ll need a count variable saying how many seconds of the minute were occupied by events. [sent-97, score-0.292]
</p><p>44 (If there are multiple mutually exclusive event types, then each count variable depends on all the preceding variables. [sent-99, score-0.225]
</p><p>45 ) Each count variable can take on 61 values, which leads to huge conditional distributions summarizing how the preceding 60 seconds could be divided among the various event types. [sent-100, score-0.253]
</p><p>46 However, as we will now see, CPTs for the slow model need not be estimated or guessed—they can be derived from the fast model. [sent-102, score-0.05]
</p><p>47 Let the fast time step be and a measurement interval Nj 1 be N (where N = 60 in our domain). [sent-105, score-0.146]
</p><p>48 fi = 1 iff an event is occurring at time i ; GN j f i = N( j 1) i counts the number of fast time steps within the jth measurement interval during which an event is occurring. [sent-106, score-0.535]
</p><p>49 The jth observed measurement EN j is determined entirely by GN j ; therefore, it sufﬁces GNt . [sent-107, score-0.061]
</p><p>50 to consider the joint distribution over G0 GN To obtain a model containing only variables at the slow intervals, we simply need to sum out the fi variables other than the ones at interval boundaries. [sent-108, score-0.284]
</p><p>51 We can do this topologically by a series of arc reversal and node removal operations (Shachter, 1986); a simple proof by induction (omitted) shows that, regardless of the number of fast steps per slow step, we obtain the reduced structure GNt . [sent-109, score-0.05]
</p><p>52 That is, how many “ones” do we expect in an interval, given the event status at the beginning of the interval, and what is the probability that an event is occurring at the beginning of the next interval, given also the number of ones in the current interval? [sent-113, score-0.298]
</p><p>53 4  a table is constructed for the variables fi and Ci for i from 1 up to N, where Ci is the number of ones up to i − 1 and C0 = 0. [sent-115, score-0.168]
</p><p>54 Now we have the following result: Theorem 1 Given the conditional distributions computed by Equations 1 and 2, the reduced model in Figure 3(b) yields the same distribution for the count sequence G0 , GN , . [sent-118, score-0.059]
</p><p>55 In particular, when events are short compared to measurement intervals and occur frequently, we expect the dependence on fN( j−1) to disappear and the distribution for GN j to be approximately N Gaussian with mean 1+p/(1−q) . [sent-123, score-0.14]
</p><p>56 Generalizing the analysis to the case of multiple disjoint event types (i. [sent-128, score-0.126]
</p><p>57 , fi takes on more than two values) is mathematically straightforward and the details are omitted. [sent-130, score-0.137]
</p><p>58 There is, however, a complexity problem as the number of event types increases. [sent-131, score-0.126]
</p><p>59 The count variables GN j , HN j , and so on at time N j are all dependent on each other given fN( j−1) , and fN j depends on all of them; thus, using the approach given above, the precomputed tables will scale exponentially with the number of event types. [sent-132, score-0.216]
</p><p>60 The preceding analysis covers only the case in which fi depends just on fi−1 , leading to independently occurring events with a geometric length distribution. [sent-135, score-0.256]
</p><p>61 Handling non-independent event occurrence is often more important; for example, blood draws may occur in clusters if multiple samples are required. [sent-137, score-0.676]
</p><p>62 Before we move on to describe the complete model, it is important to note that a model with a ﬁner time scale that the measurement frequency can provide useful extra information. [sent-139, score-0.061]
</p><p>63 By analogy with sub-pixel localization in computer vision, such a model can estimate the time of occurrence of an event within a measurement interval. [sent-140, score-0.214]
</p><p>64 4  Combined model  The complete model for blood pressure measurements is shown in Figure 4. [sent-141, score-1.139]
</p><p>65 The evidence variables ENj are just the three reported blood pressure values ObservedDiaBP, ObservedSysBP, and ObservedMeanBP. [sent-143, score-1.165]
</p><p>66 These reﬂect, with some Gaussian noise, idealized Apparent values, determined in turn by • the true time-averaged pressures: TrueDiaBP, TrueSysBP, and TrueMeanBP; • the total duration of artifacts within the preceding minute (i. [sent-144, score-0.347]
</p><p>67 , the GN j variables): BagTime and ZeroTime; • the average induced pressure to which the transducer is exposed during each event type: BagPressure and ZeroPressure (these have their own slowly varying dynamics). [sent-146, score-0.825]
</p><p>68 5  Figure 4: The blood pressure artifact detection DBN. [sent-147, score-1.315]
</p><p>69 The Apparent  variables are deterministic functions of their parents. [sent-150, score-0.054]
</p><p>70 2 The key event variable in the model, corresponding to fN j in Figure 3(b), is EndingValveState. [sent-153, score-0.126]
</p><p>71 5  Experimental Results  To estimate the CPT parameters (P( ft+1 = 1 ft = 0) and P( ft+1 = 1 ft = 1)) for the one-second model, and to evaluate the one-minute model’s performance, we ﬁrst needed ground truth for event occurrence and length. [sent-157, score-0.227]
</p><p>72 By special arrangement we were able to obtain 300 hours of 1Hz data, in which the artifacts we describe here are obvious to the human eye; one of us (a physician) then tagged each of those data points for artifact presence and type, giving the ground truth. [sent-158, score-0.357]
</p><p>73 (There were a total of 228 events of various lengths in the 300 hours’ data. [sent-159, score-0.079]
</p><p>74 ) With half the annotated data we veriﬁed that event durations were indeed approximately geometrically distributed, and estimated the one-second CPT parameters; from those, as described in Section 3, we calculated corresponding one-minute-interval CPTs. [sent-160, score-0.175]
</p><p>75 Using averaging equivalent to that used by the regular system, we transformed the other half of the high-resolution data into 1-minute average blood pressures with associated artifact-time ground truth. [sent-161, score-0.591]
</p><p>76 , 1993) with 8000 particles to derive posteriors for true blood pressure and the presence and length of each type of artifact at each minute. [sent-163, score-1.271]
</p><p>77 6  Figure 5: ROC curves for the DBN’s performance detecting bag events (left) and zeroing events (right), as compared with an SVM, a deterministic model-based detector, and a physician. [sent-165, score-0.311]
</p><p>78 Figure 6: Two days’ blood pressure data for one patient, with the hypertension threshold overlaid. [sent-166, score-1.11]
</p><p>79 Raw data are on the left; on the right are ﬁltering results showing elimination (here) of false declarations of hypertension. [sent-167, score-0.067]
</p><p>80 given the assumption that the true blood pressure is that recorded at the most recent minute during which no artifact was detected; it predicts artifact presence if the sum of the estimates’ squared distances from their mean is below some threshold. [sent-168, score-1.608]
</p><p>81 (Because this model’s prediction for any particular minute depends on its prediction at the previous minute, its sensitivity and speciﬁcity do not vary monotonically with changes in the threshold; the ROC curve shown is of only the undominated points. [sent-169, score-0.148]
</p><p>82 Figure 5(left) shows results for the detection of bag events. [sent-171, score-0.113]
</p><p>83 The DBN achieves a true positive rate of 80% with almost no false positives, or a TPR of 90% with 10% false positives. [sent-172, score-0.134]
</p><p>84 It does less well with zeroing events, as shown in Figure 5(b), achieving a TPR of nearly 70% with minimal false positives, but beyond that having unacceptable false positive levels. [sent-173, score-0.195]
</p><p>85 The physician had an even lower false positive rate for each artifact type, but with a true positive rate of only about 50%; the SVM and deterministic model-based detector both had better-than-chance performance but were clinically useless due to high false positive rates. [sent-174, score-0.431]
</p><p>86 The model’s accuracy in tracking true blood pressure is harder to evaluate because we have no minute-by-minute gold standard. [sent-175, score-1.11]
</p><p>87 (Arterial blood pressure measurements as we’ve described them, despite their artifacts, are the gold standard in the ICU. [sent-176, score-1.139]
</p><p>88 Placing a second arterial line, besides being subject to the same artifacts, also exposes patients to unnecessary infection risk. [sent-177, score-0.076]
</p><p>89 ) However, on a more qualitative level, four physicians in our group have examined many hours of measured and inferred blood pressure traces, a typical example of which is shown in Figure 7, and have nearly always agreed with the inference results. [sent-178, score-1.147]
</p><p>90 Where the system’s inferences are questionable, examining other sensors often helps to reveal whether a pressure change was real or artifactual. [sent-179, score-0.649]
</p><p>91 7  Figure 7: Sensed blood pressure (dark lines) and inferred true blood pressure (lighter bands, representing mean 1SD) across an observed blood draw with following zeroing. [sent-180, score-2.746]
</p><p>92 The lowest two lines show the inferred fraction of each minute occupied by bag or zero artifact. [sent-181, score-0.274]
</p><p>93 6  Conclusions and Further Work  We have applied dynamic Bayesian network modeling to the problem of handling aggregated data with sub-interval artifacts. [sent-182, score-0.055]
</p><p>94 In preliminary experiments, this model of a typical blood pressure sensor appears quite successful at tracking true blood pressure and identifying and classifying artifacts. [sent-183, score-2.326]
</p><p>95 Our approach has reduced the need for learning (as distinct from modeling and inference) to the small but crucial role of determining the distribution of event durations. [sent-184, score-0.156]
</p><p>96 Modiﬁed to run at 1Hz, this model could run on-line at the bedside, helping to reduce false alarms. [sent-186, score-0.067]
</p><p>97 We are currently extending the model to include more sensors and physiological state variables and anticipate further improvements in detection accuracy as a result of combining multiple sensors. [sent-187, score-0.169]
</p><p>98 Stochastic chemical kinetics and the quasi-steady-state assumption: Application to the Gillespie algorithm. [sent-225, score-0.052]
</p><p>99 Poor prognosis for existing monitors in the intensive care unit. [sent-249, score-0.108]
</p><p>100 Factorial switching Kalman ﬁlters for condition monitoring in neonatal intensive care. [sent-257, score-0.156]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('pressure', 0.61), ('blood', 0.5), ('gn', 0.167), ('artifact', 0.161), ('artifacts', 0.159), ('minute', 0.148), ('fi', 0.137), ('event', 0.126), ('icu', 0.121), ('sensor', 0.106), ('pressures', 0.091), ('fn', 0.082), ('alarms', 0.08), ('events', 0.079), ('ci', 0.078), ('arterial', 0.076), ('monitoring', 0.069), ('bag', 0.069), ('false', 0.067), ('measurement', 0.061), ('bagtime', 0.061), ('gnt', 0.061), ('physician', 0.061), ('transducer', 0.061), ('zeroing', 0.061), ('zerotime', 0.061), ('interval', 0.06), ('count', 0.059), ('occupied', 0.057), ('intensive', 0.057), ('patient', 0.055), ('dbn', 0.054), ('care', 0.051), ('durations', 0.049), ('artifactual', 0.049), ('cpt', 0.045), ('norvig', 0.045), ('ushes', 0.045), ('detection', 0.044), ('russell', 0.043), ('preceding', 0.04), ('sensors', 0.039), ('ft', 0.037), ('hours', 0.037), ('systolic', 0.036), ('traces', 0.035), ('dbns', 0.034), ('francisco', 0.033), ('cooper', 0.032), ('variables', 0.031), ('aliferis', 0.03), ('arkin', 0.03), ('atmospheric', 0.03), ('bagpressure', 0.03), ('bedside', 0.03), ('cpts', 0.03), ('diastolic', 0.03), ('fackler', 0.03), ('heartbeat', 0.03), ('heldt', 0.03), ('mcintosh', 0.03), ('neonatal', 0.03), ('oxygen', 0.03), ('quinn', 0.03), ('recurrences', 0.03), ('sieben', 0.03), ('stopcock', 0.03), ('tpr', 0.03), ('truediabp', 0.03), ('tsien', 0.03), ('ush', 0.03), ('venous', 0.03), ('zeropressure', 0.03), ('modeling', 0.03), ('state', 0.029), ('measurements', 0.029), ('exposed', 0.028), ('chemical', 0.028), ('detector', 0.028), ('recorded', 0.028), ('seconds', 0.028), ('beinlich', 0.027), ('sensed', 0.027), ('gather', 0.027), ('occurrence', 0.027), ('draw', 0.026), ('physiological', 0.026), ('fast', 0.025), ('slow', 0.025), ('handling', 0.025), ('cardiac', 0.024), ('heading', 0.024), ('kinetics', 0.024), ('shachter', 0.024), ('useless', 0.024), ('san', 0.024), ('evidence', 0.024), ('draws', 0.023), ('beginning', 0.023), ('deterministic', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="186-tfidf-1" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<p>Author: Norm Aleks, Stuart Russell, Michael G. Madden, Diane Morabito, Kristan Staudenmayer, Mitchell Cohen, Geoffrey T. Manley</p><p>Abstract: We describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit (ICU). In particular, we consider the arterial-line blood pressure sensor, which is subject to frequent data artifacts that cause false alarms in the ICU and make the raw data almost useless for automated decision making. The problem is complicated by the fact that the sensor data are averaged over ﬁxed intervals whereas the events causing data artifacts may occur at any time and often have durations signiﬁcantly shorter than the data collection interval. We show that careful modeling of the sensor, combined with a general technique for detecting sub-interval events and estimating their duration, enables detection of artifacts and accurate estimation of the underlying blood pressure values. Our model’s performance identifying artifacts is superior to two other classiﬁers’ and about as good as a physician’s. 1</p><p>2 0.070874311 <a title="186-tfidf-2" href="./nips-2008-Improving_on_Expectation_Propagation.html">105 nips-2008-Improving on Expectation Propagation</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: A series of corrections is developed for the ﬁxed points of Expectation Propagation (EP), which is one of the most popular methods for approximate probabilistic inference. These corrections can lead to improvements of the inference approximation or serve as a sanity check, indicating when EP yields unrealiable results.</p><p>3 0.05012602 <a title="186-tfidf-3" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>Author: Daphna Weinshall, Hynek Hermansky, Alon Zweig, Jie Luo, Holly Jimison, Frank Ohl, Misha Pavel</p><p>Abstract: Unexpected stimuli are a challenge to any machine learning algorithm. Here we identify distinct types of unexpected events, focusing on ’incongruent events’ when ’general level’ and ’speciﬁc level’ classiﬁers give conﬂicting predictions. We deﬁne a formal framework for the representation and processing of incongruent events: starting from the notion of label hierarchy, we show how partial order on labels can be deduced from such hierarchies. For each event, we compute its probability in different ways, based on adjacent levels (according to the partial order) in the label hierarchy. An incongruent event is an event where the probability computed based on some more speciﬁc level (in accordance with the partial order) is much smaller than the probability computed based on some more general level, leading to conﬂicting predictions. We derive algorithms to detect incongruent events from different types of hierarchies, corresponding to class membership or part membership. Respectively, we show promising results with real data on two speciﬁc problems: Out Of Vocabulary words in speech recognition, and the identiﬁcation of a new sub-class (e.g., the face of a new individual) in audio-visual facial object recognition.</p><p>4 0.044133388 <a title="186-tfidf-4" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>Author: Iain Murray, David MacKay, Ryan P. Adams</p><p>Abstract: We present the Gaussian Process Density Sampler (GPDS), an exchangeable generative model for use in nonparametric Bayesian density estimation. Samples drawn from the GPDS are consistent with exact, independent samples from a ﬁxed density function that is a transformation of a function drawn from a Gaussian process prior. Our formulation allows us to infer an unknown density from data using Markov chain Monte Carlo, which gives samples from the posterior distribution over density functions and from the predictive distribution on data space. We can also infer the hyperparameters of the Gaussian process. We compare this density modeling technique to several existing techniques on a toy problem and a skullreconstruction task. 1</p><p>5 0.040398844 <a title="186-tfidf-5" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>Author: Jun Zhu, Eric P. Xing, Bo Zhang</p><p>Abstract: Learning graphical models with hidden variables can offer semantic insights to complex data and lead to salient structured predictors without relying on expensive, sometime unattainable fully annotated training data. While likelihood-based methods have been extensively explored, to our knowledge, learning structured prediction models with latent variables based on the max-margin principle remains largely an open problem. In this paper, we present a partially observed Maximum Entropy Discrimination Markov Network (PoMEN) model that attempts to combine the advantages of Bayesian and margin based paradigms for learning Markov networks from partially labeled data. PoMEN leads to an averaging prediction rule that resembles a Bayes predictor that is more robust to overﬁtting, but is also built on the desirable discriminative laws resemble those of the M3 N. We develop an EM-style algorithm utilizing existing convex optimization algorithms for M3 N as a subroutine. We demonstrate competent performance of PoMEN over existing methods on a real-world web data extraction task. 1</p><p>6 0.037825108 <a title="186-tfidf-6" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>7 0.035525039 <a title="186-tfidf-7" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>8 0.035364967 <a title="186-tfidf-8" href="./nips-2008-Nonlinear_causal_discovery_with_additive_noise_models.html">153 nips-2008-Nonlinear causal discovery with additive noise models</a></p>
<p>9 0.033597365 <a title="186-tfidf-9" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>10 0.032939907 <a title="186-tfidf-10" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>11 0.032399878 <a title="186-tfidf-11" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>12 0.031571828 <a title="186-tfidf-12" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>13 0.0315441 <a title="186-tfidf-13" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>14 0.031259645 <a title="186-tfidf-14" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>15 0.031161018 <a title="186-tfidf-15" href="./nips-2008-Hierarchical_Fisher_Kernels_for_Longitudinal_Data.html">97 nips-2008-Hierarchical Fisher Kernels for Longitudinal Data</a></p>
<p>16 0.030977542 <a title="186-tfidf-16" href="./nips-2008-Cyclizing_Clusters_via_Zeta_Function_of_a_Graph.html">55 nips-2008-Cyclizing Clusters via Zeta Function of a Graph</a></p>
<p>17 0.030953072 <a title="186-tfidf-17" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>18 0.02924311 <a title="186-tfidf-18" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>19 0.028139781 <a title="186-tfidf-19" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>20 0.028006904 <a title="186-tfidf-20" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.094), (1, 0.014), (2, 0.039), (3, 0.018), (4, -0.004), (5, -0.017), (6, 0.003), (7, 0.03), (8, 0.032), (9, 0.004), (10, -0.001), (11, 0.025), (12, 0.043), (13, -0.055), (14, -0.01), (15, 0.026), (16, -0.024), (17, -0.022), (18, 0.019), (19, 0.002), (20, 0.056), (21, 0.054), (22, 0.023), (23, -0.013), (24, 0.006), (25, -0.019), (26, -0.021), (27, -0.036), (28, -0.027), (29, -0.028), (30, -0.024), (31, -0.025), (32, 0.032), (33, -0.042), (34, 0.053), (35, 0.034), (36, 0.01), (37, 0.067), (38, 0.079), (39, 0.075), (40, -0.062), (41, -0.025), (42, 0.038), (43, 0.01), (44, 0.08), (45, 0.094), (46, 0.095), (47, 0.114), (48, -0.086), (49, 0.098)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90614319 <a title="186-lsi-1" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<p>Author: Norm Aleks, Stuart Russell, Michael G. Madden, Diane Morabito, Kristan Staudenmayer, Mitchell Cohen, Geoffrey T. Manley</p><p>Abstract: We describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit (ICU). In particular, we consider the arterial-line blood pressure sensor, which is subject to frequent data artifacts that cause false alarms in the ICU and make the raw data almost useless for automated decision making. The problem is complicated by the fact that the sensor data are averaged over ﬁxed intervals whereas the events causing data artifacts may occur at any time and often have durations signiﬁcantly shorter than the data collection interval. We show that careful modeling of the sensor, combined with a general technique for detecting sub-interval events and estimating their duration, enables detection of artifacts and accurate estimation of the underlying blood pressure values. Our model’s performance identifying artifacts is superior to two other classiﬁers’ and about as good as a physician’s. 1</p><p>2 0.67153448 <a title="186-lsi-2" href="./nips-2008-Improving_on_Expectation_Propagation.html">105 nips-2008-Improving on Expectation Propagation</a></p>
<p>Author: Manfred Opper, Ulrich Paquet, Ole Winther</p><p>Abstract: A series of corrections is developed for the ﬁxed points of Expectation Propagation (EP), which is one of the most popular methods for approximate probabilistic inference. These corrections can lead to improvements of the inference approximation or serve as a sanity check, indicating when EP yields unrealiable results.</p><p>3 0.55090296 <a title="186-lsi-3" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>Author: Liang Zhang, Deepak Agarwal</p><p>Abstract: Multi-level hierarchical models provide an attractive framework for incorporating correlations induced in a response variable that is organized hierarchically. Model ﬁtting is challenging, especially for a hierarchy with a large number of nodes. We provide a novel algorithm based on a multi-scale Kalman ﬁlter that is both scalable and easy to implement. For Gaussian response, we show our method provides the maximum a-posteriori (MAP) parameter estimates; for non-Gaussian response, parameter estimation is performed through a Laplace approximation. However, the Laplace approximation provides biased parameter estimates that is corrected through a parametric bootstrap procedure. We illustrate through simulation studies and analyses of real world data sets in health care and online advertising.</p><p>4 0.49667186 <a title="186-lsi-4" href="./nips-2008-The_Gaussian_Process_Density_Sampler.html">233 nips-2008-The Gaussian Process Density Sampler</a></p>
<p>Author: Iain Murray, David MacKay, Ryan P. Adams</p><p>Abstract: We present the Gaussian Process Density Sampler (GPDS), an exchangeable generative model for use in nonparametric Bayesian density estimation. Samples drawn from the GPDS are consistent with exact, independent samples from a ﬁxed density function that is a transformation of a function drawn from a Gaussian process prior. Our formulation allows us to infer an unknown density from data using Markov chain Monte Carlo, which gives samples from the posterior distribution over density functions and from the predictive distribution on data space. We can also infer the hyperparameters of the Gaussian process. We compare this density modeling technique to several existing techniques on a toy problem and a skullreconstruction task. 1</p><p>5 0.47850665 <a title="186-lsi-5" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>Author: Jun Zhu, Eric P. Xing, Bo Zhang</p><p>Abstract: Learning graphical models with hidden variables can offer semantic insights to complex data and lead to salient structured predictors without relying on expensive, sometime unattainable fully annotated training data. While likelihood-based methods have been extensively explored, to our knowledge, learning structured prediction models with latent variables based on the max-margin principle remains largely an open problem. In this paper, we present a partially observed Maximum Entropy Discrimination Markov Network (PoMEN) model that attempts to combine the advantages of Bayesian and margin based paradigms for learning Markov networks from partially labeled data. PoMEN leads to an averaging prediction rule that resembles a Bayes predictor that is more robust to overﬁtting, but is also built on the desirable discriminative laws resemble those of the M3 N. We develop an EM-style algorithm utilizing existing convex optimization algorithms for M3 N as a subroutine. We demonstrate competent performance of PoMEN over existing methods on a real-world web data extraction task. 1</p><p>6 0.45642236 <a title="186-lsi-6" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>7 0.43353865 <a title="186-lsi-7" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>8 0.39966115 <a title="186-lsi-8" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>9 0.38890895 <a title="186-lsi-9" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>10 0.38042483 <a title="186-lsi-10" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>11 0.37897235 <a title="186-lsi-11" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>12 0.3744835 <a title="186-lsi-12" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>13 0.37103757 <a title="186-lsi-13" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>14 0.36365372 <a title="186-lsi-14" href="./nips-2008-A_spatially_varying_two-sample_recombinant_coalescent%2C_with_applications_to_HIV_escape_response.html">11 nips-2008-A spatially varying two-sample recombinant coalescent, with applications to HIV escape response</a></p>
<p>15 0.35053474 <a title="186-lsi-15" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>16 0.34602275 <a title="186-lsi-16" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>17 0.34059638 <a title="186-lsi-17" href="./nips-2008-Cyclizing_Clusters_via_Zeta_Function_of_a_Graph.html">55 nips-2008-Cyclizing Clusters via Zeta Function of a Graph</a></p>
<p>18 0.32797045 <a title="186-lsi-18" href="./nips-2008-Kernel_Change-point_Analysis.html">111 nips-2008-Kernel Change-point Analysis</a></p>
<p>19 0.3226459 <a title="186-lsi-19" href="./nips-2008-Breaking_Audio_CAPTCHAs.html">41 nips-2008-Breaking Audio CAPTCHAs</a></p>
<p>20 0.32255065 <a title="186-lsi-20" href="./nips-2008-Measures_of_Clustering_Quality%3A_A_Working_Set_of_Axioms_for_Clustering.html">132 nips-2008-Measures of Clustering Quality: A Working Set of Axioms for Clustering</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.055), (7, 0.063), (12, 0.029), (15, 0.029), (18, 0.339), (28, 0.154), (57, 0.072), (59, 0.026), (63, 0.017), (71, 0.029), (77, 0.04), (78, 0.022), (83, 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.91358918 <a title="186-lda-1" href="./nips-2008-Bayesian_Model_of_Behaviour_in_Economic_Games.html">33 nips-2008-Bayesian Model of Behaviour in Economic Games</a></p>
<p>Author: Debajyoti Ray, Brooks King-casas, P. R. Montague, Peter Dayan</p><p>Abstract: Classical game theoretic approaches that make strong rationality assumptions have difﬁculty modeling human behaviour in economic games. We investigate the role of ﬁnite levels of iterated reasoning and non-selﬁsh utility functions in a Partially Observable Markov Decision Process model that incorporates game theoretic notions of interactivity. Our generative model captures a broad class of characteristic behaviours in a multi-round Investor-Trustee game. We invert the generative process for a recognition model that is used to classify 200 subjects playing this game against randomly matched opponents. 1</p><p>same-paper 2 0.76257908 <a title="186-lda-2" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<p>Author: Norm Aleks, Stuart Russell, Michael G. Madden, Diane Morabito, Kristan Staudenmayer, Mitchell Cohen, Geoffrey T. Manley</p><p>Abstract: We describe an application of probabilistic modeling and inference technology to the problem of analyzing sensor data in the setting of an intensive care unit (ICU). In particular, we consider the arterial-line blood pressure sensor, which is subject to frequent data artifacts that cause false alarms in the ICU and make the raw data almost useless for automated decision making. The problem is complicated by the fact that the sensor data are averaged over ﬁxed intervals whereas the events causing data artifacts may occur at any time and often have durations signiﬁcantly shorter than the data collection interval. We show that careful modeling of the sensor, combined with a general technique for detecting sub-interval events and estimating their duration, enables detection of artifacts and accurate estimation of the underlying blood pressure values. Our model’s performance identifying artifacts is superior to two other classiﬁers’ and about as good as a physician’s. 1</p><p>3 0.72736239 <a title="186-lda-3" href="./nips-2008-Hierarchical_Fisher_Kernels_for_Longitudinal_Data.html">97 nips-2008-Hierarchical Fisher Kernels for Longitudinal Data</a></p>
<p>Author: Zhengdong Lu, Jeffrey Kaye, Todd K. Leen</p><p>Abstract: We develop new techniques for time series classiﬁcation based on hierarchical Bayesian generative models (called mixed-effect models) and the Fisher kernel derived from them. A key advantage of the new formulation is that one can compute the Fisher information matrix despite varying sequence lengths and varying sampling intervals. This avoids the commonly-used ad hoc replacement of the Fisher information matrix with the identity which destroys the geometric invariance of the kernel. Our construction retains the geometric invariance, resulting in a kernel that is properly invariant under change of coordinates in the model parameter space. Experiments on detecting cognitive decline show that classiﬁers based on the proposed kernel out-perform those based on generative models and other feature extraction routines, and on Fisher kernels that use the identity in place of the Fisher information.</p><p>4 0.59707814 <a title="186-lda-4" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>Author: Rui M. Castro, Charles Kalish, Robert Nowak, Ruichen Qian, Tim Rogers, Xiaojin Zhu</p><p>Abstract: We investigate a topic at the interface of machine learning and cognitive science. Human active learning, where learners can actively query the world for information, is contrasted with passive learning from random examples. Furthermore, we compare human active learning performance with predictions from statistical learning theory. We conduct a series of human category learning experiments inspired by a machine learning task for which active and passive learning error bounds are well understood, and dramatically distinct. Our results indicate that humans are capable of actively selecting informative queries, and in doing so learn better and faster than if they are given random training data, as predicted by learning theory. However, the improvement over passive learning is not as dramatic as that achieved by machine active learning algorithms. To the best of our knowledge, this is the ﬁrst quantitative study comparing human category learning in active versus passive settings. 1</p><p>5 0.49974844 <a title="186-lda-5" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>Author: Charles Cadieu, Bruno A. Olshausen</p><p>Abstract: We describe a hierarchical, probabilistic model that learns to extract complex motion from movies of the natural environment. The model consists of two hidden layers: the ﬁrst layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables. The second layer learns the higher-order structure among the time-varying phase variables. After training on natural movies, the top layer units discover the structure of phase-shifts within the ﬁrst layer. We show that the top layer units encode transformational invariants: they are selective for the speed and direction of a moving pattern, but are invariant to its spatial structure (orientation/spatial-frequency). The diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in V1 and MT. In addition, the model demonstrates how feedback from higher levels can inﬂuence representations at lower levels as a by-product of inference in a graphical model. 1</p><p>6 0.49807277 <a title="186-lda-6" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>7 0.49750552 <a title="186-lda-7" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>8 0.49585256 <a title="186-lda-8" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>9 0.49452731 <a title="186-lda-9" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>10 0.49452078 <a title="186-lda-10" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>11 0.49409443 <a title="186-lda-11" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>12 0.49405524 <a title="186-lda-12" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>13 0.49387759 <a title="186-lda-13" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>14 0.49382997 <a title="186-lda-14" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>15 0.49345607 <a title="186-lda-15" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>16 0.49308926 <a title="186-lda-16" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>17 0.49298781 <a title="186-lda-17" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>18 0.49288392 <a title="186-lda-18" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>19 0.49280816 <a title="186-lda-19" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>20 0.49180156 <a title="186-lda-20" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
