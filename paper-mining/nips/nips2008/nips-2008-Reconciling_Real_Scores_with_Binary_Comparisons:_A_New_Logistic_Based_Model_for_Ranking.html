<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>190 nips-2008-Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-190" href="#">nips2008-190</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>190 nips-2008-Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking</h1>
<br/><p>Source: <a title="nips-2008-190-pdf" href="http://papers.nips.cc/paper/3463-reconciling-real-scores-with-binary-comparisons-a-new-logistic-based-model-for-ranking.pdf">pdf</a></p><p>Author: Nir Ailon</p><p>Abstract: The problem of ranking arises ubiquitously in almost every aspect of life, and in particular in Machine Learning/Information Retrieval. A statistical model for ranking predicts how humans rank subsets V of some universe U . In this work we deﬁne a statistical model for ranking that satisﬁes certain desirable properties. The model automatically gives rise to a logistic regression based approach to learning how to rank, for which the score and comparison based approaches are dual views. This offers a new generative approach to ranking which can be used for IR. There are two main contexts for this work. The ﬁrst is the theory of econometrics and study of statistical models explaining human choice of alternatives. In this context, we will compare our model with other well known models. The second context is the problem of ranking in machine learning, usually arising in the context of information retrieval. Here, much work has been done in the discriminative setting, where different heuristics are used to deﬁne ranking risk functions. Our model is built rigorously and axiomatically based on very simple desirable properties deﬁned locally for comparisons, and automatically implies the existence of a global score function serving as a natural model parameter which can be efﬁciently ﬁtted to pairwise comparison judgment data by solving a convex optimization problem. 1</p><p>Reference: <a title="nips-2008-190-reference" href="../nips2008_reference/nips-2008-Reconciling_Real_Scores_with_Binary_Comparisons%3A_A_New_Logistic_Based_Model_for_Ranking_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract The problem of ranking arises ubiquitously in almost every aspect of life, and in particular in Machine Learning/Information Retrieval. [sent-2, score-0.41]
</p><p>2 A statistical model for ranking predicts how humans rank subsets V of some universe U . [sent-3, score-0.583]
</p><p>3 In this work we deﬁne a statistical model for ranking that satisﬁes certain desirable properties. [sent-4, score-0.435]
</p><p>4 The model automatically gives rise to a logistic regression based approach to learning how to rank, for which the score and comparison based approaches are dual views. [sent-5, score-0.18]
</p><p>5 This offers a new generative approach to ranking which can be used for IR. [sent-6, score-0.41]
</p><p>6 The second context is the problem of ranking in machine learning, usually arising in the context of information retrieval. [sent-10, score-0.466]
</p><p>7 Here, much work has been done in the discriminative setting, where different heuristics are used to deﬁne ranking risk functions. [sent-11, score-0.41]
</p><p>8 The subject of preference and ranking has been thoroughly studied in the context of statistics and econometric theory [8, 7, 29, 36, 34, 31], combinatorial optimization [26, 37, 20, 3, 4, 14] and machine learning [6, 9, 33, 21, 19, 35, 23, 22, 25, 16, 17, 1, 13, 15, 28, 18]. [sent-15, score-0.474]
</p><p>9 Recently Ailon and Mehryar [5] following Balcan et al [9] have made signiﬁcant progress in reducing the task of learning ranking to the binary classiﬁcation problem of learning preferences. [sent-16, score-0.41]
</p><p>10 This comparison based approach is in contrast with a score based approach which tries to regress to a score function on the elements we wish to rank, and sort the elements based on this score as a ﬁnal step. [sent-17, score-0.487]
</p><p>11 The difference between the score based and comparison approaches is an example of ”local vs. [sent-18, score-0.155]
</p><p>12 global” views: A comparison is local (how do two elements compare with each other), and a score is global (how do we embed the universe on a scale). [sent-19, score-0.282]
</p><p>13 The score based approach seems reasonable in cases where the score can be deﬁned naturally in terms of measurable utility. [sent-20, score-0.238]
</p><p>14 In some real world scenarios, either (i) an interpretable score is difﬁcult to deﬁne (e. [sent-21, score-0.119]
</p><p>15 a relevance score in information retrieval) and (ii) an interpretable score is easy to deﬁne (e. [sent-23, score-0.238]
</p><p>16 how much a random person is willing  to pay for product X in some population) but learning the score is difﬁcult due to noisy or costly label acquisition for scores on individual points [7]. [sent-25, score-0.154]
</p><p>17 This phenomenon makes acquisition of comparison labels for learning tasks more appealing, but raises the question of how to go back and ﬁt a latent score function that explains the comparisons. [sent-27, score-0.155]
</p><p>18 Moreover, the score parameter ﬁtting must be computationally efﬁcient. [sent-28, score-0.119]
</p><p>19 2  Ranking in Context  The study of ranking alternatives has not been introduced by ML/IR, and has been studied throughly from the early years of the 20th century in the context of statistics and econometrics. [sent-30, score-0.49]
</p><p>20 ML/IR is usually interested in the question of how a machine should correctly rank alternatives based on experience from human feedback, whereas in statistics and econometrics the focus is on the question of how a human chooses from alternatives (for the purpose of e. [sent-33, score-0.213]
</p><p>21 For example, any attempt to correctly choose from a set (predominantly asked in the classic context) can be converted into a ranking algorithm by repeatedly choosing and removing from the set. [sent-38, score-0.45]
</p><p>22 1 A ranking model for U is a function D mapping any ﬁnite subset V ⊆ U to a distribution on rankings of V . [sent-40, score-0.494]
</p><p>23 A Thurstonian model for ranking (so named after L. [sent-43, score-0.435]
</p><p>24 Thurstone [36]) is one in which an independent random real valued variable Zv is associated with each v ∈ V , and the ranking is obtained by sorting the elements if V in decreasing order (assuming the value represents utility). [sent-44, score-0.457]
</p><p>25 Given items u, v, a subject would prefer u over v with probability puv = 1−pvu . [sent-49, score-0.103]
</p><p>26 Note that the marginal probability of u being preferred over v in the context of a set V ⊃ {u, v} in the Babington-Smith model is in general not puv , even in Mallows’s special case. [sent-53, score-0.156]
</p><p>27 In distance based models it is assumed that there is a ”modal” ranking of the set V , and the probability of any ranking decreases with its distance from the mode. [sent-54, score-0.85]
</p><p>28 The classic model most related to this work is Plackett and Luce’s [29, 34] multistage model for ranking. [sent-59, score-0.09]
</p><p>29 1 The winner is removed from V and the process is repeated for the remaining elements, until a ranking is obtained. [sent-63, score-0.459]
</p><p>30 The underlying winner choice model satisﬁes Luce’s choice axiom [29] which, roughly speaking, stipulates that the probability of an element u winning in V is the same as the product of the probability of the winner contained in V ′ ⊆ V and the probability of u winning in V ′ . [sent-66, score-0.429]
</p><p>31 It turns out that this axiom (often used as criticism of the model) implies the underlying choice function of the Plackett-Luce model. [sent-67, score-0.118]
</p><p>32 The model cannot explain both ranking by successive loser choice and successive winner choice simultaneously unless it is trivial (this point was noticed by McCullagh [32]). [sent-69, score-0.55]
</p><p>33 It is clear however that breaking down the process of ranking by humans to an iterated choice of winners ignores the process of elimination (placing alternatives at the bottom of the list). [sent-70, score-0.495]
</p><p>34 In the following sections we propose a new symmetric model for ranking, in which the basic discrete task is a comparison of pairs of elements, and not choice of an element from arbitrarily large sets (as in Plackett-Luce). [sent-71, score-0.144]
</p><p>35 3  An Axiomatic Approach for Deﬁning a Pairwise-Stable Model for Ranking  For a ranking π of some subset V ⊆ U , we use the notation u ≺π v to denote that u precedes2 v according to π. [sent-72, score-0.41]
</p><p>36 1 A ranking model D for U satisﬁes pairwise stability if for any u, v ∈ U and for any V1 , V2 ⊇ {u, v}, Prπ∼D(V1 ) [u ≺π v] = Prπ∼D(V2 ) [u ≺ v]. [sent-80, score-0.657]
</p><p>37 Pairwise stability means that the preference (or comparison) of u, v is statistically independent of the context (subset) they are ranked in. [sent-81, score-0.177]
</p><p>38 Note that Plackett-Luce is pairwise stable (this follows from the fact that the model is Thurstonian) but Babington-Smith/Mallows is not. [sent-82, score-0.134]
</p><p>39 If a ranking model D satisﬁes pairwise stability, then the probability PrD [u ≺ v] is naturally deﬁned and equals Prπ∼D(V ) [u ≺π v] for any V ⊇ {u, v}. [sent-83, score-0.574]
</p><p>40 Pairwise stability is a weak property which permits a very wide family of ranking distributions. [sent-84, score-0.554]
</p><p>41 In particular, if the universe U is a ﬁnite set then any distribution Π on rankings on the entire universe U gives rise to a model DΠ with DΠ (V ) deﬁned as the restriction of Π to V . [sent-85, score-0.244]
</p><p>42 This model clearly satisﬁes pairwise stability but does not have a succint description and hence undesirable. [sent-86, score-0.247]
</p><p>43 In words, this would mean that the comparison of u with w conditioned on the comparison being determined by pivoting around v is distributed like D({u, w}). [sent-94, score-0.31]
</p><p>44 We write this desired property as follows (the second line follows from the ﬁrst): 2 We choose in this work to use the convention that an element u precedes v if u is in a more favorable position. [sent-95, score-0.111]
</p><p>45 When a score function is introduced later, the convention will be that higher scores correspond to more favorable positions. [sent-96, score-0.184]
</p><p>46 2 Assume D is a ranking model for U satisfying pairwise stability. [sent-102, score-0.544]
</p><p>47 For a pair u, w ∈ U and another element v ∈ U we say that u and w satisfy the pivot condition with respect to v if (1) holds. [sent-103, score-0.397]
</p><p>48 We will see in what follows that the score function can be extended to a larger set by patching scores on triplets. [sent-108, score-0.154]
</p><p>49 By the symmetry it is now clear that the pivoting condition of u and w with respect to v implies the pivoting condition of u and v with respect to w and of v and w with respect to u. [sent-109, score-0.53]
</p><p>50 In other words, the pivoting condition is a property of the triplet {u, v, w}. [sent-110, score-0.322]
</p><p>51 3 Assume a ranking model D for U satisﬁes pairwise stability, and let ∆D : U × U → R denote the comparison logit as deﬁned above. [sent-112, score-0.683]
</p><p>52 A triplet {u, v, w} ⊆ U is said to satisfy the pivot condition in D if ∆D (u, v)+∆D (v, w)+∆D (w, u) = 0 . [sent-113, score-0.373]
</p><p>53 We say that U satisﬁes the pivot condition in D if {u, v, w} satisﬁes the pivot condition for all {u, v, w} ⊆ U . [sent-114, score-0.694]
</p><p>54 1 If U satisﬁes the pivot condition in a pairwise stability model D for U , then there exists a real valued score function s : V → R such that for all a, b ∈ V , ∆D (a, b) = s(a) − s(b) . [sent-116, score-0.713]
</p><p>55 Indeed, by construction s(a) − s(b) = ∆D (a, u) − ∆D (b, u) but by the pivot property this equals exactly ∆D (a, b), as required (remember that ∆D (a, b) = −∆D (a, b) by deﬁnition of ∆D ). [sent-120, score-0.406]
</p><p>56 By starting with local assumptions (pairwise stability and the pivoting property), we obtained a natural global score function s on the universe of elements. [sent-121, score-0.55]
</p><p>57 The score function governs the probability of u preceding v via the difference s(u) − s(v) passed through the inverse logit. [sent-122, score-0.149]
</p><p>58 Note that we used the assumption that the comparison logit is ﬁnite on all u, v (equivalently, that 0 < PrD (u ≺ v) < 1 for all u, v), but this assumption can be dropped if we allow the score function to obtain values in R + ωZ, where ω is the limit ordinal of R. [sent-123, score-0.284]
</p><p>59 The Plackett-Luce model satisﬁes both pairwise stability and the pivot condition with s(u) = log α(u). [sent-124, score-0.594]
</p><p>60 4  The New Ranking Model  We deﬁne a model called QSs (short for QuickSort), parametrized by a score function s : U → R as follows. [sent-127, score-0.144]
</p><p>61 Recurse on the left and on the right sides, and output the ranking of V obtained by joining the results in an obvious way (left ≺ pivot ≺ right). [sent-134, score-0.756]
</p><p>62 (The function 1/(1 + e−x ) is the inverse logit function. [sent-135, score-0.103]
</p><p>63 ) We shall soon see that QuickSort gives us back all the desired statistical local properties of a ranking models. [sent-136, score-0.41]
</p><p>64 That the model QSs can be sampled efﬁciently is a simple consequence of the fact that QuickSort runs in expected time O(n log n) (some attention needs to be paid the fact that unlike in the textbook proofs for QuickSort the pivoting process is randomized, but this is not difﬁcult [5]). [sent-137, score-0.263]
</p><p>65 1 The ranking model QSs for U satisﬁes both pairwise stability and the pivoting condition. [sent-139, score-0.895]
</p><p>66 Additionally, for any subset V ⊆ U the mode of QSs (V ) is any ranking π ∗ satisfying u ≺π∗ v whenever s(u) > s(v). [sent-140, score-0.434]
</p><p>67 1): First we note that if QSs satisﬁes pairwise stability, then the pivot property will be implied as well. [sent-142, score-0.46]
</p><p>68 Indeed, by taking V = {u, v} we would get from the model that PrQSs (u ≺ v) = 1/(1 + es(v)−s(u) ), immediately implying the pivot property. [sent-143, score-0.345]
</p><p>69 To see that QSs satisﬁes pairwise stability, we show that for any u, v and V ⊇ {u, v}, the probability of the event u ≺π v is exactly 1/(1 + es(v)−s(u) ), where π ∼ QSs (V ). [sent-144, score-0.199]
</p><p>70 (i) Directly: u or v are chosen as pivot when the other is present in the same recursive call. [sent-146, score-0.349]
</p><p>71 Conditioned on this event, clearly the probability that u ≺π v is exactly the required probability 1/(1 + es(v)−s(u) ) by step 2 of QuickSort (note that it doesn’t matter which one of v or u is the pivot). [sent-148, score-0.091]
</p><p>72 (ii) Indirectly: A third element w ∈ V is the pivot when both u and v are present in the recursive call, and w sends u and v to opposite ′ recursion sides. [sent-149, score-0.424]
</p><p>73 Conditioned on this event, the probability that u ≺π v, is exactly as required (by using the same logit calculus we used in Section 3). [sent-151, score-0.164]
</p><p>74 To conclude the proof of pairwise stability, it remains to observe that the collection of events ′ {E{u,v} } ∪ E{u,v},w : w ∈ V \ {u, v} is a pairwise disjoint cover of the probability space. [sent-152, score-0.248]
</p><p>75 This implies that Prπ∼QSs (V ) (u ≺π v) is the desired quantity 1/(1 + es(v)−s(u) ), concluding the proof of pairwise stability. [sent-153, score-0.109]
</p><p>76 Let τ, σ be two permutations on V such that a1 ≺τ a2 ≺τ · · · ≺τ ak ≺τ u ≺τ v ≺τ ak+1 ≺τ · · · ≺τ an−2 a1 ≺ a2 ≺σ · · · ≺σ ak ≺σ v ≺σ u ≺σ ak+1 ≺σ · · · ≺σ an−2 , where V = {u, v}∪{a1 , . [sent-155, score-0.113]
</p><p>77 Since π ∗ , the permutation sorting by s, can be obtained from any permutation by a sequence of swapping incorrectly ordered (according to s) adjacent pairs, this would prove the theorem by a standard inductive argument. [sent-162, score-0.1]
</p><p>78 This tree records the ﬁnal position of the pivots chosen in each step as follows: The label L of the root of the tree is the rank of the pivot in the ﬁnal solution (which equals the size of the left recursion plus 1). [sent-166, score-0.519]
</p><p>79 The left subtree is the tree recursively constructed on the left, and the right subtree is the tree recursively constructed on the right with L added to the labels of all the vertices. [sent-167, score-0.254]
</p><p>80 Clearly the resulting tree has exactly n nodes with each label in {1 . [sent-168, score-0.112]
</p><p>81 Let pπ,T denote the probability that QuickSort outputs a permutation π and (implicitly) constructs a pivot selection tree T . [sent-172, score-0.428]
</p><p>82 Let Tx denote the subtree rooted by x and let ℓ(Tx ) denote the 4 By that we mean a tree in which each node has at most one left child node and at most one right child node, and the nodes are labeled with integers. [sent-178, score-0.295]
</p><p>83 By construction, if QuickSort outputted a ranking π with an (implicitly constructed) tree T , then at some point the recursive call to QuickSort took π −1 (ℓ(Tx )) as input and chose π −1 (ℓ(x)) as pivot, for any node x of T . [sent-180, score-0.536]
</p><p>84 To compute qπ,T for π = τ, σ we proceed as follows: At each node x of T we will attach a number Pπ (x) which is the likelihood of the decisions made at that level, namely, the choice of the pivot itself and the separation of the rest of the elements to its right and left. [sent-183, score-0.457]
</p><p>85 Pπ (x) =  1 |Tx |  Pr[π −1 (ℓ(y)) ≺ π −1 (ℓ(x))] × y∈TL (x)  QS  Pr[π −1 (ℓ(x)) ≺ π −1 (ℓ(y))] , y∈TR (x)  QS  Where |Tx | is the number of nodes in Tx , TR (x) is the set of vertices in the left subtree of x and similarly for TL (x). [sent-184, score-0.141]
</p><p>86 The factor 1/|Tx | comes from the likelihood of uniformly at random having chosen the pivot π −1 (ℓ(x)) from the set of nodes of Tx . [sent-185, score-0.361]
</p><p>87 By our assumption that τ and σ differ only on the order of the adjacent elements u, v, Pτ (x) and Pσ (x) could differ only on nodes x on the path between x1 and x2 . [sent-190, score-0.112]
</p><p>88 that x1 is an ancestor of x2 , and that x2 is a node in the left subtree of x1 . [sent-195, score-0.157]
</p><p>89 Let W denote the set of nodes in the left (and only) subtree of x2 , and let Z denote the set of remaining nodes in TL (x1 ): Z = TL (x1 ) \ (W ∪ Y ∪ {x2 }). [sent-198, score-0.182]
</p><p>90 Since τ −1 (ℓ(z)) = σ −1 (ℓ(z)) for all z ∈ Z we can deﬁne elt(z) = τ −1 (ℓ(z)) = σ −1 (ℓ(z)) and similarly we can correspond each y ∈ Y with a single element elt(y) and each w ∈ W with a single elements elt(w) of V . [sent-199, score-0.097]
</p><p>91 Also for all w ∈ W the probability of throwing elt(w) to the left of u (pivoting on u) times the probability of throwing elt(w) to the left of v (pivoting on v) appears exactly once in both Pτ (x1 )Pτ (x2 ) and Pσ (x1 )Pσ (x2 ) (though in reversed order). [sent-202, score-0.319]
</p><p>92 Both satisfy the intuitive property that the mode of the distribution corresponding to a set V is any ranking which sorts the elements of V in decreasing s(v) = log α(v) value. [sent-205, score-0.512]
</p><p>93 This also holds true in a generalized linear model, in which s(v) is given as the dot product of a feature vector φ(v) with an unknown weight 5  The rightmost node of T is the root if it has no right descendent, or the rightmost node of its right subtree. [sent-212, score-0.178]
</p><p>94 A simple linear ranking algorithm using query dependent intercept variables. [sent-225, score-0.41]
</p><p>95 Learning to rank: from pairwise approach to listwise approach. [sent-273, score-0.109]
</p><p>96 Ordering by weighted number of wins gives a good ranking for weighted tournamnets. [sent-287, score-0.434]
</p><p>97 In STOC ’07: Proceedings of the thirty-ninth annual ACM symposium on Theory of computing, pages 95– 103, New York, NY, USA, 2007. [sent-338, score-0.09]
</p><p>98 Cranking: Combining rankings using conditional probability models on permutations. [sent-342, score-0.089]
</p><p>99 Probability models and statistical analyses for ranking data, pages 196–215, 1993. [sent-366, score-0.443]
</p><p>100 ”deterministic algorithms for rank aggregation and other ranking and clustering problems”. [sent-390, score-0.478]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('prd', 0.425), ('ranking', 0.41), ('pivot', 0.32), ('quicksort', 0.274), ('elt', 0.238), ('pivoting', 0.238), ('qss', 0.22), ('score', 0.119), ('stability', 0.113), ('pairwise', 0.109), ('logit', 0.103), ('tx', 0.091), ('throwing', 0.088), ('mehryar', 0.088), ('universe', 0.08), ('subtree', 0.074), ('corinna', 0.073), ('prqs', 0.073), ('puv', 0.073), ('rank', 0.068), ('pr', 0.067), ('tl', 0.065), ('yoram', 0.064), ('ailon', 0.064), ('cortes', 0.064), ('acm', 0.059), ('rankings', 0.059), ('nir', 0.059), ('node', 0.057), ('thurstone', 0.055), ('zv', 0.055), ('alternatives', 0.052), ('element', 0.05), ('winner', 0.049), ('qs', 0.049), ('permutations', 0.049), ('satis', 0.049), ('axiom', 0.048), ('luce', 0.048), ('elements', 0.047), ('es', 0.045), ('mohri', 0.044), ('econometrics', 0.041), ('nodes', 0.041), ('classic', 0.04), ('tree', 0.04), ('colt', 0.04), ('proceedings', 0.039), ('permutation', 0.038), ('coppersmith', 0.037), ('criticism', 0.037), ('mallows', 0.037), ('thore', 0.037), ('thurstonian', 0.037), ('comparison', 0.036), ('preference', 0.036), ('june', 0.036), ('ny', 0.035), ('scores', 0.035), ('robert', 0.034), ('pages', 0.033), ('choice', 0.033), ('rightmost', 0.032), ('ralf', 0.032), ('lebanon', 0.032), ('ak', 0.032), ('symposium', 0.031), ('demand', 0.031), ('exactly', 0.031), ('property', 0.031), ('usa', 0.03), ('favorable', 0.03), ('probability', 0.03), ('recursive', 0.029), ('bradley', 0.029), ('ashish', 0.029), ('event', 0.029), ('schapire', 0.029), ('context', 0.028), ('klaus', 0.027), ('balcan', 0.027), ('condition', 0.027), ('electric', 0.026), ('judgment', 0.026), ('aggregating', 0.026), ('winning', 0.026), ('italy', 0.026), ('soda', 0.026), ('triplet', 0.026), ('ordinal', 0.026), ('left', 0.026), ('annual', 0.026), ('model', 0.025), ('recursion', 0.025), ('graepel', 0.025), ('decide', 0.025), ('adjacent', 0.024), ('construction', 0.024), ('mode', 0.024), ('wins', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000014 <a title="190-tfidf-1" href="./nips-2008-Reconciling_Real_Scores_with_Binary_Comparisons%3A_A_New_Logistic_Based_Model_for_Ranking.html">190 nips-2008-Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking</a></p>
<p>Author: Nir Ailon</p><p>Abstract: The problem of ranking arises ubiquitously in almost every aspect of life, and in particular in Machine Learning/Information Retrieval. A statistical model for ranking predicts how humans rank subsets V of some universe U . In this work we deﬁne a statistical model for ranking that satisﬁes certain desirable properties. The model automatically gives rise to a logistic regression based approach to learning how to rank, for which the score and comparison based approaches are dual views. This offers a new generative approach to ranking which can be used for IR. There are two main contexts for this work. The ﬁrst is the theory of econometrics and study of statistical models explaining human choice of alternatives. In this context, we will compare our model with other well known models. The second context is the problem of ranking in machine learning, usually arising in the context of information retrieval. Here, much work has been done in the discriminative setting, where different heuristics are used to deﬁne ranking risk functions. Our model is built rigorously and axiomatically based on very simple desirable properties deﬁned locally for comparisons, and automatically implies the existence of a global score function serving as a natural model parameter which can be efﬁciently ﬁtted to pairwise comparison judgment data by solving a convex optimization problem. 1</p><p>2 0.30670369 <a title="190-tfidf-2" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>Author: Tao Qin, Tie-yan Liu, Xu-dong Zhang, De-sheng Wang, Hang Li</p><p>Abstract: This paper studies global ranking problem by learning to rank methods. Conventional learning to rank methods are usually designed for ‘local ranking’, in the sense that the ranking model is deﬁned on a single object, for example, a document in information retrieval. For many applications, this is a very loose approximation. Relations always exist between objects and it is better to deﬁne the ranking model as a function on all the objects to be ranked (i.e., the relations are also included). This paper refers to the problem as global ranking and proposes employing a Continuous Conditional Random Fields (CRF) for conducting the learning task. The Continuous CRF model is deﬁned as a conditional probability distribution over ranking scores of objects conditioned on the objects. It can naturally represent the content information of objects as well as the relation information between objects, necessary for global ranking. Taking two speciﬁc information retrieval tasks as examples, the paper shows how the Continuous CRF method can perform global ranking better than baselines.</p><p>3 0.16542496 <a title="190-tfidf-3" href="./nips-2008-Structured_ranking_learning_using_cumulative_distribution_networks.html">224 nips-2008-Structured ranking learning using cumulative distribution networks</a></p>
<p>Author: Jim C. Huang, Brendan J. Frey</p><p>Abstract: Ranking is at the heart of many information retrieval applications. Unlike standard regression or classiﬁcation in which we predict outputs independently, in ranking we are interested in predicting structured outputs so that misranking one object can signiﬁcantly affect whether we correctly rank the other objects. In practice, the problem of ranking involves a large number of objects to be ranked and either approximate structured prediction methods are required, or assumptions of independence between object scores must be made in order to make the problem tractable. We present a probabilistic method for learning to rank using the graphical modelling framework of cumulative distribution networks (CDNs), where we can take into account the structure inherent to the problem of ranking by modelling the joint cumulative distribution functions (CDFs) over multiple pairwise preferences. We apply our framework to the problem of document retrieval in the case of the OHSUMED benchmark dataset. We will show that the RankNet, ListNet and ListMLE probabilistic models can be viewed as particular instances of CDNs and that our proposed framework allows for the exploration of a broad class of ﬂexible structured loss functionals for learning to rank. 1</p><p>4 0.11716685 <a title="190-tfidf-4" href="./nips-2008-Empirical_performance_maximization_for_linear_rank_statistics.html">72 nips-2008-Empirical performance maximization for linear rank statistics</a></p>
<p>Author: Stéphan J. Clémençcon, Nicolas Vayatis</p><p>Abstract: The ROC curve is known to be the golden standard for measuring performance of a test/scoring statistic regarding its capacity of discrimination between two populations in a wide variety of applications, ranging from anomaly detection in signal processing to information retrieval, through medical diagnosis. Most practical performance measures used in scoring applications such as the AUC, the local AUC, the p-norm push, the DCG and others, can be seen as summaries of the ROC curve. This paper highlights the fact that many of these empirical criteria can be expressed as (conditional) linear rank statistics. We investigate the properties of empirical maximizers of such performance criteria and provide preliminary results for the concentration properties of a novel class of random variables that we will call a linear rank process. 1</p><p>5 0.10087827 <a title="190-tfidf-5" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>Author: Srikanth Jagabathula, Devavrat Shah</p><p>Abstract: Motivated by applications like elections, web-page ranking, revenue maximization etc., we consider the question of inferring popular rankings using constrained data. More speciﬁcally, we consider the problem of inferring a probability distribution over the group of permutations using its ﬁrst order marginals. We ﬁrst prove that it is not possible to recover more than O(n) permutations over n elements with the given information. We then provide a simple and novel algorithm that can recover up to O(n) permutations under a natural stochastic model; in this sense, the algorithm is optimal. In certain applications, the interest is in recovering only the most popular (or mode) ranking. As a second result, we provide an algorithm based on the Fourier Transform over the symmetric group to recover the mode under a natural majority condition; the algorithm turns out to be a maximum weight matching on an appropriately deﬁned weighted bipartite graph. The questions considered are also thematically related to Fourier Transforms over the symmetric group and the currently popular topic of compressed sensing. 1</p><p>6 0.097439133 <a title="190-tfidf-6" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>7 0.067754522 <a title="190-tfidf-7" href="./nips-2008-Overlaying_classifiers%3A_a_practical_approach_for_optimal_ranking.html">174 nips-2008-Overlaying classifiers: a practical approach for optimal ranking</a></p>
<p>8 0.061880153 <a title="190-tfidf-8" href="./nips-2008-Predictive_Indexing_for_Fast_Search.html">184 nips-2008-Predictive Indexing for Fast Search</a></p>
<p>9 0.060363594 <a title="190-tfidf-9" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>10 0.058887511 <a title="190-tfidf-10" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>11 0.053835485 <a title="190-tfidf-11" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>12 0.045102768 <a title="190-tfidf-12" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>13 0.044401255 <a title="190-tfidf-13" href="./nips-2008-Online_Optimization_in_X-Armed_Bandits.html">170 nips-2008-Online Optimization in X-Armed Bandits</a></p>
<p>14 0.043857992 <a title="190-tfidf-14" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>15 0.040535934 <a title="190-tfidf-15" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>16 0.040170375 <a title="190-tfidf-16" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>17 0.039081469 <a title="190-tfidf-17" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>18 0.038592961 <a title="190-tfidf-18" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>19 0.037661579 <a title="190-tfidf-19" href="./nips-2008-On_Bootstrapping_the_ROC_Curve.html">159 nips-2008-On Bootstrapping the ROC Curve</a></p>
<p>20 0.037064604 <a title="190-tfidf-20" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.144), (1, -0.026), (2, -0.048), (3, -0.004), (4, -0.044), (5, -0.087), (6, 0.229), (7, -0.041), (8, 0.084), (9, 0.074), (10, -0.147), (11, -0.026), (12, -0.035), (13, 0.115), (14, 0.09), (15, 0.032), (16, 0.057), (17, -0.05), (18, 0.192), (19, 0.196), (20, 0.016), (21, -0.032), (22, -0.045), (23, -0.028), (24, 0.066), (25, -0.039), (26, -0.213), (27, 0.09), (28, 0.095), (29, -0.077), (30, 0.112), (31, -0.124), (32, -0.042), (33, 0.112), (34, -0.011), (35, -0.035), (36, -0.095), (37, -0.043), (38, 0.131), (39, -0.038), (40, -0.091), (41, -0.003), (42, -0.069), (43, -0.024), (44, -0.062), (45, -0.044), (46, 0.064), (47, 0.028), (48, 0.083), (49, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95331132 <a title="190-lsi-1" href="./nips-2008-Reconciling_Real_Scores_with_Binary_Comparisons%3A_A_New_Logistic_Based_Model_for_Ranking.html">190 nips-2008-Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking</a></p>
<p>Author: Nir Ailon</p><p>Abstract: The problem of ranking arises ubiquitously in almost every aspect of life, and in particular in Machine Learning/Information Retrieval. A statistical model for ranking predicts how humans rank subsets V of some universe U . In this work we deﬁne a statistical model for ranking that satisﬁes certain desirable properties. The model automatically gives rise to a logistic regression based approach to learning how to rank, for which the score and comparison based approaches are dual views. This offers a new generative approach to ranking which can be used for IR. There are two main contexts for this work. The ﬁrst is the theory of econometrics and study of statistical models explaining human choice of alternatives. In this context, we will compare our model with other well known models. The second context is the problem of ranking in machine learning, usually arising in the context of information retrieval. Here, much work has been done in the discriminative setting, where different heuristics are used to deﬁne ranking risk functions. Our model is built rigorously and axiomatically based on very simple desirable properties deﬁned locally for comparisons, and automatically implies the existence of a global score function serving as a natural model parameter which can be efﬁciently ﬁtted to pairwise comparison judgment data by solving a convex optimization problem. 1</p><p>2 0.86630219 <a title="190-lsi-2" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>Author: Tao Qin, Tie-yan Liu, Xu-dong Zhang, De-sheng Wang, Hang Li</p><p>Abstract: This paper studies global ranking problem by learning to rank methods. Conventional learning to rank methods are usually designed for ‘local ranking’, in the sense that the ranking model is deﬁned on a single object, for example, a document in information retrieval. For many applications, this is a very loose approximation. Relations always exist between objects and it is better to deﬁne the ranking model as a function on all the objects to be ranked (i.e., the relations are also included). This paper refers to the problem as global ranking and proposes employing a Continuous Conditional Random Fields (CRF) for conducting the learning task. The Continuous CRF model is deﬁned as a conditional probability distribution over ranking scores of objects conditioned on the objects. It can naturally represent the content information of objects as well as the relation information between objects, necessary for global ranking. Taking two speciﬁc information retrieval tasks as examples, the paper shows how the Continuous CRF method can perform global ranking better than baselines.</p><p>3 0.77090871 <a title="190-lsi-3" href="./nips-2008-Structured_ranking_learning_using_cumulative_distribution_networks.html">224 nips-2008-Structured ranking learning using cumulative distribution networks</a></p>
<p>Author: Jim C. Huang, Brendan J. Frey</p><p>Abstract: Ranking is at the heart of many information retrieval applications. Unlike standard regression or classiﬁcation in which we predict outputs independently, in ranking we are interested in predicting structured outputs so that misranking one object can signiﬁcantly affect whether we correctly rank the other objects. In practice, the problem of ranking involves a large number of objects to be ranked and either approximate structured prediction methods are required, or assumptions of independence between object scores must be made in order to make the problem tractable. We present a probabilistic method for learning to rank using the graphical modelling framework of cumulative distribution networks (CDNs), where we can take into account the structure inherent to the problem of ranking by modelling the joint cumulative distribution functions (CDFs) over multiple pairwise preferences. We apply our framework to the problem of document retrieval in the case of the OHSUMED benchmark dataset. We will show that the RankNet, ListNet and ListMLE probabilistic models can be viewed as particular instances of CDNs and that our proposed framework allows for the exploration of a broad class of ﬂexible structured loss functionals for learning to rank. 1</p><p>4 0.45727611 <a title="190-lsi-4" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>Author: Srikanth Jagabathula, Devavrat Shah</p><p>Abstract: Motivated by applications like elections, web-page ranking, revenue maximization etc., we consider the question of inferring popular rankings using constrained data. More speciﬁcally, we consider the problem of inferring a probability distribution over the group of permutations using its ﬁrst order marginals. We ﬁrst prove that it is not possible to recover more than O(n) permutations over n elements with the given information. We then provide a simple and novel algorithm that can recover up to O(n) permutations under a natural stochastic model; in this sense, the algorithm is optimal. In certain applications, the interest is in recovering only the most popular (or mode) ranking. As a second result, we provide an algorithm based on the Fourier Transform over the symmetric group to recover the mode under a natural majority condition; the algorithm turns out to be a maximum weight matching on an appropriately deﬁned weighted bipartite graph. The questions considered are also thematically related to Fourier Transforms over the symmetric group and the currently popular topic of compressed sensing. 1</p><p>5 0.4203966 <a title="190-lsi-5" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>Author: Olivier Chapelle, Chuong B. Do, Choon H. Teo, Quoc V. Le, Alex J. Smola</p><p>Abstract: Large-margin structured estimation methods minimize a convex upper bound of loss functions. While they allow for efﬁcient optimization algorithms, these convex formulations are not tight and sacriﬁce the ability to accurately model the true loss. We present tighter non-convex bounds based on generalizing the notion of a ramp loss from binary classiﬁcation to structured estimation. We show that a small modiﬁcation of existing optimization algorithms sufﬁces to solve this modiﬁed problem. On structured prediction tasks such as protein sequence alignment and web page ranking, our algorithm leads to improved accuracy. 1</p><p>6 0.40348169 <a title="190-lsi-6" href="./nips-2008-Empirical_performance_maximization_for_linear_rank_statistics.html">72 nips-2008-Empirical performance maximization for linear rank statistics</a></p>
<p>7 0.319134 <a title="190-lsi-7" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>8 0.31179282 <a title="190-lsi-8" href="./nips-2008-Online_Models_for_Content_Optimization.html">169 nips-2008-Online Models for Content Optimization</a></p>
<p>9 0.30024555 <a title="190-lsi-9" href="./nips-2008-Predictive_Indexing_for_Fast_Search.html">184 nips-2008-Predictive Indexing for Fast Search</a></p>
<p>10 0.29987335 <a title="190-lsi-10" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>11 0.2774899 <a title="190-lsi-11" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>12 0.25659198 <a title="190-lsi-12" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>13 0.24221237 <a title="190-lsi-13" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>14 0.22430779 <a title="190-lsi-14" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>15 0.21855514 <a title="190-lsi-15" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>16 0.21473515 <a title="190-lsi-16" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>17 0.21469563 <a title="190-lsi-17" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>18 0.2131463 <a title="190-lsi-18" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>19 0.21110477 <a title="190-lsi-19" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>20 0.20937319 <a title="190-lsi-20" href="./nips-2008-Online_Optimization_in_X-Armed_Bandits.html">170 nips-2008-Online Optimization in X-Armed Bandits</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.039), (7, 0.031), (12, 0.023), (15, 0.016), (28, 0.667), (57, 0.038), (59, 0.013), (63, 0.016), (71, 0.02), (77, 0.021), (83, 0.033)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99869561 <a title="190-lda-1" href="./nips-2008-Empirical_performance_maximization_for_linear_rank_statistics.html">72 nips-2008-Empirical performance maximization for linear rank statistics</a></p>
<p>Author: Stéphan J. Clémençcon, Nicolas Vayatis</p><p>Abstract: The ROC curve is known to be the golden standard for measuring performance of a test/scoring statistic regarding its capacity of discrimination between two populations in a wide variety of applications, ranging from anomaly detection in signal processing to information retrieval, through medical diagnosis. Most practical performance measures used in scoring applications such as the AUC, the local AUC, the p-norm push, the DCG and others, can be seen as summaries of the ROC curve. This paper highlights the fact that many of these empirical criteria can be expressed as (conditional) linear rank statistics. We investigate the properties of empirical maximizers of such performance criteria and provide preliminary results for the concentration properties of a novel class of random variables that we will call a linear rank process. 1</p><p>same-paper 2 0.99806213 <a title="190-lda-2" href="./nips-2008-Reconciling_Real_Scores_with_Binary_Comparisons%3A_A_New_Logistic_Based_Model_for_Ranking.html">190 nips-2008-Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking</a></p>
<p>Author: Nir Ailon</p><p>Abstract: The problem of ranking arises ubiquitously in almost every aspect of life, and in particular in Machine Learning/Information Retrieval. A statistical model for ranking predicts how humans rank subsets V of some universe U . In this work we deﬁne a statistical model for ranking that satisﬁes certain desirable properties. The model automatically gives rise to a logistic regression based approach to learning how to rank, for which the score and comparison based approaches are dual views. This offers a new generative approach to ranking which can be used for IR. There are two main contexts for this work. The ﬁrst is the theory of econometrics and study of statistical models explaining human choice of alternatives. In this context, we will compare our model with other well known models. The second context is the problem of ranking in machine learning, usually arising in the context of information retrieval. Here, much work has been done in the discriminative setting, where different heuristics are used to deﬁne ranking risk functions. Our model is built rigorously and axiomatically based on very simple desirable properties deﬁned locally for comparisons, and automatically implies the existence of a global score function serving as a natural model parameter which can be efﬁciently ﬁtted to pairwise comparison judgment data by solving a convex optimization problem. 1</p><p>3 0.99646324 <a title="190-lda-3" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>Author: Angela J. Yu, Jonathan D. Cohen</p><p>Abstract: In a variety of behavioral tasks, subjects exhibit an automatic and apparently suboptimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no real predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reﬂect the inadvertent engagement of mechanisms critical for adapting to a changing environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential ﬁltering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential ﬁlter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that parameter-tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities. 1</p><p>4 0.99572963 <a title="190-lda-4" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>Author: Gal Elidan, Stephen Gould</p><p>Abstract: With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufﬁciently expressive for generalization while also allowing for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overﬁtting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modiﬁcations and that is polynomial in the size of the graph and the treewidth bound. At the heart of our method is a triangulated graph that we dynamically update in a way that facilitates the addition of chain structures that increase the bound on the model’s treewidth by at most one. We demonstrate the effectiveness of our “treewidth-friendly” method on several real-life datasets. Importantly, we also show that by using global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth. 1</p><p>5 0.98817128 <a title="190-lda-5" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>Author: Matthew Blaschko, Arthur Gretton</p><p>Abstract: We introduce a family of unsupervised algorithms, numerical taxonomy clustering, to simultaneously cluster data, and to learn a taxonomy that encodes the relationship between the clusters. The algorithms work by maximizing the dependence between the taxonomy and the original data. The resulting taxonomy is a more informative visualization of complex data than simple clustering; in addition, taking into account the relations between different clusters is shown to substantially improve the quality of the clustering, when compared with state-ofthe-art algorithms in the literature (both spectral clustering and a previous dependence maximization approach). We demonstrate our algorithm on image and text data. 1</p><p>6 0.9876703 <a title="190-lda-6" href="./nips-2008-Overlaying_classifiers%3A_a_practical_approach_for_optimal_ranking.html">174 nips-2008-Overlaying classifiers: a practical approach for optimal ranking</a></p>
<p>7 0.98663461 <a title="190-lda-7" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>8 0.98259342 <a title="190-lda-8" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>9 0.97982782 <a title="190-lda-9" href="./nips-2008-Stress%2C_noradrenaline%2C_and_realistic_prediction_of_mouse_behaviour_using_reinforcement_learning.html">222 nips-2008-Stress, noradrenaline, and realistic prediction of mouse behaviour using reinforcement learning</a></p>
<p>10 0.96997476 <a title="190-lda-10" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>11 0.96281958 <a title="190-lda-11" href="./nips-2008-On_Bootstrapping_the_ROC_Curve.html">159 nips-2008-On Bootstrapping the ROC Curve</a></p>
<p>12 0.94677681 <a title="190-lda-12" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>13 0.93841934 <a title="190-lda-13" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>14 0.93816835 <a title="190-lda-14" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>15 0.93604565 <a title="190-lda-15" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>16 0.9355647 <a title="190-lda-16" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>17 0.93167013 <a title="190-lda-17" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>18 0.93109453 <a title="190-lda-18" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>19 0.93028891 <a title="190-lda-19" href="./nips-2008-Measures_of_Clustering_Quality%3A_A_Working_Set_of_Axioms_for_Clustering.html">132 nips-2008-Measures of Clustering Quality: A Working Set of Axioms for Clustering</a></p>
<p>20 0.92761528 <a title="190-lda-20" href="./nips-2008-Structured_ranking_learning_using_cumulative_distribution_networks.html">224 nips-2008-Structured ranking learning using cumulative distribution networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
