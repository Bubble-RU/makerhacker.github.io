<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-20" href="#">nips2008-20</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</h1>
<br/><p>Source: <a title="nips-2008-20-pdf" href="http://papers.nips.cc/paper/3520-an-extended-level-method-for-efficient-multiple-kernel-learning.pdf">pdf</a></p><p>Author: Zenglin Xu, Rong Jin, Irwin King, Michael Lyu</p><p>Abstract: We consider the problem of multiple kernel learning (MKL), which can be formulated as a convex-concave problem. In the past, two efﬁcient methods, i.e., Semi-Inﬁnite Linear Programming (SILP) and Subgradient Descent (SD), have been proposed for large-scale multiple kernel learning. Despite their success, both methods have their own shortcomings: (a) the SD method utilizes the gradient of only the current solution, and (b) the SILP method does not regularize the approximate solution obtained from the cutting plane model. In this work, we extend the level method, which was originally designed for optimizing non-smooth objective functions, to convex-concave optimization, and apply it to multiple kernel learning. The extended level method overcomes the drawbacks of SILP and SD by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set. Empirical study with eight UCI datasets shows that the extended level method can signiﬁcantly improve efﬁciency by saving on average 91.9% of computational time over the SILP method and 70.3% over the SD method. 1</p><p>Reference: <a title="nips-2008-20-reference" href="../nips2008_reference/nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu †  Abstract We consider the problem of multiple kernel learning (MKL), which can be formulated as a convex-concave problem. [sent-11, score-0.203]
</p><p>2 , Semi-Inﬁnite Linear Programming (SILP) and Subgradient Descent (SD), have been proposed for large-scale multiple kernel learning. [sent-14, score-0.203]
</p><p>3 Despite their success, both methods have their own shortcomings: (a) the SD method utilizes the gradient of only the current solution, and (b) the SILP method does not regularize the approximate solution obtained from the cutting plane model. [sent-15, score-0.743]
</p><p>4 In this work, we extend the level method, which was originally designed for optimizing non-smooth objective functions, to convex-concave optimization, and apply it to multiple kernel learning. [sent-16, score-0.519]
</p><p>5 The extended level method overcomes the drawbacks of SILP and SD by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set. [sent-17, score-0.732]
</p><p>6 Empirical study with eight UCI datasets shows that the extended level method can signiﬁcantly improve efﬁciency by saving on average 91. [sent-18, score-0.319]
</p><p>7 This is due to the importance of kernel methods in that kernel functions deﬁne a generalized similarity measure among data. [sent-22, score-0.336]
</p><p>8 A generic approach to learning a kernel function is known as multiple kernel learning (MKL) [5]: given a list of base kernel functions/matrices, MKL searches for the linear combination of base kernel functions which maximizes a generalized performance measure. [sent-23, score-0.819]
</p><p>9 Previous studies [5, 14, 13, 4, 1] have shown that MKL is usually able to identify appropriate combination of kernel functions, and as a result to improve the performance. [sent-24, score-0.168]
</p><p>10 For instance, base kernels can be created by using different kernel functions; they can also be created by using a single kernel function but with different subsets of features. [sent-26, score-0.42]
</p><p>11 As for the performance measures needed to ﬁnd the optimal kernel function, several measures have been studied for multiple kernel learning, including maximum margin classiﬁcation errors [5], kernel-target alignment [4], and Fisher discriminative analysis [13]. [sent-27, score-0.393]
</p><p>12 The multiple kernel learning problem was ﬁrst formulated as a semi-deﬁnite programming (SDP) problem by [5]. [sent-28, score-0.243]
</p><p>13 SILP is an iterative algorithm that alternates between the optimization of kernel weights and the optimization of the SVM classiﬁer. [sent-31, score-0.273]
</p><p>14 In each step, given the current solution of kernel weights, it solves a classical SVM with the combined kernel; it then constructs a cutting plane model for the objective function and updates the kernel weights by solving a corresponding linear programming  problem. [sent-32, score-1.059]
</p><p>15 One shortcoming of the SILP method is that it updates kernel weights solely based on the cutting plane model. [sent-34, score-0.762]
</p><p>16 Given that a cutting plane model usually differs significantly from the original objective function when the solution is far away from the points where the cutting plane model is constructed, the optimal solution to the cutting plane model could be signiﬁcantly off target. [sent-35, score-1.64]
</p><p>17 To further improve the computational efﬁciency of MKL, we extended the level method [6], which was originally designed for optimizing non-smooth functions, to the optimization of convex-concave problems. [sent-38, score-0.387]
</p><p>18 In the present work, similar to the SILP method, we construct in each iteration a cutting plane model for the target objective function using the solutions to the intermediate SVM problems. [sent-40, score-0.599]
</p><p>19 A new solution for kernel weights is obtained by solving the cutting plane model. [sent-41, score-0.746]
</p><p>20 We furthermore adjust the new solution via a projection to a level set. [sent-42, score-0.317]
</p><p>21 This adjustment is critical in that it ensures on one hand the new solution is sufﬁciently close to the current solution, and on the other hand the new solution signiﬁcantly reduces the objective function. [sent-43, score-0.192]
</p><p>22 We show that the extended level method has a convergence rate of O(1/ε2 ) for a ε-accurate solution. [sent-44, score-0.311]
</p><p>23 Although this is similar to that of the SD method, the extended level method is advantageous in that it utilizes all the gradients that have been computed so far. [sent-45, score-0.392]
</p><p>24 Empirical results with eight UCI datasets show that the extended level method is able to greatly improve the efﬁciency of multiple kernel learning in comparison with the SILP method and the SD method. [sent-46, score-0.587]
</p><p>25 In section 2, we review the efﬁcient algorithms that have been designed for multiple kernel learning. [sent-48, score-0.226]
</p><p>26 In section 3, we describe the details of the extended level method for MKL, including a study of its convergence rate. [sent-49, score-0.311]
</p><p>27 In section 4, we present experimental results by comparing both the effectiveness and the efﬁciency of the extended level method with the corresponding measures of SILP and SD. [sent-50, score-0.288]
</p><p>28 Here, e is a vector of all ones, C is the trade-off parameter in SVM, {Ki }m is a group i=1 of base kernel matrices, and ◦ deﬁnes the element-wise product between two vectors. [sent-62, score-0.224]
</p><p>29 They differ in the 4th step in Algorithm 1: the SILP method updates p by solving a cutting plane model, while the SD method updates p using the subgradient of the current solution. [sent-73, score-0.737]
</p><p>30 , i},  (2)  ϕi (p; α) SD  =  1 p − pi 2  (3)  ν  2 2  + γi (p − pi )⊤ ∇p f (pi , αi ),  where γi is the step size that needs to be decided dynamically (e. [sent-77, score-0.316]
</p><p>31 Comparing the two methods, we observe • In SILP, the cutting plane model ϕSILP (p) utilizes all the {αj }i obtained in past iteraj=1 tions. [sent-84, score-0.581]
</p><p>32 In contrast, SD only utilizes αi of the current solution pi . [sent-85, score-0.294]
</p><p>33 • SILP updates the solution for p based on the cutting plane model ϕSILP (p). [sent-86, score-0.558]
</p><p>34 Since the cutting plane model is usually inaccurate when p is far away from {pj }i , the updated j=1 solution p could be signiﬁcantly off target [3]. [sent-87, score-0.595]
</p><p>35 In contrast, a regularization term p − pi 2 /2 is introduced in SD to prevent the new solution being far from the current one, pi . [sent-88, score-0.424]
</p><p>36 2 The proposed level method combines the strengths of both methods. [sent-89, score-0.25]
</p><p>37 Similar to SILP, it utilizes the gradient information of all the iterations; similar to SD, a regularization scheme is introduced to prevent the updated solution from being too far from the current solution. [sent-90, score-0.207]
</p><p>38 3  Extended Level Method for MKL  We ﬁrst introduce the basic steps of the level method, followed by the extension of the level method to convex-concave problems and its application to MKL. [sent-91, score-0.435]
</p><p>39 1  Introduction to the Level Method  The level method [6] is from the family of bundle methods, which have recently been employed to efﬁciently solve regularized risk minimization problems [11]. [sent-93, score-0.301]
</p><p>40 In the ith iteration, the level method ﬁrst constructs a lower bound for f (x) by a cutting plane model, denoted by g i (x). [sent-96, score-0.709]
</p><p>41 The optimal solution, denoted i by xi , that minimizes the cutting plane model g i (x) is then computed. [sent-97, score-0.481]
</p><p>42 Next, a level set for the cutting plane model g i (x) is constructed, denoted by Li = {x ∈ G : ˆ i g i (x) ≤ λf + (1 − λ)f i } where λ ∈ (0, 1) is a tradeoff constant. [sent-99, score-0.644]
</p><p>43 Finally, a new solution xi+1 is computed by projecting xi onto the level set Li . [sent-100, score-0.287]
</p><p>44 It is important to note that the projection step, serving a similar purpose to the regularization term in SD, prevents the new solution xi+1 from being too far away from the old one xi . [sent-101, score-0.205]
</p><p>45 The cutting plane model at x0 is g 0 (x) = 9 − 6(x + 3). [sent-104, score-0.459]
</p><p>46 The level method alleviates this problem by projecting x0 = −3 to the level set L0 = {x : g 0 (x) ≤ 0. [sent-107, score-0.454]
</p><p>47 2  Extension of the Level Method to MKL  We now extend the level method, which was originally designed for optimizing non-smooth functions, to convex-concave optimization. [sent-114, score-0.248]
</p><p>48 First, since f (p, α) is convex in p and concave in α, according to van Neuman Lemma, for any optimal solution (p∗ , α∗ ) we have f (p, α∗ ) = max f (p, α) ≥ f (p∗ , α∗ ) ≥ f (p∗ , α) = min f (p, α). [sent-115, score-0.167]
</p><p>49 To apply the level method, we ﬁrst construct the cutting plane model. [sent-117, score-0.666]
</p><p>50 We construct a cutting plane model g i (p) as follows: g i (p) = max f (p, αj ). [sent-120, score-0.516]
</p><p>51 (5)  1≤j≤i  We have the following proposition for the cutting plane model g i (x) Proposition 1. [sent-121, score-0.459]
</p><p>52 p∈P α∈Q  p∈P  Second, since f (pj , αj ) = max f (pj , α), we have α∈Q  i  f = min f (pj , αj ) = 1≤j≤i  min  max f (p, α) ≥ min max f (p, α) = f (p∗ , α∗ ). [sent-136, score-0.198]
</p><p>53 The following corollary indicates that the gap ∆i can be used to measure the sub-optimality for solution pi and αi . [sent-143, score-0.271]
</p><p>54 i  In the third step, we construct the level set Li using the estimated bounds f and f i as follows: i  Li = {p ∈ P : g i (p) ≤ ℓi = λf + (1 − λ)f i },  (7)  where λ ∈ (0, 1) is a predeﬁned constant. [sent-152, score-0.229]
</p><p>55 The new solution, denoted by pi+1 , is computed as the projection of pi onto the level set Li , which is equivalent to solving the following optimization problem: pi+1 = arg min p  p − pi  2 2  : p ∈ P, f (p, αj ) ≤ ℓi , j = 1, . [sent-153, score-0.683]
</p><p>56 (8)  Although the projection is regarded as a quadratic programming problem, it can often be solved efﬁciently because its solution is likely to be the projection onto one of the hyperplanes of polyhedron Li . [sent-157, score-0.263]
</p><p>57 As we argue in the last subsection, by means of the projection, we on the one hand ensure pi+1 is not very far away from pi , and on the other hand ensure signiﬁcant progress is made in terms of g i (p) when the solution is updated from pi to pi+1 . [sent-160, score-0.452]
</p><p>58 Note that the projection step in the level method saves the effort of searching for the optimal step size in SD, which is computationally expensive as will be revealed later. [sent-161, score-0.371]
</p><p>59 We summarize the steps of the extended level method in Algorithm 2. [sent-162, score-0.288]
</p><p>60 2) 6: Compute the projection of pi onto the level set Li by solving the optimization problem in (8) 7: Update i = i + 1 8: until ∆i ≤ ε Finally, we discuss the convergence behavior of the level method. [sent-164, score-0.702]
</p><p>61 The following theorem shows the convergence rate of the level method when applied to multiple kernel learning. [sent-166, score-0.493]
</p><p>62 , | maxα∈Q f (p, α) − f (p∗ , α∗ )| ≤ ε, the maximum number of iterations N that the level method requires is bounded 2 1√ 2 max Λmax (Ki ). [sent-170, score-0.318]
</p><p>63 Theorem 3 tells us that the convergence rate of the level method is O(1/ε2 ). [sent-173, score-0.273]
</p><p>64 It is important to note that according to Information Based Complexity (IBC) theory, given a function family F(L) with a ﬁxed Lipschitz constant L, O(1/ε2 ) is almost the optimal convergence rate that can be achieved for any optimization method based on the black box ﬁrst order oracle. [sent-174, score-0.146]
</p><p>65 In other words, no matter which optimization method is used, there always exists an function f (·) ∈ F(L) such that the convergence rate is O(1/ε2 ) as long as the optimization method is based on a black box ﬁrst order oracle. [sent-175, score-0.225]
</p><p>66 1  Experimental Setup  We follow the settings in [10] to construct the base kernel matrices, i. [sent-179, score-0.246]
</p><p>67 0  Each base kernel matrix is normalized to unit trace. [sent-331, score-0.224]
</p><p>68 According to the above scheme of constructing base kernel matrices, we select a batch of UCI data sets, with the cardinality and dimension allowed by the memory limit of the PC, from the UCI repository for evaluation. [sent-334, score-0.224]
</p><p>69 , m max (α◦y)⊤ Ki (α◦y)−(α◦y)⊤ j=1 pj Kj (α◦y), and stop the algorithm when the criterion 1≤i≤m  is less than 0. [sent-341, score-0.142]
</p><p>70 01 for all experiments, since a larger λ accelerates the projection when the solution is close to the optimal one. [sent-346, score-0.154]
</p><p>71 The linear programming in the SILP method and the auxiliary subproblems in the level method are solved using a general optimization toolbox MOSEK (http://www. [sent-348, score-0.422]
</p><p>72 The toolbox for the level method can be downloaded from http://www. [sent-351, score-0.281]
</p><p>73 We also observe that the level method is the most efﬁcient among three methods in comparison. [sent-363, score-0.276]
</p><p>74 To obtain a better picture of the computational efﬁciency of the proposed level method, we compute the time-saving ratio, as shown in Table 2. [sent-364, score-0.185]
</p><p>75 In order to see more details of each optimization algorithm, we plot the logarithm values of the MKL objective function to base 10 against time in Figure 1. [sent-368, score-0.212]
</p><p>76 It is interesting to ﬁnd that the level method converges overwhelmingly faster than the other two methods. [sent-370, score-0.25]
</p><p>77 The efﬁciency of the level method arises from two aspects: (a) the cutting plane model utilizes the computational results of all iterations and therefore boosts the search efﬁciency, and (b) the projection to the level sets ensures the stability of the new solution. [sent-371, score-1.071]
</p><p>78 As an example, we found that for dataset “Iono”, although SD and the level method require similar numbers of iterations, SD calls the SVM solver 1231 times on average, while the level method only calls it 47 times. [sent-374, score-0.557]
</p><p>79 This instability leads to very slow convergence when the solution is close to the optimal one, as indicated by the long tail of SILP in Figure 1. [sent-376, score-0.132]
</p><p>80 The instability of SILP is further conﬁrmed by the examination of kernel weights, as shown below. [sent-377, score-0.193]
</p><p>81 , p), we plot the evolution curves of the ﬁve largest kernel weights for datasets “Iono”, “Breast”, and “Pima” in Figure 2. [sent-380, score-0.357]
</p><p>82 We observe that the values of p computed by the SILP method are the most unstable due to oscillation of the solutions to the cutting plane models. [sent-381, score-0.595]
</p><p>83 In contrast, for the proposed level method, the values of p change smoothly through iterations. [sent-383, score-0.203]
</p><p>84 We believe that the stability of the level method is mainly due to the accurate estimation of bounds as well as the regularization of the projection to the level sets. [sent-384, score-0.551]
</p><p>85 This observation also sheds light on why the level method can be more efﬁcient than the SILP and the SD methods. [sent-385, score-0.25]
</p><p>86 Table 2: Time-saving ratio of the level method over the SILP and the SD method Iono 78. [sent-386, score-0.315]
</p><p>87 2  0  10  20  30  40  50  60  70  time (s)  (c) Pima  Figure 1: Evolution of objective values over time (seconds) for datasets “Iono”, “Breast”, and “Pima”. [sent-430, score-0.153]
</p><p>88 Only parts of the evolution curves are plotted for SILP due to their long tails. [sent-432, score-0.125]
</p><p>89 5  Conclusion and Future Work  In this paper, we propose an extended level method to efﬁciently solve the multiple kernel learning problem. [sent-433, score-0.51]
</p><p>90 In particular, the level method overcomes the drawbacks of both the SILP method and the SD method for MKL. [sent-434, score-0.422]
</p><p>91 It is the employment of the projection step that guarantees ﬁnding an updated solution that, on the one hand, is close to the existing one, and one the other hand, signiﬁcantly reduces the objective function. [sent-436, score-0.225]
</p><p>92 Our experimental results have shown that the level method is able to greatly reduce the computational time of MKL over both the SD method and the SILP method. [sent-437, score-0.333]
</p><p>93 For future work, we plan to ﬁnd a scheme to adaptively set the value of λ in the level method and apply the level method to other tasks, such as one-class classiﬁcation, multi-class classiﬁcation, and regression. [sent-438, score-0.5]
</p><p>94 Consistency of the group Lasso and multiple kernel learning. [sent-443, score-0.203]
</p><p>95 Evolution of the kernel weight values in SD  Evolution of the kernel weight values in SILP  Evolution of the kernel weight values in Level method  0. [sent-445, score-0.719]
</p><p>96 1 0  100  200  iteration  300  400  0  500  (a) Iono/SD  10  (b) Iono/SILP  Evolution of the kernel weight values in SD  15  20  25  30  35  (c) Iono/Level  Evolution of the kernel weight values in SILP  Evolution of the kernel weight values in Level method  0. [sent-472, score-0.769]
</p><p>97 1 0  20  40  60  iteration  80  100  120  0  140  0  5  10  iteration  (d) Breast/SD  (e) Breast/SILP  Evolution of the kernel weight values in SD  15  20  iteration  (f) Breast/Level  Evolution of the kernel weight values in SILP  Evolution of the kernel weight values in Level method 1  0. [sent-499, score-0.869]
</p><p>98 1 0  20  40  60  80  100  0  0  iteration  (h) Pima/SILP  5  10  15  20  25  30  iteration  (i) Pima/Level  Figure 2: The evolution curves of the ﬁve largest kernel weights for datasets “Iono”, “Breast” and “Pima” computed by the three MKL algorithms [2] F. [sent-526, score-0.457]
</p><p>99 Multiple kernel learning, conic duality, and the SMO algorithm. [sent-534, score-0.168]
</p><p>100 Discriminant kernel and regularization parameter learning via semideﬁnite programming. [sent-617, score-0.192]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('silp', 0.603), ('sd', 0.396), ('mkl', 0.313), ('cutting', 0.272), ('plane', 0.187), ('level', 0.185), ('kernel', 0.168), ('pi', 0.158), ('iono', 0.15), ('evolution', 0.125), ('pima', 0.094), ('pj', 0.087), ('breast', 0.079), ('utilizes', 0.074), ('projection', 0.07), ('objective', 0.068), ('method', 0.065), ('solution', 0.062), ('svm', 0.058), ('base', 0.056), ('ciency', 0.051), ('subgradient', 0.05), ('iteration', 0.05), ('sonar', 0.044), ('programming', 0.04), ('extended', 0.038), ('li', 0.038), ('updates', 0.037), ('optimization', 0.036), ('multiple', 0.035), ('max', 0.035), ('kong', 0.034), ('nemirovski', 0.033), ('iterations', 0.033), ('weights', 0.033), ('stopping', 0.033), ('gap', 0.033), ('bundle', 0.032), ('hong', 0.032), ('weight', 0.032), ('toolbox', 0.031), ('min', 0.031), ('datasets', 0.031), ('uci', 0.031), ('gradients', 0.03), ('chal', 0.029), ('lemar', 0.029), ('saves', 0.029), ('wpbc', 0.029), ('kj', 0.029), ('kernels', 0.028), ('verify', 0.027), ('oscillation', 0.027), ('away', 0.027), ('ki', 0.026), ('observe', 0.026), ('instability', 0.025), ('king', 0.025), ('lyu', 0.025), ('wdbc', 0.025), ('updated', 0.025), ('regularization', 0.024), ('ef', 0.024), ('solving', 0.024), ('vote', 0.024), ('saddle', 0.024), ('sdp', 0.024), ('designed', 0.023), ('convergence', 0.023), ('construct', 0.022), ('bounds', 0.022), ('optimal', 0.022), ('overcomes', 0.022), ('far', 0.022), ('past', 0.022), ('onto', 0.021), ('originally', 0.021), ('lanckriet', 0.021), ('criterion', 0.02), ('drawbacks', 0.02), ('calls', 0.02), ('cristianini', 0.02), ('pc', 0.02), ('projecting', 0.019), ('cantly', 0.019), ('optimizing', 0.019), ('solve', 0.019), ('corollary', 0.018), ('regularize', 0.018), ('values', 0.018), ('time', 0.018), ('initialize', 0.017), ('repeat', 0.017), ('heart', 0.017), ('theorem', 0.017), ('semide', 0.017), ('solver', 0.017), ('convex', 0.017), ('cycles', 0.016), ('logarithm', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="20-tfidf-1" href="./nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</a></p>
<p>Author: Zenglin Xu, Rong Jin, Irwin King, Michael Lyu</p><p>Abstract: We consider the problem of multiple kernel learning (MKL), which can be formulated as a convex-concave problem. In the past, two efﬁcient methods, i.e., Semi-Inﬁnite Linear Programming (SILP) and Subgradient Descent (SD), have been proposed for large-scale multiple kernel learning. Despite their success, both methods have their own shortcomings: (a) the SD method utilizes the gradient of only the current solution, and (b) the SILP method does not regularize the approximate solution obtained from the cutting plane model. In this work, we extend the level method, which was originally designed for optimizing non-smooth objective functions, to convex-concave optimization, and apply it to multiple kernel learning. The extended level method overcomes the drawbacks of SILP and SD by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set. Empirical study with eight UCI datasets shows that the extended level method can signiﬁcantly improve efﬁciency by saving on average 91.9% of computational time over the SILP method and 70.3% over the SD method. 1</p><p>2 0.32411075 <a title="20-tfidf-2" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>Author: Shuiwang Ji, Liang Sun, Rong Jin, Jieping Ye</p><p>Abstract: We present a multi-label multiple kernel learning (MKL) formulation in which the data are embedded into a low-dimensional space directed by the instancelabel correlations encoded into a hypergraph. We formulate the problem in the kernel-induced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the MKL framework. The proposed learning formulation leads to a non-smooth min-max problem, which can be cast into a semi-inﬁnite linear program (SILP). We further propose an approximate formulation with a guaranteed error bound which involves an unconstrained convex optimization problem. In addition, we show that the objective function of the approximate formulation is differentiable with Lipschitz continuous gradient, and hence existing methods can be employed to compute the optimal solution efﬁciently. We apply the proposed formulation to the automated annotation of Drosophila gene expression pattern images, and promising results have been reported in comparison with representative algorithms.</p><p>3 0.13197185 <a title="20-tfidf-3" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>Author: Yen-yu Lin, Tyng-luh Liu, Chiou-shann Fuh</p><p>Abstract: In solving complex visual learning tasks, adopting multiple descriptors to more precisely characterize the data has been a feasible way for improving performance. These representations are typically high dimensional and assume diverse forms. Thus ﬁnding a way to transform them into a uniﬁed space of lower dimension generally facilitates the underlying tasks, such as object recognition or clustering. We describe an approach that incorporates multiple kernel learning with dimensionality reduction (MKL-DR). While the proposed framework is ﬂexible in simultaneously tackling data in various feature representations, the formulation itself is general in that it is established upon graph embedding. It follows that any dimensionality reduction techniques explainable by graph embedding can be generalized by our method to consider data in multiple feature representations.</p><p>4 0.10596494 <a title="20-tfidf-4" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>5 0.098560922 <a title="20-tfidf-5" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>Author: Chunhua Shen, Alan Welsh, Lei Wang</p><p>Abstract: In this work, we consider the problem of learning a positive semideﬁnite matrix. The critical issue is how to preserve positive semideﬁniteness during the course of learning. Our algorithm is mainly inspired by LPBoost [1] and the general greedy convex optimization framework of Zhang [2]. We demonstrate the essence of the algorithm, termed PSDBoost (positive semideﬁnite Boosting), by focusing on a few different applications in machine learning. The proposed PSDBoost algorithm extends traditional Boosting algorithms in that its parameter is a positive semideﬁnite matrix with trace being one instead of a classiﬁer. PSDBoost is based on the observation that any trace-one positive semideﬁnite matrix can be decomposed into linear convex combinations of trace-one rank-one matrices, which serve as base learners of PSDBoost. Numerical experiments are presented. 1</p><p>6 0.091012798 <a title="20-tfidf-6" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>7 0.062165227 <a title="20-tfidf-7" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>8 0.061904084 <a title="20-tfidf-8" href="./nips-2008-Extended_Grassmann_Kernels_for_Subspace-Based_Learning.html">80 nips-2008-Extended Grassmann Kernels for Subspace-Based Learning</a></p>
<p>9 0.061290216 <a title="20-tfidf-9" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>10 0.061181381 <a title="20-tfidf-10" href="./nips-2008-Hierarchical_Fisher_Kernels_for_Longitudinal_Data.html">97 nips-2008-Hierarchical Fisher Kernels for Longitudinal Data</a></p>
<p>11 0.058828559 <a title="20-tfidf-11" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>12 0.055397373 <a title="20-tfidf-12" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>13 0.055080615 <a title="20-tfidf-13" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>14 0.049928427 <a title="20-tfidf-14" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>15 0.048305728 <a title="20-tfidf-15" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>16 0.046610292 <a title="20-tfidf-16" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>17 0.045634035 <a title="20-tfidf-17" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>18 0.04533945 <a title="20-tfidf-18" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>19 0.044361159 <a title="20-tfidf-19" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>20 0.043350086 <a title="20-tfidf-20" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.154), (1, -0.048), (2, -0.064), (3, 0.062), (4, 0.059), (5, -0.008), (6, -0.017), (7, -0.054), (8, -0.02), (9, -0.037), (10, 0.219), (11, -0.069), (12, -0.0), (13, 0.066), (14, 0.174), (15, 0.016), (16, 0.058), (17, -0.158), (18, -0.074), (19, -0.06), (20, 0.046), (21, 0.176), (22, -0.022), (23, 0.015), (24, -0.084), (25, -0.022), (26, 0.092), (27, 0.043), (28, 0.043), (29, -0.155), (30, -0.079), (31, 0.073), (32, -0.188), (33, -0.142), (34, -0.07), (35, -0.106), (36, -0.042), (37, -0.09), (38, -0.041), (39, -0.078), (40, 0.01), (41, -0.173), (42, -0.007), (43, -0.024), (44, 0.03), (45, -0.118), (46, 0.012), (47, 0.059), (48, -0.094), (49, -0.012)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93886751 <a title="20-lsi-1" href="./nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</a></p>
<p>Author: Zenglin Xu, Rong Jin, Irwin King, Michael Lyu</p><p>Abstract: We consider the problem of multiple kernel learning (MKL), which can be formulated as a convex-concave problem. In the past, two efﬁcient methods, i.e., Semi-Inﬁnite Linear Programming (SILP) and Subgradient Descent (SD), have been proposed for large-scale multiple kernel learning. Despite their success, both methods have their own shortcomings: (a) the SD method utilizes the gradient of only the current solution, and (b) the SILP method does not regularize the approximate solution obtained from the cutting plane model. In this work, we extend the level method, which was originally designed for optimizing non-smooth objective functions, to convex-concave optimization, and apply it to multiple kernel learning. The extended level method overcomes the drawbacks of SILP and SD by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set. Empirical study with eight UCI datasets shows that the extended level method can signiﬁcantly improve efﬁciency by saving on average 91.9% of computational time over the SILP method and 70.3% over the SD method. 1</p><p>2 0.85423988 <a title="20-lsi-2" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>Author: Shuiwang Ji, Liang Sun, Rong Jin, Jieping Ye</p><p>Abstract: We present a multi-label multiple kernel learning (MKL) formulation in which the data are embedded into a low-dimensional space directed by the instancelabel correlations encoded into a hypergraph. We formulate the problem in the kernel-induced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the MKL framework. The proposed learning formulation leads to a non-smooth min-max problem, which can be cast into a semi-inﬁnite linear program (SILP). We further propose an approximate formulation with a guaranteed error bound which involves an unconstrained convex optimization problem. In addition, we show that the objective function of the approximate formulation is differentiable with Lipschitz continuous gradient, and hence existing methods can be employed to compute the optimal solution efﬁciently. We apply the proposed formulation to the automated annotation of Drosophila gene expression pattern images, and promising results have been reported in comparison with representative algorithms.</p><p>3 0.57899898 <a title="20-lsi-3" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>Author: Yen-yu Lin, Tyng-luh Liu, Chiou-shann Fuh</p><p>Abstract: In solving complex visual learning tasks, adopting multiple descriptors to more precisely characterize the data has been a feasible way for improving performance. These representations are typically high dimensional and assume diverse forms. Thus ﬁnding a way to transform them into a uniﬁed space of lower dimension generally facilitates the underlying tasks, such as object recognition or clustering. We describe an approach that incorporates multiple kernel learning with dimensionality reduction (MKL-DR). While the proposed framework is ﬂexible in simultaneously tackling data in various feature representations, the formulation itself is general in that it is established upon graph embedding. It follows that any dimensionality reduction techniques explainable by graph embedding can be generalized by our method to consider data in multiple feature representations.</p><p>4 0.50868875 <a title="20-lsi-4" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>5 0.48542166 <a title="20-lsi-5" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>Author: Chunhua Shen, Alan Welsh, Lei Wang</p><p>Abstract: In this work, we consider the problem of learning a positive semideﬁnite matrix. The critical issue is how to preserve positive semideﬁniteness during the course of learning. Our algorithm is mainly inspired by LPBoost [1] and the general greedy convex optimization framework of Zhang [2]. We demonstrate the essence of the algorithm, termed PSDBoost (positive semideﬁnite Boosting), by focusing on a few different applications in machine learning. The proposed PSDBoost algorithm extends traditional Boosting algorithms in that its parameter is a positive semideﬁnite matrix with trace being one instead of a classiﬁer. PSDBoost is based on the observation that any trace-one positive semideﬁnite matrix can be decomposed into linear convex combinations of trace-one rank-one matrices, which serve as base learners of PSDBoost. Numerical experiments are presented. 1</p><p>6 0.40813982 <a title="20-lsi-6" href="./nips-2008-Extended_Grassmann_Kernels_for_Subspace-Based_Learning.html">80 nips-2008-Extended Grassmann Kernels for Subspace-Based Learning</a></p>
<p>7 0.4021697 <a title="20-lsi-7" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>8 0.38435295 <a title="20-lsi-8" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>9 0.37992805 <a title="20-lsi-9" href="./nips-2008-Characteristic_Kernels_on_Groups_and_Semigroups.html">44 nips-2008-Characteristic Kernels on Groups and Semigroups</a></p>
<p>10 0.37282935 <a title="20-lsi-10" href="./nips-2008-Hierarchical_Fisher_Kernels_for_Longitudinal_Data.html">97 nips-2008-Hierarchical Fisher Kernels for Longitudinal Data</a></p>
<p>11 0.36892718 <a title="20-lsi-11" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>12 0.36782074 <a title="20-lsi-12" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>13 0.35320914 <a title="20-lsi-13" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>14 0.33169898 <a title="20-lsi-14" href="./nips-2008-A_mixture_model_for_the_evolution_of_gene_expression_in_non-homogeneous_datasets.html">9 nips-2008-A mixture model for the evolution of gene expression in non-homogeneous datasets</a></p>
<p>15 0.32099262 <a title="20-lsi-15" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>16 0.30947053 <a title="20-lsi-16" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<p>17 0.29987496 <a title="20-lsi-17" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>18 0.29303926 <a title="20-lsi-18" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>19 0.27709576 <a title="20-lsi-19" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>20 0.27484855 <a title="20-lsi-20" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.08), (7, 0.09), (12, 0.031), (15, 0.018), (19, 0.284), (28, 0.144), (57, 0.061), (59, 0.011), (63, 0.041), (71, 0.029), (77, 0.035), (82, 0.011), (83, 0.048), (86, 0.011)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8115235 <a title="20-lda-1" href="./nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</a></p>
<p>Author: Ingo Steinwart, Andreas Christmann</p><p>Abstract: In this paper lower and upper bounds for the number of support vectors are derived for support vector machines (SVMs) based on the -insensitive loss function. It turns out that these bounds are asymptotically tight under mild assumptions on the data generating distribution. Finally, we brieﬂy discuss a trade-off in between sparsity and accuracy if the SVM is used to estimate the conditional median. 1</p><p>same-paper 2 0.72701705 <a title="20-lda-2" href="./nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</a></p>
<p>Author: Zenglin Xu, Rong Jin, Irwin King, Michael Lyu</p><p>Abstract: We consider the problem of multiple kernel learning (MKL), which can be formulated as a convex-concave problem. In the past, two efﬁcient methods, i.e., Semi-Inﬁnite Linear Programming (SILP) and Subgradient Descent (SD), have been proposed for large-scale multiple kernel learning. Despite their success, both methods have their own shortcomings: (a) the SD method utilizes the gradient of only the current solution, and (b) the SILP method does not regularize the approximate solution obtained from the cutting plane model. In this work, we extend the level method, which was originally designed for optimizing non-smooth objective functions, to convex-concave optimization, and apply it to multiple kernel learning. The extended level method overcomes the drawbacks of SILP and SD by exploiting all the gradients computed in past iterations and by regularizing the solution via a projection to a level set. Empirical study with eight UCI datasets shows that the extended level method can signiﬁcantly improve efﬁciency by saving on average 91.9% of computational time over the SILP method and 70.3% over the SD method. 1</p><p>3 0.71668726 <a title="20-lda-3" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>Author: Steven J. Phillips, Miroslav Dudík</p><p>Abstract: We apply robust Bayesian decision theory to improve both generative and discriminative learners under bias in class proportions in labeled training data, when the true class proportions are unknown. For the generative case, we derive an entropybased weighting that maximizes expected log likelihood under the worst-case true class proportions. For the discriminative case, we derive a multinomial logistic model that minimizes worst-case conditional log loss. We apply our theory to the modeling of species geographic distributions from presence data, an extreme case of labeling bias since there is no absence data. On a benchmark dataset, we ﬁnd that entropy-based weighting offers an improvement over constant estimates of class proportions, consistently reducing log loss on unbiased test data. 1</p><p>4 0.59149384 <a title="20-lda-4" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>5 0.5811004 <a title="20-lda-5" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>Author: Shuiwang Ji, Liang Sun, Rong Jin, Jieping Ye</p><p>Abstract: We present a multi-label multiple kernel learning (MKL) formulation in which the data are embedded into a low-dimensional space directed by the instancelabel correlations encoded into a hypergraph. We formulate the problem in the kernel-induced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the MKL framework. The proposed learning formulation leads to a non-smooth min-max problem, which can be cast into a semi-inﬁnite linear program (SILP). We further propose an approximate formulation with a guaranteed error bound which involves an unconstrained convex optimization problem. In addition, we show that the objective function of the approximate formulation is differentiable with Lipschitz continuous gradient, and hence existing methods can be employed to compute the optimal solution efﬁciently. We apply the proposed formulation to the automated annotation of Drosophila gene expression pattern images, and promising results have been reported in comparison with representative algorithms.</p><p>6 0.58035946 <a title="20-lda-6" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>7 0.58012533 <a title="20-lda-7" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>8 0.57784629 <a title="20-lda-8" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>9 0.57781196 <a title="20-lda-9" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>10 0.57744038 <a title="20-lda-10" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>11 0.57396454 <a title="20-lda-11" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>12 0.57387412 <a title="20-lda-12" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>13 0.57321423 <a title="20-lda-13" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>14 0.57275951 <a title="20-lda-14" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>15 0.57223976 <a title="20-lda-15" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>16 0.57187361 <a title="20-lda-16" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>17 0.57154244 <a title="20-lda-17" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>18 0.57082599 <a title="20-lda-18" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>19 0.57082081 <a title="20-lda-19" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>20 0.56944001 <a title="20-lda-20" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
