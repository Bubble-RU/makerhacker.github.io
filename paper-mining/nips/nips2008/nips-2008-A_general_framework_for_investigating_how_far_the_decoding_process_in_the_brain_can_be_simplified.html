<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-8" href="#">nips2008-8</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</h1>
<br/><p>Source: <a title="nips-2008-8-pdf" href="http://papers.nips.cc/paper/3582-a-general-framework-for-investigating-how-far-the-decoding-process-in-the-brain-can-be-simplified.pdf">pdf</a></p><p>Author: Masafumi Oizumi, Toshiyuki Ishii, Kazuya Ishibashi, Toshihiko Hosoya, Masato Okada</p><p>Abstract: “How is information decoded in the brain?” is one of the most difﬁcult and important questions in neuroscience. Whether neural correlation is important or not in decoding neural activities is of special interest. We have developed a general framework for investigating how far the decoding process in the brain can be simpliﬁed. First, we hierarchically construct simpliﬁed probabilistic models of neural responses that ignore more than Kth-order correlations by using a maximum entropy principle. Then, we compute how much information is lost when information is decoded using the simpliﬁed models, i.e., “mismatched decoders”. We introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders. We applied our proposed framework to spike data for vertebrate retina. We used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies. We found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding. We also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies, pseudo correlations seem to carry a large portion of the information. 1</p><p>Reference: <a title="nips-2008-8-reference" href="../nips2008_reference/nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 A general framework for investigating how far the decoding process in the brain can be simpliﬁed  Masafumi Oizumi1 , Toshiyuki Ishii2 , Kazuya Ishibashi1 Toshihiko Hosoya2 , Masato Okada1,2 oizumi@mns. [sent-1, score-0.307]
</p><p>2 jp 1 University of Tokyo, Kashiwa-shi, Chiba, JAPAN 2 RIKEN Brain Science Institute, Wako-shi, Saitama, JAPAN  Abstract “How is information decoded in the brain? [sent-16, score-0.216]
</p><p>3 Whether neural correlation is important or not in decoding neural activities is of special interest. [sent-18, score-0.511]
</p><p>4 We have developed a general framework for investigating how far the decoding process in the brain can be simpliﬁed. [sent-19, score-0.307]
</p><p>5 First, we hierarchically construct simpliﬁed probabilistic models of neural responses that ignore more than Kth-order correlations by using a maximum entropy principle. [sent-20, score-0.305]
</p><p>6 Then, we compute how much information is lost when information is decoded using the simpliﬁed models, i. [sent-21, score-0.31]
</p><p>7 We introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders. [sent-24, score-0.706]
</p><p>8 We applied our proposed framework to spike data for vertebrate retina. [sent-25, score-0.145]
</p><p>9 We used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies. [sent-26, score-0.543]
</p><p>10 We found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding. [sent-27, score-0.867]
</p><p>11 We also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies, pseudo correlations seem to carry a large portion of the information. [sent-28, score-0.532]
</p><p>12 1  Introduction  An ultimate goal of neuroscience is to elucidate how information is encoded and decoded by neural activities. [sent-29, score-0.363]
</p><p>13 To investigate what information is encoded by neurons in certain area of the brain, the mutual information between stimuli and neural responses is often calculated. [sent-30, score-0.485]
</p><p>14 In the analysis of mutual information, it is implicitly assumed that encoded information is decoded by an optimal decoder, which exactly matches the encoder. [sent-31, score-0.407]
</p><p>15 In other words, the brain is assumed to have full knowledge of the encoding process. [sent-32, score-0.154]
</p><p>16 Generally, if the neural activities are correlated, the amount of data needed for the optimal decoding scales exponentially with the number of neurons. [sent-33, score-0.439]
</p><p>17 Since a large amount of data and many complex computations are needed for optimal decoding, the assumption of an optimal decoder in the brain is doubtful. [sent-34, score-0.543]
</p><p>18 The reason mutual information is widely used in neuroscience despite the doubtfulness of the optimal decoder is that we are completely ignorant of how information is decoded in the brain. [sent-35, score-0.84]
</p><p>19 Thus, we simply evaluate the maximal amount of information that can be extracted from neural activities by calculating the mutual information. [sent-36, score-0.508]
</p><p>20 To address this lack of knowledge, we can ask a different question: “How much information can be obtained by a decoder that has partial knowledge of the encoding process? [sent-37, score-0.604]
</p><p>21 ” [10, 14] We call this type of a decoder “simpliﬁed decoder” or a “mismatched decoder”. [sent-38, score-0.472]
</p><p>22 For example, an independent decoder is a simpliﬁed decoder; it takes only the marginal 1  distribution of the neural responses into consideration and ignores the correlations between neuronal activities. [sent-39, score-0.767]
</p><p>23 The independent decoder is of particular importance because several studies have shown that maximum likelihood estimation can be implemented by a biologically plausible network [2, 4]. [sent-40, score-0.458]
</p><p>24 If it is experimentally shown that a sufﬁciently large portion of information is obtained by the independent decoder, we can say that the brain may function in a manner similar to the independent decoder. [sent-41, score-0.265]
</p><p>25 computed the amount of information obtained by the independent decoder in pairs of retinal ganglion cells activities [10]. [sent-43, score-1.255]
</p><p>26 They showed that no pair of cells showed a loss of information greater than 11%. [sent-44, score-0.318]
</p><p>27 Because only pairs of cells were considered in their analysis, it has not been still elucidated whether correlations are not important in population activities. [sent-45, score-0.431]
</p><p>28 To elucidate whether correlations are important or not in population activities, we have developed a general framework for investigating the importance of correlation in decoding neural activities. [sent-46, score-0.595]
</p><p>29 When population activities are analyzed, we have to deal with not only second-order correlations but also higher-order correlations in general. [sent-47, score-0.547]
</p><p>30 Therefore, we need to hierarchically construct simpliﬁed decoders that account of up to Kth-order correlations, where K = 1, 2, . [sent-48, score-0.4]
</p><p>31 By computing how much information is obtained by the simpliﬁed decoders, we investigate how many orders of correlation should be taken into account to extract enough information. [sent-52, score-0.185]
</p><p>32 To compute the information obtained by the mismatched decoders, we introduce a information theoretically correct quantity derived by Merhav et al. [sent-53, score-0.762]
</p><p>33 Information for mismatched decoders previously proposed by Nirenberg and Latham is the lower bound on the correct information [5, 11]. [sent-55, score-0.983]
</p><p>34 Because this lower bound can be very loose and their proposed information can be negative when many cells are analyzed as is shown in the paper, we need to accurately evaluate the information obtained by mismatched decoders. [sent-56, score-0.926]
</p><p>35 In Section 2, we describe a way of computing the information that can be extracted from neural activities by mismatched decoders using the information derived by Merhav et al. [sent-58, score-1.206]
</p><p>36 Using analytical computation, we demonstrate how information for mismatched decoders previously proposed by Nirenberg and Latham differs from the correct information derived by Merhav et al. [sent-60, score-1.045]
</p><p>37 In Section 3, we apply our framework to spike data for ganglion cells in the salamander retina. [sent-62, score-0.539]
</p><p>38 We ﬁrst describe the method of hierarchically constructing simpliﬁed decoders by using the maximum entropy principle [12]. [sent-63, score-0.4]
</p><p>39 We then compute the information obtained with the simpliﬁed decoders. [sent-64, score-0.085]
</p><p>40 We ﬁnd that more than 90% of the information can be extracted from the population activities of ganglion cells even if all orders of correlations are ignored in decoding. [sent-65, score-0.998]
</p><p>41 We also describe the problem of previous studies [10, 12] in which the stationarity of stimuli is assumed for a duration that is too long. [sent-66, score-0.167]
</p><p>42 Using a toy model, we demonstrate that pseudo correlations seem to carry a large portion of the information because of the stationarity assumption. [sent-67, score-0.387]
</p><p>43 2  Information for mismatched decoders  Let us consider how much information about stimuli can be extracted from neural responses. [sent-68, score-1.014]
</p><p>44 We assume that we experimentally obtain the conditional probability distribution p(r|s) that neural responses r are evoked by stimulus s. [sent-69, score-0.244]
</p><p>45 We can say that the stimulus is encoded by neural response r, which obeys the distribution p(r|s). [sent-70, score-0.207]
</p><p>46 The maximal amount of information obtained with the optimal decoder can be evaluated by using the mutual information: I=−  drp(r) log2 p(r) +  dr  p(s)p(r|s) log2 p(r|s),  (1)  s  where p(r) = s p(r|s)p(s) and p(s) is the prior probability of stimuli. [sent-72, score-0.886]
</p><p>47 In the optimal decoder, the probability distribution q(r|s) that exactly matches the encoding model p(r|s) is used for decoding; that is, q(r|s) = p(r|s). [sent-73, score-0.102]
</p><p>48 We can also compute the maximal amount of information obtained by a decoder using a decoding model q(r|s) that does not match the encoding model p(r|s) by using an equation derived by Merhav et al. [sent-75, score-0.916]
</p><p>49 [8]: I ∗ (β) = −  p(s)q(r|s)β +  drp(r) log2 s  p(s)p(r|s) log2 q(r|s)β ,  dr  (2)  s  where β takes the value that maximizes I ∗ (β). [sent-76, score-0.174]
</p><p>50 We call a decoder using the mismatched decoding model a “mismatched decoder”. [sent-78, score-1.094]
</p><p>51 2  Figure 1: Comparison between correct information I ∗ derived by Merhav et al. [sent-79, score-0.194]
</p><p>52 A: Difference between I ∗ /I (solid line) and I N L /I (dotted line) in Gaussian model where correlations and derivatives of mean ﬁring rates are uniform. [sent-81, score-0.178]
</p><p>53 B: Difference between I1 /I (solid line) and I1 L /I (dotted line) when spike data in Figure 3A are used. [sent-84, score-0.118]
</p><p>54 For this spike data and other spike data analyzed, Nirenberg-Latham information provides a tight lower bound on the correct information, possibly because the number of cells is small. [sent-85, score-0.644]
</p><p>55 Previously, Nirenberg and Latham proposed that the information obtained by mismatched decoders can be evaluated by using [11] INL = −  drp(r) log2  p(s)q(r|s) +  dr  p(s)p(r|s) log2 q(r|s). [sent-86, score-1.028]
</p><p>56 s  (3)  s  We call their proposed information “Nirenberg-Latham information”. [sent-87, score-0.096]
</p><p>57 Thus, Nirenberg-Latham information does not give correct information; instead, it simply provides the lower bound on the correct information, I ∗ (β), which is the maximum value with respect to β [5, 8]. [sent-90, score-0.27]
</p><p>58 The lower bound provided by Nirenberg-Latham information can be very loose and the Nirenberg-Latham information can be negative when many cells are analyzed. [sent-91, score-0.427]
</p><p>59 Theoretical evaluation of information I, I ∗ , and I N L We consider the problem where mutual information is computed when stimulus s, which is a single variable, and slightly different stimulus s + ∆s are presented. [sent-92, score-0.414]
</p><p>60 Neural responses evoked by the stimuli are denoted by r, which is considered here to be the neuron ﬁring rate. [sent-94, score-0.197]
</p><p>61 When the difference between two stimuli is small, the conditional probability p(r|s + ∆s) can be expanded with respect 1 to ∆s as p(r|s+∆s) = p(r|s)+p′ (r|s)∆s+ 2 p′′ (r|s)(∆s)2 +. [sent-95, score-0.102]
</p><p>62 Using the expansion, to leading order of ∆s, we can write mutual information I as (p′ (r|s))2 ∆s2 dr I= , (4) 8 p(r|s) ′  2  (r|s) where dr pp(r|s) is the Fisher information. [sent-99, score-0.534]
</p><p>63 Thus, we can see that the mutual information is proportional to the Fisher information when ∆s is small. [sent-100, score-0.27]
</p><p>64 Similarly, the correct information I ∗ for the mismatched decoders and the Nirenberg-Latham information I N L can be written as  I∗ =  ∆s2 8  INL =  ∆s2 8  dr −  p′ (r|s)q ′ (r|s) q(r|s) drp(r|s)  2  dr  q ′ (r|s) q(r|s)  p(r|s)(q ′ (r|s))2 q(r|s)2  2  +2  dr  −1  p′ (r|s)q(r|s) q(r|s)  ,  (5) . [sent-101, score-1.513]
</p><p>65 (6)  Taking into consideration the proportionality of the mutual information to the Fisher information, we ′  ′  can interpret that dr p (r|s)q (r|s) q(r|s) quantity for mismatched decoders. [sent-102, score-0.877]
</p><p>66 2  ′  dr p(r|s)(q (r|s)) q(r|s)2  3  2  −1  in Eq. [sent-103, score-0.174]
</p><p>67 We consider an independent decoding model q(r|s) that ignores correlations: q(r|s) =  1 1 exp − (r − f (s))T C−1 (r − f (s)) , D ZD 2  (8)  where CD is the diagonal covariance matrix obtained by setting the off-diagonal elements of C to 0. [sent-105, score-0.289]
</p><p>68 4-5, I, I ∗ , and I N L can be written as ∆s2 ′T f (s)C−1 f ′ (s), 8 ∆s2 (f ′T (s)C−1 f ′ (s))2 D I∗ = , 8 f ′T (s)C−1 CC−1 f ′ (s) D D I=  (9) (10)  ∆s2 −f ′T (s)C−1 CC−1 f ′ (s) + 2f ′T (s)C−1 f ′ (s) . [sent-107, score-0.022]
</p><p>69 (11) D D D 8 The correct information obtained by the independent decoder for the Gaussian model (Eq. [sent-108, score-0.619]
</p><p>70 10) is inversely proportional to the decoding error of s when the independent decoder is applied, which was computed from the generalized Cram´ r Rao bound by Wu et al. [sent-109, score-0.759]
</p><p>71 e INL =  As a simple example, we consider a uniform correlation model [1, 14] in which covariance matrix C is given by Cij = σ 2 [δij + c(1 − δij )] and assume that the derivatives of the ﬁring rates are uniform: that is fi′ = f ′ . [sent-111, score-0.11]
</p><p>72 We can see that I ∗ is equal to I, which means that information is not lost even if correlation is ignored in the decoding process. [sent-113, score-0.394]
</p><p>73 Figure 1A shows I N L /I and I ∗ /I when the degree of correlation c is 0. [sent-114, score-0.064]
</p><p>74 As shown in Figure 1A, the difference between the correct information I ∗ and Nirenberg-Latham information I N L is very large when the number of cells N is large. [sent-116, score-0.414]
</p><p>75 Analysis showed that using Nirenberg-Latham information c I N L as a lower bound on the correct information I ∗ can lead to wrong conclusions, especially when many cells are analyzed. [sent-118, score-0.491]
</p><p>76 1  Analysis of information in population activities of ganglion cells Methods  We analyzed the data obtained when N = 7 retinal ganglion cells were simultaneously recorded using a multielectrode array. [sent-120, score-1.294]
</p><p>77 The stimulus was a natural movie, which was 200 s long and repeated 45 times. [sent-121, score-0.106]
</p><p>78 We divided the movie into many short natural movies and considered them as stimuli over which information contained in neural activities is computed. [sent-122, score-0.602]
</p><p>79 For instance, when it was divided into 10-s-long natural movies, there were 20 stimuli. [sent-123, score-0.044]
</p><p>80 Figure 2A shows the response of the seven retinal ganglion cells to natural movies from 0 to 10 s in length. [sent-124, score-0.665]
</p><p>81 To apply information theoretic techniques, we ﬁrst discretized the time into small time bins ∆τ and indicated whether a spike was emitted or not in each time bin with a binary variable: σi = 1 means that the cell i spiked and σi = 0 means that it did not spike. [sent-125, score-0.226]
</p><p>82 We set the length of the time, ∆τ , to 5 ms so that it was short enough to avoid two spikes falling into the same bin. [sent-126, score-0.022]
</p><p>83 In this way, the spike pattern of ganglion cells was transformed into an N -letter binary word, σ = {σ1 , σ2 , . [sent-127, score-0.563]
</p><p>84 Then, we determined the 4  Figure 2: A: Raster plot of seven retinal ganglion cells responding to a natural movie. [sent-131, score-0.575]
</p><p>85 frequency with which a particular spike pattern, σ, was observed during each stimulus and estimated the conditional probability distribution pdata (σ|s) from experimental data. [sent-133, score-0.201]
</p><p>86 Using these conditional probabilities, we evaluated the information contained in N -letter binary words σ. [sent-134, score-0.117]
</p><p>87 Generally, the joint probability of N binary variables can be written as [9]   1 pN (σ) = exp  θi σi + θij σi σj + · · · + θ12. [sent-135, score-0.046]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('decoder', 0.438), ('mismatched', 0.431), ('decoders', 0.338), ('cells', 0.214), ('ganglion', 0.207), ('decoding', 0.191), ('activities', 0.176), ('dr', 0.174), ('correlations', 0.154), ('decoded', 0.154), ('merhav', 0.154), ('mutual', 0.124), ('drp', 0.123), ('inl', 0.123), ('spike', 0.118), ('movies', 0.109), ('nirenberg', 0.108), ('stimuli', 0.102), ('simpli', 0.092), ('retinal', 0.083), ('stimulus', 0.083), ('encoding', 0.081), ('correct', 0.076), ('latham', 0.074), ('brain', 0.073), ('stationarity', 0.065), ('correlation', 0.064), ('population', 0.063), ('hierarchically', 0.062), ('information', 0.062), ('fisher', 0.059), ('ring', 0.052), ('responses', 0.049), ('evoked', 0.046), ('encoded', 0.046), ('analyzed', 0.045), ('ignored', 0.045), ('investigating', 0.043), ('portion', 0.041), ('extracted', 0.041), ('neural', 0.04), ('cc', 0.04), ('elucidate', 0.04), ('japan', 0.04), ('movie', 0.038), ('obeys', 0.038), ('orders', 0.036), ('pseudo', 0.036), ('call', 0.034), ('bound', 0.033), ('consideration', 0.033), ('ignores', 0.033), ('loose', 0.033), ('maximal', 0.033), ('lost', 0.032), ('amount', 0.032), ('contained', 0.031), ('et', 0.03), ('dotted', 0.03), ('carry', 0.029), ('seven', 0.029), ('ij', 0.027), ('chiba', 0.027), ('masato', 0.027), ('okada', 0.027), ('proportionality', 0.027), ('riken', 0.027), ('saitama', 0.027), ('vertebrate', 0.027), ('theoretically', 0.026), ('quantity', 0.026), ('derived', 0.026), ('experimentally', 0.026), ('inversely', 0.025), ('zd', 0.025), ('binary', 0.024), ('derivatives', 0.024), ('obtained', 0.023), ('natural', 0.023), ('raster', 0.023), ('lower', 0.023), ('covariance', 0.022), ('written', 0.022), ('emitted', 0.022), ('falling', 0.022), ('solid', 0.022), ('proportional', 0.022), ('divided', 0.021), ('showed', 0.021), ('cij', 0.021), ('tokyo', 0.021), ('ultimate', 0.021), ('matches', 0.021), ('independent', 0.02), ('pp', 0.02), ('differentiation', 0.02), ('durations', 0.02), ('previously', 0.02), ('line', 0.02), ('responding', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="8-tfidf-1" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>Author: Masafumi Oizumi, Toshiyuki Ishii, Kazuya Ishibashi, Toshihiko Hosoya, Masato Okada</p><p>Abstract: “How is information decoded in the brain?” is one of the most difﬁcult and important questions in neuroscience. Whether neural correlation is important or not in decoding neural activities is of special interest. We have developed a general framework for investigating how far the decoding process in the brain can be simpliﬁed. First, we hierarchically construct simpliﬁed probabilistic models of neural responses that ignore more than Kth-order correlations by using a maximum entropy principle. Then, we compute how much information is lost when information is decoded using the simpliﬁed models, i.e., “mismatched decoders”. We introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders. We applied our proposed framework to spike data for vertebrate retina. We used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies. We found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding. We also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies, pseudo correlations seem to carry a large portion of the information. 1</p><p>2 0.13345701 <a title="8-tfidf-2" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Matthias Munk, Klaus Obermayer</p><p>Abstract: Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three ﬁndings: (1) copula-based distributions provide signiﬁcantly better ﬁts than discretized multivariate normal distributions; (2) negative binomial margins ﬁt the data signiﬁcantly better than Poisson margins; and (3) the dependence structure carries 12% of the mutual information between stimuli and responses. 1</p><p>3 0.13337655 <a title="8-tfidf-3" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>Author: Adam Ponzi, Jeff Wickens</p><p>Abstract: Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2]. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with ﬁxed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and ﬁring state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between ﬁring and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition[3], shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points. 1</p><p>4 0.12281749 <a title="8-tfidf-4" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>Author: Dominik Endres, Peter Foldiak</p><p>Abstract: We propose a novel application of Formal Concept Analysis (FCA) to neural decoding: instead of just trying to ﬁgure out which stimulus was presented, we demonstrate how to explore the semantic relationships in the neural representation of large sets of stimuli. FCA provides a way of displaying and interpreting such relationships via concept lattices. We explore the effects of neural code sparsity on the lattice. We then analyze neurophysiological data from high-level visual cortical area STSa, using an exact Bayesian approach to construct the formal context needed by FCA. Prominent features of the resulting concept lattices are discussed, including hierarchical face representation and indications for a product-of-experts code in real neurons. 1</p><p>5 0.11902721 <a title="8-tfidf-5" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>6 0.10244866 <a title="8-tfidf-6" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>7 0.068335749 <a title="8-tfidf-7" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>8 0.06802576 <a title="8-tfidf-8" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>9 0.067309462 <a title="8-tfidf-9" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>10 0.065688945 <a title="8-tfidf-10" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>11 0.055436134 <a title="8-tfidf-11" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>12 0.053880427 <a title="8-tfidf-12" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>13 0.052482281 <a title="8-tfidf-13" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>14 0.047989193 <a title="8-tfidf-14" href="./nips-2008-Nonparametric_sparse_hierarchical_models_describe_V1_fMRI_responses_to_natural_images.html">156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</a></p>
<p>15 0.047918271 <a title="8-tfidf-15" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>16 0.047416281 <a title="8-tfidf-16" href="./nips-2008-Dependence_of_Orientation_Tuning_on_Recurrent_Excitation_and_Inhibition_in_a_Network_Model_of_V1.html">58 nips-2008-Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1</a></p>
<p>17 0.044960979 <a title="8-tfidf-17" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>18 0.043697331 <a title="8-tfidf-18" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>19 0.040319853 <a title="8-tfidf-19" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>20 0.040294968 <a title="8-tfidf-20" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.104), (1, 0.043), (2, 0.165), (3, 0.151), (4, -0.115), (5, 0.012), (6, 0.016), (7, -0.01), (8, -0.005), (9, 0.019), (10, -0.002), (11, -0.009), (12, -0.036), (13, 0.009), (14, -0.022), (15, 0.052), (16, 0.079), (17, 0.021), (18, 0.022), (19, -0.063), (20, -0.057), (21, 0.044), (22, 0.075), (23, -0.088), (24, 0.001), (25, -0.057), (26, 0.045), (27, 0.028), (28, 0.076), (29, -0.043), (30, -0.035), (31, -0.038), (32, 0.057), (33, 0.002), (34, 0.058), (35, 0.005), (36, -0.102), (37, 0.086), (38, -0.146), (39, 0.03), (40, -0.013), (41, 0.059), (42, -0.019), (43, -0.024), (44, -0.045), (45, -0.001), (46, -0.14), (47, -0.018), (48, -0.148), (49, 0.099)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95937449 <a title="8-lsi-1" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>Author: Masafumi Oizumi, Toshiyuki Ishii, Kazuya Ishibashi, Toshihiko Hosoya, Masato Okada</p><p>Abstract: “How is information decoded in the brain?” is one of the most difﬁcult and important questions in neuroscience. Whether neural correlation is important or not in decoding neural activities is of special interest. We have developed a general framework for investigating how far the decoding process in the brain can be simpliﬁed. First, we hierarchically construct simpliﬁed probabilistic models of neural responses that ignore more than Kth-order correlations by using a maximum entropy principle. Then, we compute how much information is lost when information is decoded using the simpliﬁed models, i.e., “mismatched decoders”. We introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders. We applied our proposed framework to spike data for vertebrate retina. We used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies. We found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding. We also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies, pseudo correlations seem to carry a large portion of the information. 1</p><p>2 0.76655692 <a title="8-lsi-2" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>Author: Dominik Endres, Peter Foldiak</p><p>Abstract: We propose a novel application of Formal Concept Analysis (FCA) to neural decoding: instead of just trying to ﬁgure out which stimulus was presented, we demonstrate how to explore the semantic relationships in the neural representation of large sets of stimuli. FCA provides a way of displaying and interpreting such relationships via concept lattices. We explore the effects of neural code sparsity on the lattice. We then analyze neurophysiological data from high-level visual cortical area STSa, using an exact Bayesian approach to construct the formal context needed by FCA. Prominent features of the resulting concept lattices are discussed, including hierarchical face representation and indications for a product-of-experts code in real neurons. 1</p><p>3 0.61211604 <a title="8-lsi-3" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>Author: Adam Ponzi, Jeff Wickens</p><p>Abstract: Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2]. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with ﬁxed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and ﬁring state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between ﬁring and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition[3], shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points. 1</p><p>4 0.50817311 <a title="8-lsi-4" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>5 0.47542366 <a title="8-lsi-5" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>Author: K. Wong, Si Wu, Chi Fung</p><p>Abstract: Continuous attractor neural networks (CANNs) are emerging as promising models for describing the encoding of continuous stimuli in neural systems. Due to the translational invariance of their neuronal interactions, CANNs can hold a continuous family of neutrally stable states. In this study, we systematically explore how neutral stability of a CANN facilitates its tracking performance, a capacity believed to have wide applications in brain functions. We develop a perturbative approach that utilizes the dominant movement of the network stationary states in the state space. We quantify the distortions of the bump shape during tracking, and study their effects on the tracking performance. Results are obtained on the maximum speed for a moving stimulus to be trackable, and the reaction time to catch up an abrupt change in stimulus. 1</p><p>6 0.46421888 <a title="8-lsi-6" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>7 0.4251402 <a title="8-lsi-7" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>8 0.42421168 <a title="8-lsi-8" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>9 0.40716767 <a title="8-lsi-9" href="./nips-2008-Nonparametric_sparse_hierarchical_models_describe_V1_fMRI_responses_to_natural_images.html">156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</a></p>
<p>10 0.39751178 <a title="8-lsi-10" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>11 0.39670581 <a title="8-lsi-11" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>12 0.38379323 <a title="8-lsi-12" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>13 0.38086888 <a title="8-lsi-13" href="./nips-2008-Dependence_of_Orientation_Tuning_on_Recurrent_Excitation_and_Inhibition_in_a_Network_Model_of_V1.html">58 nips-2008-Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1</a></p>
<p>14 0.38066208 <a title="8-lsi-14" href="./nips-2008-Estimation_of_Information_Theoretic_Measures_for_Continuous_Random_Variables.html">76 nips-2008-Estimation of Information Theoretic Measures for Continuous Random Variables</a></p>
<p>15 0.33893099 <a title="8-lsi-15" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>16 0.33667037 <a title="8-lsi-16" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>17 0.32858136 <a title="8-lsi-17" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>18 0.30236161 <a title="8-lsi-18" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<p>19 0.29936558 <a title="8-lsi-19" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>20 0.29455984 <a title="8-lsi-20" href="./nips-2008-A_computational_model_of_hippocampal_function_in_trace_conditioning.html">7 nips-2008-A computational model of hippocampal function in trace conditioning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.108), (7, 0.114), (12, 0.022), (15, 0.012), (18, 0.013), (28, 0.148), (57, 0.021), (59, 0.016), (63, 0.012), (71, 0.048), (77, 0.026), (82, 0.317), (83, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7542879 <a title="8-lda-1" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>Author: Masafumi Oizumi, Toshiyuki Ishii, Kazuya Ishibashi, Toshihiko Hosoya, Masato Okada</p><p>Abstract: “How is information decoded in the brain?” is one of the most difﬁcult and important questions in neuroscience. Whether neural correlation is important or not in decoding neural activities is of special interest. We have developed a general framework for investigating how far the decoding process in the brain can be simpliﬁed. First, we hierarchically construct simpliﬁed probabilistic models of neural responses that ignore more than Kth-order correlations by using a maximum entropy principle. Then, we compute how much information is lost when information is decoded using the simpliﬁed models, i.e., “mismatched decoders”. We introduce an information theoretically correct quantity for evaluating the information obtained by mismatched decoders. We applied our proposed framework to spike data for vertebrate retina. We used 100-ms natural movies as stimuli and computed the information contained in neural activities about these movies. We found that the information loss is negligibly small in population activities of ganglion cells even if all orders of correlation are ignored in decoding. We also found that if we assume stationarity for long durations in the information analysis of dynamically changing stimuli like natural movies, pseudo correlations seem to carry a large portion of the information. 1</p><p>2 0.70055997 <a title="8-lda-2" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>Author: Shuiwang Ji, Liang Sun, Rong Jin, Jieping Ye</p><p>Abstract: We present a multi-label multiple kernel learning (MKL) formulation in which the data are embedded into a low-dimensional space directed by the instancelabel correlations encoded into a hypergraph. We formulate the problem in the kernel-induced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the MKL framework. The proposed learning formulation leads to a non-smooth min-max problem, which can be cast into a semi-inﬁnite linear program (SILP). We further propose an approximate formulation with a guaranteed error bound which involves an unconstrained convex optimization problem. In addition, we show that the objective function of the approximate formulation is differentiable with Lipschitz continuous gradient, and hence existing methods can be employed to compute the optimal solution efﬁciently. We apply the proposed formulation to the automated annotation of Drosophila gene expression pattern images, and promising results have been reported in comparison with representative algorithms.</p><p>3 0.55850303 <a title="8-lda-3" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>4 0.55453384 <a title="8-lda-4" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>5 0.55119491 <a title="8-lda-5" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>Author: Karthik Sridharan, Shai Shalev-shwartz, Nathan Srebro</p><p>Abstract: We study convergence properties of empirical minimization of a stochastic strongly convex objective, where the stochastic component is linear. We show that the value attained by the empirical minimizer converges to the optimal value with rate 1/n. The result applies, in particular, to the SVM objective. Thus, we obtain a rate of 1/n on the convergence of the SVM objective (with ﬁxed regularization parameter) to its inﬁnite data limit. We demonstrate how this is essential for obtaining certain type of oracle inequalities for SVMs. The results extend also to approximate minimization as well as to strong convexity with respect to an arbitrary norm, and so also to objectives regularized using other p norms. 1</p><p>6 0.55100906 <a title="8-lda-6" href="./nips-2008-Posterior_Consistency_of_the_Silverman_g-prior_in_Bayesian_Model_Choice.html">182 nips-2008-Posterior Consistency of the Silverman g-prior in Bayesian Model Choice</a></p>
<p>7 0.54961622 <a title="8-lda-7" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>8 0.54907131 <a title="8-lda-8" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>9 0.54841125 <a title="8-lda-9" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>10 0.54541594 <a title="8-lda-10" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>11 0.54495859 <a title="8-lda-11" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>12 0.54386663 <a title="8-lda-12" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>13 0.54376984 <a title="8-lda-13" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>14 0.5435645 <a title="8-lda-14" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>15 0.54336095 <a title="8-lda-15" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>16 0.54248202 <a title="8-lda-16" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>17 0.542081 <a title="8-lda-17" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>18 0.54198295 <a title="8-lda-18" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>19 0.54160446 <a title="8-lda-19" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>20 0.54129577 <a title="8-lda-20" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
