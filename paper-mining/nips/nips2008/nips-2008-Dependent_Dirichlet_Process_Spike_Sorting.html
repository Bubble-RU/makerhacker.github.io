<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>59 nips-2008-Dependent Dirichlet Process Spike Sorting</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-59" href="#">nips2008-59</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>59 nips-2008-Dependent Dirichlet Process Spike Sorting</h1>
<br/><p>Source: <a title="nips-2008-59-pdf" href="http://papers.nips.cc/paper/3606-dependent-dirichlet-process-spike-sorting.pdf">pdf</a></p><p>Author: Jan Gasthaus, Frank Wood, Dilan Gorur, Yee W. Teh</p><p>Abstract: In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle “appearance” and “disappearance” of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of inﬁnite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known. 1</p><p>Reference: <a title="nips-2008-59-reference" href="../nips2008_reference/nips-2008-Dependent_Dirichlet_Process_Spike_Sorting_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle “appearance” and “disappearance” of neurons. [sent-5, score-1.402]
</p><p>2 Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of inﬁnite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. [sent-6, score-0.742]
</p><p>3 We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known. [sent-7, score-0.647]
</p><p>4 1  Introduction  Spike sorting (see [1] and [2] for review and methodological background) is the name given to the problem of grouping action potentials by source neuron. [sent-8, score-0.563]
</p><p>5 In each of these, instead of pursuing the optimal sorting, multiple sortings of the spikes are produced (in fact what each model produces is a posterior distribution over spike trains). [sent-14, score-0.557]
</p><p>6 Neural data analyses may then be averaged over the resulting spike train distribution to account for uncertainties that may have arisen at various points in the spike sorting process and would not have been explicitly accounted for otherwise. [sent-15, score-1.165]
</p><p>7 Our work builds on this new Bayesian approach to spike sorting; going beyond them in the way steps ﬁve and six are accomplished. [sent-16, score-0.413]
</p><p>8 Speciﬁcally we apply the generalized Polya urn dependent Dirichlet process mixture model (GPUDPM) [7, 8] to the problem of spike sorting and show how it allows us to model waveform drift and account for neuron appearance and disappearance. [sent-17, score-1.318]
</p><p>9 By introducing a time dependent likelihood into the model we are also able to eliminate refractory period violations. [sent-18, score-0.279]
</p><p>10 1  The need for a spike sorting approach with these features arises from several domains. [sent-19, score-0.729]
</p><p>11 Waveform non-stationarities either due to changes in the recording environment (e. [sent-20, score-0.144]
</p><p>12 movement of the electrode) or due to changes in the ﬁring activity of the neuron itself (e. [sent-22, score-0.149]
</p><p>13 burstiness) cause almost all current spike sorting approaches to fail. [sent-24, score-0.729]
</p><p>14 This is because most pool waveforms over time, discarding the time at which the action potentials were observed. [sent-25, score-0.467]
</p><p>15 A notable exception to this is the spike sorting approach of [9], in which waveforms were pooled and clustered in short ﬁxed time intervals. [sent-26, score-0.949]
</p><p>16 Multiple Gaussian mixture models are then ﬁt to the waveforms in each interval and then are pruned and smoothed until a single coherent sequence of mixture models is left that describes the entire time course of the data. [sent-27, score-0.312]
</p><p>17 A recent study by [10] puts forward a compelling case for online spike sorting algorithms that can handle waveform non-stationarity, as well as sudden jumps in waveform shape (e. [sent-30, score-1.221]
</p><p>18 abrupt electrode movements due to high acceleration events), and appearance and disappearance of neurons from the recording over time. [sent-32, score-0.318]
</p><p>19 This paper introduces a chronical recording paradigm in which a chronically implanted recording device is mated with appropriate storage such that very long term recordings can be made. [sent-33, score-0.336]
</p><p>20 Unfortunately as the animal being recorded from is allowed its full range of natural movements, accelerations may cause the signal characteristics of the recording to vary dramatically over short time intervals. [sent-34, score-0.214]
</p><p>21 As such data theoretically can be recorded forever without stopping, forward-backward spike sorting algorithms such as that in [9] are ruled out. [sent-35, score-0.786]
</p><p>22 2  Review  Our model is based on the generalized Polya urn Dirichlet process mixture model (GPUDPM) described in [7, 8]. [sent-37, score-0.121]
</p><p>23 The GPUDPM is a time dependent Dirichlet process (DDP) mixture model formulated in the Chinese restaurant process (CRP) sampling representation of a Dirichlet process mixture model (DPM). [sent-38, score-0.247]
</p><p>24 In this representation we track the distinct φk drawn from G0 for each cluster, and use the Chinese restaurant process to sample the conditional distributions of the indicator variables ci mk P (ci = k|c1 , . [sent-51, score-0.138]
</p><p>25 , T , all tied together through a particular way of sharing the component parameters φt and table occupancy k counts mt between adjacent time steps (here t indexes the parameters and cluster sizes of the T k DPMs). [sent-61, score-0.263]
</p><p>26 Dependence among the mt is introduced by perturbing the number of customers sitting at each table k when moving forward through time. [sent-62, score-0.273]
</p><p>27 , mt t ) the vector containing the 1 K 2  number of customers sitting at each table at time t before a “deletion” step, where K t is the number of non-empty tables at time t. [sent-66, score-0.351]
</p><p>28 Then the perturbation of the class counts from one step to the next is governed by the process mt − ξ t mt − ζ t  mt+1 |mt , ρ ∼  with probability γ with probability 1 − γ  t t where ξk ∼ Binomial(mt , 1 − ρ) and ζj = mt for j = j k t  Kt k=1  t and ζj = 0 for j =  (3) where  ∼  mt ). [sent-68, score-0.575]
</p><p>29 k  Before seating the customers arriving at time step t + 1, the number of Discrete(m / customers sitting at each table is initialized to mt+1 . [sent-69, score-0.304]
</p><p>30 This perturbation process can either remove some number of customers from a table or effectively delete a table altogether. [sent-70, score-0.139]
</p><p>31 This deletion procedure accounts for the ability of the GPUDPM to model births and deaths of clusters. [sent-71, score-0.115]
</p><p>32 This drift is modeled by tying together the component parameters φt through a transition kernel P(φt |φt−1 ) from which the class k k k parameter at time t is sampled given the class parameter at time t − 1. [sent-73, score-0.194]
</p><p>33 k k  3  Model  In order to apply the GPUDPM model to spike sorting problems one ﬁrst has to make a number of modeling assumptions. [sent-78, score-0.729]
</p><p>34 In the following we describe modeling choices we made for the spike sorting task, as well as how the continuous spike occurrence times can be incorporated into the model to allow for correct treatment of neuron behaviour during the absolute refractory period. [sent-80, score-1.502]
</p><p>35 Let {xt }T be the the set of action potential waveforms extracted from an extracellular recording t=1 (referred to as “spikes” in the following), and let τ 1 , . [sent-81, score-0.665]
</p><p>36 , τ T be the time stamps (in ms) associated with these spikes in ascending order (i. [sent-84, score-0.161]
</p><p>37 , T corresponding to the time steps in the GPUDPM model and the actual spike times τ t at which the spike xt occurs in the recording. [sent-90, score-0.865]
</p><p>38 We assume that only one spike occurs per time step t, i. [sent-91, score-0.452]
</p><p>39 we set N = 1 in the model above and identify ct = (ct ) = ct . [sent-93, score-0.118]
</p><p>40 1 It is well known that the distribution of action potential waveforms originating from a single neuron in a PCA feature space is well approximated by a Normal distribution [1]. [sent-94, score-0.54]
</p><p>41 While correlations between the components can be observed in neural recordings, they can at least partially be attributed to temporal waveform variation. [sent-111, score-0.234]
</p><p>42 To account for the fact that neurons have an absolute refractory period following each action potential during which no further action potential can occur, we extend the GPUDPM by conditioning the model 3  on the spike occurrence times τ1 , . [sent-112, score-1.17]
</p><p>43 , τT and modifying the conditional probability of assigning a spike to a cluster given the other cluster labels and the spike occurrence times τ1 , . [sent-115, score-1.013]
</p><p>44 , τt in the following way:  if τ t − τk ≤ rabs ˆt 0 t 1:t t t P(ct = k|m , c1:t−1 , τ , α) ∝ mk if τ − τk > rabs and k ∈ {1, . [sent-118, score-0.201]
</p><p>45 , Kt−1 } (6) ˆt  t α ifτ − τk > rabs and k = Kt−1 + 1 ˆt where τk is the spike time of the last spike assigned to cluster k before time step t, i. [sent-121, score-1.054]
</p><p>46 Essentially, the conditional probability of assigning the spike at time t to cluster k is zero if the difference of the occurrence time of this spike and the occurrence time of the last spike associated with cluster k is smaller than the refractory period rabs . [sent-124, score-1.931]
</p><p>47 If the time difference is larger than rabs then the usual CRP conditional probabilities are used. [sent-125, score-0.126]
</p><p>48 In terms of the Chinese restaurant metaphor, this setup corresponds to a restaurant in which seating a customer at a table removes that table as an option for new customers for some period of time. [sent-126, score-0.332]
</p><p>49 The transition kernel P(φt |φt−1 ) speciﬁes how the action potential waveshape can vary over time. [sent-131, score-0.34]
</p><p>50 To k k meet the technical requirements of the GPUDPM and because its waveform drift modeling semantics are reasonable we use the update rule of the Metropolis algorithm [11] as the transition kernel P(φt |φt−1 ), i. [sent-132, score-0.35]
</p><p>51 This choice of P (φt |φt−1 ) ensures that G0 is the invariant distribution of the transition k k k kernel, while at the same time allowing us to control the amount of correlation between time steps through σ. [sent-136, score-0.115]
</p><p>52 A transition kernel of this form allows the distribution of the action potential waveforms to vary slowly (if σ is chosen small) from one time step to the next both in mean waveform shape as well as in variance. [sent-137, score-0.714]
</p><p>53 1  Methodology  Experiments were performed on a subset of the publicly available1 data set described in [12, 13], which consists of simultaneous intracellular and extracellular recordings of cells in the hippocampus of anesthetized rats. [sent-141, score-0.454]
</p><p>54 Recordings from an extracellular tetrode and an intracellular electrode were made simultaneously, such that the cell recorded on the intracellular electrode was also recorded extracellularly by a tetrode. [sent-142, score-0.799]
</p><p>55 Action potentials detected on the intracellular (IC) channel are an almost certain indicator that the cell being recorded spiked. [sent-143, score-0.396]
</p><p>56 Action potentials detected on the extracellular (EC) channels may include the action potentials generated by the intracellularly recorded cell, but almost certainly include spiking activity from other cells as well. [sent-144, score-0.684]
</p><p>57 The intracellular recording therefore can be used to obtain a ground truth labeling for the spikes originating from one neuron that can be used to evaluate the performance of human sorters and automatic spike sorting algorithms that sort extracellular recordings [13]. [sent-145, score-1.737]
</p><p>58 However, by this method ground truth can only be determined for one of the neurons whose spikes are present in the extracellular recording, and this should be kept in mind when evaluating the performance of spike sorting algorithms on such a data set. [sent-146, score-1.208]
</p><p>59 Neither the correct number of distinct neurons recorded from by the extracellular electrode nor the correct labeling for any spikes not originating from the neuron recorded intracellularly can be determined by this methodology. [sent-147, score-0.796]
</p><p>60 81% 0  Table 1: Performance of both algorithms on the two data sets: % false positives (FP), % false negatives (FN), # of refratory period violations (RPV). [sent-165, score-0.195]
</p><p>61 The subset of that data set that was used for the experiments consisted of two recordings from different animals (4 minutes each), recorded at 10 kHz. [sent-167, score-0.157]
</p><p>62 The data was bandpass ﬁltered (300Hz – 3kHz), and spikes on the intracellular channel were detected as the local maxima of the ﬁrst derivative of the signal larger than a manually chosen threshold. [sent-168, score-0.326]
</p><p>63 Spikes on the extracellular channels were determined as the local minima exceeding 4 standard deviations in magnitude. [sent-169, score-0.206]
</p><p>64 Spike waveforms of length 1 ms were extracted from around each spike (4 samples before and 5 samples after the peak). [sent-170, score-0.617]
</p><p>65 The positions of the minima within the spike waveforms were aligned by upsampling, shifting and then downsampling the waveforms. [sent-171, score-0.594]
</p><p>66 The extracellular spikes corresponding to action potentials from the identiﬁed neuron were determined as the spikes occurring within 0. [sent-172, score-0.781]
</p><p>67 For each spike the signals from the four tetrode channels were combined into a vector of length 40. [sent-174, score-0.522]
</p><p>68 Each dimensions was scaled by the maximal variance among all dimensions and PCA dimensionality reduction was performed on the scaled data sets (for each of the two recordings separately). [sent-175, score-0.1]
</p><p>69 The ﬁrst three principal components were used as input to our spike sorting algorithm. [sent-176, score-0.729]
</p><p>70 The ﬁrst recording (data set 1) consists of 3187 spikes, 831 originate from the identiﬁed neuron, while the second (data set 2) contains 3502 spikes, 553 of which were also detected on the IC channel. [sent-177, score-0.16]
</p><p>71 As shown in Figure 1, there is a clearly visible change in waveform shape of the identiﬁed neuron over time in data set 1, while in data set 2 the waveform shapes remain roughly constant. [sent-178, score-0.654]
</p><p>72 Presumably this change in waveform shape is due to the slow death of the cell as a result of the damage done to the cell by the intracellular recording procedure. [sent-179, score-0.617]
</p><p>73 985 and γ = 1 − 10−5 , reﬂecting the fact that we consider relative ﬁring rates of the neurons to stay roughly constant over time and neuron death a relatively rare process respectively. [sent-183, score-0.271]
</p><p>74 01, favoring small changes in the cluster parameters from one time step to the next. [sent-185, score-0.128]
</p><p>75 For comparison, the same data set was also sorted using the DPM-based spike sorting algorithm described in [6]2 , which pools waveforms over time and thus does not make use of any information about the occurrence times of the spikes. [sent-191, score-1.01]
</p><p>76 As expected, our algorithm outperforms the DPM-based algorithm on data set 1, which includes waveform drift which the DPM cannot account for. [sent-201, score-0.313]
</p><p>77 As data set 2 does not show waveform drift it can be adequately modeled without introducing time dependence. [sent-202, score-0.352]
</p><p>78 html  5  (a) Ground Truth  (b) Ground Truth  (c) DPM  (d) DPM  (e) GPUDPM  (f) GPUDPM  Figure 1: A comparison of DPM to GPUDPM spike sorting for two channels of tetrode data for which the ground truth labeling of one neuron is known. [sent-208, score-1.165]
</p><p>79 The top row of graphs shows the ground truth labeling of both data sets where the action potentials known to have been generated by a single neuron are labeled with x’s. [sent-211, score-0.574]
</p><p>80 Other points in the top row of graphs may also correspond to action potentials but as we do not know the ground truth labeling for them we label them all with dots. [sent-212, score-0.451]
</p><p>81 The middle row shows the maximum a posteriori labeling of both data sets produced by a DP mixture model spike sorting algorithm which does not utilize the time at which waveforms were captured, nor does it model waveform shape change. [sent-213, score-1.347]
</p><p>82 The bottom row shows the maximum a posteriori labeling of both data sets produced by our GPUDPM spike sorting algorithm which does model both the time at which the spikes occurred and the changing action potential waveshape. [sent-214, score-1.21]
</p><p>83 The left column shows that the GPUDPM performs better than the DPM when the waveshape of the underlying neurons changes over time. [sent-215, score-0.188]
</p><p>84 The right column shows that the GPUDPM performs no worse than the DPM when the waveshape of the underlying neurons stays constant. [sent-216, score-0.162]
</p><p>85 With a larger number of particles (or samples in the Gibbs sampler), one would expect both models to perform equally well, with possibly a slight advantage for the GPUDPM which can exploit the information contained in the refractory period violations. [sent-219, score-0.283]
</p><p>86 As dictated by the model, the GPUDPM algorithm does not assign two spikes that are within the refractory period of each other to the same cluster, whereas the DPM does not incorporate this restriction, and therefore can produce labelings containing refractory period violations. [sent-220, score-0.625]
</p><p>87 In the former case the algorithm incorrectly places the waveforms from the IC channel and the waveform of another neuron in one cluster, in the latter case the algorithm starts assigning the IC waveforms to a different cluster after some point in time. [sent-223, score-0.807]
</p><p>88 Here the labels assigned to both the the neuron with changing waveshape and one of the neurons with stationary waveshape change approximately half-way through the recording. [sent-230, score-0.389]
</p><p>89 While for this data set we know this labeling to be wrong because we know the ground truth, in other recordings such an “injection of noise” could, for instance, signal a shift in electrode position requiring similar rapid births and deaths of clusters. [sent-233, score-0.375]
</p><p>90 5  Discussion  We have demonstrated that spike sorting using time-varying Dirichlet process mixtures in general, and more speciﬁcally our spike sorting specialization of the GPUDPM, produce promising results. [sent-234, score-1.481]
</p><p>91 With such a spike sorting approach we, within a single model, are able to account for action potential waveform drift, refractory period violations, and neuron appearance and disappearance from a recording. [sent-235, score-1.599]
</p><p>92 Previously no single model addressed all of these simultaneously, requiring solutions in the form of ad hoc combinations of strategies and algorithms that produces spike sorting results that were potentially difﬁcult to characterize. [sent-236, score-0.729]
</p><p>93 Directions for further research include the development of a more efﬁcient sequential inference scheme or a hybrid sequential/Gibbs sampler scheme that allows propagation of interspike interval information backwards in time. [sent-240, score-0.084]
</p><p>94 Parametric models for the interspike interval density for each neuron whose parameters are inferred from the data, which can improve spike sorting results [15], can also be incorporated into the model. [sent-241, score-0.927]
</p><p>95 A review of methods for spike sorting: the detection and classiﬁcation of neural action potentials. [sent-247, score-0.563]
</p><p>96 An application of reversible-jump Markov chain Monte Carlo to spike classiﬁcation of multi-unit extracellular recordings. [sent-260, score-0.58]
</p><p>97 HermesB: A continuous neural recording system for freely behaving primates. [sent-315, score-0.118]
</p><p>98 Intracellular features a predicted by extracellular recordings in the hippocampus in vivo. [sent-338, score-0.29]
</p><p>99 Accuracy of tetrode spike separation as a determined by simultaneous intracellular and extracellular measurements. [sent-348, score-0.787]
</p><p>100 Improved spike-sorting by modeling ﬁring statistics and burst-dependent spike amplitude attenuation: A Markov Chain Monte Carlo approach. [sent-359, score-0.413]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gpudpm', 0.452), ('spike', 0.413), ('sorting', 0.316), ('waveform', 0.234), ('waveforms', 0.181), ('dpm', 0.181), ('extracellular', 0.167), ('refractory', 0.153), ('action', 0.15), ('mt', 0.138), ('intracellular', 0.137), ('neuron', 0.123), ('spikes', 0.122), ('recording', 0.118), ('waveshape', 0.104), ('recordings', 0.1), ('potentials', 0.097), ('period', 0.087), ('rabs', 0.087), ('drift', 0.079), ('labeling', 0.072), ('customers', 0.07), ('dpms', 0.07), ('tetrode', 0.07), ('electrode', 0.068), ('truth', 0.067), ('ground', 0.065), ('cluster', 0.063), ('occurrence', 0.061), ('ct', 0.059), ('neurons', 0.058), ('recorded', 0.057), ('ic', 0.056), ('dirichlet', 0.053), ('interspike', 0.052), ('rpv', 0.052), ('urn', 0.052), ('violations', 0.052), ('potential', 0.049), ('restaurant', 0.047), ('polya', 0.046), ('mixture', 0.046), ('deletion', 0.045), ('particles', 0.043), ('detected', 0.042), ('disappearance', 0.042), ('sitting', 0.042), ('ci', 0.041), ('particle', 0.039), ('channels', 0.039), ('wood', 0.039), ('time', 0.039), ('cell', 0.038), ('dp', 0.038), ('lille', 0.037), ('originating', 0.037), ('avg', 0.037), ('transition', 0.037), ('births', 0.035), ('buzs', 0.035), ('csicsvari', 0.035), ('deaths', 0.035), ('henze', 0.035), ('intracellularly', 0.035), ('rosenbluth', 0.035), ('seating', 0.035), ('cj', 0.034), ('fp', 0.034), ('appearance', 0.032), ('sampler', 0.032), ('neurophysiology', 0.031), ('fn', 0.031), ('dilan', 0.03), ('ddp', 0.03), ('chinese', 0.029), ('false', 0.028), ('crp', 0.028), ('death', 0.028), ('mk', 0.027), ('proposal', 0.027), ('kt', 0.027), ('occurred', 0.027), ('publicly', 0.027), ('changes', 0.026), ('harris', 0.026), ('carlo', 0.026), ('channel', 0.025), ('monte', 0.025), ('arriving', 0.025), ('shape', 0.024), ('labelings', 0.023), ('hippocampus', 0.023), ('frank', 0.023), ('incorporated', 0.023), ('gibbs', 0.023), ('ms', 0.023), ('table', 0.023), ('process', 0.023), ('metropolis', 0.023), ('produced', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="59-tfidf-1" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>Author: Jan Gasthaus, Frank Wood, Dilan Gorur, Yee W. Teh</p><p>Abstract: In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle “appearance” and “disappearance” of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of inﬁnite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known. 1</p><p>2 0.43475974 <a title="59-tfidf-2" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>Author: Zhi Yang, Qi Zhao, Wentai Liu</p><p>Abstract: This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer. 1</p><p>3 0.23278117 <a title="59-tfidf-3" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Matthias Munk, Klaus Obermayer</p><p>Abstract: Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three ﬁndings: (1) copula-based distributions provide signiﬁcantly better ﬁts than discretized multivariate normal distributions; (2) negative binomial margins ﬁt the data signiﬁcantly better than Poisson margins; and (3) the dependence structure carries 12% of the mutual information between stimuli and responses. 1</p><p>4 0.22457224 <a title="59-tfidf-4" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>5 0.20234308 <a title="59-tfidf-5" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>Author: Jonathan L. Roux, Alain D. Cheveigné, Lucas C. Parra</p><p>Abstract: How does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes? One could think of using template matching or matching pursuit to ﬁnd the arbitrarily shifted linear components. However, traditional matching approaches require that the templates be known a priori. To overcome this restriction we use instead semi Non-Negative Matrix Factorization (semiNMF) that we extend to allow for time shifts when matching the templates to the signal. The algorithm estimates templates directly from the data along with their non-negative amplitudes. The resulting method can be thought of as an adaptive template matching procedure. We demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings. On these data the algorithm essentially performs spike detection and unsupervised spike clustering. Results on simulated data and extracellular recordings indicate that the method performs well for signalto-noise ratios of 6dB or higher and that spike templates are recovered accurately provided they are suﬃciently diﬀerent. 1</p><p>6 0.1516531 <a title="59-tfidf-6" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>7 0.1350764 <a title="59-tfidf-7" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>8 0.11087389 <a title="59-tfidf-8" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>9 0.10158247 <a title="59-tfidf-9" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>10 0.093777731 <a title="59-tfidf-10" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>11 0.084564544 <a title="59-tfidf-11" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>12 0.075306237 <a title="59-tfidf-12" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>13 0.067309462 <a title="59-tfidf-13" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>14 0.066033885 <a title="59-tfidf-14" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>15 0.059442054 <a title="59-tfidf-15" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>16 0.057557065 <a title="59-tfidf-16" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>17 0.056319166 <a title="59-tfidf-17" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>18 0.056023899 <a title="59-tfidf-18" href="./nips-2008-Dependence_of_Orientation_Tuning_on_Recurrent_Excitation_and_Inhibition_in_a_Network_Model_of_V1.html">58 nips-2008-Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1</a></p>
<p>19 0.051982857 <a title="59-tfidf-19" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>20 0.046453953 <a title="59-tfidf-20" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.171), (1, 0.115), (2, 0.299), (3, 0.26), (4, -0.259), (5, 0.057), (6, 0.039), (7, -0.097), (8, -0.143), (9, -0.071), (10, 0.02), (11, -0.133), (12, 0.077), (13, -0.064), (14, 0.047), (15, 0.028), (16, -0.072), (17, -0.064), (18, 0.003), (19, 0.091), (20, 0.049), (21, 0.018), (22, 0.03), (23, -0.033), (24, -0.005), (25, 0.047), (26, -0.014), (27, -0.029), (28, -0.03), (29, 0.097), (30, 0.055), (31, -0.025), (32, 0.038), (33, -0.057), (34, -0.193), (35, -0.059), (36, -0.049), (37, -0.014), (38, 0.034), (39, 0.079), (40, 0.009), (41, 0.001), (42, 0.06), (43, -0.018), (44, 0.027), (45, -0.03), (46, -0.073), (47, 0.008), (48, -0.006), (49, -0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96130002 <a title="59-lsi-1" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>Author: Jan Gasthaus, Frank Wood, Dilan Gorur, Yee W. Teh</p><p>Abstract: In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle “appearance” and “disappearance” of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of inﬁnite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known. 1</p><p>2 0.95848233 <a title="59-lsi-2" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>Author: Zhi Yang, Qi Zhao, Wentai Liu</p><p>Abstract: This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer. 1</p><p>3 0.82781571 <a title="59-lsi-3" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>Author: Peng Xu, Timothy K. Horiuchi, Pamela A. Abshire</p><p>Abstract: We report a compact realization of short-term depression (STD) in a VLSI stochastic synapse. The behavior of the circuit is based on a subtractive single release model of STD. Experimental results agree well with simulation and exhibit expected STD behavior: the transmitted spike train has negative autocorrelation and lower power spectral density at low frequencies which can remove redundancy in the input spike train, and the mean transmission probability is inversely proportional to the input spike rate which has been suggested as an automatic gain control mechanism in neural systems. The dynamic stochastic synapse could potentially be a powerful addition to existing deterministic VLSI spiking neural systems. 1</p><p>4 0.80390596 <a title="59-lsi-4" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>5 0.79189444 <a title="59-lsi-5" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>Author: Jonathan L. Roux, Alain D. Cheveigné, Lucas C. Parra</p><p>Abstract: How does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes? One could think of using template matching or matching pursuit to ﬁnd the arbitrarily shifted linear components. However, traditional matching approaches require that the templates be known a priori. To overcome this restriction we use instead semi Non-Negative Matrix Factorization (semiNMF) that we extend to allow for time shifts when matching the templates to the signal. The algorithm estimates templates directly from the data along with their non-negative amplitudes. The resulting method can be thought of as an adaptive template matching procedure. We demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings. On these data the algorithm essentially performs spike detection and unsupervised spike clustering. Results on simulated data and extracellular recordings indicate that the method performs well for signalto-noise ratios of 6dB or higher and that spike templates are recovered accurately provided they are suﬃciently diﬀerent. 1</p><p>6 0.62399405 <a title="59-lsi-6" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>7 0.45326257 <a title="59-lsi-7" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>8 0.45263079 <a title="59-lsi-8" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>9 0.42648625 <a title="59-lsi-9" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>10 0.37703922 <a title="59-lsi-10" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>11 0.32216799 <a title="59-lsi-11" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>12 0.30289003 <a title="59-lsi-12" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>13 0.28498697 <a title="59-lsi-13" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>14 0.2594972 <a title="59-lsi-14" href="./nips-2008-Breaking_Audio_CAPTCHAs.html">41 nips-2008-Breaking Audio CAPTCHAs</a></p>
<p>15 0.24854606 <a title="59-lsi-15" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>16 0.23563367 <a title="59-lsi-16" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>17 0.22878018 <a title="59-lsi-17" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<p>18 0.2261519 <a title="59-lsi-18" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>19 0.22384824 <a title="59-lsi-19" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>20 0.22229663 <a title="59-lsi-20" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.061), (7, 0.071), (12, 0.041), (15, 0.023), (28, 0.124), (51, 0.307), (57, 0.081), (59, 0.023), (63, 0.014), (71, 0.076), (77, 0.031), (78, 0.011), (83, 0.048)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73129052 <a title="59-lda-1" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>Author: Jan Gasthaus, Frank Wood, Dilan Gorur, Yee W. Teh</p><p>Abstract: In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle “appearance” and “disappearance” of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of inﬁnite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known. 1</p><p>2 0.62001085 <a title="59-lda-2" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>Author: Daniel Acuna, Paul R. Schrater</p><p>Abstract: We use graphical models and structure learning to explore how people learn policies in sequential decision making tasks. Studies of sequential decision-making in humans frequently ﬁnd suboptimal performance relative to an ideal actor that knows the graph model that generates reward in the environment. We argue that the learning problem humans face also involves learning the graph structure for reward generation in the environment. We formulate the structure learning problem using mixtures of reward models, and solve the optimal action selection problem using Bayesian Reinforcement Learning. We show that structure learning in one and two armed bandit problems produces many of the qualitative behaviors deemed suboptimal in previous studies. Our argument is supported by the results of experiments that demonstrate humans rapidly learn and exploit new reward structure. 1</p><p>3 0.51048213 <a title="59-lda-3" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>Author: Xiaodi Hou, Liqing Zhang</p><p>Abstract: A visual attention system should respond placidly when common stimuli are presented, while at the same time keep alert to anomalous visual inputs. In this paper, a dynamic visual attention model based on the rarity of features is proposed. We introduce the Incremental Coding Length (ICL) to measure the perspective entropy gain of each feature. The objective of our model is to maximize the entropy of the sampled visual features. In order to optimize energy consumption, the limit amount of energy of the system is re-distributed amongst features according to their Incremental Coding Length. By selecting features with large coding length increments, the computational system can achieve attention selectivity in both static and dynamic scenes. We demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation. Moreover, we also show that our model captures several less-reported dynamic visual search behaviors, such as attentional swing and inhibition of return. 1</p><p>4 0.50921744 <a title="59-lda-4" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>5 0.50632036 <a title="59-lda-5" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>Author: Zhi Yang, Qi Zhao, Wentai Liu</p><p>Abstract: This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer. 1</p><p>6 0.50488454 <a title="59-lda-6" href="./nips-2008-A_spatially_varying_two-sample_recombinant_coalescent%2C_with_applications_to_HIV_escape_response.html">11 nips-2008-A spatially varying two-sample recombinant coalescent, with applications to HIV escape response</a></p>
<p>7 0.50405037 <a title="59-lda-7" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>8 0.50284797 <a title="59-lda-8" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>9 0.502334 <a title="59-lda-9" href="./nips-2008-MDPs_with_Non-Deterministic_Policies.html">131 nips-2008-MDPs with Non-Deterministic Policies</a></p>
<p>10 0.50230318 <a title="59-lda-10" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>11 0.50224137 <a title="59-lda-11" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>12 0.50210655 <a title="59-lda-12" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>13 0.50185692 <a title="59-lda-13" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>14 0.50152475 <a title="59-lda-14" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>15 0.50105041 <a title="59-lda-15" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>16 0.50084335 <a title="59-lda-16" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>17 0.50026256 <a title="59-lda-17" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>18 0.49939847 <a title="59-lda-18" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>19 0.49931806 <a title="59-lda-19" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>20 0.49928826 <a title="59-lda-20" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
