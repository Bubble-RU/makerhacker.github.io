<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-137" href="#">nips2008-137</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</h1>
<br/><p>Source: <a title="nips-2008-137-pdf" href="http://papers.nips.cc/paper/3552-modeling-short-term-noise-dependence-of-spike-counts-in-macaque-prefrontal-cortex.pdf">pdf</a></p><p>Author: Arno Onken, Steffen Grünewälder, Matthias Munk, Klaus Obermayer</p><p>Abstract: Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three ﬁndings: (1) copula-based distributions provide signiﬁcantly better ﬁts than discretized multivariate normal distributions; (2) negative binomial margins ﬁt the data signiﬁcantly better than Poisson margins; and (3) the dependence structure carries 12% of the mutual information between stimuli and responses. 1</p><p>Reference: <a title="nips-2008-137-reference" href="../nips2008_reference/nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract Correlations between spike counts are often used to analyze neural coding. [sent-12, score-0.693]
</p><p>2 Yet, this assumption is often inappropriate, especially for low spike counts. [sent-14, score-0.463]
</p><p>3 In this study, we present copulas as an alternative approach. [sent-15, score-0.328]
</p><p>4 With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. [sent-16, score-1.163]
</p><p>5 Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. [sent-17, score-0.613]
</p><p>6 We develop a framework to analyze spike count data by means of copulas. [sent-18, score-0.667]
</p><p>7 Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. [sent-19, score-0.031]
</p><p>8 We apply the method to our data recorded from macaque prefrontal cortex. [sent-20, score-0.216]
</p><p>9 1  Introduction  Understanding neural coding is at the heart of theoretical neuroscience. [sent-22, score-0.065]
</p><p>10 Analyzing spike counts of a population is one way to gain insight into neural coding properties. [sent-23, score-0.789]
</p><p>11 Even when the same stimulus is presented repeatedly, responses from the neurons vary, i. [sent-24, score-0.152]
</p><p>12 from trial to trial responses of neurons are subject to noise. [sent-26, score-0.126]
</p><p>13 The noise variations of neighboring neurons are typically correlated (noise correlations). [sent-27, score-0.241]
</p><p>14 Due to their relevance for neural coding, noise correlations have been subject of a considerable number of studies (see [1] for a review). [sent-28, score-0.218]
</p><p>15 Thus, correlated spike rates were generally modeled by multivariate normal distributions with a speciﬁc covariance matrix that describes all pairwise linear correlations. [sent-30, score-0.936]
</p><p>16 For long time intervals or high ﬁring rates, the average number of spikes is sufﬁciently large for the central limit theorem to apply and thus the normal distribution is a good approximation for the spike count distributions. [sent-31, score-0.914]
</p><p>17 However, several experimental ﬁndings suggest that noise correlations as well as sensory information processing predominantly take place on a shorter time scale, on the order of tens to hundreds of milliseconds [2, 3]. [sent-32, score-0.218]
</p><p>18 It is therefore questionable if the normal distribution is still an appropriate approximation and if the results of studies based on Gaussian noise apply to short time intervals and low ﬁring rates. [sent-33, score-0.316]
</p><p>19 (b): The distributions of the spike counts of a neuron pair from the data described in Section 4 for 100 ms time bins. [sent-35, score-0.728]
</p><p>20 Dark squares represent a high number of occurrences of corresponding pairs of spike counts. [sent-36, score-0.463]
</p><p>21 One can see that the spike counts are correlated since the ratios are high near the diagonal. [sent-37, score-0.734]
</p><p>22 The distributions of the individual spike counts are plotted below and left of the axes. [sent-38, score-0.728]
</p><p>23 (c): Density of a ﬁt with a bivariate normal distribution. [sent-39, score-0.188]
</p><p>24 (d): Distribution of a ﬁt with negative binomial margins coupled with the Clayton copula. [sent-40, score-0.35]
</p><p>25 This is due to several major drawbacks of the multivariate normal distribution: (1) Its margins are continuous with a symmetric shape, whereas empirical distributions of real spike counts tend to have a positive skew, i. [sent-41, score-1.232]
</p><p>26 the mass of the distribution is concentrated at the left of its mode. [sent-43, score-0.046]
</p><p>27 Moreover, the normal distribution allows negative values which are not meaningful for spike counts. [sent-44, score-0.697]
</p><p>28 Especially for low rates, this can become a major issue, since the probability of negative values will be high. [sent-45, score-0.051]
</p><p>29 (2) The dependence structure of a multivariate normal distribution is always elliptical, whereas spike counts of short time bins can have a bulb-shaped dependence structure (see Fig. [sent-46, score-1.328]
</p><p>30 (3) The multivariate normal distribution does not allow higher order correlations of its elements. [sent-48, score-0.537]
</p><p>31 It was shown that pairwise interactions are sufﬁcient for retinal ganglion cells and cortex cells in vitro [4]. [sent-50, score-0.29]
</p><p>32 However, there is evidence that they are insufﬁcient for subsequent cortex areas in vivo [5]. [sent-51, score-0.081]
</p><p>33 We will show that our data recorded in prefrontal cortex suggest that higher order interactions (which involve more than two neurons) do play an important role in the prefrontal cortex as well. [sent-52, score-0.457]
</p><p>34 In this paper, we present a method that addresses the above shortcomings of the multivariate normal distribution. [sent-53, score-0.282]
</p><p>35 We apply copulas [6] to form multivariate distributions with a rich set of dependence structures and discrete marginal distributions, including the Poisson distribution. [sent-54, score-0.865]
</p><p>36 Copulas were previously applied to model the distribution of continuous ﬁrst-spike-latencies [7]. [sent-55, score-0.077]
</p><p>37 2  Copulas  We give an informal introduction to copulas and apply the concept to a pair of neurons from our data which are described and fully analyzed in Section 4. [sent-57, score-0.507]
</p><p>38 A copula is a cumulative distribution function that can couple arbitrary marginal distributions. [sent-60, score-0.622]
</p><p>39 There are many families of copulas, each with a different dependence structure. [sent-61, score-0.235]
</p><p>40 Some families have an elliptical dependence structure, similar to the multivariate normal distribution. [sent-62, score-0.572]
</p><p>41 However, it is also possible to use completely different dependence structures which are more appropriate for the data at hand. [sent-63, score-0.193]
</p><p>42 As an example, consider the modeling of spike count dependencies of two neurons (Fig. [sent-64, score-0.766]
</p><p>43 Spike trains are recorded from the neurons and transformed to spike counts (Fig. [sent-66, score-0.889]
</p><p>44 The distribution of the counts depends on the length of the time bin that is used to count the spikes, here 100 ms. [sent-70, score-0.426]
</p><p>45 In the case considered, the correlation at low counts is higher than at high counts. [sent-71, score-0.277]
</p><p>46 The density of a typical population model based on the multivariate normal (MVN) distribution is shown in Fig. [sent-73, score-0.386]
</p><p>47 Here, we did not discretize the distribution since the standard approach to investigate noise correlations also uses the continuous distribution [1]. [sent-75, score-0.341]
</p><p>48 The mean and covariance matrix of the MVN distribution correspond to the sample mean and the sample covariances of the empirical distribution. [sent-76, score-0.046]
</p><p>49 Yet, the dependence structure does not reﬂect the true dependence structure of the counts. [sent-77, score-0.334]
</p><p>50 But the spike count probabilities for a copula-based distribution (Fig. [sent-78, score-0.686]
</p><p>51 1d) correspond well to the empirical distribution in Fig. [sent-79, score-0.046]
</p><p>52 The modeling of spike count data with the help of a copula is done in three steps: (1) A marginal distribution, e. [sent-81, score-1.1]
</p><p>53 a Poisson or a negative binomial distribution is chosen, based on the spike count distribution of the individual neurons. [sent-83, score-0.86]
</p><p>54 (2) The counts are transformed to probabilities using the cumulative distribution function of the marginal distribution. [sent-84, score-0.422]
</p><p>55 (3) The probabilities and thereby the cumulative marginal distributions are coupled with the help of a so-called copula function. [sent-85, score-0.669]
</p><p>56 As an example, consider the Clayton copula family [6]. [sent-86, score-0.387]
</p><p>57 For two variables the copula is given by 1  C(p1 , p2 , α) = α  max{ p1 α 1  +  1 pα 2  , − 1, 0}  where pi denotes the probability of the spike count Xi of the ith neuron being lower or equal to ri (i. [sent-87, score-1.129]
</p><p>58 Note that there are generalizations to more than two margins (see Section 3. [sent-90, score-0.191]
</p><p>59 The function C(p1 , p2 , α) generates a joint cumulative distribution function by coupling the margins and thereby introduces correlations of second and higher order between the spike count variables. [sent-92, score-1.229]
</p><p>60 The ratio of the joint probability that corresponds to statistically independent spike counts P (X1 ≤ r1 , X2 ≤ r2 ) = p1 p2 and the dependence introduced by the Clayton copula (for 1 1 pα + pα − 1 ≥ 0) is given by 1  2  p1 p2 = p1 p2 C(p1 , p2 , α)  α  1 1 + α −1= pα p2 1  α  pα + pα − pα pα . [sent-93, score-1.22]
</p><p>61 Since pi ∈ [0, 1] the deviation from the ratio 1 will be larger for small probabilities. [sent-95, score-0.042]
</p><p>62 Thus, the copula generates correlations whose strengths depend on the magnitude of the probabilities. [sent-96, score-0.585]
</p><p>63 1d) can then be calculated from the cumulative probability using the difference scheme as described in Section 3. [sent-98, score-0.074]
</p><p>64 Care must be taken whenever copulas are applied to form discrete distributions: while for continuous distributions typical measures of dependence are determined by the copula function C only, these measures are affected by the shape of the marginal distributions in the discrete case [8]. [sent-100, score-1.221]
</p><p>65 3  Parametric spike count models and model selection procedure  We will now describe the formal aspects of the multivariate normal distribution on the one hand and copula-based models as the proposed alternative on the other hand, both in terms of their application to spike counts. [sent-101, score-1.431]
</p><p>66 1  The discretized multivariate normal distribution  The MVN distribution is continuous and needs to be discretized (and rectiﬁed) before it can be applied to spike count data (which are discrete and non-negative). [sent-103, score-1.206]
</p><p>67 The cumulative distribution function (cdf) of the spike count vector X is then given by FX (r1 , . [sent-104, score-0.76]
</p><p>68 ⌋ denotes the ﬂoor operation for the discretization, Φµ,Σ denotes the cdf of the MVN distribution with mean µ and correlation matrix Σ, and d denotes the dimension of the multivariate distribution and corresponds to the number of neurons that are modeled. [sent-114, score-0.543]
</p><p>69 According to the central limit theorem, the distribution of spike counts approaches the MVN distribution only for large counts. [sent-118, score-0.758]
</p><p>70 2  Copula-based models  Formally, a copula C is a cdf with uniform margins. [sent-120, score-0.531]
</p><p>71 , FXd to form a joint cdf FX , such that FX (r1 , . [sent-124, score-0.144]
</p><p>72 There are many families of copulas with different dependence shapes and different numbers of parameters, e. [sent-131, score-0.563]
</p><p>73 the multivariate Clayton copula family with a scalar parameter α: −1/α  d  Cα (u) =  u−α , i  max 1 − d +  0  . [sent-133, score-0.532]
</p><p>74 i=1  Thus, for a given realization r, which can represent the counts of two neurons, we can set ui = FXi (ri ) and FX (r) = Cα (u), where FXi can be arbitrary univariate cdf’s. [sent-134, score-0.203]
</p><p>75 Thereby, we can generate a multivariate distribution with speciﬁc margins FXi and a dependence structure determined by C. [sent-135, score-0.549]
</p><p>76 In the case of discrete marginal distributions, however, typical measures of dependence, such as the linear correlation coefﬁcient or Kendall’s τ are effected by the shape of these margins [8]. [sent-136, score-0.374]
</p><p>77 Note that α does not only control the strength of pairwise interactions but also the degree of higher order interactions. [sent-137, score-0.135]
</p><p>78 Another copula family is the Farlie-Gumbel-Morgenstern (FGM) copula [6]. [sent-138, score-0.774]
</p><p>79 It is special in that it has 2d − d − 1 parameters that individually determine the pairwise and higher order interactions. [sent-139, score-0.099]
</p><p>80 Its cdf takes the form   d  Cα (u) = 1 +  k  i=1  k=2 1≤j1 <··· <··· < 20). [sent-140, score-0.144]
</p><p>81 At present, the approach is computationally too demanding for higher numbers of neurons. [sent-141, score-0.038]
</p><p>82 Directions for future research are the exploration of other copula families and the validation of population coding principles that were obtained on the assumption of Gaussian noise. [sent-143, score-0.578]
</p><p>83 Newsome, Correlated ﬁring in macaque visual area MT: time scales and relationship to behavior. [sent-158, score-0.06]
</p><p>84 Smith, Stimulus dependence of neuronal correlation in primary visual cortex of the macaque. [sent-163, score-0.284]
</p><p>85 Bialek, Weak pairwise correlations imply strongly correlated network states in a neural population. [sent-170, score-0.3]
</p><p>86 Jacobs, The costs of ignoring high-order correlations in populations of model neurons. [sent-176, score-0.171]
</p><p>87 Dean, The statistical reliability of signals in single neurons in cat and monkey visual cortex. [sent-198, score-0.126]
</p><p>88 Xu, The estimation method of inference functions for margins for multivariate models. [sent-203, score-0.336]
</p><p>89 Nirenberg, Synergy, redundancy, and independence in population codes, revisited. [sent-213, score-0.058]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('spike', 0.463), ('copula', 0.387), ('copulas', 0.328), ('counts', 0.203), ('margins', 0.191), ('count', 0.177), ('correlations', 0.171), ('mvn', 0.17), ('dependence', 0.167), ('multivariate', 0.145), ('cdf', 0.144), ('normal', 0.137), ('neurons', 0.126), ('clayton', 0.109), ('berlin', 0.1), ('fx', 0.092), ('prefrontal', 0.092), ('technische', 0.089), ('fxi', 0.089), ('cortex', 0.081), ('binomial', 0.077), ('cumulative', 0.074), ('marginal', 0.073), ('bccn', 0.068), ('fxd', 0.068), ('correlated', 0.068), ('families', 0.068), ('coding', 0.065), ('poisson', 0.065), ('distributions', 0.062), ('discretized', 0.062), ('pairwise', 0.061), ('ri', 0.06), ('macaque', 0.06), ('population', 0.058), ('universit', 0.056), ('latham', 0.055), ('elliptical', 0.055), ('negative', 0.051), ('bivariate', 0.051), ('noise', 0.047), ('distribution', 0.046), ('recti', 0.046), ('ring', 0.043), ('couple', 0.042), ('neuroscience', 0.042), ('pi', 0.042), ('thereby', 0.042), ('rd', 0.041), ('higher', 0.038), ('recorded', 0.037), ('shape', 0.037), ('discrete', 0.037), ('interactions', 0.036), ('correlation', 0.036), ('trains', 0.034), ('intervals', 0.032), ('spikes', 0.032), ('continuous', 0.031), ('coupled', 0.031), ('mutual', 0.031), ('nirenberg', 0.03), ('vitro', 0.03), ('dean', 0.03), ('astin', 0.03), ('genest', 0.03), ('jenison', 0.03), ('schneidman', 0.03), ('tolhurst', 0.03), ('synergy', 0.03), ('kendall', 0.03), ('illinois', 0.03), ('bialek', 0.03), ('joe', 0.03), ('newsome', 0.03), ('germany', 0.028), ('cells', 0.028), ('ndings', 0.027), ('weaver', 0.027), ('michel', 0.027), ('segev', 0.027), ('steffen', 0.027), ('disposal', 0.027), ('movshon', 0.027), ('jacobs', 0.027), ('questionable', 0.027), ('apply', 0.027), ('analyze', 0.027), ('generates', 0.027), ('stimulus', 0.026), ('transformed', 0.026), ('structures', 0.026), ('ganglion', 0.026), ('primer', 0.026), ('informal', 0.026), ('pouget', 0.026), ('klaus', 0.026), ('obermayer', 0.026), ('bmbf', 0.026), ('skew', 0.026)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999946 <a title="137-tfidf-1" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Matthias Munk, Klaus Obermayer</p><p>Abstract: Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three ﬁndings: (1) copula-based distributions provide signiﬁcantly better ﬁts than discretized multivariate normal distributions; (2) negative binomial margins ﬁt the data signiﬁcantly better than Poisson margins; and (3) the dependence structure carries 12% of the mutual information between stimuli and responses. 1</p><p>2 0.52712512 <a title="137-tfidf-2" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>Author: Pietro Berkes, Frank Wood, Jonathan W. Pillow</p><p>Abstract: The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. However, there is no simple model that can simultaneously account for (1) marginal distributions over single-neuron spike counts that are discrete and non-negative; and (2) joint distributions over the responses of multiple neurons that are often strongly dependent. Here, we show that both marginal and joint properties of neural responses can be captured using copula models. Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefﬁcients. We explore a variety of copula models for joint neural response distributions, and derive an efﬁcient maximum likelihood procedure for estimating them. We apply these models to neuronal data collected in macaque pre-motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons. We ﬁnd that more than one third of neuron pairs shows dependency concentrated in the lower or upper tails for their ﬁring rate distribution. 1</p><p>3 0.33910808 <a title="137-tfidf-3" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>4 0.33029723 <a title="137-tfidf-4" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>Author: Zhi Yang, Qi Zhao, Wentai Liu</p><p>Abstract: This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer. 1</p><p>5 0.23278117 <a title="137-tfidf-5" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>Author: Jan Gasthaus, Frank Wood, Dilan Gorur, Yee W. Teh</p><p>Abstract: In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle “appearance” and “disappearance” of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of inﬁnite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known. 1</p><p>6 0.16848053 <a title="137-tfidf-6" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>7 0.1338885 <a title="137-tfidf-7" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>8 0.13345701 <a title="137-tfidf-8" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>9 0.10319708 <a title="137-tfidf-9" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>10 0.096477322 <a title="137-tfidf-10" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>11 0.089971051 <a title="137-tfidf-11" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>12 0.083841242 <a title="137-tfidf-12" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>13 0.082911663 <a title="137-tfidf-13" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>14 0.072768532 <a title="137-tfidf-14" href="./nips-2008-Dependence_of_Orientation_Tuning_on_Recurrent_Excitation_and_Inhibition_in_a_Network_Model_of_V1.html">58 nips-2008-Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1</a></p>
<p>15 0.067789719 <a title="137-tfidf-15" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>16 0.066161536 <a title="137-tfidf-16" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>17 0.059402753 <a title="137-tfidf-17" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>18 0.058213502 <a title="137-tfidf-18" href="./nips-2008-Structured_ranking_learning_using_cumulative_distribution_networks.html">224 nips-2008-Structured ranking learning using cumulative distribution networks</a></p>
<p>19 0.056726467 <a title="137-tfidf-19" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>20 0.053285081 <a title="137-tfidf-20" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.158), (1, 0.107), (2, 0.329), (3, 0.364), (4, -0.329), (5, 0.031), (6, 0.055), (7, -0.092), (8, -0.144), (9, -0.022), (10, -0.037), (11, -0.12), (12, 0.057), (13, -0.073), (14, 0.017), (15, 0.011), (16, -0.003), (17, -0.008), (18, -0.036), (19, 0.198), (20, -0.046), (21, -0.04), (22, 0.076), (23, -0.105), (24, -0.052), (25, -0.059), (26, 0.076), (27, 0.079), (28, -0.04), (29, -0.099), (30, -0.109), (31, 0.077), (32, 0.1), (33, -0.014), (34, 0.367), (35, 0.007), (36, 0.083), (37, -0.149), (38, 0.027), (39, -0.04), (40, -0.034), (41, -0.028), (42, -0.021), (43, 0.034), (44, -0.011), (45, -0.034), (46, 0.052), (47, -0.007), (48, 0.028), (49, -0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97344226 <a title="137-lsi-1" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Matthias Munk, Klaus Obermayer</p><p>Abstract: Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three ﬁndings: (1) copula-based distributions provide signiﬁcantly better ﬁts than discretized multivariate normal distributions; (2) negative binomial margins ﬁt the data signiﬁcantly better than Poisson margins; and (3) the dependence structure carries 12% of the mutual information between stimuli and responses. 1</p><p>2 0.8813284 <a title="137-lsi-2" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>Author: Pietro Berkes, Frank Wood, Jonathan W. Pillow</p><p>Abstract: The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. However, there is no simple model that can simultaneously account for (1) marginal distributions over single-neuron spike counts that are discrete and non-negative; and (2) joint distributions over the responses of multiple neurons that are often strongly dependent. Here, we show that both marginal and joint properties of neural responses can be captured using copula models. Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefﬁcients. We explore a variety of copula models for joint neural response distributions, and derive an efﬁcient maximum likelihood procedure for estimating them. We apply these models to neuronal data collected in macaque pre-motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons. We ﬁnd that more than one third of neuron pairs shows dependency concentrated in the lower or upper tails for their ﬁring rate distribution. 1</p><p>3 0.68334168 <a title="137-lsi-3" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>4 0.58268714 <a title="137-lsi-4" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>Author: Zhi Yang, Qi Zhao, Wentai Liu</p><p>Abstract: This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer. 1</p><p>5 0.55430061 <a title="137-lsi-5" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>Author: Jan Gasthaus, Frank Wood, Dilan Gorur, Yee W. Teh</p><p>Abstract: In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle “appearance” and “disappearance” of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of inﬁnite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known. 1</p><p>6 0.46231273 <a title="137-lsi-6" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>7 0.4434799 <a title="137-lsi-7" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>8 0.37179071 <a title="137-lsi-8" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>9 0.33764893 <a title="137-lsi-9" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>10 0.29855913 <a title="137-lsi-10" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>11 0.27465564 <a title="137-lsi-11" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>12 0.26441887 <a title="137-lsi-12" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>13 0.22489579 <a title="137-lsi-13" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>14 0.21921085 <a title="137-lsi-14" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>15 0.21446469 <a title="137-lsi-15" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>16 0.20360605 <a title="137-lsi-16" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<p>17 0.18252365 <a title="137-lsi-17" href="./nips-2008-Temporal_Difference_Based_Actor_Critic_Learning_-_Convergence_and_Neural_Implementation.html">230 nips-2008-Temporal Difference Based Actor Critic Learning - Convergence and Neural Implementation</a></p>
<p>18 0.18241863 <a title="137-lsi-18" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>19 0.17946391 <a title="137-lsi-19" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>20 0.17907725 <a title="137-lsi-20" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.067), (7, 0.17), (12, 0.027), (15, 0.011), (27, 0.295), (28, 0.129), (57, 0.061), (59, 0.021), (63, 0.021), (71, 0.011), (77, 0.034), (83, 0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.81599027 <a title="137-lda-1" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>Author: Mauricio Alvarez, Neil D. Lawrence</p><p>Abstract: We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network. 1</p><p>same-paper 2 0.77311301 <a title="137-lda-2" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Matthias Munk, Klaus Obermayer</p><p>Abstract: Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three ﬁndings: (1) copula-based distributions provide signiﬁcantly better ﬁts than discretized multivariate normal distributions; (2) negative binomial margins ﬁt the data signiﬁcantly better than Poisson margins; and (3) the dependence structure carries 12% of the mutual information between stimuli and responses. 1</p><p>3 0.73053467 <a title="137-lda-3" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>Author: Guillaume R. Obozinski, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: We study the behavior of block 1 / 2 regularization for multivariate regression, where a K-dimensional response vector is regressed upon a ﬁxed set of p covariates. The problem of support union recovery is to recover the subset of covariates that are active in at least one of the regression problems. Studying this problem under high-dimensional scaling (where the problem parameters as well as sample size n tend to inﬁnity simultaneously), our main result is to show that exact recovery is possible once the order parameter given by θ 1 / 2 (n, p, s) : = n/[2ψ(B ∗ ) log(p − s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the size of the union of supports, and ψ(B ∗ ) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefﬁcient vectors that constitute the model. This sparsity-overlap function reveals that block 1 / 2 regularization for multivariate regression never harms performance relative to a naive 1 -approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal relative to the design. We complement our theoretical results with simulations that demonstrate the sharpness of the result, even for relatively small problems. 1</p><p>4 0.71221817 <a title="137-lda-4" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>Author: Ping Li, Kenneth W. Church, Trevor J. Hastie</p><p>Abstract: Conditional Random Sampling (CRS) was originally proposed for efﬁciently computing pairwise (l2 , l1 ) distances, in static, large-scale, and sparse data. This study modiﬁes the original CRS and extends CRS to handle dynamic or streaming data, which much better reﬂect the real-world situation than assuming static data. Compared with many other sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a signiﬁcant advantage in that it is “one-sketch-for-all.” In particular, we demonstrate the effectiveness of CRS in efﬁciently computing the Hamming norm, the Hamming distance, the lp distance, and the χ2 distance. A generic estimator and an approximate variance formula are also provided, for approximating any type of distances. We recommend CRS as a promising tool for building highly scalable systems, in machine learning, data mining, recommender systems, and information retrieval. 1</p><p>5 0.6510998 <a title="137-lda-5" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>Author: Pietro Berkes, Frank Wood, Jonathan W. Pillow</p><p>Abstract: The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. However, there is no simple model that can simultaneously account for (1) marginal distributions over single-neuron spike counts that are discrete and non-negative; and (2) joint distributions over the responses of multiple neurons that are often strongly dependent. Here, we show that both marginal and joint properties of neural responses can be captured using copula models. Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefﬁcients. We explore a variety of copula models for joint neural response distributions, and derive an efﬁcient maximum likelihood procedure for estimating them. We apply these models to neuronal data collected in macaque pre-motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons. We ﬁnd that more than one third of neuron pairs shows dependency concentrated in the lower or upper tails for their ﬁring rate distribution. 1</p><p>6 0.62019038 <a title="137-lda-6" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>7 0.60931957 <a title="137-lda-7" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>8 0.60618663 <a title="137-lda-8" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>9 0.60392433 <a title="137-lda-9" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>10 0.58802503 <a title="137-lda-10" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>11 0.57429522 <a title="137-lda-11" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>12 0.57240897 <a title="137-lda-12" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>13 0.56928486 <a title="137-lda-13" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>14 0.56898439 <a title="137-lda-14" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>15 0.56781977 <a title="137-lda-15" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>16 0.56607592 <a title="137-lda-16" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>17 0.56539494 <a title="137-lda-17" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>18 0.56461871 <a title="137-lda-18" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>19 0.56272972 <a title="137-lda-19" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>20 0.56230265 <a title="137-lda-20" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
