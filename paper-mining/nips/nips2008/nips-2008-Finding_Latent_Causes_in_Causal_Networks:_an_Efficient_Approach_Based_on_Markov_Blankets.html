<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>86 nips-2008-Finding Latent Causes in Causal Networks: an Efficient Approach Based on Markov Blankets</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-86" href="#">nips2008-86</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>86 nips-2008-Finding Latent Causes in Causal Networks: an Efficient Approach Based on Markov Blankets</h1>
<br/><p>Source: <a title="nips-2008-86-pdf" href="http://papers.nips.cc/paper/3525-finding-latent-causes-in-causal-networks-an-efficient-approach-based-on-markov-blankets.pdf">pdf</a></p><p>Author: Jean-philippe Pellet, AndrÄ&sbquo;Ĺ  Elisseeff</p><p>Abstract: Causal structure-discovery techniques usually assume that all causes of more than one variable are observed. This is the so-called causal sufficiency assumption. In practice, it is untestable, and often violated. In this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data. Similar to algorithms such as IC* and FCI, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables. Assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditionalindependence tests w.r.t. the number of variables. We show with experiments that our algorithm is comparable to the state-of-the-art FCI algorithm in accuracy, while being several orders of magnitude faster on large problems. We conclude that MBCS* makes a new range of causally insufficient problems computationally tractable. Keywords: Graphical Models, Structure Learning, Causal Inference. 1 Introduction: Task Definition & Related Work The statistical definition of causality pioneered by Pearl (2000) and Spirtes et al. (2001) has shed new light on how to detect causation. Central in this approach is the automated detection of causeeffect relationships using observational (i.e., non-experimental) data. This can be a necessary task, as in many situations, performing randomized controlled experiments to unveil causation can be impossible, unethical , or too costly. When the analysis deals with variables that cannot be manipulated, being able to learn from data collected by observing the running system is the only possibility. It turns out that learning the full causal structure of a set of variables is, in its most general form , impossible. If we suppose that the</p><p>Reference: <a title="nips-2008-86-reference" href="../nips2008_reference/nips-2008-Finding_Latent_Causes_in_Causal_Networks%3A_an_Efficient_Approach_Based_on_Markov_Blankets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 In this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data. [sent-8, score-0.344]
</p><p>2 Similar to algorithms such as IC* and FCI, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables. [sent-9, score-0.398]
</p><p>3 Assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditionalindependence tests w. [sent-10, score-0.094]
</p><p>4 We conclude that MBCS* makes a new range of causally insufficient problems computationally tractable. [sent-15, score-0.072]
</p><p>5 1 Introduction: Task Definition & Related Work The statistical definition of causality pioneered by Pearl (2000) and Spirtes et al. [sent-17, score-0.134]
</p><p>6 This can be a necessary task, as in many situations, performing randomized controlled experiments to unveil causation can be impossible, unethical , or too costly. [sent-22, score-0.112]
</p><p>7 It turns out that learning the full causal structure of a set of variables is, in its most general form , impossible. [sent-24, score-0.248]
</p><p>8 This class can be represented by a partially directed acyclic graph (PDAG), where arcs between variables may be undirected, indicating that both directions are equaJly possible given the data. [sent-26, score-0.131]
</p><p>9 This is know as the problem of causal underdetermination (Pearl, 2000). [sent-27, score-0.204]
</p><p>10 Common to most structure-learning algorithms are three important assumptions which ensure the correctness of the causal claims entailed by the returned PDAG (see Scheines, 1997, for a more extensive discussion of these assumptions and of their implications). [sent-28, score-0.307]
</p><p>11 First, the causal Markov condition states that every variable is independent of its non-effects given its direct causes. [sent-29, score-0.204]
</p><p>12 It implies that every dependency can be explained by some form of causation (direct, indirect, common cause, or any combination). [sent-30, score-0.112]
</p><p>13 Third, causal sufficiency of the data states that every common cause for two variables in V is also in V. [sent-34, score-0.385]
</p><p>14 Causal sufficiency often appears as the most controversial assumption as it is generally considered impossible to ensure that all possible causes are measured-there is no such thing as a closed world. [sent-35, score-0.132]
</p><p>15 In this paper, we are interested in relaxing causal sufficiency: we do not require the data to contain all common causes of pairs of variables. [sent-36, score-0.243]
</p><p>16 Some of the few algorithms that relax causal sufficiency are Inductive Causation* (IC*) by Pearl and Verma (1991); Pearl (2000), and Fast Causal Inference (FCI) by Spirtes et al. [sent-37, score-0.297]
</p><p>17 The kind of graph IC* and FCI return is known as a partial ancestral graph (PAG), which indicate for each link whether it (potentially) is the manifestation of a hidden common cause for the two linked variables. [sent-39, score-0.476]
</p><p>18 Assuming continuous variables with linear causal influences, Silva et al. [sent-40, score-0.248]
</p><p>19 (2006) recover hidden variables that are the cause for more than two observed variables, to infer the relationships between the hidden variables themselves. [sent-41, score-0.25]
</p><p>20 They check additional constraints on the covariance matrix, known as tetrad constraints (Scheines et aI. [sent-42, score-0.046]
</p><p>21 There are more specialized techniques to deal with hidden variables. [sent-44, score-0.059]
</p><p>22 (2001) look for structural signatures of hidden variables in a learned DAG model. [sent-46, score-0.103]
</p><p>23 (1999) describe a technique that looks for violation of the Markov condition to infer the presence of latent variables in Bayesian networks. [sent-48, score-0.106]
</p><p>24 Once a hidden variable is identified, Elidan and Friedman (2001) discuss how to assign it a given dimensionality to best model its interactions with the observed variables. [sent-49, score-0.059]
</p><p>25 Correctness proofs are provided in the supplemental material l . [sent-55, score-0.062]
</p><p>26 Notation Throughout this paper, uppercase capitals such as X and Y denote variables or nodes in a graph and sets of variables are set in boldface, such as V. [sent-56, score-0.148]
</p><p>27 Hand L (possibly with indices) denote latent (unobserved) variables. [sent-57, score-0.062]
</p><p>28 2 Mixed Ancestral Graphs & Partial Ancestral Graphs In this section, we first introduce the notation of mixed ancestral graphs (MAGs) and partial ancestral graphs (pAGs) used by Spirtes et al. [sent-60, score-0.333]
</p><p>29 1 (V-structure) In a causal DAG, a V-structure is a triplet X ---> Z f - Y, where X and Yare nonadjacent. [sent-64, score-0.204]
</p><p>30 Z is then called an unshielded collider for X and Y. [sent-65, score-0.108]
</p><p>31 This is the base fact that allows initial edge orientation in causal structure learning. [sent-68, score-0.284]
</p><p>32 Let us now suppose we are learning from data whose (unknown) actual causal DAG is: (2)  Further assume that HI and H2 are hidden. [sent-69, score-0.204]
</p><p>33 Assuming the adjacencies X - Y - Z - W have been found, conditional-independence tests will reveal that (X Jl Z) and (X . [sent-70, score-0.144]
</p><p>34 (3)  Actually, (3) is the projection of the latent structure in (2). [sent-80, score-0.088]
</p><p>35 In the projection of a latent structure as defined by Pearl (2000), all hidden variables are parentless and have only two direct effects. [sent-81, score-0.191]
</p><p>36 Verma (1993) proved that any hidden structure has at least one projection. [sent-82, score-0.059]
</p><p>37 Notice that we cannot recover the information about the two separate hidden variables HI and H 2 . [sent-83, score-0.103]
</p><p>38 In the projection, information can thus be lost with respect to the true latent structure. [sent-84, score-0.062]
</p><p>39 Whereas causally sufficient datasets are represented as DAGs and learned as PDAGs to represent independence-equivalent DAGs, the projection of latent structures is represented by special graphs known as mixed ancestral graphs (MAGs) (Spirtes et aI. [sent-85, score-0.347]
</p><p>40 , 2001), which allow for bidirected arrows to represent a hidden cause for a pair of variables. [sent-86, score-0.13]
</p><p>41 Independence-equivalent MAGs are represented by partial ancestral graphs (PAGs). [sent-87, score-0.185]
</p><p>42 Additionally, we also use the notation X <--* ~ <--* Y to indicate that Z is a definite noncollider for X and Y, such that any of X . [sent-95, score-0.108]
</p><p>43 4 Efficient Structure Learning with the MBCS* Algorithm In this section, we propose a PAG-Iearning algorithm, MBCS*, which is more efficient than FCI in the sense that it performs much fewer conditional-independence tests, whose average conditioningset size is smaller. [sent-111, score-0.068]
</p><p>44 MBCS* proceeds in three steps: first, it detects the Markov blankets for each variable; second, it examines the triangle structures to identify colliders and noncolliders; finally, it uses the same orientation rules as FCI to obtain the maximally oriented PAG. [sent-114, score-0.305]
</p><p>45 We detail the first two steps below; the orientation rules are the same as for FCI. [sent-115, score-0.108]
</p><p>46 1  Step 1: Learning the Markov Blanket  The first phase of MBCS * builds an undirected graph where each variable is connected to all members of its Markov blanket. [sent-117, score-0.159]
</p><p>47 1 (Markov blanket) The Markov blanket of a node X is the smallest set of variables Mb(X) such that 'VY E V \ Mb(X) \ {X} : (X Jl Y I Mb(X)). [sent-119, score-0.163]
</p><p>48 In a DAG, it corresponds to the parents, children, and children's parents (spouses) of X. [sent-121, score-0.068]
</p><p>49 ) maximally oriented partial ancestral graph  1: 9 f-- fully connected graph over V 2: i f-- 0  3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15 : 16: 17:  II Detect adjacencies while :3(X - Y) s. [sent-126, score-0.442]
</p><p>50 INb(X)1 > i do for each S <:;; Nb (X) \ {Y} of size i do if (X JL Y I S) then remove link X - Y from 9 S Xy, S yX f-- S break from loop line 4 end if end for end for if--i+l end while for each X - Z - Y s. [sent-130, score-0.315]
</p><p>51 X , Y nonadjacent do if Z S XY then orient as X . [sent-146, score-0.172]
</p><p>52 Z f-* Y with S *-0 Z and S E S XY do orient as S . [sent-193, score-0.104]
</p><p>53 Y where X is adjacent to Z on 11" and X , Z, Yare a triangle then 7: if S SY exists and Z S SY then orient as X . [sent-206, score-0.138]
</p><p>54 Y 9: end if 10: end for 11: end while 0-;<  II Rule 1 II Rule 2 II Rule 3 II Rule 4  tt  Algorithm 3 Input:  V : I :  Output:  g:  1:  9  9 = MBCS*(V, J) set of observed variables a conditional-independence oracle, called with the notation ( . [sent-225, score-0.24]
</p><p>55 ) maximally oriented partial ancestral graph  II Initialization f-- empty graph over V  II Find Markov blankets (Grow-Shrink) 2: for each X E V do 3: S f-- empty set of Markov blanket variables while:3Y E V \ {X} s. [sent-228, score-0.566]
</p><p>56 (X JL Y I S \ {Y} ) do 7: remove Y from S 8: for each Y E S do add link X 0-0 Y 9: end for II Add noncollider constraints 10: for each X 0-0 Z 0-0 Y s. [sent-233, score-0.264]
</p><p>57 X , Y nonadjacent do 11 : mark as noncollider X 0-0 Z 0-0 Y 12: end for  13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26:  II Adjust local structures (Collider Set search) C f-- empty list of collider-orientation directives for each X . [sent-235, score-0.254]
</p><p>58 Y in a fully connected triangle do if :3collider set Z <:;; Tri(X - Y) then S XY f-- d-separating set for (X , Y) remove link X . [sent-241, score-0.171]
</p><p>59 Y from 9 for each Z E Z do add ordered triplet (X , Z, Y) to C for each Z E S XY do mark as noncollider X . [sent-247, score-0.133]
</p><p>60 Y end if end for for each orientation directive (X, Z , Y) E C do if X . [sent-259, score-0.186]
</p><p>61 Z f-* Y end for  27: return ORIENTMAXIMALLY(9 , \f(X ,Y):  S XY )  Property 4. [sent-278, score-0.053]
</p><p>62 ) We use algorithmic ideas from Margaritis and Thrun (1999) to learn the Markov blanket of a node with a linear number of conditional-independence tests (proof in the supplemental material of Margaritis and Thrun , 1999). [sent-281, score-0.275]
</p><p>63 The resulting graph is an undirected graph called moral graph where each node is connected to its Markov blanket. [sent-283, score-0.314]
</p><p>64 Therefore, it contains spurious links to its spouses, to members of its district, to members of its children's district, and to parents of nodes in those districts, which we all call SD links (for Spouse/District). [sent-284, score-0.2]
</p><p>65 2  Step 2: Removing the SD Links  In the second step of MBCS*, each undirected edge must be identified as either an SD link to be removed, or a true link of the original MAG to be kept. [sent-287, score-0.21]
</p><p>66 Direct parents and children are dependent given any conditioning set, while spouses and district members (and their parents) can be made independent. [sent-288, score-0.317]
</p><p>67 For each link X - Y, a search is thus performed to try to d-separate the two connected nodes. [sent-289, score-0.11]
</p><p>68 This search can be limited to the smallest of the Markov blankets of X and Y, as by definition they contain all nodes that can minimally make them independent from each other, provided they are linked by an SD link. [sent-290, score-0.201]
</p><p>69 If such a d-separating set S Xy is found, the link is removed. [sent-291, score-0.076]
</p><p>70 Interestingly, identifying a d-separating set SXY also identifies the collider set for X and Y. [sent-292, score-0.149]
</p><p>71 3 (Collider set) In an undirected graph 9 over V, let Thi(X - Y) (X , Y adjacent) be the set of all vertices that form a triangle with X and Y. [sent-294, score-0.127]
</p><p>72 Suppose that 9 is the moral graph of the DAG or MAG representing the causal structure of a faithful dataset. [sent-295, score-0.299]
</p><p>73 Collider sets are useful because each node in them satisfies the property of a collider (1) and reveals a V-structure. [sent-298, score-0.14]
</p><p>74 Suppose (X Jl Y I SXy): then each node Z (connected by a non-SD link to both X and Y) not in S Xy is a collider. [sent-299, score-0.108]
</p><p>75 S xy , the only structural possibility is to have arrow head pointing into Z by the definition of dseparation (Pearl, 1988). [sent-315, score-0.285]
</p><p>76 Similarly, if Z E SXY, then any orientation is possible save for a collider. [sent-316, score-0.08]
</p><p>77 Note that more noncollider constraints are added in line 11: in the case X Z Y with X , Y nonadjacent, we know that Z cannot be a V-structure owing to the following lemma. [sent-318, score-0.108]
</p><p>78 4 In the moral graph gm of a DAG or a MAG g, whenever the pattern X occurs in g, then X and Yare linked in gm. [sent-320, score-0.12]
</p><p>79 )  ;<--7  Z  +--->  Y  In practice, the search for collider sets and simultaneously for d-separating sets in lines 15 and 16 is performed following the implementation proposed by Pellet and Elisseeff (2008). [sent-322, score-0.108]
</p><p>80 In the supplemental material to this paper, we prove that MBCS* correctly identifies all adjacencies and V-structures. [sent-324, score-0.153]
</p><p>81 The final orientation step (Algorithm 2) requires d-separating-set information for Rules 3 and 4: we also prove that MBCS* provides all necessary information. [sent-325, score-0.08]
</p><p>82 Conditioning  Table 1: Comparison of MBCS* and FCI where conditional-independence tests are done using a d-separation oracle. [sent-329, score-0.094]
</p><p>83 We report the number of tests t; the weighted number of tests wt, where each test contributes to wt a summand equal to the size of its conditioning set; and the ratio of t for FCI over the t for MBCS*: r ~ t (FCI) I t (MBCS*). [sent-330, score-0.251]
</p><p>84 Figure 3: Comparison of MBCS* and FCI where conditional-independence tests are done using Fisher's z-test. [sent-352, score-0.094]
</p><p>85 We compare the number of edge errors (missing/extraneous) and orientation errors (including missing/extraneous hidden variables). [sent-353, score-0.139]
</p><p>86 In a second series, multivariate Gaussian datasets (with 500 datapoints) were sampled from the networks and data corresponding to the hidden variables were removed. [sent-357, score-0.103]
</p><p>87 Table 1 shows in the columns named t that MBCS* makes up to 3 orders of magnitude fewer conditional-independence tests than FCI on the tested networks. [sent-364, score-0.094]
</p><p>88 As the number of tests alone does not reflect the quality of the algorithm, we also list in the wt column a weighted sum of tests, where each test is weighted by the size of its conditioning set. [sent-365, score-0.157]
</p><p>89 On each learning problem, the returned PAGs have been checked for correctness with respect to the maximally oriented PAG go theoretically obtainable (as returned by the first series of experiments). [sent-371, score-0.184]
</p><p>90 The discrepancies were classified either as edge errors (when an arc was missing or extraneous in the returned PAG W. [sent-372, score-0.061]
</p><p>91 go), or orientation errors (when a predicted arc in the returned PAG was indeed present in go, but had a reversed direction or different end points). [sent-375, score-0.194]
</p><p>92 On aIlS learning problems, both edge and orientation errors are similar within the margin indicated by the standarddeviation error bars. [sent-376, score-0.08]
</p><p>93 Note that the overall relatively high error rate comes from the failure of statistical tests with limited sample size. [sent-377, score-0.094]
</p><p>94 This indicates that structure learning is a hard problem and that low-sample-size situations where tests typically fail must be investigated further. [sent-378, score-0.094]
</p><p>95 6  Conclusion  With the formalism of MAGs and PAGs, it is possible to learn an independence-equivalence class of projections of latent structures. [sent-379, score-0.062]
</p><p>96 We have shown an algorithm, MBCS*, which is much more efficient than the reference FCI algorithms on networks that are sufficiently sparse, making up to three orders of magnitude fewer conditional-independence tests to retrieve the same structure. [sent-380, score-0.162]
</p><p>97 MBCS* is based on a first phase that identifies the Markov blanket of the underlying MAG, and then makes local adjustments to remove the spurious links and identify all colliders. [sent-382, score-0.189]
</p><p>98 The last step involving orientation rules is the same as for FCI. [sent-383, score-0.108]
</p><p>99 The TETRAD project: Constraint based aids to causal model specification. [sent-447, score-0.204]
</p><p>100 Causal inference in the presence of latent variables and selection bias. [sent-461, score-0.106]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('mbcs', 0.604), ('fci', 0.434), ('causal', 0.204), ('xy', 0.186), ('jl', 0.121), ('ancestral', 0.115), ('causation', 0.112), ('spirtes', 0.112), ('noncollider', 0.108), ('collider', 0.108), ('dag', 0.105), ('orient', 0.104), ('definition', 0.099), ('pearl', 0.095), ('tests', 0.094), ('sufficiency', 0.093), ('blanket', 0.087), ('pag', 0.081), ('pags', 0.081), ('orientation', 0.08), ('blankets', 0.077), ('district', 0.077), ('mag', 0.077), ('mags', 0.077), ('sxy', 0.077), ('link', 0.076), ('parents', 0.068), ('efficient', 0.068), ('nonadjacent', 0.068), ('scheines', 0.068), ('latent', 0.062), ('supplemental', 0.062), ('hailfinder', 0.062), ('spouses', 0.062), ('graph', 0.06), ('hidden', 0.059), ('end', 0.053), ('markov', 0.051), ('adjacencies', 0.05), ('mb', 0.047), ('maximally', 0.047), ('margaritis', 0.046), ('orientmaximally', 0.046), ('pdag', 0.046), ('tetrad', 0.046), ('yare', 0.046), ('zurich', 0.046), ('children', 0.046), ('cause', 0.044), ('elidan', 0.044), ('variables', 0.044), ('causally', 0.041), ('entailed', 0.041), ('identifies', 0.041), ('pellet', 0.041), ('sy', 0.041), ('alarm', 0.04), ('causes', 0.039), ('sd', 0.039), ('oriented', 0.039), ('partial', 0.037), ('sufficient', 0.037), ('tt', 0.037), ('glymour', 0.037), ('ii', 0.036), ('returned', 0.036), ('moral', 0.035), ('dags', 0.035), ('causality', 0.035), ('links', 0.034), ('triangle', 0.034), ('connected', 0.034), ('undirected', 0.033), ('graphs', 0.033), ('node', 0.032), ('conditioning', 0.032), ('members', 0.032), ('boyen', 0.031), ('districts', 0.031), ('faithfulness', 0.031), ('inb', 0.031), ('insufficient', 0.031), ('isxy', 0.031), ('pdags', 0.031), ('thi', 0.031), ('wt', 0.031), ('ic', 0.03), ('rules', 0.028), ('remove', 0.027), ('arcs', 0.027), ('bidirected', 0.027), ('correctness', 0.026), ('projection', 0.026), ('mark', 0.025), ('linked', 0.025), ('oracle', 0.025), ('identified', 0.025), ('verma', 0.025), ('arc', 0.025), ('friedman', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="86-tfidf-1" href="./nips-2008-Finding_Latent_Causes_in_Causal_Networks%3A_an_Efficient_Approach_Based_on_Markov_Blankets.html">86 nips-2008-Finding Latent Causes in Causal Networks: an Efficient Approach Based on Markov Blankets</a></p>
<p>Author: Jean-philippe Pellet, AndrÄ&sbquo;Ĺ  Elisseeff</p><p>Abstract: Causal structure-discovery techniques usually assume that all causes of more than one variable are observed. This is the so-called causal sufficiency assumption. In practice, it is untestable, and often violated. In this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data. Similar to algorithms such as IC* and FCI, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables. Assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditionalindependence tests w.r.t. the number of variables. We show with experiments that our algorithm is comparable to the state-of-the-art FCI algorithm in accuracy, while being several orders of magnitude faster on large problems. We conclude that MBCS* makes a new range of causally insufficient problems computationally tractable. Keywords: Graphical Models, Structure Learning, Causal Inference. 1 Introduction: Task Definition & Related Work The statistical definition of causality pioneered by Pearl (2000) and Spirtes et al. (2001) has shed new light on how to detect causation. Central in this approach is the automated detection of causeeffect relationships using observational (i.e., non-experimental) data. This can be a necessary task, as in many situations, performing randomized controlled experiments to unveil causation can be impossible, unethical , or too costly. When the analysis deals with variables that cannot be manipulated, being able to learn from data collected by observing the running system is the only possibility. It turns out that learning the full causal structure of a set of variables is, in its most general form , impossible. If we suppose that the</p><p>2 0.315559 <a title="86-tfidf-2" href="./nips-2008-Integrating_Locally_Learned_Causal_Structures_with_Overlapping_Variables.html">108 nips-2008-Integrating Locally Learned Causal Structures with Overlapping Variables</a></p>
<p>Author: David Danks, Clark Glymour, Robert E. Tillman</p><p>Abstract: In many domains, data are distributed among datasets that share only some variables; other recorded variables may occur in only one dataset. While there are asymptotically correct, informative algorithms for discovering causal relationships from a single dataset, even with missing values and hidden variables, there have been no such reliable procedures for distributed data with overlapping variables. We present a novel, asymptotically correct procedure that discovers a minimal equivalence class of causal DAG structures using local independence information from distributed data of this form and evaluate its performance using synthetic and real-world data against causal discovery algorithms for single datasets and applying Structural EM, a heuristic DAG structure learning procedure for data with missing values, to the concatenated data.</p><p>3 0.13753437 <a title="86-tfidf-3" href="./nips-2008-Nonlinear_causal_discovery_with_additive_noise_models.html">153 nips-2008-Nonlinear causal discovery with additive noise models</a></p>
<p>Author: Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jan R. Peters, Bernhard Schölkopf</p><p>Abstract: The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models with additive noise are often used because these models are well understood and there are well-known methods to ﬁt them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that the basic linear framework can be generalized to nonlinear models. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identiﬁed. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identiﬁcation power provided by nonlinearities. 1</p><p>4 0.10377559 <a title="86-tfidf-4" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>Author: Neil D. Lawrence, Magnus Rattray, Michalis K. Titsias</p><p>Abstract: Sampling functions in Gaussian process (GP) models is challenging because of the highly correlated posterior distribution. We describe an efﬁcient Markov chain Monte Carlo algorithm for sampling from the posterior process of the GP model. This algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function. At each iteration, the algorithm proposes new values for the control variables and generates the function from the conditional GP prior. The control variable input locations are found by minimizing an objective function. We demonstrate the algorithm on regression and classiﬁcation problems and we use it to estimate the parameters of a differential equation model of gene regulation. 1</p><p>5 0.05274022 <a title="86-tfidf-5" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>Author: Erik Talvitie, Satinder P. Singh</p><p>Abstract: We present a novel mathematical formalism for the idea of a “local model” of an uncontrolled dynamical system, a model that makes only certain predictions in only certain situations. As a result of its restricted responsibilities, a local model may be far simpler than a complete model of the system. We then show how one might combine several local models to produce a more detailed model. We demonstrate our ability to learn a collection of local models on a large-scale example and do a preliminary empirical comparison of learning a collection of local models and some other model learning methods. 1</p><p>6 0.052428473 <a title="86-tfidf-6" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>7 0.046394087 <a title="86-tfidf-7" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>8 0.043487169 <a title="86-tfidf-8" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>9 0.039843548 <a title="86-tfidf-9" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>10 0.036326934 <a title="86-tfidf-10" href="./nips-2008-Fast_Prediction_on_a_Tree.html">84 nips-2008-Fast Prediction on a Tree</a></p>
<p>11 0.035674322 <a title="86-tfidf-11" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>12 0.035508662 <a title="86-tfidf-12" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>13 0.034828283 <a title="86-tfidf-13" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>14 0.034113597 <a title="86-tfidf-14" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>15 0.033751689 <a title="86-tfidf-15" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>16 0.032735627 <a title="86-tfidf-16" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>17 0.031953894 <a title="86-tfidf-17" href="./nips-2008-Hierarchical_Fisher_Kernels_for_Longitudinal_Data.html">97 nips-2008-Hierarchical Fisher Kernels for Longitudinal Data</a></p>
<p>18 0.03162434 <a title="86-tfidf-18" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<p>19 0.03159672 <a title="86-tfidf-19" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>20 0.031430487 <a title="86-tfidf-20" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.104), (1, -0.0), (2, 0.025), (3, 0.023), (4, 0.061), (5, -0.091), (6, 0.021), (7, 0.095), (8, 0.049), (9, -0.048), (10, -0.031), (11, 0.098), (12, 0.004), (13, -0.109), (14, 0.083), (15, -0.071), (16, 0.058), (17, -0.072), (18, -0.123), (19, -0.046), (20, -0.021), (21, 0.157), (22, 0.129), (23, -0.055), (24, 0.312), (25, -0.197), (26, -0.173), (27, -0.059), (28, -0.153), (29, 0.172), (30, -0.041), (31, -0.004), (32, 0.083), (33, -0.158), (34, 0.02), (35, -0.069), (36, -0.056), (37, -0.048), (38, -0.057), (39, 0.022), (40, 0.026), (41, -0.04), (42, 0.023), (43, 0.085), (44, -0.082), (45, -0.002), (46, 0.005), (47, -0.0), (48, 0.012), (49, 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.95838726 <a title="86-lsi-1" href="./nips-2008-Finding_Latent_Causes_in_Causal_Networks%3A_an_Efficient_Approach_Based_on_Markov_Blankets.html">86 nips-2008-Finding Latent Causes in Causal Networks: an Efficient Approach Based on Markov Blankets</a></p>
<p>Author: Jean-philippe Pellet, AndrÄ&sbquo;Ĺ  Elisseeff</p><p>Abstract: Causal structure-discovery techniques usually assume that all causes of more than one variable are observed. This is the so-called causal sufficiency assumption. In practice, it is untestable, and often violated. In this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data. Similar to algorithms such as IC* and FCI, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables. Assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditionalindependence tests w.r.t. the number of variables. We show with experiments that our algorithm is comparable to the state-of-the-art FCI algorithm in accuracy, while being several orders of magnitude faster on large problems. We conclude that MBCS* makes a new range of causally insufficient problems computationally tractable. Keywords: Graphical Models, Structure Learning, Causal Inference. 1 Introduction: Task Definition & Related Work The statistical definition of causality pioneered by Pearl (2000) and Spirtes et al. (2001) has shed new light on how to detect causation. Central in this approach is the automated detection of causeeffect relationships using observational (i.e., non-experimental) data. This can be a necessary task, as in many situations, performing randomized controlled experiments to unveil causation can be impossible, unethical , or too costly. When the analysis deals with variables that cannot be manipulated, being able to learn from data collected by observing the running system is the only possibility. It turns out that learning the full causal structure of a set of variables is, in its most general form , impossible. If we suppose that the</p><p>2 0.90631151 <a title="86-lsi-2" href="./nips-2008-Integrating_Locally_Learned_Causal_Structures_with_Overlapping_Variables.html">108 nips-2008-Integrating Locally Learned Causal Structures with Overlapping Variables</a></p>
<p>Author: David Danks, Clark Glymour, Robert E. Tillman</p><p>Abstract: In many domains, data are distributed among datasets that share only some variables; other recorded variables may occur in only one dataset. While there are asymptotically correct, informative algorithms for discovering causal relationships from a single dataset, even with missing values and hidden variables, there have been no such reliable procedures for distributed data with overlapping variables. We present a novel, asymptotically correct procedure that discovers a minimal equivalence class of causal DAG structures using local independence information from distributed data of this form and evaluate its performance using synthetic and real-world data against causal discovery algorithms for single datasets and applying Structural EM, a heuristic DAG structure learning procedure for data with missing values, to the concatenated data.</p><p>3 0.71471041 <a title="86-lsi-3" href="./nips-2008-Nonlinear_causal_discovery_with_additive_noise_models.html">153 nips-2008-Nonlinear causal discovery with additive noise models</a></p>
<p>Author: Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jan R. Peters, Bernhard Schölkopf</p><p>Abstract: The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models with additive noise are often used because these models are well understood and there are well-known methods to ﬁt them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that the basic linear framework can be generalized to nonlinear models. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identiﬁed. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identiﬁcation power provided by nonlinearities. 1</p><p>4 0.43521109 <a title="86-lsi-4" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>Author: Rama Natarajan, Iain Murray, Ladan Shams, Richard S. Zemel</p><p>Abstract: We explore a recently proposed mixture model approach to understanding interactions between conﬂicting sensory cues. Alternative model formulations, differing in their sensory noise models and inference methods, are compared based on their ﬁt to experimental data. Heavy-tailed sensory likelihoods yield a better description of the subjects’ response behavior than standard Gaussian noise models. We study the underlying cause for this result, and then present several testable predictions of these models. 1</p><p>5 0.34747064 <a title="86-lsi-5" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>Author: Erik Talvitie, Satinder P. Singh</p><p>Abstract: We present a novel mathematical formalism for the idea of a “local model” of an uncontrolled dynamical system, a model that makes only certain predictions in only certain situations. As a result of its restricted responsibilities, a local model may be far simpler than a complete model of the system. We then show how one might combine several local models to produce a more detailed model. We demonstrate our ability to learn a collection of local models on a large-scale example and do a preliminary empirical comparison of learning a collection of local models and some other model learning methods. 1</p><p>6 0.34270838 <a title="86-lsi-6" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>7 0.32188913 <a title="86-lsi-7" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>8 0.30848765 <a title="86-lsi-8" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>9 0.2612792 <a title="86-lsi-9" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<p>10 0.24964665 <a title="86-lsi-10" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>11 0.23021844 <a title="86-lsi-11" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>12 0.21927045 <a title="86-lsi-12" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>13 0.21357988 <a title="86-lsi-13" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>14 0.21305978 <a title="86-lsi-14" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>15 0.20414472 <a title="86-lsi-15" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>16 0.19739193 <a title="86-lsi-16" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>17 0.19647925 <a title="86-lsi-17" href="./nips-2008-Goal-directed_decision_making_in_prefrontal_cortex%3A_a_computational_framework.html">94 nips-2008-Goal-directed decision making in prefrontal cortex: a computational framework</a></p>
<p>18 0.19376597 <a title="86-lsi-18" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>19 0.19185251 <a title="86-lsi-19" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>20 0.18623994 <a title="86-lsi-20" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, 0.323), (6, 0.054), (7, 0.045), (12, 0.029), (21, 0.046), (27, 0.036), (28, 0.132), (54, 0.011), (57, 0.052), (59, 0.026), (63, 0.02), (64, 0.037), (71, 0.013), (77, 0.026), (78, 0.01), (83, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74639589 <a title="86-lda-1" href="./nips-2008-Finding_Latent_Causes_in_Causal_Networks%3A_an_Efficient_Approach_Based_on_Markov_Blankets.html">86 nips-2008-Finding Latent Causes in Causal Networks: an Efficient Approach Based on Markov Blankets</a></p>
<p>Author: Jean-philippe Pellet, AndrÄ&sbquo;Ĺ  Elisseeff</p><p>Abstract: Causal structure-discovery techniques usually assume that all causes of more than one variable are observed. This is the so-called causal sufficiency assumption. In practice, it is untestable, and often violated. In this paper, we present an efficient causal structure-learning algorithm, suited for causally insufficient data. Similar to algorithms such as IC* and FCI, the proposed approach drops the causal sufficiency assumption and learns a structure that indicates (potential) latent causes for pairs of observed variables. Assuming a constant local density of the data-generating graph, our algorithm makes a quadratic number of conditionalindependence tests w.r.t. the number of variables. We show with experiments that our algorithm is comparable to the state-of-the-art FCI algorithm in accuracy, while being several orders of magnitude faster on large problems. We conclude that MBCS* makes a new range of causally insufficient problems computationally tractable. Keywords: Graphical Models, Structure Learning, Causal Inference. 1 Introduction: Task Definition & Related Work The statistical definition of causality pioneered by Pearl (2000) and Spirtes et al. (2001) has shed new light on how to detect causation. Central in this approach is the automated detection of causeeffect relationships using observational (i.e., non-experimental) data. This can be a necessary task, as in many situations, performing randomized controlled experiments to unveil causation can be impossible, unethical , or too costly. When the analysis deals with variables that cannot be manipulated, being able to learn from data collected by observing the running system is the only possibility. It turns out that learning the full causal structure of a set of variables is, in its most general form , impossible. If we suppose that the</p><p>2 0.59203434 <a title="86-lda-2" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>Author: Joshua W. Robinson, Alexander J. Hartemink</p><p>Abstract: A principled mechanism for identifying conditional dependencies in time-series data is provided through structure learning of dynamic Bayesian networks (DBNs). An important assumption of DBN structure learning is that the data are generated by a stationary process—an assumption that is not true in many important settings. In this paper, we introduce a new class of graphical models called non-stationary dynamic Bayesian networks, in which the conditional dependence structure of the underlying data-generation process is permitted to change over time. Non-stationary dynamic Bayesian networks represent a new framework for studying problems in which the structure of a network is evolving over time. We deﬁne the non-stationary DBN model, present an MCMC sampling algorithm for learning the structure of the model from time-series data under different assumptions, and demonstrate the effectiveness of the algorithm on both simulated and biological data. 1</p><p>3 0.46344599 <a title="86-lda-3" href="./nips-2008-Nonlinear_causal_discovery_with_additive_noise_models.html">153 nips-2008-Nonlinear causal discovery with additive noise models</a></p>
<p>Author: Patrik O. Hoyer, Dominik Janzing, Joris M. Mooij, Jan R. Peters, Bernhard Schölkopf</p><p>Abstract: The discovery of causal relationships between a set of observed variables is a fundamental problem in science. For continuous-valued data linear acyclic causal models with additive noise are often used because these models are well understood and there are well-known methods to ﬁt them to data. In reality, of course, many causal relationships are more or less nonlinear, raising some doubts as to the applicability and usefulness of purely linear methods. In this contribution we show that the basic linear framework can be generalized to nonlinear models. In this extended framework, nonlinearities in the data-generating process are in fact a blessing rather than a curse, as they typically provide information on the underlying causal system and allow more aspects of the true data-generating mechanisms to be identiﬁed. In addition to theoretical results we show simulations and some simple real data experiments illustrating the identiﬁcation power provided by nonlinearities. 1</p><p>4 0.46241501 <a title="86-lda-4" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>Author: Edoardo M. Airoldi, David M. Blei, Stephen E. Fienberg, Eric P. Xing</p><p>Abstract: In many settings, such as protein interactions and gene regulatory networks, collections of author-recipient email, and social networks, the data consist of pairwise measurements, e.g., presence or absence of links between pairs of objects. Analyzing such data with probabilistic models requires non-standard assumptions, since the usual independence or exchangeability assumptions no longer hold. In this paper, we introduce a class of latent variable models for pairwise measurements: mixed membership stochastic blockmodels. Models in this class combine a global model of dense patches of connectivity (blockmodel) with a local model to instantiate node-speciﬁc variability in the connections (mixed membership). We develop a general variational inference algorithm for fast approximate posterior inference. We demonstrate the advantages of mixed membership stochastic blockmodel with applications to social networks and protein interaction networks. 1</p><p>5 0.4557893 <a title="86-lda-5" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>Author: Rama Natarajan, Iain Murray, Ladan Shams, Richard S. Zemel</p><p>Abstract: We explore a recently proposed mixture model approach to understanding interactions between conﬂicting sensory cues. Alternative model formulations, differing in their sensory noise models and inference methods, are compared based on their ﬁt to experimental data. Heavy-tailed sensory likelihoods yield a better description of the subjects’ response behavior than standard Gaussian noise models. We study the underlying cause for this result, and then present several testable predictions of these models. 1</p><p>6 0.45549521 <a title="86-lda-6" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>7 0.45491949 <a title="86-lda-7" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>8 0.45432505 <a title="86-lda-8" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>9 0.4540889 <a title="86-lda-9" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>10 0.45312721 <a title="86-lda-10" href="./nips-2008-Integrating_Locally_Learned_Causal_Structures_with_Overlapping_Variables.html">108 nips-2008-Integrating Locally Learned Causal Structures with Overlapping Variables</a></p>
<p>11 0.45101333 <a title="86-lda-11" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>12 0.45026168 <a title="86-lda-12" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>13 0.44899952 <a title="86-lda-13" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>14 0.44686761 <a title="86-lda-14" href="./nips-2008-Improving_on_Expectation_Propagation.html">105 nips-2008-Improving on Expectation Propagation</a></p>
<p>15 0.44642442 <a title="86-lda-15" href="./nips-2008-Clustering_via_LP-based_Stabilities.html">48 nips-2008-Clustering via LP-based Stabilities</a></p>
<p>16 0.44564715 <a title="86-lda-16" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>17 0.44527623 <a title="86-lda-17" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>18 0.44499558 <a title="86-lda-18" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>19 0.44483399 <a title="86-lda-19" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>20 0.44375426 <a title="86-lda-20" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
