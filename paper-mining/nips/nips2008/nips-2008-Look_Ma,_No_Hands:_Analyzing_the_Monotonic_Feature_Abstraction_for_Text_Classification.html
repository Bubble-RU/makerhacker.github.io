<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-128" href="#">nips2008-128</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</h1>
<br/><p>Source: <a title="nips-2008-128-pdf" href="http://papers.nips.cc/paper/3381-look-ma-no-hands-analyzing-the-monotonic-feature-abstraction-for-text-classification.pdf">pdf</a></p><p>Author: Doug Downey, Oren Etzioni</p><p>Abstract: Is accurate classiﬁcation possible in the absence of hand-labeled data? This paper introduces the Monotonic Feature (MF) abstraction—where the probability of class membership increases monotonically with the MF’s value. The paper proves that when an MF is given, PAC learning is possible with no hand-labeled data under certain assumptions. We argue that MFs arise naturally in a broad range of textual classiﬁcation applications. On the classic “20 Newsgroups” data set, a learner given an MF and unlabeled data achieves classiﬁcation accuracy equal to that of a state-of-the-art semi-supervised learner relying on 160 hand-labeled examples. Even when MFs are not given as input, their presence or absence can be determined from a small amount of hand-labeled data, which yields a new semi-supervised learning method that reduces error by 15% on the 20 Newsgroups data. 1</p><p>Reference: <a title="nips-2008-128-reference" href="../nips2008_reference/nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 On the classic “20 Newsgroups” data set, a learner given an MF and unlabeled data achieves classiﬁcation accuracy equal to that of a state-of-the-art semi-supervised learner relying on 160 hand-labeled examples. [sent-9, score-0.153]
</p><p>2 A feature is monotonic when the probability of class membership increases monotonically with that feature’s value, all else being equal. [sent-14, score-0.753]
</p><p>3 For example, it can be shown that Naive Bayes text classiﬁers return probability estimates that are monotonic in the frequency of a word—for the class in which the word is most common. [sent-16, score-0.699]
</p><p>4 Thus, if we are trying to discriminate between documents about New York and Boston, then we expect to ﬁnd that the Naive Bayes feature measuring the frequency of “Giants” in the corpus is an MF for the class New York, and likewise for “Patriots” and Boston. [sent-17, score-0.118]
</p><p>5 In document classiﬁcation, the name of the class is a natural MF—the more times it is repeated in a document, all other things being equal, the more likely it is that the document belongs to the class. [sent-18, score-0.096]
</p><p>6 The term frequency component of this metric is a monotonic feature. [sent-21, score-0.608]
</p><p>7 Instead, this paper presents theoretical and empirical results showing that even relatively weak MFs can be used to induce a noisy labeling over examples, and these examples can then be used to train effective classiﬁers utilizing existing supervised or semi-supervised techniques. [sent-35, score-0.105]
</p><p>8 We prove that the Monotonic Feature (MF) structure guarantees PAC learnability using only unlabeled data, and that MFs are distinct from and complementary to standard biases used in semisupervised learning, including the manifold and cluster assumptions. [sent-37, score-0.209]
</p><p>9 Section 2 formally deﬁnes our problem structure and the properties of monotonic features. [sent-42, score-0.608]
</p><p>10 We assume the following inputs: • A set of zero or more labeled examples DL = {(xi , yi )|i = 1 . [sent-50, score-0.211]
</p><p>11 • A set of zero or more unlabeled examples DU = {(xi )|i = 1 . [sent-57, score-0.148]
</p><p>12 , d} of zero or more monotonic features for the positive class y = 1. [sent-64, score-0.744]
</p><p>13 We further deﬁne CM ⊂ C as the concept class of binary classiﬁers that use only the monotonic features. [sent-71, score-0.688]
</p><p>14 Monotonic features exhibit a monotonically increasing relationship with the probability that an example is a member of the positive class. [sent-73, score-0.135]
</p><p>15 More formally, we deﬁne monotonic features as follows: Deﬁnition 1 A monotonic feature for class y is a feature i ∈ {1, . [sent-74, score-1.478]
</p><p>16 2 • The conditional probability that an example is an element of class y = 1 increases strictly monotonically with the value of xi . [sent-78, score-0.112]
</p><p>17 With this deﬁnition, we can state precisely the monotonic feature structure: Deﬁnition 2 For a learning problem from the input space X of d-tuples x = (x1 , . [sent-82, score-0.671]
</p><p>18 , xd ) to the output space Y, the monotonic feature structure (MFS) holds if and only if at least one of the features i ∈ {1, . [sent-85, score-0.797]
</p><p>19 , d} is a monotonic feature for the positive class y = 1. [sent-88, score-0.698]
</p><p>20 First, monotonic features may be known in the absence of labeled data (|M | > 0, DL = ∅). [sent-90, score-0.898]
</p><p>21 This is the setting considered in previous applications of monotonic features, as discussed in the introduction. [sent-91, score-0.608]
</p><p>22 Second, monotonic features may be unknown, but labeled data may be provided (M = ∅, |DL | > 0); this corresponds to standard semi-supervised learning. [sent-92, score-0.875]
</p><p>23 In this case, the MFS can still be exploited by identifying monotonic features using the labeled data. [sent-93, score-0.875]
</p><p>24 Lastly, both monotonic features and labeled data may be provided (|M |, |DL | > 0). [sent-94, score-0.875]
</p><p>25 3  Theory of Monotonic Features  This section shows that under certain assumptions, knowing the identity of a single monotonic feature is sufﬁcient to PAC learn a target concept from only unlabeled data. [sent-96, score-0.819]
</p><p>26 Further, we prove that monotonic features become more informative relative to labeled examples as the feature set size increases. [sent-97, score-0.991]
</p><p>27 Lastly, we discuss and formally establish distinctions between the monotonic feature abstraction and other semi-supervised techniques. [sent-98, score-0.714]
</p><p>28 We start by introducing the conditional independence assumption, which states that the monotonic features are conditionally independent of the non-monotonic features given the class. [sent-99, score-0.882]
</p><p>29 We show that when the concept class C¬M is learnable in the PAC model with classiﬁcation noise, and the conditional independence assumption holds, then knowledge of a single monotonic feature makes the full concept class C learnable from only unlabeled data. [sent-105, score-1.074]
</p><p>30 2 For convenience, we present our analysis in terms of discrete and ﬁnite monotonic features, but the results can be extended naturally to the continuous case. [sent-107, score-0.608]
</p><p>31 Theorem 4 If the conditional independence assumption is satisﬁed and the concept class C¬M is learnable in the PAC model with classiﬁcation noise, then given a single monotonic feature, C is learnable from unlabeled data only. [sent-108, score-0.931]
</p><p>32 The result follows from Theorem 1 in [2] and an application of Hoeffding bounds to show that the monotonic feature can be used to construct a weakly-useful classiﬁer. [sent-110, score-0.671]
</p><p>33 3 The next theorem demonstrates that monotonic features are relatively more informative than labeled examples as the feature space increases in size. [sent-111, score-1.027]
</p><p>34 This result suggests that MF-based approaches to text classiﬁcation may become increasingly valuable over time, as corpora become larger and the number of distinct words and phrases available to serve as features increases. [sent-112, score-0.184]
</p><p>35 We compare the value of monotonic features and labeled examples in terms of information gain, deﬁned below. [sent-113, score-0.928]
</p><p>36 For convenience, these results are presented using a feature space XB , in which all features are binaryvalued. [sent-114, score-0.172]
</p><p>37 Deﬁnition 5 The information gain with respect to an unlabeled example’s label y provided by a variable v is deﬁned as the reduction in entropy of y when v is given, that is: y =0,1 P (y = y |v) log P (y = y |v) − P (y = y ) log P (y = y ). [sent-115, score-0.163]
</p><p>38 Informally speaking, the ﬁrst property states that the feature space does not have fully redundant features, whereas the second states that examples which are far apart have less dependent labels than those which are close together. [sent-117, score-0.176]
</p><p>39 Deﬁnition 6 A distribution D on (XB , Y) has bounded feature dependence if there exists F > 0 such that the conditional probability PD (xi = r|{xj = rj : j = i}) < 1 − F for all i, r, and sets {xj = rj : j = i} of assignments to one or more xj . [sent-119, score-0.11]
</p><p>40 Deﬁnition 7 A distribution D on (XB , Y) has distance-diminishing information gain if the information gain of an example x with respect to the label of any neighboring example x is less than r KI δI for some δI < 1, where r is the Hamming distance between x and x . [sent-120, score-0.115]
</p><p>41 The following theorem shows that whenever the above properties hold to a sufﬁcient degree, the expected information gain from a labeled example falls as the size of the feature space increases. [sent-121, score-0.29]
</p><p>42 The portion of Theorem 8 which concerns information gain of a labeled example is a version of the well-known “curse of dimensionality” [1], which states that the number of examples needed to estimate a function scales exponentially with the number of dimensions under certain assumptions. [sent-124, score-0.279]
</p><p>43 Theorem 8 differs in detail, however; it states the curse of dimensionality in terms of information gain, making possible a direct comparison with monotonic features. [sent-125, score-0.646]
</p><p>44 features are labeled by hand, we show in Section 4 that MFs can either obviate hand-labeled data, or can be estimated automatically from a small set of hand-labeled instances. [sent-133, score-0.283]
</p><p>45 Co-training [2] is a semi-supervised technique that also considers a partition of the feature space into two distinct “views. [sent-134, score-0.109]
</p><p>46 ” One might ask if monotonic feature classiﬁcation is equivalent to cotraining with the monotonic features serving as one view, and the other features forming the other. [sent-135, score-1.497]
</p><p>47 However, co-training requires labeled data to train classiﬁers for each view, unlike monotonic feature classiﬁcation which can operate without any labeled data. [sent-136, score-0.987]
</p><p>48 Even when labeled data is available, co-training takes the partition of the feature set as input, whereas monotonic features can be detected automatically using the labeled data. [sent-138, score-1.112]
</p><p>49 Also, co-training is an iterative algorithm in which the most likely examples of a class according to one view are used to train a classiﬁer on the other view in a mutually recursive fashion. [sent-139, score-0.112]
</p><p>50 For a given set of monotonic features, however, iterating this process is ineffective, because the mostly likely examples of a class according to the monotonic feature view are ﬁxed by the monotonicity property. [sent-140, score-1.4]
</p><p>51 1  Semi-supervised Smoothness Assumptions  The MFS is provably distinct from certain smoothness properties typically assumed in previous semi-supervised learning methods, known as the cluster and manifold assumptions. [sent-143, score-0.097]
</p><p>52 The manifold assumption states that the distribution P (x) is embedded on a manifold of strictly lower dimension than the full input space X . [sent-145, score-0.128]
</p><p>53 It can be shown that classiﬁcation tasks with the MFS exist for which neither the cluster assumption nor the manifold assumption holds. [sent-146, score-0.113]
</p><p>54 Theorem 9 The monotonic feature structure neither implies nor is implied by the manifold assumption, the cluster assumption, or their conjunction or disjuntion. [sent-149, score-0.738]
</p><p>55 4  Experiments  This section reports on our experiments in utilizing MFs for text classiﬁcation. [sent-150, score-0.095]
</p><p>56 We show that MFs can be employed to perform accurate classiﬁcation even without labeled examples, extending the results from [8] and [3] to a semi-supervised setting. [sent-153, score-0.176]
</p><p>57 1  General Methods for Monotonic Feature Classiﬁcation  Here, we deﬁne a set of abstract methods for incorporating monotonic features into any existing learning algorithm. [sent-156, score-0.717]
</p><p>58 It is applicable when monotonic features are given but labeled examples are not provided. [sent-158, score-0.942]
</p><p>59 The second, M FA - SSL , applies in the standard semi-supervised learning case when some labeled examples are provided, but the identities of the MFs are unknown and must be learned. [sent-159, score-0.256]
</p><p>60 Lastly, M FA - BOTH applies when both labeled data and MF identities are given. [sent-160, score-0.203]
</p><p>61 M FA labels the unlabeled examples DU as elements of class y = 1 iff some monotonic feature value xi for i ∈ M exceeds a threshold τ . [sent-162, score-0.935]
</p><p>62 The threshold is set using unlabeled data so as to maximize the minimum probability mass on either side of the threshold. [sent-163, score-0.095]
</p><p>63 4 This set of bootstrapped examples DL is then fed as training data into a supervised or semi-supervised algorithm Φ(DL , DU ), and M FA outputs the resulting classiﬁer. [sent-164, score-0.105]
</p><p>64 DL = Labeled examples (x, y) such that y = 1 iff a xi > τ for some i ∈ M 2. [sent-168, score-0.11]
</p><p>65 The inputs are M , a set of monotonic features, DU , a set of unlabeled examples, and Φ(L, U ), a supervised or semi-supervised machine learning algorithm which outputs a classiﬁer given labeled data L and unlabeled data U . [sent-170, score-1.011]
</p><p>66 The threshold τ is derived from the unlabeled data and M (see text). [sent-171, score-0.095]
</p><p>67 DL = Examples from DU probabilistically labeled with ΦM (M, DL , DU ) 3. [sent-174, score-0.178]
</p><p>68 The additional inputs include labeled data DL and a machine learning algorithm ΦM (M, L, U ) which given labeled data L and unlabeled data U outputs a probabilistic classiﬁer that uses only monotonic features M . [sent-177, score-1.166]
</p><p>69 When MFs are unknown, but some labeled data is given, the M FA - SSL (Figure 2) algorithm attempts to identify MFs using the labeled training data DL , adding the most strongly monotonic features to the set M . [sent-179, score-1.033]
</p><p>70 Monotonicity strength can be measured in various ways; in our experiments, we rank each feature xi by the quantity f (y, xi ) = r P (y, xi = r)r for each class y. [sent-180, score-0.18]
</p><p>71 5 M FA - SSL adds monotonic features to M in descending order of this value, up to a limit of k = 5 per class. [sent-181, score-0.717]
</p><p>72 6 M FA - SSL then invokes a given machine learning algorithm ΦM (M, DL , DU ) to learn a probabilistic classiﬁer that employs only the monotonic features in M . [sent-182, score-0.717]
</p><p>73 Note that when no monotonic features are identiﬁed, M FA - SSL defaults to the underlying algorithm Φ. [sent-185, score-0.717]
</p><p>74 When monotonic features are known and labeled examples are available, we run a derivative of M FA - SSL denoted as M FA - BOTH. [sent-186, score-0.928]
</p><p>75 The algorithm is the same as M FA - SSL, except that any given monotonic features are added to the learned set in Step 1 of Figure 2, and bootstrapped examples using the given monotonic features (from Step 1 in Figure 1) are added to DL . [sent-187, score-1.508]
</p><p>76 2  Experimental Methodology and Baseline Methods  The task we investigate is to determine from the text of a newsgroup post the newsgroup in which it appeared. [sent-189, score-0.111]
</p><p>77 We used bag-of-word features after converting terms to lowercase, discarding the 100 most frequent terms and all terms appearing only once. [sent-190, score-0.109]
</p><p>78 Below, we present results averaged over four disjoint training sets of variable size, using a disjoint test set of 5,000 documents and an unlabeled set of 10,000 documents. [sent-191, score-0.123]
</p><p>79 We compared the monotonic feature approach with two alternative algorithms, which represent two distinct points in the space of semi-supervised learning algorithms. [sent-192, score-0.701]
</p><p>80 When the identities of monotonic features are given, we obtained one-word monotonic features simply using the newsgroup name, with minor modiﬁcations to expand abbreviations. [sent-196, score-1.512]
</p><p>81 For example, the occurrence count of the term “politics” was 5  This measure is applicable for features with numeric values. [sent-198, score-0.123]
</p><p>82 We also expanded the set of monotonic features to include singular/plural variants. [sent-207, score-0.717]
</p><p>83 We weighted the set of examples labeled using the monotonic features (DL ) equally with the original labeled set, increasing the weight by the equivalent of 200 labeled examples when monotonic features are given to the algorithm. [sent-209, score-2.014]
</p><p>84 3  Experimental Results  The ﬁrst question we investigate is what level of performance M FA can achieve without labeled training data, when monotonic features are given. [sent-211, score-0.875]
</p><p>85 Another way to measure this accuracy is in terms of the number of labeled examples that a baseline semi-supervised technique would require in order to achieve comparable performance. [sent-215, score-0.273]
</p><p>86 We found that M FA outperformed NBEM with up to 160 labeled examples. [sent-216, score-0.158]
</p><p>87 Could the monotonic features, on their own, sufﬁce to directly classify the test data? [sent-218, score-0.608]
</p><p>88 To address this question, the table also reports the performance of using the given monotonic features exclusively to label the test data (MF Alone), without using the semi-supervised technique Φ. [sent-219, score-0.788]
</p><p>89 We ﬁnd that the bootstrapping step provides large beneﬁts to performance; M FA has an effective number of labeled examples eight times more than that of MF Alone. [sent-220, score-0.211]
</p><p>90 33x)  0  20  160 (8x)  Table 1: Performance of M FA when monotonic features are given, and no labeled examples are provided. [sent-222, score-0.928]
</p><p>91 563, which is ten fold that of a Random Baseline classiﬁer that assigns labels randomly, and more than double that of “MF Alone”, which uses only the monotonic features and ignores the other features. [sent-224, score-0.753]
</p><p>92 M FA’s accuracy exceeds that of the NB-EM baseline with 160 labeled training examples, and is eight fold that of “MF Alone”. [sent-225, score-0.236]
</p><p>93 The second question we investigate is whether the monotonic feature approach can improve performance even when the class name is not given. [sent-226, score-0.717]
</p><p>94 M FA - SSL takes the same inputs as the NB-EM technique, without the identities of monotonic features. [sent-227, score-0.677]
</p><p>95 The performance of M FA - SSL as the size of the labeled data set varies is shown in Figure 3. [sent-228, score-0.158]
</p><p>96 The graph shows that for small labeled data sets of size 100-400, M FA - SSL outperforms NB-EM by an average error reduction of 15%. [sent-229, score-0.158]
</p><p>97 One important question is whether M FA SSL ’s performance advantage over NB-EM is in fact due to the presence of monotonic features, or if it instead results from simply utilizing feature selection in Step 2 of Figure 2. [sent-232, score-0.706]
</p><p>98 We investigated this by replacing M FA - SSL’s monotonic feature measure f (y, xi ) with a standard information gain measure, and learning an equal number of features distinct from those selected by M FA - SSL originally. [sent-233, score-0.887]
</p><p>99 Lastly, when both monotonic features and labeled examples are available, M FA - BOTH reduces error over the NB-EM baseline by an average of 31% across the training set sizes shown in Figure 3. [sent-235, score-0.96]
</p><p>100 5  Conclusions  We have presented a general framework for utilizing Monotonic Features (MFs) to perform classiﬁcation without hand-labeled data, or in a semi-supervised setting where monotonic features can be discovered from small numbers of hand-labeled examples. [sent-237, score-0.752]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('monotonic', 0.608), ('mfs', 0.459), ('fa', 0.36), ('ssl', 0.223), ('mf', 0.219), ('dl', 0.182), ('labeled', 0.158), ('features', 0.109), ('du', 0.099), ('unlabeled', 0.095), ('classi', 0.088), ('feature', 0.063), ('newsgroups', 0.059), ('concept', 0.053), ('examples', 0.053), ('lastly', 0.049), ('gain', 0.047), ('text', 0.045), ('identities', 0.045), ('learnable', 0.045), ('textual', 0.045), ('abstraction', 0.043), ('pac', 0.043), ('manifold', 0.042), ('etzioni', 0.042), ('extraction', 0.037), ('utilizing', 0.035), ('newsgroup', 0.033), ('baseline', 0.032), ('cation', 0.032), ('naive', 0.032), ('xb', 0.031), ('distinct', 0.03), ('xi', 0.03), ('disambiguation', 0.03), ('ers', 0.029), ('downey', 0.028), ('turing', 0.028), ('documents', 0.028), ('class', 0.027), ('iff', 0.027), ('washington', 0.027), ('monotonically', 0.026), ('cluster', 0.025), ('document', 0.025), ('monotonicity', 0.025), ('er', 0.025), ('idealization', 0.024), ('xf', 0.024), ('alone', 0.024), ('inputs', 0.024), ('assumption', 0.023), ('bayes', 0.023), ('convenience', 0.023), ('absence', 0.023), ('cities', 0.022), ('theorem', 0.022), ('learner', 0.022), ('bootstrapped', 0.021), ('seattle', 0.021), ('states', 0.021), ('label', 0.021), ('independence', 0.02), ('ie', 0.02), ('probabilistically', 0.02), ('name', 0.019), ('word', 0.019), ('exclusively', 0.019), ('employed', 0.018), ('labels', 0.018), ('city', 0.018), ('fold', 0.018), ('noun', 0.018), ('supervised', 0.017), ('xd', 0.017), ('semisupervised', 0.017), ('broad', 0.017), ('phrase', 0.017), ('curse', 0.017), ('view', 0.016), ('rj', 0.016), ('boston', 0.016), ('redundancy', 0.016), ('technique', 0.016), ('automatically', 0.016), ('aaai', 0.016), ('pseudocode', 0.015), ('reports', 0.015), ('conditional', 0.015), ('linguistics', 0.015), ('nition', 0.015), ('membership', 0.015), ('accuracy', 0.014), ('formal', 0.014), ('outputs', 0.014), ('increases', 0.014), ('exceeds', 0.014), ('cm', 0.014), ('york', 0.014), ('applicable', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="128-tfidf-1" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>Author: Doug Downey, Oren Etzioni</p><p>Abstract: Is accurate classiﬁcation possible in the absence of hand-labeled data? This paper introduces the Monotonic Feature (MF) abstraction—where the probability of class membership increases monotonically with the MF’s value. The paper proves that when an MF is given, PAC learning is possible with no hand-labeled data under certain assumptions. We argue that MFs arise naturally in a broad range of textual classiﬁcation applications. On the classic “20 Newsgroups” data set, a learner given an MF and unlabeled data achieves classiﬁcation accuracy equal to that of a state-of-the-art semi-supervised learner relying on 160 hand-labeled examples. Even when MFs are not given as input, their presence or absence can be determined from a small amount of hand-labeled data, which yields a new semi-supervised learning method that reduces error by 15% on the 20 Newsgroups data. 1</p><p>2 0.15878379 <a title="128-tfidf-2" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>3 0.1471511 <a title="128-tfidf-3" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>Author: Yi Zhang, Artur Dubrawski, Jeff G. Schneider</p><p>Abstract: In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any speciﬁc task in the same space through regularization. In an empirical study, we construct 190 different text classiﬁcation tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classiﬁcation tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the prediction model used.</p><p>4 0.13353392 <a title="128-tfidf-4" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>5 0.077564001 <a title="128-tfidf-5" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>6 0.070418172 <a title="128-tfidf-6" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>7 0.069866158 <a title="128-tfidf-7" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>8 0.066897921 <a title="128-tfidf-8" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>9 0.066296905 <a title="128-tfidf-9" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>10 0.065689191 <a title="128-tfidf-10" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<p>11 0.060922198 <a title="128-tfidf-11" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>12 0.053100787 <a title="128-tfidf-12" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>13 0.052865606 <a title="128-tfidf-13" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>14 0.052721534 <a title="128-tfidf-14" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>15 0.050734721 <a title="128-tfidf-15" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>16 0.050717756 <a title="128-tfidf-16" href="./nips-2008-An_Online_Algorithm_for_Maximizing_Submodular_Functions.html">22 nips-2008-An Online Algorithm for Maximizing Submodular Functions</a></p>
<p>17 0.049927868 <a title="128-tfidf-17" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>18 0.046608448 <a title="128-tfidf-18" href="./nips-2008-Extended_Grassmann_Kernels_for_Subspace-Based_Learning.html">80 nips-2008-Extended Grassmann Kernels for Subspace-Based Learning</a></p>
<p>19 0.046528589 <a title="128-tfidf-19" href="./nips-2008-Translated_Learning%3A_Transfer_Learning_across_Different_Feature_Spaces.html">242 nips-2008-Translated Learning: Transfer Learning across Different Feature Spaces</a></p>
<p>20 0.046112724 <a title="128-tfidf-20" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.138), (1, -0.074), (2, -0.006), (3, -0.067), (4, -0.061), (5, 0.035), (6, 0.065), (7, 0.008), (8, -0.067), (9, 0.028), (10, 0.107), (11, 0.047), (12, -0.14), (13, -0.14), (14, -0.095), (15, 0.002), (16, -0.091), (17, 0.128), (18, 0.054), (19, 0.02), (20, -0.027), (21, -0.027), (22, -0.054), (23, -0.104), (24, 0.004), (25, -0.058), (26, -0.07), (27, -0.018), (28, 0.067), (29, 0.038), (30, -0.05), (31, -0.001), (32, -0.03), (33, -0.066), (34, -0.063), (35, -0.04), (36, 0.073), (37, 0.015), (38, -0.072), (39, 0.058), (40, -0.012), (41, 0.038), (42, -0.097), (43, 0.085), (44, -0.014), (45, -0.036), (46, 0.026), (47, 0.007), (48, -0.097), (49, 0.096)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91845655 <a title="128-lsi-1" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>Author: Doug Downey, Oren Etzioni</p><p>Abstract: Is accurate classiﬁcation possible in the absence of hand-labeled data? This paper introduces the Monotonic Feature (MF) abstraction—where the probability of class membership increases monotonically with the MF’s value. The paper proves that when an MF is given, PAC learning is possible with no hand-labeled data under certain assumptions. We argue that MFs arise naturally in a broad range of textual classiﬁcation applications. On the classic “20 Newsgroups” data set, a learner given an MF and unlabeled data achieves classiﬁcation accuracy equal to that of a state-of-the-art semi-supervised learner relying on 160 hand-labeled examples. Even when MFs are not given as input, their presence or absence can be determined from a small amount of hand-labeled data, which yields a new semi-supervised learning method that reduces error by 15% on the 20 Newsgroups data. 1</p><p>2 0.81242603 <a title="128-lsi-2" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>3 0.70645392 <a title="128-lsi-3" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>4 0.70261866 <a title="128-lsi-4" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>Author: Yi Zhang, Artur Dubrawski, Jeff G. Schneider</p><p>Abstract: In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any speciﬁc task in the same space through regularization. In an empirical study, we construct 190 different text classiﬁcation tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classiﬁcation tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the prediction model used.</p><p>5 0.56951004 <a title="128-lsi-5" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>Author: Vikas Sindhwani, Jianying Hu, Aleksandra Mojsilovic</p><p>Abstract: By attempting to simultaneously partition both the rows (examples) and columns (features) of a data matrix, Co-clustering algorithms often demonstrate surprisingly impressive performance improvements over traditional one-sided row clustering techniques. A good clustering of features may be seen as a combinatorial transformation of the data matrix, effectively enforcing a form of regularization that may lead to a better clustering of examples (and vice-versa). In many applications, partial supervision in the form of a few row labels as well as column labels may be available to potentially assist co-clustering. In this paper, we develop two novel semi-supervised multi-class classiﬁcation algorithms motivated respectively by spectral bipartite graph partitioning and matrix approximation formulations for co-clustering. These algorithms (i) support dual supervision in the form of labels for both examples and/or features, (ii) provide principled predictive capability on out-of-sample test data, and (iii) arise naturally from the classical Representer theorem applied to regularization problems posed on a collection of Reproducing Kernel Hilbert Spaces. Empirical results demonstrate the effectiveness and utility of our algorithms. 1</p><p>6 0.54296494 <a title="128-lsi-6" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>7 0.49730673 <a title="128-lsi-7" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>8 0.47187933 <a title="128-lsi-8" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>9 0.46620911 <a title="128-lsi-9" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>10 0.38216767 <a title="128-lsi-10" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>11 0.37999117 <a title="128-lsi-11" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>12 0.37750453 <a title="128-lsi-12" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>13 0.35269481 <a title="128-lsi-13" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<p>14 0.34774193 <a title="128-lsi-14" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>15 0.34734723 <a title="128-lsi-15" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>16 0.33891958 <a title="128-lsi-16" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>17 0.33061314 <a title="128-lsi-17" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>18 0.33009234 <a title="128-lsi-18" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>19 0.29685998 <a title="128-lsi-19" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>20 0.28809014 <a title="128-lsi-20" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.013), (6, 0.058), (7, 0.067), (12, 0.042), (15, 0.019), (28, 0.129), (39, 0.284), (57, 0.064), (59, 0.022), (63, 0.011), (71, 0.026), (77, 0.03), (83, 0.113)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.74943006 <a title="128-lda-1" href="./nips-2008-The_Recurrent_Temporal_Restricted_Boltzmann_Machine.html">237 nips-2008-The Recurrent Temporal Restricted Boltzmann Machine</a></p>
<p>Author: Ilya Sutskever, Geoffrey E. Hinton, Graham W. Taylor</p><p>Abstract: The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difﬁculty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modiﬁcation of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls. 1</p><p>same-paper 2 0.73694682 <a title="128-lda-2" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>Author: Doug Downey, Oren Etzioni</p><p>Abstract: Is accurate classiﬁcation possible in the absence of hand-labeled data? This paper introduces the Monotonic Feature (MF) abstraction—where the probability of class membership increases monotonically with the MF’s value. The paper proves that when an MF is given, PAC learning is possible with no hand-labeled data under certain assumptions. We argue that MFs arise naturally in a broad range of textual classiﬁcation applications. On the classic “20 Newsgroups” data set, a learner given an MF and unlabeled data achieves classiﬁcation accuracy equal to that of a state-of-the-art semi-supervised learner relying on 160 hand-labeled examples. Even when MFs are not given as input, their presence or absence can be determined from a small amount of hand-labeled data, which yields a new semi-supervised learning method that reduces error by 15% on the 20 Newsgroups data. 1</p><p>3 0.57500696 <a title="128-lda-3" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>4 0.57398003 <a title="128-lda-4" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>Author: Jo-anne Ting, Mrinal Kalakrishnan, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise varies spatially. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. We introduce a Bayesian formulation of nonparametric regression that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efﬁcient, requires no sampling, automatically rejects outliers and has only one prior to be speciﬁed. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law. 1</p><p>5 0.57255536 <a title="128-lda-5" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>6 0.56916857 <a title="128-lda-6" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>7 0.56490964 <a title="128-lda-7" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>8 0.56358773 <a title="128-lda-8" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>9 0.56347835 <a title="128-lda-9" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>10 0.56247115 <a title="128-lda-10" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>11 0.56075537 <a title="128-lda-11" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>12 0.56046879 <a title="128-lda-12" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>13 0.55898064 <a title="128-lda-13" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>14 0.55766743 <a title="128-lda-14" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>15 0.55735475 <a title="128-lda-15" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>16 0.55716509 <a title="128-lda-16" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>17 0.55663681 <a title="128-lda-17" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>18 0.55581129 <a title="128-lda-18" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>19 0.5531131 <a title="128-lda-19" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>20 0.5509392 <a title="128-lda-20" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
