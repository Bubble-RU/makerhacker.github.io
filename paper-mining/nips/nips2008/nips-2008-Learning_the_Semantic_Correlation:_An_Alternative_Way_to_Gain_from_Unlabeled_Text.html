<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-120" href="#">nips2008-120</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</h1>
<br/><p>Source: <a title="nips-2008-120-pdf" href="http://papers.nips.cc/paper/3490-learning-the-semantic-correlation-an-alternative-way-to-gain-from-unlabeled-text.pdf">pdf</a></p><p>Author: Yi Zhang, Artur Dubrawski, Jeff G. Schneider</p><p>Abstract: In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any speciﬁc task in the same space through regularization. In an empirical study, we construct 190 different text classiﬁcation tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classiﬁcation tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the prediction model used.</p><p>Reference: <a title="nips-2008-120-reference" href="../nips2008_reference/nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. [sent-7, score-0.645]
</p><p>2 We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. [sent-8, score-1.297]
</p><p>3 This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any speciﬁc task in the same space through regularization. [sent-9, score-0.751]
</p><p>4 In an empirical study, we construct 190 different text classiﬁcation tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. [sent-10, score-0.775]
</p><p>5 We test the ability of various algorithms to use the mixed unlabeled text to enhance all classiﬁcation tasks. [sent-11, score-0.476]
</p><p>6 Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the prediction model used. [sent-12, score-0.565]
</p><p>7 1 Introduction The availability of large amounts of unlabeled data such as text on the Internet is a strong motivation for research in semi-supervised learning [4]. [sent-13, score-0.471]
</p><p>8 Currently, most of these methods assume that the unlabeled data belong to the same classes or share the generative distributions with the labeled examples, e. [sent-14, score-0.459]
</p><p>9 As indicated in [11], unlabeled data in real-world applications do not necessarily follow the classes or distribution of labeled examples, and semi-supervised learning algorithms that give up this assumption have wider applicability in practice. [sent-17, score-0.459]
</p><p>10 As a result, some algorithms avoid using unlabeled examples directly in model training and instead focus on “changes of representation” that ﬁnd a more informative representation from unlabeled data and use it to encode the labeled examples [4, 1, 11]. [sent-18, score-0.977]
</p><p>11 However, even algorithms for learning good features from unlabeled data still make a strong assumption: those learned high-level features will be relevant to the speciﬁc prediction task at hand. [sent-19, score-0.526]
</p><p>12 The feature extraction on unlabeled data is an unsupervised process and thus a “blindly” learned representation might be irrelevant to a speciﬁc task, especially when the unlabeled data are not from the same task. [sent-22, score-0.844]
</p><p>13 In this paper, we explore the possibility of extracting generally transferable knowledge from unlabeled text without information about the task to be enhanced. [sent-24, score-0.785]
</p><p>14 This knowledge is represented as the semantic correlation structure of the words in the text domain and is shown to be transferable among documents of different themes. [sent-25, score-1.335]
</p><p>15 This structure is extracted using a latent topic model combined with a bootstrapping procedure. [sent-26, score-0.427]
</p><p>16 The use of covariance or correlation structure has already been mentioned in transfer learning [12, 9]. [sent-29, score-0.425]
</p><p>17 A covariance structure can be transferred from a few related tasks to a target task [12] or inferred from meta-features [9]. [sent-30, score-0.254]
</p><p>18 1 Latent Topics and Semantic Structure Latent topics extracted from unlabeled text might be irrelevant to a particular task, but the composition of these topics in terms of word distribution reveals information about the semantic structure of the language. [sent-33, score-1.447]
</p><p>19 Assume a latent topic model [7, 2] of the word space X, or more generally, a latent variable model characterizing the input space X: x = Az  (1)  where x = [x1 , x2 , . [sent-34, score-0.72]
</p><p>20 , zk ]T represents latent variables in the k-dimensional latent space Z. [sent-40, score-0.489]
</p><p>21 For a latent topic model, x corresponds to the bag-of-words vector of a document divided by the document length, z is the distribution of k latent topics in the document, and A is the distribution of p words in k latent topics. [sent-42, score-1.005]
</p><p>22 Different documents have different topic distributions, z, and thus different word distributions, x, but A can be considered an invariant structure of the language. [sent-44, score-0.512]
</p><p>23 Each pdimensional column vector of A denotes the word distribution in a latent topic, and serves as an “observation” in the p dimensional word space, indicating the semantic roles of p  words in this topic. [sent-45, score-0.996]
</p><p>24 Given a large set of k latent topics represented by k p-dimensional vectors {a(,1) , a(,2) , . [sent-46, score-0.363]
</p><p>25 , a(,k) }, we can deﬁne the semantic covariance of p words as follows. [sent-49, score-0.526]
</p><p>26 The semantic covariance of word i and word j is deﬁned as: covs (xi , xj ) =  1 k  k  (ait − a(i,) )(ajt − a(j,) ) = t=1  1 k  k  ait ajt − a(i,) a(j,)  (2)  t=1  where a(i,) is the mean of the ith row in A. [sent-54, score-1.187]
</p><p>27 Naturally, the semantic correlation is: corrs (xi , xj ) =  covs (xi , xj ) covs (xi , xi )covs (xj , xj )  (3)  2. [sent-55, score-1.012]
</p><p>28 2 Comparing Semantic Correlation and Data Correlation Suppose we observe a set of n documents in word space X, denoted by an n × p data matrix DX where each document corresponds to a p-dimensional bag-of-words vector of counts. [sent-56, score-0.529]
</p><p>29 We refer to the correlation between words computed directly from DX as the data correlation. [sent-57, score-0.333]
</p><p>30 This data correlation may not be transferable between tasks since documents from different themes may have distinct topic distributions and word distributions, which lead to different word correlations in data space. [sent-58, score-1.24]
</p><p>31 Here we show intuitively why we expect the data correlation to have limited use across distinct tasks, while we expect the semantic correlation to be transferable. [sent-59, score-0.923]
</p><p>32 We focus on semantic covariance and data covariance, and assume that the bag-of-words vector is divided by the length of the document so that it corresponds to x in eq. [sent-62, score-0.532]
</p><p>33 Documents from different sources have different topic distributions and thus different covariance terms cov(zt , zt′ ) in latent space. [sent-66, score-0.366]
</p><p>34 As a result, the data covariance learned from one source of documents may not be transferable to another class of documents. [sent-67, score-0.6]
</p><p>35 On the other hand, the semantic covariance in eq. [sent-68, score-0.47]
</p><p>36 Intuitively, the data covariance among words must contain some information about the semantic relationship of words. [sent-70, score-0.576]
</p><p>37 If we ignore the effect of the covariance among topics by assuming that latent topics are independently distributed and have the same variance (denoted as σ 2 ), eq. [sent-73, score-0.611]
</p><p>38 (2), we see the similarity between data and semantic covariance. [sent-81, score-0.415]
</p><p>39 In fact, our empirical study shows that data correlation from unlabeled text does contain useful information, but is not as informative as semantic correlation. [sent-82, score-1.194]
</p><p>40 3 Semantic Structure Learning and Informative Regularization l Consider a set of nl labeled documents Dl = {(xl , yi ) ∈ X × Yl , i = 1, · · · nl }, where i p X ⊆ R is the p-dimensional word space, and Yl = {−1, 1} for classiﬁcation and Yl ⊆ R for regression. [sent-83, score-0.618]
</p><p>41 Also assume that a large set of nu unlabeled documents Du = {xu ∈ i X, i = 1, · · · nu } is available. [sent-84, score-0.672]
</p><p>42 In this section we introduce a framework to transfer knowledge from unlabeled text. [sent-86, score-0.436]
</p><p>43 1 proposes an approach to learning the semantic structure of the word space from a set of unlabeled text. [sent-88, score-0.994]
</p><p>44 1 Learning the Semantic Correlation The semantic correlation among words can be estimated using eq. [sent-92, score-0.729]
</p><p>45 (3) by observing a large number of different latent topics. [sent-93, score-0.22]
</p><p>46 However, obtaining a large set of diverse but meaningful topics is hard, since the number of meaningful topics extracted by a latent topic model is usually not very large. [sent-94, score-0.686]
</p><p>47 To solve this problem, resampling techniques such as bootstrapping [5] can be combined with a chosen latent variable model, which provides a principled way to estimate the semantic correlation. [sent-95, score-0.638]
</p><p>48 The procedure is given in Algorithm 1, which uses all the available data D = Du ∪ Dl and a latent variable model M as the input. [sent-96, score-0.243]
</p><p>49 In each iteration it draws an α percentage sample1 from the data and extracts k latent topics from the sample by applying the model M . [sent-98, score-0.386]
</p><p>50 After N iterations, the p × p semantic correlation matrix Σs is estimated from the kN observations of word distribution in latent topics. [sent-99, score-1.03]
</p><p>51 The algorithm requires an appropriate latent variable model M (e. [sent-100, score-0.22]
</p><p>52 , latent dirichlet allocation for text data), and a number k of latent variables extracted each iteration from the sampled data. [sent-102, score-0.668]
</p><p>53 2 Knowledge Transfer by Informative Regularization This section discusses how to use the semantic structure Σs in any speciﬁc learning task deﬁned on the input space X. [sent-105, score-0.522]
</p><p>54 To learn w and b, we minimize a loss function L on the training examples plus a regularization term on w: nl l L(yi , wT xl + b) + λwT w i  argmin w,b  (6)  i=1  Different models correspond to different loss functions [6], e. [sent-113, score-0.337]
</p><p>55 , SVMs use hinge loss, logistic regression uses log-likelihood loss, and ridge regression uses squared error loss. [sent-115, score-0.232]
</p><p>56 The regularization term λwT w = λwT I−1 w is well known to be equivalent to the Bayesian approach that imposes a Gaussian prior with zero mean and an identity correlation matrix. [sent-116, score-0.307]
</p><p>57 The correlation is often set to an identity matrix due to lack of knowledge about the input space. [sent-117, score-0.281]
</p><p>58 If a covariance or correlation structure is known, e. [sent-118, score-0.381]
</p><p>59 , the semantic structure of the word space, the prior can be more informative [12]. [sent-120, score-0.682]
</p><p>60 Incorporating Σs into the Gaussian prior leads to a new regularization term and the resulting model is: nl l L(yi , wT xl + b) + λwT Σ−1 w i s  argmin w,b  (7)  i=1  Extending the discussion on SVMs in [9], all regularized linear models in the form of eq. [sent-121, score-0.3]
</p><p>61 Semantic ˜ ˜i ˜ ˜ s i correlation is transferable to any speciﬁc task and thus can be computed ofﬂine. [sent-124, score-0.527]
</p><p>62 4 Experiments We use the by-date version of the 20-NewsGroups data set2 , where 11314 training and 7532 testing documents are divided by date and denoted as Dtr and Dts here. [sent-127, score-0.302]
</p><p>63 For each task, a few documents in the two newsgroups are selected from Dtr as the labeled examples, denoted as Dl in section 3. [sent-132, score-0.442]
</p><p>64 The rest of the documents in Dtr are used as the unlabeled data, denoted by Du . [sent-133, score-0.644]
</p><p>65 In this sense, semi-supervised learning algorithms that assume the unlabeled data come from the target task or the same generative distribution are unlikely to work very well. [sent-135, score-0.445]
</p><p>66 The test data for each binary task are all the relevant documents in Dts , i. [sent-136, score-0.336]
</p><p>67 , documents in Dts that belong to one of the two chosen newsgroups. [sent-138, score-0.231]
</p><p>68 edu/jrennie/20Newsgroups/  always have Du ∪ Dl = Dtr , so Algorithm 1 is run only once on Dtr to learn the semantic correlation structure Σs that is used by all 190 tasks. [sent-142, score-0.719]
</p><p>69 The documents are well distributed over the 20 newsgroups and thus there are large numbers of training documents in Dtr for each newsgroup. [sent-143, score-0.554]
</p><p>70 To limit the number of labeled examples for each binary prediction task, we use 5%, 10%, 20% of the relevant documents in Dtr as the labeled examples Dl , and the rest of the relevant and all irrelevant documents in Dtr as the unlabeled data Du . [sent-144, score-1.194]
</p><p>71 The testing data for each task are ﬁxed to be all relevant documents in Dts , which is invariant for a task among different tests and random runs. [sent-147, score-0.42]
</p><p>72 Ridge regression (denoted RR) is used as the base classiﬁer: examples are labeled as +1 and −1, and prediction is made by wT x+b > 0. [sent-154, score-0.215]
</p><p>73 Recently a fast semi-supervised SVM using L-2 loss was proposed [13], which makes it possible to handle large-scale unlabeled documents. [sent-156, score-0.365]
</p><p>74 We compare: L2-SVM directly trained on Dl (L2-SV M ), semi-supervised L2SVM trained on Dl ∪ Du (L2-S 3 V M ), and L2-SVM trained on Dl via informative regularization with semantic correlation (L2-SV MIR ). [sent-157, score-0.908]
</p><p>75 The semi-supervised SVM should not work well since the unlabeled data is a mixture from all tasks. [sent-158, score-0.414]
</p><p>76 Therefore, we also test an “oracle” semi-supervised SVM, using labeled examples together with unlabeled examples coming only from the two relevant newsgroups (L2-S 3 V Moracle ). [sent-159, score-0.629]
</p><p>77 For latent dirichlet allocation, we use the implementation at http://chasen. [sent-165, score-0.261]
</p><p>78 We tried k = 10, 20, 30, 50, 100, 200 latent topics with 30 topics performing best. [sent-167, score-0.506]
</p><p>79 For the proposed method, Algorithm 1 uses latent dirichlet allocation with k = 30 topics per sampling, repeats N = 100 iterations, and Σs is estimated from these 3000 latent topics. [sent-168, score-0.69]
</p><p>80 L2-S 3 V M (code available as SVMlin [13]) has a second parameter λu for unlabeled examples, which is set to 1 as in [13]. [sent-169, score-0.365]
</p><p>81 Unlabeled data for L2-S 3V M is downsampled to 3000 documents for each run to make training (and cross-validation) feasible. [sent-170, score-0.254]
</p><p>82 The former measures the effectiveness of using the unlabeled data, while the latter measures the reliability of the knowledge transfer. [sent-173, score-0.392]
</p><p>83 From Tables 1 - 3, IR based methods with semantic correlation signiﬁcantly outperform standard supervised learning, LDA based methods, PCA based methods, and is also generally more effective than IR with data correlation. [sent-174, score-0.706]
</p><p>84 The LDA based algorithms slightly improve the prediction performance when using SVM or logistic regression as the base classiﬁer, while decreasing the performance when using ridge  Table 1: Comparison over 190 tasks, based on SVMs 5%-Test 10%-Test 20%-Test SV M 14. [sent-175, score-0.248]
</p><p>85 This is possibly because the loss function of ridge regression is not a good approximation to the 0/1 classiﬁcation error, and therefore, ridge regression is more sensitive to irrelevant latent features extracted from mixed unlabeled documents. [sent-233, score-1.013]
</p><p>86 The PCA based methods are generally worse than standard supervised learning, which indicates they are sensitive to the mixed unlabeled data. [sent-234, score-0.43]
</p><p>87 In Table 4, the L2-S 3 V M performs worse than standard L2-SV M , showing that traditional semi-supervised learning cannot handle unlabeled data outside the target task. [sent-235, score-0.388]
</p><p>88 This is a very promising result since it shows that information can be gained from other tasks even in excess of what can be gained from a signiﬁcant amount of unlabeled data on the task at hand. [sent-237, score-0.515]
</p><p>89 In conclusion, the empirical results show that the proposed approach is an effective and reliable (also scalable) method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the base prediction model used. [sent-238, score-0.564]
</p><p>90 It is interesting to directly compare the semantic correlation Σs and the data correlation Σ matrices learned from the data. [sent-239, score-0.947]
</p><p>91 We  Table 5: Top 10 distinct word pairs in terms of semantic correlation vs. [sent-243, score-0.81]
</p><p>92 025 have 1617834 entries with higher data correlation and 462972 entries with higher semantic correlation. [sent-264, score-0.717]
</p><p>93 2) However, if we list the top 1000 pairs of words with the largest absolute difference between the two correlations, they all have very high semantic correlation and low data correlation. [sent-266, score-0.725]
</p><p>94 In conclusion, entries in Σs seem to have a power-law distribution where a few pairs of words have very high correlation and the rest have low correlation, which is consistent with our intuition about words. [sent-269, score-0.334]
</p><p>95 However, the data correlation misses highly correlated words found by the semantic correlation even though it generally assigns higher correlation to most word pairs. [sent-270, score-1.434]
</p><p>96 This is consistent with the data correlation not being transferable among documents of different themes. [sent-271, score-0.751]
</p><p>97 When the unlabeled documents are a mixture from different sources, the estimation of data correlation is affected by the fact that the mixture of input documents is not consistent. [sent-272, score-1.156]
</p><p>98 A framework for learning predictive structures from multiple tasks and unlabeled data. [sent-278, score-0.435]
</p><p>99 Learning from labeled and unlabeled data using graph mincuts. [sent-292, score-0.459]
</p><p>100 Text classiﬁcation from labeled and unlabeled documents using em. [sent-329, score-0.667]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('semantic', 0.392), ('unlabeled', 0.365), ('correlation', 0.254), ('dl', 0.242), ('documents', 0.231), ('latent', 0.22), ('transferable', 0.216), ('dtr', 0.194), ('word', 0.164), ('mir', 0.151), ('topics', 0.143), ('sv', 0.14), ('ait', 0.132), ('ajt', 0.13), ('wt', 0.128), ('du', 0.121), ('zt', 0.113), ('xl', 0.112), ('ridge', 0.101), ('newsgroups', 0.092), ('covs', 0.086), ('dts', 0.086), ('ezt', 0.086), ('text', 0.083), ('covariance', 0.078), ('informative', 0.077), ('nl', 0.076), ('labeled', 0.071), ('tasks', 0.07), ('svm', 0.069), ('topic', 0.068), ('moracle', 0.065), ('extracted', 0.064), ('yl', 0.061), ('task', 0.057), ('enhanced', 0.057), ('words', 0.056), ('regularization', 0.053), ('pca', 0.052), ('cov', 0.052), ('structure', 0.049), ('denoted', 0.048), ('regression', 0.045), ('trained', 0.044), ('transfer', 0.044), ('irrelevant', 0.044), ('corrs', 0.043), ('dsamp', 0.043), ('lgr', 0.043), ('lgrir', 0.043), ('mlda', 0.043), ('rrir', 0.043), ('svms', 0.041), ('classi', 0.041), ('xj', 0.041), ('dirichlet', 0.041), ('logistic', 0.041), ('allocation', 0.04), ('document', 0.039), ('examples', 0.038), ('nu', 0.038), ('generally', 0.037), ('argmin', 0.034), ('prediction', 0.032), ('rr', 0.032), ('principal', 0.031), ('scalable', 0.03), ('base', 0.029), ('carnegie', 0.029), ('mellon', 0.029), ('xi', 0.028), ('mixed', 0.028), ('incorporating', 0.028), ('mp', 0.028), ('source', 0.028), ('correlations', 0.027), ('reliable', 0.027), ('among', 0.027), ('knowledge', 0.027), ('regardless', 0.026), ('bootstrapping', 0.026), ('kn', 0.026), ('repeats', 0.026), ('mixture', 0.026), ('regularized', 0.025), ('zk', 0.025), ('raina', 0.025), ('relevant', 0.025), ('ca', 0.025), ('learned', 0.024), ('ir', 0.024), ('space', 0.024), ('learn', 0.024), ('meaningful', 0.024), ('entries', 0.024), ('comparison', 0.024), ('tables', 0.023), ('oracle', 0.023), ('robotics', 0.023), ('data', 0.023)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="120-tfidf-1" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>Author: Yi Zhang, Artur Dubrawski, Jeff G. Schneider</p><p>Abstract: In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any speciﬁc task in the same space through regularization. In an empirical study, we construct 190 different text classiﬁcation tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classiﬁcation tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the prediction model used.</p><p>2 0.25098854 <a title="120-tfidf-2" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>3 0.1881658 <a title="120-tfidf-3" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>Author: Kilian Q. Weinberger, Olivier Chapelle</p><p>Abstract: Applications of multi-class classiﬁcation, such as document categorization, often appear in cost-sensitive settings. Recent work has signiﬁcantly improved the state of the art by moving beyond “ﬂat” classiﬁcation through incorporation of class hierarchies [4]. We present a novel algorithm that goes beyond hierarchical classiﬁcation and estimates the latent semantic space that underlies the class hierarchy. In this space, each class is represented by a prototype and classiﬁcation is done with the simple nearest neighbor rule. The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. We show that our optimization is convex and can be solved efﬁciently for large data sets. Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization. 1</p><p>4 0.16005722 <a title="120-tfidf-4" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>5 0.15778059 <a title="120-tfidf-5" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>6 0.15360081 <a title="120-tfidf-6" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>7 0.15024893 <a title="120-tfidf-7" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>8 0.1471511 <a title="120-tfidf-8" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>9 0.14399493 <a title="120-tfidf-9" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>10 0.13394433 <a title="120-tfidf-10" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>11 0.13158678 <a title="120-tfidf-11" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>12 0.12847491 <a title="120-tfidf-12" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>13 0.12184785 <a title="120-tfidf-13" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>14 0.11293589 <a title="120-tfidf-14" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>15 0.10704511 <a title="120-tfidf-15" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>16 0.10043177 <a title="120-tfidf-16" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>17 0.099675983 <a title="120-tfidf-17" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>18 0.098567344 <a title="120-tfidf-18" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>19 0.087193415 <a title="120-tfidf-19" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>20 0.083358712 <a title="120-tfidf-20" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.228), (1, -0.165), (2, 0.008), (3, -0.125), (4, -0.136), (5, 0.061), (6, 0.139), (7, 0.201), (8, -0.265), (9, 0.047), (10, 0.066), (11, -0.03), (12, -0.224), (13, -0.074), (14, -0.077), (15, -0.014), (16, -0.061), (17, 0.132), (18, -0.024), (19, -0.028), (20, -0.121), (21, -0.018), (22, -0.049), (23, -0.06), (24, 0.048), (25, -0.051), (26, -0.095), (27, -0.01), (28, 0.007), (29, -0.025), (30, -0.034), (31, 0.05), (32, -0.016), (33, -0.029), (34, -0.073), (35, 0.05), (36, 0.087), (37, -0.04), (38, -0.081), (39, 0.024), (40, -0.04), (41, 0.067), (42, -0.021), (43, -0.031), (44, -0.078), (45, -0.04), (46, 0.043), (47, 0.043), (48, 0.022), (49, 0.1)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96616733 <a title="120-lsi-1" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>Author: Yi Zhang, Artur Dubrawski, Jeff G. Schneider</p><p>Abstract: In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any speciﬁc task in the same space through regularization. In an empirical study, we construct 190 different text classiﬁcation tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classiﬁcation tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the prediction model used.</p><p>2 0.83574349 <a title="120-lsi-2" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>3 0.71438313 <a title="120-lsi-3" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>Author: Doug Downey, Oren Etzioni</p><p>Abstract: Is accurate classiﬁcation possible in the absence of hand-labeled data? This paper introduces the Monotonic Feature (MF) abstraction—where the probability of class membership increases monotonically with the MF’s value. The paper proves that when an MF is given, PAC learning is possible with no hand-labeled data under certain assumptions. We argue that MFs arise naturally in a broad range of textual classiﬁcation applications. On the classic “20 Newsgroups” data set, a learner given an MF and unlabeled data achieves classiﬁcation accuracy equal to that of a state-of-the-art semi-supervised learner relying on 160 hand-labeled examples. Even when MFs are not given as input, their presence or absence can be determined from a small amount of hand-labeled data, which yields a new semi-supervised learning method that reduces error by 15% on the 20 Newsgroups data. 1</p><p>4 0.63679862 <a title="120-lsi-4" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>Author: Simon Lacoste-julien, Fei Sha, Michael I. Jordan</p><p>Abstract: Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in ﬁnding a reduced dimensionality representation. Speciﬁcally, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classiﬁcation. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classiﬁcation task and show how our model can identify shared topics across classes as well as class-dependent topics.</p><p>5 0.60345531 <a title="120-lsi-5" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>Author: Kilian Q. Weinberger, Olivier Chapelle</p><p>Abstract: Applications of multi-class classiﬁcation, such as document categorization, often appear in cost-sensitive settings. Recent work has signiﬁcantly improved the state of the art by moving beyond “ﬂat” classiﬁcation through incorporation of class hierarchies [4]. We present a novel algorithm that goes beyond hierarchical classiﬁcation and estimates the latent semantic space that underlies the class hierarchy. In this space, each class is represented by a prototype and classiﬁcation is done with the simple nearest neighbor rule. The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. We show that our optimization is convex and can be solved efﬁciently for large data sets. Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization. 1</p><p>6 0.56324995 <a title="120-lsi-6" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>7 0.54378635 <a title="120-lsi-7" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>8 0.53425348 <a title="120-lsi-8" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>9 0.51677513 <a title="120-lsi-9" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>10 0.49667668 <a title="120-lsi-10" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>11 0.49371088 <a title="120-lsi-11" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>12 0.48566392 <a title="120-lsi-12" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>13 0.47253197 <a title="120-lsi-13" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>14 0.46426252 <a title="120-lsi-14" href="./nips-2008-Asynchronous_Distributed_Learning_of_Topic_Models.html">28 nips-2008-Asynchronous Distributed Learning of Topic Models</a></p>
<p>15 0.46110898 <a title="120-lsi-15" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>16 0.44445193 <a title="120-lsi-16" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>17 0.43274507 <a title="120-lsi-17" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>18 0.37399426 <a title="120-lsi-18" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>19 0.37183866 <a title="120-lsi-19" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>20 0.35967311 <a title="120-lsi-20" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.087), (7, 0.083), (12, 0.038), (28, 0.114), (57, 0.073), (59, 0.013), (63, 0.021), (71, 0.024), (77, 0.046), (78, 0.016), (83, 0.129), (97, 0.267)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.75471401 <a title="120-lda-1" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>Author: Yi Zhang, Artur Dubrawski, Jeff G. Schneider</p><p>Abstract: In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any speciﬁc task in the same space through regularization. In an empirical study, we construct 190 different text classiﬁcation tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classiﬁcation tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the prediction model used.</p><p>2 0.60989368 <a title="120-lda-2" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>3 0.60936612 <a title="120-lda-3" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>Author: Jo-anne Ting, Mrinal Kalakrishnan, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise varies spatially. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. We introduce a Bayesian formulation of nonparametric regression that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efﬁcient, requires no sampling, automatically rejects outliers and has only one prior to be speciﬁed. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law. 1</p><p>4 0.60374117 <a title="120-lda-4" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>Author: Takafumi Kanamori, Shohei Hido, Masashi Sugiyama</p><p>Abstract: We address the problem of estimating the ratio of two probability density functions (a.k.a. the importance). The importance values can be used for various succeeding tasks such as non-stationarity adaptation or outlier detection. In this paper, we propose a new importance estimation method that has a closed-form solution; the leave-one-out cross-validation score can also be computed analytically. Therefore, the proposed method is computationally very efﬁcient and numerically stable. We also elucidate theoretical properties of the proposed method such as the convergence rate and approximation error bound. Numerical experiments show that the proposed method is comparable to the best existing method in accuracy, while it is computationally more efﬁcient than competing approaches. 1</p><p>5 0.60122961 <a title="120-lda-5" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>6 0.59933436 <a title="120-lda-6" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>7 0.59854102 <a title="120-lda-7" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>8 0.59797394 <a title="120-lda-8" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>9 0.59783763 <a title="120-lda-9" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>10 0.59613389 <a title="120-lda-10" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>11 0.59560299 <a title="120-lda-11" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>12 0.59190357 <a title="120-lda-12" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>13 0.59078902 <a title="120-lda-13" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>14 0.59059882 <a title="120-lda-14" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>15 0.58753759 <a title="120-lda-15" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>16 0.58578491 <a title="120-lda-16" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>17 0.58565223 <a title="120-lda-17" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>18 0.58512241 <a title="120-lda-18" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>19 0.58511072 <a title="120-lda-19" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>20 0.58476919 <a title="120-lda-20" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
