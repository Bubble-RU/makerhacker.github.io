<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-213" href="#">nips2008-213</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</h1>
<br/><p>Source: <a title="nips-2008-213-pdf" href="http://papers.nips.cc/paper/3553-sparse-convolved-gaussian-processes-for-multi-output-regression.pdf">pdf</a></p><p>Author: Mauricio Alvarez, Neil D. Lawrence</p><p>Abstract: We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network. 1</p><p>Reference: <a title="nips-2008-213-reference" href="../nips2008_reference/nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression_reference.html">text</a></p><br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kf', 0.556), ('pitc', 0.366), ('ur', 0.364), ('gp', 0.239), ('fq', 0.202), ('convolv', 0.159), ('yq', 0.159), ('fitc', 0.135), ('kur', 0.135), ('tid', 0.135), ('kqr', 0.135), ('cov', 0.127), ('output', 0.109), ('cadm', 0.096), ('wq', 0.093), ('fs', 0.091), ('gps', 0.09), ('bramblemet', 0.077), ('kfq', 0.077), ('height', 0.076)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="213-tfidf-1" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>Author: Mauricio Alvarez, Neil D. Lawrence</p><p>Abstract: We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network. 1</p><p>2 0.16103609 <a title="213-tfidf-2" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>Author: Ben Calderhead, Mark Girolami, Neil D. Lawrence</p><p>Abstract: Identiﬁcation and comparison of nonlinear dynamical system models using noisy and sparse experimental data is a vital task in many ﬁelds, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced. We present an accelerated sampling procedure which enables Bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of Gaussian processes (GP). Our method involves GP regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time. We demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods. 1</p><p>3 0.125879 <a title="213-tfidf-3" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>Author: Jo-anne Ting, Mrinal Kalakrishnan, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise varies spatially. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. We introduce a Bayesian formulation of nonparametric regression that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efﬁcient, requires no sampling, automatically rejects outliers and has only one prior to be speciﬁed. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law. 1</p><p>4 0.11128457 <a title="213-tfidf-4" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>Author: Christopher Williams, Stefan Klanke, Sethu Vijayakumar, Kian M. Chai</p><p>Abstract: The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneﬁcial to be able to learn this function for adaptive control. A robotic manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem. By placing independent Gaussian process priors over the latent functions of the inverse dynamics, we obtain a multi-task Gaussian process prior for handling multiple loads, where the inter-task similarity depends on the underlying inertial parameters. Experiments demonstrate that this multi-task formulation is effective in sharing information among the various loads, and generally improves performance over either learning only on single tasks or pooling the data over all tasks. 1</p><p>5 0.10333662 <a title="213-tfidf-5" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>Author: Neil D. Lawrence, Magnus Rattray, Michalis K. Titsias</p><p>Abstract: Sampling functions in Gaussian process (GP) models is challenging because of the highly correlated posterior distribution. We describe an efﬁcient Markov chain Monte Carlo algorithm for sampling from the posterior process of the GP model. This algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function. At each iteration, the algorithm proposes new values for the control variables and generates the function from the conditional GP prior. The control variable input locations are found by minimizing an objective function. We demonstrate the algorithm on regression and classiﬁcation problems and we use it to estimate the parameters of a differential equation model of gene regulation. 1</p><p>6 0.094259351 <a title="213-tfidf-6" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>7 0.070336021 <a title="213-tfidf-7" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>8 0.069384523 <a title="213-tfidf-8" href="./nips-2008-Empirical_performance_maximization_for_linear_rank_statistics.html">72 nips-2008-Empirical performance maximization for linear rank statistics</a></p>
<p>9 0.057915028 <a title="213-tfidf-9" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<p>10 0.057534207 <a title="213-tfidf-10" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>11 0.05058334 <a title="213-tfidf-11" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>12 0.050424829 <a title="213-tfidf-12" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>13 0.045973904 <a title="213-tfidf-13" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>14 0.04531157 <a title="213-tfidf-14" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>15 0.044602927 <a title="213-tfidf-15" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>16 0.043126289 <a title="213-tfidf-16" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>17 0.042534154 <a title="213-tfidf-17" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>18 0.041971628 <a title="213-tfidf-18" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>19 0.035041496 <a title="213-tfidf-19" href="./nips-2008-Improving_on_Expectation_Propagation.html">105 nips-2008-Improving on Expectation Propagation</a></p>
<p>20 0.034784876 <a title="213-tfidf-20" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.116), (1, 0.023), (2, -0.015), (3, -0.03), (4, 0.021), (5, 0.071), (6, 0.108), (7, -0.027), (8, -0.04), (9, 0.032), (10, 0.01), (11, -0.011), (12, 0.157), (13, 0.025), (14, 0.138), (15, -0.119), (16, -0.049), (17, -0.091), (18, -0.114), (19, -0.046), (20, 0.074), (21, 0.059), (22, -0.06), (23, 0.114), (24, 0.004), (25, -0.048), (26, 0.043), (27, 0.026), (28, -0.071), (29, -0.029), (30, -0.073), (31, 0.072), (32, -0.026), (33, -0.081), (34, -0.025), (35, -0.044), (36, 0.053), (37, 0.05), (38, 0.009), (39, -0.022), (40, 0.033), (41, 0.042), (42, 0.073), (43, -0.079), (44, 0.014), (45, -0.018), (46, 0.004), (47, 0.016), (48, 0.069), (49, 0.066)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.88931316 <a title="213-lsi-1" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>Author: Mauricio Alvarez, Neil D. Lawrence</p><p>Abstract: We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network. 1</p><p>2 0.7420125 <a title="213-lsi-2" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>Author: Neil D. Lawrence, Magnus Rattray, Michalis K. Titsias</p><p>Abstract: Sampling functions in Gaussian process (GP) models is challenging because of the highly correlated posterior distribution. We describe an efﬁcient Markov chain Monte Carlo algorithm for sampling from the posterior process of the GP model. This algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function. At each iteration, the algorithm proposes new values for the control variables and generates the function from the conditional GP prior. The control variable input locations are found by minimizing an objective function. We demonstrate the algorithm on regression and classiﬁcation problems and we use it to estimate the parameters of a differential equation model of gene regulation. 1</p><p>3 0.70368838 <a title="213-lsi-3" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>Author: Christopher Williams, Stefan Klanke, Sethu Vijayakumar, Kian M. Chai</p><p>Abstract: The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneﬁcial to be able to learn this function for adaptive control. A robotic manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem. By placing independent Gaussian process priors over the latent functions of the inverse dynamics, we obtain a multi-task Gaussian process prior for handling multiple loads, where the inter-task similarity depends on the underlying inertial parameters. Experiments demonstrate that this multi-task formulation is effective in sharing information among the various loads, and generally improves performance over either learning only on single tasks or pooling the data over all tasks. 1</p><p>4 0.6945588 <a title="213-lsi-4" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>Author: Ben Calderhead, Mark Girolami, Neil D. Lawrence</p><p>Abstract: Identiﬁcation and comparison of nonlinear dynamical system models using noisy and sparse experimental data is a vital task in many ﬁelds, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced. We present an accelerated sampling procedure which enables Bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of Gaussian processes (GP). Our method involves GP regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time. We demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods. 1</p><p>5 0.65453029 <a title="213-lsi-5" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>Author: Jo-anne Ting, Mrinal Kalakrishnan, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise varies spatially. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. We introduce a Bayesian formulation of nonparametric regression that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efﬁcient, requires no sampling, automatically rejects outliers and has only one prior to be speciﬁed. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law. 1</p><p>6 0.63342834 <a title="213-lsi-6" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>7 0.59690875 <a title="213-lsi-7" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>8 0.44674689 <a title="213-lsi-8" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>9 0.40888458 <a title="213-lsi-9" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>10 0.3905035 <a title="213-lsi-10" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>11 0.34909141 <a title="213-lsi-11" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>12 0.33725265 <a title="213-lsi-12" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>13 0.3249988 <a title="213-lsi-13" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>14 0.3231667 <a title="213-lsi-14" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>15 0.31344643 <a title="213-lsi-15" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>16 0.31140646 <a title="213-lsi-16" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>17 0.31046075 <a title="213-lsi-17" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>18 0.3062225 <a title="213-lsi-18" href="./nips-2008-A_Massively_Parallel_Digital_Learning_Processor.html">3 nips-2008-A Massively Parallel Digital Learning Processor</a></p>
<p>19 0.29831463 <a title="213-lsi-19" href="./nips-2008-On_Computational_Power_and_the_Order-Chaos_Phase_Transition_in_Reservoir_Computing.html">160 nips-2008-On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing</a></p>
<p>20 0.28624463 <a title="213-lsi-20" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.022), (29, 0.027), (30, 0.055), (40, 0.051), (51, 0.43), (60, 0.023), (63, 0.075), (64, 0.089), (71, 0.068), (98, 0.051)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.59619313 <a title="213-lda-1" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>Author: Mauricio Alvarez, Neil D. Lawrence</p><p>Abstract: We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network. 1</p><p>2 0.51546973 <a title="213-lda-2" href="./nips-2008-Reconciling_Real_Scores_with_Binary_Comparisons%3A_A_New_Logistic_Based_Model_for_Ranking.html">190 nips-2008-Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking</a></p>
<p>Author: Nir Ailon</p><p>Abstract: The problem of ranking arises ubiquitously in almost every aspect of life, and in particular in Machine Learning/Information Retrieval. A statistical model for ranking predicts how humans rank subsets V of some universe U . In this work we deﬁne a statistical model for ranking that satisﬁes certain desirable properties. The model automatically gives rise to a logistic regression based approach to learning how to rank, for which the score and comparison based approaches are dual views. This offers a new generative approach to ranking which can be used for IR. There are two main contexts for this work. The ﬁrst is the theory of econometrics and study of statistical models explaining human choice of alternatives. In this context, we will compare our model with other well known models. The second context is the problem of ranking in machine learning, usually arising in the context of information retrieval. Here, much work has been done in the discriminative setting, where different heuristics are used to deﬁne ranking risk functions. Our model is built rigorously and axiomatically based on very simple desirable properties deﬁned locally for comparisons, and automatically implies the existence of a global score function serving as a natural model parameter which can be efﬁciently ﬁtted to pairwise comparison judgment data by solving a convex optimization problem. 1</p><p>3 0.49534994 <a title="213-lda-3" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>Author: Markus Maier, Ulrike V. Luxburg, Matthias Hein</p><p>Abstract: Graph clustering methods such as spectral clustering are deﬁned for general weighted graphs. In machine learning, however, data often is not given in form of a graph, but in terms of similarity (or distance) values between points. In this case, ﬁrst a neighborhood graph is constructed using the similarities between the points and then a graph clustering algorithm is applied to this graph. In this paper we investigate the inﬂuence of the construction of the similarity graph on the clustering results. We ﬁrst study the convergence of graph clustering criteria such as the normalized cut (Ncut) as the sample size tends to inﬁnity. We ﬁnd that the limit expressions are different for different types of graph, for example the r-neighborhood graph or the k-nearest neighbor graph. In plain words: Ncut on a kNN graph does something systematically different than Ncut on an r-neighborhood graph! This ﬁnding shows that graph clustering criteria cannot be studied independently of the kind of graph they are applied to. We also provide examples which show that these differences can be observed for toy and real data already for rather small sample sizes. 1</p><p>4 0.4886798 <a title="213-lda-4" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>Author: Adrian Haith, Carl P. Jackson, R. C. Miall, Sethu Vijayakumar</p><p>Abstract: Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a uniﬁed model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereﬀects. This uniﬁed model also makes the surprising prediction that force ﬁeld adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We conﬁrm this prediction with an experiment. 1</p><p>5 0.34196147 <a title="213-lda-5" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>Author: Stefan Haufe, Vadim V. Nikulin, Andreas Ziehe, Klaus-Robert Müller, Guido Nolte</p><p>Abstract: We introduce a novel framework for estimating vector ﬁelds using sparse basis ﬁeld expansions (S-FLEX). The notion of basis ﬁelds, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector ﬁeld, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that signiﬁcantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art. 1</p><p>6 0.33655041 <a title="213-lda-6" href="./nips-2008-Understanding_Brain_Connectivity_Patterns_during_Motor_Imagery_for_Brain-Computer_Interfacing.html">243 nips-2008-Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing</a></p>
<p>7 0.33355147 <a title="213-lda-7" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>8 0.32910642 <a title="213-lda-8" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>9 0.32811886 <a title="213-lda-9" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>10 0.32799935 <a title="213-lda-10" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>11 0.32772529 <a title="213-lda-11" href="./nips-2008-Characterizing_neural_dependencies_with_copula_models.html">45 nips-2008-Characterizing neural dependencies with copula models</a></p>
<p>12 0.32688299 <a title="213-lda-12" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>13 0.32498518 <a title="213-lda-13" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>14 0.32448313 <a title="213-lda-14" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>15 0.32381976 <a title="213-lda-15" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>16 0.32341194 <a title="213-lda-16" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>17 0.32230276 <a title="213-lda-17" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<p>18 0.32203424 <a title="213-lda-18" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>19 0.32185677 <a title="213-lda-19" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>20 0.32172436 <a title="213-lda-20" href="./nips-2008-On_Computational_Power_and_the_Order-Chaos_Phase_Transition_in_Reservoir_Computing.html">160 nips-2008-On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
