<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-146" href="#">nips2008-146</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</h1>
<br/><p>Source: <a title="nips-2008-146-pdf" href="http://papers.nips.cc/paper/3385-multi-task-gaussian-process-learning-of-robot-inverse-dynamics.pdf">pdf</a></p><p>Author: Christopher Williams, Stefan Klanke, Sethu Vijayakumar, Kian M. Chai</p><p>Abstract: The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneﬁcial to be able to learn this function for adaptive control. A robotic manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem. By placing independent Gaussian process priors over the latent functions of the inverse dynamics, we obtain a multi-task Gaussian process prior for handling multiple loads, where the inter-task similarity depends on the underlying inertial parameters. Experiments demonstrate that this multi-task formulation is effective in sharing information among the various loads, and generally improves performance over either learning only on single tasks or pooling the data over all tasks. 1</p><p>Reference: <a title="nips-2008-146-reference" href="../nips2008_reference/nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneﬁcial to be able to learn this function for adaptive control. [sent-14, score-0.9]
</p><p>2 A robotic manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem. [sent-15, score-0.443]
</p><p>3 By placing independent Gaussian process priors over the latent functions of the inverse dynamics, we obtain a multi-task Gaussian process prior for handling multiple loads, where the inter-task similarity depends on the underlying inertial parameters. [sent-16, score-0.418]
</p><p>4 1  Introduction  The inverse dynamics problem for a robotic manipulator is to compute the torques τ needed at the joints to drive it along a given trajectory, i. [sent-18, score-0.865]
</p><p>5 the motion speciﬁed by the joint angles q(t), velocities ˙ ¨ ˙ ¨ q(t) and accelerations q (t), through time t. [sent-20, score-0.187]
</p><p>6 Analytical models for the inverse dynamics τ (q, q, q ) are often infeasible, for example due to uncertainty in the physical parameters of the robot, or the difﬁculty of modelling friction. [sent-21, score-0.38]
</p><p>7 This leads to the need to learn the inverse dynamics. [sent-22, score-0.161]
</p><p>8 A given robotic manipulator will often need to be controlled while holding different loads in its end effector. [sent-23, score-0.443]
</p><p>9 The inverse dynamics functions depend on the different contexts. [sent-25, score-0.315]
</p><p>10 The aim of this paper is to show how this can be carried out for the inverse dynamics problem using a multi-task Gaussian process (GP) framework. [sent-29, score-0.281]
</p><p>11 2  Theory  We ﬁrst describe the relationship of inverse dynamics functions among contexts in §2. [sent-33, score-0.551]
</p><p>12 1  Linear relationship of inverse dynamics between contexts  Suppose we have a robotic manipulator consisting of J joints, and a set of M loads. [sent-39, score-0.704]
</p><p>13 Figure 1 illustrates a six-jointed manipulator, with joint j connecting links j −1 and j. [sent-40, score-0.167]
</p><p>14 We wish to learn the inverse  Joint 1 Waist  m τj  q3  Joint 2 Shoulder  Base  ···  Joint 6 Flange Joint 3 Elbow Joint 5 Wrist Bend  yjJ,1 hj j = 1. [sent-41, score-0.222]
</p><p>15 Figure 1: Schematic of the PUMA 560 without the end-effector (to be connected to joint 6). [sent-49, score-0.13]
</p><p>16 dynamics model of the manipulator for the mth context, i. [sent-50, score-0.388]
</p><p>17 when it handles the mth load in its enddef ˙ ¨ effector connected to the last link. [sent-52, score-0.392]
</p><p>18 The inertial parameters for a joint depend on the physical characteristics of its corresponding link (e. [sent-55, score-0.423]
</p><p>19 When, as in our case, the loads are rigidly attached to the end effector, each load may be considered as part of the last link, and thus modiﬁes the inertia parameters for the last link only [5]. [sent-58, score-0.573]
</p><p>20 The parameters for the other links remain unchanged since the parameters are local to the links and their frames. [sent-59, score-0.162]
</p><p>21 Denoting the common inertial parameters of the j ′th link by π •′ , we can write j m τj (x) = hj (x) + y T (x)π m , jJ J  def where hj (x) =  ˜ m def  J−1 T • j ′ =j y jj ′ (x)π j ′ . [sent-60, score-0.849]
</p><p>22 (2)  def ˜ Note Deﬁne y j (x) = (hj (x), (y jJ (x))T )T and π = (1, (π m )T )T , then J ˜ j s are shared among the contexts, while the π m s are shared among the J links, as illustrated ˜ that the y in Figure 2. [sent-62, score-0.243]
</p><p>23 This decomposition is not unique, since given a non-singular square 11×11 matrix Aj , def def ˜ setting z j (x) = A−T y j (x) and ρm = Aj π m , we also have j j ˜  m ˜ ˜ τj (x) = y j (x)T A−1 Aj π m = z j (x)T ρm . [sent-63, score-0.342]
</p><p>24 Let tm be the observation of the mth function at x. [sent-69, score-0.217]
</p><p>25 Then the model is given by ′  def  f 2 f m (x)f m (x′ ) = Kmm′ k x (x, x′ ) tm ∼ N (f m (x), σm ), (4) x f where k is a covariance function over inputs, K is a positive semi-deﬁnite (p. [sent-70, score-0.34]
</p><p>26 3  Multi-task GP model for multiple contexts  We now show that the multi-task GP model can be used for inferring inverse dynamics for multiple contexts. [sent-74, score-0.481]
</p><p>27 Let α be an index into the elements of the vector function z j (·), then our prior is x zjα (x)zj ′ α′ (x′ ) = δjj ′ δαα′ kj (x, x′ ). [sent-79, score-0.245]
</p><p>28 In addition to independence speciﬁed by the Kronecker delta functions δ·· , this model also imposes the constraint that all component functions for a given joint j share the same covariance function x m kj (·, ·). [sent-83, score-0.499]
</p><p>29 The rank of Kj is the rank of Pj , and is upper bounded by min(M , 11), reﬂecting the fact that there are at most 11 underlying latent functions (see Figure 2). [sent-86, score-0.307]
</p><p>30 The deviations from τj (x) may be modelled with j m m 2 M def 1 tm (x) ∼ N (τj (x), (σj )2 ), though in practice we let σj = σj ≡ σj . [sent-88, score-0.284]
</p><p>31 The observations over all contexts for a given joint j will be used to make the predictions. [sent-99, score-0.33]
</p><p>32 1  The relationship among task similarity matrices  ˜ def ˜ ˜ ˜ Let Π = (π 1 | · · · |π M ). [sent-104, score-0.207]
</p><p>33 However, if the different loads in the end effector do not explore the full space (e. [sent-106, score-0.304]
</p><p>34 if some of the inertial parameters are constant def ˜ over all loads), then it can happen that s = rank(Π) ≤ min(M , 11). [sent-108, score-0.404]
</p><p>35 3 that ρ m m def ˜ ˜ ˜ ˜ ρj = Aj π , where Aj is a full-rank square matrix. [sent-111, score-0.171]
</p><p>36 3  Learning the hyperparameters — a staged optimization heuristic  In this section, we drop the joint index j for the sake of brevity and clarity. [sent-115, score-0.22]
</p><p>37 Let tm be the vector of nm observed torques at the joint for context m, and X m be the corresponding 3J ×nm design matrix. [sent-117, score-0.468]
</p><p>38 Given this data, we wish to optimize the def marginal likelihood L(θ x , K ρ , σ 2 ) = p({tm }M |X, θ x , K ρ , σ 2 ), where θ x are the parameters of m=1 k x . [sent-119, score-0.274]
</p><p>39 For K0 = 11T , ρ we initially assume the contexts to be indistinguishable from each other; while for K0 = I, we initially assume the contexts to be independent given the kernel parameters, which is a multi-task learning model that has been previously explored, e. [sent-137, score-0.4]
</p><p>40 Let T be an N×M matrix which 1 corresponds to the true values of the torque function τ m (xi ) for m = 1, . [sent-147, score-0.142]
</p><p>41 The ﬁrst is that the rank of T θ0 is ˜ ρ ρ upper bounded by that of K0 , so that the rank of KEM is similarly upper bounded. [sent-161, score-0.234]
</p><p>42 3  Incorporating a novel task  Above we have assumed that data from all contexts is available at training time. [sent-172, score-0.24]
</p><p>43 4  Model selection  ρ ˜ The choice of the rank r of Kj in the model is important, since it reﬂects on the rank s of Π. [sent-178, score-0.234]
</p><p>44 def context, and n = j,m nm be the total number of observations; and let dj be the dimensionality of j x θ j . [sent-184, score-0.25]
</p><p>45 Since the likelihood of the model factorizes over joints, we have  BIC(r) = −2  J j=1  log Ljr +  J j=1  dj + J r(2M + 1 − r) + J log n, 2  (8)  where r(2M + 1 − r)/2 is the number of parameters needed to deﬁne an incomplete Cholesky ρ decomposition of rank r for an M ×M matrix. [sent-185, score-0.215]
</p><p>46 5  Relationships to other work  We consider related work ﬁrst with regard to the inverse dynamics problem, and then to multi-task learning with Gaussian processes. [sent-187, score-0.281]
</p><p>47 Learning methods for the single-context inverse dynamics problem can be found in e. [sent-188, score-0.281]
</p><p>48 Assuming the original estimated torque functions are imperfect, having more than 11 models for distinct known inertial parameters will improve load estimation. [sent-195, score-0.642]
</p><p>49 If the inertial parameters are unknown, the novel torque function can still be represented as a linear combination of a set of 11 linearly independent torque functions, and so one can estimate the inverse dynamics in a novel context by linear regression on those estimated functions. [sent-196, score-0.88]
</p><p>50 J Comparing our approach with [5], we note that: (a) their approach does not exploit the knowledge that the torque functions for the different contexts are known to share latent functions as in eq. [sent-200, score-0.449]
</p><p>51 2, and thus it may be useful to learn the M inverse dynamics models jointly. [sent-201, score-0.345]
</p><p>52 Earlier work on multiple model learning such as Multiple Model Switching and Tuning (MMST) [10] uses an inverse dynamics model and a controller for each context, switching among the models to the one producing the most accurate predictions. [sent-206, score-0.346]
</p><p>53 MMST involves very little dynamics learning, estimating only the linear parameters of the models. [sent-208, score-0.199]
</p><p>54 A closely related approach is Modular Selection and Identiﬁcation for Control (MOSAIC) [11], which uses inverse dynamics models for control and forward dynamics models for context identiﬁcation. [sent-209, score-0.567]
</p><p>55 However, MOSAIC was developed and tested on linear dynamics models without the insights into how eq. [sent-210, score-0.184]
</p><p>56 1 may be used across contexts for more efﬁcient and robust learning and control. [sent-211, score-0.2]
</p><p>57 An important related work is the semiparametric latent factor model [12] which has a number of latent processes which are linearly combined to produce observable functions as in eq. [sent-217, score-0.143]
</p><p>58 However, in our model all the latent functions share a common covariance function, which reduces the number of free parameters and should thus help to reduce over-ﬁtting. [sent-219, score-0.173]
</p><p>59 [12, §4] used a forward dynamics problem on a four-jointed robot arm for a single context, with an artiﬁcial linear mixing of the four target joint accelerations to produce six response variables. [sent-221, score-0.382]
</p><p>60 In contrast, we have shown how linear mixing arises naturally in a multi-context inverse dynamics situation. [sent-222, score-0.281]
</p><p>61 Table 1: The trajectories at which the training samples for each load are acquired. [sent-234, score-0.282]
</p><p>62 All loads have training samples from the common trajectory (p2 , s3 ). [sent-235, score-0.358]
</p><p>63 s1 s2 s3 s4 p1 c1 c7 c13 c14 c6 c12 c1 · · · c15 c5 p2 p3 c11 c3 c4 c10 c2 c8 c9 c15 ∗ p4  Table 2: The average nMSEs of the predictions by LR and sGP, for joint 3 and for both kinds of test sets. [sent-237, score-0.162]
</p><p>64 average nMSE for the interpm sets average nMSE for the extrapm sets 20 170 1004 4000 20 170 1004 4000 LR 1×10−1 7×10−4 6×10−4 6×10−4 5×10−1 2×10−1 2×10−1 2×10−1 sGP 1×10−2 2×10−7 2×10−8 3×10−9 1×10−1 3×10−2 4×10−3 3×10−3 to work by Bonilla et al. [sent-243, score-0.378]
</p><p>65 We learn the inverse dynamic models of this robot manipulating M = 15 different loads c1 , . [sent-247, score-0.507]
</p><p>66 In general, loads can have very different physical characteristics; in our case, this is done by representing each load as a cuboid with differing dimensions and mass, and attaching each load rigidly to a random point at the end-effector. [sent-268, score-0.691]
</p><p>67 For each load cm , 4000 data points are sampled at regular intervals along the path for each path-speed (trajectory) combination (p· , s· ). [sent-272, score-0.266]
</p><p>68 Each sample is the pair (t, x), where t ∈ RJ are the observed torques at the joints, and x ∈ R3J are the joint angles, velocities and accelerations. [sent-273, score-0.285]
</p><p>69 One may imagine, however, that training data for the handling of a load can be obtained along a ﬁxed reference trajectory Tr for calibration purposes, and also along a trajectory typical for that load, say Tm for the mth load. [sent-276, score-0.604]
</p><p>70 Thus, for each load, 2000 random training samples are acquired at a common reference trajectory Tr = (p2 , s3 ), and an additional 2000 random training samples are acquired at a trajectory unique to each load; Table 1 gives the combinations. [sent-277, score-0.342]
</p><p>71 Therefore each load has a training set of 4000 samples, but acquired only on two different trajectories. [sent-278, score-0.277]
</p><p>72 Following [14], two kinds of test sets are used to assess our models for (a) control along a repeated trajectory (which is of practical interest in industry), and (b) control along arbitrary trajectories (which is of general interest to roboticists). [sent-279, score-0.287]
</p><p>73 The test for (a) assesses the accuracy of torque predictions for staying within the trajectories that were used for training. [sent-280, score-0.254]
</p><p>74 In this case, the test set for load cm , denoted by interpm for interpolation, consists of the rest of the samples from Tr and Tm that are not used for training. [sent-281, score-0.425]
</p><p>75 The test set for this, denoted by extrapm , consists of all the samples that are not training samples for cm . [sent-283, score-0.261]
</p><p>76 Results comparing GP with linear regression We ﬁrst compare learning the inverse dynamics with Bayesian linear regression (LR) to learning with single-task Gaussian processes (sGP). [sent-286, score-0.361]
</p><p>77 The hyperparameters of sGP are initialized by giving equal weightings among the covariates and among the components of the covariance function, and then learnt by optimizing the marginal likelihood independently for each context and each joint. [sent-292, score-0.303]
</p><p>78 The trained LR and sGP models are used to predict torques for the interpm and extrapm data sets. [sent-293, score-0.536]
</p><p>79 The nMSEs are then averaged over the 15 contexts for the interpm and extrapm tests. [sent-295, score-0.578]
</p><p>80 Table 2 shows how the averages for joint 3 vary with the number of training samples. [sent-296, score-0.17]
</p><p>81 As one would expect, the errors of LR level-off early at around 200 training samples, while the quality of predictions by sGP continues to improve with training sample size, especially so for the interpm sets. [sent-299, score-0.301]
</p><p>82 Both sGP and LR do reasonably well on the interpm sets, but not so well on the extrapm sets. [sent-300, score-0.378]
</p><p>83 This suggests that learning from multiple contexts which have training data from different parts of the trajectory space will be advantageous. [sent-301, score-0.338]
</p><p>84 Results for multi-task GP We now investigate the merit of using MTL, using the training data tabulated in Table 1 for loads c1 , . [sent-302, score-0.32]
</p><p>85 We use n to denote the number of observed torques for each joint totalled across the 14 contexts. [sent-306, score-0.259]
</p><p>86 Note that trajectory (p4 , s4 ) is entirely unobserved during learning, but is included in the extrapm sets. [sent-307, score-0.287]
</p><p>87 We learn the hyperparameters of a multi-task GP model (mGP) for each joint by optimizing the marginal likelihood for all training data (accumulated across contexts) for that joint, as discussed in §3, using the same kernel and parameterization as for the sGP. [sent-308, score-0.339]
</p><p>88 Finally, a common rank r for all the joints is chosen using the selection criterion given in §4. [sent-310, score-0.319]
</p><p>89 Figure 4 gives results of sGP, iGP, pGP and mGP-BIC for both the interpm and extrapm test sets, and for joints 1 and 4. [sent-317, score-0.58]
</p><p>90 Plots for the other joints are omitted due to space constraints, but they are qualitatively similar to the plots for joint 4. [sent-318, score-0.366]
</p><p>91 The plots are the average nMSEs over the 14 contexts against n. [sent-319, score-0.234]
</p><p>92 For joint 1, we see a close match between the predictive performances of mGP-BIC and pGP, with mGP-BIC slightly better than pGP for the interpolation task. [sent-323, score-0.155]
</p><p>93 This is due to the limited variation among observed torques for this joint across the different contexts for the range of end-effector  ×10−5 5  ×10−4 2  4  ×10−2 2  ×10−4 4  1. [sent-324, score-0.495]
</p><p>94 Therefore it is not surprising that pGP produces good predictions for joint 1. [sent-333, score-0.162]
</p><p>95 In particular, iGP is better than sGP, showing that (in this case) combining all the data to estimate the parameters of a single common covariance function is better than separating the data to estimate the parameters of 14 covariance functions. [sent-335, score-0.2]
</p><p>96 7  Summary  We have shown how the structure of the multiple-context inverse dynamics problem maps onto a multi-task GP prior as given in eq. [sent-336, score-0.281]
</p><p>97 6, how the corresponding marginal likelihood can be optimized ρ effectively, and how the rank of the Kj s can be chosen. [sent-337, score-0.176]
</p><p>98 Therefore it is advantageous to learn inverse dynamics models jointly using mGP-BIC, especially when each context/task explores different portions of the data space, a common case in dynamics learning. [sent-339, score-0.526]
</p><p>99 In future work we would like to investigate if coupling learning over joints is beneﬁcial. [sent-340, score-0.231]
</p><p>100 Adaptive control of robotic manipulators using multiple models and switching. [sent-407, score-0.154]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sgp', 0.275), ('kj', 0.245), ('gp', 0.23), ('loads', 0.22), ('pgp', 0.21), ('load', 0.204), ('joints', 0.202), ('contexts', 0.2), ('extrapm', 0.189), ('inertial', 0.189), ('interpm', 0.189), ('def', 0.171), ('igp', 0.165), ('dynamics', 0.155), ('torque', 0.142), ('joint', 0.13), ('manipulator', 0.129), ('torques', 0.129), ('lr', 0.128), ('inverse', 0.126), ('jj', 0.118), ('rank', 0.117), ('tm', 0.113), ('taug', 0.105), ('mth', 0.104), ('trajectory', 0.098), ('robotic', 0.094), ('nmses', 0.092), ('aj', 0.09), ('effector', 0.084), ('kaug', 0.084), ('lwpr', 0.073), ('robot', 0.066), ('kem', 0.063), ('mgp', 0.063), ('mosaic', 0.063), ('mtl', 0.063), ('hj', 0.061), ('bic', 0.06), ('covariance', 0.056), ('pj', 0.055), ('nm', 0.054), ('hyperparameters', 0.048), ('extrapolation', 0.047), ('nmse', 0.047), ('parameters', 0.044), ('th', 0.044), ('vijayakumar', 0.043), ('context', 0.042), ('ans', 0.042), ('assesses', 0.042), ('ljr', 0.042), ('mmst', 0.042), ('petkos', 0.042), ('puma', 0.042), ('staged', 0.042), ('regression', 0.04), ('training', 0.04), ('latent', 0.039), ('trajectories', 0.038), ('tr', 0.037), ('links', 0.037), ('ranks', 0.037), ('wrist', 0.037), ('bonilla', 0.037), ('rigidly', 0.037), ('tests', 0.036), ('among', 0.036), ('learn', 0.035), ('link', 0.034), ('plots', 0.034), ('sgn', 0.034), ('eu', 0.034), ('inertia', 0.034), ('chai', 0.034), ('functions', 0.034), ('paths', 0.033), ('acquired', 0.033), ('predictions', 0.032), ('cm', 0.032), ('accelerations', 0.031), ('merit', 0.031), ('semiparametric', 0.031), ('manipulating', 0.031), ('control', 0.031), ('marginal', 0.03), ('along', 0.03), ('placing', 0.03), ('sharing', 0.029), ('likelihood', 0.029), ('investigate', 0.029), ('models', 0.029), ('parameterization', 0.027), ('portions', 0.026), ('velocities', 0.026), ('cholesky', 0.026), ('covariates', 0.026), ('physical', 0.026), ('dj', 0.025), ('interpolation', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999958 <a title="146-tfidf-1" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>Author: Christopher Williams, Stefan Klanke, Sethu Vijayakumar, Kian M. Chai</p><p>Abstract: The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneﬁcial to be able to learn this function for adaptive control. A robotic manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem. By placing independent Gaussian process priors over the latent functions of the inverse dynamics, we obtain a multi-task Gaussian process prior for handling multiple loads, where the inter-task similarity depends on the underlying inertial parameters. Experiments demonstrate that this multi-task formulation is effective in sharing information among the various loads, and generally improves performance over either learning only on single tasks or pooling the data over all tasks. 1</p><p>2 0.1402792 <a title="146-tfidf-2" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>Author: Ben Calderhead, Mark Girolami, Neil D. Lawrence</p><p>Abstract: Identiﬁcation and comparison of nonlinear dynamical system models using noisy and sparse experimental data is a vital task in many ﬁelds, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced. We present an accelerated sampling procedure which enables Bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of Gaussian processes (GP). Our method involves GP regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time. We demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods. 1</p><p>3 0.13579829 <a title="146-tfidf-3" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>Author: Jo-anne Ting, Mrinal Kalakrishnan, Sethu Vijayakumar, Stefan Schaal</p><p>Abstract: In kernel-based regression learning, optimizing each kernel individually is useful when the data density, curvature of regression surfaces (or decision boundaries) or magnitude of output noise varies spatially. Previous work has suggested gradient descent techniques or complex statistical hypothesis methods for local kernel shaping, typically requiring some amount of manual tuning of meta parameters. We introduce a Bayesian formulation of nonparametric regression that, with the help of variational approximations, results in an EM-like algorithm for simultaneous estimation of regression and kernel parameters. The algorithm is computationally efﬁcient, requires no sampling, automatically rejects outliers and has only one prior to be speciﬁed. It can be used for nonparametric regression with local polynomials or as a novel method to achieve nonstationary regression with Gaussian processes. Our methods are particularly useful for learning control, where reliable estimation of local tangent planes is essential for adaptive controllers and reinforcement learning. We evaluate our methods on several synthetic data sets and on an actual robot which learns a task-level control law. 1</p><p>4 0.12898441 <a title="146-tfidf-4" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>Author: Duy Nguyen-tuong, Jan R. Peters, Matthias Seeger</p><p>Abstract: Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up standard Gaussian process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Comparisons with other non-parametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and ν-SVR. 1</p><p>5 0.11428012 <a title="146-tfidf-5" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>Author: Shuiwang Ji, Liang Sun, Rong Jin, Jieping Ye</p><p>Abstract: We present a multi-label multiple kernel learning (MKL) formulation in which the data are embedded into a low-dimensional space directed by the instancelabel correlations encoded into a hypergraph. We formulate the problem in the kernel-induced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the MKL framework. The proposed learning formulation leads to a non-smooth min-max problem, which can be cast into a semi-inﬁnite linear program (SILP). We further propose an approximate formulation with a guaranteed error bound which involves an unconstrained convex optimization problem. In addition, we show that the objective function of the approximate formulation is differentiable with Lipschitz continuous gradient, and hence existing methods can be employed to compute the optimal solution efﬁciently. We apply the proposed formulation to the automated annotation of Drosophila gene expression pattern images, and promising results have been reported in comparison with representative algorithms.</p><p>6 0.10859045 <a title="146-tfidf-6" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>7 0.10836364 <a title="146-tfidf-7" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>8 0.10274793 <a title="146-tfidf-8" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>9 0.092013575 <a title="146-tfidf-9" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>10 0.075522944 <a title="146-tfidf-10" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>11 0.063416459 <a title="146-tfidf-11" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>12 0.061129238 <a title="146-tfidf-12" href="./nips-2008-A_mixture_model_for_the_evolution_of_gene_expression_in_non-homogeneous_datasets.html">9 nips-2008-A mixture model for the evolution of gene expression in non-homogeneous datasets</a></p>
<p>13 0.059290122 <a title="146-tfidf-13" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>14 0.057193983 <a title="146-tfidf-14" href="./nips-2008-Nonrigid_Structure_from_Motion_in_Trajectory_Space.html">157 nips-2008-Nonrigid Structure from Motion in Trajectory Space</a></p>
<p>15 0.056902315 <a title="146-tfidf-15" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>16 0.054089807 <a title="146-tfidf-16" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>17 0.052769717 <a title="146-tfidf-17" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>18 0.050278034 <a title="146-tfidf-18" href="./nips-2008-Empirical_performance_maximization_for_linear_rank_statistics.html">72 nips-2008-Empirical performance maximization for linear rank statistics</a></p>
<p>19 0.04979774 <a title="146-tfidf-19" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>20 0.047945876 <a title="146-tfidf-20" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.17), (1, 0.021), (2, 0.05), (3, 0.036), (4, 0.108), (5, -0.05), (6, -0.006), (7, 0.143), (8, 0.023), (9, 0.081), (10, 0.075), (11, -0.018), (12, 0.083), (13, 0.001), (14, 0.171), (15, -0.044), (16, -0.132), (17, 0.076), (18, 0.072), (19, -0.005), (20, 0.012), (21, -0.081), (22, 0.009), (23, 0.035), (24, 0.075), (25, 0.09), (26, 0.017), (27, 0.113), (28, -0.005), (29, -0.089), (30, 0.06), (31, 0.062), (32, -0.055), (33, 0.052), (34, 0.041), (35, -0.149), (36, -0.012), (37, 0.089), (38, -0.076), (39, 0.05), (40, 0.018), (41, -0.029), (42, -0.005), (43, 0.019), (44, -0.079), (45, -0.17), (46, 0.013), (47, -0.115), (48, 0.109), (49, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91868103 <a title="146-lsi-1" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>Author: Christopher Williams, Stefan Klanke, Sethu Vijayakumar, Kian M. Chai</p><p>Abstract: The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneﬁcial to be able to learn this function for adaptive control. A robotic manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem. By placing independent Gaussian process priors over the latent functions of the inverse dynamics, we obtain a multi-task Gaussian process prior for handling multiple loads, where the inter-task similarity depends on the underlying inertial parameters. Experiments demonstrate that this multi-task formulation is effective in sharing information among the various loads, and generally improves performance over either learning only on single tasks or pooling the data over all tasks. 1</p><p>2 0.69250238 <a title="146-lsi-2" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>Author: Mauricio Alvarez, Neil D. Lawrence</p><p>Abstract: We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network. 1</p><p>3 0.67579222 <a title="146-lsi-3" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>Author: Duy Nguyen-tuong, Jan R. Peters, Matthias Seeger</p><p>Abstract: Learning in real-time applications, e.g., online approximation of the inverse dynamics model for model-based robot control, requires fast online regression techniques. Inspired by local learning, we propose a method to speed up standard Gaussian process regression (GPR) with local GP models (LGP). The training data is partitioned in local regions, for each an individual GP model is trained. The prediction for a query point is performed by weighted estimation using nearby local models. Unlike other GP approximations, such as mixtures of experts, we use a distance based measure for partitioning of the data and weighted prediction. The proposed method achieves online learning and prediction in real-time. Comparisons with other non-parametric regression methods show that LGP has higher accuracy than LWPR and close to the performance of standard GPR and ν-SVR. 1</p><p>4 0.67045784 <a title="146-lsi-4" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>Author: Neil D. Lawrence, Magnus Rattray, Michalis K. Titsias</p><p>Abstract: Sampling functions in Gaussian process (GP) models is challenging because of the highly correlated posterior distribution. We describe an efﬁcient Markov chain Monte Carlo algorithm for sampling from the posterior process of the GP model. This algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function. At each iteration, the algorithm proposes new values for the control variables and generates the function from the conditional GP prior. The control variable input locations are found by minimizing an objective function. We demonstrate the algorithm on regression and classiﬁcation problems and we use it to estimate the parameters of a differential equation model of gene regulation. 1</p><p>5 0.65649962 <a title="146-lsi-5" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>Author: Ben Calderhead, Mark Girolami, Neil D. Lawrence</p><p>Abstract: Identiﬁcation and comparison of nonlinear dynamical system models using noisy and sparse experimental data is a vital task in many ﬁelds, however current methods are computationally expensive and prone to error due in part to the nonlinear nature of the likelihood surfaces induced. We present an accelerated sampling procedure which enables Bayesian inference of parameters in nonlinear ordinary and delay differential equations via the novel use of Gaussian processes (GP). Our method involves GP regression over time-series data, and the resulting derivative and time delay estimates make parameter inference possible without solving the dynamical system explicitly, resulting in dramatic savings of computational time. We demonstrate the speed and statistical accuracy of our approach using examples of both ordinary and delay differential equations, and provide a comprehensive comparison with current state of the art methods. 1</p><p>6 0.65117532 <a title="146-lsi-6" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>7 0.53373551 <a title="146-lsi-7" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>8 0.48948279 <a title="146-lsi-8" href="./nips-2008-Variational_Mixture_of_Gaussian_Process_Experts.html">249 nips-2008-Variational Mixture of Gaussian Process Experts</a></p>
<p>9 0.44479039 <a title="146-lsi-9" href="./nips-2008-A_mixture_model_for_the_evolution_of_gene_expression_in_non-homogeneous_datasets.html">9 nips-2008-A mixture model for the evolution of gene expression in non-homogeneous datasets</a></p>
<p>10 0.42337793 <a title="146-lsi-10" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>11 0.40858012 <a title="146-lsi-11" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>12 0.40575773 <a title="146-lsi-12" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>13 0.37925589 <a title="146-lsi-13" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>14 0.37516296 <a title="146-lsi-14" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>15 0.37418655 <a title="146-lsi-15" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>16 0.32969043 <a title="146-lsi-16" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>17 0.32886449 <a title="146-lsi-17" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>18 0.32346019 <a title="146-lsi-18" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>19 0.3144266 <a title="146-lsi-19" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>20 0.31185231 <a title="146-lsi-20" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.051), (7, 0.103), (12, 0.037), (15, 0.01), (28, 0.143), (47, 0.32), (57, 0.054), (59, 0.044), (63, 0.021), (71, 0.014), (77, 0.036), (78, 0.012), (83, 0.056)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.73210603 <a title="146-lda-1" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>Author: Christopher Williams, Stefan Klanke, Sethu Vijayakumar, Kian M. Chai</p><p>Abstract: The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneﬁcial to be able to learn this function for adaptive control. A robotic manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem. By placing independent Gaussian process priors over the latent functions of the inverse dynamics, we obtain a multi-task Gaussian process prior for handling multiple loads, where the inter-task similarity depends on the underlying inertial parameters. Experiments demonstrate that this multi-task formulation is effective in sharing information among the various loads, and generally improves performance over either learning only on single tasks or pooling the data over all tasks. 1</p><p>2 0.71157449 <a title="146-lda-2" href="./nips-2008-On_Computational_Power_and_the_Order-Chaos_Phase_Transition_in_Reservoir_Computing.html">160 nips-2008-On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing</a></p>
<p>Author: Benjamin Schrauwen, Lars Buesing, Robert A. Legenstein</p><p>Abstract: Randomly connected recurrent neural circuits have proven to be very powerful models for online computations when a trained memoryless readout function is appended. Such Reservoir Computing (RC) systems are commonly used in two ﬂavors: with analog or binary (spiking) neurons in the recurrent circuits. Previous work showed a fundamental difference between these two incarnations of the RC idea. The performance of a RC system built from binary neurons seems to depend strongly on the network connectivity structure. In networks of analog neurons such dependency has not been observed. In this article we investigate this apparent dichotomy in terms of the in-degree of the circuit nodes. Our analyses based amongst others on the Lyapunov exponent reveal that the phase transition between ordered and chaotic network behavior of binary circuits qualitatively differs from the one in analog circuits. This explains the observed decreased computational performance of binary circuits of high node in-degree. Furthermore, a novel mean-ﬁeld predictor for computational performance is introduced and shown to accurately predict the numerically obtained results. 1</p><p>3 0.5872795 <a title="146-lda-3" href="./nips-2008-On_Bootstrapping_the_ROC_Curve.html">159 nips-2008-On Bootstrapping the ROC Curve</a></p>
<p>Author: Patrice Bertail, Stéphan J. Clémençcon, Nicolas Vayatis</p><p>Abstract: This paper is devoted to thoroughly investigating how to bootstrap the ROC curve, a widely used visual tool for evaluating the accuracy of test/scoring statistics in the bipartite setup. The issue of conﬁdence bands for the ROC curve is considered and a resampling procedure based on a smooth version of the empirical distribution called the ”smoothed bootstrap” is introduced. Theoretical arguments and simulation results are presented to show that the ”smoothed bootstrap” is preferable to a ”naive” bootstrap in order to construct accurate conﬁdence bands. 1</p><p>4 0.52680421 <a title="146-lda-4" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>5 0.52639723 <a title="146-lda-5" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>Author: Yen-yu Lin, Tyng-luh Liu, Chiou-shann Fuh</p><p>Abstract: In solving complex visual learning tasks, adopting multiple descriptors to more precisely characterize the data has been a feasible way for improving performance. These representations are typically high dimensional and assume diverse forms. Thus ﬁnding a way to transform them into a uniﬁed space of lower dimension generally facilitates the underlying tasks, such as object recognition or clustering. We describe an approach that incorporates multiple kernel learning with dimensionality reduction (MKL-DR). While the proposed framework is ﬂexible in simultaneously tackling data in various feature representations, the formulation itself is general in that it is established upon graph embedding. It follows that any dimensionality reduction techniques explainable by graph embedding can be generalized by our method to consider data in multiple feature representations.</p><p>6 0.52575177 <a title="146-lda-6" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>7 0.52556527 <a title="146-lda-7" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>8 0.52519721 <a title="146-lda-8" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>9 0.5245133 <a title="146-lda-9" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>10 0.52102882 <a title="146-lda-10" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>11 0.52030897 <a title="146-lda-11" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>12 0.52006829 <a title="146-lda-12" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>13 0.51995355 <a title="146-lda-13" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>14 0.51812804 <a title="146-lda-14" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>15 0.51785725 <a title="146-lda-15" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>16 0.51782113 <a title="146-lda-16" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>17 0.51756459 <a title="146-lda-17" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>18 0.51717716 <a title="146-lda-18" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>19 0.5167672 <a title="146-lda-19" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>20 0.51590437 <a title="146-lda-20" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
