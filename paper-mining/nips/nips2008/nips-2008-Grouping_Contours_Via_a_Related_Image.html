<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>95 nips-2008-Grouping Contours Via a Related Image</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-95" href="#">nips2008-95</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>95 nips-2008-Grouping Contours Via a Related Image</h1>
<br/><p>Source: <a title="nips-2008-95-pdf" href="http://papers.nips.cc/paper/3576-grouping-contours-via-a-related-image.pdf">pdf</a></p><p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>Reference: <a title="nips-2008-95-reference" href="../nips2008_reference/nips-2008-Grouping_Contours_Via_a_Related_Image_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). [sent-8, score-0.852]
</p><p>2 We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. [sent-9, score-1.834]
</p><p>3 Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. [sent-10, score-1.945]
</p><p>4 Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. [sent-12, score-0.849]
</p><p>5 For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. [sent-13, score-1.715]
</p><p>6 The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. [sent-14, score-2.214]
</p><p>7 We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. [sent-15, score-0.322]
</p><p>8 1  Introduction  Researchers in biological vision have long hypothesized that image contours (ordered sets of edge pixels, or contour points) are a compact yet descriptive representation of object shape. [sent-17, score-1.224]
</p><p>9 In computer vision, there has been substantial interest in extracting contours from images as well as using object models based on contours for object recognition ([15, 5]), and 3D image interpretation [11]. [sent-18, score-1.917]
</p><p>10 We examine the problem of grouping contours in a single image aided by a related image, such as stereo pair, a frame from the same motion sequence, or a similar image. [sent-19, score-1.426]
</p><p>11 Relative motion of contours in one image to their matching contours in the other provides a cue for grouping. [sent-20, score-1.959]
</p><p>12 The contours themselves are detected bottom-up without a model, and are provided as input to our method. [sent-21, score-0.756]
</p><p>13 While contours already represent groupings of edges, they typically lack large spatial support. [sent-22, score-0.847]
</p><p>14 Region segments, on the other hand, have large spatial support, but lack the structure that contours provide. [sent-23, score-0.796]
</p><p>15 Therefore, additional grouping of contours can give us both qualities. [sent-24, score-0.895]
</p><p>16 This has important applications for object recognition and scene understanding, since these larger groups of contours are often large pieces of objects. [sent-25, score-0.877]
</p><p>17 Figure 1 shows a single image in the 1st column, with contours; in the other columns, top row, are different images related by stereo, motion and similarity to the ﬁrst, shown with their contours. [sent-26, score-0.438]
</p><p>18 Below each of these images are idealized groupings of contours in the original image. [sent-27, score-0.933]
</p><p>19 Note that internal contours on cars and buildings are grouped, providing rich, structured shape information over a larger image region. [sent-28, score-1.169]
</p><p>20 1  Stereo  Motion  Similarity Image 2 Image 1  Grouping in image 1 using related images  Figure 1: Contours (white) in the image on the left can be further grouped using the contours of a second, related image (top row). [sent-29, score-1.442]
</p><p>21 2  Related Work  Stereo, motion, and similar image matching have been studied largely in isolation, and often with different purposes in mind than perceptual grouping. [sent-31, score-0.324]
</p><p>22 Much of the stereo literature focuses on perpixel depth recovery; however, as [7] noted, stereo can be used for perceptual grouping without requiring precise depth estimation. [sent-32, score-0.639]
</p><p>23 These approaches to motion and stereo are largely region-based, and therefore do not provide the same internal structure that groups of contours provide. [sent-34, score-1.172]
</p><p>24 Similar image matching has been used for object recognition [1], but is rarely applied to image segmentation. [sent-35, score-0.577]
</p><p>25 In work on contours, [12] matched contour points in the context of aerial imagery, but use constraints such as ordering of matches along scanlines that are not appropriate for motion or similar images, and do not provide grouping information. [sent-36, score-0.72]
</p><p>26 [9] grouped image pixels into contours according to similar motion using optical ﬂow as a local cue. [sent-37, score-1.124]
</p><p>27 [8] grouped and matched image regions across different images and unstable segmentations (as we do with contours), but the regions lack internal structure. [sent-39, score-0.394]
</p><p>28 [2, 6] used stereo pairs of images to detect depth discontinuities as potential object boundaries. [sent-40, score-0.441]
</p><p>29 However, these methods will not detect and group group contours in the interior of fronto-parallel surfaces. [sent-41, score-0.82]
</p><p>30 A set of contours Ii , where the jth contour Ci Ii is an ordered subset of points in Ii : j C pkn ] Ii . [sent-48, score-1.027]
</p><p>31 A transformation Ti that aligns a subset of contours (e. [sent-50, score-0.788]
</p><p>32 A subset of contours Coni in each image, known as a context, such that the two subsets C C 0 1 I1 , Con2 0 1 I2 . [sent-55, score-0.756]
</p><p>33 Coni = Con1 Con2 ; Con1 i i i i j Each Coni is vector that indicates which contours are in the context for image Ij . [sent-57, score-1.044]
</p><p>34 j We further deﬁne the following variables on contours C1 = [pk1 pkn ]: j 1. [sent-59, score-0.789]
</p><p>35 Good continuation - for contours that overlap signiﬁcantly, we prefer that they are present in the same group, if they are grouped at all. [sent-69, score-0.794]
</p><p>36 Common fate by shape: contours with similar transformations mapping them to their matches should be grouped. [sent-71, score-0.93]
</p><p>37 We also require that each contour point in a grouped contour and its matching point in the second image have similar local shape. [sent-72, score-0.794]
</p><p>38 Shape in each image is deﬁned with respect to a subset of image contours known as a context; we will explain the importance of choosing the correct context for shape comparison, as ﬁrst noted in [16]. [sent-73, score-1.418]
</p><p>39 Maximality/simplicity: We would like to group as many of the contours as possible into as few groups as possible, while still maintaining the similarity of local shape described above. [sent-75, score-1.063]
</p><p>40 This combination of the MRF standard graph matching technique with an LP for inferring context for accurate matching by shape is our main contribution. [sent-81, score-0.604]
</p><p>41 4  Matching and Context Selection  We can evaluate the hypothesis of a particular contour point match by comparing the local shape around the point and around its match. [sent-84, score-0.492]
</p><p>42 Although local features such as curvature and simple proximity (in the case of roughly aligned contours) have been used for matching ([12]), inconsistencies in the input contours across two images make these them prone to error. [sent-85, score-1.049]
</p><p>43 In a), the contours do not match, while in e), a “7” shape is common to both. [sent-89, score-0.947]
</p><p>44 As an example, we use the shape context, an image feature that has been widely used for shape matching ([1]). [sent-98, score-0.706]
</p><p>45 Brieﬂy, a shape context provides a log-polar spatial histogram that records the number of points that fall into a particular bin. [sent-99, score-0.336]
</p><p>46 In Figure 2 c,g), shape contexts (darker bins mean larger bin count) with large spatial support placed according to the rough alignment exhibit high dissimilarity in both cases, failing to ﬁnd a match in a). [sent-100, score-0.519]
</p><p>47 The large feature failed because contours in each image that had no match in the other image were used in computing the shape context. [sent-101, score-1.398]
</p><p>48 Inferring which contours to include and which to omit would give better features, as in Figure 2 d),h). [sent-102, score-0.756]
</p><p>49 This ﬁxes the completeness problem, while retaining soundness: no combination of contours in the two images in a) can produce matching shapes. [sent-103, score-1.012]
</p><p>50 Therefore, with rough alignment we can cast the ﬁrst step of shape matching as context selection: which subset of contours, or context, to use for feature computation. [sent-104, score-0.478]
</p><p>51 Given the correct context, matching individual contour points is much easier. [sent-105, score-0.379]
</p><p>52 1 Selection Problem We can neatly summarize the effect of a particular context selection on a shape context as seen in Figure 3. [sent-107, score-0.45]
</p><p>53 The bin counts for a shape context can be encoded as a vector, represented by the shape contexts and their vector representations alongside. [sent-110, score-0.676]
</p><p>54 SC has dimensions nBins by nContours, where nBins is the number of bins in the shape context (and the length of the associated vector). [sent-112, score-0.321]
</p><p>55 The entry SC(i j) is the bin count for bin i and contour j. [sent-113, score-0.352]
</p><p>56 For each j 0 1 , which indicates contour Ci in an image Ii , we associate an selection indicator variable selj i whether or not the contour is selected; the vector of these indicator variables is seli . [sent-114, score-0.727]
</p><p>57 Then the shape context bin counts realized by a particular selection of contours is SCseli , simply the multiplication of the matrix SC and the vector seli . [sent-115, score-1.254]
</p><p>58 d) shows the effect on the shape context histogram of various context selections. [sent-116, score-0.401]
</p><p>59 2 Shape context matching cost The effectiveness of selection depends signiﬁcantly on the shape context matching cost. [sent-118, score-0.755]
</p><p>60 Traditional matching costs (Chi-square, L1 , L2 ) only measure similarity, but selecting no contours in either image gives a perfect matching cost, since in both shape contexts, all bins will be 0. [sent-119, score-1.437]
</p><p>61 While similarity is important, so is including more contours rather than fewer (maximality). [sent-120, score-0.789]
</p><p>62 The selection process is run for each transformation to infer a context suitable for evaluating contour point matches via shape. [sent-122, score-0.5]
</p><p>63 The intersection term encourages higher bin counts in each shape context and therefore the inclusion of more contours. [sent-124, score-0.394]
</p><p>64 We begin with two C input images, where the contours are in rough alignment (by applying known Ti to I1 ). [sent-130, score-0.797]
</p><p>65 Multiple shape contexts are placed in each image on a uniform grid (an approximation of Matchj l(j)=i , since we initially have no matches). [sent-131, score-0.465]
</p><p>66 Our goal is to select contours in each image to minimize the sum of SCMatchCost for each pair of shape contexts. [sent-133, score-1.13]
</p><p>67 For each shape context j in each image i, we compute the corresponding j j shape context matrix SCi . [sent-134, score-0.775]
</p><p>68 SCi j for each image has been color coded to show the SCi matrix corresponding to each shape context. [sent-136, score-0.374]
</p><p>69 SCi selci is then the realized bin counts for i all the shape contexts in image Ii under selection selci . [sent-139, score-0.692]
</p><p>70 We seek to choose selc1 and selc2 such that SC1 selc1 SC2 selc2 in a shape sense; entries of SC1 selc1 and SC2 selc2 , or realized bin counts, are in correspondence, so we can score these pairs of bin counts using BinMatchCost. [sent-140, score-0.405]
</p><p>71 Figure 5 depicts an idealized selection process for two images (only the contours are shown). [sent-156, score-0.931]
</p><p>72 For groups of SIFT matches that describe similar transformations, a transformation Ti is extracted and warps the contours in image 1 to line up with those of image 2, in c). [sent-157, score-1.32]
</p><p>73 The selection problem is formulated separately for each set of aligned contours d). [sent-158, score-0.84]
</p><p>74 Two correct transforms align the car and person, and the selection result includes the respective contours (rows 1,2 of e). [sent-160, score-0.825]
</p><p>75 We can view the context selection procedure for minimizing Fi as choosing the context of contours so as to best reduce the matching cost of the hypothesized inter-image matches for contours with label i, under the transformation Ti . [sent-162, score-2.081]
</p><p>76 5  Graph Cuts for Group Assignment and Matching  We previously computed context selections (as solutions to the SelectionCost LP), which found groups of contours in each image that have similar shape, Con = {Con1 , . [sent-165, score-1.115]
</p><p>77 Note that a j contour C1 need not be selected as context in a particular group a in order to have lj = a. [sent-174, score-0.472]
</p><p>78 j k The binary potentials φ(lj , lk ) encode the preference that overlapping contours C1 , C1 have the same label: 1 a=b φ(lj = a, lk = b) = (1) 1−τ a=b  where 0 ≤ τ ≤ 1 controls the penalty of having different labels. [sent-177, score-0.808]
</p><p>79 Two contours overlap if they contain at least one point in common. [sent-179, score-0.756]
</p><p>80 , pkn ] can be matched in the  second image with respect to the context {Con1 , Con2 }. [sent-183, score-0.338]
</p><p>81 Instead of using all contours image 1 as nodes in the MN, we only allow contours were selected in at least one of the context Con1 ; likewise, we only permit i matches to points in image 2 that appear in a contour selected in at least one Con2 . [sent-186, score-2.319]
</p><p>82 This better j allows us to deal with contours that appear only in one image and thus cannot be reliably grouped based on relative motion. [sent-187, score-0.977]
</p><p>83 Columns 3,4: grouping results for our method and baseline; groups of contours are a single color. [sent-190, score-0.946]
</p><p>84 If more than half the points in the ﬁnal assignment ∗ lj for a contour were occluded, we marked the entire contour as occluded, and it was not displayed. [sent-200, score-0.573]
</p><p>85 Since we omitted all selection information, all contours in the 1st image were included in the MN P as nodes, and their contour points were allowed to match to any contour point in I2 . [sent-201, score-1.527]
</p><p>86 6  Experiments  We tested our method and the baseline over stereo, motion and similar image pairs. [sent-204, score-0.35]
</p><p>87 Input contours in each image were extracted automatically using the method of [15]. [sent-205, score-0.956]
</p><p>88 SIFT matches were extracted from images, keeping only conﬁdent matches as described in [10]; matches proposing similar transformations were pruned to a small set, typically 10-20. [sent-206, score-0.403]
</p><p>89 Because of the high quality of the inferred contexts, we used large shape contexts (radius 90 pixels, in images of size 400 by 500), which 7  made matching very robust. [sent-207, score-0.522]
</p><p>90 Shape contexts were placed on a uniform grid atop the registered contours (via Ti ) with a spacing 50 pixels in the x and y dimensions. [sent-209, score-0.871]
</p><p>91 Image pairs were taken from the Caltech 101 dataset [4] and from a stereo rig with 1m baseline mounted on a car from our lab (providing stereo and motion images). [sent-210, score-0.637]
</p><p>92 Groups for stereo image pairs are colored according to disparity. [sent-214, score-0.408]
</p><p>93 Due to the lack of large context, the baseline method is able to ﬁnd a good match for a given contour point under almost any group hypothesis lj = a, since in cluttered regions, there are always nearby matches. [sent-215, score-0.518]
</p><p>94 7  Conclusion  We introduced the problem of grouping of contours in an image using a related image, such as stereo, motion or similar, as an important step for object recognition and scene understanding. [sent-219, score-1.271]
</p><p>95 Grouping depends on the ability to match contours across images to determine their relative motion. [sent-220, score-0.958]
</p><p>96 Selecting a good context for shape evaluation was key to robust simultaneous and grouping of contours across images. [sent-221, score-1.209]
</p><p>97 Future work will include trying to learn 3D object models from stereo and motion images, and a probabilistic formulation of the matching framework. [sent-223, score-0.542]
</p><p>98 Introducing learning to improve the grouping result is also an area of signiﬁcant interest; some shape conﬁgurations are more reliable for matching than others. [sent-224, score-0.471]
</p><p>99 Recognition of object contours from stereo images: an edge combination approach. [sent-258, score-1.034]
</p><p>100 Contour context selection for object detection: A set-to-set contour matching approach. [sent-317, score-0.564]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('contours', 0.756), ('stereo', 0.225), ('contour', 0.216), ('shape', 0.191), ('image', 0.183), ('matching', 0.141), ('grouping', 0.139), ('sc', 0.132), ('motion', 0.123), ('lj', 0.119), ('coni', 0.114), ('context', 0.105), ('images', 0.099), ('matches', 0.098), ('contexts', 0.091), ('match', 0.085), ('selectioncost', 0.076), ('transformations', 0.076), ('bin', 0.068), ('matchj', 0.063), ('ti', 0.057), ('object', 0.053), ('groups', 0.051), ('groupings', 0.051), ('scmatchcost', 0.051), ('selection', 0.049), ('mn', 0.046), ('sift', 0.046), ('baseline', 0.044), ('pami', 0.044), ('sci', 0.043), ('grouped', 0.038), ('lp', 0.038), ('maximality', 0.038), ('pki', 0.038), ('selj', 0.038), ('ta', 0.038), ('aligned', 0.035), ('pkn', 0.033), ('similarity', 0.033), ('transformation', 0.032), ('group', 0.032), ('counts', 0.03), ('realized', 0.03), ('ii', 0.027), ('idealized', 0.027), ('graph', 0.026), ('lk', 0.026), ('occluded', 0.026), ('binmatchcost', 0.025), ('liming', 0.025), ('matchcostincontext', 0.025), ('nbins', 0.025), ('ngroups', 0.025), ('occlusionthresh', 0.025), ('qrn', 0.025), ('scmatchcos', 0.025), ('selci', 0.025), ('seli', 0.025), ('bins', 0.025), ('closest', 0.025), ('depth', 0.025), ('unary', 0.025), ('pixels', 0.024), ('min', 0.023), ('mismatch', 0.023), ('cost', 0.023), ('lack', 0.022), ('points', 0.022), ('criteria', 0.022), ('jianbo', 0.022), ('grasp', 0.022), ('buildings', 0.022), ('discontinuities', 0.022), ('rough', 0.022), ('mrf', 0.02), ('selections', 0.02), ('soundness', 0.02), ('ci', 0.02), ('car', 0.02), ('cuts', 0.02), ('dense', 0.02), ('alignment', 0.019), ('across', 0.018), ('fi', 0.018), ('seek', 0.018), ('swap', 0.018), ('spatial', 0.018), ('shapes', 0.017), ('recognition', 0.017), ('xes', 0.017), ('internal', 0.017), ('extracted', 0.017), ('segments', 0.017), ('matched', 0.017), ('potential', 0.017), ('completeness', 0.016), ('hypothesized', 0.016), ('edges', 0.016), ('proposing', 0.016)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="95-tfidf-1" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>2 0.39855045 <a title="95-tfidf-2" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>Author: Abhinav Gupta, Jianbo Shi, Larry S. Davis</p><p>Abstract: We present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufﬁcient for representing context. Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or “informative” background(context). We present a “shape-aware” model which utilizes contour information for efﬁcient and accurate labeling of features in the image. Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity. 1</p><p>3 0.24526483 <a title="95-tfidf-3" href="./nips-2008-Multiscale_Random_Fields_with_Application_to_Contour_Grouping.html">147 nips-2008-Multiscale Random Fields with Application to Contour Grouping</a></p>
<p>Author: Longin J. Latecki, Chengen Lu, Marc Sobel, Xiang Bai</p><p>Abstract: We introduce a new interpretation of multiscale random ﬁelds (MSRFs) that admits efﬁcient optimization in the framework of regular (single level) random ﬁelds (RFs). It is based on a new operator, called append, that combines sets of random variables (RVs) to single RVs. We assume that a MSRF can be decomposed into disjoint trees that link RVs at different pyramid levels. The append operator is then applied to map RVs in each tree structure to a single RV. We demonstrate the usefulness of the proposed approach on a challenging task involving grouping contours of target shapes in images. It provides a natural representation of multiscale contour models, which is needed in order to cope with unstable contour decompositions. The append operator allows us to ﬁnd optimal image segment labels using the classical framework of relaxation labeling. Alternative methods like Markov Chain Monte Carlo (MCMC) could also be used.</p><p>4 0.15586901 <a title="95-tfidf-4" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>Author: Alex J. Smola, Julian J. Mcauley, Tibério S. Caetano</p><p>Abstract: Models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the isometric assumption. However, real shapes from image datasets, even when expected to be related by “almost isometric” transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale. In this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction. The outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations. Our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times. 1</p><p>5 0.11666238 <a title="95-tfidf-5" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>Author: Geremy Heitz, Gal Elidan, Benjamin Packer, Daphne Koller</p><p>Abstract: Discriminative tasks, including object categorization and detection, are central components of high-level computer vision. Sometimes, however, we are interested in more reﬁned aspects of the object in an image, such as pose or particular regions. In this paper we develop a method (LOOPS) for learning a shape and image feature model that can be trained on a particular object class, and used to outline instances of the class in novel images. Furthermore, while the training data consists of uncorresponded outlines, the resulting LOOPS model contains a set of landmark points that appear consistently across instances, and can be accurately localized in an image. Our model achieves state-of-the-art results in precisely outlining objects that exhibit large deformations and articulations in cluttered natural images. These localizations can then be used to address a range of tasks, including descriptive classiﬁcation, search, and clustering. 1</p><p>6 0.10880589 <a title="95-tfidf-6" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>7 0.10279786 <a title="95-tfidf-7" href="./nips-2008-Nonrigid_Structure_from_Motion_in_Trajectory_Space.html">157 nips-2008-Nonrigid Structure from Motion in Trajectory Space</a></p>
<p>8 0.094628647 <a title="95-tfidf-8" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>9 0.091668487 <a title="95-tfidf-9" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>10 0.085031047 <a title="95-tfidf-10" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>11 0.078332685 <a title="95-tfidf-11" href="./nips-2008-Model_selection_and_velocity_estimation_using_novel_priors_for_motion_patterns.html">136 nips-2008-Model selection and velocity estimation using novel priors for motion patterns</a></p>
<p>12 0.07824865 <a title="95-tfidf-12" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>13 0.076961234 <a title="95-tfidf-13" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>14 0.073982388 <a title="95-tfidf-14" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>15 0.073580228 <a title="95-tfidf-15" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>16 0.071404755 <a title="95-tfidf-16" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>17 0.06367778 <a title="95-tfidf-17" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>18 0.059039302 <a title="95-tfidf-18" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>19 0.054956526 <a title="95-tfidf-19" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>20 0.049637631 <a title="95-tfidf-20" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.151), (1, -0.132), (2, 0.154), (3, -0.194), (4, 0.031), (5, -0.01), (6, -0.142), (7, -0.258), (8, 0.036), (9, -0.044), (10, -0.1), (11, -0.128), (12, 0.094), (13, -0.046), (14, 0.125), (15, -0.001), (16, 0.085), (17, 0.186), (18, -0.068), (19, 0.02), (20, -0.083), (21, -0.002), (22, -0.022), (23, 0.009), (24, 0.087), (25, 0.197), (26, 0.058), (27, 0.077), (28, 0.008), (29, 0.115), (30, -0.1), (31, 0.069), (32, 0.092), (33, -0.001), (34, -0.067), (35, 0.045), (36, -0.086), (37, -0.049), (38, -0.058), (39, 0.188), (40, -0.119), (41, 0.071), (42, 0.008), (43, -0.025), (44, 0.033), (45, -0.067), (46, 0.124), (47, -0.031), (48, -0.042), (49, 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.97285897 <a title="95-lsi-1" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>2 0.85058832 <a title="95-lsi-2" href="./nips-2008-Multiscale_Random_Fields_with_Application_to_Contour_Grouping.html">147 nips-2008-Multiscale Random Fields with Application to Contour Grouping</a></p>
<p>Author: Longin J. Latecki, Chengen Lu, Marc Sobel, Xiang Bai</p><p>Abstract: We introduce a new interpretation of multiscale random ﬁelds (MSRFs) that admits efﬁcient optimization in the framework of regular (single level) random ﬁelds (RFs). It is based on a new operator, called append, that combines sets of random variables (RVs) to single RVs. We assume that a MSRF can be decomposed into disjoint trees that link RVs at different pyramid levels. The append operator is then applied to map RVs in each tree structure to a single RV. We demonstrate the usefulness of the proposed approach on a challenging task involving grouping contours of target shapes in images. It provides a natural representation of multiscale contour models, which is needed in order to cope with unstable contour decompositions. The append operator allows us to ﬁnd optimal image segment labels using the classical framework of relaxation labeling. Alternative methods like Markov Chain Monte Carlo (MCMC) could also be used.</p><p>3 0.79483604 <a title="95-lsi-3" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>Author: Abhinav Gupta, Jianbo Shi, Larry S. Davis</p><p>Abstract: We present an approach that combines bag-of-words and spatial models to perform semantic and syntactic analysis for recognition of an object based on its internal appearance and its context. We argue that while object recognition requires modeling relative spatial locations of image features within the object, a bag-of-word is sufﬁcient for representing context. Learning such a model from weakly labeled data involves labeling of features into two classes: foreground(object) or “informative” background(context). We present a “shape-aware” model which utilizes contour information for efﬁcient and accurate labeling of features in the image. Our approach iterates between an MCMC-based labeling and contour based labeling of features to integrate co-occurrence of features and shape similarity. 1</p><p>4 0.61560947 <a title="95-lsi-4" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>Author: Geremy Heitz, Gal Elidan, Benjamin Packer, Daphne Koller</p><p>Abstract: Discriminative tasks, including object categorization and detection, are central components of high-level computer vision. Sometimes, however, we are interested in more reﬁned aspects of the object in an image, such as pose or particular regions. In this paper we develop a method (LOOPS) for learning a shape and image feature model that can be trained on a particular object class, and used to outline instances of the class in novel images. Furthermore, while the training data consists of uncorresponded outlines, the resulting LOOPS model contains a set of landmark points that appear consistently across instances, and can be accurately localized in an image. Our model achieves state-of-the-art results in precisely outlining objects that exhibit large deformations and articulations in cluttered natural images. These localizations can then be used to address a range of tasks, including descriptive classiﬁcation, search, and clustering. 1</p><p>5 0.5954594 <a title="95-lsi-5" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>Author: Alex J. Smola, Julian J. Mcauley, Tibério S. Caetano</p><p>Abstract: Models for near-rigid shape matching are typically based on distance-related features, in order to infer matches that are consistent with the isometric assumption. However, real shapes from image datasets, even when expected to be related by “almost isometric” transformations, are actually subject not only to noise but also, to some limited degree, to variations in appearance and scale. In this paper, we introduce a graphical model that parameterises appearance, distance, and angle features and we learn all of the involved parameters via structured prediction. The outcome is a model for near-rigid shape matching which is robust in the sense that it is able to capture the possibly limited but still important scale and appearance variations. Our experimental results reveal substantial improvements upon recent successful models, while maintaining similar running times. 1</p><p>6 0.46364892 <a title="95-lsi-6" href="./nips-2008-Nonrigid_Structure_from_Motion_in_Trajectory_Space.html">157 nips-2008-Nonrigid Structure from Motion in Trajectory Space</a></p>
<p>7 0.37555972 <a title="95-lsi-7" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>8 0.37258118 <a title="95-lsi-8" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>9 0.36014003 <a title="95-lsi-9" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>10 0.32860202 <a title="95-lsi-10" href="./nips-2008-An_ideal_observer_model_of_infant_object_perception.html">23 nips-2008-An ideal observer model of infant object perception</a></p>
<p>11 0.32182726 <a title="95-lsi-11" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>12 0.31023952 <a title="95-lsi-12" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>13 0.30727342 <a title="95-lsi-13" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>14 0.29308999 <a title="95-lsi-14" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>15 0.27823117 <a title="95-lsi-15" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>16 0.27722415 <a title="95-lsi-16" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>17 0.25965813 <a title="95-lsi-17" href="./nips-2008-Breaking_Audio_CAPTCHAs.html">41 nips-2008-Breaking Audio CAPTCHAs</a></p>
<p>18 0.24383064 <a title="95-lsi-18" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>19 0.24106999 <a title="95-lsi-19" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>20 0.22222747 <a title="95-lsi-20" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.031), (7, 0.078), (12, 0.089), (15, 0.015), (28, 0.104), (57, 0.088), (59, 0.038), (63, 0.017), (66, 0.235), (71, 0.027), (77, 0.032), (81, 0.023), (83, 0.112)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.84422803 <a title="95-lda-1" href="./nips-2008-An_Efficient_Sequential_Monte_Carlo_Algorithm_for_Coalescent_Clustering.html">18 nips-2008-An Efficient Sequential Monte Carlo Algorithm for Coalescent Clustering</a></p>
<p>Author: Dilan Gorur, Yee W. Teh</p><p>Abstract: We propose an efﬁcient sequential Monte Carlo inference scheme for the recently proposed coalescent clustering model [1]. Our algorithm has a quadratic runtime while those in [1] is cubic. In experiments, we were surprised to ﬁnd that in addition to being more efﬁcient, it is also a better sequential Monte Carlo sampler than the best in [1], when measured in terms of variance of estimated likelihood and effective sample size. 1</p><p>same-paper 2 0.7979843 <a title="95-lda-2" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>3 0.70910734 <a title="95-lda-3" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>4 0.61552614 <a title="95-lda-4" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>Author: Geremy Heitz, Gal Elidan, Benjamin Packer, Daphne Koller</p><p>Abstract: Discriminative tasks, including object categorization and detection, are central components of high-level computer vision. Sometimes, however, we are interested in more reﬁned aspects of the object in an image, such as pose or particular regions. In this paper we develop a method (LOOPS) for learning a shape and image feature model that can be trained on a particular object class, and used to outline instances of the class in novel images. Furthermore, while the training data consists of uncorresponded outlines, the resulting LOOPS model contains a set of landmark points that appear consistently across instances, and can be accurately localized in an image. Our model achieves state-of-the-art results in precisely outlining objects that exhibit large deformations and articulations in cluttered natural images. These localizations can then be used to address a range of tasks, including descriptive classiﬁcation, search, and clustering. 1</p><p>5 0.60892338 <a title="95-lda-5" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>Author: Xuming He, Richard S. Zemel</p><p>Abstract: Extensive labeled data for image annotation systems, which learn to assign class labels to image regions, is difﬁcult to obtain. We explore a hybrid model framework for utilizing partially labeled data that integrates a generative topic model for image appearance with discriminative label prediction. We propose three alternative formulations for imposing a spatial smoothness prior on the image labels. Tests of the new models and some baseline approaches on three real image datasets demonstrate the effectiveness of incorporating the latent structure. 1</p><p>6 0.59625506 <a title="95-lda-6" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>7 0.59599251 <a title="95-lda-7" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>8 0.595415 <a title="95-lda-8" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>9 0.59304357 <a title="95-lda-9" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>10 0.58990169 <a title="95-lda-10" href="./nips-2008-Bayesian_Kernel_Shaping_for_Learning_Control.html">32 nips-2008-Bayesian Kernel Shaping for Learning Control</a></p>
<p>11 0.5886662 <a title="95-lda-11" href="./nips-2008-Analyzing_human_feature_learning_as_nonparametric_Bayesian_inference.html">26 nips-2008-Analyzing human feature learning as nonparametric Bayesian inference</a></p>
<p>12 0.58329272 <a title="95-lda-12" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>13 0.58190548 <a title="95-lda-13" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>14 0.58015561 <a title="95-lda-14" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>15 0.57982802 <a title="95-lda-15" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>16 0.57952136 <a title="95-lda-16" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>17 0.57836449 <a title="95-lda-17" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>18 0.57588446 <a title="95-lda-18" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>19 0.57517022 <a title="95-lda-19" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>20 0.57424563 <a title="95-lda-20" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
