<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>124 nips-2008-Load and Attentional Bayes</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-124" href="#">nips2008-124</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>124 nips-2008-Load and Attentional Bayes</h1>
<br/><p>Source: <a title="nips-2008-124-pdf" href="http://papers.nips.cc/paper/3516-load-and-attentional-bayes.pdf">pdf</a></p><p>Author: Peter Dayan</p><p>Abstract: Selective attention is a most intensively studied psychological phenomenon, rife with theoretical suggestions and schisms. A critical idea is that of limited capacity, the allocation of which has produced continual conﬂict about such phenomena as early and late selection. An inﬂuential resolution of this debate is based on the notion of perceptual load (Lavie, 2005), which suggests that low-load, easy tasks, because they underuse the total capacity of attention, mandatorily lead to the processing of stimuli that are irrelevant to the current attentional set; whereas high-load, difﬁcult tasks grab all resources for themselves, leaving distractors high and dry. We argue that this theory presents a challenge to Bayesian theories of attention, and suggest an alternative, statistical, account of key supporting data. 1</p><p>Reference: <a title="nips-2008-124-reference" href="../nips2008_reference/nips-2008-Load_and_Attentional_Bayes_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 uk  Abstract Selective attention is a most intensively studied psychological phenomenon, rife with theoretical suggestions and schisms. [sent-4, score-0.096]
</p><p>2 A critical idea is that of limited capacity, the allocation of which has produced continual conﬂict about such phenomena as early and late selection. [sent-5, score-0.082]
</p><p>3 1  Introduction  It was some ﬁfty years after James (1950)’s famously poetic description of our capacities for attention that more analytically-directed experiments began, based originally on dichotic listening Cherry (1953). [sent-8, score-0.182]
</p><p>4 Various forms, interpretations and conﬂicts about these three tasks have permeated the ﬁeld of attention ever since (Driver, 2001; Paschler, 1998), driven by different notions of the computational tasks and constraints at hand. [sent-10, score-0.18]
</p><p>5 The experiments in dichotic listening coincided with the quickly burgeoning realization that mathematical concepts from Shannonian information theory would be very helpful for understanding biological information processing. [sent-11, score-0.086]
</p><p>6 One central concept in information theory is that of a limited capacity channel, and Broadbent (1958) adopted this as a formal basis for understanding the necessity for, and hence the nature of, selection. [sent-12, score-0.056]
</p><p>7 James (1986) (based on inﬂuential experiments on distractor processing such as Eriksen and Eriksen, 1974), which suggests that the smaller the attentional focus, the more intense it can somehow be, given that the limited capacity is ‘spread’ over a smaller area. [sent-16, score-0.662]
</p><p>8 However, of course, late selection makes little sense from a limited capacity viewpoint; and short of a theory of what controls the degree of attenuation of irrelevant stimuli, Treisman (1960)’s idea is hard to falsify. [sent-17, score-0.222]
</p><p>9 To reiterate, the attentional load hypothesis, although an attractive formalization of attenuation, suggests that the brain is unable on easy tasks to exclude information that is known to be irrelevant. [sent-19, score-0.786]
</p><p>10 It therefore involves an arguably infelicitous combination of sophisticated attentional shaping (as to what can be attended in high-load situations) with inept control. [sent-20, score-0.295]
</p><p>11 Although the Bayesian revolution in cognitive science has had a huge impact over modern views of sensory processing (see, for instance, Rao et al. [sent-21, score-0.045]
</p><p>12 This is despite the many other computational models of attention (see Itti and Koch, 2001; Zhaoping, 2006). [sent-24, score-0.096]
</p><p>13 Indeed, Whiteley and Sahani (2008) have suggested that this lacuna arises from a focus on optimal Bayesian inference in the face of small numbers of objects in the focus of attention, rather than the necessity of using approximate methods in the light of realistic, cluttered, complex scenes. [sent-25, score-0.099]
</p><p>14 They acknowledge that there is a critical limited resource coming from the existence of neurons with large receptive ﬁelds into which experimenters slot multiple sensory objects, some relevant, some irrelevant. [sent-28, score-0.197]
</p><p>15 Probabilistically-correct inference should then implement selection, when data that is known to be irrelevant is excluded to the advantage of the relevant information (eg Dayan and Zemel, 1999; Palmer, 1994). [sent-29, score-0.091]
</p><p>16 However, in other circumstances, it will be appropriate to take advantage of the information about the target that is available in the neurons with large ﬁelds, even if this means allowing some inﬂuence on the ﬁnal decisions from distractors. [sent-30, score-0.115]
</p><p>17 Here, we build a Bayesian-inspired account of key data used to argue for the attentional load hypothesis (based on an extension of Yu et al. [sent-31, score-0.77]
</p><p>18 Subjects had to report the identity of a target letter that was either an ‘X’ or an ‘N’ (here, the former) presented in one of eight locations arranged in a circle around the ﬁxation point. [sent-35, score-0.18]
</p><p>19 There was also a distractor letter in the further periphery (the larger ‘N’) which was either compatible (ie the same as the target), incompatible (as here, the opposite of the target), or, in so-called neutral trials, a different letter altogether. [sent-37, score-0.784]
</p><p>20 Figure 1A is a high-load condition, in that there are irrelevant non-targets in the remaining 7 positions around the circle. [sent-39, score-0.054]
</p><p>21 Figure 1C is a critical control, called the degraded low-load condition, and was actually the main topic of Lavie and de Fockert (2003). [sent-41, score-0.13]
</p><p>22 In this, the difﬁculty of the sensory processing was increased (by making the target smaller and dimmer) without changing the attentional (ie selectional) load. [sent-42, score-0.429]
</p><p>23 Figure 1D shows the mean reaction times (RTs) for these conditions for the three sorts of distractor (RTs sufﬁce here, since there was no speed accuracy tradeoff at work in the different conditions; data not shown). [sent-43, score-0.372]
</p><p>24 The central ﬁnding about attentional load is that the distractor exerted a signiﬁcant effect over target processing only in the low load case – that is, an incompatible distractor slowed down the RTs compared with a neutral distractor for the low load case but not the high load case. [sent-45, score-3.656]
</p><p>25 Figure 1: The attentional load task, from Lavie and de Fockert (2003). [sent-46, score-0.744]
</p><p>26 Subjects had to judge whether a target letter in the central circle around ﬁxation was ‘N’ or ‘X’ in the face of a compatible, incompatible (shown) or neutral distractor. [sent-47, score-0.43]
</p><p>27 C) degraded low-load condition with no non-targets but a smaller (not shown) and darker target. [sent-50, score-0.138]
</p><p>28 Since, in the degraded low-load case the RTs were slower but the inﬂuence of the distractor was if anything greater, this could not just be a function of the processing time or difﬁculty. [sent-53, score-0.438]
</p><p>29 It is apparent that compatible distractors were of almost no help in any case, whereas incompatible distractors were harmful. [sent-56, score-0.435]
</p><p>30 3  The Bayesian model  The data in ﬁgure 1 pose the question for normative modeling as to why the distractor would corrupt processing of the target in the easy, low-load, case, but not the difﬁcult, high-load case. [sent-57, score-0.491]
</p><p>31 No normative account could simply assume that extra data ‘leak’ through in the low-load condition (which is the attentional load hypothesis) if the subjects have the ability to fashion attention far more ﬁnely in other cases, such as that of high load. [sent-58, score-0.978]
</p><p>32 In this case, normative processing will combine information from all the receptive ﬁelds, with Bayesian inference and marginalization exactly eliminating any substantial impact from those that are useless or confusing. [sent-60, score-0.16]
</p><p>33 In the high load case, the proximal non-target stimuli have the effect of adding so much extra noise to the units with large receptive ﬁelds compared with their signal about the target, that only the smallest receptive ﬁelds will be substantially useful. [sent-61, score-0.838]
</p><p>34 This implies that the distractor will exert little inﬂuence. [sent-62, score-0.376]
</p><p>35 In the low load case, large receptive ﬁelds that also include the distractor will be usefully informative about the target, and so the distractor will exert an inﬂuence. [sent-63, score-1.343]
</p><p>36 Note that this happens automatically through inference – indeed to make this point starkly, there is no explicit attentional control signal in our model whatsoever, only inference and marginalization. [sent-64, score-0.343]
</p><p>37 load low high  n 0 +1  neutral t n +c 0 +c -1  d 0 0  n 0 +1  incompatible t n d +c 0 -1 +c -1 -1  n 0 +1  compatible t n +c 0 +c -1  d +1 +1  Table 1: Our version of the task. [sent-66, score-0.912]
</p><p>38 Each display consists of four stimulus positions labelled n for the non-targets; t for the target (shown in the table, though not the display, as being boxed); and d for the distractor, which is relatively far from the target. [sent-68, score-0.115]
</p><p>39 The target takes the values ±c, where c acts like a contrast; subjects have to report its sign. [sent-69, score-0.148]
</p><p>40 The distractor can be 0 (neutral) or ±1; and is compatible if it has the same sign as the target (and conversely, incompatible). [sent-70, score-0.609]
</p><p>41 3) only being run for the case of low load, as in ﬁgure 1D. [sent-74, score-0.041]
</p><p>42 The target takes the value ±c; subjects have to report its sign. [sent-77, score-0.148]
</p><p>43 The distractor can be neutral (0) or have the same sign as (compatible) or a different sign from (incompatible) the target. [sent-78, score-0.529]
</p><p>44 In the low load condition, the non-target units are 0; in the high load, one is +1; the other is −1, making them balanced, but confusing, because they lead to excess noise. [sent-79, score-0.651]
</p><p>45 We assume that the subject performs inference about the sign of the target based on noisy observations created by a generative model. [sent-81, score-0.232]
</p><p>46 In the generative model, the values in table 1 amount to hidden structure, which, as in Yu et al. [sent-82, score-0.07]
</p><p>47 (2008), is mapped and mixed through various receptive ﬁelds to provide the noisy input to a Bayesian recognition model. [sent-83, score-0.084]
</p><p>48 The job of the recognition model is to calculate the posterior probability of the various hidden settings given data, and, by marginalizing (summing) out all the hidden settings apart from the state of the target, report on its sign. [sent-84, score-0.06]
</p><p>49 Figure 2A shows the generative model, indicating the receptive ﬁelds (RFs) associated with this mixing. [sent-85, score-0.124]
</p><p>50 We consider 8 topographically-mapped units, 4 with small RFs covering only a single input (the generative weights are just the identity map); and 4 with large RFs (in which the inputs are mixed together more holistically). [sent-86, score-0.066]
</p><p>51 For simplicity, we treat the distractor as equidistant from the target and non-target input, partially modeling the fact that it can be in different locations. [sent-88, score-0.452]
</p><p>52 We assume a crude form of signal-dependent noise; it is this that makes the non-target stimuli so devastating. [sent-89, score-0.06]
</p><p>53 Figure 2B shows the means and standard deviations arising from the generative model for the 8 units (one per column) for the six conditions in table 1 (rows from top to bottom – low load: neutral, incompatible, compatible; then high load: neutral, incompatible, compatible). [sent-90, score-0.268]
</p><p>54 The means associated with the small and large RF target units show the lack of bias from the non-targets in the high-load condition; and for the large RF case, the bias associated with the distractor. [sent-92, score-0.221]
</p><p>55 The standard deviations play the most critical role in the model, deﬁning what it means for the nontarget stimuli, when present, to make inference difﬁcult. [sent-93, score-0.118]
</p><p>56 In the high load case, the units with the large RFs are assumed to have very high standard deviations, coming from a crude form of signal-dependent noise. [sent-95, score-0.678]
</p><p>57 This captures the relatively uselessness of these large RFs in the high load condition. [sent-96, score-0.504]
</p><p>58 A  B t  n  d  weights 1  2  3  4  small RFs  5  6  7  large RFs  8  attn load high low  n  mean  unit # 1 2 3 4 5 6 7 8  input  std 1 2 34 5 6 7 8  inco neut comp inco neut comp  n t nd small large RF size  n t nd small large RF size  Figure 2: The generative model. [sent-98, score-0.705]
</p><p>59 A) In the model, the four input units, representing non-targets, the target and the distractor, are assumed to generate 8 input units which fall into two groups, with small and large receptive ﬁelds (RFs). [sent-99, score-0.305]
</p><p>60 B) These plots show the means and standard deviations in the generative model associated with the 8 input units for the low and high load cases shown in table 1 (in raster scan order). [sent-102, score-0.743]
</p><p>61 The means for the large RFs (based on the weights in A) are unaffected by the load; the standard deviations for the units with large receptive ﬁelds are much higher in the high load condition. [sent-103, score-0.746]
</p><p>62 Standard deviations are affected by a coarse form of signal-dependent noise. [sent-104, score-0.052]
</p><p>63 In all cases, a new sample from the generative model is provided at each time step; the noise corrupting each of the observed units is assumed to be Gaussian, and independent across units and over time. [sent-105, score-0.278]
</p><p>64 (2008), it is necessary to perform inference over all the possible values of the hidden variables (all the possible values of the hidden structure2 ), then marginalizing out all the variables apart the the target itself. [sent-108, score-0.212]
</p><p>65 9 is reached on the probability that the target is either positive or negative (reporting whichever one is more likely). [sent-110, score-0.151]
</p><p>66 01 per step of stopping the accumulation and reporting whichever sign of target has a higher probability (guessing randomly if this probability is 0. [sent-112, score-0.191]
</p><p>67 This general pattern of results is robust to many different parameter values; though it is possible (by reducing c) to make inference take very much longer still in the degraded low load condition whilst maintaining and boosting the effect of high load. [sent-122, score-0.72]
</p><p>68 In the low load case, the lack of non-targets means that the inputs based on the large RFs are usefully informative about the target, and therefore automatically play a key role in posterior inference. [sent-125, score-0.572]
</p><p>69 Since these inputs are also inﬂuenced by the distractor, there is an RT 2  In fact, also including the possibility of a degraded high-load case  A  RT  B  30  error rate  steps  Incompatible Neutral Compatible  0. [sent-126, score-0.127]
</p><p>70 1 0  low load  high load  degraded low load  Figure 3: Results. [sent-131, score-1.637]
</p><p>71 A) Mean RTs (steps of inference) for correct choices in each of the 9 cases (since the target is equally often positive and negative, we averaged over these cases. [sent-132, score-0.115]
</p><p>72 01 per step that inference would terminate early with whichever response was more probable. [sent-135, score-0.098]
</p><p>73 However, in the high load case, the non-target stimuli are closer to the target and exert substantial inﬂuence over the noise corrupting the large RF units associated with it (and no net signal). [sent-139, score-0.85]
</p><p>74 This makes these large RF units relatively poor sources of information about the target. [sent-140, score-0.106]
</p><p>75 Thus the smaller RF units are relied upon instead, which are not affected by the distractor. [sent-141, score-0.106]
</p><p>76 The compatible distractor is helpful to a lesser extent than the incompatible one is harmful, for a couple of reasons. [sent-145, score-0.592]
</p><p>77 Second, compared with a neutral distractor, the compatible distractor increases the (signal-dependent) noise associated with the units with large RFs, reducing their informativeness about the target. [sent-147, score-0.672]
</p><p>78 4  Discussion  In this paper, we have shown how to account for key results used to argue for an attentional load hypothesis. [sent-148, score-0.77]
</p><p>79 Our model involves simple Bayesian inference based on a generative process recognizing the existence of small and large receptive ﬁelds. [sent-149, score-0.161]
</p><p>80 That is, the model does not employ an explicit attentional mechanism in inference which has the capacity to downplay some input units over others. [sent-155, score-0.468]
</p><p>81 It would be interesting to design neurophysiological experiments to probe the form of online selection at work in the attentional load tasks. [sent-158, score-0.744]
</p><p>82 (2008) is that the model here includes RFs of different sizes, whereas in that model, the distractors were always close to the target. [sent-160, score-0.09]
</p><p>83 Further, the two neutral conditions here (no distractor, and low load) were not modeled in the earlier study. [sent-161, score-0.153]
</p><p>84 (2008) suggested that the anterior cingulate might monitor conﬂict between the cases of compatible and incompatible distractors as part of an approximate inference strategy. [sent-163, score-0.414]
</p><p>85 The assumptions of large RFs and their high standard deviations in the high load condition are certainly rather simplistic. [sent-165, score-0.622]
</p><p>86 The attentional load theory has been applied to many tasks (including the regular Eriksen task, Eriksen and Eriksen, 1974) as well as the one here. [sent-168, score-0.786]
</p><p>87 Perhaps the most signiﬁcant lacuna is that, as in the Eriksen task, we assumed that the subjects knew the location of the target in the stimulus array, whereas in the real experiment, this had to be inferred from the letters in the circle of targets close to ﬁxation (ﬁgure 1A). [sent-170, score-0.203]
</p><p>88 It would also be worth extending the current model to the much wider range of other tasks used to explore the effects of attentional load (such as Forster and Lavie, 2008). [sent-173, score-0.811]
</p><p>89 In conclusion, we have suggested a particular rationale for an attenuation theory of attention, which puts together the three tasks suggested at the outset for dichotic listening. [sent-174, score-0.25]
</p><p>90 The key resource limitation is the restricted number, and therefore, the necessarily broad tuning of RFs; the normative response to his makes attenuation and combination kissing cousins. [sent-176, score-0.123]
</p><p>91 A selective review of selective attention research from the past century. [sent-206, score-0.294]
</p><p>92 The locus of interference in the perception of simultaneous stimuli. [sent-210, score-0.064]
</p><p>93 Effects of noise-letters on identiﬁcation of a target letter in a nonsearch task. [sent-215, score-0.155]
</p><p>94 Visual attention within and around the ﬁeld of focal attention: a zoom lens model. [sent-223, score-0.096]
</p><p>95 Failures to ignore entirely irrelevant distractors: the role of load. [sent-228, score-0.054]
</p><p>96 Contrasting effects of sensory limits and capacity limits in visual selective attention. [sent-248, score-0.262]
</p><p>97 Perceptual load as a major determinant of the locus of selection in visual attention. [sent-253, score-0.512]
</p><p>98 Selective attention gates visual processing in the extrastriate cortex. [sent-262, score-0.133]
</p><p>99 Set-size effects in visual search: the effect of attention is independent of the stimulus for simple tasks. [sent-290, score-0.158]
</p><p>100 Theoretical understanding of the early visual processes by data compression and data selection. [sent-354, score-0.062]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('load', 0.475), ('distractor', 0.337), ('rfs', 0.314), ('lavie', 0.27), ('attentional', 0.269), ('eriksen', 0.225), ('fockert', 0.15), ('incompatible', 0.138), ('compatible', 0.117), ('target', 0.115), ('neutral', 0.112), ('units', 0.106), ('degraded', 0.101), ('rts', 0.101), ('selective', 0.099), ('attention', 0.096), ('distractors', 0.09), ('yu', 0.086), ('attenuation', 0.084), ('receptive', 0.084), ('treisman', 0.075), ('rf', 0.074), ('dayan', 0.07), ('rev', 0.067), ('itti', 0.066), ('stimuli', 0.06), ('baldwin', 0.06), ('desimone', 0.06), ('dichotic', 0.06), ('duncan', 0.06), ('psychophys', 0.06), ('capacity', 0.056), ('irrelevant', 0.054), ('psychol', 0.053), ('deviations', 0.052), ('elds', 0.051), ('percept', 0.048), ('deutsch', 0.048), ('sensory', 0.045), ('streams', 0.045), ('bobrow', 0.045), ('broadbent', 0.045), ('navalpakkam', 0.045), ('norman', 0.045), ('whiteley', 0.045), ('zhaoping', 0.045), ('palmer', 0.042), ('tasks', 0.042), ('low', 0.041), ('mozer', 0.04), ('letter', 0.04), ('generative', 0.04), ('sign', 0.04), ('exert', 0.039), ('coming', 0.039), ('normative', 0.039), ('visual', 0.037), ('condition', 0.037), ('inference', 0.037), ('gure', 0.036), ('whichever', 0.036), ('driver', 0.036), ('reaction', 0.035), ('zemel', 0.035), ('eg', 0.034), ('interference', 0.034), ('subjects', 0.033), ('stream', 0.033), ('james', 0.033), ('xation', 0.032), ('suggested', 0.032), ('hidden', 0.03), ('chelazzi', 0.03), ('confusing', 0.03), ('forster', 0.03), ('inco', 0.03), ('lacuna', 0.03), ('moran', 0.03), ('neut', 0.03), ('paschler', 0.03), ('shaw', 0.03), ('tsal', 0.03), ('usefully', 0.03), ('perception', 0.03), ('psychology', 0.03), ('high', 0.029), ('critical', 0.029), ('late', 0.028), ('perceptual', 0.028), ('ict', 0.027), ('listening', 0.026), ('leak', 0.026), ('annu', 0.026), ('attended', 0.026), ('corrupting', 0.026), ('argue', 0.026), ('inputs', 0.026), ('circle', 0.025), ('effects', 0.025), ('early', 0.025)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="124-tfidf-1" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>Author: Peter Dayan</p><p>Abstract: Selective attention is a most intensively studied psychological phenomenon, rife with theoretical suggestions and schisms. A critical idea is that of limited capacity, the allocation of which has produced continual conﬂict about such phenomena as early and late selection. An inﬂuential resolution of this debate is based on the notion of perceptual load (Lavie, 2005), which suggests that low-load, easy tasks, because they underuse the total capacity of attention, mandatorily lead to the processing of stimuli that are irrelevant to the current attentional set; whereas high-load, difﬁcult tasks grab all resources for themselves, leaving distractors high and dry. We argue that this theory presents a challenge to Bayesian theories of attention, and suggest an alternative, statistical, account of key supporting data. 1</p><p>2 0.10836364 <a title="124-tfidf-2" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>Author: Christopher Williams, Stefan Klanke, Sethu Vijayakumar, Kian M. Chai</p><p>Abstract: The inverse dynamics problem for a robotic manipulator is to compute the torques needed at the joints to drive it along a given trajectory; it is beneﬁcial to be able to learn this function for adaptive control. A robotic manipulator will often need to be controlled while holding different loads in its end effector, giving rise to a multi-task learning problem. By placing independent Gaussian process priors over the latent functions of the inverse dynamics, we obtain a multi-task Gaussian process prior for handling multiple loads, where the inter-task similarity depends on the underlying inertial parameters. Experiments demonstrate that this multi-task formulation is effective in sharing information among the various loads, and generally improves performance over either learning only on single tasks or pooling the data over all tasks. 1</p><p>3 0.079382278 <a title="124-tfidf-3" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>Author: Xiaodi Hou, Liqing Zhang</p><p>Abstract: A visual attention system should respond placidly when common stimuli are presented, while at the same time keep alert to anomalous visual inputs. In this paper, a dynamic visual attention model based on the rarity of features is proposed. We introduce the Incremental Coding Length (ICL) to measure the perspective entropy gain of each feature. The objective of our model is to maximize the entropy of the sampled visual features. In order to optimize energy consumption, the limit amount of energy of the system is re-distributed amongst features according to their Incremental Coding Length. By selecting features with large coding length increments, the computational system can achieve attention selectivity in both static and dynamic scenes. We demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation. Moreover, we also show that our model captures several less-reported dynamic visual search behaviors, such as attentional swing and inhibition of return. 1</p><p>4 0.070937604 <a title="124-tfidf-4" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>Author: Jeremy Reynolds, Michael C. Mozer</p><p>Abstract: Cognitive control refers to the ﬂexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efﬁciently reﬁned with subsequent task experience. 1</p><p>5 0.066506945 <a title="124-tfidf-5" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>Author: Matt Jones, Sachiko Kinoshita, Michael C. Mozer</p><p>Abstract: In most cognitive and motor tasks, speed-accuracy tradeoffs are observed: Individuals can respond slowly and accurately, or quickly yet be prone to errors. Control mechanisms governing the initiation of behavioral responses are sensitive not only to task instructions and the stimulus being processed, but also to the recent stimulus history. When stimuli can be characterized on an easy-hard dimension (e.g., word frequency in a naming task), items preceded by easy trials are responded to more quickly, and with more errors, than items preceded by hard trials. We propose a rationally motivated mathematical model of this sequential adaptation of control, based on a diffusion model of the decision process in which difﬁculty corresponds to the drift rate for the correct response. The model assumes that responding is based on the posterior distribution over which response is correct, conditioned on the accumulated evidence. We derive this posterior as a function of the drift rate, and show that higher estimates of the drift rate lead to (normatively) faster responding. Trial-by-trial tracking of difﬁculty thus leads to sequential effects in speed and accuracy. Simulations show the model explains a variety of phenomena in human speeded decision making. We argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theories based on elaborate control structures. 1</p><p>6 0.058655072 <a title="124-tfidf-6" href="./nips-2008-Linear_Classification_and_Selective_Sampling_Under_Low_Noise_Conditions.html">123 nips-2008-Linear Classification and Selective Sampling Under Low Noise Conditions</a></p>
<p>7 0.054473687 <a title="124-tfidf-7" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>8 0.05037554 <a title="124-tfidf-8" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>9 0.049367912 <a title="124-tfidf-9" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>10 0.047561601 <a title="124-tfidf-10" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<p>11 0.046502713 <a title="124-tfidf-11" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>12 0.045545619 <a title="124-tfidf-12" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<p>13 0.042045653 <a title="124-tfidf-13" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>14 0.041607961 <a title="124-tfidf-14" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>15 0.041362185 <a title="124-tfidf-15" href="./nips-2008-Psychiatry%3A_Insights_into_depression_through_normative_decision-making_models.html">187 nips-2008-Psychiatry: Insights into depression through normative decision-making models</a></p>
<p>16 0.041175496 <a title="124-tfidf-16" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>17 0.040934436 <a title="124-tfidf-17" href="./nips-2008-On_Computational_Power_and_the_Order-Chaos_Phase_Transition_in_Reservoir_Computing.html">160 nips-2008-On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing</a></p>
<p>18 0.03920719 <a title="124-tfidf-18" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>19 0.038487747 <a title="124-tfidf-19" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>20 0.037487783 <a title="124-tfidf-20" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.112), (1, 0.028), (2, 0.072), (3, -0.005), (4, 0.003), (5, 0.026), (6, -0.042), (7, 0.04), (8, 0.079), (9, 0.055), (10, -0.004), (11, 0.102), (12, -0.076), (13, 0.029), (14, -0.012), (15, 0.055), (16, -0.012), (17, -0.021), (18, -0.012), (19, -0.006), (20, -0.02), (21, -0.041), (22, 0.014), (23, 0.008), (24, -0.018), (25, 0.014), (26, 0.028), (27, 0.04), (28, -0.026), (29, 0.053), (30, -0.038), (31, 0.04), (32, -0.013), (33, 0.063), (34, 0.024), (35, -0.035), (36, -0.059), (37, 0.046), (38, 0.064), (39, 0.035), (40, 0.036), (41, -0.002), (42, 0.001), (43, 0.023), (44, 0.007), (45, -0.084), (46, -0.036), (47, -0.065), (48, 0.171), (49, -0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.92692107 <a title="124-lsi-1" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>Author: Peter Dayan</p><p>Abstract: Selective attention is a most intensively studied psychological phenomenon, rife with theoretical suggestions and schisms. A critical idea is that of limited capacity, the allocation of which has produced continual conﬂict about such phenomena as early and late selection. An inﬂuential resolution of this debate is based on the notion of perceptual load (Lavie, 2005), which suggests that low-load, easy tasks, because they underuse the total capacity of attention, mandatorily lead to the processing of stimuli that are irrelevant to the current attentional set; whereas high-load, difﬁcult tasks grab all resources for themselves, leaving distractors high and dry. We argue that this theory presents a challenge to Bayesian theories of attention, and suggest an alternative, statistical, account of key supporting data. 1</p><p>2 0.62562305 <a title="124-lsi-2" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>Author: Matt Jones, Sachiko Kinoshita, Michael C. Mozer</p><p>Abstract: In most cognitive and motor tasks, speed-accuracy tradeoffs are observed: Individuals can respond slowly and accurately, or quickly yet be prone to errors. Control mechanisms governing the initiation of behavioral responses are sensitive not only to task instructions and the stimulus being processed, but also to the recent stimulus history. When stimuli can be characterized on an easy-hard dimension (e.g., word frequency in a naming task), items preceded by easy trials are responded to more quickly, and with more errors, than items preceded by hard trials. We propose a rationally motivated mathematical model of this sequential adaptation of control, based on a diffusion model of the decision process in which difﬁculty corresponds to the drift rate for the correct response. The model assumes that responding is based on the posterior distribution over which response is correct, conditioned on the accumulated evidence. We derive this posterior as a function of the drift rate, and show that higher estimates of the drift rate lead to (normatively) faster responding. Trial-by-trial tracking of difﬁculty thus leads to sequential effects in speed and accuracy. Simulations show the model explains a variety of phenomena in human speeded decision making. We argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theories based on elaborate control structures. 1</p><p>3 0.57620382 <a title="124-lsi-3" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>Author: Jeremy Reynolds, Michael C. Mozer</p><p>Abstract: Cognitive control refers to the ﬂexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efﬁciently reﬁned with subsequent task experience. 1</p><p>4 0.53552997 <a title="124-lsi-4" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>Author: Rama Natarajan, Iain Murray, Ladan Shams, Richard S. Zemel</p><p>Abstract: We explore a recently proposed mixture model approach to understanding interactions between conﬂicting sensory cues. Alternative model formulations, differing in their sensory noise models and inference methods, are compared based on their ﬁt to experimental data. Heavy-tailed sensory likelihoods yield a better description of the subjects’ response behavior than standard Gaussian noise models. We study the underlying cause for this result, and then present several testable predictions of these models. 1</p><p>5 0.51803297 <a title="124-lsi-5" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>Author: Xiaodi Hou, Liqing Zhang</p><p>Abstract: A visual attention system should respond placidly when common stimuli are presented, while at the same time keep alert to anomalous visual inputs. In this paper, a dynamic visual attention model based on the rarity of features is proposed. We introduce the Incremental Coding Length (ICL) to measure the perspective entropy gain of each feature. The objective of our model is to maximize the entropy of the sampled visual features. In order to optimize energy consumption, the limit amount of energy of the system is re-distributed amongst features according to their Incremental Coding Length. By selecting features with large coding length increments, the computational system can achieve attention selectivity in both static and dynamic scenes. We demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation. Moreover, we also show that our model captures several less-reported dynamic visual search behaviors, such as attentional swing and inhibition of return. 1</p><p>6 0.50552338 <a title="124-lsi-6" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>7 0.47223163 <a title="124-lsi-7" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>8 0.42061707 <a title="124-lsi-8" href="./nips-2008-Learning_to_Use_Working_Memory_in_Partially_Observable_Environments_through_Dopaminergic_Reinforcement.html">121 nips-2008-Learning to Use Working Memory in Partially Observable Environments through Dopaminergic Reinforcement</a></p>
<p>9 0.42057282 <a title="124-lsi-9" href="./nips-2008-A_Massively_Parallel_Digital_Learning_Processor.html">3 nips-2008-A Massively Parallel Digital Learning Processor</a></p>
<p>10 0.41645694 <a title="124-lsi-10" href="./nips-2008-Psychiatry%3A_Insights_into_depression_through_normative_decision-making_models.html">187 nips-2008-Psychiatry: Insights into depression through normative decision-making models</a></p>
<p>11 0.4061029 <a title="124-lsi-11" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>12 0.39925465 <a title="124-lsi-12" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>13 0.39686009 <a title="124-lsi-13" href="./nips-2008-Stress%2C_noradrenaline%2C_and_realistic_prediction_of_mouse_behaviour_using_reinforcement_learning.html">222 nips-2008-Stress, noradrenaline, and realistic prediction of mouse behaviour using reinforcement learning</a></p>
<p>14 0.38632953 <a title="124-lsi-14" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>15 0.38376394 <a title="124-lsi-15" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<p>16 0.37907517 <a title="124-lsi-16" href="./nips-2008-An_ideal_observer_model_of_infant_object_perception.html">23 nips-2008-An ideal observer model of infant object perception</a></p>
<p>17 0.37437236 <a title="124-lsi-17" href="./nips-2008-A_computational_model_of_hippocampal_function_in_trace_conditioning.html">7 nips-2008-A computational model of hippocampal function in trace conditioning</a></p>
<p>18 0.37396628 <a title="124-lsi-18" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>19 0.36771828 <a title="124-lsi-19" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<p>20 0.35386628 <a title="124-lsi-20" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.047), (7, 0.062), (12, 0.023), (15, 0.022), (28, 0.126), (57, 0.051), (59, 0.024), (63, 0.019), (71, 0.015), (74, 0.013), (77, 0.038), (78, 0.453), (83, 0.03)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.82921308 <a title="124-lda-1" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>Author: Yik-cheung Tam, Tanja Schultz</p><p>Abstract: We present a correlated bigram LSA approach for unsupervised LM adaptation for automatic speech recognition. The model is trained using efﬁcient variational EM and smoothed using the proposed fractional Kneser-Ney smoothing which handles fractional counts. We address the scalability issue to large training corpora via bootstrapping of bigram LSA from unigram LSA. For LM adaptation, unigram and bigram LSA are integrated into the background N-gram LM via marginal adaptation and linear interpolation respectively. Experimental results on the Mandarin RT04 test set show that applying unigram and bigram LSA together yields 6%–8% relative perplexity reduction and 2.5% relative character error rate reduction which is statistically signiﬁcant compared to applying only unigram LSA. On the large-scale evaluation on Arabic, 3% relative word error rate reduction is achieved which is also statistically signiﬁcant. 1</p><p>2 0.80503845 <a title="124-lda-2" href="./nips-2008-Kernel_Change-point_Analysis.html">111 nips-2008-Kernel Change-point Analysis</a></p>
<p>Author: Zaïd Harchaoui, Eric Moulines, Francis R. Bach</p><p>Abstract: We introduce a kernel-based method for change-point analysis within a sequence of temporal observations. Change-point analysis of an unlabelled sample of observations consists in, ﬁrst, testing whether a change in the distribution occurs within the sample, and second, if a change occurs, estimating the change-point instant after which the distribution of the observations switches from one distribution to another different distribution. We propose a test statistic based upon the maximum kernel Fisher discriminant ratio as a measure of homogeneity between segments. We derive its limiting distribution under the null hypothesis (no change occurs), and establish the consistency under the alternative hypothesis (a change occurs). This allows to build a statistical hypothesis testing procedure for testing the presence of a change-point, with a prescribed false-alarm probability and detection probability tending to one in the large-sample setting. If a change actually occurs, the test statistic also yields an estimator of the change-point location. Promising experimental results in temporal segmentation of mental tasks from BCI data and pop song indexation are presented. 1</p><p>same-paper 3 0.7960605 <a title="124-lda-3" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>Author: Peter Dayan</p><p>Abstract: Selective attention is a most intensively studied psychological phenomenon, rife with theoretical suggestions and schisms. A critical idea is that of limited capacity, the allocation of which has produced continual conﬂict about such phenomena as early and late selection. An inﬂuential resolution of this debate is based on the notion of perceptual load (Lavie, 2005), which suggests that low-load, easy tasks, because they underuse the total capacity of attention, mandatorily lead to the processing of stimuli that are irrelevant to the current attentional set; whereas high-load, difﬁcult tasks grab all resources for themselves, leaving distractors high and dry. We argue that this theory presents a challenge to Bayesian theories of attention, and suggest an alternative, statistical, account of key supporting data. 1</p><p>4 0.72695476 <a title="124-lda-4" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>Author: Liang Zhang, Deepak Agarwal</p><p>Abstract: Multi-level hierarchical models provide an attractive framework for incorporating correlations induced in a response variable that is organized hierarchically. Model ﬁtting is challenging, especially for a hierarchy with a large number of nodes. We provide a novel algorithm based on a multi-scale Kalman ﬁlter that is both scalable and easy to implement. For Gaussian response, we show our method provides the maximum a-posteriori (MAP) parameter estimates; for non-Gaussian response, parameter estimation is performed through a Laplace approximation. However, the Laplace approximation provides biased parameter estimates that is corrected through a parametric bootstrap procedure. We illustrate through simulation studies and analyses of real world data sets in health care and online advertising.</p><p>5 0.43228757 <a title="124-lda-5" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>Author: Cédric Archambeau, Francis R. Bach</p><p>Abstract: We present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a preprocessing tool for the construction of template attacks. 1</p><p>6 0.40912458 <a title="124-lda-6" href="./nips-2008-Multiscale_Random_Fields_with_Application_to_Contour_Grouping.html">147 nips-2008-Multiscale Random Fields with Application to Contour Grouping</a></p>
<p>7 0.40729398 <a title="124-lda-7" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>8 0.40603805 <a title="124-lda-8" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>9 0.40274209 <a title="124-lda-9" href="./nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</a></p>
<p>10 0.40068221 <a title="124-lda-10" href="./nips-2008-Online_Models_for_Content_Optimization.html">169 nips-2008-Online Models for Content Optimization</a></p>
<p>11 0.38766116 <a title="124-lda-11" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>12 0.38061881 <a title="124-lda-12" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>13 0.3772707 <a title="124-lda-13" href="./nips-2008-A_rational_model_of_preference_learning_and_choice_prediction_by_children.html">10 nips-2008-A rational model of preference learning and choice prediction by children</a></p>
<p>14 0.37680796 <a title="124-lda-14" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>15 0.37674198 <a title="124-lda-15" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>16 0.37659085 <a title="124-lda-16" href="./nips-2008-Using_matrices_to_model_symbolic_relationship.html">248 nips-2008-Using matrices to model symbolic relationship</a></p>
<p>17 0.37527841 <a title="124-lda-17" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>18 0.37280244 <a title="124-lda-18" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>19 0.37110949 <a title="124-lda-19" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>20 0.36985502 <a title="124-lda-20" href="./nips-2008-Logistic_Normal_Priors_for_Unsupervised_Probabilistic_Grammar_Induction.html">127 nips-2008-Logistic Normal Priors for Unsupervised Probabilistic Grammar Induction</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
