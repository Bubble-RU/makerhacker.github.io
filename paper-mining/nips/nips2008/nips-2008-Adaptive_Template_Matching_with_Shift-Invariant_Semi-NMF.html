<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-16" href="#">nips2008-16</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</h1>
<br/><p>Source: <a title="nips-2008-16-pdf" href="http://papers.nips.cc/paper/3408-adaptive-template-matching-with-shift-invariant-semi-nmf.pdf">pdf</a></p><p>Author: Jonathan L. Roux, Alain D. Cheveigné, Lucas C. Parra</p><p>Abstract: How does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes? One could think of using template matching or matching pursuit to ﬁnd the arbitrarily shifted linear components. However, traditional matching approaches require that the templates be known a priori. To overcome this restriction we use instead semi Non-Negative Matrix Factorization (semiNMF) that we extend to allow for time shifts when matching the templates to the signal. The algorithm estimates templates directly from the data along with their non-negative amplitudes. The resulting method can be thought of as an adaptive template matching procedure. We demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings. On these data the algorithm essentially performs spike detection and unsupervised spike clustering. Results on simulated data and extracellular recordings indicate that the method performs well for signalto-noise ratios of 6dB or higher and that spike templates are recovered accurately provided they are suﬃciently diﬀerent. 1</p><p>Reference: <a title="nips-2008-16-reference" href="../nips2008_reference/nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract How does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes? [sent-11, score-0.215]
</p><p>2 One could think of using template matching or matching pursuit to ﬁnd the arbitrarily shifted linear components. [sent-12, score-0.345]
</p><p>3 However, traditional matching approaches require that the templates be known a priori. [sent-13, score-0.631]
</p><p>4 To overcome this restriction we use instead semi Non-Negative Matrix Factorization (semiNMF) that we extend to allow for time shifts when matching the templates to the signal. [sent-14, score-0.692]
</p><p>5 The algorithm estimates templates directly from the data along with their non-negative amplitudes. [sent-15, score-0.576]
</p><p>6 The resulting method can be thought of as an adaptive template matching procedure. [sent-16, score-0.217]
</p><p>7 We demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings. [sent-17, score-0.249]
</p><p>8 On these data the algorithm essentially performs spike detection and unsupervised spike clustering. [sent-18, score-0.547]
</p><p>9 Results on simulated data and extracellular recordings indicate that the method performs well for signalto-noise ratios of 6dB or higher and that spike templates are recovered accurately provided they are suﬃciently diﬀerent. [sent-19, score-1.058]
</p><p>10 1  Introduction  It is often the case that an observed waveform is the superposition of elementary waveforms, taken from a limited set and added with variable latencies and variable but positive amplitudes. [sent-20, score-0.249]
</p><p>11 Examples are a music waveform, made up of the superposition of stereotyped instrumental notes, or extracellular recordings of nerve activity, made up of the superposition of spikes from multiple neurons. [sent-21, score-0.56]
</p><p>12 Additionally, the elementary events are often temporally compact and their occurrence temporally sparse. [sent-23, score-0.229]
</p><p>13 Conventional template matching uses a known template and correlates it with the signal; events are assumed to occur at times where the correlation is high. [sent-24, score-0.537]
</p><p>14 Multiple template matching raises combinatorial issues that are addressed by Matching Pursuit [1]. [sent-25, score-0.217]
</p><p>15 We ∗  Corresponding author  wondered whether one can estimate the templates directly from the data, together with their timing and amplitude. [sent-27, score-0.606]
</p><p>16 Over the last decade a number of blind decomposition methods have been developed that address a similar problem: given data, can one ﬁnd the amplitudes and proﬁles of constituent signals that explain the data in some optimal way. [sent-28, score-0.389]
</p><p>17 This includes independent component analysis (ICA), non-negative matrix factorization (NMF), and a variety of other blind source separation algorithms. [sent-29, score-0.103]
</p><p>18 The diﬀerent algorithms all assume a linear superposition of the templates, but vary in their speciﬁc assumptions about the statistics of the templates and the mixing process. [sent-30, score-0.657]
</p><p>19 NMF constrains weights to be nonnegative but requires templates to also be non-negative. [sent-33, score-0.576]
</p><p>20 We begin with the conventional formulation of the NMF modeling task as a matrix factorization problem and then derive in the subsequent section the case of a 1D sequence of data. [sent-36, score-0.103]
</p><p>21 Matrix A can be thought of as component amplitudes and the rows of matrix B are the component templates. [sent-38, score-0.431]
</p><p>22 Semi-NMF drops the non-negative constraint for B, while shift-NMF allows the component templates to be shifted in time. [sent-39, score-0.656]
</p><p>23 ) The goal is to model this data as a linear superposition of K component templates Bkt with amplitudes Ank , i. [sent-44, score-1.046]
</p><p>24 (5)  In these expressions, k is a summation index; (M )−1 stands for matrix inverse of M ; and, 1 (M )+ = 2 (|M | + M ) and (M )− = 1 (|M | − M ) are to be applied on each element of matrix 2 M . [sent-51, score-0.108]
</p><p>25 In the course of time, various events of unknown identity and variable amplitude appear in this signal. [sent-55, score-0.216]
</p><p>26 We describe an event of type k with a template Bkl of length L. [sent-56, score-0.233]
</p><p>27 An event can occur at any point in time, say at time sample n, and it may have a variable amplitude. [sent-58, score-0.121]
</p><p>28 In addition, we do not know a priori what the event type is and so we assign to each time sample n and each event type k an amplitude Ank ≥ 0. [sent-59, score-0.25]
</p><p>29 The goal is to ﬁnd the templates B and amplitudes A that explain the data. [sent-60, score-0.965]
</p><p>30 In this formulation of the model, the timing of an event is given by a non-zero sample in the amplitude matrix A. [sent-61, score-0.225]
</p><p>31 This means that for a given n the estimated amplitudes are positive for only one k, and neighboring samples in time have zero amplitudes. [sent-63, score-0.439]
</p><p>32 Each block implements a convolution of the k-th template Bkl with amplitudes signal Ank . [sent-68, score-0.574]
</p><p>33 We will also require a unit-norm constraint on the K templates in B, namely, l Bk Bkl = 1, to disambiguate the arbitrary scale in the product of A and B. [sent-70, score-0.615]
</p><p>34 2  Optimization criterion with sparseness prior  Under the assumption that the data represent a small set of well-localized events, matrix A should consist of a sparse series of pulses, the other samples having zero amplitude. [sent-72, score-0.111]
</p><p>35 Assuming Gaussian white noise, the new cost function given by the negative log-posterior reads (up to a scaling factor), E  = =  1 ˆ 2 ||X − X||2 + β ||A||α α 2 2 1 ˜kl Xt − At Bkl + β 2 t  (10) Aα , kl  (11)  kl  where ||·||p denotes the Lp norm (or quasi-norm for 0 < p < 1). [sent-74, score-0.102]
</p><p>36 3  A update  The update for A which minimizes this cost function is similar to update (5) with some modiﬁcations. [sent-79, score-0.105]
</p><p>37 In (5), amplitudes A can be treated as a matrix of dimensions T × K and each update can be applied separately for every n. [sent-80, score-0.466]
</p><p>38 B is now a T K × T matrix of shifted ˜ templates deﬁned as Bnkt = Bk,t−n . [sent-82, score-0.659]
</p><p>39 (12)  + αβAα−1 nk  The summation in the BB T term is over t, and is 0 most of the time when the events do not ˜l ˜l overlap. [sent-84, score-0.184]
</p><p>40 4  B update  The templates B that minimize the square modeling error, i. [sent-91, score-0.611]
</p><p>41 (13)  The matrix inverse is now over a matrix of LK by LK elements. [sent-94, score-0.084]
</p><p>42 The unit-norm constraint for the templates B therefore prevents A from shrinking arbitrarily. [sent-97, score-0.615]
</p><p>43 5  Normalization  The normalization constraint of the templates B can be implemented using Lagrange multipliers, leading to the constrained least squares solution: ˜kl ˜ Bkl = At Atk l + Λkl,k l  −1  ˜t Ak l X t . [sent-99, score-0.615]
</p><p>44 (14)  Here, Λkl,k l represents a diagonal matrix of size KL × KL with K diﬀerent Lagrange l multipliers as parameters that need to be adjusted so that Bk Bkl = 1 for all k. [sent-100, score-0.084]
</p><p>45 The algorithm is then applied to extracellular recordings of neuronal spiking activity and we evaluate its ability to recover two distinct spike types that are typically superimposed in this data. [sent-106, score-0.481]
</p><p>46 In addition we report how accurately the templates have been recovered. [sent-115, score-0.576]
</p><p>47 We generated synthetic spike trains with two types of “spikes” and added Gaussian white noise. [sent-116, score-0.315]
</p><p>48 The two sets of panels show the templates B (original on the left and recovered on the right), amplitudes A (same as above) and noisy data ˆ X (left) and estimated X (right). [sent-118, score-1.04]
</p><p>49 Clearly, for this SNR the templates have been recovered accurately and their occurrences within the waveform have been found with only a few missing events. [sent-120, score-0.732]
</p><p>50 Detection rate is measured as the number of events recovered over the total number of events in the original data. [sent-122, score-0.319]
</p><p>51 Presence or absence of a recovered event is determined by comparing the original pulse train with the reconstructed pulse train A (channel number k is ignored). [sent-124, score-0.268]
</p><p>52 Templates in this example have a correlation time (3 dB down) of 2-4 samples and so we tolerate a misalignment of events of up to ±2 samples. [sent-125, score-0.16]
</p><p>53 We simulated 30 events with amplitudes uniformly distributed in [0, 1]. [sent-126, score-0.523]
</p><p>54 The algorithm tends to miss smaller events with amplitudes comparable to the noise amplitude. [sent-127, score-0.546]
</p><p>55 To capture this eﬀect, we also report a detection rate that is weighted by event amplitude. [sent-128, score-0.13]
</p><p>56 Some events may be detected but assigned to the wrong template. [sent-129, score-0.134]
</p><p>57 Finally, we report the goodness of ﬁt as R2 for the templates B and the continuous valued amplitudes A for the events that are present in the original data. [sent-131, score-1.099]
</p><p>58 Obviously, the performance of this type of unsupervised clustering will degrade as the templates become more and more similar. [sent-133, score-0.576]
</p><p>59 Figure 2 shows the same performance numbers as a function of the similarity of the templates (without additive noise). [sent-134, score-0.625]
</p><p>60 A similarity of 0 corresponds to the templates shown as examples in Figure 1 (these are almost orthogonal with a cosine of 74◦ ), and similarity 1 means identical templates. [sent-135, score-0.696]
</p><p>61 Evidently the algorithm is most reliable when the target templates are dissimilar. [sent-136, score-0.576]
</p><p>62 2  Analysis of extracellular recordings  The original motivation for this algorithm was to analyze extracellular recordings from single electrodes in the guinea pig cochlear nucleus. [sent-138, score-0.439]
</p><p>63 Spherical and globular bushy cells in the anteroventral cochlear nucleus (AVCN) are assumed to function as reliable relays of spike trains from the auditory nerve, with “primary-like” responses that resemble those of auditory nerve ﬁbers. [sent-139, score-0.541]
</p><p>64 Every incoming spike evokes a discharge within the outgoing axon [6]. [sent-140, score-0.274]
</p><p>65 Error bars represent standard deviation over 100 repetitions with varying random amplitudes and random noise. [sent-172, score-0.389]
</p><p>66 Bottom left: false alarm rate (detected events which do not correspond to an event in the original data). [sent-176, score-0.263]
</p><p>67 However, recent observations give a more nuanced picture, suggesting that the post-synaptic spike may sometimes be suppressed according to a process that is not well understood [7]. [sent-180, score-0.244]
</p><p>68 Their relative amplitudes depend upon the position of the electrode tip relative to the cell. [sent-183, score-0.413]
</p><p>69 Our aim is to isolate each of these components to understand the process by which the SD spike is sometimes suppressed. [sent-184, score-0.276]
</p><p>70 The events may overlap in time (in particular the SD spike always overlaps with an IS spike), with varying positive amplitudes. [sent-185, score-0.404]
</p><p>71 The assumptions of our algorithm are met by these data, as well as by multi-unit recordings reﬂecting the activity of several neurons (the “spike sorting problem”). [sent-187, score-0.09]
</p><p>72 The automatically recovered spike templates seem to capture a number of the key features. [sent-193, score-0.871]
</p><p>73 Template 1, in blue, resembles the SD spike, and template 2, in red, is similar to the IS spike. [sent-194, score-0.162]
</p><p>74 The SD spikes are larger and have sharper peaks as compared to the IS spikes, while the IS spikes have an initial peak at 0. [sent-195, score-0.26]
</p><p>75 The larger size of the extracted spikes corresponding to template 1 is correctly reﬂected in the histogram of the recovered amplitudes. [sent-197, score-0.343]
</p><p>76 The main diﬀerence is in the small peak preceding the template 1. [sent-199, score-0.162]
</p><p>77 This is perhaps to be expected as the SD spike is always preceded in the raw data by a smaller IS spike. [sent-200, score-0.244]
</p><p>78 The expected templates were very similar (with a cosine of 38◦ as estimated from the manually extracted spikes), making the task particularly diﬃcult. [sent-201, score-0.646]
</p><p>79 Reconstructed waveform and residual 2  Manually constructed templates B  Reconstructed Residual  1 SD 2 IS mV  1 0 −1 0  0. [sent-202, score-0.704]
</p><p>80 875 Time (ms) Distribution of the amplitudes A  2. [sent-210, score-0.389]
</p><p>81 Top left: reconstructed waveform (blue) and residual between the original data and the reconstructed waveform (red). [sent-220, score-0.369]
</p><p>82 Top right: templates B estimated manually from the data. [sent-221, score-0.624]
</p><p>83 The SD spikes (blue) generally occur with larger amplitudes than the IS spikes (red). [sent-224, score-0.673]
</p><p>84 In addition to these multiple restarts, we use a few heuristics that are motivated by the desired result of spike detection. [sent-228, score-0.244]
</p><p>85 We can thus prevent the algorithm from converging to some obviously suboptimal solutions: Re-centering the templates: We noticed that local minima with poor performance typically occurred when the templates B were not centered within the L lags. [sent-229, score-0.576]
</p><p>86 In those cases the main peaks could be adjusted to ﬁt the data, but the portion of the template that extends outside the window of L samples could not be adjusted. [sent-230, score-0.187]
</p><p>87 To prune these suboptimal solutions, it was suﬃcient to center the templates during the updates while shifting the amplitudes accordingly. [sent-231, score-0.965]
</p><p>88 Pruning events: We observed that spikes tended to generate non-zero amplitudes in A in clusters of 1 to 3 samples. [sent-232, score-0.519]
</p><p>89 Spike amplitude was preserved by scaling the pulse amplitudes to match the sum of amplitudes in the cluster. [sent-234, score-0.899]
</p><p>90 Re-training with a less conservative sparseness constraint: To ensure that templates B are not aﬀected by noise we initially train the algorithm with a strong penalty term (large 2 β eﬀectively assuming strong noise power σN ). [sent-235, score-0.691]
</p><p>91 Only spikes with large amplitudes remain after convergence and the templates are determined by only those strong spikes that have high SNR. [sent-236, score-1.225]
</p><p>92 After extracting templates accurately, we retrain the model amplitudes A while keeping the templates B ﬁxed assuming now a weaker noise power (smaller β). [sent-237, score-1.564]
</p><p>93 We have also derived a version of the model with observations X arranged as a matrix, as well as a version in which event timing is encoded explicitly as time delays τn  following [8]. [sent-241, score-0.127]
</p><p>94 This is particularly true for the semi-NMF algorithm since, provided a suﬃciently large K and without enforcing a sparsity constraint, the positivity constraint on A actually amounts to no constraint at all (identical templates with opposite sign can accomplish the same as allowing negative A). [sent-250, score-0.654]
</p><p>95 For instance, without sparseness constraint on the amplitudes, a trivial solution in our examples above would be a template B1l with a single positive spike somewhere and another template B2l with a single negative spike, and all the time course encoded in An1 and An2 . [sent-251, score-0.702]
</p><p>96 MISO identiﬁcation: The identiﬁability problem is compounded by the fact that the estimation of templates B in this present formulation represents a multiple-input singleoutput (MISO) system identiﬁcation problem. [sent-252, score-0.576]
</p><p>97 Jordan, “Convex and semi-nonnegative matrix factorization for clustering and low-dimension representation,” Lawrence Berkeley National Laboratory, Tech. [sent-268, score-0.103]
</p><p>98 Ding, “The relationships among various nonnegative matrix factorization methods for clustering,” in Proc. [sent-273, score-0.103]
</p><p>99 Yin, “Enhancement of neural synchronization in the anteroventral cochlear nucleus. [sent-300, score-0.121]
</p><p>100 Winter, “Spike waveforms in the anteroventral cochlear nucleus revisited,” in ARO midwinter meeting, no. [sent-311, score-0.191]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('templates', 0.576), ('amplitudes', 0.389), ('spike', 0.244), ('bkl', 0.241), ('ank', 0.185), ('bkt', 0.167), ('template', 0.162), ('events', 0.134), ('spikes', 0.13), ('extracellular', 0.119), ('sd', 0.115), ('nmf', 0.111), ('waveform', 0.105), ('amplitude', 0.082), ('superposition', 0.081), ('miso', 0.074), ('rup', 0.074), ('ak', 0.072), ('event', 0.071), ('sparseness', 0.069), ('reconstructed', 0.068), ('recordings', 0.068), ('cochlear', 0.065), ('di', 0.061), ('factorization', 0.061), ('nerve', 0.059), ('detection', 0.059), ('bk', 0.058), ('erent', 0.058), ('snr', 0.056), ('anteroventral', 0.056), ('bnkt', 0.056), ('xnt', 0.056), ('matching', 0.055), ('kl', 0.051), ('recovered', 0.051), ('similarity', 0.049), ('su', 0.046), ('bb', 0.045), ('waveforms', 0.045), ('synthetic', 0.043), ('matrix', 0.042), ('ding', 0.042), ('multipliers', 0.042), ('shifted', 0.041), ('pulse', 0.039), ('constraint', 0.039), ('atk', 0.037), ('atkl', 0.037), ('avcn', 0.037), ('kameoka', 0.037), ('misclassification', 0.037), ('morten', 0.037), ('parra', 0.037), ('bottom', 0.037), ('alarm', 0.036), ('lag', 0.036), ('xn', 0.036), ('update', 0.035), ('shifts', 0.035), ('lagrange', 0.035), ('elementary', 0.033), ('isolate', 0.032), ('auditory', 0.032), ('db', 0.032), ('pursuit', 0.032), ('temporally', 0.031), ('timing', 0.03), ('axon', 0.03), ('hansen', 0.03), ('latencies', 0.03), ('mv', 0.03), ('pulses', 0.03), ('spherically', 0.03), ('trigger', 0.03), ('xt', 0.029), ('identi', 0.028), ('superimposed', 0.028), ('trains', 0.028), ('top', 0.027), ('time', 0.026), ('nucleus', 0.025), ('interleaved', 0.025), ('lk', 0.025), ('portion', 0.025), ('ms', 0.024), ('occur', 0.024), ('estimated', 0.024), ('summation', 0.024), ('city', 0.024), ('electrode', 0.024), ('manually', 0.024), ('signal', 0.023), ('residual', 0.023), ('noise', 0.023), ('activity', 0.022), ('false', 0.022), ('bn', 0.022), ('cosine', 0.022), ('music', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000001 <a title="16-tfidf-1" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>Author: Jonathan L. Roux, Alain D. Cheveigné, Lucas C. Parra</p><p>Abstract: How does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes? One could think of using template matching or matching pursuit to ﬁnd the arbitrarily shifted linear components. However, traditional matching approaches require that the templates be known a priori. To overcome this restriction we use instead semi Non-Negative Matrix Factorization (semiNMF) that we extend to allow for time shifts when matching the templates to the signal. The algorithm estimates templates directly from the data along with their non-negative amplitudes. The resulting method can be thought of as an adaptive template matching procedure. We demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings. On these data the algorithm essentially performs spike detection and unsupervised spike clustering. Results on simulated data and extracellular recordings indicate that the method performs well for signalto-noise ratios of 6dB or higher and that spike templates are recovered accurately provided they are suﬃciently diﬀerent. 1</p><p>2 0.2396777 <a title="16-tfidf-2" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>Author: Zhi Yang, Qi Zhao, Wentai Liu</p><p>Abstract: This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer. 1</p><p>3 0.20234308 <a title="16-tfidf-3" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>Author: Jan Gasthaus, Frank Wood, Dilan Gorur, Yee W. Teh</p><p>Abstract: In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle “appearance” and “disappearance” of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of inﬁnite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known. 1</p><p>4 0.19380088 <a title="16-tfidf-4" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>Author: Silvia Chiappa, Jens Kober, Jan R. Peters</p><p>Abstract: Motor primitives or motion templates have become an important concept for both modeling human motor control as well as generating robot behaviors using imitation learning. Recent impressive results range from humanoid robot movement generation to timing models of human motions. The automatic generation of skill libraries containing multiple motion templates is an important step in robot learning. Such a skill learning system needs to cluster similar movements together and represent each resulting motion template as a generative model which is subsequently used for the execution of the behavior by a robot system. In this paper, we show how human trajectories captured as multi-dimensional time-series can be clustered using Bayesian mixtures of linear Gaussian state-space models based on the similarity of their dynamics. The appropriate number of templates is automatically determined by enforcing a parsimonious parametrization. As the resulting model is intractable, we introduce a novel approximation method based on variational Bayes, which is especially designed to enable the use of efﬁcient inference algorithms. On recorded human Balero movements, this method is not only capable of ﬁnding reasonable motion templates but also yields a generative model which works well in the execution of this complex task on a simulated anthropomorphic SARCOS arm.</p><p>5 0.1801351 <a title="16-tfidf-5" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>Author: Leo Zhu, Yuanhao Chen, Yuan Lin, Chenxi Lin, Alan L. Yuille</p><p>Abstract: Language and image understanding are two major goals of artiﬁcial intelligence which can both be conceptually formulated in terms of parsing the input signal into a hierarchical representation. Natural language researchers have made great progress by exploiting the 1D structure of language to design efﬁcient polynomialtime parsing algorithms. By contrast, the two-dimensional nature of images makes it much harder to design efﬁcient image parsers and the form of the hierarchical representations is also unclear. Attempts to adapt representations and algorithms from natural language have only been partially successful. In this paper, we propose a Hierarchical Image Model (HIM) for 2D image parsing which outputs image segmentation and object recognition. This HIM is represented by recursive segmentation and recognition templates in multiple layers and has advantages for representation, inference, and learning. Firstly, the HIM has a coarse-to-ﬁne representation which is capable of capturing long-range dependency and exploiting different levels of contextual information. Secondly, the structure of the HIM allows us to design a rapid inference algorithm, based on dynamic programming, which enables us to parse the image rapidly in polynomial time. Thirdly, we can learn the HIM efﬁciently in a discriminative manner from a labeled dataset. We demonstrate that HIM outperforms other state-of-the-art methods by evaluation on the challenging public MSRC image dataset. Finally, we sketch how the HIM architecture can be extended to model more complex image phenomena. 1</p><p>6 0.14438948 <a title="16-tfidf-6" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>7 0.1338885 <a title="16-tfidf-7" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>8 0.10976325 <a title="16-tfidf-8" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>9 0.071187481 <a title="16-tfidf-9" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>10 0.0697577 <a title="16-tfidf-10" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>11 0.066048332 <a title="16-tfidf-11" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>12 0.062137984 <a title="16-tfidf-12" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>13 0.057382595 <a title="16-tfidf-13" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>14 0.056786213 <a title="16-tfidf-14" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>15 0.056730054 <a title="16-tfidf-15" href="./nips-2008-Resolution_Limits_of_Sparse_Coding_in_High_Dimensions.html">198 nips-2008-Resolution Limits of Sparse Coding in High Dimensions</a></p>
<p>16 0.055576835 <a title="16-tfidf-16" href="./nips-2008-Signal-to-Noise_Ratio_Analysis_of_Policy_Gradient_Algorithms.html">210 nips-2008-Signal-to-Noise Ratio Analysis of Policy Gradient Algorithms</a></p>
<p>17 0.055267129 <a title="16-tfidf-17" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>18 0.055080615 <a title="16-tfidf-18" href="./nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</a></p>
<p>19 0.053562175 <a title="16-tfidf-19" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>20 0.053133499 <a title="16-tfidf-20" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.163), (1, 0.029), (2, 0.206), (3, 0.173), (4, -0.147), (5, 0.035), (6, -0.025), (7, -0.09), (8, -0.051), (9, -0.047), (10, -0.024), (11, -0.126), (12, 0.078), (13, 0.032), (14, 0.026), (15, 0.036), (16, -0.163), (17, -0.043), (18, -0.028), (19, 0.082), (20, 0.143), (21, 0.017), (22, 0.061), (23, -0.039), (24, -0.033), (25, 0.006), (26, -0.106), (27, -0.097), (28, -0.086), (29, -0.022), (30, 0.061), (31, -0.087), (32, -0.04), (33, -0.062), (34, -0.244), (35, -0.101), (36, 0.041), (37, 0.047), (38, -0.009), (39, -0.003), (40, -0.005), (41, 0.045), (42, 0.032), (43, -0.027), (44, -0.016), (45, -0.018), (46, 0.024), (47, 0.054), (48, 0.061), (49, 0.049)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9487462 <a title="16-lsi-1" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>Author: Jonathan L. Roux, Alain D. Cheveigné, Lucas C. Parra</p><p>Abstract: How does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes? One could think of using template matching or matching pursuit to ﬁnd the arbitrarily shifted linear components. However, traditional matching approaches require that the templates be known a priori. To overcome this restriction we use instead semi Non-Negative Matrix Factorization (semiNMF) that we extend to allow for time shifts when matching the templates to the signal. The algorithm estimates templates directly from the data along with their non-negative amplitudes. The resulting method can be thought of as an adaptive template matching procedure. We demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings. On these data the algorithm essentially performs spike detection and unsupervised spike clustering. Results on simulated data and extracellular recordings indicate that the method performs well for signalto-noise ratios of 6dB or higher and that spike templates are recovered accurately provided they are suﬃciently diﬀerent. 1</p><p>2 0.78657305 <a title="16-lsi-2" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>Author: Zhi Yang, Qi Zhao, Wentai Liu</p><p>Abstract: This paper presents a spike feature extraction algorithm that targets real-time spike sorting and facilitates miniaturized microchip implementation. The proposed algorithm has been evaluated on synthesized waveforms and experimentally recorded sequences. When compared with many spike sorting approaches our algorithm demonstrates improved speed, accuracy and allows unsupervised execution. A preliminary hardware implementation has been realized using an integrated microchip interfaced with a personal computer. 1</p><p>3 0.75957769 <a title="16-lsi-3" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>Author: Jan Gasthaus, Frank Wood, Dilan Gorur, Yee W. Teh</p><p>Abstract: In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle “appearance” and “disappearance” of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of inﬁnite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which a partial ground truth labeling is known. 1</p><p>4 0.66931856 <a title="16-lsi-4" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>Author: Peng Xu, Timothy K. Horiuchi, Pamela A. Abshire</p><p>Abstract: We report a compact realization of short-term depression (STD) in a VLSI stochastic synapse. The behavior of the circuit is based on a subtractive single release model of STD. Experimental results agree well with simulation and exhibit expected STD behavior: the transmitted spike train has negative autocorrelation and lower power spectral density at low frequencies which can remove redundancy in the input spike train, and the mean transmission probability is inversely proportional to the input spike rate which has been suggested as an automatic gain control mechanism in neural systems. The dynamic stochastic synapse could potentially be a powerful addition to existing deterministic VLSI spiking neural systems. 1</p><p>5 0.56415999 <a title="16-lsi-5" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>Author: Kentaro Katahira, Jun Nishikawa, Kazuo Okanoya, Masato Okada</p><p>Abstract: Neural activity is non-stationary and varies across time. Hidden Markov Models (HMMs) have been used to track the state transition among quasi-stationary discrete neural states. Within this context, independent Poisson models have been used for the output distribution of HMMs; hence, the model is incapable of tracking the change in correlation without modulating the ﬁring rate. To achieve this, we applied a multivariate Poisson distribution with correlation terms for the output distribution of HMMs. We formulated a Variational Bayes (VB) inference for the model. The VB could automatically determine the appropriate number of hidden states and correlation types while avoiding the overlearning problem. We developed an efﬁcient algorithm for computing posteriors using the recursive relationship of a multivariate Poisson distribution. We demonstrated the performance of our method on synthetic data and a real spike train recorded from a songbird. 1</p><p>6 0.41879082 <a title="16-lsi-6" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>7 0.37963492 <a title="16-lsi-7" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>8 0.36969739 <a title="16-lsi-8" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>9 0.34873322 <a title="16-lsi-9" href="./nips-2008-Hierarchical_Semi-Markov_Conditional_Random_Fields_for_Recursive_Sequential_Data.html">98 nips-2008-Hierarchical Semi-Markov Conditional Random Fields for Recursive Sequential Data</a></p>
<p>10 0.34771892 <a title="16-lsi-10" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>11 0.33913851 <a title="16-lsi-11" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>12 0.3358646 <a title="16-lsi-12" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>13 0.29468545 <a title="16-lsi-13" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>14 0.29249948 <a title="16-lsi-14" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>15 0.28508222 <a title="16-lsi-15" href="./nips-2008-Probabilistic_detection_of_short_events%2C_with_application_to_critical_care_monitoring.html">186 nips-2008-Probabilistic detection of short events, with application to critical care monitoring</a></p>
<p>16 0.27435952 <a title="16-lsi-16" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>17 0.26702791 <a title="16-lsi-17" href="./nips-2008-Breaking_Audio_CAPTCHAs.html">41 nips-2008-Breaking Audio CAPTCHAs</a></p>
<p>18 0.26647815 <a title="16-lsi-18" href="./nips-2008-Learning_a_discriminative_hidden_part_model_for_human_action_recognition.html">119 nips-2008-Learning a discriminative hidden part model for human action recognition</a></p>
<p>19 0.2574248 <a title="16-lsi-19" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>20 0.25502589 <a title="16-lsi-20" href="./nips-2008-Kernel_Change-point_Analysis.html">111 nips-2008-Kernel Change-point Analysis</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.063), (7, 0.065), (12, 0.057), (28, 0.125), (29, 0.275), (51, 0.015), (57, 0.058), (59, 0.02), (63, 0.042), (71, 0.06), (77, 0.035), (78, 0.015), (83, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.72847295 <a title="16-lda-1" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>Author: Jonathan L. Roux, Alain D. Cheveigné, Lucas C. Parra</p><p>Abstract: How does one extract unknown but stereotypical events that are linearly superimposed within a signal with variable latencies and variable amplitudes? One could think of using template matching or matching pursuit to ﬁnd the arbitrarily shifted linear components. However, traditional matching approaches require that the templates be known a priori. To overcome this restriction we use instead semi Non-Negative Matrix Factorization (semiNMF) that we extend to allow for time shifts when matching the templates to the signal. The algorithm estimates templates directly from the data along with their non-negative amplitudes. The resulting method can be thought of as an adaptive template matching procedure. We demonstrate the procedure on the task of extracting spikes from single channel extracellular recordings. On these data the algorithm essentially performs spike detection and unsupervised spike clustering. Results on simulated data and extracellular recordings indicate that the method performs well for signalto-noise ratios of 6dB or higher and that spike templates are recovered accurately provided they are suﬃciently diﬀerent. 1</p><p>2 0.54356414 <a title="16-lda-2" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>3 0.5420233 <a title="16-lda-3" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>4 0.54035151 <a title="16-lda-4" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>Author: Stefan Haufe, Vadim V. Nikulin, Andreas Ziehe, Klaus-Robert Müller, Guido Nolte</p><p>Abstract: We introduce a novel framework for estimating vector ﬁelds using sparse basis ﬁeld expansions (S-FLEX). The notion of basis ﬁelds, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector ﬁeld, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that signiﬁcantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art. 1</p><p>5 0.53873169 <a title="16-lda-5" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>6 0.53779227 <a title="16-lda-6" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>7 0.53584182 <a title="16-lda-7" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>8 0.53496265 <a title="16-lda-8" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>9 0.53471404 <a title="16-lda-9" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>10 0.53465396 <a title="16-lda-10" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>11 0.53453487 <a title="16-lda-11" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>12 0.53360361 <a title="16-lda-12" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>13 0.53275347 <a title="16-lda-13" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>14 0.53103006 <a title="16-lda-14" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>15 0.53065079 <a title="16-lda-15" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>16 0.52956831 <a title="16-lda-16" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>17 0.52938491 <a title="16-lda-17" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>18 0.52913702 <a title="16-lda-18" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>19 0.52896267 <a title="16-lda-19" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>20 0.52889317 <a title="16-lda-20" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
