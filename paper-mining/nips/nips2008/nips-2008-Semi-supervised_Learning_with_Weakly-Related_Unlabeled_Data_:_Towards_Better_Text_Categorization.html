<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-205" href="#">nips2008-205</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</h1>
<br/><p>Source: <a title="nips-2008-205-pdf" href="http://papers.nips.cc/paper/3488-semi-supervised-learning-with-weakly-related-unlabeled-data-towards-better-text-categorization.pdf">pdf</a></p><p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>Reference: <a title="nips-2008-205-reference" href="../nips2008_reference/nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. [sent-13, score-0.991]
</p><p>2 In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. [sent-14, score-0.444]
</p><p>3 We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. [sent-15, score-0.551]
</p><p>4 Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. [sent-16, score-0.401]
</p><p>5 The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. [sent-17, score-0.214]
</p><p>6 To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. [sent-18, score-0.608]
</p><p>7 For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. [sent-19, score-0.294]
</p><p>8 We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain. [sent-20, score-0.747]
</p><p>9 1 Introduction Semi-supervised Learning (SSL) takes advantage of a large amount of unlabeled data to enhance classiﬁcation accuracy. [sent-21, score-0.376]
</p><p>10 Its application to text categorization is stimulated by the easy availability of an overwhelming number of unannotated web pages, in contrast to the limited number of annotated ones. [sent-22, score-0.39]
</p><p>11 Intuitively, corpora with different topics may not be content wise related, however, word usage exhibits consistent patterns within a language. [sent-23, score-0.236]
</p><p>12 Then the question is, what would be an effective SSL strategy to extract these valuable word usage patterns embedded in the unlabeled corpus? [sent-24, score-0.549]
</p><p>13 In this paper, we aim to identify a new data representation, that is on one hand informative to the target class (category), and on the other hand consistent with the feature coherence patterns exhibiting in the weakly related unlabeled data. [sent-25, score-0.579]
</p><p>14 In this section, we ﬁrst review the two types of semisupervised learning: transductive SSL and inductive SSL. [sent-27, score-0.256]
</p><p>15 Then we state SSL with weakly related unlabeled data as a new challenge. [sent-28, score-0.486]
</p><p>16 Finally, we provide a strategy of how to address this challenge in the domain of text categorization, as well as a brief summary of related work in text categorization. [sent-29, score-0.304]
</p><p>17 Speciﬁcally, TSVM extends the maximum margin principle of SVM to unlabeled data. [sent-32, score-0.39]
</p><p>18 It combines the regularization of SVMs on the labeled points with the cluster assumption on the unlabeled points, to enforce the decision boundary to lie in low density regions. [sent-33, score-0.645]
</p><p>19 Data based methods discover an inherent geometry in the data, and exploit it in ﬁnding a good classiﬁer, to which additional regularization based on unlabeled data is added to avoid overﬁtting. [sent-34, score-0.418]
</p><p>20 Information Regularization seeks a good conditional Pr(y|x), assuming that the decision boundary lies in a low density area and Pr(y|x) only varies a little in the area of high density. [sent-37, score-0.143]
</p><p>21 In addition, it further computes a new data representation based on the unlabeled data, which often results in better classiﬁcation performance for SSL. [sent-39, score-0.356]
</p><p>22 In this sense, some SSL algorithms, though named as “transductive”, have an inductive nature. [sent-42, score-0.154]
</p><p>23 For example, TSVM is an inductive learner, because it learns a classiﬁer from a mixture of labeled and unlabeled data. [sent-43, score-0.574]
</p><p>24 Similarly, as an inductive component of Low Density Separation (LDS) [11], ∆ TSVMs learns the SVM classiﬁcation model in the primal, which can be used for predicting new data. [sent-44, score-0.154]
</p><p>25 Manifold Regularization [1] also has an implementation with inductive nature. [sent-46, score-0.154]
</p><p>26 As stated in [11], either directly or indirectly, all successful semi-supervised algorithms typically make the cluster assumption, which puts the decision boundary in low density areas without crossing the high density regions. [sent-51, score-0.2]
</p><p>27 Note that the cluster assumption is only meaningful when the labeled and unlabeled data are somehow closely related. [sent-52, score-0.461]
</p><p>28 When the unlabeled data comes from arbitrary data sources, their input patterns may not be closely related to that of labeled ones. [sent-53, score-0.49]
</p><p>29 As a result, the labeled and unlabeled data could be well separated, which makes it difﬁcult, if not impossible, to exploit the cluster assumption. [sent-54, score-0.461]
</p><p>30 Hence, the key challenge is how to leverage the seemingly unrelated unlabeled data to improve the classiﬁcation accuracy of the target classes. [sent-55, score-0.41]
</p><p>31 Analogous to transfer learning in which information from one category may be generalized to the others, we propose a scheme that helps the categorization of one data source, by making use of information from other unlabeled data sources with little relevance. [sent-56, score-0.684]
</p><p>32 Our study stands in contrast to the previous ones in that we aim to make maximum use of the unlabeled data that is weakly related to the test bed. [sent-57, score-0.486]
</p><p>33 We refer to this problem as “SSL with weakly related unlabeled data”, or SSLW for short. [sent-58, score-0.486]
</p><p>34 We ﬁrst build a maximum margin framework for SSL with weakly related unlabeled data. [sent-59, score-0.52]
</p><p>35 A typical approach for semi-supervised learning with weakly related unlabeled data, presented in the recent study [13] is to ﬁrst derive a new data representation from unlabeled data, and then apply supervised learning technique to the derived new data representation. [sent-61, score-0.842]
</p><p>36 The new dimensions derived from the unlabeled data can then be used to represent the labeled data points for supervised learning. [sent-63, score-0.42]
</p><p>37 In particular, the data representation generated by our method  2  exploits both labeled and unlabeled data, which differentiates the proposed framework from selftaught learning. [sent-66, score-0.42]
</p><p>38 However in this study, we focus on text categorization with a small training set. [sent-68, score-0.401]
</p><p>39 Text categorization has been actively studied in the communities of Web data mining, information retrieval and statistical learning [9, 20]. [sent-69, score-0.224]
</p><p>40 A number of statistical learning techniques have been applied to text categorization [19], including the K Nearest Neighbor approaches, decision trees, Bayesian classiﬁers, inductive rule learning, neural networks, support vector machines (SVM), and logistic regression. [sent-70, score-0.597]
</p><p>41 Empirical studies [7] have shown that support vector machines (SVM) is the leading technique for text categorization. [sent-71, score-0.194]
</p><p>42 Given the limited amount of labeled documents, the key of semi-supervised text categorization is to exploit the unlabeled documents. [sent-72, score-0.81]
</p><p>43 The popular implementations of semi-supervised SVMs in [8, 15] are considered to be state-of-the-art in text categorization. [sent-73, score-0.14]
</p><p>44 For text categorization with a small training pool, it is very likely that a large portion of words used by the testing documents are unseen in the training set, which could lead to a poor estimation of the similarity between documents. [sent-74, score-0.654]
</p><p>45 , word correlation) from both the labeled and unlabeled documents, we will be able to more accurately estimate the document similarity, particularly for documents sharing few or no common words, thus improving the overall classiﬁcation accuracy. [sent-77, score-0.663]
</p><p>46 A straightforward approach is to utilize the word cooccurrence information for computing document similarity. [sent-78, score-0.149]
</p><p>47 However, this straightforward approach may not serve the best interests of word correlation, because not all of the co-occurrence patterns are useful. [sent-79, score-0.172]
</p><p>48 To address this problem, SSLW explicitly estimates the optimal wordcorrelation matrix for the target document categorization problem. [sent-84, score-0.388]
</p><p>49 In Section 3, we propose the framework of SSL with weakly-related unlabeled data, followed by an efﬁcient algorithm for its computation in Section 4. [sent-87, score-0.356]
</p><p>50 , (xl , yl )} as the collection of labeled documents, where yi is +1 when document xi belongs to a given document category and −1 when it does not (text categorization problem for multi-labeled documents can be treated as a set of independent binary classiﬁcation problems). [sent-93, score-0.551]
</p><p>51 Importantly, as an SSL task with weakly-related unlabeled data, U comes from some external resources that are weakly related to the test domain. [sent-99, score-0.538]
</p><p>52 To solve this problem, we take into account a word-correlation matrix when computing the kernel similarity matrix, and we search for an optimal word-correlation matrix, towards maximizing the categorization margin. [sent-123, score-0.308]
</p><p>53 Speciﬁcally, we deﬁne the kernel matrix as K = D RD, by introducing the word-correlation matrix R ∈ RV ×V , where each element Ri,j represents the correlation between the ith and the jth words. [sent-124, score-0.16]
</p><p>54 Note G G is not a desirable solution to R, because it is improper to assign a high correlation to two words simply because of their high co-occurrence; the two words may be not closely related as judged by the maximum-margin criterion. [sent-125, score-0.164]
</p><p>55 (1) as κ(K): 1 κ(K) = max α e − (α ◦ y) K(α ◦ y) (2) α 2 Given the fact that κ(K) is inversely-related to the categorization margin [4], minimizing κ(K) is equivalent to maximizing the categorization margin. [sent-129, score-0.482]
</p><p>56 The G matrix is crucial in capturing the word correlation information from the weakly-related external source U. [sent-131, score-0.291]
</p><p>57 Thus, to incorporate the external source into the learning of the word-correlation matrix R, we regularize R according to G by introducing an internal representation of words W = (w1 , w2 , . [sent-132, score-0.195]
</p><p>58 , wV ), where vector wi is the internal representation of the ith word (This idea is similar to non-negative matrix factorization (NMF) [6]). [sent-135, score-0.166]
</p><p>59 As there exists a matrix U such that the matrix G can be recovered from W by a linear transformation G = U W , the word-correlation matrix can be computed as R = W W . [sent-139, score-0.18]
</p><p>60 Another strategy we use to involve the unlabeled data into the learning of word correlation, is to construct the word correlation matrix R as a non-negative linear combination of the top p right eigenvectors of G, i. [sent-141, score-0.697]
</p><p>61 This simpliﬁcation of R allows us to effectively extract and utilize the word co-occurrence information in the external source U. [sent-152, score-0.191]
</p><p>62 (1), to search for an optimal word-correlation matrix R, by exploiting the word co-occurrence information in the external U, under maximum-margin criterion, i. [sent-163, score-0.218]
</p><p>63 5 Evaluation In this section, we evaluate SSLW on text categorization with limited training data. [sent-209, score-0.427]
</p><p>64 As an SSL 5  task with weakly-related unlabeled data, the provided unlabeled data have little relevance to the test domain. [sent-213, score-0.733]
</p><p>65 We show that SSLW can achieve noticeable gains over the state-of-the-art methods in both inductive SSL and text categorization, and we provide insight into why this happens. [sent-214, score-0.329]
</p><p>66 Following [18], our implementation of SSLW selects the top 200 right eigenvectors of the document-word matrix G matrix to construct the R matrix. [sent-215, score-0.149]
</p><p>67 As deﬁned in Section 3, the G matrix covers both the training sets and the weakly-related external collection. [sent-216, score-0.149]
</p><p>68 Evaluation datasets Two standard datasets for text categorization are used as the evaluation test bed: the Reuters-21578 dataset and the WebKB dataset. [sent-217, score-0.411]
</p><p>69 For computational simplicity, 1000 documents are randomly selected from the TREC AP88 dataset and are used as an external information source for both datasets. [sent-218, score-0.226]
</p><p>70 The AP88 dataset includes a collection of news documents reported by Associated Press in 1988. [sent-219, score-0.141]
</p><p>71 For the Reuters-21578 dataset, among the 135 TOPICS categories, the 10 categories with the largest amount of documents are selected (see Table 1). [sent-221, score-0.135]
</p><p>72 The Reuters-21578 dataset and the TREC AP88 dataset have very limited relevance in topic; and the WebKB dataset and the TREC AP88 dataset are even less content-wise related. [sent-225, score-0.214]
</p><p>73 Category # Samples  earn 3987  acq 2448  money-fx 801  crude 634  grain 628  trade 552  interest 513  wheat 306  ship 305  corn 254  Table 1: The ten categories of the Reuters-21578 dataset with the largest amount of documents. [sent-226, score-0.373]
</p><p>74 Category # Samples  course 930  department 182  faculty 1124  project 504  staff 137  student 1641  Table 2: The six categories of the WebKB dataset. [sent-227, score-0.203]
</p><p>75 Therefore, we adopt the area under the ROC curve (AUR) [12] as the quantitative measurement of the binary classiﬁcation performance for text categorization. [sent-231, score-0.14]
</p><p>76 The third baseline is TSVM 2 , the inductive component of LDS, which delivers the state-of-the-art performance of SSL. [sent-236, score-0.232]
</p><p>77 The fourth baseline Manifold Regularization 3 (ManifoldR for short) is included as a state-of-the-art SSL approach with an inductive nature, and more importantly, being able to incorporate word relationship into the regularization. [sent-237, score-0.314]
</p><p>78 For the ﬁfth baseline, we compare the word-correlation matrix estimated by SSLW, with the trivial word-correlation matrix G G; and we name this baseline as COR. [sent-238, score-0.174]
</p><p>79 It uses the unlabeled data to ﬁnd an low-dimension representation, and then conducts standard classiﬁcation in this new space. [sent-240, score-0.386]
</p><p>80 This observation reveals that if the unlabeled data are only weakly relevant to the target class, it could 1  http://www. [sent-249, score-0.489]
</p><p>81 html 2  6  harm the categorization accuracy by simply pushing the decision boundary towards the low density regions, and away from the high density areas of the unlabeled data. [sent-259, score-0.739]
</p><p>82 These results demonstrate that SSLW is effective in improving text categorization accuracy with a small amount of training data. [sent-264, score-0.401]
</p><p>83 This proves SSLW is more effective than self-taught learning in using unlabeled data to improve classiﬁcation. [sent-268, score-0.356]
</p><p>84 Over all the ten categories except the “money-ﬁx” category, SSLW always delivers the lowest or the second lowest standard deviation, among all the six methods. [sent-274, score-0.132]
</p><p>85 In extreme cases where these words do not appear in any of the training documents, no association can be established between these words and the class labels. [sent-278, score-0.137]
</p><p>86 Evidently, test documents related to these unseen words are likely to be classiﬁed incorrectly. [sent-279, score-0.195]
</p><p>87 It is shown that SSLW maintains its clear advantage over the six baseline methods, across all the six categories. [sent-283, score-0.14]
</p><p>88 Category earn acq money-fx crude grain trade interest wheat ship corn  SVM 82. [sent-284, score-0.261]
</p><p>89 , how to leverage the unlabeled information that is weakly related to the target classes, to improve classiﬁcation performance. [sent-513, score-0.54]
</p><p>90 SSLW extends the theory of support vector machines to effectively identify those co-occurrence patterns that are most informative to the categorization margin and ignore those that are irrelevant to the categorization task. [sent-515, score-0.582]
</p><p>91 Applied to text categorization with limited number of training samples, SSLW automatically estimates the word correlation matrix by effectively exploiting the word co-occurrence embedded in the weakly-related unlabeled corpus. [sent-516, score-1.095]
</p><p>92 Empirical studies show that SSLW signiﬁcantly improves both the accuracy and the reliability of text categorization, given a small training pool and the additional unlabeled data that are weakly related to the test bed. [sent-517, score-0.663]
</p><p>93 Although SSLW is presented in the context of text categorization, it potentially facilitates classiﬁcation tasks in a variety of domains. [sent-518, score-0.14]
</p><p>94 We will also investigate SSLW’s dependencies on the number of eigenvectors used, and its behavior when varying the number of labeled training examples. [sent-520, score-0.13]
</p><p>95 Manifold regularization: A geometric framework for learning from labeled and unlabeled examples. [sent-527, score-0.42]
</p><p>96 Text categorization with support vector machines: learning with many relevant features. [sent-567, score-0.247]
</p><p>97 Transductive inference for text classiﬁcation using support vector machines. [sent-572, score-0.163]
</p><p>98 A comprehensive comparative study on term weighting schemes for text categorization with support vector machines. [sent-583, score-0.387]
</p><p>99 Discriminative cluster reﬁnement: Improving object category recognition given limited training data. [sent-647, score-0.187]
</p><p>100 A comparative study on feature selection in text categorization. [sent-659, score-0.14]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('sslw', 0.645), ('unlabeled', 0.356), ('categorization', 0.224), ('ssl', 0.218), ('tsvm', 0.208), ('aur', 0.187), ('inductive', 0.154), ('text', 0.14), ('webkb', 0.119), ('weakly', 0.106), ('word', 0.106), ('transductive', 0.102), ('documents', 0.094), ('category', 0.083), ('svm', 0.074), ('manifoldr', 0.068), ('classi', 0.067), ('manifold', 0.066), ('labeled', 0.064), ('regularization', 0.062), ('matrix', 0.06), ('baseline', 0.054), ('lds', 0.054), ('external', 0.052), ('cor', 0.051), ('ship', 0.051), ('staff', 0.051), ('wheat', 0.051), ('words', 0.05), ('sdp', 0.048), ('dataset', 0.047), ('patterns', 0.046), ('document', 0.043), ('six', 0.043), ('categories', 0.041), ('cluster', 0.041), ('usage', 0.041), ('socp', 0.041), ('correlation', 0.04), ('trec', 0.038), ('faculty', 0.038), ('dual', 0.037), ('density', 0.037), ('training', 0.037), ('gains', 0.035), ('margin', 0.034), ('boundary', 0.034), ('acq', 0.034), ('corn', 0.034), ('earn', 0.034), ('grain', 0.034), ('wordcorrelation', 0.034), ('iv', 0.034), ('trace', 0.033), ('cr', 0.033), ('source', 0.033), ('machines', 0.031), ('cation', 0.031), ('si', 0.031), ('sedumi', 0.03), ('conducts', 0.03), ('student', 0.03), ('eigenvectors', 0.029), ('baselines', 0.028), ('leverage', 0.027), ('forbes', 0.027), ('target', 0.027), ('unseen', 0.027), ('rank', 0.026), ('limited', 0.026), ('coding', 0.026), ('rd', 0.026), ('low', 0.026), ('jin', 0.025), ('ct', 0.025), ('decision', 0.025), ('similarity', 0.024), ('pittsburgh', 0.024), ('tr', 0.024), ('delivers', 0.024), ('related', 0.024), ('ten', 0.024), ('support', 0.023), ('crude', 0.023), ('yang', 0.023), ('topics', 0.022), ('icml', 0.022), ('xl', 0.022), ('little', 0.021), ('testing', 0.021), ('corpora', 0.021), ('avenue', 0.021), ('battle', 0.021), ('coherence', 0.02), ('enhance', 0.02), ('valid', 0.02), ('serve', 0.02), ('separation', 0.02), ('cone', 0.019), ('formalism', 0.019)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="205-tfidf-1" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>2 0.25098854 <a title="205-tfidf-2" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>Author: Yi Zhang, Artur Dubrawski, Jeff G. Schneider</p><p>Abstract: In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any speciﬁc task in the same space through regularization. In an empirical study, we construct 190 different text classiﬁcation tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classiﬁcation tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the prediction model used.</p><p>3 0.24407852 <a title="205-tfidf-3" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>4 0.14808832 <a title="205-tfidf-4" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>Author: Sudheendra Vijayanarasimhan, Kristen Grauman</p><p>Abstract: We introduce a framework for actively learning visual categories from a mixture of weakly and strongly labeled image examples. We propose to allow the categorylearner to strategically choose what annotations it receives—based on both the expected reduction in uncertainty as well as the relative costs of obtaining each annotation. We construct a multiple-instance discriminative classiﬁer based on the initial training data. Then all remaining unlabeled and weakly labeled examples are surveyed to actively determine which annotation ought to be requested next. After each request, the current classiﬁer is incrementally updated. Unlike previous work, our approach accounts for the fact that the optimal use of manual annotation may call for a combination of labels at multiple levels of granularity (e.g., a full segmentation on some images and a present/absent ﬂag on others). As a result, it is possible to learn more accurate category models with a lower total expenditure of manual annotation effort. 1</p><p>5 0.13353392 <a title="205-tfidf-5" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>Author: Doug Downey, Oren Etzioni</p><p>Abstract: Is accurate classiﬁcation possible in the absence of hand-labeled data? This paper introduces the Monotonic Feature (MF) abstraction—where the probability of class membership increases monotonically with the MF’s value. The paper proves that when an MF is given, PAC learning is possible with no hand-labeled data under certain assumptions. We argue that MFs arise naturally in a broad range of textual classiﬁcation applications. On the classic “20 Newsgroups” data set, a learner given an MF and unlabeled data achieves classiﬁcation accuracy equal to that of a state-of-the-art semi-supervised learner relying on 160 hand-labeled examples. Even when MFs are not given as input, their presence or absence can be determined from a small amount of hand-labeled data, which yields a new semi-supervised learning method that reduces error by 15% on the 20 Newsgroups data. 1</p><p>6 0.12657103 <a title="205-tfidf-6" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>7 0.1179284 <a title="205-tfidf-7" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>8 0.11303762 <a title="205-tfidf-8" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>9 0.11264227 <a title="205-tfidf-9" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>10 0.1056402 <a title="205-tfidf-10" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>11 0.092574492 <a title="205-tfidf-11" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>12 0.087389477 <a title="205-tfidf-12" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>13 0.080728345 <a title="205-tfidf-13" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>14 0.074901216 <a title="205-tfidf-14" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>15 0.073225908 <a title="205-tfidf-15" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>16 0.06920436 <a title="205-tfidf-16" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>17 0.065518849 <a title="205-tfidf-17" href="./nips-2008-Translated_Learning%3A_Transfer_Learning_across_Different_Feature_Spaces.html">242 nips-2008-Translated Learning: Transfer Learning across Different Feature Spaces</a></p>
<p>18 0.063835032 <a title="205-tfidf-18" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>19 0.061680667 <a title="205-tfidf-19" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>20 0.060872685 <a title="205-tfidf-20" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.199), (1, -0.146), (2, -0.024), (3, -0.102), (4, -0.081), (5, 0.053), (6, 0.098), (7, 0.048), (8, -0.143), (9, 0.042), (10, 0.124), (11, 0.004), (12, -0.248), (13, -0.155), (14, -0.097), (15, 0.045), (16, -0.083), (17, 0.184), (18, 0.05), (19, 0.006), (20, -0.084), (21, 0.038), (22, -0.084), (23, -0.127), (24, 0.024), (25, -0.078), (26, -0.09), (27, -0.063), (28, 0.058), (29, 0.001), (30, -0.019), (31, 0.058), (32, -0.154), (33, -0.052), (34, -0.027), (35, 0.006), (36, 0.109), (37, -0.036), (38, -0.049), (39, 0.033), (40, -0.078), (41, 0.001), (42, -0.063), (43, 0.041), (44, -0.027), (45, -0.02), (46, 0.028), (47, -0.02), (48, -0.006), (49, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94298464 <a title="205-lsi-1" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>2 0.82938534 <a title="205-lsi-2" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>Author: Doug Downey, Oren Etzioni</p><p>Abstract: Is accurate classiﬁcation possible in the absence of hand-labeled data? This paper introduces the Monotonic Feature (MF) abstraction—where the probability of class membership increases monotonically with the MF’s value. The paper proves that when an MF is given, PAC learning is possible with no hand-labeled data under certain assumptions. We argue that MFs arise naturally in a broad range of textual classiﬁcation applications. On the classic “20 Newsgroups” data set, a learner given an MF and unlabeled data achieves classiﬁcation accuracy equal to that of a state-of-the-art semi-supervised learner relying on 160 hand-labeled examples. Even when MFs are not given as input, their presence or absence can be determined from a small amount of hand-labeled data, which yields a new semi-supervised learning method that reduces error by 15% on the 20 Newsgroups data. 1</p><p>3 0.80345362 <a title="205-lsi-3" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>Author: Yi Zhang, Artur Dubrawski, Jeff G. Schneider</p><p>Abstract: In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any speciﬁc task in the same space through regularization. In an empirical study, we construct 190 different text classiﬁcation tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classiﬁcation tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the prediction model used.</p><p>4 0.72957212 <a title="205-lsi-4" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>5 0.64340979 <a title="205-lsi-5" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>Author: Vikas Sindhwani, Jianying Hu, Aleksandra Mojsilovic</p><p>Abstract: By attempting to simultaneously partition both the rows (examples) and columns (features) of a data matrix, Co-clustering algorithms often demonstrate surprisingly impressive performance improvements over traditional one-sided row clustering techniques. A good clustering of features may be seen as a combinatorial transformation of the data matrix, effectively enforcing a form of regularization that may lead to a better clustering of examples (and vice-versa). In many applications, partial supervision in the form of a few row labels as well as column labels may be available to potentially assist co-clustering. In this paper, we develop two novel semi-supervised multi-class classiﬁcation algorithms motivated respectively by spectral bipartite graph partitioning and matrix approximation formulations for co-clustering. These algorithms (i) support dual supervision in the form of labels for both examples and/or features, (ii) provide principled predictive capability on out-of-sample test data, and (iii) arise naturally from the classical Representer theorem applied to regularization problems posed on a collection of Reproducing Kernel Hilbert Spaces. Empirical results demonstrate the effectiveness and utility of our algorithms. 1</p><p>6 0.55799145 <a title="205-lsi-6" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>7 0.53433126 <a title="205-lsi-7" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>8 0.49176651 <a title="205-lsi-8" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>9 0.48184678 <a title="205-lsi-9" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>10 0.4651368 <a title="205-lsi-10" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>11 0.41632926 <a title="205-lsi-11" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>12 0.40896657 <a title="205-lsi-12" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>13 0.39840773 <a title="205-lsi-13" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>14 0.39684629 <a title="205-lsi-14" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>15 0.37001595 <a title="205-lsi-15" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>16 0.36707214 <a title="205-lsi-16" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>17 0.33494043 <a title="205-lsi-17" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>18 0.33021018 <a title="205-lsi-18" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>19 0.33000171 <a title="205-lsi-19" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>20 0.31796527 <a title="205-lsi-20" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.062), (7, 0.087), (12, 0.078), (28, 0.127), (57, 0.065), (59, 0.017), (63, 0.033), (71, 0.027), (77, 0.064), (78, 0.02), (83, 0.095), (91, 0.211)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83999819 <a title="205-lda-1" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>Author: Jeremy Hill, Jason Farquhar, Suzanna Martens, Felix Biessmann, Bernhard Schölkopf</p><p>Abstract: From an information-theoretic perspective, a noisy transmission system such as a visual Brain-Computer Interface (BCI) speller could beneﬁt from the use of errorcorrecting codes. However, optimizing the code solely according to the maximal minimum-Hamming-distance criterion tends to lead to an overall increase in target frequency of target stimuli, and hence a signiﬁcantly reduced average target-to-target interval (TTI), leading to difﬁculties in classifying the individual event-related potentials (ERPs) due to overlap and refractory effects. Clearly any change to the stimulus setup must also respect the possible psychophysiological consequences. Here we report new EEG data from experiments in which we explore stimulus types and codebooks in a within-subject design, ﬁnding an interaction between the two factors. Our data demonstrate that the traditional, rowcolumn code has particular spatial properties that lead to better performance than one would expect from its TTIs and Hamming-distances alone, but nonetheless error-correcting codes can improve performance provided the right stimulus type is used. 1</p><p>same-paper 2 0.79412889 <a title="205-lda-2" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>3 0.73606175 <a title="205-lda-3" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>Author: Ydo Wexler, Christopher Meek</p><p>Abstract: We propose a multiplicative approximation scheme (MAS) for inference problems in graphical models, which can be applied to various inference algorithms. The method uses -decompositions which decompose functions used throughout the inference procedure into functions over smaller sets of variables with a known error . MAS translates these local approximations into bounds on the accuracy of the results. We show how to optimize -decompositions and provide a fast closed-form solution for an L2 approximation. Applying MAS to the Variable Elimination inference algorithm, we introduce an algorithm we call DynaDecomp which is extremely fast in practice and provides guaranteed error bounds on the result. The superior accuracy and efﬁciency of DynaDecomp is demonstrated. 1</p><p>4 0.68455577 <a title="205-lda-4" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>5 0.6839546 <a title="205-lda-5" href="./nips-2008-Grouping_Contours_Via_a_Related_Image.html">95 nips-2008-Grouping Contours Via a Related Image</a></p>
<p>Author: Praveen Srinivasan, Liming Wang, Jianbo Shi</p><p>Abstract: Contours have been established in the biological and computer vision literature as a compact yet descriptive representation of object shape. While individual contours provide structure, they lack the large spatial support of region segments (which lack internal structure). We present a method for further grouping of contours in an image using their relationship to the contours of a second, related image. Stereo, motion, and similarity all provide cues that can aid this task; contours that have similar transformations relating them to their matching contours in the second image likely belong to a single group. To ﬁnd matches for contours, we rely only on shape, which applies directly to all three modalities without modiﬁcation, in contrast to the specialized approaches developed for each independently. Visually salient contours are extracted in each image, along with a set of candidate transformations for aligning subsets of them. For each transformation, groups of contours with matching shape across the two images are identiﬁed to provide a context for evaluating matches of individual contour points across the images. The resulting contexts of contours are used to perform a ﬁnal grouping on contours in the original image while simultaneously ﬁnding matches in the related image, again by shape matching. We demonstrate grouping results on image pairs consisting of stereo, motion, and similar images. Our method also produces qualitatively better results against a baseline method that does not use the inferred contexts. 1</p><p>6 0.67827731 <a title="205-lda-6" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>7 0.67790711 <a title="205-lda-7" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>8 0.67138261 <a title="205-lda-8" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>9 0.67011577 <a title="205-lda-9" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>10 0.66714895 <a title="205-lda-10" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>11 0.6671207 <a title="205-lda-11" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>12 0.66573191 <a title="205-lda-12" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>13 0.66454297 <a title="205-lda-13" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>14 0.66385627 <a title="205-lda-14" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>15 0.66361457 <a title="205-lda-15" href="./nips-2008-Shape-Based_Object_Localization_for_Descriptive_Classification.html">207 nips-2008-Shape-Based Object Localization for Descriptive Classification</a></p>
<p>16 0.66046202 <a title="205-lda-16" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>17 0.65925777 <a title="205-lda-17" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>18 0.65888375 <a title="205-lda-18" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>19 0.65840507 <a title="205-lda-19" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>20 0.65738595 <a title="205-lda-20" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
