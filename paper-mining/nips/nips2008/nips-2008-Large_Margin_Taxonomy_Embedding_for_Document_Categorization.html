<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-114" href="#">nips2008-114</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</h1>
<br/><p>Source: <a title="nips-2008-114-pdf" href="http://papers.nips.cc/paper/3597-large-margin-taxonomy-embedding-for-document-categorization.pdf">pdf</a></p><p>Author: Kilian Q. Weinberger, Olivier Chapelle</p><p>Abstract: Applications of multi-class classiﬁcation, such as document categorization, often appear in cost-sensitive settings. Recent work has signiﬁcantly improved the state of the art by moving beyond “ﬂat” classiﬁcation through incorporation of class hierarchies [4]. We present a novel algorithm that goes beyond hierarchical classiﬁcation and estimates the latent semantic space that underlies the class hierarchy. In this space, each class is represented by a prototype and classiﬁcation is done with the simple nearest neighbor rule. The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. We show that our optimization is convex and can be solved efﬁciently for large data sets. Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization. 1</p><p>Reference: <a title="nips-2008-114-reference" href="../nips2008_reference/nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 com  Abstract Applications of multi-class classiﬁcation, such as document categorization, often appear in cost-sensitive settings. [sent-5, score-0.204]
</p><p>2 We present a novel algorithm that goes beyond hierarchical classiﬁcation and estimates the latent semantic space that underlies the class hierarchy. [sent-7, score-0.32]
</p><p>3 In this space, each class is represented by a prototype and classiﬁcation is done with the simple nearest neighbor rule. [sent-8, score-0.341]
</p><p>4 The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. [sent-9, score-0.72]
</p><p>5 Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization. [sent-11, score-0.365]
</p><p>6 Similarly, in the context of document categorization it seems more severe to misclassify a medical journal on heart attack as a publication on athlete’s foot than on Coronary artery disease. [sent-15, score-0.479]
</p><p>7 Although the scope of the proposed method is by no means limited to text data and topic hierarchies, for improved clarity we will restrict ourselves to terminology from document categorization throughout this paper. [sent-16, score-0.651]
</p><p>8 The most common approach to document categorization is to reduce the problem to a “ﬂat” classiﬁcation problem [13]. [sent-17, score-0.333]
</p><p>9 However, it is often the case that the topics are not just discrete classes, but are nodes in a complex taxonomy with rich inter-topic relationships. [sent-18, score-0.395]
</p><p>10 web taxonomy or medical journals can be categorized into the Medical Subject Headings (MeSH) taxonomy. [sent-20, score-0.442]
</p><p>11 Moving beyond ﬂat classiﬁcation to settings that utilize these hierarchical representations of topics has signiﬁcantly pushed the state-of-the art [4, 15]. [sent-21, score-0.136]
</p><p>12 Additional information about inter-topic relationships can for example be leveraged through cost-sensitive decision boundaries or knowledge sharing between documents from closely related classes. [sent-22, score-0.15]
</p><p>13 In reality, however, the topic taxonomy is a crude approximation of topic relations, created by an editor with knowledge of the true underlying semantic space of topics. [sent-23, score-1.083]
</p><p>14 In this paper we propose a method that moves beyond hierarchical presentations and aims to re-discover the continuous latent semantic space underlying the topic taxonomy. [sent-24, score-0.607]
</p><p>15 Instead of regarding document categorization as classiﬁcation, we will think of it as a regression problem where new documents are mapped into this latent semantic topic space. [sent-25, score-1.015]
</p><p>16 Very different from approaches like LSI or LDA [1, 7], our algorithm is entirely supervised and explicitly embeds the topic taxonomy and the documents into a single latent semantic space with “semantically meaningful” Euclidean distances. [sent-26, score-1.009]
</p><p>17 1  Topic taxonomy  T  Low dimensional semantic space  pneumonia  X  W  P stroke arthritis  High dimensional input space  F  original inputs  heart attack class prototypes  pα  embedded inputs  xi  Wxi  Figure 1: A schematic layout of our taxem method (for Taxonomy Embedding). [sent-27, score-1.585]
</p><p>18 The classes are embedded as prototypes inside the semantic space. [sent-28, score-0.666]
</p><p>19 The input documents are mapped into the same space, placed closest to their topic prototypes. [sent-29, score-0.512]
</p><p>20 In this paper we derive a method to embed the taxonomy of topics into a latent semantic space in form of topic prototypes. [sent-30, score-0.942]
</p><p>21 A new document can be classiﬁed by ﬁrst mapping it into this space and then assigning the label of the closest prototype. [sent-31, score-0.32]
</p><p>22 A key contribution of our paper is the derivation of a convex problem that learns the regressor for the documents and the placement of the prototypes in a single optimization. [sent-32, score-0.681]
</p><p>23 In particular, it places the topic prototypes such that for each document the prototype of the correct topic is much closer than any other prototype by a large margin. [sent-33, score-1.848]
</p><p>24 Our paper is structured as follows: In section 2 we introduce necessary notation and a ﬁrst version of the algorithm based on a two-step approach of ﬁrst embedding the hierarchical taxonomy into a semantic space and then regressing the input documents close to their respective topic prototypes. [sent-35, score-1.09]
</p><p>25 In section 3 we extend our model to a single optimization that learns both steps in one convex optimization with large margin constraints. [sent-36, score-0.258]
</p><p>26 We evaluate our method in section 4 and demonstrate state-of-the-art results on eight different document categorization tasks from the OHSUMED medical journal data set. [sent-37, score-0.411]
</p><p>27 In addition, the documents are accompanied by single topic labels y1 yn 1 c that lie in some taxonomy T with c total topics. [sent-41, score-0.736]
</p><p>28 This taxonomy T gives rise to some cost matrix C Rc c , where C 0 deﬁnes the cost of misclassifying an element of topic as and C = 0. [sent-42, score-0.774]
</p><p>29 Technically, we only require knowledge of the cost matrix C, which could also be obtained from side-information independent of a topic taxonomy. [sent-43, score-0.385]
</p><p>30 However, we would like to point out that a common way to infer a cost matrix from a taxonomy is to set C to the length of the shortest path between node and , but other approaches have also been studied [3]. [sent-45, score-0.423]
</p><p>31 Throughout this paper we denote document indices as i j 1 n and topic indices as 1 c . [sent-46, score-0.491]
</p><p>32 We would like to create a low dimensional semantic F and each document feature space F in which we represent each topic as a topic prototype p xi X as a low dimensional vector zi F. [sent-53, score-1.676]
</p><p>33 Our goal is to discover a representation of the data where distances reﬂect true underlying dissimilarities and proximity to prototypes indicates topic membership. [sent-54, score-0.774]
</p><p>34 In other words, documents on the same or related topics should be close to the respective topic prototypes, documents on highly different topics should be well separated. [sent-55, score-0.741]
</p><p>35 As an essential part of our method is to embed the classes that are typically found in a taxonomy, we refer to our algorithm as taxem (short for “taxonomy embedding”). [sent-57, score-0.252]
</p><p>36 Embedding topic prototypes The ﬁrst step of our algorithm is to embed the document taxonomy into a Euclidean vector space. [sent-58, score-1.29]
</p><p>37 , pc ∈ F based on the cost matrix C, where pα is the prototype that represents topic α. [sent-62, score-0.695]
</p><p>38 , pc ] ∈ Rc×c whose columns consist of the topic prototypes. [sent-66, score-0.312]
</p><p>39 There are many ways to derive the prototypes from the cost matrix C. [sent-67, score-0.556]
</p><p>40 Better results can be expected when the prototypes of similar topics are closer than those of dissimilar topics. [sent-71, score-0.565]
</p><p>41 We use the cost matrix C as an estimate of dissimilarity and aim to place the prototypes such that the distance pα − pβ 2 reﬂects the cost speciﬁed in Cαβ . [sent-72, score-0.622]
</p><p>42 (1)  α,β=1  If the cost matrix C deﬁnes squared Euclidean distances (e. [sent-74, score-0.127]
</p><p>43 1 Both prototypes embeddings PI and Pmds are still independent of the input data {xi }. [sent-81, score-0.458]
</p><p>44 Before we can derive a more sophisticated method to place the prototypes with large margin constraints on the document vectors, we will brieﬂy describe the mapping W : X → F of the input documents into the low dimensional feature space F. [sent-82, score-1.133]
</p><p>45 We need to ﬁnd an appropriate mapping W : X → F, that maps each input xi with label yi as close as possible to its topic prototype pyi . [sent-84, score-0.837]
</p><p>46 We can ﬁnd such a linear transformation zi = Wxi by setting pyi − Wxi  W = argminW  2  +λ W  2 F. [sent-85, score-0.156]
</p><p>47 Inference Given an input vector xt we ﬁrst map it into F and estimate its label as the topic with the closest prototype pα yt = argminα pα − Wxt 2 . [sent-94, score-0.622]
</p><p>48 3  Topic taxonomy  T  I  α  eα  Low dimensional semantic space  yi High dimensional input space  xi  pα  ey i py i  P  X  A  F  zi  xi  Rd  embedded inputs  Rc  large margin  Rc  class prototypes  Figure 2: The schematic layout of the large-margin embedding of the taxonomy and the documents. [sent-96, score-1.978]
</p><p>49 As a ﬁrst step, we represent topic as the vector e and document xi as xi = Axi . [sent-97, score-0.647]
</p><p>50 We then learn the matrix P whose columns are the prototypes p = Pe and which deﬁnes the ﬁnal transformation of the documents zi = Pxi . [sent-98, score-0.691]
</p><p>51 This ﬁnal transformation is learned such that the correct prototype pyi is closer to zi than any other prototype p by a large margin. [sent-99, score-0.768]
</p><p>52 However, learning the prototypes independent of the data xi is far from optimal in order to reduce the loss in (5). [sent-101, score-0.59]
</p><p>53 In this section we will create a joint optimization problem that places the prototypes P and learns the mapping W while minimizing an upper bound on (5). [sent-102, score-0.611]
</p><p>54 We want to map the input documents closest to their prototypes and at the same time place the prototypes where the documents of the respective topic are mapped to. [sent-104, score-1.602]
</p><p>55 (6) is entirely independent of P and can be pre-computed before the prototypes have been positioned. [sent-109, score-0.482]
</p><p>56 We can then rewrite both, the topic prototypes p and the low dimensional documents zi , Rc : as vectors within the range of P : Rc p = Pe  and zi = Pxi  (7)  Optimization Ideally we would like to learn P to minimize (5) directly. [sent-113, score-1.119]
</p><p>57 4  The loss for a speciﬁc document xi is zero if its corresponding vector zi is closer to the correct prototype pyi than to any other prototype pα . [sent-116, score-1.104]
</p><p>58 For better generalization it would be preferable if prototype pyi was in fact much closer by a large margin. [sent-117, score-0.432]
</p><p>59 We can go even further and demand that prototypes that would incur a larger misclassiﬁcation loss should be further separated than those with a small cost. [sent-118, score-0.512]
</p><p>60 We can express this condition as a set of “soft” inequality constraints, in terms of squared-distances, ∀i, α = yi  P(eyi − xi )  2 2  + Cyi α ≤ P(eα − xi )  2 2  + ξiα ,  (8)  where the slack-variable ξiα ≥ 0 absorbs the amount of violation of prototype pα into the margin of xi . [sent-120, score-0.649]
</p><p>61 Given this formulation, we create an upper bound on the loss function (5): Theorem 1 Given a prototype matrix P, the training error (5) is bounded above by  1 n  iα ξiα . [sent-121, score-0.401]
</p><p>62 Proof: First, note that we can rewrite the assignment of the closest prototype (4) as yi = ˆ argminα P(eα − xi ) 2 . [sent-122, score-0.458]
</p><p>63 It follows that P(eyi − xi ) 2 − P(eyi − xi ) 2 ≥ 0 for all i (with ˆ 2 2 equality when yi = yi ). [sent-123, score-0.246]
</p><p>64 We therefore obtain: ˆ ξiˆi = P(eyi − xi ) y  2 2  + Cyi yi − P(eyi − xi ) ˆ ˆ  2 2  ≥ Cyi yi . [sent-124, score-0.246]
</p><p>65 (8), allows us to create an optimization problem that minimizes an upper bound on the average loss in eq. [sent-127, score-0.134]
</p><p>66 (5) with maximum-margin constraints: ξiα subject to:  Minimize P  i,α  (1) P(eyi − xi ) (2) ξiα ≥ 0  2 2  + Cyi α ≤ P(eα − xi )  2 2  (11)  + ξiα  Note that if we have a very large number of classes, it might be beneﬁcial to choose P ∈ Rr×c with r < c. [sent-128, score-0.156]
</p><p>67 We can make (11) invariant to rotation by deﬁning Q = P P, and rewriting all distances in terms of Q, P(eα − xi )  2 2  = (eα − xi ) Q(eα − xi ) = eα − xi  2 Q. [sent-134, score-0.341]
</p><p>68 We can now solve (11) in terms of Q instead of P with the large-margin constraints ∀i, α = yi  eyi − xi  2 Q  + Cyi α ≤ eα − xi  2 Q  + ξiα . [sent-139, score-0.39]
</p><p>69 Even if the training data might differ from the test data, we know that the taxonomy does not change. [sent-142, score-0.299]
</p><p>70 for all i we have xi = eyi , Pmds satisﬁes all constraints (8) as equalities with zero slack. [sent-145, score-0.267]
</p><p>71 The ﬁnal convex optimization of F taxem with regularized objective becomes: ¯ ξiα + µ Q − C  2 F  subject to:  + Cyi α ≤ eα − xi  2 Q  + ξiα  Minimize (1 − µ) Q  (1) eyi − xi (2) ξiα ≥ 0 (3) Q 0  i,α 2 Q  (14)  The constant µ ∈ [0, 1] regulates the impact of the regularization term. [sent-150, score-0.63]
</p><p>72 Once the optimal solution Q∗ is found, one can obtain the position of the prototypes with a simple svd or cholesky decomposition Q∗ = P P and consequently also obtains the mapping W from W = PA. [sent-153, score-0.527]
</p><p>73 4  Results  We evaluated our algorithm taxem on several classiﬁcation problems derived from categorizing publications in the public OHSUMED medical journal data base into the Medical Subject Headings (MeSH) taxonomy. [sent-154, score-0.288]
</p><p>74 Setup and data set description We used the OHSUMED 87 corpus [9], which consists of abstracts and titles of medical publications. [sent-155, score-0.125]
</p><p>75 We removed all topic categories that did not appear in the MeSH taxonomy (due to out-dated topic names). [sent-160, score-0.909]
</p><p>76 For each problem, we created a 70%/30% random split in training and test samples, ensuring however that each topic had at least one document in the training corpus. [sent-166, score-0.491]
</p><p>77 50  Table 2: The cost-sensitive test error results on various ohsumed classiﬁcation data sets. [sent-238, score-0.141]
</p><p>78 The taxem algorithm obtains the lowest overall loss and the lowest individual loss on each data set except B. [sent-242, score-0.432]
</p><p>79 compared taxem against four commonly used algorithms for document categorization: 1. [sent-243, score-0.414]
</p><p>80 the Cai and Hoffmann SVM formulation with a cost sensitive hierarchical loss function (SVM tax) [4]. [sent-248, score-0.196]
</p><p>81 All SVM classiﬁers were trained with regularization constant C = 1 (which worked best on problem B; this value is also commonly used in text classiﬁcation when the documents have unit length). [sent-249, score-0.217]
</p><p>82 Further, we also evaluated the difference between our large margin formulation (taxem) and the results with the simplex (PI -taxem) and mds (Pmds -taxem) prototypes. [sent-250, score-0.201]
</p><p>83 Up to statistical signiﬁcance, taxem obtains the lowest loss on all data sets and the lowest overall loss. [sent-254, score-0.378]
</p><p>84 Ignoring statistical signiﬁcance, taxem has the lowest loss on all data sets except B. [sent-255, score-0.305]
</p><p>85 5  Related Work  In recent years, several algorithms for document categorization have been proposed. [sent-260, score-0.333]
</p><p>86 Several authors proposed adaptations of support vector machines that incorporate the topic taxonomy through costsensitive loss re-weighting and classiﬁcation at multiple nodes in the hierarchy [4, 8, 11]. [sent-261, score-0.671]
</p><p>87 It differs from all these methods in that it learns a low dimensional semantic representation of the documents and classiﬁes by ﬁnding the nearest prototype. [sent-263, score-0.52]
</p><p>88 Although their algorithm also reduces the dimensionality with a linear projection, their low dimensional space is obtained through supervised clustering on the document data. [sent-265, score-0.355]
</p><p>89 In contrast, the semantic space obtained with taxem is obtained through a convex optimization with maximum margin constraints. [sent-266, score-0.592]
</p><p>90 Further, the low dimensional representation of our method is explicitly constructed to give rise to meaningful Euclidean distances. [sent-267, score-0.146]
</p><p>91 The optimization with large-margin constraints was partially inspired by recent work on large margin distance metric learning for nearest neighbor classiﬁcation [16]. [sent-268, score-0.239]
</p><p>92 However our formulation is a much more light-weight optimization problem with O(cn) constraints instead of O(n2 ) as in [16]. [sent-269, score-0.13]
</p><p>93 7  6  Conclusion  In this paper, we have presented a novel framework for classiﬁcation with inter-class relationships based on taxonomy embedding and supervised dimensionality reduction. [sent-271, score-0.375]
</p><p>94 We derived a single convex optimization problem that learns an embedding of the topic taxonomy as well as a linear mapping from the document space to the resulting low dimensional semantic space. [sent-272, score-1.358]
</p><p>95 As future work we are planning to extend our algorithm to the more general setting of document categorization with multiple topic memberships and multi-modal topic distributions. [sent-273, score-0.907]
</p><p>96 Further, we are keen to explore the implications of our proposed conversion of discrete topic taxonomies into continuous semantic spaces. [sent-274, score-0.468]
</p><p>97 A natural step is to consider the document matching problem (e. [sent-276, score-0.204]
</p><p>98 of web pages and advertisements) in the semantic space: a fast nearest neighbor search can be performed in a joint low dimensional space without having to resort to classiﬁcation all together. [sent-278, score-0.422]
</p><p>99 Although this paper is presented in the context of document categorization, it is important to emphasize that our method is by no means limited to text data or class hierarchies. [sent-279, score-0.235]
</p><p>100 Concept indexing a fast dimensionality reduction algorithm with applications to document retrieval & categorization, 2000. [sent-343, score-0.273]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('prototypes', 0.458), ('taxonomy', 0.299), ('topic', 0.287), ('prototype', 0.285), ('pmds', 0.21), ('taxem', 0.21), ('document', 0.204), ('cyi', 0.189), ('semantic', 0.181), ('documents', 0.15), ('eyi', 0.141), ('ohsumed', 0.141), ('categorization', 0.129), ('pyi', 0.105), ('rc', 0.105), ('dimensional', 0.087), ('margin', 0.085), ('mesh', 0.084), ('mcsvm', 0.084), ('svm', 0.079), ('medical', 0.078), ('xi', 0.078), ('embedding', 0.076), ('classi', 0.067), ('cost', 0.066), ('topics', 0.065), ('karypis', 0.063), ('wxi', 0.063), ('headings', 0.055), ('cai', 0.055), ('loss', 0.054), ('zi', 0.051), ('optimization', 0.05), ('closest', 0.05), ('constraints', 0.048), ('abstracts', 0.047), ('cance', 0.046), ('yi', 0.045), ('mds', 0.045), ('hierarchical', 0.044), ('pi', 0.043), ('embed', 0.042), ('retrieval', 0.042), ('hoffmann', 0.042), ('pxi', 0.042), ('closer', 0.042), ('lowest', 0.041), ('latent', 0.039), ('simplex', 0.039), ('layout', 0.039), ('cation', 0.038), ('mapping', 0.037), ('euclidean', 0.037), ('convex', 0.037), ('misclassify', 0.037), ('populated', 0.037), ('tax', 0.037), ('kilian', 0.037), ('categories', 0.036), ('regularization', 0.036), ('learns', 0.036), ('xx', 0.036), ('yahoo', 0.036), ('low', 0.035), ('web', 0.034), ('dumais', 0.034), ('misclassi', 0.033), ('formulation', 0.032), ('obtains', 0.032), ('matrix', 0.032), ('weinberger', 0.031), ('categorized', 0.031), ('attack', 0.031), ('rr', 0.031), ('nearest', 0.031), ('nodes', 0.031), ('text', 0.031), ('acm', 0.031), ('create', 0.03), ('schematic', 0.03), ('wordnet', 0.03), ('axi', 0.03), ('highlighted', 0.03), ('han', 0.03), ('distances', 0.029), ('space', 0.029), ('pe', 0.028), ('embedded', 0.027), ('indexing', 0.027), ('crammer', 0.027), ('beyond', 0.027), ('shortest', 0.026), ('df', 0.026), ('mapped', 0.025), ('neighbor', 0.025), ('pc', 0.025), ('entirely', 0.024), ('respective', 0.024), ('category', 0.024), ('rise', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="114-tfidf-1" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>Author: Kilian Q. Weinberger, Olivier Chapelle</p><p>Abstract: Applications of multi-class classiﬁcation, such as document categorization, often appear in cost-sensitive settings. Recent work has signiﬁcantly improved the state of the art by moving beyond “ﬂat” classiﬁcation through incorporation of class hierarchies [4]. We present a novel algorithm that goes beyond hierarchical classiﬁcation and estimates the latent semantic space that underlies the class hierarchy. In this space, each class is represented by a prototype and classiﬁcation is done with the simple nearest neighbor rule. The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. We show that our optimization is convex and can be solved efﬁciently for large data sets. Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization. 1</p><p>2 0.22346869 <a title="114-tfidf-2" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>Author: Jordan L. Boyd-graber, David M. Blei</p><p>Abstract: We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-speciﬁc topic weights and parse-tree-speciﬁc syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents. 1</p><p>3 0.1881658 <a title="114-tfidf-3" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>Author: Yi Zhang, Artur Dubrawski, Jeff G. Schneider</p><p>Abstract: In this paper, we address the question of what kind of knowledge is generally transferable from unlabeled text. We suggest and analyze the semantic correlation of words as a generally transferable structure of the language and propose a new method to learn this structure using an appropriately chosen latent variable model. This semantic correlation contains structural information of the language space and can be used to control the joint shrinkage of model parameters for any speciﬁc task in the same space through regularization. In an empirical study, we construct 190 different text classiﬁcation tasks from a real-world benchmark, and the unlabeled documents are a mixture from all these tasks. We test the ability of various algorithms to use the mixed unlabeled text to enhance all classiﬁcation tasks. Empirical results show that the proposed approach is a reliable and scalable method for semi-supervised learning, regardless of the source of unlabeled data, the speciﬁc task to be enhanced, and the prediction model used.</p><p>4 0.16581002 <a title="114-tfidf-4" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>Author: Simon Lacoste-julien, Fei Sha, Michael I. Jordan</p><p>Abstract: Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in ﬁnding a reduced dimensionality representation. Speciﬁcally, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classiﬁcation. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classiﬁcation task and show how our model can identify shared topics across classes as well as class-dependent topics.</p><p>5 0.13856837 <a title="114-tfidf-5" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>Author: Matthew Blaschko, Arthur Gretton</p><p>Abstract: We introduce a family of unsupervised algorithms, numerical taxonomy clustering, to simultaneously cluster data, and to learn a taxonomy that encodes the relationship between the clusters. The algorithms work by maximizing the dependence between the taxonomy and the original data. The resulting taxonomy is a more informative visualization of complex data than simple clustering; in addition, taking into account the relations between different clusters is shown to substantially improve the quality of the clustering, when compared with state-ofthe-art algorithms in the literature (both spectral clustering and a previous dependence maximization approach). We demonstrate our algorithm on image and text data. 1</p><p>6 0.13017423 <a title="114-tfidf-6" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>7 0.12684599 <a title="114-tfidf-7" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>8 0.11125603 <a title="114-tfidf-8" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>9 0.10801467 <a title="114-tfidf-9" href="./nips-2008-A_%60%60Shape_Aware%27%27_Model_for_semi-supervised_Learning_of_Objects_and_its_Context.html">6 nips-2008-A ``Shape Aware'' Model for semi-supervised Learning of Objects and its Context</a></p>
<p>10 0.1056402 <a title="114-tfidf-10" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>11 0.093482576 <a title="114-tfidf-11" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>12 0.090806313 <a title="114-tfidf-12" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>13 0.088905632 <a title="114-tfidf-13" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>14 0.083026022 <a title="114-tfidf-14" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>15 0.075656638 <a title="114-tfidf-15" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>16 0.071807459 <a title="114-tfidf-16" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>17 0.069453321 <a title="114-tfidf-17" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>18 0.06835866 <a title="114-tfidf-18" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>19 0.066817448 <a title="114-tfidf-19" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>20 0.064127132 <a title="114-tfidf-20" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.211), (1, -0.16), (2, -0.014), (3, -0.133), (4, -0.091), (5, 0.004), (6, 0.146), (7, 0.099), (8, -0.224), (9, 0.014), (10, 0.015), (11, -0.053), (12, -0.115), (13, 0.118), (14, -0.0), (15, 0.071), (16, 0.064), (17, -0.014), (18, -0.021), (19, -0.031), (20, -0.025), (21, 0.009), (22, -0.03), (23, 0.097), (24, 0.073), (25, -0.011), (26, -0.041), (27, 0.006), (28, -0.012), (29, -0.106), (30, 0.034), (31, 0.016), (32, 0.101), (33, 0.003), (34, -0.029), (35, -0.01), (36, -0.073), (37, 0.03), (38, -0.02), (39, -0.074), (40, -0.046), (41, -0.03), (42, 0.119), (43, -0.028), (44, 0.0), (45, 0.041), (46, -0.031), (47, -0.077), (48, -0.002), (49, 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93723875 <a title="114-lsi-1" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>Author: Kilian Q. Weinberger, Olivier Chapelle</p><p>Abstract: Applications of multi-class classiﬁcation, such as document categorization, often appear in cost-sensitive settings. Recent work has signiﬁcantly improved the state of the art by moving beyond “ﬂat” classiﬁcation through incorporation of class hierarchies [4]. We present a novel algorithm that goes beyond hierarchical classiﬁcation and estimates the latent semantic space that underlies the class hierarchy. In this space, each class is represented by a prototype and classiﬁcation is done with the simple nearest neighbor rule. The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. We show that our optimization is convex and can be solved efﬁciently for large data sets. Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization. 1</p><p>2 0.82120818 <a title="114-lsi-2" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>Author: Simon Lacoste-julien, Fei Sha, Michael I. Jordan</p><p>Abstract: Probabilistic topic models have become popular as methods for dimensionality reduction in collections of text documents or images. These models are usually treated as generative models and trained using maximum likelihood or Bayesian methods. In this paper, we discuss an alternative: a discriminative framework in which we assume that supervised side information is present, and in which we wish to take that side information into account in ﬁnding a reduced dimensionality representation. Speciﬁcally, we present DiscLDA, a discriminative variation on Latent Dirichlet Allocation (LDA) in which a class-dependent linear transformation is introduced on the topic mixture proportions. This parameter is estimated by maximizing the conditional likelihood. By using the transformed topic mixture proportions as a new representation of documents, we obtain a supervised dimensionality reduction algorithm that uncovers the latent structure in a document collection while preserving predictive power for the task of classiﬁcation. We compare the predictive power of the latent structure of DiscLDA with unsupervised LDA on the 20 Newsgroups document classiﬁcation task and show how our model can identify shared topics across classes as well as class-dependent topics.</p><p>3 0.77324873 <a title="114-lsi-3" href="./nips-2008-Syntactic_Topic_Models.html">229 nips-2008-Syntactic Topic Models</a></p>
<p>Author: Jordan L. Boyd-graber, David M. Blei</p><p>Abstract: We develop the syntactic topic model (STM), a nonparametric Bayesian model of parsed documents. The STM generates words that are both thematically and syntactically constrained, which combines the semantic insights of topic models with the syntactic information available from parse trees. Each word of a sentence is generated by a distribution that combines document-speciﬁc topic weights and parse-tree-speciﬁc syntactic transitions. Words are assumed to be generated in an order that respects the parse tree. We derive an approximate posterior inference method based on variational methods for hierarchical Dirichlet processes, and we report qualitative and quantitative results on both synthetic data and hand-parsed documents. 1</p><p>4 0.72948027 <a title="114-lsi-4" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>Author: Indraneel Mukherjee, David M. Blei</p><p>Abstract: Hierarchical probabilistic modeling of discrete data has emerged as a powerful tool for text analysis. Posterior inference in such models is intractable, and practitioners rely on approximate posterior inference methods such as variational inference or Gibbs sampling. There has been much research in designing better approximations, but there is yet little theoretical understanding of which of the available techniques are appropriate, and in which data analysis settings. In this paper we provide the beginnings of such understanding. We analyze the improvement that the recently proposed collapsed variational inference (CVB) provides over mean ﬁeld variational inference (VB) in latent Dirichlet allocation. We prove that the difference in the tightness of the bound on the likelihood of a document decreases as O(k − 1) + log m/m, where k is the number of topics in the model and m is the number of words in a document. As a consequence, the advantage of CVB over VB is lost for long documents but increases with the number of topics. We demonstrate empirically that the theory holds, using simulated text data and two text corpora. We provide practical guidelines for choosing an approximation. 1</p><p>5 0.71526349 <a title="114-lsi-5" href="./nips-2008-Asynchronous_Distributed_Learning_of_Topic_Models.html">28 nips-2008-Asynchronous Distributed Learning of Topic Models</a></p>
<p>Author: Padhraic Smyth, Max Welling, Arthur U. Asuncion</p><p>Abstract: Distributed learning is a problem of fundamental interest in machine learning and cognitive science. In this paper, we present asynchronous distributed learning algorithms for two well-known unsupervised learning frameworks: Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Processes (HDP). In the proposed approach, the data are distributed across P processors, and processors independently perform Gibbs sampling on their local data and communicate their information in a local asynchronous manner with other processors. We demonstrate that our asynchronous algorithms are able to learn global topic models that are statistically as accurate as those learned by the standard LDA and HDP samplers, but with signiﬁcant improvements in computation time and memory. We show speedup results on a 730-million-word text corpus using 32 processors, and we provide perplexity results for up to 1500 virtual processors. As a stepping stone in the development of asynchronous HDP, a parallel HDP sampler is also introduced. 1</p><p>6 0.62525952 <a title="114-lsi-6" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>7 0.61089361 <a title="114-lsi-7" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>8 0.52935636 <a title="114-lsi-8" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>9 0.52441144 <a title="114-lsi-9" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>10 0.45790723 <a title="114-lsi-10" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>11 0.45630768 <a title="114-lsi-11" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>12 0.4523114 <a title="114-lsi-12" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>13 0.41755432 <a title="114-lsi-13" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>14 0.40430516 <a title="114-lsi-14" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>15 0.39837646 <a title="114-lsi-15" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>16 0.3888751 <a title="114-lsi-16" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>17 0.3845377 <a title="114-lsi-17" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>18 0.38250834 <a title="114-lsi-18" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>19 0.37976867 <a title="114-lsi-19" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>20 0.37922603 <a title="114-lsi-20" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.065), (7, 0.109), (12, 0.033), (28, 0.153), (57, 0.063), (59, 0.015), (63, 0.033), (71, 0.027), (74, 0.267), (77, 0.058), (83, 0.084)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.8088271 <a title="114-lda-1" href="./nips-2008-Using_matrices_to_model_symbolic_relationship.html">248 nips-2008-Using matrices to model symbolic relationship</a></p>
<p>Author: Ilya Sutskever, Geoffrey E. Hinton</p><p>Abstract: We describe a way of learning matrix representations of objects and relationships. The goal of learning is to allow multiplication of matrices to represent symbolic relationships between objects and symbolic relationships between relationships, which is the main novelty of the method. We demonstrate that this leads to excellent generalization in two different domains: modular arithmetic and family relationships. We show that the same system can learn ﬁrst-order propositions such as (2, 5) ∈ +3 or (Christopher, Penelope) ∈ has wife, and higher-order propositions such as (3, +3) ∈ plus and (+3, −3) ∈ inverse or (has husband, has wife) ∈ higher oppsex. We further demonstrate that the system understands how higher-order propositions are related to ﬁrst-order ones by showing that it can correctly answer questions about ﬁrst-order propositions involving the relations +3 or has wife even though it has not been trained on any ﬁrst-order examples involving these relations. 1</p><p>2 0.80671865 <a title="114-lda-2" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>Author: Matt Jones, Sachiko Kinoshita, Michael C. Mozer</p><p>Abstract: In most cognitive and motor tasks, speed-accuracy tradeoffs are observed: Individuals can respond slowly and accurately, or quickly yet be prone to errors. Control mechanisms governing the initiation of behavioral responses are sensitive not only to task instructions and the stimulus being processed, but also to the recent stimulus history. When stimuli can be characterized on an easy-hard dimension (e.g., word frequency in a naming task), items preceded by easy trials are responded to more quickly, and with more errors, than items preceded by hard trials. We propose a rationally motivated mathematical model of this sequential adaptation of control, based on a diffusion model of the decision process in which difﬁculty corresponds to the drift rate for the correct response. The model assumes that responding is based on the posterior distribution over which response is correct, conditioned on the accumulated evidence. We derive this posterior as a function of the drift rate, and show that higher estimates of the drift rate lead to (normatively) faster responding. Trial-by-trial tracking of difﬁculty thus leads to sequential effects in speed and accuracy. Simulations show the model explains a variety of phenomena in human speeded decision making. We argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theories based on elaborate control structures. 1</p><p>same-paper 3 0.79721379 <a title="114-lda-3" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>Author: Kilian Q. Weinberger, Olivier Chapelle</p><p>Abstract: Applications of multi-class classiﬁcation, such as document categorization, often appear in cost-sensitive settings. Recent work has signiﬁcantly improved the state of the art by moving beyond “ﬂat” classiﬁcation through incorporation of class hierarchies [4]. We present a novel algorithm that goes beyond hierarchical classiﬁcation and estimates the latent semantic space that underlies the class hierarchy. In this space, each class is represented by a prototype and classiﬁcation is done with the simple nearest neighbor rule. The optimization of the semantic space incorporates large margin constraints that ensure that for each instance the correct class prototype is closer than any other. We show that our optimization is convex and can be solved efﬁciently for large data sets. Experiments on the OHSUMED medical journal data base yield state-of-the-art results on topic categorization. 1</p><p>4 0.64378387 <a title="114-lda-4" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>5 0.64199793 <a title="114-lda-5" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>6 0.63909817 <a title="114-lda-6" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>7 0.63853681 <a title="114-lda-7" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>8 0.6378895 <a title="114-lda-8" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>9 0.63741177 <a title="114-lda-9" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>10 0.63406128 <a title="114-lda-10" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>11 0.63153183 <a title="114-lda-11" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>12 0.63042372 <a title="114-lda-12" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>13 0.63027573 <a title="114-lda-13" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>14 0.6288563 <a title="114-lda-14" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>15 0.627473 <a title="114-lda-15" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>16 0.62655199 <a title="114-lda-16" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>17 0.62654239 <a title="114-lda-17" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>18 0.62634629 <a title="114-lda-18" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>19 0.62532467 <a title="114-lda-19" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>20 0.62523293 <a title="114-lda-20" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
