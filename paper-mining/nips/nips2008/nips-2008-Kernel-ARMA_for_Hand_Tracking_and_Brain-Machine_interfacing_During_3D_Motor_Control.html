<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-110" href="#">nips2008-110</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</h1>
<br/><p>Source: <a title="nips-2008-110-pdf" href="http://papers.nips.cc/paper/3607-kernel-arma-for-hand-tracking-and-brain-machine-interfacing-during-3d-motor-control.pdf">pdf</a></p><p>Author: Lavi Shpigelman, Hagai Lalazar, Eilon Vaadia</p><p>Abstract: Using machine learning algorithms to decode intended behavior from neural activity serves a dual purpose. First, these tools allow patients to interact with their environment through a Brain-Machine Interface (BMI). Second, analyzing the characteristics of such methods can reveal the relative signiﬁcance of various features of neural activity, task stimuli, and behavior. In this study we adapted, implemented and tested a machine learning method called Kernel Auto-Regressive Moving Average (KARMA), for the task of inferring movements from neural activity in primary motor cortex. Our version of this algorithm is used in an online learning setting and is updated after a sequence of inferred movements is completed. We ﬁrst used it to track real hand movements executed by a monkey in a standard 3D reaching task. We then applied it in a closed-loop BMI setting to infer intended movement, while the monkey’s arms were comfortably restrained, thus performing the task using the BMI alone. KARMA is a recurrent method that learns a nonlinear model of output dynamics. It uses similarity functions (termed kernels) to compare between inputs. These kernels can be structured to incorporate domain knowledge into the method. We compare KARMA to various state-of-the-art methods by evaluating tracking performance and present results from the KARMA based BMI experiments. 1</p><p>Reference: <a title="nips-2008-110-reference" href="../nips2008_reference/nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 il  Abstract Using machine learning algorithms to decode intended behavior from neural activity serves a dual purpose. [sent-9, score-0.157]
</p><p>2 In this study we adapted, implemented and tested a machine learning method called Kernel Auto-Regressive Moving Average (KARMA), for the task of inferring movements from neural activity in primary motor cortex. [sent-12, score-0.292]
</p><p>3 We ﬁrst used it to track real hand movements executed by a monkey in a standard 3D reaching task. [sent-14, score-0.351]
</p><p>4 1  Introduction  Performing a behavioral action such as picking up a sandwich and bringing it to one’s mouth is a motor control task achieved easily every day by millions of people. [sent-20, score-0.288]
</p><p>5 In the future, patients with enough cortical activity remaining may beneﬁt from Brain Machine Interfaces that will restore motor control with agility, precision, and the degrees of freedom comparable to natural movements. [sent-22, score-0.28]
</p><p>6 The BMI framework involves recording neural activity, typically using chronically implanted electrodes, which is fed in real-time to a decoding algorithm. [sent-24, score-0.178]
</p><p>7 The algorithm’s predictions can be used to artiﬁcially control an end-effector: a cursor on a screen, a prosthetic arm, a wheelchair, or the subject’s own limbs by stimulation of their muscles. [sent-26, score-0.324]
</p><p>8 Neural activity in primary motor cortex (MI) is part of this process. [sent-29, score-0.167]
</p><p>9 An early approach at decoding movement from MI activity for BMI (see [1]) was rather simplistic. [sent-30, score-0.304]
</p><p>10 This algorithm (known as the Population Vector Algorithm) is equivalent to modelling each neuron as a consine function of movement velocity. [sent-32, score-0.179]
</p><p>11 For example, MI activity has been shown to encode arm posture [3], the dynamic aspects of the movement (such as current acceleration, or interaction forces) and the interactions between neurons and their dynamics [4]. [sent-35, score-0.306]
</p><p>12 State-of-the-art movement decoding methods typically involve improved modeling of behavior, neural activity, and the relations between them. [sent-36, score-0.243]
</p><p>13 Thus, the hand movement is assumed to have roughly constant acceleration (with added Gaussian noise and, consequently, minimal jerk) and the neural activity is assumed to be a linear function of the hand state (with added Gaussian noise). [sent-38, score-0.447]
</p><p>14 Support Vector Regression (SVR) from neural activity to current hand velocity (see [7]) has the advantage of allowing for extraction of nonlinear information from neuronal interactions, but is missing a movement model. [sent-40, score-0.455]
</p><p>15 One of our previous studies ([8]) combines a linear movement model (as in Kalman ﬁltering) with SVR-based nonlinear regression from neural activity. [sent-41, score-0.242]
</p><p>16 It estimates the next system state as a function of both the time window of previous state estimates (the Auto-Regressive part) and the time window of previous observations (the Moving-Average part). [sent-44, score-0.318]
</p><p>17 We explain the available modeling options and how they can be used to either improve performance or to test ideas regarding motor control and neural encoding. [sent-50, score-0.179]
</p><p>18 Section 4 describes KARMA’s performance in tracking hand movements and compares it with other state-of-the-art methods. [sent-51, score-0.213]
</p><p>19 Section 5 presents results from our BMI experiment using KARMA and, ﬁnally, we summarize in section 6  2  Lab setup and problem setting  In our experiments, a monkey performed a visuomotor control task that involved moving a cursor on a screen from target to target in 3D virtual space. [sent-52, score-0.615]
</p><p>20 Neuronal activity from a population of single and multi-units was recorded with a chronically implanted array of 96 electrodes (Cyberkinetics, Inc. [sent-53, score-0.253]
</p><p>21 In hand-control (open-loop) mode, the monkey used its hand (continuously tracked by an optical motion capture system; Phoenix Tech. [sent-55, score-0.228]
</p><p>22 Data collected from these sessions is used here to assess algorithm performance, by using the real arm movements as the target trajectories. [sent-58, score-0.227]
</p><p>23 In hand-control, behavior is segmented into sequences of continuous recordings, separated by time periods during which the monkey’s hand is not in view of the tracking device (e. [sent-59, score-0.22]
</p><p>24 A successful trial consisted of one reaching movement that started at rest in one target and ended at rest in the next target (with required target hold periods during which the cursor must not leave the target). [sent-66, score-0.701]
</p><p>25 In case of failure, the cursor disappears for 1-2 seconds (failure period). [sent-69, score-0.248]
</p><p>26 In the BMI (closed-loop) setting, the monkey’s hands were comfortably restrained and the KARMA algorithm’s inference was used to move the cursor. [sent-71, score-0.169]
</p><p>27 Trial failures occured if the reaching movement took longer than 6 seconds or if the cursor was not kept inside the target during a 0. [sent-72, score-0.538]
</p><p>28 During trial-failure periods the inference was stopped and at the next trial the cursor reappeared where it left off. [sent-74, score-0.383]
</p><p>29 We will use xi ∈ I q to designate the neural activity (of q cortical R t units) at time bin t in behavior sequence i, which we refer to as observations. [sent-78, score-0.255]
</p><p>30 Given a window size, 2  s, xi t−s+1:t =  xi t−s+1  T  , . [sent-79, score-0.149]
</p><p>31 , xi t  T  T  ∈ I sq is an sq-long vector comprising a concatenated R  window of observations ending at time t, of trajectory i. [sent-82, score-0.26]
</p><p>32 Similarly, yt ∈ I d are used R 1:t i i to designate cursor position (d = 3). [sent-84, score-0.394]
</p><p>33 Estimated states are differentiated from true R ˆi states (or desired states, as will be explained later) by addition of a hat: yt . [sent-87, score-0.145]
</p><p>34 Furthermore (given s and ˆi r) we will use vt =  ˆi yt−r:t−1  T  xi t−s+1:t  T  T  ∈ I rd+sq to concatenate windows of estimated R  i states and of neural observations and vt to concatenate true (rather than estimated) state values. [sent-88, score-0.351]
</p><p>35 In BMI mode, since hand movements were not performed, we do not know the correct cursor movement. [sent-93, score-0.384]
</p><p>36 Instead, during learning we use the cursor movement generated in the BMI and the positions of the targets that the monkey was instructed to reach to guess a desired cursor trajectory which is used to replace the missing true trajectory as feedback. [sent-94, score-1.012]
</p><p>37 Given these model parameters t and initial state values, the rest of the state trajectory can be estimated from the observations by recursive application, replacing true state values with the estimated ones. [sent-98, score-0.181]
</p><p>38 Inference takes the form yt = j,τ αj k vt , vτ where τ j d j ατ ∈ I are learned weight vectors and vτ are examples from a training set, known as the support R ˆi ˆi set. [sent-109, score-0.168]
</p><p>39 Conceptually, KARMA inference can be viewed as yt = Wφ φ vt where, as compared i j ˆ with ARMA, vt is replaced by its feature vector, W is replaced by Wφ = j,τ αj φT (vτ ) and τ each recursive step of KARMA is linear regression in the feature space of observations + states. [sent-110, score-0.253]
</p><p>40 Beyond the necessary selection of the cursor trajectories as the states, augmenting state dimensions (whose values are known at training and inferred during model testing) can be added in order to make the model use them as explicit features. [sent-118, score-0.332]
</p><p>41 This idea was tried in our hand tracking experiments using features such as absolute velocity and current trial state (reaching target, holding at target and trial-failure time). [sent-119, score-0.283]
</p><p>42 We chose 5000 for the tracking (hand-control) experiments since in those experiments there is no real-time inference constraint and the performance improves a bit (suggesting that the 3000 size is not optimal in terms of inference quality). [sent-144, score-0.133]
</p><p>43 4  Open-loop hand tracking testing  To test various parametrization of KARMA and to compare its performance to other methods we used data from 11 hand-control recording days. [sent-147, score-0.184]
</p><p>44 Cortical units were sorted in an automated manner every day with additional experimenter tuning. [sent-150, score-0.203]
</p><p>45 Most of the different algorithms that are compared here have free hyper-parameters that need to be set (such as a Gaussian kernel width for spike rates, the maximal informative time window of neural activities, s and the c trade-off parameter). [sent-153, score-0.223]
</p><p>46 These variations consisted of all combinations of summing or multiplying Gaussian or linear kernels for the spike rates and movement states. [sent-160, score-0.288]
</p><p>47 The next best result was achieved by summing a Gaussian spike rate kernel and a linear movement kernel (which we will call lin-y-KARMA). [sent-163, score-0.281]
</p><p>48 The main performance measure that we use here is the (Pearson) correlation coefﬁcient (CC) between true and estimated values of positions (in each of the 3 movement dimensions). [sent-166, score-0.207]
</p><p>49 To gauge changes in prediction quality over time we use CC in a running window of sequences (window size is chosen so as to decrease the noise in the CC estimate). [sent-167, score-0.172]
</p><p>50 To illustrate KARMA’s performance we provide a movie (see video 1 in supplementary material) showing the true hand position (in black) and KARMA’s tracking estimate (in blue) during a continuous 150 second sequence of target-to target reach movements. [sent-169, score-0.204]
</p><p>51 The initial position of the cursor is not given to the algorithm. [sent-171, score-0.28]
</p><p>52 The CC in the time window of the previous 40 sequences (0. [sent-173, score-0.151]
</p><p>53 A shows tracking quality in terms of a running window of CC’s over time. [sent-181, score-0.166]
</p><p>54 C  A earliest delay train  delays test  D  positions  B  Figure 1: KARMA performance: (A) Correlation coefﬁcients in a running window of 20 sequences (or less at the session start) for a whole 95 minute session (mean CC window size is 9. [sent-185, score-0.551]
</p><p>55 (C) Effect of loosing recording electrodes: tracking was performed over a full recording session using randomly chosen subsets of electrodes. [sent-189, score-0.283]
</p><p>56 CC’s were calculated per run over the last two thirds of the session (to avoid effects of initial performance transients) and averaged over the 3 movement dimensions. [sent-191, score-0.267]
</p><p>57 Figure (A) shows the time window that corresponded to opening a maximal time difference of 60 minutes between the last inferred sequence (at minute 90) and the last learned sequence (at minute 30). [sent-194, score-0.33]
</p><p>58 CC’s for the test windows (averaged over movement dimensions) are shown as a function of delay time for the two days. [sent-195, score-0.29]
</p><p>59 6%  SVR  Above diagonal is better  Figure 2: Algorithm comparisons: 10 hand-movement sessions were each divided into 3 equally long blocks of 25-35 minutes (the last few minutes were discarded since during this time the monkey often stopped paying attention to the task) to create 30 data-sets. [sent-197, score-0.395]
</p><p>60 A Kalman Filter was also implemented so as to allow for a window of observations as input and a window of movement positions as the state (this version was previously shown to outperform the standard Kalman Filter which has r = s = 1). [sent-201, score-0.444]
</p><p>61 Results are shown as scatter plots of CC values (30 data-sets and 3 movement dimensions produce 90 points per scatter plot). [sent-203, score-0.255]
</p><p>62 Each point compares KARMA to another algorithm in a speciﬁc data-set and movement dimension pair. [sent-204, score-0.179]
</p><p>63 The movement reconstruction on the right (horizontal position only) shows KARMA vs. [sent-209, score-0.211]
</p><p>64 Let’s also assume that with 50 repetitions, minimal values roughly represent a worst case scenario in terms of mishaps that do not involve movement of the whole array. [sent-216, score-0.202]
</p><p>65 This may be relevant in situations involving implanted chips that extract and wirelessly transmit neural activity and may have constraints in terms of energy expenditure or bandwidth. [sent-220, score-0.167]
</p><p>66 In order to check the importance of adapting to changes in the recorded neural activity we ran an experiment in which variable delays were opened between the inference times and the latest available data for learning. [sent-225, score-0.212]
</p><p>67 D shows a degradation in performance during the test periods as the delay grows for two recording sessions. [sent-230, score-0.145]
</p><p>68 One is changes in the neural activity within the brain. [sent-233, score-0.156]
</p><p>69 The other is changes in the process that extracts the neural activity in the BMI (e. [sent-234, score-0.156]
</p><p>70 The subject might be able to effortlessly modulate his neural activity and keep it in good ﬁt with the algorithm. [sent-239, score-0.135]
</p><p>71 It is clear that KARMA performs much better than ARMA and the Kalman Filter, suggesting that a nonlinear interpretation of neural activity is helpful. [sent-242, score-0.161]
</p><p>72 Looking at the movement reconstruction comparison it seems that SVR has a good average estimate of the current position, 6  ˆi however, missing a movement model (SVR has the form yt = Wφ φ(xi t−s:t ) ) it ﬂuctuates rapidly around the true value. [sent-244, score-0.449]
</p><p>73 Lin-y-KARMA uses a linear movement model (and ˆi has the form: yt = Aˆ t−r:t−1 + Wφ φ(xi yi t−s:t )). [sent-246, score-0.295]
</p><p>74 Having a nonlinear movement model means that different areas of the state-space get treated in locally relevant fashion. [sent-248, score-0.205]
</p><p>75 100  hand moving  mixed mode  pure BMI control  both hands restrained  Success rate  80 60 mixing factor: 0−>35%  40  learning algorithm frozen  mixing factor: 0−>50%  days start with full amplitude targets  20 Day 1  Day 2  Day 4  Day 3  144 min. [sent-254, score-0.394]
</p><p>76 On day 4 both hands were comfortably restrained for the ﬁrst time and though performance levels dropped, the monkey did not attempt to move its hands. [sent-263, score-0.518]
</p><p>77 On day 5 an attempt to freeze the model was made. [sent-264, score-0.224]
</p><p>78 When performance dropped and the monkey became agitated we restarted the BMI from scratch and performance improved rapidly. [sent-265, score-0.226]
</p><p>79 On days 7 and 8 a BMI block was interleaved with hand control blocks. [sent-268, score-0.155]
</p><p>80 The full three blocks of day 8 are shown in the bottom right graph. [sent-270, score-0.175]
</p><p>81 Bottom left graph shows a recording session during which the model was frozen. [sent-271, score-0.147]
</p><p>82 5  BMI experiment  The BMI experiment was conducted after approximately four months of neural recordings during which the monkey learned and performed the hand-control task. [sent-272, score-0.238]
</p><p>83 5 days consisted of work in a mixed mode, during which control of the cursor was a linear combination of the hand position and KARMA’s predictions. [sent-276, score-0.458]
</p><p>84 We did this in order to see if the monkey would accept a noisier control scheme than it was used to. [sent-277, score-0.224]
</p><p>85 5 days the cursor was fully controlled by KARMA, but the monkey kept moving as if it was doing hand-control. [sent-279, score-0.544]
</p><p>86 On day 4 the monkey’s free hand was comfortably restrained. [sent-283, score-0.272]
</p><p>87 Despite our concerns that the monkey would stop performing it seemed impervious to this change, except that control over the cursor became more difﬁcult. [sent-284, score-0.493]
</p><p>88 On days 5 and 6 we made some tweaks to the algorithm (tried to freeze learning and change the way in which correct movement trajectories are generated) and the 7  task (tried to decrease target size and the distance between targets) which had some effects of task difﬁculty and on success rates. [sent-285, score-0.398]
</p><p>89 We repeated the freezing of model learning on two days (one of these session appears in ﬁgure 3). [sent-289, score-0.151]
</p><p>90 In all cases where we froze the model, we noticed that the monkey starts experiencing difﬁculty in controlling the cursor after a period of 10-15 minutes and stopped working completely when this happened. [sent-290, score-0.532]
</p><p>91 One possible explanation for the deterioration is that because our monkey is trained in using an adaptive model it does not have experience in conforming to such a model’s constraints. [sent-292, score-0.18]
</p><p>92 As mentioned in section 2, in BMI mode no hand movements are performed and therefore model learning is based on our guess of what is the desired cursor trajectory (the monkey’s emphintended cursor movement). [sent-294, score-0.722]
</p><p>93 We chose to design the desired trajectory as a time varying linear combination of t t i ˜ ˆi the cursor trajectory that the monkey saw and the target location: yt = 1 − tf yt + tf yi where i  i  ˜ yi is the target location on trial i. [sent-295, score-0.936]
</p><p>94 Note that this trajectory starts at the observed cursor location at the trial start and ends at the target location (regardless of where the cursor actually was at the end of the trial). [sent-296, score-0.677]
</p><p>95 We showed in open-loop hand tracking experiments that the incorporation of a nonlinear movement model, interpreting the neural activity as a whole (rather than as the sum of contributions made by single neurons) and allowing the model to adapt to changes, results in better predictions. [sent-299, score-0.488]
</p><p>96 Muscle and movement representations in the primary motor cortex. [sent-330, score-0.248]
</p><p>97 Dynamics of neuronal interactions in monkey cortex in relation to behavioral events. [sent-340, score-0.215]
</p><p>98 Bayesian population coding of motor cortical activity using a kalman ﬁlter. [sent-350, score-0.297]
</p><p>99 Spikernels: Predicting hand movements by embedding population spike rates in inner-product spaces. [sent-366, score-0.208]
</p><p>100 A temporal kernel-based model for tracking hand movements from neural activities. [sent-374, score-0.25]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('bmi', 0.576), ('karma', 0.464), ('cursor', 0.248), ('cc', 0.182), ('monkey', 0.18), ('movement', 0.179), ('day', 0.175), ('arma', 0.16), ('svr', 0.124), ('activity', 0.098), ('yt', 0.091), ('window', 0.089), ('movements', 0.088), ('session', 0.088), ('tracking', 0.077), ('electrodes', 0.076), ('motor', 0.069), ('sessions', 0.063), ('days', 0.063), ('kalman', 0.061), ('recording', 0.059), ('restrained', 0.056), ('vt', 0.056), ('minute', 0.053), ('delay', 0.053), ('sq', 0.049), ('comfortably', 0.049), ('freeze', 0.049), ('minutes', 0.049), ('hand', 0.048), ('trajectory', 0.048), ('spike', 0.048), ('target', 0.047), ('cortical', 0.045), ('control', 0.044), ('mode', 0.042), ('trial', 0.042), ('online', 0.042), ('sequences', 0.04), ('filter', 0.038), ('kernels', 0.038), ('neural', 0.037), ('state', 0.037), ('windows', 0.036), ('hands', 0.036), ('neuronal', 0.035), ('success', 0.035), ('reaching', 0.035), ('targets', 0.033), ('periods', 0.033), ('velocity', 0.032), ('position', 0.032), ('implanted', 0.032), ('stopped', 0.032), ('prosthetic', 0.032), ('xi', 0.03), ('cache', 0.03), ('kept', 0.029), ('options', 0.029), ('arm', 0.029), ('latest', 0.028), ('units', 0.028), ('inference', 0.028), ('hagai', 0.028), ('mimo', 0.028), ('paz', 0.028), ('shpigelman', 0.028), ('positions', 0.028), ('kernel', 0.027), ('scatter', 0.027), ('decoding', 0.027), ('states', 0.027), ('nonlinear', 0.026), ('trajectories', 0.025), ('yi', 0.025), ('mi', 0.025), ('concatenate', 0.025), ('discontinued', 0.025), ('scratch', 0.025), ('vaadia', 0.025), ('visuomotor', 0.025), ('moving', 0.024), ('population', 0.024), ('mixing', 0.024), ('patients', 0.024), ('repetitions', 0.024), ('consisted', 0.023), ('period', 0.023), ('whole', 0.023), ('chronically', 0.023), ('designate', 0.023), ('location', 0.022), ('dimensions', 0.022), ('intended', 0.022), ('time', 0.022), ('observations', 0.022), ('changes', 0.021), ('learned', 0.021), ('became', 0.021), ('opening', 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="110-tfidf-1" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>Author: Lavi Shpigelman, Hagai Lalazar, Eilon Vaadia</p><p>Abstract: Using machine learning algorithms to decode intended behavior from neural activity serves a dual purpose. First, these tools allow patients to interact with their environment through a Brain-Machine Interface (BMI). Second, analyzing the characteristics of such methods can reveal the relative signiﬁcance of various features of neural activity, task stimuli, and behavior. In this study we adapted, implemented and tested a machine learning method called Kernel Auto-Regressive Moving Average (KARMA), for the task of inferring movements from neural activity in primary motor cortex. Our version of this algorithm is used in an online learning setting and is updated after a sequence of inferred movements is completed. We ﬁrst used it to track real hand movements executed by a monkey in a standard 3D reaching task. We then applied it in a closed-loop BMI setting to infer intended movement, while the monkey’s arms were comfortably restrained, thus performing the task using the BMI alone. KARMA is a recurrent method that learns a nonlinear model of output dynamics. It uses similarity functions (termed kernels) to compare between inputs. These kernels can be structured to incorporate domain knowledge into the method. We compare KARMA to various state-of-the-art methods by evaluating tracking performance and present results from the KARMA based BMI experiments. 1</p><p>2 0.10150441 <a title="110-tfidf-2" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>Author: Adrian Haith, Carl P. Jackson, R. C. Miall, Sethu Vijayakumar</p><p>Abstract: Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a uniﬁed model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereﬀects. This uniﬁed model also makes the surprising prediction that force ﬁeld adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We conﬁrm this prediction with an experiment. 1</p><p>3 0.090486482 <a title="110-tfidf-3" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>Author: Silvia Chiappa, Jens Kober, Jan R. Peters</p><p>Abstract: Motor primitives or motion templates have become an important concept for both modeling human motor control as well as generating robot behaviors using imitation learning. Recent impressive results range from humanoid robot movement generation to timing models of human motions. The automatic generation of skill libraries containing multiple motion templates is an important step in robot learning. Such a skill learning system needs to cluster similar movements together and represent each resulting motion template as a generative model which is subsequently used for the execution of the behavior by a robot system. In this paper, we show how human trajectories captured as multi-dimensional time-series can be clustered using Bayesian mixtures of linear Gaussian state-space models based on the similarity of their dynamics. The appropriate number of templates is automatically determined by enforcing a parsimonious parametrization. As the resulting model is intractable, we introduce a novel approximation method based on variational Bayes, which is especially designed to enable the use of efﬁcient inference algorithms. On recorded human Balero movements, this method is not only capable of ﬁnding reasonable motion templates but also yields a generative model which works well in the execution of this complex task on a simulated anthropomorphic SARCOS arm.</p><p>4 0.07581269 <a title="110-tfidf-4" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>Author: Christoph Kolodziejski, Bernd Porr, Minija Tamosiunaite, Florentin Wörgötter</p><p>Abstract: In this theoretical contribution we provide mathematical proof that two of the most important classes of network learning - correlation-based differential Hebbian learning and reward-based temporal difference learning - are asymptotically equivalent when timing the learning with a local modulatory signal. This opens the opportunity to consistently reformulate most of the abstract reinforcement learning framework from a correlation based perspective that is more closely related to the biophysics of neurons. 1</p><p>5 0.069869846 <a title="110-tfidf-5" href="./nips-2008-Understanding_Brain_Connectivity_Patterns_during_Motor_Imagery_for_Brain-Computer_Interfacing.html">243 nips-2008-Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing</a></p>
<p>Author: Moritz Grosse-wentrup</p><p>Abstract: EEG connectivity measures could provide a new type of feature space for inferring a subject’s intention in Brain-Computer Interfaces (BCIs). However, very little is known on EEG connectivity patterns for BCIs. In this study, EEG connectivity during motor imagery (MI) of the left and right is investigated in a broad frequency range across the whole scalp by combining Beamforming with Transfer Entropy and taking into account possible volume conduction effects. Observed connectivity patterns indicate that modulation intentionally induced by MI is strongest in the γ-band, i.e., above 35 Hz. Furthermore, modulation between MI and rest is found to be more pronounced than between MI of different hands. This is in contrast to results on MI obtained with bandpower features, and might provide an explanation for the so far only moderate success of connectivity features in BCIs. It is concluded that future studies on connectivity based BCIs should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions. 1</p><p>6 0.067536585 <a title="110-tfidf-6" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>7 0.067108296 <a title="110-tfidf-7" href="./nips-2008-Playing_Pinball_with_non-invasive_BCI.html">180 nips-2008-Playing Pinball with non-invasive BCI</a></p>
<p>8 0.065942734 <a title="110-tfidf-8" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>9 0.062408306 <a title="110-tfidf-9" href="./nips-2008-Spike_Feature_Extraction_Using_Informative_Samples.html">220 nips-2008-Spike Feature Extraction Using Informative Samples</a></p>
<p>10 0.061734211 <a title="110-tfidf-10" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>11 0.060158629 <a title="110-tfidf-11" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>12 0.059882499 <a title="110-tfidf-12" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>13 0.058450315 <a title="110-tfidf-13" href="./nips-2008-Policy_Search_for_Motor_Primitives_in_Robotics.html">181 nips-2008-Policy Search for Motor Primitives in Robotics</a></p>
<p>14 0.057557065 <a title="110-tfidf-14" href="./nips-2008-Dependent_Dirichlet_Process_Spike_Sorting.html">59 nips-2008-Dependent Dirichlet Process Spike Sorting</a></p>
<p>15 0.052774515 <a title="110-tfidf-15" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>16 0.050917033 <a title="110-tfidf-16" href="./nips-2008-Translated_Learning%3A_Transfer_Learning_across_Different_Feature_Spaces.html">242 nips-2008-Translated Learning: Transfer Learning across Different Feature Spaces</a></p>
<p>17 0.05047664 <a title="110-tfidf-17" href="./nips-2008-Particle_Filter-based_Policy_Gradient_in_POMDPs.html">177 nips-2008-Particle Filter-based Policy Gradient in POMDPs</a></p>
<p>18 0.047555588 <a title="110-tfidf-18" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<p>19 0.047428597 <a title="110-tfidf-19" href="./nips-2008-Online_Metric_Learning_and_Fast_Similarity_Search.html">168 nips-2008-Online Metric Learning and Fast Similarity Search</a></p>
<p>20 0.04558605 <a title="110-tfidf-20" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.15), (1, 0.087), (2, 0.082), (3, 0.058), (4, -0.031), (5, 0.045), (6, -0.01), (7, 0.03), (8, 0.081), (9, -0.002), (10, 0.046), (11, 0.014), (12, -0.046), (13, 0.058), (14, 0.039), (15, 0.038), (16, -0.075), (17, 0.004), (18, -0.085), (19, 0.063), (20, 0.071), (21, -0.006), (22, -0.044), (23, 0.043), (24, 0.057), (25, 0.036), (26, 0.009), (27, 0.059), (28, -0.006), (29, -0.058), (30, 0.022), (31, 0.037), (32, 0.066), (33, -0.063), (34, -0.043), (35, 0.08), (36, 0.097), (37, -0.007), (38, -0.066), (39, -0.015), (40, 0.052), (41, 0.095), (42, -0.029), (43, -0.01), (44, -0.118), (45, 0.008), (46, -0.04), (47, -0.005), (48, -0.094), (49, 0.109)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.90534186 <a title="110-lsi-1" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>Author: Lavi Shpigelman, Hagai Lalazar, Eilon Vaadia</p><p>Abstract: Using machine learning algorithms to decode intended behavior from neural activity serves a dual purpose. First, these tools allow patients to interact with their environment through a Brain-Machine Interface (BMI). Second, analyzing the characteristics of such methods can reveal the relative signiﬁcance of various features of neural activity, task stimuli, and behavior. In this study we adapted, implemented and tested a machine learning method called Kernel Auto-Regressive Moving Average (KARMA), for the task of inferring movements from neural activity in primary motor cortex. Our version of this algorithm is used in an online learning setting and is updated after a sequence of inferred movements is completed. We ﬁrst used it to track real hand movements executed by a monkey in a standard 3D reaching task. We then applied it in a closed-loop BMI setting to infer intended movement, while the monkey’s arms were comfortably restrained, thus performing the task using the BMI alone. KARMA is a recurrent method that learns a nonlinear model of output dynamics. It uses similarity functions (termed kernels) to compare between inputs. These kernels can be structured to incorporate domain knowledge into the method. We compare KARMA to various state-of-the-art methods by evaluating tracking performance and present results from the KARMA based BMI experiments. 1</p><p>2 0.5854522 <a title="110-lsi-2" href="./nips-2008-Understanding_Brain_Connectivity_Patterns_during_Motor_Imagery_for_Brain-Computer_Interfacing.html">243 nips-2008-Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing</a></p>
<p>Author: Moritz Grosse-wentrup</p><p>Abstract: EEG connectivity measures could provide a new type of feature space for inferring a subject’s intention in Brain-Computer Interfaces (BCIs). However, very little is known on EEG connectivity patterns for BCIs. In this study, EEG connectivity during motor imagery (MI) of the left and right is investigated in a broad frequency range across the whole scalp by combining Beamforming with Transfer Entropy and taking into account possible volume conduction effects. Observed connectivity patterns indicate that modulation intentionally induced by MI is strongest in the γ-band, i.e., above 35 Hz. Furthermore, modulation between MI and rest is found to be more pronounced than between MI of different hands. This is in contrast to results on MI obtained with bandpower features, and might provide an explanation for the so far only moderate success of connectivity features in BCIs. It is concluded that future studies on connectivity based BCIs should focus on high frequency bands and consider experimental paradigms that maximally vary cognitive demands between conditions. 1</p><p>3 0.58245569 <a title="110-lsi-3" href="./nips-2008-Playing_Pinball_with_non-invasive_BCI.html">180 nips-2008-Playing Pinball with non-invasive BCI</a></p>
<p>Author: Matthias Krauledat, Konrad Grzeska, Max Sagebaum, Benjamin Blankertz, Carmen Vidaurre, Klaus-Robert Müller, Michael Schröder</p><p>Abstract: Compared to invasive Brain-Computer Interfaces (BCI), non-invasive BCI systems based on Electroencephalogram (EEG) signals have not been applied successfully for precisely timed control tasks. In the present study, however, we demonstrate and report on the interaction of subjects with a real device: a pinball machine. Results of this study clearly show that fast and well-timed control well beyond chance level is possible, even though the environment is extremely rich and requires precisely timed and complex predictive behavior. Using machine learning methods for mental state decoding, BCI-based pinball control is possible within the ﬁrst session without the necessity to employ lengthy subject training. The current study shows clearly that very compelling control with excellent timing and dynamics is possible for a non-invasive BCI. 1</p><p>4 0.52730691 <a title="110-lsi-4" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>Author: Byron M. Yu, John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, Maneesh Sahani</p><p>Abstract: We consider the problem of extracting smooth, low-dimensional neural trajectories that summarize the activity recorded simultaneously from tens to hundreds of neurons on individual experimental trials. Current methods for extracting neural trajectories involve a two-stage process: the data are ﬁrst “denoised” by smoothing over time, then a static dimensionality reduction technique is applied. We ﬁrst describe extensions of the two-stage methods that allow the degree of smoothing to be chosen in a principled way, and account for spiking variability that may vary both across neurons and across time. We then present a novel method for extracting neural trajectories, Gaussian-process factor analysis (GPFA), which uniﬁes the smoothing and dimensionality reduction operations in a common probabilistic framework. We applied these methods to the activity of 61 neurons recorded simultaneously in macaque premotor and motor cortices during reach planning and execution. By adopting a goodness-of-ﬁt metric that measures how well the activity of each neuron can be predicted by all other recorded neurons, we found that GPFA provided a better characterization of the population activity than the two-stage methods. 1</p><p>5 0.51834476 <a title="110-lsi-5" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>Author: Silvia Chiappa, Jens Kober, Jan R. Peters</p><p>Abstract: Motor primitives or motion templates have become an important concept for both modeling human motor control as well as generating robot behaviors using imitation learning. Recent impressive results range from humanoid robot movement generation to timing models of human motions. The automatic generation of skill libraries containing multiple motion templates is an important step in robot learning. Such a skill learning system needs to cluster similar movements together and represent each resulting motion template as a generative model which is subsequently used for the execution of the behavior by a robot system. In this paper, we show how human trajectories captured as multi-dimensional time-series can be clustered using Bayesian mixtures of linear Gaussian state-space models based on the similarity of their dynamics. The appropriate number of templates is automatically determined by enforcing a parsimonious parametrization. As the resulting model is intractable, we introduce a novel approximation method based on variational Bayes, which is especially designed to enable the use of efﬁcient inference algorithms. On recorded human Balero movements, this method is not only capable of ﬁnding reasonable motion templates but also yields a generative model which works well in the execution of this complex task on a simulated anthropomorphic SARCOS arm.</p><p>6 0.51043683 <a title="110-lsi-6" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>7 0.47552791 <a title="110-lsi-7" href="./nips-2008-Local_Gaussian_Process_Regression_for_Real_Time_Online_Model_Learning.html">125 nips-2008-Local Gaussian Process Regression for Real Time Online Model Learning</a></p>
<p>8 0.42624477 <a title="110-lsi-8" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>9 0.4252212 <a title="110-lsi-9" href="./nips-2008-Online_Models_for_Content_Optimization.html">169 nips-2008-Online Models for Content Optimization</a></p>
<p>10 0.40406141 <a title="110-lsi-10" href="./nips-2008-Online_Metric_Learning_and_Fast_Similarity_Search.html">168 nips-2008-Online Metric Learning and Fast Similarity Search</a></p>
<p>11 0.3902548 <a title="110-lsi-11" href="./nips-2008-Scalable_Algorithms_for_String_Kernels_with_Inexact_Matching.html">203 nips-2008-Scalable Algorithms for String Kernels with Inexact Matching</a></p>
<p>12 0.3868736 <a title="110-lsi-12" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>13 0.38186827 <a title="110-lsi-13" href="./nips-2008-Adaptive_Template_Matching_with_Shift-Invariant_Semi-NMF.html">16 nips-2008-Adaptive Template Matching with Shift-Invariant Semi-NMF</a></p>
<p>14 0.37734801 <a title="110-lsi-14" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>15 0.37542909 <a title="110-lsi-15" href="./nips-2008-Extracting_State_Transition_Dynamics_from_Multiple_Spike_Trains_with_Correlated_Poisson_HMM.html">81 nips-2008-Extracting State Transition Dynamics from Multiple Spike Trains with Correlated Poisson HMM</a></p>
<p>16 0.36658427 <a title="110-lsi-16" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>17 0.36296549 <a title="110-lsi-17" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>18 0.36202207 <a title="110-lsi-18" href="./nips-2008-Hierarchical_Fisher_Kernels_for_Longitudinal_Data.html">97 nips-2008-Hierarchical Fisher Kernels for Longitudinal Data</a></p>
<p>19 0.35702333 <a title="110-lsi-19" href="./nips-2008-The_Recurrent_Temporal_Restricted_Boltzmann_Machine.html">237 nips-2008-The Recurrent Temporal Restricted Boltzmann Machine</a></p>
<p>20 0.35030159 <a title="110-lsi-20" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.048), (7, 0.065), (12, 0.039), (15, 0.012), (28, 0.566), (57, 0.034), (59, 0.017), (63, 0.024), (71, 0.016), (77, 0.034), (78, 0.011), (81, 0.015), (83, 0.038)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99728543 <a title="110-lda-1" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>Author: Angela J. Yu, Jonathan D. Cohen</p><p>Abstract: In a variety of behavioral tasks, subjects exhibit an automatic and apparently suboptimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no real predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reﬂect the inadvertent engagement of mechanisms critical for adapting to a changing environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential ﬁltering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential ﬁlter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that parameter-tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities. 1</p><p>2 0.9972651 <a title="110-lda-2" href="./nips-2008-Reconciling_Real_Scores_with_Binary_Comparisons%3A_A_New_Logistic_Based_Model_for_Ranking.html">190 nips-2008-Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking</a></p>
<p>Author: Nir Ailon</p><p>Abstract: The problem of ranking arises ubiquitously in almost every aspect of life, and in particular in Machine Learning/Information Retrieval. A statistical model for ranking predicts how humans rank subsets V of some universe U . In this work we deﬁne a statistical model for ranking that satisﬁes certain desirable properties. The model automatically gives rise to a logistic regression based approach to learning how to rank, for which the score and comparison based approaches are dual views. This offers a new generative approach to ranking which can be used for IR. There are two main contexts for this work. The ﬁrst is the theory of econometrics and study of statistical models explaining human choice of alternatives. In this context, we will compare our model with other well known models. The second context is the problem of ranking in machine learning, usually arising in the context of information retrieval. Here, much work has been done in the discriminative setting, where different heuristics are used to deﬁne ranking risk functions. Our model is built rigorously and axiomatically based on very simple desirable properties deﬁned locally for comparisons, and automatically implies the existence of a global score function serving as a natural model parameter which can be efﬁciently ﬁtted to pairwise comparison judgment data by solving a convex optimization problem. 1</p><p>3 0.99720967 <a title="110-lda-3" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>Author: Gal Elidan, Stephen Gould</p><p>Abstract: With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufﬁciently expressive for generalization while also allowing for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overﬁtting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modiﬁcations and that is polynomial in the size of the graph and the treewidth bound. At the heart of our method is a triangulated graph that we dynamically update in a way that facilitates the addition of chain structures that increase the bound on the model’s treewidth by at most one. We demonstrate the effectiveness of our “treewidth-friendly” method on several real-life datasets. Importantly, we also show that by using global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth. 1</p><p>4 0.99693894 <a title="110-lda-4" href="./nips-2008-Empirical_performance_maximization_for_linear_rank_statistics.html">72 nips-2008-Empirical performance maximization for linear rank statistics</a></p>
<p>Author: Stéphan J. Clémençcon, Nicolas Vayatis</p><p>Abstract: The ROC curve is known to be the golden standard for measuring performance of a test/scoring statistic regarding its capacity of discrimination between two populations in a wide variety of applications, ranging from anomaly detection in signal processing to information retrieval, through medical diagnosis. Most practical performance measures used in scoring applications such as the AUC, the local AUC, the p-norm push, the DCG and others, can be seen as summaries of the ROC curve. This paper highlights the fact that many of these empirical criteria can be expressed as (conditional) linear rank statistics. We investigate the properties of empirical maximizers of such performance criteria and provide preliminary results for the concentration properties of a novel class of random variables that we will call a linear rank process. 1</p><p>same-paper 5 0.99564868 <a title="110-lda-5" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>Author: Lavi Shpigelman, Hagai Lalazar, Eilon Vaadia</p><p>Abstract: Using machine learning algorithms to decode intended behavior from neural activity serves a dual purpose. First, these tools allow patients to interact with their environment through a Brain-Machine Interface (BMI). Second, analyzing the characteristics of such methods can reveal the relative signiﬁcance of various features of neural activity, task stimuli, and behavior. In this study we adapted, implemented and tested a machine learning method called Kernel Auto-Regressive Moving Average (KARMA), for the task of inferring movements from neural activity in primary motor cortex. Our version of this algorithm is used in an online learning setting and is updated after a sequence of inferred movements is completed. We ﬁrst used it to track real hand movements executed by a monkey in a standard 3D reaching task. We then applied it in a closed-loop BMI setting to infer intended movement, while the monkey’s arms were comfortably restrained, thus performing the task using the BMI alone. KARMA is a recurrent method that learns a nonlinear model of output dynamics. It uses similarity functions (termed kernels) to compare between inputs. These kernels can be structured to incorporate domain knowledge into the method. We compare KARMA to various state-of-the-art methods by evaluating tracking performance and present results from the KARMA based BMI experiments. 1</p><p>6 0.99478775 <a title="110-lda-6" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>7 0.99404263 <a title="110-lda-7" href="./nips-2008-Overlaying_classifiers%3A_a_practical_approach_for_optimal_ranking.html">174 nips-2008-Overlaying classifiers: a practical approach for optimal ranking</a></p>
<p>8 0.99165291 <a title="110-lda-8" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>9 0.98887026 <a title="110-lda-9" href="./nips-2008-Stress%2C_noradrenaline%2C_and_realistic_prediction_of_mouse_behaviour_using_reinforcement_learning.html">222 nips-2008-Stress, noradrenaline, and realistic prediction of mouse behaviour using reinforcement learning</a></p>
<p>10 0.97796607 <a title="110-lda-10" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>11 0.97659695 <a title="110-lda-11" href="./nips-2008-On_Bootstrapping_the_ROC_Curve.html">159 nips-2008-On Bootstrapping the ROC Curve</a></p>
<p>12 0.96392238 <a title="110-lda-12" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>13 0.95679617 <a title="110-lda-13" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>14 0.95625597 <a title="110-lda-14" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>15 0.95372927 <a title="110-lda-15" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>16 0.95277584 <a title="110-lda-16" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>17 0.95276254 <a title="110-lda-17" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>18 0.94999951 <a title="110-lda-18" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>19 0.94863862 <a title="110-lda-19" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>20 0.94529814 <a title="110-lda-20" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
