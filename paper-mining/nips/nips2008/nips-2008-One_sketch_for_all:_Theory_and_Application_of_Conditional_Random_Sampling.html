<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-167" href="#">nips2008-167</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</h1>
<br/><p>Source: <a title="nips-2008-167-pdf" href="http://papers.nips.cc/paper/3572-one-sketch-for-all-theory-and-application-of-conditional-random-sampling.pdf">pdf</a></p><p>Author: Ping Li, Kenneth W. Church, Trevor J. Hastie</p><p>Abstract: Conditional Random Sampling (CRS) was originally proposed for efﬁciently computing pairwise (l2 , l1 ) distances, in static, large-scale, and sparse data. This study modiﬁes the original CRS and extends CRS to handle dynamic or streaming data, which much better reﬂect the real-world situation than assuming static data. Compared with many other sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a signiﬁcant advantage in that it is “one-sketch-for-all.” In particular, we demonstrate the effectiveness of CRS in efﬁciently computing the Hamming norm, the Hamming distance, the lp distance, and the χ2 distance. A generic estimator and an approximate variance formula are also provided, for approximating any type of distances. We recommend CRS as a promising tool for building highly scalable systems, in machine learning, data mining, recommender systems, and information retrieval. 1</p><p>Reference: <a title="nips-2008-167-reference" href="../nips2008_reference/nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract Conditional Random Sampling (CRS) was originally proposed for efﬁciently computing pairwise (l2 , l1 ) distances, in static, large-scale, and sparse data. [sent-8, score-0.061]
</p><p>2 This study modiﬁes the original CRS and extends CRS to handle dynamic or streaming data, which much better reﬂect the real-world situation than assuming static data. [sent-9, score-0.137]
</p><p>3 Compared with many other sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a signiﬁcant advantage in that it is “one-sketch-for-all. [sent-10, score-0.114]
</p><p>4 ” In particular, we demonstrate the effectiveness of CRS in efﬁciently computing the Hamming norm, the Hamming distance, the lp distance, and the χ2 distance. [sent-11, score-0.065]
</p><p>5 A generic estimator and an approximate variance formula are also provided, for approximating any type of distances. [sent-12, score-0.193]
</p><p>6 1  Introduction  Learning algorithms often assume a data matrix A ∈ Rn×D with n observations and D attributes and operate on the data matrix A through pairwise distances. [sent-14, score-0.098]
</p><p>7 The task of computing and maintaining distances becomes non-trivial, when the data (both n and D) are large and possibly dynamic. [sent-15, score-0.105]
</p><p>8 1  Pairwise Distances Used in Machine Learning  The lp distance and χ2 distance are both popular. [sent-35, score-0.132]
</p><p>9 The lp distance (raised to the pth power), and the χ2 distance, are, respectively, D  D  |u1,i − u2,i |p ,  dp (u1 , u2 ) =  dχ2 (u1 , u2 ) =  i=1  i=1  (u1,i − u2,i )2 , u1,i + u2,i  (  0 = 0). [sent-37, score-0.088]
</p><p>10 For applications in text and images using SVM, empirical studies have demonstrated the superiority of Helbertian metrics over lp distances[3, 7, 9]. [sent-42, score-0.07]
</p><p>11 More generally, we are interested in any linear summary statistics which can be written in the form: D  g(u1,i , u2,i ),  dg (u1 , u2 ) =  (1)  i=1  for any generic function g. [sent-43, score-0.131]
</p><p>12 For popular kernel SVM solvers including the SMO algorithm[16], storing and computing kernels is the major bottleneck[2], because computing kernels is expensive, and more seriously, storing the full kernel matrix in memory is infeasible when the number of observations n > 105 . [sent-47, score-0.161]
</p><p>13 With high-dimensional data, however, either computing distances ondemand becomes too slow or the data matrix A ∈ Rn×D itself may not ﬁt in memory. [sent-52, score-0.128]
</p><p>14 In addition to computing and storing distances, another general issue is that, for many real-world applications, entries of the data matrix may be frequently updated, for example, data streams[15]. [sent-55, score-0.128]
</p><p>15 Since streaming data are often not stored (even on disks), computing and updating distances becomes challenging. [sent-59, score-0.161]
</p><p>16 3  Contributions and Paper Organization  Conditional Random Sampling (CRS)[12, 13] was originally proposed for efﬁciently computing pairwise (l2 and l1 ) distances, in large-scale static data. [sent-61, score-0.082]
</p><p>17 For example, entries of a matrix may vary over time, or the data matrix may not be stored at all. [sent-64, score-0.109]
</p><p>18 We illustrate that CRS has the one-sketchfor-all property, meaning that the same set of samples/sketches can be used for computing any linear summary statistics (1). [sent-65, score-0.062]
</p><p>19 This is a signiﬁcant advantage over many other dimension reduction or data stream algorithms. [sent-66, score-0.076]
</p><p>20 For example, the method of stable random projections (SRP)[8, 10, 14] was designed for estimating the lp norms/distances for a ﬁxed p with 0 < p ≤ 2. [sent-67, score-0.16]
</p><p>21 Recently, a new method named Compressed Counting[11] is able to very efﬁciently approximate the lp moments of data streams when p ≈ 1. [sent-68, score-0.137]
</p><p>22 We apply CRS for computing Hilbertian metrics[7], a popular family of distances for constructing kernels in SVM. [sent-73, score-0.124]
</p><p>23 Section 4 focuses on using CRS to estimate the Hamming norm of a single vector, based on which Section 5 provides a generic estimation procedure for CRS, for estimating any linear summary statistics, with the focus on the Hamming distance and the χ2 distance. [sent-77, score-0.128]
</p><p>24 The next step of CRS is to construct a  sketch for each row of the data matrix. [sent-85, score-0.092]
</p><p>25 A sketch can be viewed as a linked list which stores a small fraction of the non-zero entries from the front of each row. [sent-86, score-0.09]
</p><p>26 Figure 1(b) demonstrates three sketches corresponding to the three rows of the (column) permuted data matrix in Figure 1(a). [sent-87, score-0.218]
</p><p>27 (b): Sketches are the ﬁrst ki non-zero entries ascending by IDs (here ki = 4). [sent-90, score-0.077]
</p><p>28 In Figure 1, the sketch for row ui is denoted by Ki . [sent-91, score-0.076]
</p><p>29 The last (largest) IDs of sketches K1 and K2 are max(ID(K1 )) = 10 and max(ID(K2 )) = 8, respectively. [sent-94, score-0.11]
</p><p>30 Here, “ID(K)” stands for the vector of IDs in the sketch K. [sent-95, score-0.059]
</p><p>31 Had we directly taken the ﬁrst Ds = 8 columns from the permuted data matrix, we would obtain the same non-zero entries as in K1 and K2 , if we exclude elements in K1 and K2 whose IDs > Ds = 8. [sent-97, score-0.112]
</p><p>32 in this example, the element 10{8} in sketch K1 is excluded. [sent-98, score-0.059]
</p><p>33 This means, by only looking at sketches K1 and K2 , one can obtain a “random” sample of size Ds . [sent-100, score-0.11]
</p><p>34 By statistics theory, one can easily obtain an unbiased estimate of any linear summary statistics from a random sample. [sent-101, score-0.065]
</p><p>35 When considering the rows u1 and u3 , the sketches K1 and K3 suggest their Ds = min(max(ID(K1 )), max(ID(K3 ))) = min(10,12) = 10. [sent-104, score-0.136]
</p><p>36 Consider a true random sample of size Ds , directly obtained from the ﬁrst Ds columns of the permuted data matrix. [sent-107, score-0.081]
</p><p>37 For a more obvious example, we can consider two rows with exactly one non-zero entry in each row at the same column. [sent-111, score-0.062]
</p><p>38 The original CRS can not obtain an unbiased estimate unless Ds = D. [sent-112, score-0.063]
</p><p>39 3  CRS for Dynamic Data and Introduction to Stable Random Projections  The original CRS was proposed for static data. [sent-113, score-0.063]
</p><p>40 When data arrive in a streaming fashion, they often will not be stored (even on disks)[15]. [sent-115, score-0.072]
</p><p>41 Thus, a one-pass algorithm is needed to compute and update distances for training. [sent-116, score-0.068]
</p><p>42 At each time t, there is an input stream st = (it , It ), it ∈ [1, D] which updates u (denoted by ut ) by ut [it ] = H(ut−1 [it ], It ), where It is the increment/decrement at time t and H is an updating function. [sent-122, score-0.14]
</p><p>43 In terms of the data matrix A ∈ Rn×D , we can view it to be a collection of n data streams. [sent-133, score-0.055]
</p><p>44 2  CRS for Streaming Data  For each stream ut , we maintain a sketch K with length (i. [sent-135, score-0.151]
</p><p>45 The procedure for sketch construction works as follows: 1. [sent-140, score-0.059]
</p><p>46 Apply the procedure to each data stream using the same random permutation mapping π. [sent-149, score-0.084]
</p><p>47 Once sketches are constructed, the estimation procedure will be the same regardless whether the original data are dynamic or static. [sent-150, score-0.182]
</p><p>48 Thus, we will use static data to verify some estimators of CRS. [sent-151, score-0.081]
</p><p>49 3  (Symmetric) Stable Random Projections (SRP)  Since the method of (symmetric) stable random projections (SRP)[8, 10] has become a standard algorithm for data stream computations, we very brieﬂy introduce SRP for the sake of comparisons. [sent-153, score-0.157]
</p><p>50 The procedure of SRP is to multiply the data matrix A ∈ Rn×D by a random matrix R ∈ RD×k , whose entries are i. [sent-154, score-0.093]
</p><p>51 samples from a standard (symmetric) stable distribution S(p, 1), 0 < p ≤ 2. [sent-157, score-0.053]
</p><p>52 By properties of stable distributions, the projected vectors v1 = RT u1 and v2 = RT u2 have i. [sent-159, score-0.053]
</p><p>53 i=1  Thus, one can estimate an individual norm or distance from k samples. [sent-165, score-0.063]
</p><p>54 Compared with Conditional Random Sampling (CRS), SRP has an elegant mathematical derivation, with various interesting estimators and rigorous sample complexity bounds, i. [sent-168, score-0.057]
</p><p>55 The same sketch of CRS can approximate any linear summary statistics (1). [sent-173, score-0.106]
</p><p>56 SRP is limited to the lp norm and distance with 0 < p ≤ 2. [sent-174, score-0.107]
</p><p>57 In machine learning, the distances √ are often computed using weighted data (e. [sent-177, score-0.084]
</p><p>58 For static data, one can ﬁrst term-weight the data before applying SRP. [sent-180, score-0.057]
</p><p>59 For dynamic data, however, there is no way to trace back the original data after projections. [sent-181, score-0.072]
</p><p>60 (3) In this case, despite its simplicity, CRS theoretically achieves similar accuracy as stable random projections (SRP). [sent-191, score-0.097]
</p><p>61 1  The Proposed (Unbiased) Estimator and Variance  Suppose we have obtained the sketch K. [sent-197, score-0.059]
</p><p>62 Lemma 1 (whose proof is omitted) proposes an unbiased ˆ estimator of f , denoted by f , and a biased estimator based on the maximum likelihood, fmle . [sent-199, score-0.223]
</p><p>63 ˆ ˆ Note that, since Var f /f 2 ≈ 1/k, independent of the data, the estimator f actually has the worstcase complexity bound similar to that of SRP[10], although the precise constant is not easy to obtain. [sent-203, score-0.068]
</p><p>64 2  The Approximation Using the Conditioning Argument  D(k−1) ˆ Interestingly, this estimator, f = max(ID(K))−1 , appears to be the estimator for a hypergeometric random sample of size Ds = max(ID(K)) − 1. [sent-205, score-0.088]
</p><p>65 That is, suppose we randomly pick Ds balls (without replacement) from a pool of D balls and we observe that k balls are red; then a natural (and D unbiased) estimator for the total number of red balls would be Ds k ; here k = k − 1. [sent-206, score-0.172]
</p><p>66 This seems to imply that the “conditioning” argument in the original CRS in Section 2 is “correct” if we make a simple modiﬁcation by using the Ds which is the original Ds minus 1. [sent-207, score-0.066]
</p><p>67 ˆ ˆ ˆ Consider fapp = f , where we assume fapp is the estimator for the hypergeometric distribution, then ˆ Var fapp |Ds = Z − 1 ˆ Var fapp  4. [sent-209, score-0.316]
</p><p>68 stable samples with scale parameter Fp = i=1 |ui |p . [sent-215, score-0.053]
</p><p>69 The harmonic mean estimator is ˆ Fp,hm =  2 − π Γ(−p) sin  ˆ Var Fp,hm lim −  p→0+  k j=1 2  = Fp  π 2p  |vj |−p  1 k  2 Γ(−p) sin π  k−  −πΓ(−2p) sin (πp) Γ(−p) sin  −πΓ(−2p) sin (πp) Γ(−p) sin π p 2  → 1,  π 2p  2  −1  lim −  p→0+  π 2p  +O  2  1 k2  −1  ,  . [sent-216, score-0.36]
</p><p>70 ˆ ˆ Denote this estimator by fsrp (using p as small as possible), whose variance is Var fsrp ≈ ˆ which is roughly equivalent to the variance of f , the unbiased estimator for CRS. [sent-218, score-0.287]
</p><p>71 (2): The variance (3) based on the approximate “conditioning” argument is very accurate. [sent-227, score-0.077]
</p><p>72 (3): The ˆ ˆ unbiased estimator f is more accurate than fmle ; the latter actually uses one more sample. [sent-228, score-0.155]
</p><p>73 Var  Standardized MSE  0  10  CUSTOMER 20 k  30  40  3  10  20 k  30  40  Figure 2: Comparing CRS with SRP for approximating Hamming norms in Web crawl data (four word vectors), using the normalized mean square errors (MSE, normalized by f 2 ). [sent-233, score-0.096]
</p><p>74 ”SRP” corresponds to the harmonic mean estimator of SRP using p = 0. [sent-235, score-0.09]
</p><p>75 Suppose we are interested in the distance between rows u1 and u2 and we have access to sketches K1 and K2 . [sent-241, score-0.18]
</p><p>76 1  A Generic Estimator and Approximate Variance  Rigorous theoretical analysis on one pair of sketches is difﬁcult. [sent-247, score-0.11]
</p><p>77 We consider a generic distance dg (u1 , u2 ) = D Ds u ˜ i=1 g (u1,i , u2,i ), and assume that, conditioning on Ds , the sample {˜1,j , u2,j }j=1 is exactly equivalent to the sample from randomly selected Ds columns without replacement. [sent-249, score-0.206]
</p><p>78 Under this assumption, an “unbiased” estimator of dg (u1 , u2 ) (and two special cases) would be D ˆ dg (u1 , u2 ) = Ds  D  D ˆ g(˜1,j , u2,j ), dp = u ˜ Ds i=1  Ds  D ˆ |˜1,j − u2,j |p , dχ2 = u ˜ Ds j=1  Ds j=1  (˜1,j − u2,j )2 u ˜ . [sent-250, score-0.238]
</p><p>79 d2 g D (5)  Here, k1 and k2 are the sketch sizes of K1 and K2 , respectively, f1 and f2 are the numbers of nonzeros in the original data, u1 , u2 , respectively. [sent-253, score-0.081]
</p><p>80 , f2 f1 max k1 −1 , k2 −1 is small, then the variance also tends to be small. [sent-258, score-0.062]
</p><p>81 The next two subsections apply CRS to estimating the Hamming distance and the χ2 distance. [sent-266, score-0.063]
</p><p>82 Empirical studies [3, 7, 9] have demonstrated that, in text and image data, using the Hamming distance or the χ2 distance for kernel SVMs achieved good performance. [sent-267, score-0.088]
</p><p>83 The approximate variance (5) becomes ˆ Var h ≈  D D−1  max  f1 f2 , k1 − 1 k2 − 1  −1  h−  h2 D  . [sent-270, score-0.085]
</p><p>84 (1): CRS can be considerably more accurate than SRP for estimating Hamming distances in [4]. [sent-273, score-0.087]
</p><p>85 (2): The approximate variance formula (6) is very accurate. [sent-274, score-0.074]
</p><p>86 Var SRP  −2  10  20 k  30  10  40  10  20 k  30  40  Figure 3: Approximating Hamming distances (h) using two pairs of words. [sent-277, score-0.068]
</p><p>87 Var” correspond to the approximate variance of CRS in (6). [sent-280, score-0.055]
</p><p>88 An alternative deﬁnition of Hamming distance is D h(u1 , u2 ) = i=1 [1 {u1,i = 0 and u2,i = 0} + 1 {u1,i = 0 and u2,i = 0}], which is basically the lp distance after a binary term-weighting. [sent-282, score-0.132]
</p><p>89 CRS does not provide any worst-case guarantees; its performance relies on the assumption that the data are often reasonably sparse and the second moments should be reasonably bounded in machine learning applications. [sent-288, score-0.056]
</p><p>90 And again, the approximate variance formula (7) is accurate. [sent-296, score-0.074]
</p><p>91 Var  Standardized MSE  Standardized MSE  10  0  10  −1  10  −2  80  100  10  3 5  10  15 k  20  25  30  Figure 4: Left two panels: CRS for approximating the χ2 distance using two pairs of words (D = 216 ). [sent-301, score-0.073]
</p><p>92 The curves report the normalized MSE and the approximate variance in (7). [sent-302, score-0.072]
</p><p>93 It is highly desirable to achieve compact data presentation and efﬁciently computing and retrieving summary statistics, in particular, various types of distances. [sent-309, score-0.061]
</p><p>94 Compared with other “main stream” sketching algorithms such as stable random projections (SRP), the major advantage of CRS is that it is “one-sketch-for-all,” meaning that the same set of sketches can approximate any linear summary statistics. [sent-311, score-0.301]
</p><p>95 Originally based on a heuristic argument, the preliminary version of CRS, was proposed as a tool for computing pairwise l2 and l1 distances in static data. [sent-317, score-0.15]
</p><p>96 Comparing data streams using hamming norms (how to zero in). [sent-337, score-0.183]
</p><p>97 Stable distributions, pseudorandom generators, embeddings, and data stream computation. [sent-351, score-0.06]
</p><p>98 Estimators and tail bounds for dimension reduction in lα (0 < α ≤ 2) using stable random projections. [sent-358, score-0.069]
</p><p>99 A sketch algorithm for estimating two-way and multi-way Associations. [sent-365, score-0.078]
</p><p>100 Computationally efﬁcient estimators for dimension reductions using stable random projections. [sent-374, score-0.108]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('crs', 0.777), ('srp', 0.353), ('ds', 0.293), ('var', 0.137), ('id', 0.121), ('hamming', 0.116), ('sketches', 0.11), ('mse', 0.085), ('dg', 0.085), ('estimator', 0.068), ('distances', 0.068), ('ping', 0.064), ('sketch', 0.059), ('fapp', 0.057), ('standardized', 0.057), ('stable', 0.053), ('ut', 0.048), ('dexter', 0.046), ('fmle', 0.046), ('turnstile', 0.046), ('sin', 0.045), ('distance', 0.044), ('lp', 0.044), ('projections', 0.044), ('stream', 0.044), ('ids', 0.043), ('permuted', 0.043), ('unbiased', 0.041), ('static', 0.041), ('streaming', 0.04), ('mle', 0.037), ('streams', 0.034), ('helbertian', 0.034), ('dynamic', 0.034), ('rigorous', 0.033), ('conditioning', 0.033), ('variance', 0.032), ('entries', 0.031), ('max', 0.03), ('val', 0.03), ('church', 0.03), ('piotr', 0.03), ('kenneth', 0.03), ('sketching', 0.03), ('approximating', 0.029), ('rows', 0.026), ('balls', 0.026), ('customer', 0.026), ('metrics', 0.026), ('modi', 0.024), ('permutation', 0.024), ('estimators', 0.024), ('summary', 0.024), ('approximate', 0.023), ('ki', 0.023), ('trevor', 0.023), ('fsrp', 0.023), ('pagehits', 0.023), ('vfu', 0.023), ('matrix', 0.023), ('argument', 0.022), ('columns', 0.022), ('harmonic', 0.022), ('fp', 0.022), ('generic', 0.022), ('original', 0.022), ('computing', 0.021), ('storing', 0.021), ('pairwise', 0.02), ('moments', 0.02), ('hypergeometric', 0.02), ('disks', 0.02), ('hilbertian', 0.02), ('sparse', 0.02), ('kernels', 0.019), ('entry', 0.019), ('formula', 0.019), ('web', 0.019), ('estimating', 0.019), ('norm', 0.019), ('sampling', 0.018), ('recommend', 0.018), ('indyk', 0.018), ('rn', 0.018), ('icdm', 0.017), ('norms', 0.017), ('meaning', 0.017), ('row', 0.017), ('normalized', 0.017), ('stored', 0.016), ('soda', 0.016), ('engine', 0.016), ('olivier', 0.016), ('data', 0.016), ('hastie', 0.016), ('capacity', 0.016), ('conditional', 0.016), ('popular', 0.016), ('dimension', 0.016), ('reductions', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000008 <a title="167-tfidf-1" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>Author: Ping Li, Kenneth W. Church, Trevor J. Hastie</p><p>Abstract: Conditional Random Sampling (CRS) was originally proposed for efﬁciently computing pairwise (l2 , l1 ) distances, in static, large-scale, and sparse data. This study modiﬁes the original CRS and extends CRS to handle dynamic or streaming data, which much better reﬂect the real-world situation than assuming static data. Compared with many other sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a signiﬁcant advantage in that it is “one-sketch-for-all.” In particular, we demonstrate the effectiveness of CRS in efﬁciently computing the Hamming norm, the Hamming distance, the lp distance, and the χ2 distance. A generic estimator and an approximate variance formula are also provided, for approximating any type of distances. We recommend CRS as a promising tool for building highly scalable systems, in machine learning, data mining, recommender systems, and information retrieval. 1</p><p>2 0.073591232 <a title="167-tfidf-2" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>Author: Ralf M. Haefner, Bruce G. Cumming</p><p>Abstract: A crucial part of developing mathematical models of information processing in the brain is the quantiﬁcation of their success. One of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model. Unfortunately, this metric is biased due to the intrinsic variability in the data. We derive a simple analytical modiﬁcation of the traditional formula that signiﬁcantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying Variance Explained by the model class. Our estimator advances on previous work by a) accounting for overﬁtting due to free model parameters mitigating the need for a separate validation data set, b) adjusting for the uncertainty in the noise estimate and c) adding a conditioning term. We apply our new estimator to binocular disparity tuning curves of a set of macaque V1 neurons and ﬁnd that on a population level almost all of the variance unexplained by Gabor functions is attributable to noise. 1</p><p>3 0.068659708 <a title="167-tfidf-3" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>Author: Julia Owen, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan, David P. Wipf</p><p>Abstract: The synchronous brain activity measured via MEG (or EEG) can be interpreted as arising from a collection (possibly large) of current dipoles or sources located throughout the cortex. Estimating the number, location, and orientation of these sources remains a challenging task, one that is signiﬁcantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity, sensor noise, and other artifacts. This paper derives an empirical Bayesian method for addressing each of these issues in a principled fashion. The resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations. Robust interference suppression is also easily incorporated. In a restricted setting, the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi-component dipoles even in the presence of correlations, unlike a variety of existing Bayesian localization methods or common signal processing techniques such as beamforming and sLORETA. Empirical results on both simulated and real data sets verify the efﬁcacy of this approach. 1</p><p>4 0.052192204 <a title="167-tfidf-4" href="./nips-2008-Nonparametric_Bayesian_Learning_of_Switching_Linear_Dynamical_Systems.html">154 nips-2008-Nonparametric Bayesian Learning of Switching Linear Dynamical Systems</a></p>
<p>Author: Alan S. Willsky, Erik B. Sudderth, Michael I. Jordan, Emily B. Fox</p><p>Abstract: Many nonlinear dynamical phenomena can be effectively modeled by a system that switches among a set of conditionally linear dynamical modes. We consider two such models: the switching linear dynamical system (SLDS) and the switching vector autoregressive (VAR) process. Our nonparametric Bayesian approach utilizes a hierarchical Dirichlet process prior to learn an unknown number of persistent, smooth dynamical modes. We develop a sampling algorithm that combines a truncated approximation to the Dirichlet process with efﬁcient joint sampling of the mode and state sequences. The utility and ﬂexibility of our model are demonstrated on synthetic data, sequences of dancing honey bees, and the IBOVESPA stock index.</p><p>5 0.045879841 <a title="167-tfidf-5" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>Author: Maxim Raginsky, Svetlana Lazebnik, Rebecca Willett, Jorge Silva</p><p>Abstract: This paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions. For d covariates, there are 2d basis coefﬁcients to estimate, which renders conventional approaches computationally prohibitive when d is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error, and (2) the computational complexity is lower for sparser densities. Our method also allows for ﬂexible control of the trade-off between mean-squared error and computational complexity.</p><p>6 0.040835936 <a title="167-tfidf-6" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>7 0.039511144 <a title="167-tfidf-7" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>8 0.039077114 <a title="167-tfidf-8" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>9 0.037398439 <a title="167-tfidf-9" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>10 0.037362251 <a title="167-tfidf-10" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>11 0.033154972 <a title="167-tfidf-11" href="./nips-2008-A_computational_model_of_hippocampal_function_in_trace_conditioning.html">7 nips-2008-A computational model of hippocampal function in trace conditioning</a></p>
<p>12 0.030913027 <a title="167-tfidf-12" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>13 0.029621126 <a title="167-tfidf-13" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>14 0.029320039 <a title="167-tfidf-14" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>15 0.028496448 <a title="167-tfidf-15" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>16 0.027993901 <a title="167-tfidf-16" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>17 0.026987035 <a title="167-tfidf-17" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>18 0.026266666 <a title="167-tfidf-18" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>19 0.025848124 <a title="167-tfidf-19" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>20 0.025660783 <a title="167-tfidf-20" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.091), (1, -0.007), (2, -0.002), (3, 0.018), (4, 0.02), (5, -0.004), (6, -0.006), (7, 0.01), (8, 0.015), (9, -0.009), (10, -0.014), (11, -0.015), (12, -0.013), (13, 0.032), (14, -0.064), (15, -0.003), (16, 0.056), (17, 0.049), (18, 0.0), (19, -0.006), (20, 0.012), (21, 0.08), (22, 0.042), (23, -0.005), (24, 0.002), (25, 0.051), (26, 0.024), (27, -0.026), (28, 0.075), (29, 0.024), (30, 0.009), (31, -0.079), (32, 0.021), (33, -0.062), (34, 0.022), (35, -0.005), (36, 0.053), (37, -0.002), (38, -0.009), (39, -0.039), (40, -0.06), (41, 0.008), (42, 0.03), (43, -0.19), (44, -0.055), (45, -0.036), (46, 0.099), (47, 0.07), (48, 0.102), (49, -0.054)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91076946 <a title="167-lsi-1" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>Author: Ping Li, Kenneth W. Church, Trevor J. Hastie</p><p>Abstract: Conditional Random Sampling (CRS) was originally proposed for efﬁciently computing pairwise (l2 , l1 ) distances, in static, large-scale, and sparse data. This study modiﬁes the original CRS and extends CRS to handle dynamic or streaming data, which much better reﬂect the real-world situation than assuming static data. Compared with many other sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a signiﬁcant advantage in that it is “one-sketch-for-all.” In particular, we demonstrate the effectiveness of CRS in efﬁciently computing the Hamming norm, the Hamming distance, the lp distance, and the χ2 distance. A generic estimator and an approximate variance formula are also provided, for approximating any type of distances. We recommend CRS as a promising tool for building highly scalable systems, in machine learning, data mining, recommender systems, and information retrieval. 1</p><p>2 0.79074538 <a title="167-lsi-2" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>Author: Ralf M. Haefner, Bruce G. Cumming</p><p>Abstract: A crucial part of developing mathematical models of information processing in the brain is the quantiﬁcation of their success. One of the most widely-used metrics yields the percentage of the variance in the data that is explained by the model. Unfortunately, this metric is biased due to the intrinsic variability in the data. We derive a simple analytical modiﬁcation of the traditional formula that signiﬁcantly improves its accuracy (as measured by bias) with similar or better precision (as measured by mean-square error) in estimating the true underlying Variance Explained by the model class. Our estimator advances on previous work by a) accounting for overﬁtting due to free model parameters mitigating the need for a separate validation data set, b) adjusting for the uncertainty in the noise estimate and c) adding a conditioning term. We apply our new estimator to binocular disparity tuning curves of a set of macaque V1 neurons and ﬁnd that on a population level almost all of the variance unexplained by Gabor functions is attributable to noise. 1</p><p>3 0.64238513 <a title="167-lsi-3" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>Author: Guangzhi Cao, Charles Bouman</p><p>Abstract: Covariance estimation for high dimensional vectors is a classically difﬁcult problem in statistical analysis and machine learning. In this paper, we propose a maximum likelihood (ML) approach to covariance estimation, which employs a novel sparsity constraint. More speciﬁcally, the covariance is constrained to have an eigen decomposition which can be represented as a sparse matrix transform (SMT). The SMT is formed by a product of pairwise coordinate rotations known as Givens rotations. Using this framework, the covariance can be efﬁciently estimated using greedy minimization of the log likelihood function, and the number of Givens rotations can be efﬁciently computed using a cross-validation procedure. The resulting estimator is positive deﬁnite and well-conditioned even when the sample size is limited. Experiments on standard hyperspectral data sets show that the SMT covariance estimate is consistently more accurate than both traditional shrinkage estimates and recently proposed graphical lasso estimates for a variety of different classes and sample sizes. 1</p><p>4 0.49988008 <a title="167-lsi-4" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>Author: Maxim Raginsky, Svetlana Lazebnik, Rebecca Willett, Jorge Silva</p><p>Abstract: This paper describes a recursive estimation procedure for multivariate binary densities using orthogonal expansions. For d covariates, there are 2d basis coefﬁcients to estimate, which renders conventional approaches computationally prohibitive when d is large. However, for a wide class of densities that satisfy a certain sparsity condition, our estimator runs in probabilistic polynomial time and adapts to the unknown sparsity of the underlying density in two key ways: (1) it attains near-minimax mean-squared error, and (2) the computational complexity is lower for sparser densities. Our method also allows for ﬂexible control of the trade-off between mean-squared error and computational complexity.</p><p>5 0.40331471 <a title="167-lsi-5" href="./nips-2008-Nonparametric_Bayesian_Learning_of_Switching_Linear_Dynamical_Systems.html">154 nips-2008-Nonparametric Bayesian Learning of Switching Linear Dynamical Systems</a></p>
<p>Author: Alan S. Willsky, Erik B. Sudderth, Michael I. Jordan, Emily B. Fox</p><p>Abstract: Many nonlinear dynamical phenomena can be effectively modeled by a system that switches among a set of conditionally linear dynamical modes. We consider two such models: the switching linear dynamical system (SLDS) and the switching vector autoregressive (VAR) process. Our nonparametric Bayesian approach utilizes a hierarchical Dirichlet process prior to learn an unknown number of persistent, smooth dynamical modes. We develop a sampling algorithm that combines a truncated approximation to the Dirichlet process with efﬁcient joint sampling of the mode and state sequences. The utility and ﬂexibility of our model are demonstrated on synthetic data, sequences of dancing honey bees, and the IBOVESPA stock index.</p><p>6 0.38852745 <a title="167-lsi-6" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>7 0.38179585 <a title="167-lsi-7" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>8 0.38067278 <a title="167-lsi-8" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>9 0.36939374 <a title="167-lsi-9" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>10 0.36633277 <a title="167-lsi-10" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<p>11 0.3230879 <a title="167-lsi-11" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>12 0.32178804 <a title="167-lsi-12" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>13 0.30549213 <a title="167-lsi-13" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>14 0.305401 <a title="167-lsi-14" href="./nips-2008-Stochastic_Relational_Models_for_Large-scale_Dyadic_Data_using_MCMC.html">221 nips-2008-Stochastic Relational Models for Large-scale Dyadic Data using MCMC</a></p>
<p>15 0.28864712 <a title="167-lsi-15" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>16 0.28745341 <a title="167-lsi-16" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>17 0.27687836 <a title="167-lsi-17" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>18 0.25756338 <a title="167-lsi-18" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>19 0.25574297 <a title="167-lsi-19" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>20 0.25232685 <a title="167-lsi-20" href="./nips-2008-A_computational_model_of_hippocampal_function_in_trace_conditioning.html">7 nips-2008-A computational model of hippocampal function in trace conditioning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.086), (7, 0.083), (12, 0.041), (27, 0.297), (28, 0.148), (57, 0.063), (63, 0.023), (71, 0.022), (77, 0.033), (78, 0.025), (83, 0.043)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.80018282 <a title="167-lda-1" href="./nips-2008-Sparse_Convolved_Gaussian_Processes_for_Multi-output_Regression.html">213 nips-2008-Sparse Convolved Gaussian Processes for Multi-output Regression</a></p>
<p>Author: Mauricio Alvarez, Neil D. Lawrence</p><p>Abstract: We present a sparse approximation approach for dependent output Gaussian processes (GP). Employing a latent function framework, we apply the convolution process formalism to establish dependencies between output variables, where each latent function is represented as a GP. Based on these latent functions, we establish an approximation scheme using a conditional independence assumption between the output processes, leading to an approximation of the full covariance which is determined by the locations at which the latent functions are evaluated. We show results of the proposed methodology for synthetic data and real world applications on pollution prediction and a sensor network. 1</p><p>2 0.75434226 <a title="167-lda-2" href="./nips-2008-Modeling_Short-term_Noise_Dependence_of_Spike_Counts_in_Macaque_Prefrontal_Cortex.html">137 nips-2008-Modeling Short-term Noise Dependence of Spike Counts in Macaque Prefrontal Cortex</a></p>
<p>Author: Arno Onken, Steffen Grünewälder, Matthias Munk, Klaus Obermayer</p><p>Abstract: Correlations between spike counts are often used to analyze neural coding. The noise is typically assumed to be Gaussian. Yet, this assumption is often inappropriate, especially for low spike counts. In this study, we present copulas as an alternative approach. With copulas it is possible to use arbitrary marginal distributions such as Poisson or negative binomial that are better suited for modeling noise distributions of spike counts. Furthermore, copulas place a wide range of dependence structures at the disposal and can be used to analyze higher order interactions. We develop a framework to analyze spike count data by means of copulas. Methods for parameter inference based on maximum likelihood estimates and for computation of mutual information are provided. We apply the method to our data recorded from macaque prefrontal cortex. The data analysis leads to three ﬁndings: (1) copula-based distributions provide signiﬁcantly better ﬁts than discretized multivariate normal distributions; (2) negative binomial margins ﬁt the data signiﬁcantly better than Poisson margins; and (3) the dependence structure carries 12% of the mutual information between stimuli and responses. 1</p><p>same-paper 3 0.73127514 <a title="167-lda-3" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>Author: Ping Li, Kenneth W. Church, Trevor J. Hastie</p><p>Abstract: Conditional Random Sampling (CRS) was originally proposed for efﬁciently computing pairwise (l2 , l1 ) distances, in static, large-scale, and sparse data. This study modiﬁes the original CRS and extends CRS to handle dynamic or streaming data, which much better reﬂect the real-world situation than assuming static data. Compared with many other sketching algorithms for dimension reductions such as stable random projections, CRS exhibits a signiﬁcant advantage in that it is “one-sketch-for-all.” In particular, we demonstrate the effectiveness of CRS in efﬁciently computing the Hamming norm, the Hamming distance, the lp distance, and the χ2 distance. A generic estimator and an approximate variance formula are also provided, for approximating any type of distances. We recommend CRS as a promising tool for building highly scalable systems, in machine learning, data mining, recommender systems, and information retrieval. 1</p><p>4 0.72572488 <a title="167-lda-4" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>Author: Guillaume R. Obozinski, Martin J. Wainwright, Michael I. Jordan</p><p>Abstract: We study the behavior of block 1 / 2 regularization for multivariate regression, where a K-dimensional response vector is regressed upon a ﬁxed set of p covariates. The problem of support union recovery is to recover the subset of covariates that are active in at least one of the regression problems. Studying this problem under high-dimensional scaling (where the problem parameters as well as sample size n tend to inﬁnity simultaneously), our main result is to show that exact recovery is possible once the order parameter given by θ 1 / 2 (n, p, s) : = n/[2ψ(B ∗ ) log(p − s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the size of the union of supports, and ψ(B ∗ ) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefﬁcient vectors that constitute the model. This sparsity-overlap function reveals that block 1 / 2 regularization for multivariate regression never harms performance relative to a naive 1 -approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal relative to the design. We complement our theoretical results with simulations that demonstrate the sharpness of the result, even for relatively small problems. 1</p><p>5 0.5776574 <a title="167-lda-5" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>6 0.56898814 <a title="167-lda-6" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>7 0.56510127 <a title="167-lda-7" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>8 0.56273752 <a title="167-lda-8" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>9 0.56269342 <a title="167-lda-9" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>10 0.5622564 <a title="167-lda-10" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>11 0.561854 <a title="167-lda-11" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>12 0.56091291 <a title="167-lda-12" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>13 0.56083316 <a title="167-lda-13" href="./nips-2008-Near-minimax_recursive_density_estimation_on_the_binary_hypercube.html">149 nips-2008-Near-minimax recursive density estimation on the binary hypercube</a></p>
<p>14 0.56027108 <a title="167-lda-14" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>15 0.55934417 <a title="167-lda-15" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>16 0.55888087 <a title="167-lda-16" href="./nips-2008-Covariance_Estimation_for_High_Dimensional_Data_Vectors_Using_the_Sparse_Matrix_Transform.html">54 nips-2008-Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform</a></p>
<p>17 0.55768436 <a title="167-lda-17" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>18 0.55758256 <a title="167-lda-18" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>19 0.55701667 <a title="167-lda-19" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>20 0.55674142 <a title="167-lda-20" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
