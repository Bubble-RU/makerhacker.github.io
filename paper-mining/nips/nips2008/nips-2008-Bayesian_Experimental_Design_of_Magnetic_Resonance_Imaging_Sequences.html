<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-30" href="#">nips2008-30</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</h1>
<br/><p>Source: <a title="nips-2008-30-pdf" href="http://papers.nips.cc/paper/3558-bayesian-experimental-design-of-magnetic-resonance-imaging-sequences.pdf">pdf</a></p><p>Author: Hannes Nickisch, Rolf Pohmann, Bernhard Schölkopf, Matthias Seeger</p><p>Abstract: We show how improved sequences for magnetic resonance imaging can be found through optimization of Bayesian design scores. Combining approximate Bayesian inference and natural image statistics with high-performance numerical computation, we propose the ﬁrst Bayesian experimental design framework for this problem of high relevance to clinical and brain research. Our solution requires large-scale approximate inference for dense, non-Gaussian models. We propose a novel scalable variational inference algorithm, and show how powerful methods of numerical mathematics can be modiﬁed to compute primitives in our framework. Our approach is evaluated on raw data from a 3T MR scanner. 1</p><p>Reference: <a title="nips-2008-30-reference" href="../nips2008_reference/nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 de  Abstract We show how improved sequences for magnetic resonance imaging can be found through optimization of Bayesian design scores. [sent-5, score-0.562]
</p><p>2 Combining approximate Bayesian inference and natural image statistics with high-performance numerical computation, we propose the ﬁrst Bayesian experimental design framework for this problem of high relevance to clinical and brain research. [sent-6, score-0.369]
</p><p>3 We propose a novel scalable variational inference algorithm, and show how powerful methods of numerical mathematics can be modiﬁed to compute primitives in our framework. [sent-8, score-0.222]
</p><p>4 1  Introduction  Magnetic resonance imaging (MRI) [7, 2] is a key diagnostic technique in healthcare nowadays, and of central importance for experimental research of the brain. [sent-10, score-0.215]
</p><p>5 To select the optimum sequence for a given problem, and to tune its parameters, is a difﬁcult task even for experts, and even more challenging is the design of new, customized sequences to address a particular question, making sequence development an entire ﬁeld of research [1]. [sent-13, score-0.339]
</p><p>6 The main drawbacks of MRI are high initial and running costs, since a very strong homogeneous magnetic ﬁeld has to be maintained, moreover long scanning times due to weak signals and limits to gradient amplitude. [sent-14, score-0.129]
</p><p>7 Beyond reduced costs, faster imaging also leads to higher temporal resolution in dynamic sequences for functional MRI (fMRI), less annoyance to patients, and fewer artifacts due to patient motion. [sent-16, score-0.286]
</p><p>8 In this paper, we employ Bayesian experimental design to optimize MRI sequences. [sent-17, score-0.157]
</p><p>9 Image reconstruction from MRI raw data is viewed as a problem of inference from incomplete observations. [sent-18, score-0.226]
</p><p>10 For most sequences used in hospitals today, reconstruction is done by a single fast Fourier transform (FFT). [sent-20, score-0.244]
</p><p>11 However, natural and MR images show stable low-level statistical properties,1 which allows them to be reconstructed from 1 These come from the presence of edges and smooth areas, which on a low level deﬁne image structure, and which are not present in Gaussian data (noise). [sent-21, score-0.149]
</p><p>12 A similar idea is known as compressed sensing (CS), which has been applied to MRI [8]. [sent-24, score-0.125]
</p><p>13 It has been proposed to design sequences by blindly randomizing aspects thereof [8], based on CS theoretical results. [sent-28, score-0.302]
</p><p>14 Our proposal requires efﬁcient Bayesian inference for MR images of realistic resolution. [sent-31, score-0.109]
</p><p>15 We present a novel scalable variational approximate inference algorithm inspired by [16]. [sent-32, score-0.187]
</p><p>16 Finally, we are not aware of Bayesian or classical experimental design methods for dense non-Gaussian models, scaling comparably to ours. [sent-36, score-0.157]
</p><p>17 Our model and experimental design framework are described in Section 2, a novel scalable approximate inference algorithm is developed in Section 3, and our framework is evaluated on a large-scale realistic setup with scanner raw data in Section 4. [sent-38, score-0.487]
</p><p>18 Under ideal conditions, the raw data y ∈ Rm from the scanner is a linear map3 of u, motivating the likelihood y = Xu + ε,  ε ∼ N (0, σ 2 I). [sent-41, score-0.186]
</p><p>19 In the context of this paper, the problem of experimental design is how to choose X within a space of technically feasible sequences, so that u can be best recovered given y. [sent-43, score-0.157]
</p><p>20 The posterior has the form q τ e−˜j |sj | ,  P (u|y) ∝ N (y|Xu, σ 2 I)  s = Bu, τj = τj /σ, ˜  (1)  j=1  the prior being a product of Laplacians on linear projections sj of u, among them the image gradient and wavelet coefﬁcients. [sent-46, score-0.175]
</p><p>21 In the variant of Bayesian sequential experimental design used here, an extension of X by X∗ ∈ Rd,n is scored by the entropy difference ∆(X∗ ) := H[P (u|y)] − EP (y∗ |y) [H[P (u|y, y∗ )]] ,  (2)  where P (u|y, y∗ ) is the posterior after including (X∗ , y∗ ). [sent-52, score-0.235]
</p><p>22 After each extension, a new scanner measurement is obtained for the single extended sequence only. [sent-58, score-0.224]
</p><p>23 Our Bayesian predictive approach allows us to score many candidates (X∗ , y∗ ) without performing costly MR measurements for them. [sent-59, score-0.115]
</p><p>24 First, MR sequences naturally decompose in a sequential fashion: they describe a discontinuous path of several smooth trajectories (see Section 4). [sent-61, score-0.219]
</p><p>25 Finally, the computational complexity of optimizing over complete sequences is staggering. [sent-63, score-0.127]
</p><p>26 3  Scalable Approximate Inference  In this section, we propose a novel scalable algorithm for the variational inference approxτ imation proposed in [3]. [sent-65, score-0.154]
</p><p>27 This algorithm is also essential for scoring many candidates in each design step of our method (see Section 3. [sent-89, score-0.19]
</p><p>28 It was found in [12] that aggressive sparsiﬁcation, notwithstanding being computationally convenient, hurts experimental design (and even reconstruction) for natural images. [sent-102, score-0.157]
</p><p>29 For nc candidates of d rows, computing scores would need d · nc LCG runs, which is not feasible. [sent-113, score-0.146]
</p><p>30 An MR scanner acquires Fourier coefﬁcients Y (k) at spatial frequencies5 k (the 2d Fourier domain is called k-space), along smooth trajectories k(t) determined by magnetic ﬁeld gradients g(t). [sent-131, score-0.362]
</p><p>31 In Cartesian sampling, trajectories are parallel equispaced lines in k-space, so the FFT can be used for image reconstruction. [sent-135, score-0.287]
</p><p>32 It is often used for dynamic studies, such as cardiac imaging and fMRI. [sent-138, score-0.125]
</p><p>33 As for other reconstruction methods, most of our running time is spent in the gridding (MVMs with X, X T , and X∗ ). [sent-143, score-0.18]
</p><p>34 gy in [mT/m]  gx in [mT/m]  For our experiments, we acquired data on an equispaced grid. [sent-144, score-0.148]
</p><p>35 7 In r−space: U(r) k−space: Y(k) gradients: g(t) theory, the image u is real-valued; 50 n 1/2 in reality, due to resonance fre0 quency offsets, magnetic ﬁeld inhomogeneities, and eddy currents [1, −50 0 2 4 6 0 50 ch. [sent-145, score-0.278]
</p><p>36 8 Note Figure 1: MR signal acquisition: r-space and k-space representhat |utrue |, against which recon- tation of the signal on a rectangular grid as well as the trajectory structions are judged below, is not al- obtained by means of magnetic ﬁeld gradients tered by this correction. [sent-150, score-0.186]
</p><p>37 From the corrected raw data, we simulate all further measurements under different sequences using gridding interpolation. [sent-151, score-0.296]
</p><p>38 While no noise is added to these measurements, there remain signiﬁcant highfrequency erroneous phase contributions in utrue . [sent-152, score-0.226]
</p><p>39 Interleaved outgoing Archimedian spirals employ trajectories k(t) ∝ θ(t)ei2π[θ(t)+θ0 ] , θ(0) = 0, where the gradient g(t) ∝ dk/dt grows to maximum strength at the slew rate, then stays there [1, ch. [sent-153, score-0.195]
</p><p>40 For equispaced offset angles θ0 , the Nyquist spiral (respecting the limit radially) has Nshot = 16. [sent-160, score-0.49]
</p><p>41 Our goal is to design spiral sequences with smaller Nshot , reducing scan time by a factor 16/Nshot . [sent-161, score-0.505]
</p><p>42 Since utrue is approximately real-valued, measurements at k and −k are quite redundant, which is why we restrict9 ourselves to offset angles θ0 ∈ [0, π). [sent-164, score-0.357]
</p><p>43 We score candidates (π/256)[0 : 255] in each round, comparing to equispaced placements jπ/Nshot , and to drawing θ0 uniformly at random. [sent-165, score-0.214]
</p><p>44 For the former, favoured by MRI practitioners right now, the maximum k-space distance between samples is minimized, while the latter is aligned with compressed sensing recommendations [8]. [sent-166, score-0.125]
</p><p>45 For a given sequence, we consider different image reconstructions: the posterior mode (convex MAP estimation) [8], linear least squares (LS; linear conjugate gradients), and zero ﬁlling with density compensation (ZFDC; based on Voronoi diagram) [1, ch. [sent-167, score-0.162]
</p><p>46 We selected the τ scale parameters (there are two of them, as in [12]) optimally for the Nyquist spiral Xnyq , and set σ 2 to the variance of Xnyq (utrue − |utrue |). [sent-172, score-0.203]
</p><p>47 10 We report L2 distances between reconstruction and true image |utrue |. [sent-174, score-0.209]
</p><p>48 8 We sample the center of k-space on a p × p Cartesian grid, obtaining a low-resolution reconstruction ˜ by FFT, whose phase ϕ we use to correct the raw data. [sent-181, score-0.231]
</p><p>49 While reconstruction errors generally decrease somewhat with larger p, the relative differences between all settings below are insensitive to p. [sent-183, score-0.117]
</p><p>50 9 Dropping this restriction disfavours equispaced {θ0 } setups with even Nshot . [sent-184, score-0.185]
</p><p>51 28  slices 2,4,6,10,12,14 from design of slice 8 MAPop MAPeq LSop LSeq 9. [sent-293, score-0.293]
</p><p>52 27  Figure 3: Results for spiral interleaves on slices 8, 12 (table left). [sent-329, score-0.387]
</p><p>53 Offset angles θ0 ∈ [0, π): op (optimized; our method), rd (uniformly random; avg. [sent-331, score-0.117]
</p><p>54 errors for slices 2,4,6,10,14, measured with sequences optimized on slice 8. [sent-335, score-0.343]
</p><p>55 Table lower right: Results for Nyquist spiral eq[Nshot = 16]. [sent-336, score-0.203]
</p><p>56 The standard reconstruction method ZFDC is improved upon strongly by LS (both are linear, but LS is iterative), which in turn is improved upon signiﬁcantly by MAP. [sent-337, score-0.15]
</p><p>57 This is true even for the Nyquist spiral (Nshot = 16). [sent-338, score-0.203]
</p><p>58 Results such as ours, together with the availability of affordable highperformance digital computation, strongly motivate the transition away from direct signal processing reconstruction algorithms to modern iterative statistical estimators. [sent-341, score-0.15]
</p><p>59 Note that ZFDC (and, to a lesser extent, LS) copes best with equispaced designs, while MAP works best with optimized angles. [sent-342, score-0.195]
</p><p>60 This is because the optimized designs leave larger gaps in k-space (see Figure 4). [sent-343, score-0.143]
</p><p>61 Nonlinear estimators can interpolate across such gaps to some extent, using image sparsity priors. [sent-344, score-0.137]
</p><p>62 Methods like ZFDC merely interpolate locally in k-space, uninformed about image statistics, so that violations of the Nyquist limit anywhere necessarily translate into errors. [sent-345, score-0.137]
</p><p>63 It is clearly evident that drawing the spiral offset angles at random does not work well, even if MAP reconstruction is used as in [8]. [sent-346, score-0.459]
</p><p>64 Our 6  results strongly suggest that randomizing MR sequences is not a useful design principle. [sent-353, score-0.335]
</p><p>65 Reasons why CS theory as yet fails to guide measurement design for real images, are reviewed there, see also [15]. [sent-355, score-0.175]
</p><p>66 The outcome of our Bayesian optimized design is stable, in that sequences found in several repetitions gave almost identical reconstruction performance. [sent-357, score-0.415]
</p><p>67 Since utrue is close to real, both Slice 8, Nshot=8 Slice 12, Nshot=8 attain close to Nyquist performance up from Nshot = 8. [sent-359, score-0.169]
</p><p>68 Breaking up such regular designs seems to be the major role of random- Figure 4: Spirals found by our algorithm. [sent-371, score-0.141]
</p><p>69 The ordering ization in CS theory, but our results show that is color-coded: dark spirals selected ﬁrst. [sent-372, score-0.106]
</p><p>70 We see that approximate Bayesian experimental design is useful to optimize measurement architectures for subsequent MAP reconstruction. [sent-374, score-0.241]
</p><p>71 To our knowledge, no similar design optimization method based purely on MAP estimation has been proposed (ours needs approximate inference), rendering the beneﬁcial interplay between our framework and subsequent MAP estimation all the more interesting. [sent-375, score-0.157]
</p><p>72 Our implementation requires about 5 hours on a single standard desktop machine to optimize 11 angles sequentially, 256 candidates per extension, with n and d as above. [sent-377, score-0.145]
</p><p>73 It is neither feasible nor desirable on most current MR scanners to optimize the sequence during the measurement, so an important question is whether sequences optimized on some slices work better in general as well (for the same contrast and similar objects). [sent-379, score-0.317]
</p><p>74 13 Two spirals found by our method are shown in Figure 4 (2 of 8 interleaves, Nshot = 8). [sent-382, score-0.106]
</p><p>75 5  Discussion  We have presented the ﬁrst scalable Bayesian experimental design framework for automatically optimizing MRI sequences, a problem of high impact on clinical diagnostics and brain research. [sent-385, score-0.251]
</p><p>76 Any spiral interleave samples more closely around the origin. [sent-388, score-0.237]
</p><p>77 In fact, the sampling density as a function of spatial frequency |k(t)| does not depend on the offset angles θ0 . [sent-389, score-0.139]
</p><p>78 12 In another set of experiments (not shown), we compared optimization, randomization, and equispacing of θ0 ∈ [0, 2π), in disregard of the approximate real-valuedness of utrue . [sent-390, score-0.244]
</p><p>79 Artiﬁcial phantoms of extremely simple structure, often used in MR sequence design, are not suitable in that respect. [sent-393, score-0.107]
</p><p>80 Real MR images are much more complicated than simple phantoms, even in low level statistics, and results obtained on phantoms only should not be given overly high attendance. [sent-394, score-0.12]
</p><p>81 We demonstrated the power of our approach in a study with spiral sequences, using raw data from a 3T MR scanner. [sent-396, score-0.26]
</p><p>82 The sequences found by our method lead to reconstructions of high quality, even though they are faster than traditionally used Nyquist setups by a factor up to two. [sent-397, score-0.219]
</p><p>83 They improve strongly on sequences obtained by blind randomization. [sent-398, score-0.16]
</p><p>84 Moreover, across all designs, nonlinear Bayesian MAP estimation was found to be essential for reconstructions from undersamplings, and our design optimization framework is especially useful for subsequent MAP reconstruction. [sent-399, score-0.179]
</p><p>85 Our results strongly suggest that modiﬁcations to standard sequences can be found which produce similar images at lower cost. [sent-400, score-0.217]
</p><p>86 Namely, with so many handles to turn in sequence design nowadays, this is a high-dimensional optimization problem dealing with signals (images) of high complexity, and human experts can greatly beneﬁt from goal-directed machine exploration. [sent-401, score-0.168]
</p><p>87 Randomizing parameters of a sequence, as suggested by compressed sensing theory, helps to break wasteful symmetries in regular standard sequences. [sent-402, score-0.17]
</p><p>88 As our results show, many of the advantages of regular sequences are lost by randomization though. [sent-403, score-0.213]
</p><p>89 The optimization of Bayesian information leads to irregular sequences as well, improving on regular, and especially on randomized designs. [sent-404, score-0.127]
</p><p>90 Finally, our framework seems also promising for real-time imaging [1, ch. [sent-407, score-0.125]
</p><p>91 4], where the scanner allows for on-line adaptations of the sequence depending on measurement feedback. [sent-409, score-0.224]
</p><p>92 This will come with new problems not addressed in Section 4, such as phase or image errors that depend on the sequence employed14 (which could be accounted for by a more elaborate noise model). [sent-412, score-0.193]
</p><p>93 In our experiments in Section 4, the choice of different offset angles is cost-neutral, but when a larger set of candidates is used, respective costs have to be quantiﬁed in terms of real scan time, error-proneness, heating due to rapid gradient switching, and other factors. [sent-413, score-0.256]
</p><p>94 FLASH imaging: Rapid NMR imaging a using low ﬂip-angle pulses. [sent-450, score-0.125]
</p><p>95 RARE imaging: A fast imaging method for clinical MR. [sent-459, score-0.16]
</p><p>96 Image formation by induced local interactions: Examples employing nuclear magnetic resonance. [sent-466, score-0.169]
</p><p>97 Sparse MRI: The application of compressed sensing for rapid MR imaging. [sent-472, score-0.125]
</p><p>98 Bayesian inference and optimal design for the sparse linear model. [sent-492, score-0.209]
</p><p>99 Large scale variational inference and experimental design for sparse generalized linear models. [sent-502, score-0.285]
</p><p>100 14  Some common problems with spirals are discussed in [1, ch. [sent-520, score-0.106]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('nshot', 0.465), ('lanczos', 0.24), ('mri', 0.21), ('mr', 0.204), ('spiral', 0.203), ('zfdc', 0.19), ('utrue', 0.169), ('equispaced', 0.148), ('nyquist', 0.142), ('scanner', 0.129), ('magnetic', 0.129), ('sequences', 0.127), ('mapeq', 0.127), ('imaging', 0.125), ('design', 0.124), ('reconstruction', 0.117), ('mapop', 0.106), ('spirals', 0.106), ('slices', 0.099), ('designs', 0.096), ('image', 0.092), ('interleaves', 0.085), ('mvm', 0.085), ('mvms', 0.085), ('ls', 0.082), ('map', 0.082), ('angles', 0.079), ('slice', 0.07), ('primitives', 0.068), ('fft', 0.068), ('candidates', 0.066), ('compressed', 0.065), ('qt', 0.064), ('gridding', 0.063), ('lcg', 0.063), ('lseq', 0.063), ('maprd', 0.063), ('phantoms', 0.063), ('offset', 0.06), ('sensing', 0.06), ('bayesian', 0.06), ('scalable', 0.059), ('eq', 0.058), ('resonance', 0.057), ('gradients', 0.057), ('raw', 0.057), ('phase', 0.057), ('images', 0.057), ('bu', 0.055), ('nmr', 0.055), ('reconstructions', 0.055), ('inference', 0.052), ('measurement', 0.051), ('scan', 0.051), ('randomizing', 0.051), ('sj', 0.05), ('measurements', 0.049), ('trajectories', 0.047), ('optimized', 0.047), ('cs', 0.047), ('sequential', 0.045), ('interpolate', 0.045), ('regular', 0.045), ('sequence', 0.044), ('variational', 0.043), ('equispacing', 0.042), ('img', 0.042), ('irls', 0.042), ('lsop', 0.042), ('mans', 0.042), ('nowadays', 0.042), ('nyq', 0.042), ('slew', 0.042), ('xnyq', 0.042), ('fourier', 0.042), ('randomization', 0.041), ('nc', 0.04), ('formation', 0.04), ('xu', 0.039), ('eld', 0.038), ('op', 0.038), ('loop', 0.037), ('compensation', 0.037), ('legendre', 0.037), ('setups', 0.037), ('shot', 0.037), ('diag', 0.036), ('clinical', 0.035), ('seeger', 0.035), ('resolution', 0.034), ('spectrum', 0.034), ('flash', 0.034), ('interleave', 0.034), ('maintained', 0.033), ('posterior', 0.033), ('sparse', 0.033), ('strongly', 0.033), ('experimental', 0.033), ('approximate', 0.033), ('lling', 0.032)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="30-tfidf-1" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>Author: Hannes Nickisch, Rolf Pohmann, Bernhard Schölkopf, Matthias Seeger</p><p>Abstract: We show how improved sequences for magnetic resonance imaging can be found through optimization of Bayesian design scores. Combining approximate Bayesian inference and natural image statistics with high-performance numerical computation, we propose the ﬁrst Bayesian experimental design framework for this problem of high relevance to clinical and brain research. Our solution requires large-scale approximate inference for dense, non-Gaussian models. We propose a novel scalable variational inference algorithm, and show how powerful methods of numerical mathematics can be modiﬁed to compute primitives in our framework. Our approach is evaluated on raw data from a 3T MR scanner. 1</p><p>2 0.085472018 <a title="30-tfidf-2" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>Author: Vikas Sindhwani, Jianying Hu, Aleksandra Mojsilovic</p><p>Abstract: By attempting to simultaneously partition both the rows (examples) and columns (features) of a data matrix, Co-clustering algorithms often demonstrate surprisingly impressive performance improvements over traditional one-sided row clustering techniques. A good clustering of features may be seen as a combinatorial transformation of the data matrix, effectively enforcing a form of regularization that may lead to a better clustering of examples (and vice-versa). In many applications, partial supervision in the form of a few row labels as well as column labels may be available to potentially assist co-clustering. In this paper, we develop two novel semi-supervised multi-class classiﬁcation algorithms motivated respectively by spectral bipartite graph partitioning and matrix approximation formulations for co-clustering. These algorithms (i) support dual supervision in the form of labels for both examples and/or features, (ii) provide principled predictive capability on out-of-sample test data, and (iii) arise naturally from the classical Representer theorem applied to regularization problems posed on a collection of Reproducing Kernel Hilbert Spaces. Empirical results demonstrate the effectiveness and utility of our algorithms. 1</p><p>3 0.078982361 <a title="30-tfidf-3" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>Author: Volkan Cevher, Marco F. Duarte, Chinmay Hegde, Richard Baraniuk</p><p>Abstract: Compressive Sensing (CS) combines sampling and compression into a single subNyquist linear measurement process for sparse and compressible signals. In this paper, we extend the theory of CS to include signals that are concisely represented in terms of a graphical model. In particular, we use Markov Random Fields (MRFs) to represent sparse signals whose nonzero coefﬁcients are clustered. Our new model-based recovery algorithm, dubbed Lattice Matching Pursuit (LaMP), stably recovers MRF-modeled signals using many fewer measurements and computations than the current state-of-the-art algorithms.</p><p>4 0.076799296 <a title="30-tfidf-4" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>Author: Stefan Haufe, Vadim V. Nikulin, Andreas Ziehe, Klaus-Robert Müller, Guido Nolte</p><p>Abstract: We introduce a novel framework for estimating vector ﬁelds using sparse basis ﬁeld expansions (S-FLEX). The notion of basis ﬁelds, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector ﬁeld, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that signiﬁcantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art. 1</p><p>5 0.074858516 <a title="30-tfidf-5" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>Author: Charles Cadieu, Bruno A. Olshausen</p><p>Abstract: We describe a hierarchical, probabilistic model that learns to extract complex motion from movies of the natural environment. The model consists of two hidden layers: the ﬁrst layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables. The second layer learns the higher-order structure among the time-varying phase variables. After training on natural movies, the top layer units discover the structure of phase-shifts within the ﬁrst layer. We show that the top layer units encode transformational invariants: they are selective for the speed and direction of a moving pattern, but are invariant to its spatial structure (orientation/spatial-frequency). The diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in V1 and MT. In addition, the model demonstrates how feedback from higher levels can inﬂuence representations at lower levels as a by-product of inference in a graphical model. 1</p><p>6 0.071487583 <a title="30-tfidf-6" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>7 0.069110215 <a title="30-tfidf-7" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<p>8 0.067809701 <a title="30-tfidf-8" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>9 0.063894987 <a title="30-tfidf-9" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>10 0.059542079 <a title="30-tfidf-10" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>11 0.058387175 <a title="30-tfidf-11" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>12 0.057596207 <a title="30-tfidf-12" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>13 0.0574839 <a title="30-tfidf-13" href="./nips-2008-Cascaded_Classification_Models%3A_Combining_Models_for_Holistic_Scene_Understanding.html">42 nips-2008-Cascaded Classification Models: Combining Models for Holistic Scene Understanding</a></p>
<p>14 0.056728352 <a title="30-tfidf-14" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>15 0.056341946 <a title="30-tfidf-15" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>16 0.05343179 <a title="30-tfidf-16" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>17 0.052926145 <a title="30-tfidf-17" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>18 0.052650265 <a title="30-tfidf-18" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>19 0.05238384 <a title="30-tfidf-19" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>20 0.049961377 <a title="30-tfidf-20" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.184), (1, -0.026), (2, 0.039), (3, 0.01), (4, 0.065), (5, -0.003), (6, -0.052), (7, 0.0), (8, 0.027), (9, 0.052), (10, -0.019), (11, -0.033), (12, 0.021), (13, 0.03), (14, -0.014), (15, -0.022), (16, -0.02), (17, -0.012), (18, 0.037), (19, -0.025), (20, 0.017), (21, 0.038), (22, -0.039), (23, -0.052), (24, -0.029), (25, 0.019), (26, -0.048), (27, 0.095), (28, -0.032), (29, -0.015), (30, -0.019), (31, -0.047), (32, 0.004), (33, -0.107), (34, -0.041), (35, -0.014), (36, 0.142), (37, 0.058), (38, 0.039), (39, 0.091), (40, -0.082), (41, -0.053), (42, -0.086), (43, 0.028), (44, 0.098), (45, 0.021), (46, -0.052), (47, 0.072), (48, 0.02), (49, -0.193)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.91318762 <a title="30-lsi-1" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>Author: Hannes Nickisch, Rolf Pohmann, Bernhard Schölkopf, Matthias Seeger</p><p>Abstract: We show how improved sequences for magnetic resonance imaging can be found through optimization of Bayesian design scores. Combining approximate Bayesian inference and natural image statistics with high-performance numerical computation, we propose the ﬁrst Bayesian experimental design framework for this problem of high relevance to clinical and brain research. Our solution requires large-scale approximate inference for dense, non-Gaussian models. We propose a novel scalable variational inference algorithm, and show how powerful methods of numerical mathematics can be modiﬁed to compute primitives in our framework. Our approach is evaluated on raw data from a 3T MR scanner. 1</p><p>2 0.60011268 <a title="30-lsi-2" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>Author: Stefan Haufe, Vadim V. Nikulin, Andreas Ziehe, Klaus-Robert Müller, Guido Nolte</p><p>Abstract: We introduce a novel framework for estimating vector ﬁelds using sparse basis ﬁeld expansions (S-FLEX). The notion of basis ﬁelds, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector ﬁeld, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that signiﬁcantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art. 1</p><p>3 0.52421176 <a title="30-lsi-3" href="./nips-2008-Inferring_rankings_under_constrained_sensing.html">106 nips-2008-Inferring rankings under constrained sensing</a></p>
<p>Author: Srikanth Jagabathula, Devavrat Shah</p><p>Abstract: Motivated by applications like elections, web-page ranking, revenue maximization etc., we consider the question of inferring popular rankings using constrained data. More speciﬁcally, we consider the problem of inferring a probability distribution over the group of permutations using its ﬁrst order marginals. We ﬁrst prove that it is not possible to recover more than O(n) permutations over n elements with the given information. We then provide a simple and novel algorithm that can recover up to O(n) permutations under a natural stochastic model; in this sense, the algorithm is optimal. In certain applications, the interest is in recovering only the most popular (or mode) ranking. As a second result, we provide an algorithm based on the Fourier Transform over the symmetric group to recover the mode under a natural majority condition; the algorithm turns out to be a maximum weight matching on an appropriately deﬁned weighted bipartite graph. The questions considered are also thematically related to Fourier Transforms over the symmetric group and the currently popular topic of compressed sensing. 1</p><p>4 0.49062234 <a title="30-lsi-4" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>Author: Vikas Sindhwani, Jianying Hu, Aleksandra Mojsilovic</p><p>Abstract: By attempting to simultaneously partition both the rows (examples) and columns (features) of a data matrix, Co-clustering algorithms often demonstrate surprisingly impressive performance improvements over traditional one-sided row clustering techniques. A good clustering of features may be seen as a combinatorial transformation of the data matrix, effectively enforcing a form of regularization that may lead to a better clustering of examples (and vice-versa). In many applications, partial supervision in the form of a few row labels as well as column labels may be available to potentially assist co-clustering. In this paper, we develop two novel semi-supervised multi-class classiﬁcation algorithms motivated respectively by spectral bipartite graph partitioning and matrix approximation formulations for co-clustering. These algorithms (i) support dual supervision in the form of labels for both examples and/or features, (ii) provide principled predictive capability on out-of-sample test data, and (iii) arise naturally from the classical Representer theorem applied to regularization problems posed on a collection of Reproducing Kernel Hilbert Spaces. Empirical results demonstrate the effectiveness and utility of our algorithms. 1</p><p>5 0.47034404 <a title="30-lsi-5" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>Author: Alexandre Bouchard-côté, Dan Klein, Michael I. Jordan</p><p>Abstract: Accurate and efﬁcient inference in evolutionary trees is a central problem in computational biology. While classical treatments have made unrealistic site independence assumptions, ignoring insertions and deletions, realistic approaches require tracking insertions and deletions along the phylogenetic tree—a challenging and unsolved computational problem. We propose a new ancestry resampling procedure for inference in evolutionary trees. We evaluate our method in two problem domains—multiple sequence alignment and reconstruction of ancestral sequences—and show substantial improvement over the current state of the art. 1</p><p>6 0.46899205 <a title="30-lsi-6" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>7 0.45483208 <a title="30-lsi-7" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>8 0.45479077 <a title="30-lsi-8" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>9 0.4474107 <a title="30-lsi-9" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<p>10 0.42205924 <a title="30-lsi-10" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>11 0.41727331 <a title="30-lsi-11" href="./nips-2008-Fast_Computation_of_Posterior_Mode_in_Multi-Level_Hierarchical_Models.html">82 nips-2008-Fast Computation of Posterior Mode in Multi-Level Hierarchical Models</a></p>
<p>12 0.41646093 <a title="30-lsi-12" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>13 0.41460499 <a title="30-lsi-13" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>14 0.40765631 <a title="30-lsi-14" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>15 0.40625495 <a title="30-lsi-15" href="./nips-2008-Playing_Pinball_with_non-invasive_BCI.html">180 nips-2008-Playing Pinball with non-invasive BCI</a></p>
<p>16 0.39768755 <a title="30-lsi-16" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>17 0.37744617 <a title="30-lsi-17" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>18 0.37120816 <a title="30-lsi-18" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>19 0.37004492 <a title="30-lsi-19" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>20 0.36855581 <a title="30-lsi-20" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(4, 0.013), (6, 0.062), (7, 0.068), (10, 0.271), (12, 0.044), (15, 0.019), (28, 0.154), (57, 0.095), (59, 0.023), (63, 0.043), (71, 0.016), (77, 0.043), (78, 0.017), (83, 0.044), (94, 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77442211 <a title="30-lda-1" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>Author: Hannes Nickisch, Rolf Pohmann, Bernhard Schölkopf, Matthias Seeger</p><p>Abstract: We show how improved sequences for magnetic resonance imaging can be found through optimization of Bayesian design scores. Combining approximate Bayesian inference and natural image statistics with high-performance numerical computation, we propose the ﬁrst Bayesian experimental design framework for this problem of high relevance to clinical and brain research. Our solution requires large-scale approximate inference for dense, non-Gaussian models. We propose a novel scalable variational inference algorithm, and show how powerful methods of numerical mathematics can be modiﬁed to compute primitives in our framework. Our approach is evaluated on raw data from a 3T MR scanner. 1</p><p>2 0.61064398 <a title="30-lda-2" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>Author: Ling Huang, Donghui Yan, Nina Taft, Michael I. Jordan</p><p>Abstract: Spectral clustering is useful for a wide-ranging set of applications in areas such as biological data analysis, image processing and data mining. However, the computational and/or communication resources required by the method in processing large-scale data are often prohibitively high, and practitioners are often required to perturb the original data in various ways (quantization, downsampling, etc) before invoking a spectral algorithm. In this paper, we use stochastic perturbation theory to study the effects of data perturbation on the performance of spectral clustering. We show that the error under perturbation of spectral clustering is closely related to the perturbation of the eigenvectors of the Laplacian matrix. From this result we derive approximate upper bounds on the clustering error. We show that this bound is tight empirically across a wide range of problems, suggesting that it can be used in practical settings to determine the amount of data reduction allowed in order to meet a speciﬁcation of permitted loss in clustering performance. 1</p><p>3 0.60857689 <a title="30-lda-3" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>4 0.60760313 <a title="30-lda-4" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>Author: Xiaodi Hou, Liqing Zhang</p><p>Abstract: A visual attention system should respond placidly when common stimuli are presented, while at the same time keep alert to anomalous visual inputs. In this paper, a dynamic visual attention model based on the rarity of features is proposed. We introduce the Incremental Coding Length (ICL) to measure the perspective entropy gain of each feature. The objective of our model is to maximize the entropy of the sampled visual features. In order to optimize energy consumption, the limit amount of energy of the system is re-distributed amongst features according to their Incremental Coding Length. By selecting features with large coding length increments, the computational system can achieve attention selectivity in both static and dynamic scenes. We demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation. Moreover, we also show that our model captures several less-reported dynamic visual search behaviors, such as attentional swing and inhibition of return. 1</p><p>5 0.6064865 <a title="30-lda-5" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>Author: Minh H. Nguyen, Fernando Torre</p><p>Abstract: Kernel Principal Component Analysis (KPCA) is a popular generalization of linear PCA that allows non-linear feature extraction. In KPCA, data in the input space is mapped to higher (usually) dimensional feature space where the data can be linearly modeled. The feature space is typically induced implicitly by a kernel function, and linear PCA in the feature space is performed via the kernel trick. However, due to the implicitness of the feature space, some extensions of PCA such as robust PCA cannot be directly generalized to KPCA. This paper presents a technique to overcome this problem, and extends it to a uniﬁed framework for treating noise, missing data, and outliers in KPCA. Our method is based on a novel cost function to perform inference in KPCA. Extensive experiments, in both synthetic and real data, show that our algorithm outperforms existing methods. 1</p><p>6 0.60583705 <a title="30-lda-6" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>7 0.60518271 <a title="30-lda-7" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>8 0.60399836 <a title="30-lda-8" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>9 0.60238034 <a title="30-lda-9" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>10 0.60191625 <a title="30-lda-10" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>11 0.60068887 <a title="30-lda-11" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>12 0.60049987 <a title="30-lda-12" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>13 0.60033524 <a title="30-lda-13" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>14 0.6001876 <a title="30-lda-14" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>15 0.59988868 <a title="30-lda-15" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>16 0.59951729 <a title="30-lda-16" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>17 0.59912312 <a title="30-lda-17" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>18 0.59784746 <a title="30-lda-18" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>19 0.59772313 <a title="30-lda-19" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>20 0.59688556 <a title="30-lda-20" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
