<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-19" href="#">nips2008-19</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</h1>
<br/><p>Source: <a title="nips-2008-19-pdf" href="http://papers.nips.cc/paper/3407-an-empirical-analysis-of-domain-adaptation-algorithms-for-genomic-sequence-analysis.pdf">pdf</a></p><p>Author: Gabriele Schweikert, Gunnar Rätsch, Christian Widmer, Bernhard Schölkopf</p><p>Abstract: We study the problem of domain transfer for a supervised classiﬁcation task in mRNA splicing. We consider a number of recent domain transfer methods from machine learning, including some that are novel, and evaluate them on genomic sequence data from model organisms of varying evolutionary distance. We ﬁnd that in cases where the organisms are not closely related, the use of domain adaptation methods can help improve classiﬁcation performance.</p><p>Reference: <a title="nips-2008-19-reference" href="../nips2008_reference/nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 39, 72070 T¨ bingen, Germany u ZBIT, T¨ bingen University u Sand 14, 72076 T¨ bingen, Germany u Christian. [sent-6, score-0.089]
</p><p>2 de  Abstract We study the problem of domain transfer for a supervised classiﬁcation task in mRNA splicing. [sent-18, score-0.22]
</p><p>3 We consider a number of recent domain transfer methods from machine learning, including some that are novel, and evaluate them on genomic sequence data from model organisms of varying evolutionary distance. [sent-19, score-0.789]
</p><p>4 We ﬁnd that in cases where the organisms are not closely related, the use of domain adaptation methods can help improve classiﬁcation performance. [sent-20, score-0.692]
</p><p>5 1 Introduction Ten years ago, an eight-year lasting collaborative effort resulted in the ﬁrst completely sequenced genome of a multi-cellular organism, the free-living nematode Caenorhabditis elegans. [sent-21, score-0.122]
</p><p>6 Today, a decade after the accomplishment of this landmark, 23 eukaryotic genomes have been completed and more than 400 are underway. [sent-22, score-0.051]
</p><p>7 The genomic sequence builds the basis for a large body of research on understanding the biochemical processes in these organisms. [sent-23, score-0.159]
</p><p>8 Typically, the more closely related the organisms are, the more similar the biochemical processes. [sent-24, score-0.488]
</p><p>9 It is the hope of biological research that by analyzing a wide spectrum of model organisms, one can approach an understanding of the full biological complexity. [sent-25, score-0.144]
</p><p>10 For some organisms, certain biochemical experiments can be performed more readily than for others, facilitating the analysis of particular processes. [sent-26, score-0.141]
</p><p>11 This is but one example of a situation where transfer of knowledge across domains is fruitful. [sent-28, score-0.137]
</p><p>12 In machine learning, the above information transfer is called domain adaptation, where one aims to use data or a model of a well-analyzed source domain to obtain or reﬁne a model for a less analyzed target domain. [sent-29, score-0.922]
</p><p>13 For supervised classiﬁcation, this corresponds to the case where there are ample labeled examples (xi , yi ), i = 1, . [sent-30, score-0.128]
</p><p>14 , m for the source domain, but only few such examples (xi , yi ), i = m + 1, . [sent-33, score-0.451]
</p><p>15 The examples are assumed to be drawn independently from the joint probability distributions PS (X, Y ) and PT (X, Y ), respectively. [sent-37, score-0.079]
</p><p>16 The distributions PS (X, Y ) = PS (Y |X) · PS (X) and PT (X, Y ) = PT (Y |X) · PT (X) can differ in several ways: (1) In the classical covariate shift case, it is assumed that only the distributions of the input features P (X) varies between the two domains: PS (X) = PT (X). [sent-38, score-0.111]
</p><p>17 For a given feature vector x the label y is thus independent of the domain from which the example stems. [sent-41, score-0.145]
</p><p>18 An example thereof would be if a function of some biological material is conserved between two organisms, but its composition has changed (e. [sent-42, score-0.116]
</p><p>19 (2) In a more difﬁcult scenario the conditionals differ between domains, PS (Y |X) = PT (Y |X), while P (X) may or may not vary. [sent-45, score-0.043]
</p><p>20 Here, two organisms may have evolved from a common ancestor and a certain biological function may have changed due to evolutionary pressures. [sent-47, score-0.633]
</p><p>21 The evolutionary distance may be a good indicator for how well the function is conserved. [sent-48, score-0.137]
</p><p>22 If this distance is small, we have reason to believe that the conditionals may not be completely different, and knowledge of one of them should then provide us with some information also about the other one. [sent-49, score-0.1]
</p><p>23 While such knowledge transfer is crucial for biology, and performed by biologists on a daily basis, surprisingly little work has been done to exploit it using machine learning methods on biological databases. [sent-50, score-0.194]
</p><p>24 In Section 3 we will brieﬂy review a number of known algorithms for domain adaptation, and propose certain variations. [sent-53, score-0.172]
</p><p>25 1 A Family of Classiﬁcation Problems We consider the task of identifying so-called acceptor splice sites within a large set of potential splice sites based on a sequence window around a site. [sent-56, score-0.496]
</p><p>26 The idea is to consider the recognition of splice sites in different organisms: In all cases, we used the very well studied model organism C. [sent-57, score-0.435]
</p><p>27 As target organisms we chose two additional nematodes, namely, the close relative C. [sent-59, score-0.673]
</p><p>28 elegans 100 million years ago [10], and the more distantly related P. [sent-61, score-0.459]
</p><p>29 thaliana, which has diverged from the other organisms more than 1,600 million years ago. [sent-68, score-0.636]
</p><p>30 It is assumed that a larger evolutionary distance will likely also have led to an accumulation of functional differences in the molecular splicing machinery. [sent-69, score-0.198]
</p><p>31 We therefore expect that the differences of classiﬁcation functions for recognizing splice sites in these organisms will increase with increasing evolutionary distance. [sent-70, score-0.762]
</p><p>32 2 The Classiﬁcation Model It has been demonstrated that Support Vector Machines (SVMs) [1] are well suited for the task of splice site predictions across a wide range of organisms [9]. [sent-72, score-0.643]
</p><p>33 In our previous study we have used sequences of length L = 140 and substrings of length ℓ = 22 for splice site detection [9]. [sent-74, score-0.284]
</p><p>34 Moreover, to archive the best classiﬁcation performance, a large number of training examples is very helpful ([9] used up to 10 million examples). [sent-76, score-0.242]
</p><p>35 For the designed experimental comparison we had to run all algorithms many times for different training set sizes, organisms and model parameters. [sent-77, score-0.465]
</p><p>36 We chose the source and target training set as large as possible–in our case at most 100,000 examples per domain. [sent-78, score-0.724]
</p><p>37 Moreover, not for all algorithms we had efﬁcient implementations available that can make use of kernels. [sent-79, score-0.053]
</p><p>38 Note, that this choice does not limit the generality of this study, as there is no strong reason, why efﬁcient implementations that employ kernels could not be developed for all methods. [sent-84, score-0.059]
</p><p>39 We therefore refrained from including more methods, examples or dimensions. [sent-87, score-0.079]
</p><p>40 3 Splits and Model Selection In the ﬁrst set of experiments we randomly selected a source dataset of 100,000 examples from C. [sent-89, score-0.402]
</p><p>41 elegans, while data sets of sizes 2,500, 6,500, 16,000, 40,000 and 100,000 were selected for each target organism. [sent-90, score-0.27]
</p><p>42 Subsequently we performed a second set of experiments where we combined several sources. [sent-91, score-0.034]
</p><p>43 For our comparison we used 25,000 labeled examples from each of four remaining organisms to predict on a target organism. [sent-92, score-0.721]
</p><p>44 Two thirds of each target set were used for training, while one third was used for evaluation in the course of hyper-parameter tuning. [sent-94, score-0.234]
</p><p>45 1 Additionally, test sets of 60,000 examples were set aside for each target organism. [sent-95, score-0.313]
</p><p>46 All experiments were repeated three times with different training splits (source and target), except the last one which always used the full data set. [sent-96, score-0.085]
</p><p>47 In this paper, we will therefore drop the classical covariate shift assumption, and allow for different predictive functions PS (Y |X) = PT (Y |X). [sent-100, score-0.111]
</p><p>48 1 Baseline Methods (SVMS and SVMT ) As baseline methods for the comparison we consider two methods: (a) training on the source data only (SVMS ) and (b) training on the target data only (SVMT ). [sent-102, score-0.671]
</p><p>49 For SVMS we use the source data for training however we tune the hyper-parameter on the available target data. [sent-103, score-0.639]
</p><p>50 For SVMT we use the available target data for training (67%) and model selection (33%). [sent-104, score-0.316]
</p><p>51 2 Convex Combination (SVMS +SVMT ) The most straightforward idea for domain adaptation is to reuse the two optimal functions fT and fS as generated by the base line methods SVMS and SVMT and combine them in a convex manner: F (x) = αfT (x) + (1 − α)fS (x). [sent-111, score-0.36]
</p><p>52 Here, α ∈ [0, 1] is the convex combination parameter that is tuned on the evaluation set (33%) of the target domain. [sent-112, score-0.338]
</p><p>53 3 Weighted Combination (SVMS+T ) Another simple idea is to train the method on the union of source and target data. [sent-115, score-0.611]
</p><p>54 The relative importance of each domain is integrated into the loss term of the SVM and can be adjusted by setting domain-dependent cost parameters CS and CT for the m and n training examples from the source and target domain, respectively: min w,ξ  s. [sent-116, score-0.838]
</p><p>55 1 w 2  m 2  m+n  + CS  ξi + CT i=1  ξi  (2)  i=m+1  yi ( w, Φ(xi ) + b) ≥ 1 − ξi ξi ≥ 0 ∀i ∈ [1, m + n]  ∀i ∈ [1, m + n]  This method has two model parameters and requires training on the union of the training sets. [sent-118, score-0.163]
</p><p>56 Since the computation time of most classiﬁcation methods increases super-linearly and full model selection may require to train many parameter combinations, this approach is computationally quite demanding. [sent-119, score-0.026]
</p><p>57 4  One way of extending the weighted combination approach is a variant of multi-task learning [2]. [sent-121, score-0.082]
</p><p>58 The idea is to solve the source and target classiﬁcation problems simultaneously and couple the two solutions via a regularization term. [sent-122, score-0.585]
</p><p>59 This idea can be realized by the following optimization problem: min  wS ,wT ,ξ  s. [sent-123, score-0.028]
</p><p>60 1 wS − wT 2  m+n 2  ξi  +C  (3)  i=1  yi ( wS , Φ(xi ) + b) ≥ 1 − ξi yi ( wT , Φ(xi ) + b) ≥ 1 − ξi ξi ≥ 0  ∀i ∈ 1, . [sent-125, score-0.098]
</p><p>61 Hence, we decided to approximate the soft-margin loss using the logistic loss l(f (x), y) = log(1+exp(−yf (x))) and to use a conjugate gradient method3 to minimize the resulting objective function in terms of wS and wT . [sent-137, score-0.035]
</p><p>62 5 Kernel Mean Matching (SVMS→T ) Kernel methods map the data into a reproducing kernel Hilbert space (RKHS) by means of a mapping Φ : X → H related to a positive deﬁnite kernel via k(x, x′ ) = Φ(x), Φ(x′ ) . [sent-139, score-0.068]
</p><p>63 In fact, it turns out that for a certain class of kernels, the mapping n 1 µ : (x1 , . [sent-144, score-0.027]
</p><p>64 It is often not necessary to retain all information (indeed, it may be useful to specify which information we want to retain and which one we want to disregard, see [8]). [sent-150, score-0.062]
</p><p>65 In [6] it was proposed that one could use this for covariate shift adaptation, moving the mean of a source distribution (over the inputs only) towards the mean of a target distribution by re-weighting the source training points. [sent-152, score-1.048]
</p><p>66 We have applied this to our problem, but found that a variant of this approach performed better. [sent-153, score-0.06]
</p><p>67 In this variant, we do not re-weight the source points, but rather we translate each point towards the mean of the target inputs: ˆ Φ(xj ) = Φ(xj ) − α  1 m  m  m+n  i=1  Φ(xi ) −  1 Φ(xi ) n i=m+1  ∀j = 1, . [sent-154, score-0.557]
</p><p>68 This also leads to a modiﬁed source input distribution which is statistically more similar to the target distribution and which can thus be used to improve performance when training the target task. [sent-158, score-0.848]
</p><p>69 Unlike [6], we do have a certain amount of labels also for the target distribution. [sent-159, score-0.261]
</p><p>70 We make use of them by performing the shift separately for each class y ∈ {±1}: ˆ Φ(xj ) = Φ(xj ) − α  1 my  m  i=1  [[yi = y]]Φ(xi ) −  1 ny  m+n  [[yi = y]]Φ(xi ) i=m+1  for all j = m + 1, . [sent-160, score-0.061]
</p><p>71 , m + n with yj = y, where my and ny are the number of source and target examples with label y, respectively. [sent-163, score-0.636]
</p><p>72 The shifted examples can now be used in different ways to obtain a ﬁnal classiﬁer. [sent-164, score-0.112]
</p><p>73 We decided to use the weighted combination with CS = CT for comparison. [sent-165, score-0.091]
</p><p>74 6 Feature Augmentation (SVMS×T ) In [3] a method was proposed that augments the features of source and target examples in a domainspeciﬁc way: ˆ Φ(x) = (Φ(x), Φ(x), 0)⊤ for i = 1, . [sent-167, score-0.66]
</p><p>75 The intuition behind this idea is that there exist one set of parameters that models the properties common to both sets and two additional sets of parameters that model the speciﬁcs of the two domains. [sent-174, score-0.028]
</p><p>76 7 Combination of Several Sources Most of the above algorithms can be extended in one way or another to integrate several source domains. [sent-178, score-0.323]
</p><p>77 In this work we consider only three possible algorithms: (a) convex combinations of several domains, (b) KMM on several domains and (c) an extension of the dual-task learning approach to multi-task learning. [sent-179, score-0.145]
</p><p>78 We brieﬂy describe these methods below: Multiple Convex Combinations (M-SVMS +SVMT ) The most general version would be to optimize all convex combination coefﬁcients independently. [sent-180, score-0.104]
</p><p>79 If done in a grid-search-like manner, it becomes prohibitive for more than say three source domains. [sent-181, score-0.323]
</p><p>80 In preliminary experiments we tried both approaches and they typically did not lead to better results than the following combination: 1 F (x) = αfT (x) + (1 − α) fS (x), |S| S∈S  where S is the set of all considered source domains. [sent-183, score-0.348]
</p><p>81 Multiple KMM (M-SVMS→T ) Here, we shift the source examples of each domain independently towards the target examples, but by the same relative distance (α). [sent-185, score-0.873]
</p><p>82 Then we train one classiﬁer on the shifted source examples as well as the target examples. [sent-186, score-0.695]
</p><p>83 1 2  D1 ∈D D2 ∈D  γD1 ,D2 wD1 − wD2  yi ( wDj , Φ(xi ) + b) ≥ 1 − ξi ξi ≥ 0  2  +  ξi  (4)  i  (5)  for all examples (xi , yi ) in domain Dj ∈ D, where D is the set of all considered domains. [sent-189, score-0.347]
</p><p>84 γ is a set of regularization parameters, which we parametrized by two parameters CS and CT in the following way: γD1 ,D2 = CS if D1 and D2 are source domains and CT otherwise. [sent-190, score-0.385]
</p><p>85 For the ﬁrst experiment we assume that there is one source domain with enough data that should be used to improve the performance in the target domain. [sent-192, score-0.702]
</p><p>86 In the second setting we analyze whether one can beneﬁt from several source domains. [sent-193, score-0.323]
</p><p>87 1 Single Source Domain Due to space constraints, we restrict ourselves to presenting a summary of our results with a focus on best and worst performing methods. [sent-195, score-0.037]
</p><p>88 The summary is given in Figure 1, where we illustrate which method performed best (green), similarly well (within a conﬁdence √ interval of σ/ n) as the best (light green), considerably worse than the best (yellow), not signiﬁcantly better than the worst (light red) or worst (red). [sent-197, score-0.108]
</p><p>89 Independent of the task, if there is very little target data available, the training on source data performs much better than training on the target data. [sent-199, score-0.905]
</p><p>90 Conversely, if there is much target data available then training on it easily outperforms training the source data. [sent-200, score-0.696]
</p><p>91 For a larger evolutionary distance of the target organisms to source organism C. [sent-202, score-1.261]
</p><p>92 elegans, a relatively small number of target training examples for the SVMT approach is sufﬁcient to achieve similar performance to the SVMS approach, which is always trained on 100,000 examples. [sent-203, score-0.37]
</p><p>93 We call the number of target examples with equal source and target performance the break-even point. [sent-204, score-0.87]
</p><p>94 remanei one needs nearly as many target data as source data to achieve the same performance. [sent-206, score-0.648]
</p><p>95 thaliana, less than 10% target data is sufﬁcient to outperform the source model. [sent-208, score-0.557]
</p><p>96 In almost all cases, the performance of domain adaption algorithms is considerably higher than source (SVMS ) and target only (SVMT ). [sent-210, score-0.729]
</p><p>97 Among the domain adaptation algorithms, the dual-task learning approach (SVMS,T ) performed most often best (12/20 cases). [sent-217, score-0.318]
</p><p>98 Second most often best (5/20) performed the convex combination approach (SVMS +SVMT ). [sent-218, score-0.138]
</p><p>99 From our observations we can conclude that the simple convex combination approach works surprisingly well. [sent-219, score-0.129]
</p><p>100 It is only outperformed by the dual-task learning algorithm which performs consistently well for all organisms and target training set sizes. [sent-220, score-0.699]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('organisms', 0.408), ('svmt', 0.334), ('source', 0.323), ('svms', 0.246), ('target', 0.234), ('ps', 0.196), ('splice', 0.186), ('elegans', 0.182), ('pt', 0.162), ('organism', 0.159), ('domain', 0.145), ('adaptation', 0.139), ('ws', 0.129), ('evolutionary', 0.106), ('cs', 0.1), ('remanei', 0.091), ('bingen', 0.089), ('fs', 0.082), ('diverged', 0.08), ('biochemical', 0.08), ('ago', 0.08), ('million', 0.079), ('examples', 0.079), ('transfer', 0.075), ('ct', 0.074), ('ft', 0.073), ('years', 0.069), ('germany', 0.063), ('domains', 0.062), ('sites', 0.062), ('shift', 0.061), ('auprc', 0.061), ('miescher', 0.061), ('splicing', 0.061), ('thaliana', 0.061), ('biological', 0.06), ('wt', 0.06), ('xi', 0.058), ('training', 0.057), ('combination', 0.056), ('genomic', 0.055), ('friedrich', 0.053), ('classi', 0.05), ('covariate', 0.05), ('yi', 0.049), ('site', 0.049), ('distantly', 0.049), ('mrna', 0.049), ('kmm', 0.049), ('substrings', 0.049), ('convex', 0.048), ('conditionals', 0.043), ('substring', 0.043), ('xj', 0.038), ('worst', 0.037), ('wd', 0.037), ('sizes', 0.036), ('decided', 0.035), ('distant', 0.035), ('combinations', 0.035), ('kernel', 0.034), ('performed', 0.034), ('shifted', 0.033), ('changed', 0.032), ('planck', 0.032), ('retain', 0.031), ('distance', 0.031), ('kernels', 0.031), ('chose', 0.031), ('implementations', 0.028), ('splits', 0.028), ('cation', 0.028), ('idea', 0.028), ('certain', 0.027), ('eukaryotic', 0.027), ('adaption', 0.027), ('archive', 0.027), ('chromosome', 0.027), ('cplex', 0.027), ('cus', 0.027), ('domainspeci', 0.027), ('gunnar', 0.027), ('sand', 0.027), ('sequenced', 0.027), ('completely', 0.026), ('variant', 0.026), ('train', 0.026), ('available', 0.025), ('appendix', 0.025), ('surprisingly', 0.025), ('laboratory', 0.025), ('considered', 0.025), ('green', 0.024), ('understanding', 0.024), ('conserved', 0.024), ('genomes', 0.024), ('lineage', 0.024), ('assay', 0.024), ('augmentation', 0.024), ('augments', 0.024)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9999997 <a title="19-tfidf-1" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>Author: Gabriele Schweikert, Gunnar Rätsch, Christian Widmer, Bernhard Schölkopf</p><p>Abstract: We study the problem of domain transfer for a supervised classiﬁcation task in mRNA splicing. We consider a number of recent domain transfer methods from machine learning, including some that are novel, and evaluate them on genomic sequence data from model organisms of varying evolutionary distance. We ﬁnd that in cases where the organisms are not closely related, the use of domain adaptation methods can help improve classiﬁcation performance.</p><p>2 0.20407473 <a title="19-tfidf-2" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>Author: Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents a theoretical analysis of the problem of domain adaptation with multiple sources. For each source domain, the distribution over the input points as well as a hypothesis with error at most ǫ are given. The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions beneﬁt from favorable theoretical guarantees. Our main result shows that, remarkably, for any ﬁxed target function, there exists a distribution weighted combining rule that has a loss of at most ǫ with respect to any target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3ǫ. Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.</p><p>3 0.13015816 <a title="19-tfidf-3" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>Author: Steffen Bickel, Christoph Sawade, Tobias Scheffer</p><p>Abstract: We address the problem of learning classiﬁers for several related tasks that may differ in their joint distribution of input and output variables. For each task, small – possibly even empty – labeled samples and large unlabeled samples are available. While the unlabeled samples reﬂect the target distribution, the labeled samples may be biased. This setting is motivated by the problem of predicting sociodemographic features for users of web portals, based on the content which they have accessed. Here, questionnaires offered to a portion of each portal’s users produce biased samples. We derive a transfer learning procedure that produces resampling weights which match the pool of all examples to the target distribution of any given task. Transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy. 1</p><p>4 0.10339432 <a title="19-tfidf-4" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>Author: Adrian Haith, Carl P. Jackson, R. C. Miall, Sethu Vijayakumar</p><p>Abstract: Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a uniﬁed model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereﬀects. This uniﬁed model also makes the surprising prediction that force ﬁeld adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We conﬁrm this prediction with an experiment. 1</p><p>5 0.091928326 <a title="19-tfidf-5" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>Author: Julia Owen, Hagai T. Attias, Kensuke Sekihara, Srikantan S. Nagarajan, David P. Wipf</p><p>Abstract: The synchronous brain activity measured via MEG (or EEG) can be interpreted as arising from a collection (possibly large) of current dipoles or sources located throughout the cortex. Estimating the number, location, and orientation of these sources remains a challenging task, one that is signiﬁcantly compounded by the effects of source correlations and the presence of interference from spontaneous brain activity, sensor noise, and other artifacts. This paper derives an empirical Bayesian method for addressing each of these issues in a principled fashion. The resulting algorithm guarantees descent of a cost function uniquely designed to handle unknown orientations and arbitrary correlations. Robust interference suppression is also easily incorporated. In a restricted setting, the proposed method is shown to have theoretically zero bias estimating both the location and orientation of multi-component dipoles even in the presence of correlations, unlike a variety of existing Bayesian localization methods or common signal processing techniques such as beamforming and sLORETA. Empirical results on both simulated and real data sets verify the efﬁcacy of this approach. 1</p><p>6 0.09077125 <a title="19-tfidf-6" href="./nips-2008-A_mixture_model_for_the_evolution_of_gene_expression_in_non-homogeneous_datasets.html">9 nips-2008-A mixture model for the evolution of gene expression in non-homogeneous datasets</a></p>
<p>7 0.084936306 <a title="19-tfidf-7" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>8 0.074309207 <a title="19-tfidf-8" href="./nips-2008-Empirical_performance_maximization_for_linear_rank_statistics.html">72 nips-2008-Empirical performance maximization for linear rank statistics</a></p>
<p>9 0.074278735 <a title="19-tfidf-9" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>10 0.069665119 <a title="19-tfidf-10" href="./nips-2008-Translated_Learning%3A_Transfer_Learning_across_Different_Feature_Spaces.html">242 nips-2008-Translated Learning: Transfer Learning across Different Feature Spaces</a></p>
<p>11 0.069437496 <a title="19-tfidf-11" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>12 0.067659676 <a title="19-tfidf-12" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>13 0.059841901 <a title="19-tfidf-13" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>14 0.059774831 <a title="19-tfidf-14" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>15 0.057058722 <a title="19-tfidf-15" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>16 0.05697798 <a title="19-tfidf-16" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>17 0.05629183 <a title="19-tfidf-17" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>18 0.054774515 <a title="19-tfidf-18" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>19 0.054438606 <a title="19-tfidf-19" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>20 0.052210033 <a title="19-tfidf-20" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.184), (1, -0.026), (2, -0.035), (3, 0.014), (4, -0.015), (5, 0.085), (6, 0.023), (7, 0.075), (8, 0.06), (9, 0.041), (10, 0.101), (11, 0.07), (12, -0.098), (13, -0.008), (14, 0.021), (15, 0.011), (16, 0.009), (17, 0.067), (18, -0.104), (19, 0.121), (20, 0.176), (21, 0.061), (22, -0.148), (23, 0.007), (24, -0.091), (25, 0.089), (26, -0.053), (27, 0.035), (28, 0.01), (29, 0.17), (30, -0.025), (31, 0.053), (32, -0.043), (33, 0.026), (34, 0.108), (35, 0.134), (36, 0.01), (37, 0.044), (38, 0.022), (39, 0.031), (40, 0.001), (41, -0.026), (42, 0.117), (43, -0.026), (44, 0.124), (45, -0.021), (46, -0.086), (47, 0.02), (48, 0.163), (49, 0.116)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.94421631 <a title="19-lsi-1" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>Author: Gabriele Schweikert, Gunnar Rätsch, Christian Widmer, Bernhard Schölkopf</p><p>Abstract: We study the problem of domain transfer for a supervised classiﬁcation task in mRNA splicing. We consider a number of recent domain transfer methods from machine learning, including some that are novel, and evaluate them on genomic sequence data from model organisms of varying evolutionary distance. We ﬁnd that in cases where the organisms are not closely related, the use of domain adaptation methods can help improve classiﬁcation performance.</p><p>2 0.77709937 <a title="19-lsi-2" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>Author: Yishay Mansour, Mehryar Mohri, Afshin Rostamizadeh</p><p>Abstract: This paper presents a theoretical analysis of the problem of domain adaptation with multiple sources. For each source domain, the distribution over the input points as well as a hypothesis with error at most ǫ are given. The problem consists of combining these hypotheses to derive a hypothesis with small error with respect to the target domain. We present several theoretical results relating to this problem. In particular, we prove that standard convex combinations of the source hypotheses may in fact perform very poorly and that, instead, combinations weighted by the source distributions beneﬁt from favorable theoretical guarantees. Our main result shows that, remarkably, for any ﬁxed target function, there exists a distribution weighted combining rule that has a loss of at most ǫ with respect to any target mixture of the source distributions. We further generalize the setting from a single target function to multiple consistent target functions and show the existence of a combining rule with error at most 3ǫ. Finally, we report empirical results for a multiple source adaptation problem with a real-world dataset.</p><p>3 0.68368846 <a title="19-lsi-3" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>Author: Steffen Bickel, Christoph Sawade, Tobias Scheffer</p><p>Abstract: We address the problem of learning classiﬁers for several related tasks that may differ in their joint distribution of input and output variables. For each task, small – possibly even empty – labeled samples and large unlabeled samples are available. While the unlabeled samples reﬂect the target distribution, the labeled samples may be biased. This setting is motivated by the problem of predicting sociodemographic features for users of web portals, based on the content which they have accessed. Here, questionnaires offered to a portion of each portal’s users produce biased samples. We derive a transfer learning procedure that produces resampling weights which match the pool of all examples to the target distribution of any given task. Transfer learning enables us to make predictions even for new portals with few or no training data and improves the overall prediction accuracy. 1</p><p>4 0.56729764 <a title="19-lsi-4" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<p>Author: Adrian Haith, Carl P. Jackson, R. C. Miall, Sethu Vijayakumar</p><p>Abstract: Adaptation of visually guided reaching movements in novel visuomotor environments (e.g. wearing prism goggles) comprises not only motor adaptation but also substantial sensory adaptation, corresponding to shifts in the perceived spatial location of visual and proprioceptive cues. Previous computational models of the sensory component of visuomotor adaptation have assumed that it is driven purely by the discrepancy introduced between visual and proprioceptive estimates of hand position and is independent of any motor component of adaptation. We instead propose a uniﬁed model in which sensory and motor adaptation are jointly driven by optimal Bayesian estimation of the sensory and motor contributions to perceived errors. Our model is able to account for patterns of performance errors during visuomotor adaptation as well as the subsequent perceptual aftereﬀects. This uniﬁed model also makes the surprising prediction that force ﬁeld adaptation will elicit similar perceptual shifts, even though there is never any discrepancy between visual and proprioceptive observations. We conﬁrm this prediction with an experiment. 1</p><p>5 0.45767075 <a title="19-lsi-5" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>Author: Peter Dayan</p><p>Abstract: Selective attention is a most intensively studied psychological phenomenon, rife with theoretical suggestions and schisms. A critical idea is that of limited capacity, the allocation of which has produced continual conﬂict about such phenomena as early and late selection. An inﬂuential resolution of this debate is based on the notion of perceptual load (Lavie, 2005), which suggests that low-load, easy tasks, because they underuse the total capacity of attention, mandatorily lead to the processing of stimuli that are irrelevant to the current attentional set; whereas high-load, difﬁcult tasks grab all resources for themselves, leaving distractors high and dry. We argue that this theory presents a challenge to Bayesian theories of attention, and suggest an alternative, statistical, account of key supporting data. 1</p><p>6 0.45296609 <a title="19-lsi-6" href="./nips-2008-Scalable_Algorithms_for_String_Kernels_with_Inexact_Matching.html">203 nips-2008-Scalable Algorithms for String Kernels with Inexact Matching</a></p>
<p>7 0.43493181 <a title="19-lsi-7" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>8 0.43233097 <a title="19-lsi-8" href="./nips-2008-Automatic_online_tuning_for_fast_Gaussian_summation.html">29 nips-2008-Automatic online tuning for fast Gaussian summation</a></p>
<p>9 0.4183692 <a title="19-lsi-9" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>10 0.41826296 <a title="19-lsi-10" href="./nips-2008-Adapting_to_a_Market_Shock%3A_Optimal_Sequential_Market-Making.html">13 nips-2008-Adapting to a Market Shock: Optimal Sequential Market-Making</a></p>
<p>11 0.41381547 <a title="19-lsi-11" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>12 0.39711225 <a title="19-lsi-12" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>13 0.39417019 <a title="19-lsi-13" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>14 0.39402446 <a title="19-lsi-14" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>15 0.39111367 <a title="19-lsi-15" href="./nips-2008-A_mixture_model_for_the_evolution_of_gene_expression_in_non-homogeneous_datasets.html">9 nips-2008-A mixture model for the evolution of gene expression in non-homogeneous datasets</a></p>
<p>16 0.36993486 <a title="19-lsi-16" href="./nips-2008-Breaking_Audio_CAPTCHAs.html">41 nips-2008-Breaking Audio CAPTCHAs</a></p>
<p>17 0.36075005 <a title="19-lsi-17" href="./nips-2008-Correlated_Bigram_LSA_for_Unsupervised_Language_Model_Adaptation.html">52 nips-2008-Correlated Bigram LSA for Unsupervised Language Model Adaptation</a></p>
<p>18 0.3583003 <a title="19-lsi-18" href="./nips-2008-A_spatially_varying_two-sample_recombinant_coalescent%2C_with_applications_to_HIV_escape_response.html">11 nips-2008-A spatially varying two-sample recombinant coalescent, with applications to HIV escape response</a></p>
<p>19 0.35139865 <a title="19-lsi-19" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>20 0.33354837 <a title="19-lsi-20" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.132), (7, 0.058), (12, 0.03), (28, 0.155), (33, 0.242), (57, 0.051), (59, 0.021), (63, 0.029), (71, 0.024), (77, 0.039), (78, 0.013), (83, 0.115)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7889064 <a title="19-lda-1" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>Author: Gabriele Schweikert, Gunnar Rätsch, Christian Widmer, Bernhard Schölkopf</p><p>Abstract: We study the problem of domain transfer for a supervised classiﬁcation task in mRNA splicing. We consider a number of recent domain transfer methods from machine learning, including some that are novel, and evaluate them on genomic sequence data from model organisms of varying evolutionary distance. We ﬁnd that in cases where the organisms are not closely related, the use of domain adaptation methods can help improve classiﬁcation performance.</p><p>2 0.7148068 <a title="19-lda-2" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>Author: Markus Maier, Ulrike V. Luxburg, Matthias Hein</p><p>Abstract: Graph clustering methods such as spectral clustering are deﬁned for general weighted graphs. In machine learning, however, data often is not given in form of a graph, but in terms of similarity (or distance) values between points. In this case, ﬁrst a neighborhood graph is constructed using the similarities between the points and then a graph clustering algorithm is applied to this graph. In this paper we investigate the inﬂuence of the construction of the similarity graph on the clustering results. We ﬁrst study the convergence of graph clustering criteria such as the normalized cut (Ncut) as the sample size tends to inﬁnity. We ﬁnd that the limit expressions are different for different types of graph, for example the r-neighborhood graph or the k-nearest neighbor graph. In plain words: Ncut on a kNN graph does something systematically different than Ncut on an r-neighborhood graph! This ﬁnding shows that graph clustering criteria cannot be studied independently of the kind of graph they are applied to. We also provide examples which show that these differences can be observed for toy and real data already for rather small sample sizes. 1</p><p>3 0.69284409 <a title="19-lda-3" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>Author: Steven J. Phillips, Miroslav Dudík</p><p>Abstract: We apply robust Bayesian decision theory to improve both generative and discriminative learners under bias in class proportions in labeled training data, when the true class proportions are unknown. For the generative case, we derive an entropybased weighting that maximizes expected log likelihood under the worst-case true class proportions. For the discriminative case, we derive a multinomial logistic model that minimizes worst-case conditional log loss. We apply our theory to the modeling of species geographic distributions from presence data, an extreme case of labeling bias since there is no absence data. On a benchmark dataset, we ﬁnd that entropy-based weighting offers an improvement over constant estimates of class proportions, consistently reducing log loss on unbiased test data. 1</p><p>4 0.6926021 <a title="19-lda-4" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>5 0.69007421 <a title="19-lda-5" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>Author: Tong Zhang</p><p>Abstract: Consider linear prediction models where the target function is a sparse linear combination of a set of basis functions. We are interested in the problem of identifying those basis functions with non-zero coefﬁcients and reconstructing the target function from noisy observations. Two heuristics that are widely used in practice are forward and backward greedy algorithms. First, we show that neither idea is adequate. Second, we propose a novel combination that is based on the forward greedy algorithm but takes backward steps adaptively whenever beneﬁcial. We prove strong theoretical results showing that this procedure is effective in learning sparse representations. Experimental results support our theory. 1</p><p>6 0.68391865 <a title="19-lda-6" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>7 0.68273103 <a title="19-lda-7" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>8 0.68032229 <a title="19-lda-8" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>9 0.67898548 <a title="19-lda-9" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>10 0.67864281 <a title="19-lda-10" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>11 0.6774841 <a title="19-lda-11" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>12 0.67688781 <a title="19-lda-12" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>13 0.67669201 <a title="19-lda-13" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>14 0.6763702 <a title="19-lda-14" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>15 0.67611885 <a title="19-lda-15" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>16 0.67538249 <a title="19-lda-16" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>17 0.6726644 <a title="19-lda-17" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>18 0.67083049 <a title="19-lda-18" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>19 0.67072219 <a title="19-lda-19" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>20 0.6674853 <a title="19-lda-20" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
