<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-162" href="#">nips2008-162</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</h1>
<br/><p>Source: <a title="nips-2008-162-pdf" href="http://papers.nips.cc/paper/3591-on-the-design-of-loss-functions-for-classification-theory-robustness-to-outliers-and-savageboost.pdf">pdf</a></p><p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The machine learning problem of classiﬁer design is studied from the perspective of probability elicitation, in statistics. This shows that the standard approach of proceeding from the speciﬁcation of a loss, to the minimization of conditional risk is overly restrictive. It is shown that a better alternative is to start from the speciﬁcation of a functional form for the minimum conditional risk, and derive the loss function. This has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classiﬁcation problems. These points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classiﬁer design, and is robust to the contamination of data with outliers. A new boosting algorithm, SavageBoost, is derived for the minimization of this loss. Experimental results show that it is indeed less sensitive to outliers than conventional methods, such as Ada, Real, or LogitBoost, and converges in fewer iterations. 1</p><p>Reference: <a title="nips-2008-162-reference" href="../nips2008_reference/nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 This shows that the standard approach of proceeding from the speciﬁcation of a loss, to the minimization of conditional risk is overly restrictive. [sent-4, score-0.375]
</p><p>2 It is shown that a better alternative is to start from the speciﬁcation of a functional form for the minimum conditional risk, and derive the loss function. [sent-5, score-0.457]
</p><p>3 This has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classiﬁcation problems. [sent-6, score-0.396]
</p><p>4 These points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classiﬁer design, and is robust to the contamination of data with outliers. [sent-7, score-0.401]
</p><p>5 A new boosting algorithm, SavageBoost, is derived for the minimization of this loss. [sent-8, score-0.258]
</p><p>6 Experimental results show that it is indeed less sensitive to outliers than conventional methods, such as Ada, Real, or LogitBoost, and converges in fewer iterations. [sent-9, score-0.179]
</p><p>7 Although tremendously successful, these methods have been known to suffer from some limitations, such as slow convergence, or too much sensitivity to the presence of outliers in the data [1, 2]. [sent-12, score-0.178]
</p><p>8 Such limitations can be attributed to the loss functions φ(·) on which the algorithms are based. [sent-13, score-0.291]
</p><p>9 These are convex bounds on the so-called 0-1 loss, which produces classiﬁers of minimum probability of error, but is too difﬁcult to handle from a computational point of view. [sent-14, score-0.183]
</p><p>10 We show that the two problems are identical, and probability elicitation can be seen as a reverse procedure for solving the classiﬁcation problem: 1) deﬁne the functional form of expected elicitation loss, 2) select a function class F, and 3) derive a loss function φ. [sent-16, score-1.328]
</p><p>11 Both probability elicitation and classiﬁer design reduce to the problem of minimizing a Bregman divergence. [sent-17, score-0.541]
</p><p>12 We derive equivalence results, which allow the representation of the classiﬁer design procedures in “probability elicitation form”, and the representation of the probability elicitation procedures in “machine learning form”. [sent-18, score-1.134]
</p><p>13 From the elicitation point of view, the risk functions used in machine learning can be used as new elicitation losses. [sent-20, score-1.204]
</p><p>14 From the machine learning point of view, new insights on the relationship between loss φ, optimal function f ∗ , and minimum risk are obtained. [sent-21, score-0.504]
</p><p>15 In particular, it is shown that the classical progression from loss to risk is overly restrictive: once a loss φ is speciﬁed, 1  both the optimal f ∗ , and the functional form of the minimum risk are immediately pined down. [sent-22, score-1.029]
</p><p>16 This is, however, not the case for the reverse progression: it is shown that any functional form of the minimum conditional risk, which satisﬁes some mild constraints, supports many (φ, f ∗ ) pairs. [sent-23, score-0.212]
</p><p>17 Hence, once the risk is selected, one degree of freedom remains: by selecting a class of f ∗ , it is possible to tailor the loss φ, so as to guarantee classiﬁers with desirable traits. [sent-24, score-0.438]
</p><p>18 In addition to this, the elicitation view reveals that the machine learning emphasis on convex losses φ is misguided. [sent-25, score-0.631]
</p><p>19 In particular, it is shown that what matters is the convexity of the minimum conditional risk. [sent-26, score-0.308]
</p><p>20 Once a functional form is selected for this quantity, the convexity of the loss φ does not affect the convexity of the Bregman divergence to be optimized. [sent-27, score-0.514]
</p><p>21 These results suggest that many new loss functions can be derived for classiﬁer design. [sent-28, score-0.292]
</p><p>22 We illustrate this, by deriving a new loss that trades convexity for boundedness. [sent-29, score-0.353]
</p><p>23 This is akin to robust loss functions proposed in the statistics literature to reduce the impact of outliers. [sent-31, score-0.294]
</p><p>24 We derive a new boosting algorithm, denoted SavageBoost, by combination of the new loss and the procedure used by Friedman to derive RealBoost [3]. [sent-32, score-0.435]
</p><p>25 Experimental results show that the new boosting algorithm is indeed more outlier resistant than classical methods, such as AdaBoost, RealBoost, and LogitBoost. [sent-33, score-0.226]
</p><p>26 2  Classiﬁcation and risk minimization  A classiﬁer is a mapping g : X → {−1, 1} that assigns a class label y ∈ {−1, 1} to a feature vector x ∈ X , where X is some feature space. [sent-34, score-0.285]
</p><p>27 If feature vectors are drawn with probability density PX (x), PY (y) is the probability distribution of the labels y ∈ {−1, 1}, and L(x, y) a loss function, the classiﬁcation risk is R(f ) = EX,Y [L(g(x), y)]. [sent-35, score-0.482]
</p><p>28 Under the 0-1 loss, L0/1 (x, y) = 1 if g(x) = y and 0 otherwise, this risk is the expected probability of classiﬁcation error, and is well known to be minimized by the Bayes decision rule. [sent-36, score-0.296]
</p><p>29 The minimization of the 0-1 loss requires that sign[f ∗ (x)] = sign[2η(x) − 1], ∀x  (2)  When the classes are separable, any f (x) such that yf (x) ≥ 0, ∀x has zero classiﬁcation error. [sent-39, score-0.386]
</p><p>30 The 0-1 loss can be written as a function of this quantity L0/1 (x, y) = φ0/1 [yf (x)] = sign[−yf (x)]. [sent-40, score-0.233]
</p><p>31 This motivates the minimization of the expected value of this loss as a goal for machine learning. [sent-41, score-0.302]
</p><p>32 Since these functions are non-negative, the risk is minimized by minimizing the conditional risk EY |X [φ(yf (x))|X = x] for every x ∈ X . [sent-46, score-0.556]
</p><p>33 This conditional risk can be written as Cφ (η, f ) = ηφ(f ) + (1 − η)φ(−f ),  (4)  where we have omitted the dependence of η and f on x for notational convenience. [sent-47, score-0.297]
</p><p>34 While learning algorithms based on the minimization of (4), such as SVMs, boosting, or logistic regression, can perform quite well, they are known to be overly sensitive to outliers [1, 2]. [sent-51, score-0.379]
</p><p>35 As can be seen from Figure 1, the sensitivity stems from the large 2  ∗ Table 1: Machine learning algorithms progress from loss φ, to inverse link function fφ (η), and minimum  ∗ conditional risk Cφ (η). [sent-53, score-0.771]
</p><p>36 This may, at ﬁrst thought, seem like a bad idea, given the widely held belief that the success of the aforementioned algorithms is precisely due to the convexity of these functions. [sent-56, score-0.15]
</p><p>37 What really matters is the fact, noted by [4], that the minimum conditional risk ∗ ∗ Cφ (η) = inf Cφ (η, f ) = Cφ (η, fφ ) f  (6)  ∗ satisﬁes two properties. [sent-58, score-0.392]
</p><p>38 The second property provides an interesting interpretation of the learning algorithms as methods for the estimation of the class posterior probability η(x): the search for the f (x) which minimizes (4) is equivalent to a search for the probability estimate η (x) which minimizes (7). [sent-62, score-0.233]
</p><p>39 3  Probability elicitation  This question has been extensively studied in statistics. [sent-64, score-0.474]
</p><p>40 In particular, Savage studied the problem of designing reward functions that encourage probability forecasters to make accurate predictions [6]. [sent-65, score-0.174]
</p><p>41 ˆ η η (9) Savage asked the question of which functions I1 (·), I−1 (·) make the expected reward maximal when η = η, ∀η. [sent-70, score-0.174]
</p><p>42 (11) (12)  Deﬁning the loss of the prediction of η by η as the difference to the maximum reward ˆ L(η, η ) = I(η, η) − I(η, η ) ˆ ˆ 1 Here, and throughout the paper, we omit the dependence of η on x, whenever we are referring to functions of η, i. [sent-74, score-0.352]
</p><p>43 3  Table 2: Probability elicitation form for various machine learning algorithms, and Savage’s procedure. [sent-77, score-0.474]
</p><p>44 Regression Savage 1 Savage 2  − 1−η η log η −k(1 − η)2 + m′ + l −k(1/η + log η) + m′ + l  η − 1−η log(1 − η) −kη 2 + m −k log η + m′  −2 η(1 − η) η log η + (1 − η) log(1 − η) kη 2 + lη + m m + lη − k log η  it follows that L(η, η ) = BJ (η, η ), ˆ ˆ (13) i. [sent-80, score-0.265]
</p><p>45 Hence, for any probability η, the best prediction η is the ˆ one of minimum Bregman divergence with η. [sent-83, score-0.151]
</p><p>46 the loss only depends on the difference η − η , and the admissible J are ˆ J1 (η) = kη 2 + lη + m,  (14)  for some integers (k, l, m). [sent-88, score-0.24]
</p><p>47 the loss only depends on the ratio η/ˆ, η and the admissible J are of the form J2 (η) = m + lη − k log η. [sent-91, score-0.293]
</p><p>48 probability elicitation  The discussion above shows that the optimization carried out by the learning algorithms is identical to Savage’s procedure for probability elicitation. [sent-93, score-0.569]
</p><p>49 Both procedures reduce to the search for η ∗ = arg min BF (η, η ), ˆ ˆ η ˆ  (16)  where F (η) is a convex function. [sent-94, score-0.13]
</p><p>50 The ˆ ˆ ˆ learning algorithms start from the loss φ(·). [sent-98, score-0.27]
</p><p>51 The conditional risk Cφ (η, f ) is then minimized with ∗ ∗ respect to f , so as to obtain the minimum conditional risk Cφ (η) and the corresponding fφ (ˆ). [sent-99, score-0.663]
</p><p>52 as procedures for the maximization of (9), ∗ by deriving the conditional reward functions associated with each of the Cφ (η) in Table 1. [sent-103, score-0.278]
</p><p>53 To understand the relationship between J, φ, and fφ it ∗ helps to think of the latter as an inverse link function. [sent-110, score-0.162]
</p><p>54 Or, assuming that fφ is invertible, to think of ∗ η = (fφ )−1 (v) as a link function, which maps a real v into a probability η. [sent-111, score-0.195]
</p><p>55 Under this interpretation, it is natural to consider link functions which exhibit the following symmetry  f −1 (−v) = 1 − f −1 (v). [sent-112, score-0.237]
</p><p>56 We refer to such link functions as symmetric, and show that they impose a special symmetry on J(η). [sent-116, score-0.237]
</p><p>57 4  ∗ Table 3: Probability elicitation form progresses from minimum conditional risk, and link function (fφ )−1 (η), ∗ to loss φ. [sent-117, score-0.982]
</p><p>58 Algorithm Least squares Modiﬁed LS SVM Boosting Logistic Regression  J(η) −4η(1 − η) −4η(1 − η) |2η − 1| − 1 −2 η(1 − η) η log η + (1 − η) log(1 − η)  ∗ (fφ )−1 (v) 1 2 (v + 1) NA N/A e2v 1+e2v ev 1+ev  φ(v) (1 − v)2 max(1 − v, 0)2 max(1 − v, 0) exp(−v) log(1 + e−v )  Theorem 1. [sent-119, score-0.159]
</p><p>59 Let I1 (η) and I−1 (η) be two functions derived from a continuously differentiable function J(η) according to (11) and (12), and f (η) be an invertible function which satisﬁes (19). [sent-120, score-0.17]
</p><p>60 (21)  The theorem shows that for any pair J(η), f (η), such that J(η) has the symmetry of (20) and f (η) the symmetry of (19), the expected reward of (9) can be written in the “machine learning form” of (4), using (17) and (18) with the φ(v) given by (21). [sent-123, score-0.239]
</p><p>61 Let I1 (η) and I−1 (η) be two functions derived with (11) and (12) from any continu∗ ously differentiable J(η) = −Cφ (η), such that ∗ ∗ Cφ (η) = Cφ (1 − η),  (22)  and fφ (η) be any invertible function which satisﬁes (19). [sent-126, score-0.17]
</p><p>62 The link functions associated with these algorithms are presented in Table 3. [sent-131, score-0.219]
</p><p>63 5  New loss functions  The discussion above provides an integrated picture of the “machine learning” and “probability elicitation” view of the classiﬁcation problem. [sent-133, score-0.315]
</p><p>64 Table 1 summarizes the steps of the “machine learning ∗ view”: start from the loss φ(v), and ﬁnd 1) the inverse link function fφ (η) of minimum condi∗ tional risk, and 2) the value of this risk Cφ (η). [sent-134, score-0.695]
</p><p>65 Table 3 summarizes the steps of the “probability elicitation view”: start from 1) the expected maximum reward function J(η) and 2) the link func∗ ∗ tion (fφ )−1 (v), and determine the loss function φ(v). [sent-135, score-0.999]
</p><p>66 ˆ Comparing to Table 2, it is clear that the least squares procedures are special cases of Savage 1, with k = −l = 4 and m = 0, and the link function η = (v + 1)/2. [sent-137, score-0.285]
</p><p>67 8  1  ∗ Figure 1: Loss function φ(v) (left) and minimum conditional risk Cφ (η) (right) associated with the different  methods discussed in the text. [sent-155, score-0.363]
</p><p>68 From the probability elicitation point of view, an important contribution of the machine learning research (in addition to the algorithms themselves) has been to identify new J functions, namely those associated with the techniques other than least squares. [sent-161, score-0.564]
</p><p>69 From the machine learning point of view, the elicitation perspective is interesting because it enables the derivation of new φ functions. [sent-162, score-0.496]
</p><p>70 In fact, the selection of φ can be seen as the ∗ ∗ indirect selection of a link function (fφ )−1 and a minimum conditional risk Cφ (η). [sent-164, score-0.544]
</p><p>71 The latter is an ∗ approximation to the minimum conditional risk of the 0-1 loss, Cφ0/1 (η) = 1 − max(η, 1 − η). [sent-165, score-0.363]
</p><p>72 The alternative, suggested by the probability elicitation view, is to start with the selection of the approximation directly. [sent-168, score-0.558]
</p><p>73 In addition to allowing direct control over the quantity that is usually of interest (the minimum expected risk of the classiﬁer), the selection of ∗ Cφ (η) (which is equivalent to the selection of J(η)) has the added advantage of leaving one degree of freedom open. [sent-169, score-0.389]
</p><p>74 As stated by Corollary 2 it is further possible to select across φ functions, by controlling the link function fφ . [sent-170, score-0.139]
</p><p>75 We demonstrate this point, by proposing a new loss function φ. [sent-172, score-0.211]
</p><p>76 We start by selecting the minimum ∗ conditional risk of least squares (using Savage’s version with k = −l = 1, m = 0) Cφ (η) = η(1 − η), because it provides the best approximation to the Bayes error, while avoiding the lack of differentiability of the SVM. [sent-173, score-0.493]
</p><p>77 We next replace the traditional link function of least squares by the η 1 ∗ logistic link function (classically used with logistic regression) fφ = 2 log 1−η . [sent-174, score-0.573]
</p><p>78 When used in the context of boosting (LogitBoost [3]), this link function has been found less sensitive to outliers than other variants [8]. [sent-175, score-0.488]
</p><p>79 Note that the proposed loss is very similar to that of least squares in the region where |v| is small (the margin), but quickly becomes constant as v → −∞. [sent-178, score-0.311]
</p><p>80 This is unlike all other previous φ functions, and suggests that classiﬁers designed with the new loss should be more robust to outliers. [sent-179, score-0.243]
</p><p>81 It is also interesting to note that the new loss function is not convex, violating what has been an hallmark of the φ functions used in the literature. [sent-180, score-0.262]
</p><p>82 The convexity of φ is, however, not important, a fact that is made clear by the elicitation view. [sent-181, score-0.595]
</p><p>83 Note that the convexity of the expected reward of (9) only depends on the convexity of the functions I1 (η) and I−1 (η). [sent-182, score-0.416]
</p><p>84 completely determines the convexity of the conditional risk of (4). [sent-195, score-0.396]
</p><p>85 To test this we designed a boosting algorithm based in the new loss, using the procedure proposed by Friedman to derive RealBoost [3]. [sent-198, score-0.197]
</p><p>86 At each iteration the algorithm searches for the weak learner G(x) which further reduces the conditional risk EY |X [φ(y(f (x) + G(x)))|X = x] of the current f (x), for every x ∈ X . [sent-199, score-0.333]
</p><p>87 The latter is generally considered more robust to outliers [8] and thus a good candidate for comparison. [sent-205, score-0.181]
</p><p>88 Ten binary UCI data sets were used: Pima-diabetes, breast cancer diagnostic, breast cancer prognostic, original Wisconsin breast cancer, liver disorder, sonar, echo-cardiogram, Cleveland heart disease, tic-tac-toe and Haberman’s survival. [sent-206, score-0.222]
</p><p>89 Table 4 shows the number of times each method produced the smallest error (#wins) over the ten data sets at a given contamination level, as well as the average error% over all data sets (at that contamination level). [sent-210, score-0.336]
</p><p>90 Our results conﬁrm previous studies that have noted AdaBoost’s sensitivity to outliers [1]. [sent-211, score-0.178]
</p><p>91 This conﬁrms previous reports that LogitBoost is less sensitive to outliers [8]. [sent-213, score-0.179]
</p><p>92 SavageBoost produced generally better results than Ada and RealBoost at all contamination levels, including 0% contamination. [sent-214, score-0.158]
</p><p>93 Loss (SavageBoost)  46  Exp Loss (RealBoost)  44  Log Loss (LogitBoost) Exp Loss (AdaBoost)  %Error  42 40 38 36 34 32 30 28 0  5  10  15 20 25 Outlier Percentage  30  35  40  Figure 2: Average error for four boosting methods at different contamination levels. [sent-216, score-0.348]
</p><p>94 Method Savage Loss (SavageBoost) Log Loss(LogitBoost) Exp Loss(RealBoost) Exp Loss(AdaBoost)  0% outliers (4, 19. [sent-218, score-0.149]
</p><p>95 22%)  comparable results at low contamination levels (0%, 5%) but has higher error when contamination is signiﬁcant. [sent-230, score-0.336]
</p><p>96 With 40% contamination SavageBoost has 6 wins, compared to 3 for LogitBoost and, on average, about 6% less error. [sent-231, score-0.158]
</p><p>97 Tibshirani, “Additive logistic regression: A statistical view of boosting,” Annals of Statistics, 2000. [sent-246, score-0.124]
</p><p>98 Zhang, “Statistical behavior and consistency of classiﬁcation methods based on convex risk minimization,” Annals of Statistics, 2004. [sent-248, score-0.288]
</p><p>99 Savage, “The elicitation of personal probabilities and expectations,” JASA, vol. [sent-256, score-0.474]
</p><p>100 Eckley, “An empirical comparison of three boosting algorithms on real data sets with artiﬁcial class noise,” in International Workshop on Multiple Classiﬁer Systems, 2003. [sent-266, score-0.221]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('elicitation', 0.474), ('savage', 0.451), ('savageboost', 0.271), ('loss', 0.211), ('risk', 0.205), ('logitboost', 0.203), ('realboost', 0.18), ('boosting', 0.17), ('contamination', 0.158), ('outliers', 0.149), ('link', 0.139), ('convexity', 0.121), ('yf', 0.117), ('reward', 0.09), ('minimum', 0.088), ('ls', 0.088), ('adaboost', 0.084), ('classi', 0.08), ('sign', 0.077), ('squares', 0.072), ('logistic', 0.071), ('bregman', 0.07), ('conditional', 0.07), ('er', 0.063), ('convex', 0.062), ('pw', 0.059), ('minimization', 0.058), ('outlier', 0.056), ('log', 0.053), ('view', 0.053), ('invertible', 0.053), ('functions', 0.051), ('jasa', 0.048), ('symmetries', 0.048), ('symmetry', 0.047), ('table', 0.047), ('procedures', 0.046), ('svm', 0.046), ('breast', 0.046), ('nuno', 0.045), ('wins', 0.044), ('losses', 0.042), ('overly', 0.042), ('cancer', 0.042), ('gm', 0.042), ('corollary', 0.041), ('ada', 0.039), ('bf', 0.039), ('minimizes', 0.036), ('modified', 0.036), ('jolla', 0.036), ('progression', 0.036), ('differentiable', 0.036), ('friedman', 0.035), ('design', 0.034), ('ev', 0.034), ('ers', 0.033), ('probability', 0.033), ('expected', 0.033), ('risks', 0.032), ('weak', 0.032), ('regression', 0.032), ('robust', 0.032), ('functional', 0.031), ('derived', 0.03), ('cation', 0.03), ('divergence', 0.03), ('start', 0.03), ('wi', 0.03), ('sensitive', 0.03), ('sensitivity', 0.029), ('admissible', 0.029), ('matters', 0.029), ('ey', 0.029), ('py', 0.029), ('algorithms', 0.029), ('least', 0.028), ('derive', 0.027), ('learner', 0.026), ('diego', 0.026), ('minimized', 0.025), ('annals', 0.024), ('reverse', 0.023), ('think', 0.023), ('la', 0.023), ('iterations', 0.023), ('stopping', 0.022), ('perspective', 0.022), ('class', 0.022), ('search', 0.022), ('summarizes', 0.022), ('mappings', 0.022), ('satis', 0.022), ('written', 0.022), ('leaving', 0.021), ('svms', 0.021), ('deriving', 0.021), ('consistency', 0.021), ('selection', 0.021), ('error', 0.02)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000004 <a title="162-tfidf-1" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The machine learning problem of classiﬁer design is studied from the perspective of probability elicitation, in statistics. This shows that the standard approach of proceeding from the speciﬁcation of a loss, to the minimization of conditional risk is overly restrictive. It is shown that a better alternative is to start from the speciﬁcation of a functional form for the minimum conditional risk, and derive the loss function. This has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classiﬁcation problems. These points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classiﬁer design, and is robust to the contamination of data with outliers. A new boosting algorithm, SavageBoost, is derived for the minimization of this loss. Experimental results show that it is indeed less sensitive to outliers than conventional methods, such as Ada, Real, or LogitBoost, and converges in fewer iterations. 1</p><p>2 0.11642931 <a title="162-tfidf-2" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>Author: Yves Grandvalet, Alain Rakotomamonjy, Joseph Keshet, Stéphane Canu</p><p>Abstract: We consider the problem of binary classiﬁcation where the classiﬁer may abstain instead of classifying each observation. The Bayes decision rule for this setup, known as Chow’s rule, is deﬁned by two thresholds on posterior probabilities. From simple desiderata, namely the consistency and the sparsity of the classiﬁer, we derive the double hinge loss function that focuses on estimating conditional probabilities only in the vicinity of the threshold points of the optimal decision rule. We show that, for suitable kernel machines, our approach is universally consistent. We cast the problem of minimizing the double hinge loss as a quadratic program akin to the standard SVM optimization problem and propose an active set method to solve it efﬁciently. We ﬁnally provide preliminary experimental results illustrating the interest of our constructive approach to devising loss functions. 1</p><p>3 0.11132906 <a title="162-tfidf-3" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>Author: Tae-kyun Kim, Roberto Cipolla</p><p>Abstract: We present a new co-clustering problem of images and visual features. The problem involves a set of non-object images in addition to a set of object images and features to be co-clustered. Co-clustering is performed in a way that maximises discrimination of object images from non-object images, thus emphasizing discriminative features. This provides a way of obtaining perceptual joint-clusters of object images and features. We tackle the problem by simultaneously boosting multiple strong classiﬁers which compete for images by their expertise. Each boosting classiﬁer is an aggregation of weak-learners, i.e. simple visual features. The obtained classiﬁers are useful for object detection tasks which exhibit multimodalities, e.g. multi-category and multi-view object detection tasks. Experiments on a set of pedestrian images and a face data set demonstrate that the method yields intuitive image clusters with associated features and is much superior to conventional boosting classiﬁers in object detection tasks. 1</p><p>4 0.10651966 <a title="162-tfidf-4" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>Author: Olivier Chapelle, Chuong B. Do, Choon H. Teo, Quoc V. Le, Alex J. Smola</p><p>Abstract: Large-margin structured estimation methods minimize a convex upper bound of loss functions. While they allow for efﬁcient optimization algorithms, these convex formulations are not tight and sacriﬁce the ability to accurately model the true loss. We present tighter non-convex bounds based on generalizing the notion of a ramp loss from binary classiﬁcation to structured estimation. We show that a small modiﬁcation of existing optimization algorithms sufﬁces to solve this modiﬁed problem. On structured prediction tasks such as protein sequence alignment and web page ranking, our algorithm leads to improved accuracy. 1</p><p>5 0.10003942 <a title="162-tfidf-5" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>Author: Richard Nock, Frank Nielsen</p><p>Abstract: Bartlett et al (2006) recently proved that a ground condition for convex surrogates, classiﬁcation calibration, ties up the minimization of the surrogates and classiﬁcation risks, and left as an important problem the algorithmic questions about the minimization of these surrogates. In this paper, we propose an algorithm which provably minimizes any classiﬁcation calibrated surrogate strictly convex and differentiable — a set whose losses span the exponential, logistic and squared losses —, with boosting-type guaranteed convergence rates under a weak learning assumption. A particular subclass of these surrogates, that we call balanced convex surrogates, has a key rationale that ties it to maximum likelihood estimation, zerosum games and the set of losses that satisfy some of the most common requirements for losses in supervised learning. We report experiments on more than 50 readily available domains of 11 ﬂavors of the algorithm, that shed light on new surrogates, and the potential of data dependent strategies to tune surrogates. 1</p><p>6 0.099577151 <a title="162-tfidf-6" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>7 0.098946132 <a title="162-tfidf-7" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>8 0.098161653 <a title="162-tfidf-8" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>9 0.09105207 <a title="162-tfidf-9" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>10 0.073701777 <a title="162-tfidf-10" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>11 0.070089556 <a title="162-tfidf-11" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>12 0.068934277 <a title="162-tfidf-12" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>13 0.06483569 <a title="162-tfidf-13" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>14 0.061802968 <a title="162-tfidf-14" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>15 0.060706865 <a title="162-tfidf-15" href="./nips-2008-From_Online_to_Batch_Learning_with_Cutoff-Averaging.html">88 nips-2008-From Online to Batch Learning with Cutoff-Averaging</a></p>
<p>16 0.058689147 <a title="162-tfidf-16" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>17 0.057124875 <a title="162-tfidf-17" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>18 0.057038441 <a title="162-tfidf-18" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>19 0.056979839 <a title="162-tfidf-19" href="./nips-2008-Domain_Adaptation_with_Multiple_Sources.html">65 nips-2008-Domain Adaptation with Multiple Sources</a></p>
<p>20 0.056545421 <a title="162-tfidf-20" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.175), (1, -0.02), (2, -0.12), (3, -0.014), (4, -0.076), (5, 0.051), (6, -0.023), (7, -0.052), (8, -0.018), (9, 0.079), (10, 0.052), (11, 0.117), (12, 0.036), (13, -0.048), (14, -0.006), (15, 0.006), (16, 0.003), (17, -0.077), (18, 0.044), (19, 0.041), (20, -0.042), (21, 0.03), (22, -0.026), (23, -0.011), (24, -0.011), (25, 0.113), (26, -0.032), (27, -0.117), (28, -0.116), (29, -0.058), (30, 0.115), (31, -0.074), (32, 0.04), (33, -0.051), (34, 0.063), (35, -0.039), (36, -0.067), (37, 0.02), (38, 0.031), (39, 0.025), (40, 0.152), (41, -0.041), (42, 0.053), (43, -0.158), (44, 0.154), (45, 0.085), (46, 0.025), (47, -0.031), (48, -0.022), (49, -0.06)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.93935937 <a title="162-lsi-1" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The machine learning problem of classiﬁer design is studied from the perspective of probability elicitation, in statistics. This shows that the standard approach of proceeding from the speciﬁcation of a loss, to the minimization of conditional risk is overly restrictive. It is shown that a better alternative is to start from the speciﬁcation of a functional form for the minimum conditional risk, and derive the loss function. This has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classiﬁcation problems. These points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classiﬁer design, and is robust to the contamination of data with outliers. A new boosting algorithm, SavageBoost, is derived for the minimization of this loss. Experimental results show that it is indeed less sensitive to outliers than conventional methods, such as Ada, Real, or LogitBoost, and converges in fewer iterations. 1</p><p>2 0.85602617 <a title="162-lsi-2" href="./nips-2008-On_the_Efficient_Minimization_of_Classification_Calibrated_Surrogates.html">163 nips-2008-On the Efficient Minimization of Classification Calibrated Surrogates</a></p>
<p>Author: Richard Nock, Frank Nielsen</p><p>Abstract: Bartlett et al (2006) recently proved that a ground condition for convex surrogates, classiﬁcation calibration, ties up the minimization of the surrogates and classiﬁcation risks, and left as an important problem the algorithmic questions about the minimization of these surrogates. In this paper, we propose an algorithm which provably minimizes any classiﬁcation calibrated surrogate strictly convex and differentiable — a set whose losses span the exponential, logistic and squared losses —, with boosting-type guaranteed convergence rates under a weak learning assumption. A particular subclass of these surrogates, that we call balanced convex surrogates, has a key rationale that ties it to maximum likelihood estimation, zerosum games and the set of losses that satisfy some of the most common requirements for losses in supervised learning. We report experiments on more than 50 readily available domains of 11 ﬂavors of the algorithm, that shed light on new surrogates, and the potential of data dependent strategies to tune surrogates. 1</p><p>3 0.68293387 <a title="162-lsi-3" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>Author: Yves Grandvalet, Alain Rakotomamonjy, Joseph Keshet, Stéphane Canu</p><p>Abstract: We consider the problem of binary classiﬁcation where the classiﬁer may abstain instead of classifying each observation. The Bayes decision rule for this setup, known as Chow’s rule, is deﬁned by two thresholds on posterior probabilities. From simple desiderata, namely the consistency and the sparsity of the classiﬁer, we derive the double hinge loss function that focuses on estimating conditional probabilities only in the vicinity of the threshold points of the optimal decision rule. We show that, for suitable kernel machines, our approach is universally consistent. We cast the problem of minimizing the double hinge loss as a quadratic program akin to the standard SVM optimization problem and propose an active set method to solve it efﬁciently. We ﬁnally provide preliminary experimental results illustrating the interest of our constructive approach to devising loss functions. 1</p><p>4 0.60425323 <a title="162-lsi-4" href="./nips-2008-Weighted_Sums_of_Random_Kitchen_Sinks%3A_Replacing_minimization_with_randomization_in_learning.html">250 nips-2008-Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning</a></p>
<p>Author: Ali Rahimi, Benjamin Recht</p><p>Abstract: Randomized neural networks are immortalized in this AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. “What are you doing?” asked Minsky. “I am training a randomly wired neural net to play tic-tac-toe,” Sussman replied. “Why is the net wired randomly?” asked Minsky. Sussman replied, “I do not want it to have any preconceptions of how to play.” Minsky then shut his eyes. “Why do you close your eyes?” Sussman asked his teacher. “So that the room will be empty,” replied Minsky. At that moment, Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Speciﬁcally, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlinearities. We identify conditions under which these networks exhibit good classiﬁcation performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities. 1</p><p>5 0.59014946 <a title="162-lsi-5" href="./nips-2008-Privacy-preserving_logistic_regression.html">185 nips-2008-Privacy-preserving logistic regression</a></p>
<p>Author: Kamalika Chaudhuri, Claire Monteleoni</p><p>Abstract: This paper addresses the important tradeoff between privacy and learnability, when designing algorithms for learning from private databases. We focus on privacy-preserving logistic regression. First we apply an idea of Dwork et al. [6] to design a privacy-preserving logistic regression algorithm. This involves bounding the sensitivity of regularized logistic regression, and perturbing the learned classiﬁer with noise proportional to the sensitivity. We then provide a privacy-preserving regularized logistic regression algorithm based on a new privacy-preserving technique: solving a perturbed optimization problem. We prove that our algorithm preserves privacy in the model due to [6]. We provide learning guarantees for both algorithms, which are tighter for our new algorithm, in cases in which one would typically apply logistic regression. Experiments demonstrate improved learning performance of our method, versus the sensitivity method. Our privacy-preserving technique does not depend on the sensitivity of the function, and extends easily to a class of convex loss functions. Our work also reveals an interesting connection between regularization and privacy. 1</p><p>6 0.57939363 <a title="162-lsi-6" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>7 0.56639671 <a title="162-lsi-7" href="./nips-2008-Tighter_Bounds_for_Structured_Estimation.html">239 nips-2008-Tighter Bounds for Structured Estimation</a></p>
<p>8 0.56509852 <a title="162-lsi-8" href="./nips-2008-Adaptive_Martingale_Boosting.html">15 nips-2008-Adaptive Martingale Boosting</a></p>
<p>9 0.54052061 <a title="162-lsi-9" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<p>10 0.51307011 <a title="162-lsi-10" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>11 0.44439849 <a title="162-lsi-11" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<p>12 0.42263615 <a title="162-lsi-12" href="./nips-2008-Optimization_on_a_Budget%3A_A_Reinforcement_Learning_Approach.html">173 nips-2008-Optimization on a Budget: A Reinforcement Learning Approach</a></p>
<p>13 0.3969667 <a title="162-lsi-13" href="./nips-2008-On_the_Complexity_of_Linear_Prediction%3A_Risk_Bounds%2C_Margin_Bounds%2C_and_Regularization.html">161 nips-2008-On the Complexity of Linear Prediction: Risk Bounds, Margin Bounds, and Regularization</a></p>
<p>14 0.39242405 <a title="162-lsi-14" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>15 0.39081147 <a title="162-lsi-15" href="./nips-2008-Sparsity_of_SVMs_that_use_the_epsilon-insensitive_loss.html">217 nips-2008-Sparsity of SVMs that use the epsilon-insensitive loss</a></p>
<p>16 0.38140857 <a title="162-lsi-16" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>17 0.37641111 <a title="162-lsi-17" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>18 0.37253633 <a title="162-lsi-18" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>19 0.36537266 <a title="162-lsi-19" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>20 0.3640866 <a title="162-lsi-20" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.104), (7, 0.06), (9, 0.266), (12, 0.038), (15, 0.016), (28, 0.155), (57, 0.065), (63, 0.045), (71, 0.04), (77, 0.048), (83, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.78243166 <a title="162-lda-1" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>Author: Nicol N. Schraudolph, Dmitry Kamenetsky</p><p>Abstract: We give polynomial-time algorithms for the exact computation of lowest-energy states, worst margin violators, partition functions, and marginals in certain binary undirected graphical models. Our approach provides an interesting alternative to the well-known graph cut paradigm in that it does not impose any submodularity constraints; instead we require planarity to establish a correspondence with perfect matchings in an expanded dual graph. Maximum-margin parameter estimation for a boundary detection task shows our approach to be efﬁcient and effective. A C++ implementation is available from http://nic.schraudolph.org/isinf/. 1</p><p>2 0.77840918 <a title="162-lda-2" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>Author: K. Wong, Si Wu, Chi Fung</p><p>Abstract: Continuous attractor neural networks (CANNs) are emerging as promising models for describing the encoding of continuous stimuli in neural systems. Due to the translational invariance of their neuronal interactions, CANNs can hold a continuous family of neutrally stable states. In this study, we systematically explore how neutral stability of a CANN facilitates its tracking performance, a capacity believed to have wide applications in brain functions. We develop a perturbative approach that utilizes the dominant movement of the network stationary states in the state space. We quantify the distortions of the bump shape during tracking, and study their effects on the tracking performance. Results are obtained on the maximum speed for a moving stimulus to be trackable, and the reaction time to catch up an abrupt change in stimulus. 1</p><p>same-paper 3 0.73493189 <a title="162-lda-3" href="./nips-2008-On_the_Design_of_Loss_Functions_for_Classification%3A_theory%2C_robustness_to_outliers%2C_and_SavageBoost.html">162 nips-2008-On the Design of Loss Functions for Classification: theory, robustness to outliers, and SavageBoost</a></p>
<p>Author: Hamed Masnadi-shirazi, Nuno Vasconcelos</p><p>Abstract: The machine learning problem of classiﬁer design is studied from the perspective of probability elicitation, in statistics. This shows that the standard approach of proceeding from the speciﬁcation of a loss, to the minimization of conditional risk is overly restrictive. It is shown that a better alternative is to start from the speciﬁcation of a functional form for the minimum conditional risk, and derive the loss function. This has various consequences of practical interest, such as showing that 1) the widely adopted practice of relying on convex loss functions is unnecessary, and 2) many new losses can be derived for classiﬁcation problems. These points are illustrated by the derivation of a new loss which is not convex, but does not compromise the computational tractability of classiﬁer design, and is robust to the contamination of data with outliers. A new boosting algorithm, SavageBoost, is derived for the minimization of this loss. Experimental results show that it is indeed less sensitive to outliers than conventional methods, such as Ada, Real, or LogitBoost, and converges in fewer iterations. 1</p><p>4 0.6382879 <a title="162-lda-4" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>Author: Bernhard Nessler, Michael Pfeiffer, Wolfgang Maass</p><p>Abstract: Uncertainty is omnipresent when we perceive or interact with our environment, and the Bayesian framework provides computational methods for dealing with it. Mathematical models for Bayesian decision making typically require datastructures that are hard to implement in neural networks. This article shows that even the simplest and experimentally best supported type of synaptic plasticity, Hebbian learning, in combination with a sparse, redundant neural code, can in principle learn to infer optimal Bayesian decisions. We present a concrete Hebbian learning rule operating on log-probability ratios. Modulated by reward-signals, this Hebbian plasticity rule also provides a new perspective for understanding how Bayesian inference could support fast reinforcement learning in the brain. In particular we show that recent experimental results by Yang and Shadlen [1] on reinforcement learning of probabilistic inference in primates can be modeled in this way. 1</p><p>5 0.63649589 <a title="162-lda-5" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>6 0.63014162 <a title="162-lda-6" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>7 0.62944382 <a title="162-lda-7" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>8 0.62365514 <a title="162-lda-8" href="./nips-2008-Fast_Rates_for_Regularized_Objectives.html">85 nips-2008-Fast Rates for Regularized Objectives</a></p>
<p>9 0.62313968 <a title="162-lda-9" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>10 0.62263978 <a title="162-lda-10" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>11 0.62098128 <a title="162-lda-11" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>12 0.61893332 <a title="162-lda-12" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>13 0.61692071 <a title="162-lda-13" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>14 0.61664689 <a title="162-lda-14" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>15 0.6165269 <a title="162-lda-15" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>16 0.61619747 <a title="162-lda-16" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>17 0.61542332 <a title="162-lda-17" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>18 0.61484575 <a title="162-lda-18" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>19 0.61378318 <a title="162-lda-19" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>20 0.61323106 <a title="162-lda-20" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
