<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>89 nips-2008-Gates</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-89" href="#">nips2008-89</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>89 nips-2008-Gates</h1>
<br/><p>Source: <a title="nips-2008-89-pdf" href="http://papers.nips.cc/paper/3379-gates.pdf">pdf</a></p><p>Author: Tom Minka, John Winn</p><p>Abstract: Gates are a new notation for representing mixture models and context-sensitive independence in factor graphs. Factor graphs provide a natural representation for message-passing algorithms, such as expectation propagation. However, message passing in mixture models is not well captured by factor graphs unless the entire mixture is represented by one factor, because the message equations have a containment structure. Gates capture this containment structure graphically, allowing both the independences and the message-passing equations for a model to be readily visualized. Different variational approximations for mixture models can be understood as different ways of drawing the gates in a model. We present general equations for expectation propagation and variational message passing in the presence of gates. 1</p><p>Reference: <a title="nips-2008-89-reference" href="../nips2008_reference/nips-2008-Gates_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 However, message passing in mixture models is not well captured by factor graphs unless the entire mixture is represented by one factor, because the message equations have a containment structure. [sent-5, score-0.68]
</p><p>2 Different variational approximations for mixture models can be understood as different ways of drawing the gates in a model. [sent-7, score-0.498]
</p><p>3 We present general equations for expectation propagation and variational message passing in the presence of gates. [sent-8, score-0.32]
</p><p>4 Section 2 describes what a gate is and shows how it can be used to represent context-speciﬁc independencies in a number of example models. [sent-17, score-0.719]
</p><p>5 Section 3 motivates the use of gates for inference and section 4 expands on this by showing how gates can be used within three standard inference algorithms: Expectation Propagation (EP), Variational Message Passing (VMP) and Gibbs sampling. [sent-18, score-0.868]
</p><p>6 Section 5 shows how the placement of gates can tradeoff cost versus accuracy of inference. [sent-19, score-0.413]
</p><p>7 Section 6 discusses the use of gates to implement inference algorithms. [sent-20, score-0.434]
</p><p>8 N  x  x  Figure 1: Gate examples (a) The dashed rectangle indicates a gate containing a Gaussian factor, with selector variable c. [sent-23, score-0.986]
</p><p>9 (b) Two gates with different key values used to construct a mixture of two Gaussians. [sent-24, score-0.489]
</p><p>10 (c) When multiple gates share a selector variable, they can be drawn touching with the selector variable connected to only one of the gates. [sent-25, score-1.034]
</p><p>11 (d) A mixture of N Gaussians constructed using both a gate and a plate. [sent-26, score-0.72]
</p><p>12 2  The Gate  A gate encloses part of a factor graph and switches it on or off depending on the state of a latent selector variable. [sent-28, score-1.033]
</p><p>13 The gate is on when the selector variable has a particular value, called the key, and off for all other values. [sent-29, score-0.986]
</p><p>14 A gate allows context-speciﬁc independencies to be made explicit in the graphical model: the dependencies represented by any factors inside the gate are present only in the context of the selector variable having the key value. [sent-30, score-1.959]
</p><p>15 Mathematically, a gate represents raising the δ(c=key) contained factors to the power zero if the gate is off, or one if it is on: ( i fi (x)) where c is the selector variable. [sent-31, score-1.72]
</p><p>16 In diagrams, a gate is denoted by a dashed box labelled with the value of key, with the selector variable connected to the box boundary. [sent-32, score-1.009]
</p><p>17 Whilst the examples in this paper refer to factor graphs, gate notation can also be used in both directed Bayesian networks and undirected graphs. [sent-34, score-0.794]
</p><p>18 This example represents the term N (x; m, p−1 )δ(c=true) so that when c is true the gate is on and x has a Gaussian distribution with mean m and precision p. [sent-36, score-0.671]
</p><p>19 Otherwise, the gate is off and x is uniformly distributed (since it is connected to nothing). [sent-37, score-0.694]
</p><p>20 By using several gates with different key values, multiple components of a mixture can be represented. [sent-38, score-0.489]
</p><p>21 Figure 1b shows how a mixture of two Gaussians can be represented using two gates with different key values, true and false. [sent-39, score-0.489]
</p><p>22 When multiple gates have the same selector variable but differ2 ent key values, they can be drawn as in ﬁgure 1c, with the gate rectangles touching and the selector variable connected to only one of the gates. [sent-41, score-1.788]
</p><p>23 Notice that in this example, an integer selector variable is used and the key values are the integers 1,2,3. [sent-42, score-0.342]
</p><p>24 For large homogeneous mixtures, gates can be used in conjunction with plates [6]. [sent-43, score-0.431]
</p><p>25 For example, ﬁgure 1d shows how a mixture of N Gaussians can be represented by placing the gate, Gaussian factor and mean/precision variables inside a plate, so that they are replicated N times. [sent-44, score-0.271]
</p><p>26 To avoid ambiguities, gates cannot partially overlap, nor can a gate contain its own selector variable. [sent-46, score-1.343]
</p><p>27 N  Figure 2: Examples of models which use gates (a) A line process where neighboring pixel intensities are independent if an edge exists between them. [sent-50, score-0.465]
</p><p>28 The selector variable c encodes whether the linear dependency represented by the structure inside the gate is present or absent. [sent-52, score-1.084]
</p><p>29 Such variables have the behaviour that, when the gate is off, they revert to having a default value of false or zero, depending on the variable type. [sent-54, score-0.783]
</p><p>30 Mathematically, a variable inside a gate represents a Dirac delta when the gate is off: δ(x)1−δ(c=key) where δ(x) is one only when x has its default value. [sent-55, score-1.514]
</p><p>31 Figure 2b shows an example where variables are contained in gates – this example is described in the following section. [sent-56, score-0.452]
</p><p>32 1  Examples of models with gates  Figure 2a shows a line process from [7]. [sent-58, score-0.413]
</p><p>33 The use of gates makes clear the assumption that two neighboring image pixels xi and xj have a dependency between their intensity values, unless there is an edge eij between them. [sent-59, score-0.551]
</p><p>34 In this case the selector variable is connected only to the gate, as shown in the example of ﬁgure 2b. [sent-62, score-0.338]
</p><p>35 The binary selector variable c switches on or off a linear model of the genetic variant’s contribution yn to the trait xn , across all individuals. [sent-65, score-0.45]
</p><p>36 When the gate is off, yn reverts to the default value of 0 and so the trait is explained only by a Gaussian-distributed background model zn . [sent-66, score-0.779]
</p><p>37 3  How gates arise from message-passing on mixture models  Factor graph notation arises naturally when describing message passing algorithms, such as the sum-product algorithm. [sent-68, score-0.744]
</p><p>38 Similarly, the gate notation arises naturally when considering the behavior of message passing algorithms on mixture models. [sent-69, score-1.002]
</p><p>39 For example, the update for q(mk ) can be interpreted as (message from prior)×(blurred message from f ). [sent-73, score-0.22]
</p><p>40 The update for q(x) can be interpreted as (blurred message from m1 )×(blurred message from m2 ). [sent-74, score-0.421]
</p><p>41 Blurring occurs whenever a message is sent from a factor having a random exponent to a factor without that exponent. [sent-75, score-0.401]
</p><p>42 Hence, we use a graphical notation where a gate is a container, holding all the factors switched by the gate. [sent-77, score-0.813]
</p><p>43 Graphically, the blurring operation then happens whenever a message leaves a gate. [sent-78, score-0.274]
</p><p>44 Messages passed into a gate and within a gate are unchanged. [sent-79, score-1.342]
</p><p>45 For example, EP on this model will blur the message from f to mk and from f to x, where “blurring” means a linear combination with the 1 function followed by KL-projection. [sent-81, score-0.374]
</p><p>46 1  Why gates are not equivalent to ‘pick’ factors  It is possible to rewrite this model so that the f factors do not have exponents, and therefore would not be in gates. [sent-83, score-0.561]
</p><p>47 This is because the blurring effect caused by exponents operates in one direction only, while the blurring effect caused by intermediate factors is always bidirectional. [sent-85, score-0.238]
</p><p>48 The pick factor will correctly blur the downward messages from (m1 , m2 ) to x. [sent-88, score-0.348]
</p><p>49 However, the pick factor will also blur the message upward from x before it reaches the factor f , which is incorrect. [sent-89, score-0.459]
</p><p>50 In this case, the message from x to f is not blurred, and the upward messages to (m1 , m2 ) are blurred, which is correct. [sent-91, score-0.403]
</p><p>51 However, the downward messages from (m1 , m2 ) to f are blurred before reaching f , which is incorrect. [sent-92, score-0.272]
</p><p>52 2  Variables inside gates  Now consider an example where it is natural to consider a variable to be inside a gate. [sent-94, score-0.665]
</p><p>53 The message from the gate to c can be interpreted as the evidence for the submodel containing f1 , f2 , and y. [sent-98, score-0.962]
</p><p>54 4  4  Inference with gates  In the previous section, we explained why the gate notation arises when performing message passing in some example mixture models. [sent-99, score-1.415]
</p><p>55 In this section, we describe how gate notation can be generally incorporated into Variational Message Passing [10], Expectation Propagation [11] and Gibbs Sampling [7] to allow each of these algorithms to support context-speciﬁc independence. [sent-100, score-0.706]
</p><p>56 Notice that VMP uses different messages to and from deterministic factors, that is, factors which have the form fa (xi , xa\i ) = δ(xi − h(xa\i )) where xi is the derived child variable. [sent-102, score-0.486]
</p><p>57 When performing inference on models with gates, it is useful to employ a normalised form of gate model. [sent-107, score-0.73]
</p><p>58 In this form, variables inside a gate have no links to factors outside the gate, and a variable outside a gate links to at most one factor inside the gate. [sent-108, score-1.899]
</p><p>59 Both of these requirements can be achieved by splitting a variable into a copy inside and a copy outside the gate, connected by an equality factor inside the gate. [sent-109, score-0.436]
</p><p>60 A factor inside a gate should not connect to the selector of the gate; it should be given the key value instead. [sent-110, score-1.143]
</p><p>61 In addition, gates should be balanced by ensuring that if a variable links  Alg. [sent-111, score-0.49]
</p><p>62 Type  Variable to factor  Factor to variable  mi→a (xi )  ma→i (xi ) proj  EP  xa \xi  mb→i (xi )  Any  j∈a  mi→a (xi )  b=a  VMP    ma→i (xi )  Stochastic  exp   a∋i  xa \xi  mb→i (xi )  Det. [sent-112, score-0.52]
</p><p>63 to child  mj→a (xj ) fa (xa )  sa =  xa xa  sa = exp  xa  (  mj→a (xj ))fa (xa ) mj→a (xj )ma→j (xj )  j∈a j∈a  j∈a  mj→a (xj ) log fa (xa )  Table 1: Messages and evidence computations for EP and VMP The top part of the table shows messages between a variable xi and a factor fa . [sent-115, score-1.36]
</p><p>64 The notation j ∈ a refers to all neighbors of the factor, j = i is all neighbors except i, par is the parent factor of a derived variable, and ch is the child variable of a deterministic factor. [sent-116, score-0.252]
</p><p>65 5  to a factor in a gate with selector variable c, the variable also links to factors in gates keyed on all other values of the selector variable c. [sent-119, score-1.953]
</p><p>66 This can be achieved by connecting the variable to uniform factors in gates for any missing values of c. [sent-120, score-0.543]
</p><p>67 After balancing, each gate is part of a gate block – a set of gates activated by different values of the same condition variable. [sent-121, score-1.791]
</p><p>68 1  Variational Message Passing with gates  VMP can be augmented to run on a gate model in normalised form, by changing only the messages out of the gate and by introducing messages from the gate to the selector variable. [sent-124, score-3.085]
</p><p>69 Messages sent between nodes inside the gate and messages into the gate are unchanged from standard VMP. [sent-125, score-1.645]
</p><p>70 The variational distributions for variables inside gates are implicitly conditioned on the gate selector, as at the end of section 3. [sent-126, score-1.237]
</p><p>71 In the following, an individual gate is denoted g, its selector variable c and its key kg . [sent-127, score-1.107]
</p><p>72 The messages out of a gate are modiﬁed as follows: • The message from a factor fa inside a gate g with selector c to a variable outside g is the usual VMP message, raised to the power mc→g (c = kg ), except in the following case. [sent-129, score-2.454]
</p><p>73 • Where a variable xi is the child of a number of deterministic factors inside a gate block G with selector variable c, the variable is treated as derived and the message is a momentmatched average of the individual VMP messages. [sent-130, score-1.644]
</p><p>74 Then the message to xi is   mG→i (xi ) = proj   g∈G  mc→g (c = kg )mg→i (xi )  (8)  where mg→i (xi ) is the usual VMP message from the unique parent factor in g and proj is a moment-matching projection onto the exponential family. [sent-131, score-0.824]
</p><p>75 The message from a gate g to its selector variable c is a product of evidence messages from the contained nodes: mg→c (c = kg ) =  sa a∈g  mg→c (c = kg ) = 1  si ,  (9)  i∈g  where sa and si are the VMP evidence messages from a factor and variable, respectively (Table 1). [sent-132, score-2.129]
</p><p>76 Deterministic variables and factors send evidence messages of 1, except where a deterministic factor fa parents a variable xi outside g. [sent-134, score-0.709]
</p><p>77 To allow for nested gates, we must also deﬁne an evidence message for a gate:  q(c=kg ) sg =   a∈g  4. [sent-136, score-0.315]
</p><p>78 2  sa  i∈g  si   (12)  Expectation Propagation with gates  As with VMP, EP can support gate models in normalised form by making small modiﬁcations to the message-passing rules. [sent-137, score-1.217]
</p><p>79 Once again, messages between nodes inside a gate are unchanged. [sent-138, score-0.95]
</p><p>80 Recall that, following gate balancing, all gates are part of gate blocks. [sent-139, score-1.755]
</p><p>81 In the following, an individual gate is denoted g, its selector variable c and its key kg . [sent-140, score-1.107]
</p><p>82 6  The messages into a gate are as follows: • The message from a selector variable to each gate in a gate block G is the same. [sent-142, score-2.746]
</p><p>83 It is the product of all messages into the variable excluding messages from gates in G. [sent-143, score-0.831]
</p><p>84 • The message from a variable to each neighboring factor inside a gate block G is the same. [sent-144, score-1.188]
</p><p>85 It is product of all messages into the variable excluding messages from any factor in G. [sent-145, score-0.506]
</p><p>86 Each gate computes an intermediate evidence-like quantity sg deﬁned as: sg =  sa a∈g  si i∈g  sig  where sig =  mi→g (xi )mg→i (xi )  (13)  xi  i∈nbrs(g)  where mg→i is the usual EP message to xi from its (unique) neighboring factor in g. [sent-147, score-1.429]
</p><p>87 ) • The message from a gate block G to its selector variable c is: sg mG→c (c = kg ) = g∈G sg Finally, the evidence contribution of a gate block with selector c is: g∈G sg sc = i∈nbrs(g) xi mi→g (xi )mG→i (xi ) 4. [sent-150, score-2.611]
</p><p>88 3  (14)  (15)  (16)  Gibbs sampling with gates  Gibbs sampling can easily extend to gates which contain only factors. [sent-151, score-0.826]
</p><p>89 If the factor is in a gate that is currently off, replace with a uniform distribution. [sent-158, score-0.759]
</p><p>90 For a gate g with selector xi , the conditional distribution is proportional to s for the key value and 1 otherwise, where s is the product of all factors in g. [sent-159, score-1.111]
</p><p>91 5  Enlarging gates to increase approximation accuracy  Gates induce a structured approximation as in [9], so by moving nodes inside or outside of gates, you can trade off inference accuracy versus cost. [sent-163, score-0.592]
</p><p>92 Because one gate of a gate block is always on, any node (variable or factor) outside a gate block G can be equivalently placed inside each gate of G. [sent-164, score-2.895]
</p><p>93 Their modiﬁcation can be viewed as a gate enlargement (ﬁgure 3). [sent-167, score-0.671]
</p><p>94 By enlarging the gate block to include unm , the blurring between the multiplication factor and unm is removed, increasing accuracy. [sent-168, score-1.003]
</p><p>95 This comes at no additional cost since unm is only used by one gate and therefore only one message is needed per n and m. [sent-169, score-0.924]
</p><p>96 N  Figure 3: Student-t mixture model using gates (a) Model from [13] (b) Structured approximation suggested by [14], which can be interpreted as enlarging the gate. [sent-178, score-0.512]
</p><p>97 Gates are also used for computing the evidence for a model, by placing the entire model in a gate with binary selector variable b. [sent-181, score-1.05]
</p><p>98 Similarly, gates are used for model comparison by placing each model in a different gate of a gate block. [sent-183, score-1.772]
</p><p>99 The marginal over the selector gives the posterior distribution over models. [sent-184, score-0.259]
</p><p>100 We have shown that gates are similarly effective both as a graphical modelling notation and as a construct within an inference algorithm. [sent-186, score-0.502]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('gate', 0.671), ('gates', 0.413), ('selector', 0.259), ('message', 0.201), ('messages', 0.181), ('vmp', 0.168), ('mk', 0.144), ('xa', 0.138), ('mg', 0.117), ('inside', 0.098), ('kg', 0.094), ('fa', 0.094), ('factor', 0.088), ('xi', 0.08), ('factors', 0.074), ('blurred', 0.073), ('blurring', 0.073), ('proj', 0.072), ('sg', 0.067), ('sa', 0.065), ('trait', 0.063), ('variable', 0.056), ('mi', 0.053), ('unm', 0.052), ('ep', 0.05), ('mixture', 0.049), ('mj', 0.049), ('independencies', 0.048), ('evidence', 0.047), ('passing', 0.046), ('outside', 0.041), ('normalised', 0.038), ('neighboring', 0.038), ('genetic', 0.037), ('variational', 0.036), ('block', 0.036), ('nbrs', 0.036), ('notation', 0.035), ('graphical', 0.033), ('pick', 0.032), ('enlarging', 0.031), ('si', 0.03), ('blur', 0.029), ('deterministic', 0.029), ('child', 0.028), ('gibbs', 0.028), ('exp', 0.028), ('zn', 0.027), ('key', 0.027), ('log', 0.026), ('raising', 0.025), ('sent', 0.024), ('diagrams', 0.024), ('container', 0.024), ('containment', 0.024), ('contingent', 0.024), ('mpar', 0.024), ('submodel', 0.024), ('touching', 0.024), ('usable', 0.024), ('xch', 0.024), ('gn', 0.023), ('connected', 0.023), ('dependencies', 0.022), ('graphs', 0.022), ('propagation', 0.022), ('gamma', 0.021), ('links', 0.021), ('mc', 0.021), ('upward', 0.021), ('sig', 0.021), ('archambeau', 0.021), ('inference', 0.021), ('xj', 0.02), ('contained', 0.02), ('xn', 0.02), ('variables', 0.019), ('false', 0.019), ('winn', 0.019), ('structured', 0.019), ('interpreted', 0.019), ('sending', 0.018), ('exponents', 0.018), ('downward', 0.018), ('conjunction', 0.018), ('default', 0.018), ('gure', 0.017), ('ma', 0.017), ('microsoft', 0.017), ('placing', 0.017), ('graphically', 0.016), ('balancing', 0.016), ('minka', 0.016), ('bayesian', 0.016), ('parent', 0.016), ('copy', 0.016), ('associations', 0.015), ('switches', 0.015), ('expectation', 0.015), ('intensities', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999863 <a title="89-tfidf-1" href="./nips-2008-Gates.html">89 nips-2008-Gates</a></p>
<p>Author: Tom Minka, John Winn</p><p>Abstract: Gates are a new notation for representing mixture models and context-sensitive independence in factor graphs. Factor graphs provide a natural representation for message-passing algorithms, such as expectation propagation. However, message passing in mixture models is not well captured by factor graphs unless the entire mixture is represented by one factor, because the message equations have a containment structure. Gates capture this containment structure graphically, allowing both the independences and the message-passing equations for a model to be readily visualized. Different variational approximations for mixture models can be understood as different ways of drawing the gates in a model. We present general equations for expectation propagation and variational message passing in the presence of gates. 1</p><p>2 0.12502657 <a title="89-tfidf-2" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables. The bound is obtained by propagating local bounds (convex sets of probability distributions) over a subtree of the factor graph, rooted in the variable of interest. By construction, the method not only bounds the exact marginal probability distribution of a variable, but also its approximate Belief Propagation marginal (“belief”). Thus, apart from providing a practical means to calculate bounds on marginals, our contribution also lies in providing a better understanding of the error made by Belief Propagation. We show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis. 1</p><p>3 0.098025411 <a title="89-tfidf-3" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>Author: Peng Xu, Timothy K. Horiuchi, Pamela A. Abshire</p><p>Abstract: We report a compact realization of short-term depression (STD) in a VLSI stochastic synapse. The behavior of the circuit is based on a subtractive single release model of STD. Experimental results agree well with simulation and exhibit expected STD behavior: the transmitted spike train has negative autocorrelation and lower power spectral density at low frequencies which can remove redundancy in the input spike train, and the mean transmission probability is inversely proportional to the input spike rate which has been suggested as an automatic gain control mechanism in neural systems. The dynamic stochastic synapse could potentially be a powerful addition to existing deterministic VLSI spiking neural systems. 1</p><p>4 0.092522688 <a title="89-tfidf-4" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>Author: David Sontag, Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: We propose a new class of consistency constraints for Linear Programming (LP) relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and signiﬁcantly faster than a method that does not use partitioning. 1</p><p>5 0.09085983 <a title="89-tfidf-5" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>Author: Michael Isard, John MacCormick, Kannan Achan</p><p>Abstract: Continuously-Adaptive Discretization for Message-Passing (CAD-MP) is a new message-passing algorithm for approximate inference. Most message-passing algorithms approximate continuous probability distributions using either: a family of continuous distributions such as the exponential family; a particle-set of discrete samples; or a ﬁxed, uniform discretization. In contrast, CAD-MP uses a discretization that is (i) non-uniform, and (ii) adaptive to the structure of the marginal distributions. Non-uniformity allows CAD-MP to localize interesting features (such as sharp peaks) in the marginal belief distributions with time complexity that scales logarithmically with precision, as opposed to uniform discretization which scales at best linearly. We give a principled method for altering the non-uniform discretization according to information-based measures. CAD-MP is shown in experiments to estimate marginal beliefs much more precisely than competing approaches for the same computational expense. 1</p><p>6 0.080308184 <a title="89-tfidf-6" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>7 0.067701034 <a title="89-tfidf-7" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<p>8 0.065323435 <a title="89-tfidf-8" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>9 0.049449503 <a title="89-tfidf-9" href="./nips-2008-Improving_on_Expectation_Propagation.html">105 nips-2008-Improving on Expectation Propagation</a></p>
<p>10 0.046908088 <a title="89-tfidf-10" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>11 0.045744333 <a title="89-tfidf-11" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>12 0.043936457 <a title="89-tfidf-12" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>13 0.04182085 <a title="89-tfidf-13" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>14 0.03786533 <a title="89-tfidf-14" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>15 0.035710726 <a title="89-tfidf-15" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>16 0.032820199 <a title="89-tfidf-16" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>17 0.032481343 <a title="89-tfidf-17" href="./nips-2008-Hierarchical_Fisher_Kernels_for_Longitudinal_Data.html">97 nips-2008-Hierarchical Fisher Kernels for Longitudinal Data</a></p>
<p>18 0.030240057 <a title="89-tfidf-18" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>19 0.029136892 <a title="89-tfidf-19" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>20 0.028921505 <a title="89-tfidf-20" href="./nips-2008-Near-optimal_Regret_Bounds_for_Reinforcement_Learning.html">150 nips-2008-Near-optimal Regret Bounds for Reinforcement Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.101), (1, 0.019), (2, 0.014), (3, 0.019), (4, 0.02), (5, -0.088), (6, 0.014), (7, 0.02), (8, -0.003), (9, -0.048), (10, -0.008), (11, 0.075), (12, 0.084), (13, -0.045), (14, -0.09), (15, 0.024), (16, 0.064), (17, -0.048), (18, 0.015), (19, 0.004), (20, 0.086), (21, -0.065), (22, -0.145), (23, 0.005), (24, 0.021), (25, -0.088), (26, 0.077), (27, -0.057), (28, 0.017), (29, 0.02), (30, -0.152), (31, 0.014), (32, 0.007), (33, 0.023), (34, -0.197), (35, 0.065), (36, 0.011), (37, 0.037), (38, -0.037), (39, -0.079), (40, 0.037), (41, 0.108), (42, -0.135), (43, -0.016), (44, 0.046), (45, -0.113), (46, 0.098), (47, -0.076), (48, 0.075), (49, 0.071)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96004844 <a title="89-lsi-1" href="./nips-2008-Gates.html">89 nips-2008-Gates</a></p>
<p>Author: Tom Minka, John Winn</p><p>Abstract: Gates are a new notation for representing mixture models and context-sensitive independence in factor graphs. Factor graphs provide a natural representation for message-passing algorithms, such as expectation propagation. However, message passing in mixture models is not well captured by factor graphs unless the entire mixture is represented by one factor, because the message equations have a containment structure. Gates capture this containment structure graphically, allowing both the independences and the message-passing equations for a model to be readily visualized. Different variational approximations for mixture models can be understood as different ways of drawing the gates in a model. We present general equations for expectation propagation and variational message passing in the presence of gates. 1</p><p>2 0.69590294 <a title="89-lsi-2" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>Author: Joris M. Mooij, Hilbert J. Kappen</p><p>Abstract: We propose a novel bound on single-variable marginal probability distributions in factor graphs with discrete variables. The bound is obtained by propagating local bounds (convex sets of probability distributions) over a subtree of the factor graph, rooted in the variable of interest. By construction, the method not only bounds the exact marginal probability distribution of a variable, but also its approximate Belief Propagation marginal (“belief”). Thus, apart from providing a practical means to calculate bounds on marginals, our contribution also lies in providing a better understanding of the error made by Belief Propagation. We show that our bound outperforms the state-of-the-art on some inference problems arising in medical diagnosis. 1</p><p>3 0.58005768 <a title="89-lsi-3" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>Author: Michael Isard, John MacCormick, Kannan Achan</p><p>Abstract: Continuously-Adaptive Discretization for Message-Passing (CAD-MP) is a new message-passing algorithm for approximate inference. Most message-passing algorithms approximate continuous probability distributions using either: a family of continuous distributions such as the exponential family; a particle-set of discrete samples; or a ﬁxed, uniform discretization. In contrast, CAD-MP uses a discretization that is (i) non-uniform, and (ii) adaptive to the structure of the marginal distributions. Non-uniformity allows CAD-MP to localize interesting features (such as sharp peaks) in the marginal belief distributions with time complexity that scales logarithmically with precision, as opposed to uniform discretization which scales at best linearly. We give a principled method for altering the non-uniform discretization according to information-based measures. CAD-MP is shown in experiments to estimate marginal beliefs much more precisely than competing approaches for the same computational expense. 1</p><p>4 0.54611766 <a title="89-lsi-4" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>Author: David Sontag, Amir Globerson, Tommi S. Jaakkola</p><p>Abstract: We propose a new class of consistency constraints for Linear Programming (LP) relaxations for ﬁnding the most probable (MAP) conﬁguration in graphical models. Usual cluster-based LP relaxations enforce joint consistency on the beliefs of a cluster of variables, with computational cost increasing exponentially with the size of the clusters. By partitioning the state space of a cluster and enforcing consistency only across partitions, we obtain a class of constraints which, although less tight, are computationally feasible for large clusters. We show how to solve the cluster selection and partitioning problem monotonically in the dual LP, using the current beliefs to guide these choices. We obtain a dual message passing algorithm and apply it to protein design problems where the variables have large state spaces and the usual cluster-based relaxations are very costly. The resulting method solves many of these problems exactly, and signiﬁcantly faster than a method that does not use partitioning. 1</p><p>5 0.50250608 <a title="89-lsi-5" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>Author: Rama Natarajan, Iain Murray, Ladan Shams, Richard S. Zemel</p><p>Abstract: We explore a recently proposed mixture model approach to understanding interactions between conﬂicting sensory cues. Alternative model formulations, differing in their sensory noise models and inference methods, are compared based on their ﬁt to experimental data. Heavy-tailed sensory likelihoods yield a better description of the subjects’ response behavior than standard Gaussian noise models. We study the underlying cause for this result, and then present several testable predictions of these models. 1</p><p>6 0.40935779 <a title="89-lsi-6" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>7 0.38117123 <a title="89-lsi-7" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<p>8 0.37143356 <a title="89-lsi-8" href="./nips-2008-Biasing_Approximate_Dynamic_Programming_with_a_Lower_Discount_Factor.html">37 nips-2008-Biasing Approximate Dynamic Programming with a Lower Discount Factor</a></p>
<p>9 0.36612302 <a title="89-lsi-9" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>10 0.31532145 <a title="89-lsi-10" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>11 0.30171651 <a title="89-lsi-11" href="./nips-2008-Exact_Convex_Confidence-Weighted_Learning.html">78 nips-2008-Exact Convex Confidence-Weighted Learning</a></p>
<p>12 0.29847455 <a title="89-lsi-12" href="./nips-2008-Stress%2C_noradrenaline%2C_and_realistic_prediction_of_mouse_behaviour_using_reinforcement_learning.html">222 nips-2008-Stress, noradrenaline, and realistic prediction of mouse behaviour using reinforcement learning</a></p>
<p>13 0.27480701 <a title="89-lsi-13" href="./nips-2008-Improving_on_Expectation_Propagation.html">105 nips-2008-Improving on Expectation Propagation</a></p>
<p>14 0.27091038 <a title="89-lsi-14" href="./nips-2008-On_the_asymptotic_equivalence_between_differential_Hebbian_and_temporal_difference_learning_using_a_local_third_factor.html">166 nips-2008-On the asymptotic equivalence between differential Hebbian and temporal difference learning using a local third factor</a></p>
<p>15 0.26691976 <a title="89-lsi-15" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>16 0.26288301 <a title="89-lsi-16" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>17 0.25931731 <a title="89-lsi-17" href="./nips-2008-Hebbian_Learning_of_Bayes_Optimal_Decisions.html">96 nips-2008-Hebbian Learning of Bayes Optimal Decisions</a></p>
<p>18 0.24981815 <a title="89-lsi-18" href="./nips-2008-Short-Term_Depression_in_VLSI_Stochastic_Synapse.html">209 nips-2008-Short-Term Depression in VLSI Stochastic Synapse</a></p>
<p>19 0.24917139 <a title="89-lsi-19" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>20 0.2478652 <a title="89-lsi-20" href="./nips-2008-A_Convex_Upper_Bound_on_the_Log-Partition_Function_for_Binary_Distributions.html">2 nips-2008-A Convex Upper Bound on the Log-Partition Function for Binary Distributions</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.035), (7, 0.051), (12, 0.019), (15, 0.011), (28, 0.135), (57, 0.075), (59, 0.024), (63, 0.016), (64, 0.011), (77, 0.072), (78, 0.021), (83, 0.023), (85, 0.391)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.74578708 <a title="89-lda-1" href="./nips-2008-Gates.html">89 nips-2008-Gates</a></p>
<p>Author: Tom Minka, John Winn</p><p>Abstract: Gates are a new notation for representing mixture models and context-sensitive independence in factor graphs. Factor graphs provide a natural representation for message-passing algorithms, such as expectation propagation. However, message passing in mixture models is not well captured by factor graphs unless the entire mixture is represented by one factor, because the message equations have a containment structure. Gates capture this containment structure graphically, allowing both the independences and the message-passing equations for a model to be readily visualized. Different variational approximations for mixture models can be understood as different ways of drawing the gates in a model. We present general equations for expectation propagation and variational message passing in the presence of gates. 1</p><p>2 0.42658871 <a title="89-lda-2" href="./nips-2008-Sparse_probabilistic_projections.html">216 nips-2008-Sparse probabilistic projections</a></p>
<p>Author: Cédric Archambeau, Francis R. Bach</p><p>Abstract: We present a generative model for performing sparse probabilistic projections, which includes sparse principal component analysis and sparse canonical correlation analysis as special cases. Sparsity is enforced by means of automatic relevance determination or by imposing appropriate prior distributions, such as generalised hyperbolic distributions. We derive a variational Expectation-Maximisation algorithm for the estimation of the hyperparameters and show that our novel probabilistic approach compares favourably to existing techniques. We illustrate how the proposed method can be applied in the context of cryptoanalysis as a preprocessing tool for the construction of template attacks. 1</p><p>3 0.41302973 <a title="89-lda-3" href="./nips-2008-Robust_Kernel_Principal_Component_Analysis.html">200 nips-2008-Robust Kernel Principal Component Analysis</a></p>
<p>Author: Minh H. Nguyen, Fernando Torre</p><p>Abstract: Kernel Principal Component Analysis (KPCA) is a popular generalization of linear PCA that allows non-linear feature extraction. In KPCA, data in the input space is mapped to higher (usually) dimensional feature space where the data can be linearly modeled. The feature space is typically induced implicitly by a kernel function, and linear PCA in the feature space is performed via the kernel trick. However, due to the implicitness of the feature space, some extensions of PCA such as robust PCA cannot be directly generalized to KPCA. This paper presents a technique to overcome this problem, and extends it to a uniﬁed framework for treating noise, missing data, and outliers in KPCA. Our method is based on a novel cost function to perform inference in KPCA. Extensive experiments, in both synthetic and real data, show that our algorithm outperforms existing methods. 1</p><p>4 0.40902963 <a title="89-lda-4" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>Author: Charles Cadieu, Bruno A. Olshausen</p><p>Abstract: We describe a hierarchical, probabilistic model that learns to extract complex motion from movies of the natural environment. The model consists of two hidden layers: the ﬁrst layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables. The second layer learns the higher-order structure among the time-varying phase variables. After training on natural movies, the top layer units discover the structure of phase-shifts within the ﬁrst layer. We show that the top layer units encode transformational invariants: they are selective for the speed and direction of a moving pattern, but are invariant to its spatial structure (orientation/spatial-frequency). The diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in V1 and MT. In addition, the model demonstrates how feedback from higher levels can inﬂuence representations at lower levels as a by-product of inference in a graphical model. 1</p><p>5 0.40708816 <a title="89-lda-5" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>Author: Thomas L. Griffiths, Chris Lucas, Joseph Williams, Michael L. Kalish</p><p>Abstract: Accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities: that people are estimating explicit functions, or that they are performing associative learning supported by similarity. We provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. Using the equivalence of Bayesian linear regression and Gaussian processes, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. We use this insight to deﬁne a Gaussian process model of human function learning that combines the strengths of both approaches. 1</p><p>6 0.40480199 <a title="89-lda-6" href="./nips-2008-Bayesian_Exponential_Family_PCA.html">31 nips-2008-Bayesian Exponential Family PCA</a></p>
<p>7 0.40437379 <a title="89-lda-7" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<p>8 0.40382078 <a title="89-lda-8" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>9 0.40346494 <a title="89-lda-9" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>10 0.4024322 <a title="89-lda-10" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<p>11 0.4020673 <a title="89-lda-11" href="./nips-2008-The_Infinite_Factorial_Hidden_Markov_Model.html">234 nips-2008-The Infinite Factorial Hidden Markov Model</a></p>
<p>12 0.40202022 <a title="89-lda-12" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>13 0.40162465 <a title="89-lda-13" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>14 0.40113157 <a title="89-lda-14" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>15 0.39992854 <a title="89-lda-15" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>16 0.39957377 <a title="89-lda-16" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>17 0.39917213 <a title="89-lda-17" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>18 0.39891315 <a title="89-lda-18" href="./nips-2008-Shared_Segmentation_of_Natural_Scenes_Using_Dependent_Pitman-Yor_Processes.html">208 nips-2008-Shared Segmentation of Natural Scenes Using Dependent Pitman-Yor Processes</a></p>
<p>19 0.39891243 <a title="89-lda-19" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>20 0.39843851 <a title="89-lda-20" href="./nips-2008-Fitted_Q-iteration_by_Advantage_Weighted_Regression.html">87 nips-2008-Fitted Q-iteration by Advantage Weighted Regression</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
