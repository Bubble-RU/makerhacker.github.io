<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-79" href="#">nips2008-79</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</h1>
<br/><p>Source: <a title="nips-2008-79-pdf" href="http://papers.nips.cc/paper/3418-exploring-large-feature-spaces-with-hierarchical-multiple-kernel-learning.pdf">pdf</a></p><p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>Reference: <a title="nips-2008-79-reference" href="../nips2008_reference/nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 org  Abstract For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. [sent-3, score-0.505]
</p><p>2 In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. [sent-5, score-0.149]
</p><p>3 This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance. [sent-7, score-0.492]
</p><p>4 1 Introduction In the last two decades, kernel methods have been a proliﬁc theoretical and algorithmic machine learning framework. [sent-8, score-0.26]
</p><p>5 By using appropriate regularization by Hilbertian norms, representer theorems enable to consider large and potentially inﬁnite-dimensional feature spaces while working within an implicit feature space no larger than the number of observations. [sent-9, score-0.266]
</p><p>6 This has led to numerous works on kernel design adapted to speciﬁc data types and generic kernel-based algorithms for many learning tasks (see, e. [sent-10, score-0.303]
</p><p>7 While early work has focused on efﬁcient algorithms to solve the convex optimization problems, recent research has looked at the model selection properties and predictive performance of such methods, in the linear case [3] or within the multiple kernel learning framework (see, e. [sent-14, score-0.381]
</p><p>8 Indeed, feature spaces are large and we expect the estimated predictor function to require only a small number of features, which is exactly the situation where ℓ1 -norms have proven advantageous. [sent-18, score-0.242]
</p><p>9 This leads to two natural questions that we try to answer in this paper: (1) Is it feasible to perform optimization in this very large feature space with cost which is polynomial in the size of the input space? [sent-19, score-0.298]
</p><p>10 More precisely, we consider a positive deﬁnite kernel that can be expressed as a large sum of positive deﬁnite basis or local kernels. [sent-21, score-0.298]
</p><p>11 This exactly corresponds to the situation where a large feature space is the concatenation of smaller feature spaces, and we aim to do selection among these many kernels, which may be done through multiple kernel learning. [sent-22, score-0.546]
</p><p>12 One major difﬁculty however is that the number of these smaller kernels is usually exponential in the dimension of the input space and applying multiple kernel learning directly in this decomposition would be intractable. [sent-23, score-0.715]
</p><p>13 In order to peform selection efﬁciently, we make the extra assumption that these small kernels can be embedded in a directed acyclic graph (DAG). [sent-24, score-0.598]
</p><p>14 Finally, we extend in Section 4 some of the known consistency results of the Lasso and multiple kernel learning [3, 4], and give a partial answer to the model selection capabilities of our regularization framework by giving necessary and sufﬁcient conditions for model consistency. [sent-28, score-0.476]
</p><p>15 In particular, we show that our framework is adapted to estimating consistently only the hull of the relevant variables. [sent-29, score-0.43]
</p><p>16 2 Hierarchical multiple kernel learning (HKL) We consider the problem of predicting a random variable Y ∈ Y ⊂ R from a random variable X ∈ X , where X and Y may be quite general spaces. [sent-31, score-0.305]
</p><p>17 1 Graph-structured positive deﬁnite kernels We assume that we are given a positive deﬁnite kernel k : X × X → R, and that this kernel can be expressed as the sum, over an index set V , of basis kernels kv , v ∈ V , i. [sent-45, score-1.405]
</p><p>18 e, for all x, x′ ∈ X , k(x, x′ ) = v∈V kv (x, x′ ). [sent-46, score-0.161]
</p><p>19 For each v ∈ V , we denote by Fv and Φv the feature space and feature map of kv , i. [sent-47, score-0.291]
</p><p>20 , for all x, x′ ∈ X , kv (x, x′ ) = Φv (x), Φv (x′ ) . [sent-49, score-0.161]
</p><p>21 Our sum assumption corresponds to a situation where the feature map Φ(x) and feature space F for k is the concatenation of the feature maps Φv (x) for each kernel kv , i. [sent-51, score-0.721]
</p><p>22 As mentioned earlier, we make the assumption that the set V can be embedded into a directed acyclic graph. [sent-54, score-0.163]
</p><p>23 Given a subset of nodes W ⊂ V , we can deﬁne the hull of W as the union of all ancestors of w ∈ W , i. [sent-63, score-0.407]
</p><p>24 The goal of this paper is to perform kernel selection among the kernels kv , v ∈ V . [sent-68, score-0.827]
</p><p>25 Namely, instead of considering all possible subsets of active (relevant) vertices, we are only interested in estimating correctly the hull of these relevant vertices; in Section 2. [sent-70, score-0.378]
</p><p>26 2, we design a speciﬁc sparsity-inducing norms adapted to hulls. [sent-71, score-0.15]
</p><p>27 In this paper, we primarily focus on kernels that can be expressed as “products of sums”, and on the associated p-dimensional directed grids, while noting that our framework is applicable to many other kernels. [sent-72, score-0.473]
</p><p>28 Namely, we assume that the input space X factorizes into p components X = X1 × · · ·× Xp and that we are given p sequences of length q + 1 of kernels kij (xi , x′ ), i ∈ {1, . [sent-73, score-0.362]
</p><p>29 (Middle) Example of sparsity pattern (× in light blue) and the complement of its hull (+ in light red). [sent-78, score-0.426]
</p><p>30 2, this DAG will correspond to the constraint of selecting a given product of kernels only after all the subproducts are selected. [sent-105, score-0.362]
</p><p>31 Those DAGs are especially suited to nonlinear variable selection, in particular with the polynomial and Gaussian kernels. [sent-106, score-0.16]
</p><p>32 In this context, products of kernels correspond to interactions between certain variables, and our DAG implies that we select an interaction only after all sub-interactions were already selected. [sent-107, score-0.362]
</p><p>33 Polynomial kernels We consider Xi = R, kij (xi , x′ ) = q (xi x′ )j ; the full kernel is then equal i i j p q p to k(x, x′ ) = i=1 j=0 q (xi x′ )j = i=1 (1 + xi x′ )q . [sent-108, score-0.676]
</p><p>34 Note that this is not exactly the usual i i j polynomial kernel (whose feature space is the space of multivariate polynomials of total degree less than q), since our kernel considers polynomials of maximal degree q. [sent-109, score-0.892]
</p><p>35 ′ 2  Gaussian kernels We also consider Xi = R, and the Gaussian-RBF kernel e−b(x−x ) . [sent-110, score-0.622]
</p><p>36 The following decomposition is the eigendecomposition of the non centered covariance operator for a normal distribution with variance 1/4a (see, e. [sent-111, score-0.137]
</p><p>37 The decomposition ends up being close to a polynomial kernel of inﬁnite degree, modulated by an exponential [2]. [sent-117, score-0.468]
</p><p>38 One may also use an adaptive decomposition using kernel PCA (see, e. [sent-118, score-0.308]
</p><p>39 ANOVA kernels When q = 1, the directed grid is isomorphic to the power set (i. [sent-122, score-0.44]
</p><p>40 In this setting, we can decompose the ANOVA kernel [2] as p −b(xi −x′ )2 −b(xi −x′ )2 −b xJ −x′ 2 i i J 2 , and our ) = = i=1 (1 + e J⊂{1,. [sent-125, score-0.26]
</p><p>41 , we are given a kernel (and thus a feature space) and we explore it using ℓ1 -norms. [sent-135, score-0.325]
</p><p>42 , we have a large structured set of features that we try to select from; however, the techniques developed in this paper assume that (a) each feature might be inﬁnite-dimensional and (b) that we can sum all the local kernels efﬁciently (see in particular Section 3. [sent-138, score-0.506]
</p><p>43 Following the kernel view thus seems slightly more natural. [sent-140, score-0.26]
</p><p>44 Penalizing with this norm is efﬁcient because summing all kernels kv is assumed feasible in polynomial time and we can bring to bear the usual kernel machinery; however, it does not lead to sparse solutions, where many βv will be exactly equal to zero. [sent-143, score-1.111]
</p><p>45 As said earlier, we are only interested in the hull of the selected elements βv ∈ Fv , v ∈ V ; the hull of a set I is characterized by the set of v, such that D(v) ⊂ I c , i. [sent-144, score-0.714]
</p><p>46 We thus consider the following structured block ℓ1 -norm deﬁned as = v∈V dv βD(v) dv ( w∈D(v) βw 2 )1/2 , where (dv )v∈V are positive weights. [sent-149, score-0.474]
</p><p>47 We thus consider the following minimization problem1: minβ∈Qv∈V Fv  1 n  n i=1  ℓ(yi ,  v∈V  βv , Φv (xi ) ) +  λ 2  v∈V  2  dv βD(v)  . [sent-151, score-0.237]
</p><p>48 (1)  Our Hilbertian norm is a Hilbert space instantiation of the hierarchical norms recently introduced by [5] and also considered by [7] in the MKL setting. [sent-152, score-0.203]
</p><p>49 If all Hilbert spaces are ﬁnite dimensional, our particular choice of norms corresponds to an “ℓ1 -norm of ℓ2 -norms”. [sent-153, score-0.185]
</p><p>50 In Section 3, we propose a novel algorithm to solve the associated optimization problem in time polynomial in the number of selected groups/kernels, for all group sizes, DAGs and losses. [sent-155, score-0.261]
</p><p>51 (1) consistently estimates the hull of the sparsity pattern. [sent-157, score-0.432]
</p><p>52 Finally, note that in certain settings (ﬁnite dimensional Hilbert spaces and distributions with absolutely continuous densities), these norms have the effect of selecting a given kernel only after all of its ancestors [5]. [sent-158, score-0.513]
</p><p>53 (1), as well as optimization algorithms with polynomial time complexity in the number of selected kernels. [sent-161, score-0.26]
</p><p>54 In simulations we consider total numbers of kernels larger than 1030 , and thus such efﬁcient algorithms are essential to the success of hierarchical multiple kernel learning (HKL). [sent-162, score-0.767]
</p><p>55 1 Reformulation in terms of multiple kernel learning Following [8, 9], we can simply derive an equivalent formulation of Eq. [sent-164, score-0.305]
</p><p>56 Using Cauchy-Schwarz inequality, we have that for all η ∈ RV such that η 0 and v∈V d2 ηv 1, v (  v∈V  dv βD(v) )2  v∈V  βD(v) ηv  2  =  w∈V  (  −1 v∈A(w) ηv ) −1  βw  2  ,  with equality if and only if ηv = d−1 βD(v) ( v∈V dv βD(v) ) . [sent-166, score-0.474]
</p><p>57 , weights of descendant kernels are smaller, which is consistent with the known fact that kernels should always be selected after all their ancestors. [sent-173, score-0.798]
</p><p>58 Moreover, the solution is then βw = ζw i=1 αi Φw (xi ), where α ∈ Rn are the dual parameters associated with the single kernel learning problem. [sent-179, score-0.323]
</p><p>59 (1), with ∀w, βw = ζw i=1 αi Φw (xi ), if and only if (a) given η, α is optimal for the single kernel learning problem with kernel matrix K = −1 −1 ⊤ α Kw α. [sent-182, score-0.52]
</p><p>60 w∈V ζw (η)Kw , and (b) given α, η ∈ H maximizes w∈V ( v∈A(w) ηv ) Moreover, the total duality gap can be upperbounded as the sum of the two separate duality gaps for the two optimization problems, which will be useful in Section 3. [sent-183, score-0.235]
</p><p>61 Note that in the case of “ﬂat” regular multiple kernel learning, where the DAG has no edges, we obtain back usual optimality conditions [8, 9]. [sent-185, score-0.412]
</p><p>62 In the next section, we show that this can be done in polynomial time although the number of kernels to consider leaving out is exponential (Section 3. [sent-189, score-0.522]
</p><p>63 2 Conditions for global optimality of reduced problem We let denote J the complement of the set of norms which are set to zero. [sent-192, score-0.218]
</p><p>64 We thus consider the optimal solution β of the reduced problem (on J), namely, n  2  1 , (3) minβJ ∈Qv∈J Fv n i=1 ℓ(yi , v∈J βv , Φv (xi ) ) + λ v∈V dv βD(v)∩J 2 with optimal primal variables βJ , dual variables α and optimal pair (ηJ , ζJ ). [sent-193, score-0.302]
</p><p>65 We now consider necessary conditions and sufﬁcient conditions for this solution (augmented with zeros for non active variables, i. [sent-194, score-0.19]
</p><p>66 We denote by δ = v∈J dv βD(v)∩J the optimal value of the norm for the reduced problem. [sent-198, score-0.311]
</p><p>67 (1) and all kernels in the extreme points of J are active, then we have maxt∈sources(J c ) α⊤ Kt α/d2 δ 2 . [sent-200, score-0.362]
</p><p>68 t Proposition 3 (SJ,ε ) If maxt∈sources(J c ) w∈D(t) α⊤ Kw α/( v∈A(w)∩D(t) dv )2 δ 2 + ε/λ, then the total duality gap is less than ε. [sent-201, score-0.348]
</p><p>69 The proof is fairly technical and can be found in [10]; this result constitutes the main technical contribution of the paper: it essentially allows to solve a very large optimization problem over exponentially many dimensions in polynomial time. [sent-202, score-0.192]
</p><p>70 However, the sufﬁcient condition (SJ,ε ) requires to sum over all descendants of the active kernels, which is impossible in practice (as shown in Section 5, we consider V of cardinal often greater than 1030 ). [sent-204, score-0.133]
</p><p>71 Here, we need to bring to bear the speciﬁc structure of the kernel k. [sent-205, score-0.323]
</p><p>72 In the context of directed grids we consider in this paper, if dv can also be decomposed as a product, then v∈A(w)∩D(t) dv is also factorized, and we can compute the sum over all v ∈ D(t) in linear time in p. [sent-206, score-0.638]
</p><p>73 Moreover we can cache the sums 2 w∈D(t) Kw /( v∈A(w)∩D(t) dv ) in order to save running time. [sent-207, score-0.237]
</p><p>74 3 Dual optimization for reduced or small problems When kernels kv , v ∈ V have low-dimensional feature spaces, we may use a primal representation and solve the problem in Eq. [sent-209, score-0.655]
</p><p>75 Namely, we use the same technique as [8]: we consider for ζ ∈ Z, the function B(ζ) = 1 −1 minβ∈Qv∈V Fv n n ℓ(yi , v∈V βv , Φv (xi ) )+ λ w∈V ζw βw 2 , which is the optimal value i=1 2 of the single kernel learning problem with kernel matrix w∈V ζw Kw . [sent-214, score-0.52]
</p><p>76 , positive diagonal) is added to the kernel matrices, the function B is differentiable [8]. [sent-219, score-0.298]
</p><p>77 The overall complexity of the algorithm is then proportional to O(|V |n2 )—to form the kernel matrices—plus the complexity of solving a single kernel learning problem—typically between O(n2 ) and O(n3 ). [sent-223, score-0.584]
</p><p>78 Note that the kernel matrices are never all needed explicitly, i. [sent-227, score-0.26]
</p><p>79 • Input: kernel matrices Kv ∈ Rn×n , v ∈ V , maximal gap ε, maximal # of kernels Q • Algorithm 1. [sent-231, score-0.781]
</p><p>80 (3) • Output: J, α, η  The previous algorithm will stop either when the duality gap is less than ε or when the maximal number of kernels Q has been reached. [sent-236, score-0.524]
</p><p>81 In practice, when the weights dv increase with the depth of v in the DAG (which we use in simulations), the small duality gap generally occurs before we reach a problem larger than Q. [sent-237, score-0.348]
</p><p>82 In order to obtain a polynomial complexity, the maximal out-degree of the DAG (i. [sent-239, score-0.211]
</p><p>83 , the maximal number of children of any given node) should be polynomial as well. [sent-241, score-0.211]
</p><p>84 (1) will be equal to its hull, and thus we can only hope to obtain consistency of the hull of the pattern, which we consider in this section. [sent-244, score-0.377]
</p><p>85 Following [4], we make the following assumptions on the underlying joint distribution of (X, Y ): (a) the joint covariance matrix Σ of (Φ(xv ))v∈V (deﬁned with appropriate blocks of size fv × fw ) is invertible, (b) E(Y |X) = w∈W β w , Φw (x) with W ⊂ V and var(Y |X) = σ 2 > 0 almost surely. [sent-251, score-0.268]
</p><p>86 With these simple assumptions, we obtain (see proof in [10]): Proposition 4 (Sufﬁcient condition) If  max  t∈sources(W c )  w∈D(t)  ΣwW Σ−1W Diag(dv βD(v) −1 )v∈W βW W P ( v∈A(w)∩D(t) dv )2  2  < 1, then β and the hull of W are consistently estimated when λn n1/2 → ∞ and λn → 0. [sent-252, score-0.624]
</p><p>87 Proposition 5 (Necessary condition) If the β and the hull of W are consistently estimated for some sequence λn , then maxt∈sources(W c ) ΣwW Σ−1W Diag(dv / βD(v) )v∈W β W 2 /d2 1. [sent-253, score-0.387]
</p><p>88 Moreover, the last propositions show that we indeed can estimate the correct hull of the sparsity pattern if the sufﬁcient condition is satisﬁed. [sent-255, score-0.42]
</p><p>89 5  0 2  7  HKL greedy L2  3  2  4 5 log2(p)  6  7  Figure 2: Comparison on synthetic examples: mean squared error over 40 replications (with halved standard deviations). [sent-258, score-0.129]
</p><p>90 Finally, it is worth noting that if the ratios dw / maxv∈A(w) dv tend to inﬁnity slowly with n, then we always consistently estimate the depth of the hull, i. [sent-464, score-0.285]
</p><p>91 We are currently investigating extensions to the non parametric case [4], in terms of pattern selection and universal consistency. [sent-467, score-0.133]
</p><p>92 , after the input variables were transformed by a random rotation (in this situation, the generating polynomial is not sparse anymore). [sent-474, score-0.197]
</p><p>93 We can see that in situations where the underlying predictor function is sparse (left), HKL outperforms the two other methods when the total number of variables p increases, while in the other situation where the best predictor is not sparse (right), it performs only slightly better: i. [sent-475, score-0.235]
</p><p>94 , in non sparse problems, ℓ1 -norms do not really help, but do help a lot when sparsity is expected. [sent-477, score-0.171]
</p><p>95 For all methods, the kernels were held ﬁxed, while in Table 1, we report the performance for the best regularization parameters obtained by 10 random half splits. [sent-541, score-0.42]
</p><p>96 We can see from Table 1, that HKL outperforms other methods, in particular for the datasets bank32nm, bank-32nh, pumadyn-32nm, pumadyn-32nh, which are datasets dedicated to non linear regression. [sent-542, score-0.215]
</p><p>97 6 Conclusion We have shown how to perform hierarchical multiple kernel learning (HKL) in polynomial time in the number of selected kernels. [sent-548, score-0.558]
</p><p>98 This framework may be applied to many positive deﬁnite kernels and we have focused on polynomial and Gaussian kernels used for nonlinear variable selection. [sent-549, score-0.884]
</p><p>99 We are currently investigating applications to string and graph kernels [2]. [sent-551, score-0.391]
</p><p>100 Consistency of the group Lasso and multiple kernel learning. [sent-573, score-0.305]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('kernels', 0.362), ('hull', 0.339), ('hkl', 0.311), ('kernel', 0.26), ('dv', 0.237), ('fv', 0.23), ('dag', 0.194), ('rbf', 0.175), ('kv', 0.161), ('polynomial', 0.16), ('kw', 0.153), ('hilbertian', 0.146), ('norms', 0.107), ('non', 0.089), ('sources', 0.085), ('rv', 0.085), ('jp', 0.084), ('spaces', 0.078), ('directed', 0.078), ('qv', 0.072), ('mkl', 0.072), ('dags', 0.072), ('ancestors', 0.068), ('feature', 0.065), ('ringnorm', 0.063), ('twonorm', 0.063), ('spambase', 0.063), ('datasets', 0.063), ('predictor', 0.062), ('greedy', 0.059), ('regularization', 0.058), ('maxt', 0.058), ('sj', 0.057), ('hierarchical', 0.057), ('gap', 0.057), ('descendants', 0.056), ('uci', 0.055), ('xi', 0.054), ('acyclic', 0.054), ('duality', 0.054), ('maximal', 0.051), ('polynomials', 0.048), ('grids', 0.048), ('proposition', 0.048), ('consistently', 0.048), ('kiji', 0.048), ('mushrooms', 0.048), ('decomposition', 0.048), ('rotated', 0.046), ('sparsity', 0.045), ('multiple', 0.045), ('selection', 0.044), ('simulations', 0.043), ('adapted', 0.043), ('jmlr', 0.042), ('regular', 0.042), ('ww', 0.042), ('penalizing', 0.042), ('complement', 0.042), ('try', 0.041), ('active', 0.039), ('vertices', 0.039), ('norm', 0.039), ('descendant', 0.038), ('fw', 0.038), ('replications', 0.038), ('sum', 0.038), ('differentiable', 0.038), ('consistency', 0.038), ('hilbert', 0.037), ('sparse', 0.037), ('situation', 0.037), ('selected', 0.036), ('caching', 0.036), ('hermite', 0.036), ('propositions', 0.036), ('hk', 0.036), ('reduced', 0.035), ('optimality', 0.034), ('anova', 0.034), ('abalone', 0.034), ('nj', 0.034), ('associated', 0.033), ('moreover', 0.033), ('anymore', 0.032), ('bring', 0.032), ('looking', 0.032), ('complexity', 0.032), ('optimization', 0.032), ('synthetic', 0.032), ('conditions', 0.031), ('embedded', 0.031), ('violating', 0.031), ('bear', 0.031), ('nite', 0.031), ('dual', 0.03), ('exploring', 0.03), ('concatenation', 0.03), ('summing', 0.029), ('graph', 0.029)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="79-tfidf-1" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>2 0.1831255 <a title="79-tfidf-2" href="./nips-2008-Extended_Grassmann_Kernels_for_Subspace-Based_Learning.html">80 nips-2008-Extended Grassmann Kernels for Subspace-Based Learning</a></p>
<p>Author: Jihun Hamm, Daniel D. Lee</p><p>Abstract: Subspace-based learning problems involve data whose elements are linear subspaces of a vector space. To handle such data structures, Grassmann kernels have been proposed and used previously. In this paper, we analyze the relationship between Grassmann kernels and probabilistic similarity measures. Firstly, we show that the KL distance in the limit yields the Projection kernel on the Grassmann manifold, whereas the Bhattacharyya kernel becomes trivial in the limit and is suboptimal for subspace-based problems. Secondly, based on our analysis of the KL distance, we propose extensions of the Projection kernel which can be extended to the set of afﬁne as well as scaled subspaces. We demonstrate the advantages of these extended kernels for classiﬁcation and recognition tasks with Support Vector Machines and Kernel Discriminant Analysis using synthetic and real image databases. 1</p><p>3 0.15658732 <a title="79-tfidf-3" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>Author: Yen-yu Lin, Tyng-luh Liu, Chiou-shann Fuh</p><p>Abstract: In solving complex visual learning tasks, adopting multiple descriptors to more precisely characterize the data has been a feasible way for improving performance. These representations are typically high dimensional and assume diverse forms. Thus ﬁnding a way to transform them into a uniﬁed space of lower dimension generally facilitates the underlying tasks, such as object recognition or clustering. We describe an approach that incorporates multiple kernel learning with dimensionality reduction (MKL-DR). While the proposed framework is ﬂexible in simultaneously tackling data in various feature representations, the formulation itself is general in that it is established upon graph embedding. It follows that any dimensionality reduction techniques explainable by graph embedding can be generalized by our method to consider data in multiple feature representations.</p><p>4 0.15056784 <a title="79-tfidf-4" href="./nips-2008-Characteristic_Kernels_on_Groups_and_Semigroups.html">44 nips-2008-Characteristic Kernels on Groups and Semigroups</a></p>
<p>Author: Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, Bharath K. Sriperumbudur</p><p>Abstract: Embeddings of random variables in reproducing kernel Hilbert spaces (RKHSs) may be used to conduct statistical inference based on higher order moments. For sufﬁciently rich (characteristic) RKHSs, each probability distribution has a unique embedding, allowing all statistical properties of the distribution to be taken into consideration. Necessary and sufﬁcient conditions for an RKHS to be characteristic exist for Rn . In the present work, conditions are established for an RKHS to be characteristic on groups and semigroups. Illustrative examples are provided, including characteristic kernels on periodic domains, rotation matrices, and Rn . + 1</p><p>5 0.14380032 <a title="79-tfidf-5" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>Author: Shuiwang Ji, Liang Sun, Rong Jin, Jieping Ye</p><p>Abstract: We present a multi-label multiple kernel learning (MKL) formulation in which the data are embedded into a low-dimensional space directed by the instancelabel correlations encoded into a hypergraph. We formulate the problem in the kernel-induced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the MKL framework. The proposed learning formulation leads to a non-smooth min-max problem, which can be cast into a semi-inﬁnite linear program (SILP). We further propose an approximate formulation with a guaranteed error bound which involves an unconstrained convex optimization problem. In addition, we show that the objective function of the approximate formulation is differentiable with Lipschitz continuous gradient, and hence existing methods can be employed to compute the optimal solution efﬁciently. We apply the proposed formulation to the automated annotation of Drosophila gene expression pattern images, and promising results have been reported in comparison with representative algorithms.</p><p>6 0.13582766 <a title="79-tfidf-6" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>7 0.1324098 <a title="79-tfidf-7" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>8 0.12645495 <a title="79-tfidf-8" href="./nips-2008-Hierarchical_Fisher_Kernels_for_Longitudinal_Data.html">97 nips-2008-Hierarchical Fisher Kernels for Longitudinal Data</a></p>
<p>9 0.11283199 <a title="79-tfidf-9" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>10 0.11096804 <a title="79-tfidf-10" href="./nips-2008-Scalable_Algorithms_for_String_Kernels_with_Inexact_Matching.html">203 nips-2008-Scalable Algorithms for String Kernels with Inexact Matching</a></p>
<p>11 0.11059289 <a title="79-tfidf-11" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>12 0.10596494 <a title="79-tfidf-12" href="./nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</a></p>
<p>13 0.1046053 <a title="79-tfidf-13" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>14 0.087831236 <a title="79-tfidf-14" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>15 0.086910747 <a title="79-tfidf-15" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>16 0.085501194 <a title="79-tfidf-16" href="./nips-2008-Integrating_Locally_Learned_Causal_Structures_with_Overlapping_Variables.html">108 nips-2008-Integrating Locally Learned Causal Structures with Overlapping Variables</a></p>
<p>17 0.084545925 <a title="79-tfidf-17" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>18 0.083847187 <a title="79-tfidf-18" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>19 0.083794639 <a title="79-tfidf-19" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>20 0.082386382 <a title="79-tfidf-20" href="./nips-2008-Clustered_Multi-Task_Learning%3A_A_Convex_Formulation.html">47 nips-2008-Clustered Multi-Task Learning: A Convex Formulation</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.241), (1, -0.081), (2, -0.118), (3, 0.116), (4, 0.12), (5, -0.04), (6, 0.008), (7, -0.042), (8, 0.032), (9, -0.048), (10, 0.242), (11, -0.131), (12, -0.081), (13, 0.003), (14, 0.154), (15, 0.04), (16, 0.088), (17, -0.145), (18, -0.159), (19, 0.011), (20, -0.002), (21, 0.059), (22, 0.102), (23, -0.043), (24, 0.033), (25, -0.01), (26, 0.09), (27, -0.027), (28, 0.043), (29, 0.016), (30, -0.051), (31, 0.056), (32, -0.048), (33, 0.037), (34, -0.029), (35, 0.095), (36, 0.014), (37, -0.023), (38, 0.015), (39, 0.005), (40, 0.027), (41, 0.056), (42, -0.086), (43, -0.042), (44, 0.004), (45, 0.122), (46, 0.044), (47, 0.017), (48, 0.057), (49, 0.05)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96655017 <a title="79-lsi-1" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>2 0.86161327 <a title="79-lsi-2" href="./nips-2008-Extended_Grassmann_Kernels_for_Subspace-Based_Learning.html">80 nips-2008-Extended Grassmann Kernels for Subspace-Based Learning</a></p>
<p>Author: Jihun Hamm, Daniel D. Lee</p><p>Abstract: Subspace-based learning problems involve data whose elements are linear subspaces of a vector space. To handle such data structures, Grassmann kernels have been proposed and used previously. In this paper, we analyze the relationship between Grassmann kernels and probabilistic similarity measures. Firstly, we show that the KL distance in the limit yields the Projection kernel on the Grassmann manifold, whereas the Bhattacharyya kernel becomes trivial in the limit and is suboptimal for subspace-based problems. Secondly, based on our analysis of the KL distance, we propose extensions of the Projection kernel which can be extended to the set of afﬁne as well as scaled subspaces. We demonstrate the advantages of these extended kernels for classiﬁcation and recognition tasks with Support Vector Machines and Kernel Discriminant Analysis using synthetic and real image databases. 1</p><p>3 0.79696333 <a title="79-lsi-3" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>Author: Yen-yu Lin, Tyng-luh Liu, Chiou-shann Fuh</p><p>Abstract: In solving complex visual learning tasks, adopting multiple descriptors to more precisely characterize the data has been a feasible way for improving performance. These representations are typically high dimensional and assume diverse forms. Thus ﬁnding a way to transform them into a uniﬁed space of lower dimension generally facilitates the underlying tasks, such as object recognition or clustering. We describe an approach that incorporates multiple kernel learning with dimensionality reduction (MKL-DR). While the proposed framework is ﬂexible in simultaneously tackling data in various feature representations, the formulation itself is general in that it is established upon graph embedding. It follows that any dimensionality reduction techniques explainable by graph embedding can be generalized by our method to consider data in multiple feature representations.</p><p>4 0.76732105 <a title="79-lsi-4" href="./nips-2008-Scalable_Algorithms_for_String_Kernels_with_Inexact_Matching.html">203 nips-2008-Scalable Algorithms for String Kernels with Inexact Matching</a></p>
<p>Author: Pavel P. Kuksa, Pai-hsi Huang, Vladimir Pavlovic</p><p>Abstract: We present a new family of linear time algorithms for string comparison with mismatches under the string kernels framework. Based on sufﬁcient statistics, our algorithms improve theoretical complexity bounds of existing approaches while scaling well in sequence alphabet size, the number of allowed mismatches and the size of the dataset. In particular, on large alphabets and under loose mismatch constraints our algorithms are several orders of magnitude faster than the existing algorithms for string comparison under the mismatch similarity measure. We evaluate our algorithms on synthetic data and real applications in music genre classiﬁcation, protein remote homology detection and protein fold prediction. The scalability of the algorithms allows us to consider complex sequence transformations, modeled using longer string features and larger numbers of mismatches, leading to a state-of-the-art performance with signiﬁcantly reduced running times. 1</p><p>5 0.75590616 <a title="79-lsi-5" href="./nips-2008-Characteristic_Kernels_on_Groups_and_Semigroups.html">44 nips-2008-Characteristic Kernels on Groups and Semigroups</a></p>
<p>Author: Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, Bharath K. Sriperumbudur</p><p>Abstract: Embeddings of random variables in reproducing kernel Hilbert spaces (RKHSs) may be used to conduct statistical inference based on higher order moments. For sufﬁciently rich (characteristic) RKHSs, each probability distribution has a unique embedding, allowing all statistical properties of the distribution to be taken into consideration. Necessary and sufﬁcient conditions for an RKHS to be characteristic exist for Rn . In the present work, conditions are established for an RKHS to be characteristic on groups and semigroups. Illustrative examples are provided, including characteristic kernels on periodic domains, rotation matrices, and Rn . + 1</p><p>6 0.66590911 <a title="79-lsi-6" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>7 0.66384536 <a title="79-lsi-7" href="./nips-2008-Hierarchical_Fisher_Kernels_for_Longitudinal_Data.html">97 nips-2008-Hierarchical Fisher Kernels for Longitudinal Data</a></p>
<p>8 0.66221368 <a title="79-lsi-8" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>9 0.65201008 <a title="79-lsi-9" href="./nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</a></p>
<p>10 0.64839727 <a title="79-lsi-10" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>11 0.61402887 <a title="79-lsi-11" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>12 0.55769438 <a title="79-lsi-12" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>13 0.50683564 <a title="79-lsi-13" href="./nips-2008-Efficient_Direct_Density_Ratio_Estimation_for_Non-stationarity_Adaptation_and_Outlier_Detection.html">68 nips-2008-Efficient Direct Density Ratio Estimation for Non-stationarity Adaptation and Outlier Detection</a></p>
<p>14 0.48204336 <a title="79-lsi-14" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>15 0.47073251 <a title="79-lsi-15" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>16 0.46223 <a title="79-lsi-16" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>17 0.44547379 <a title="79-lsi-17" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>18 0.43067971 <a title="79-lsi-18" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>19 0.42305505 <a title="79-lsi-19" href="./nips-2008-Theory_of_matching_pursuit.html">238 nips-2008-Theory of matching pursuit</a></p>
<p>20 0.42210704 <a title="79-lsi-20" href="./nips-2008-Fast_High-dimensional_Kernel_Summations_Using_the_Monte_Carlo_Multipole_Method.html">83 nips-2008-Fast High-dimensional Kernel Summations Using the Monte Carlo Multipole Method</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.087), (7, 0.098), (12, 0.043), (28, 0.158), (57, 0.071), (59, 0.017), (63, 0.033), (64, 0.024), (71, 0.026), (72, 0.153), (77, 0.07), (78, 0.022), (83, 0.093)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.89885348 <a title="79-lda-1" href="./nips-2008-Nonparametric_Bayesian_Learning_of_Switching_Linear_Dynamical_Systems.html">154 nips-2008-Nonparametric Bayesian Learning of Switching Linear Dynamical Systems</a></p>
<p>Author: Alan S. Willsky, Erik B. Sudderth, Michael I. Jordan, Emily B. Fox</p><p>Abstract: Many nonlinear dynamical phenomena can be effectively modeled by a system that switches among a set of conditionally linear dynamical modes. We consider two such models: the switching linear dynamical system (SLDS) and the switching vector autoregressive (VAR) process. Our nonparametric Bayesian approach utilizes a hierarchical Dirichlet process prior to learn an unknown number of persistent, smooth dynamical modes. We develop a sampling algorithm that combines a truncated approximation to the Dirichlet process with efﬁcient joint sampling of the mode and state sequences. The utility and ﬂexibility of our model are demonstrated on synthetic data, sequences of dancing honey bees, and the IBOVESPA stock index.</p><p>same-paper 2 0.88318884 <a title="79-lda-2" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>3 0.82243943 <a title="79-lda-3" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>4 0.81825817 <a title="79-lda-4" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>5 0.81794912 <a title="79-lda-5" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>6 0.81596249 <a title="79-lda-6" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>7 0.80929554 <a title="79-lda-7" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>8 0.80923778 <a title="79-lda-8" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>9 0.80763054 <a title="79-lda-9" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>10 0.80633694 <a title="79-lda-10" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>11 0.80551147 <a title="79-lda-11" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>12 0.8053214 <a title="79-lda-12" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>13 0.80318737 <a title="79-lda-13" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>14 0.80031407 <a title="79-lda-14" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<p>15 0.80008847 <a title="79-lda-15" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>16 0.80006111 <a title="79-lda-16" href="./nips-2008-Large_Margin_Taxonomy_Embedding_for_Document_Categorization.html">114 nips-2008-Large Margin Taxonomy Embedding for Document Categorization</a></p>
<p>17 0.79966652 <a title="79-lda-17" href="./nips-2008-MCBoost%3A_Multiple_Classifier_Boosting_for_Perceptual_Co-clustering_of_Images_and_Visual_Features.html">130 nips-2008-MCBoost: Multiple Classifier Boosting for Perceptual Co-clustering of Images and Visual Features</a></p>
<p>18 0.7995559 <a title="79-lda-18" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>19 0.79861921 <a title="79-lda-19" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>20 0.7983464 <a title="79-lda-20" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
