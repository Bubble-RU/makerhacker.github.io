<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-156" href="#">nips2008-156</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</h1>
<br/><p>Source: <a title="nips-2008-156-pdf" href="http://papers.nips.cc/paper/3481-nonparametric-sparse-hierarchical-models-describe-v1-fmri-responses-to-natural-images.pdf">pdf</a></p><p>Author: Vincent Q. Vu, Bin Yu, Thomas Naselaris, Kendrick Kay, Jack Gallant, Pradeep K. Ravikumar</p><p>Abstract: We propose a novel hierarchical, nonlinear model that predicts brain activity in area V1 evoked by natural images. In the study reported here brain activity was measured by means of functional magnetic resonance imaging (fMRI), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (≈ 2mm cube) of brain tissue. Our model, which we call the V-SPAM model, is based on the reasonable assumption that fMRI measurements reﬂect the (possibly nonlinearly) pooled, rectiﬁed output of a large population of simple and complex cells in V1. It has a hierarchical ﬁltering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called “pooled-complex” cells). The pooling stage then obtains the measured fMRI signals as a sparse additive model (SpAM) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed. Our results show that the V-SPAM model predicts fMRI responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells. Furthermore, the spatial receptive ﬁelds, frequency tuning and orientation tuning curves of the V-SPAM model estimated for each voxel appears to be consistent with the known properties of V1, and with previous analyses of this data set. A visualization procedure applied to the V-SPAM model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities. 1</p><p>Reference: <a title="nips-2008-156-reference" href="../nips2008_reference/nips-2008-Nonparametric_sparse_hierarchical_models_describe_V1_fMRI_responses_to_natural_images_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images  Pradeep Ravikumar, Vincent Q. [sent-1, score-0.181]
</p><p>2 Gallant Department of Psychology University of California, Berkeley Berkeley, CA  Abstract We propose a novel hierarchical, nonlinear model that predicts brain activity in area V1 evoked by natural images. [sent-4, score-0.292]
</p><p>3 In the study reported here brain activity was measured by means of functional magnetic resonance imaging (fMRI), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (≈ 2mm cube) of brain tissue. [sent-5, score-0.227]
</p><p>4 Our model, which we call the V-SPAM model, is based on the reasonable assumption that fMRI measurements reﬂect the (possibly nonlinearly) pooled, rectiﬁed output of a large population of simple and complex cells in V1. [sent-6, score-0.24]
</p><p>5 It has a hierarchical ﬁltering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called “pooled-complex” cells). [sent-7, score-0.521]
</p><p>6 The pooling stage then obtains the measured fMRI signals as a sparse additive model (SpAM) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed. [sent-8, score-1.048]
</p><p>7 Our results show that the V-SPAM model predicts fMRI responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells. [sent-9, score-0.499]
</p><p>8 Furthermore, the spatial receptive ﬁelds, frequency tuning and orientation tuning curves of the V-SPAM model estimated for each voxel appears to be consistent with the known properties of V1, and with previous analyses of this data set. [sent-10, score-0.613]
</p><p>9 A visualization procedure applied to the V-SPAM model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities. [sent-11, score-0.206]
</p><p>10 1  Introduction  An important step toward understanding the neural basis of vision is to develop computational models that describe how complex visual stimuli are mapping onto evoked neuronal responses. [sent-12, score-0.219]
</p><p>11 Although the relationship between measured fMRI activity and the spiking activity of neurons is rather complex, as a ﬁrst-order approximation the fMRI signal can be considered to be monotonically related to the pooled activity of the underlying neural population. [sent-16, score-0.198]
</p><p>12 1  In this paper we consider the task of predicting fMRI brain activity evoked by a series of grayscale natural images. [sent-17, score-0.209]
</p><p>13 Natural images are a useful stimulus set for efﬁciently probing the visual system, because they are likely to evoke response from both early visual areas and from more central, highly nonlinear visual areas. [sent-18, score-0.24]
</p><p>14 The fMRI scanner provides a three-dimensional image of the brain with a spatial resolution of a few cubic millimeters and fairly low temporal resolution (about 0. [sent-19, score-0.162]
</p><p>15 Here we restrict our analysis to voxels sampled from visual area V1, the primary visual area in humans. [sent-22, score-0.229]
</p><p>16 There are two problems that make predicting evoked responses of fMRI voxels difﬁcult. [sent-23, score-0.257]
</p><p>17 Second, each voxel reﬂects the combined inﬂuence of hundreds of thousands of neurons [4]. [sent-25, score-0.226]
</p><p>18 fMRI scans of a single voxel in human V1 likely reﬂect the nonlinearly-pooled, rectiﬁed outputs of two functionally distinct classes of neurons: simple cells that are sensitive to spatial phase, and phase-invariant complex cells [2]. [sent-26, score-0.695]
</p><p>19 In order for a predictive model to advance our understanding of the brain, the function of any predictive model must be conceptually interpretable. [sent-31, score-0.13]
</p><p>20 Our V-SPAM model is a hierarchical and sparse nonparametric additive model. [sent-33, score-0.203]
</p><p>21 It combines a biologically-inspired hierarchical ﬁltering scheme with a nonlinear (nonparametric) pooling of the outputs from various levels of the hierarchical ﬁltering stage. [sent-34, score-0.312]
</p><p>22 The model is estimated separately for each recorded fMRI voxel using a ﬁt data set, and then its predictions are evaluated against an entirely separate data set reserved for this purpose. [sent-35, score-0.253]
</p><p>23 The ﬁltering component of the model consists of three distinct layers: simple cells, complex cells, and linear combinations of the complex cells (here called pooled-complex cells). [sent-36, score-0.329]
</p><p>24 The fMRI response is then modeled as a sparse additive combination of nonlinear (nonparametric) functions of the complex and pooled-complex cell model outputs. [sent-37, score-0.533]
</p><p>25 This last step automatically learns the optimal combinatorial output nonlinearity of the hierarchical ﬁltering stage, and so permits us to model nonlinear V1 responses not captured by the simple and complex cell model components alone [6]. [sent-38, score-0.491]
</p><p>26 That study also used a ﬁltering model to describe the relationship between natural images and evoked fMRI signals, and used the estimated models in turn to decode (identify) images. [sent-40, score-0.217]
</p><p>27 However, the earlier study only provided linear pooling of model complex cell ﬁlters. [sent-41, score-0.444]
</p><p>28 Our results show that the V-SPAM model predicts fMRI responses evoked by natural images better than does the earlier linear pooling model. [sent-42, score-0.373]
</p><p>29 Furthermore, the spatial receptive ﬁelds, frequency tuning and orientation tuning curves of the V-SPAM model estimated for each voxel appear to be consistent with the known properties of V1, and with the previous results [5]. [sent-43, score-0.613]
</p><p>30 The sparse additive model (SpAM) framework of Ravikumar et al [7] extends these sparse linear models to the nonparametric domain. [sent-58, score-0.219]
</p><p>31 A sparse additive model then imposes a sparsity constraint on the set J = {j : fj ≡ 0} of functions fj that are nonzero. [sent-62, score-0.309]
</p><p>32 j ˆ Soft-threshold: fj = [1 − λ/ˆj ]+ Pj ; s Center: fj ← fj − mean(fj ). [sent-77, score-0.261]
</p><p>33 Output: Component functions fj and estimator m(Xi ) = j fj (Xij ). [sent-78, score-0.174]
</p><p>34 ˆ Figure 1: T HE S PAM BACKFITTING A LGORITHM  3  A model for pooled neural activity of voxels  Our V-SPAM model combines a biologically-inspired ﬁltering scheme and a novel algorithm that permits nonlinear pooling of the outputs of the ﬁltering stage. [sent-79, score-0.514]
</p><p>35 The ﬁltering stage itself consists of three distinct layers, arranged hierarchically: simple cells, complex cells, and linear combinations of the complex cells (here called pooled-complex cells). [sent-80, score-0.38]
</p><p>36 The output of this ﬁltering operation is then fed to an algorithm that estimates a nonlinear pooling function that optimizes predictive power. [sent-81, score-0.237]
</p><p>37 1  Simple Cell Model  The ﬁrst stage of the hierarchical ﬁlter is inspired by simple cells that are known to exist in area V1. [sent-83, score-0.268]
</p><p>38 The receptive ﬁelds of V1 simple cells are known to be generally consistent with a Gabor wavelet model [6]. [sent-84, score-0.244]
</p><p>39 Most importantly, they are spatially localized, oriented, spatial frequency band-pass and phase selective. [sent-85, score-0.159]
</p><p>40 Each row shows a family of Gabor wavelets that share a common spatial location and frequency, but differ in orientation. [sent-88, score-0.151]
</p><p>41 3  In our model the simple cell ﬁlter bank was implemented as a Gabor wavelet pyramid, as follows. [sent-90, score-0.286]
</p><p>42 Then our simple cell model, for the activation given the image I as stimulus, is given by, Xj (I) = [ j I ]+ , where is the Euclidean inner product, and [ ]+ is a non-negative rectiﬁcation. [sent-94, score-0.247]
</p><p>43 image  Gabor wavelet  output  non-negative rectiﬁcation  Figure 3: Simple cell model. [sent-97, score-0.299]
</p><p>44 The activation of a model simple cell given an image is the inner product of the image with a Gabor wavelet, followed by a non-negative rectiﬁcation. [sent-98, score-0.325]
</p><p>45 2  Complex Cell Model  The second stage of the hierarchical ﬁlter is inspired by complex cells that are also known to exist in area V1. [sent-100, score-0.352]
</p><p>46 Complex cells are similar to simple cells, except they are not sensitive to spatial phase. [sent-101, score-0.209]
</p><p>47 In our model the complex cell ﬁlter bank was implemented by taking the sum of squares of the outputs of four simple cells (corresponding to the wavelet pairs that are identical up to phase), followed by a ﬁxed output nonlinearity. [sent-102, score-0.608]
</p><p>48 The activation of the model complex cell given an image I is given by, Xj (I) = log(1 +  j  j  I ]2 + [ +  = log(1 + where ure 4). [sent-103, score-0.36]
</p><p>49 [ [  j  I ]2 + [  and  j  j j  I ]2 + [ +  j  I ]2 + [ +  j  I ]2 ) +  I ]2 )  (1) (2)  are Gabor wavelets identical up to phase (also called a quadrature pair; see Fig-  +  image  output Gabor wavelet quadrature pair  squaring  ﬁxed nonlinearity  Figure 4: Complex cell model. [sent-104, score-0.469]
</p><p>50 The activation of a model complex cell given an image is the sum of squares of the inner products of the image with a quadrature pair of Gabor wavelets followed by a nonlinearity. [sent-105, score-0.498]
</p><p>51 This is equivalently modeled by summing the squares of 4 simple cell model outputs, followed by a nonlinearity. [sent-106, score-0.247]
</p><p>52 3  Pooled-complex Cell Model  The hierarchical ﬁltering component of our model also includes a third ﬁltering stage, linear pooling of complex cells sharing a common spatial location and frequency. [sent-108, score-0.506]
</p><p>53 Xjk correspond to complex cells with the same spatial location and frequency, then If Xj1 the corresponding pooled-complex cell (which thus sums over different orientations) is given by, k Zj1 jk = l=1 Xjl . [sent-111, score-0.502]
</p><p>54 ) 4  +  +  output  image  +  complex cells  Figure 5: Pooled-complex cell model. [sent-113, score-0.461]
</p><p>55 Subsets of complex cells that share a common spatial location and frequency are summed. [sent-114, score-0.375]
</p><p>56 4  V-SPAM model  Finally, the predicted fMRI response Y is obtained as a sparse additive combination of complex Xp , and the cell and pooled-complex cell outputs. [sent-116, score-0.681]
</p><p>57 Denote the complex cell outputs by X1 pooled-complex cell outputs by Z1 Zq . [sent-117, score-0.592]
</p><p>58 Then the fMRI response Y is modeled as a sparse p q additive (nonparametric) model, Y = j=1 fj (Xj ) + l=1 gl (Zl ) + φ. [sent-118, score-0.256]
</p><p>59 Figure 6 summarizes the entire V-SPAM model, including both ﬁltering and pooling components. [sent-119, score-0.128]
</p><p>60 + image  fMRI voxel response  simple cell outputs  complex cell outputs  pooled-complex cell outputs  nonlinearities  Figure 6: V-SPAM model. [sent-120, score-1.19]
</p><p>61 The fMRI voxel response is modeled as the summation of nonlinear functions of complex and pooled-complex cell outputs. [sent-121, score-0.586]
</p><p>62 1  Experiments Data description  The data set analyzed in this paper consists of a total of 1,294 voxels recorded from area V1 of one human observer. [sent-124, score-0.143]
</p><p>63 A 4T Varian MRI scanner provided voxels of size 2mm x 2mm x 2. [sent-125, score-0.141]
</p><p>64 2  V-SPAM model ﬁtting  The V-SPAM model was ﬁtted separately for each of the 1,294 voxels using the training set of 1,750 images and the evoked fMRI responses. [sent-138, score-0.33]
</p><p>65 In the ﬁrst stage, the model complex cell outputs are computed according to equation (2) using a pyramid (or family) of Gabor wavelets sampled on a grid of 128 x 128 pixels. [sent-140, score-0.449]
</p><p>66 At each spatial frequency ω the wavelets are positioned evenly on a ω × ω grid covering the image. [sent-142, score-0.189]
</p><p>67 In the second stage, the model complex cell outputs are pre-screened in order to eliminate complex cell outputs that are unrelated to a voxel’s response, and to reduce the computational complexity of successive stages of ﬁtting. [sent-145, score-0.743]
</p><p>68 This is accomplished by considering the squared-correlation of the response of each complex cell with the evoked voxel response, using the 1,750 images in the training set. [sent-146, score-0.672]
</p><p>69 In the third stage, pooled-complex cells (see Section 3) are formed from the complex cell outputs that passed the pre-screening in ﬁtting stage 2. [sent-149, score-0.55]
</p><p>70 In the fourth and ﬁnal stage, the complex and pooled-complex cell responses to the images in the training set are used as predictors in the SpAM ﬁtting algorithm (see Figure 1), and this is optimized to ﬁt the voxel responses evoked by the same 1,750 images in the training set. [sent-150, score-0.76]
</p><p>71 3  Model validation  For each voxel, we evaluate the ﬁtted V-SPAM models by computing the predictive R2 (squared correlation) of the predicted and actual fMRI responses evoked by each of the 120 images in the validation set. [sent-153, score-0.246]
</p><p>72 The sparse linear pooling model aims to predict each voxel’s response as a linear combination of all 10,921 p estimated complex cell outputs. [sent-155, score-0.561]
</p><p>73 This model has the form, Y (I) = β0 + j=1 βj Xj (I) + , where the Xj (I) are the complex cell outputs estimated according to (2), with the p = 10, 921 Gabor wavelets described in Section 4. [sent-156, score-0.44]
</p><p>74 5  Results  Figure 7 (left) shows a scatterplot comparing the performance of the V-SPAM model with that of the sparse linear pooling model for all 1,294 voxels. [sent-163, score-0.256]
</p><p>75 The vertical axis gives performance of the V-SPAM model, and the horizontal axis the sparse linear pooling model. [sent-164, score-0.218]
</p><p>76 The inset region contains 429 voxels for which both models had some predictive power (R2 ≥ 0. [sent-166, score-0.19]
</p><p>77 For these voxels, the relative improvement of the V-SPAM model over the sparse linear pooling model is shown in the histogram to the right. [sent-168, score-0.236]
</p><p>78 The predictions of the V-SPAM model were on average 14% better than those of the sparse linear pooling model (standard deviation 17%). [sent-169, score-0.236]
</p><p>79 1  Estimated receptive ﬁelds and tuning curves  Figure 8 shows the spatial receptive-ﬁelds (RF’s) and joint frequency and orientation tuning curves estimated using the V-SPAM model for 3 voxels. [sent-171, score-0.436]
</p><p>80 These voxels were chosen because they had high predictive power (R2 ’s of 0. [sent-172, score-0.176]
</p><p>81 6  −20  sparse linear pooling model  0  20  40  60  80  100  relative improvement (%)  (mean = 14, SD = 17, median = 12, IQR = 17)  Figure 7: Predictive R2 of the ﬁtted V-SPAM model compared against the ﬁtted sparse linear pooling model. [sent-192, score-0.414]
</p><p>82 (Right) Relative performance for the 429 voxels contained in the inset region on the left. [sent-194, score-0.135]
</p><p>83 location in the spatial RF represents the standardized predicted response of the voxel to an image stimulus consisting of a single pixel at that location. [sent-195, score-0.445]
</p><p>84 The spatial RF’s of these voxels are clearly localized in space, consistent with the known retinotopic organization of V1 and previous fMRI results [9]. [sent-196, score-0.198]
</p><p>85 The lower row of Figure 8 shows the joint frequency and orientation tuning properties of these same 3 voxels. [sent-197, score-0.171]
</p><p>86 Here the tuning curves were estimated by computing the predicted response of the ﬁtted voxel model to cosine gratings of varying orientation (degrees) and spatial frequency (cycles/ﬁeld of view). [sent-198, score-0.634]
</p><p>87 All of the voxels are tuned to spatial frequencies above about 8 cycles/ﬁeld of view, while orientation tuning varies from voxel to voxel. [sent-199, score-0.512]
</p><p>88 The joint spatial frequency and orientation tuning of all 3 voxels appears to be non-separable (i. [sent-200, score-0.369]
</p><p>89 Each location in the spatial RF shows the standardized predicted response of the voxel to an image consisting of a single pixel at that location. [sent-207, score-0.445]
</p><p>90 The tuning curves show the standardized predicted response of the voxel to cosine gratings of varying orientation (degrees) and spatial frequency (cycles/ﬁeld of view). [sent-208, score-0.606]
</p><p>91 2  Nonlinearities  One of the potential advantages of the V-SPAM model over other approaches is that it can reveal novel nonlinear tuning and pooling properties, as revealed by the nonlinear summation occurring in the ﬁnal stage of the V-SPAM model. [sent-210, score-0.392]
</p><p>92 Figure 9 illustrates some of these functions estimated for a typical voxel with high predictive power (R2 of 0. [sent-211, score-0.279]
</p><p>93 These correspond to the nonlinearities appearing in the ﬁnal stage of the V-SPAM model (see Figure 6). [sent-213, score-0.169]
</p><p>94 Here the horizontal axis is the input in standard units of the corresponding model complex or pooled-complex cell outputs, and the vertical axis is the output in standard units of predicted responses. [sent-214, score-0.39]
</p><p>95 0  1 input  2  −1  0  1  2  3  input  Figure 9: Nonlinearities estimated in the V-SPAM model for a voxel with high predictive power (R2 : 0. [sent-239, score-0.308]
</p><p>96 The other 75 nonlinearities for this voxel (overlaid in the right panel) are smaller and contribute less to the predicted response. [sent-242, score-0.289]
</p><p>97 6  Discussion and conclusions  Our V-SPAM model provides better predictions of fMRI activity evoked by natural images than does a sparse linear model similar to that used in an earlier study of this data set [5]. [sent-243, score-0.333]
</p><p>98 This increased predictive power of the V-SPAM model reﬂects the fact that it can describe explicitly the nonlinear pooling that likely occurs among the many neurons whose pooled activity contributes to measured fMRI signals. [sent-244, score-0.375]
</p><p>99 These pooled output nonlinearities are likely a critical component of nonlinear computation across the visual hierarchy. [sent-245, score-0.214]
</p><p>100 Therefore, the SpAM framework may be particularly useful for modeling neurons or fMRI signals recorded in higher and more nonlinear stages of visual processing beyond V1. [sent-246, score-0.143]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('qq', 0.793), ('fmri', 0.23), ('voxel', 0.203), ('qqq', 0.201), ('cell', 0.187), ('cells', 0.132), ('pooling', 0.128), ('voxels', 0.121), ('evoked', 0.103), ('fj', 0.087), ('gabor', 0.086), ('complex', 0.084), ('stage', 0.08), ('spatial', 0.077), ('outputs', 0.067), ('qqqqq', 0.06), ('frequency', 0.06), ('nonlinearities', 0.06), ('ltering', 0.057), ('tuning', 0.057), ('additive', 0.056), ('orientation', 0.054), ('wavelet', 0.054), ('wavelets', 0.052), ('sparse', 0.05), ('rf', 0.049), ('nonlinear', 0.049), ('spam', 0.049), ('pooled', 0.049), ('images', 0.048), ('response', 0.047), ('activity', 0.042), ('qqqq', 0.04), ('qqqqqq', 0.04), ('recti', 0.039), ('quadrature', 0.037), ('predictive', 0.036), ('freq', 0.035), ('image', 0.034), ('nonparametric', 0.034), ('hierarchical', 0.034), ('responses', 0.033), ('visual', 0.032), ('tted', 0.031), ('brain', 0.031), ('pyramid', 0.03), ('receptive', 0.029), ('model', 0.029), ('predicted', 0.026), ('activation', 0.026), ('orient', 0.026), ('curves', 0.026), ('pj', 0.024), ('output', 0.024), ('neurons', 0.023), ('kendrick', 0.023), ('layers', 0.023), ('signals', 0.023), ('phase', 0.022), ('area', 0.022), ('location', 0.022), ('nonlinearity', 0.022), ('successive', 0.022), ('standardized', 0.022), ('elds', 0.021), ('estimated', 0.021), ('ravikumar', 0.021), ('predictors', 0.021), ('scanner', 0.02), ('kay', 0.02), ('naselaris', 0.02), ('gratings', 0.02), ('qqqqqqq', 0.02), ('scatterplot', 0.02), ('rj', 0.02), ('axis', 0.02), ('xj', 0.019), ('power', 0.019), ('jack', 0.018), ('pradeep', 0.018), ('tting', 0.018), ('eld', 0.018), ('grayscale', 0.017), ('overlaid', 0.016), ('natural', 0.016), ('berkeley', 0.016), ('earlier', 0.016), ('stages', 0.016), ('modeled', 0.016), ('magnetic', 0.016), ('resonance', 0.016), ('bank', 0.016), ('followed', 0.015), ('combination', 0.015), ('lter', 0.015), ('inset', 0.014), ('tibshirani', 0.014), ('regression', 0.014), ('cosine', 0.014), ('pixel', 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999988 <a title="156-tfidf-1" href="./nips-2008-Nonparametric_sparse_hierarchical_models_describe_V1_fMRI_responses_to_natural_images.html">156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</a></p>
<p>Author: Vincent Q. Vu, Bin Yu, Thomas Naselaris, Kendrick Kay, Jack Gallant, Pradeep K. Ravikumar</p><p>Abstract: We propose a novel hierarchical, nonlinear model that predicts brain activity in area V1 evoked by natural images. In the study reported here brain activity was measured by means of functional magnetic resonance imaging (fMRI), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (≈ 2mm cube) of brain tissue. Our model, which we call the V-SPAM model, is based on the reasonable assumption that fMRI measurements reﬂect the (possibly nonlinearly) pooled, rectiﬁed output of a large population of simple and complex cells in V1. It has a hierarchical ﬁltering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called “pooled-complex” cells). The pooling stage then obtains the measured fMRI signals as a sparse additive model (SpAM) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed. Our results show that the V-SPAM model predicts fMRI responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells. Furthermore, the spatial receptive ﬁelds, frequency tuning and orientation tuning curves of the V-SPAM model estimated for each voxel appears to be consistent with the known properties of V1, and with previous analyses of this data set. A visualization procedure applied to the V-SPAM model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities. 1</p><p>2 0.29622245 <a title="156-tfidf-2" href="./nips-2008-Predictive_Indexing_for_Fast_Search.html">184 nips-2008-Predictive Indexing for Fast Search</a></p>
<p>Author: Sharad Goel, John Langford, Alexander L. Strehl</p><p>Abstract: We tackle the computational problem of query-conditioned search. Given a machine-learned scoring rule and a query distribution, we build a predictive index by precomputing lists of potential results sorted based on an expected score of the result over future queries. The predictive index datastructure supports an anytime algorithm for approximate retrieval of the top elements. The general approach is applicable to webpage ranking, internet advertisement, and approximate nearest neighbor search. It is particularly effective in settings where standard techniques (e.g., inverted indices) are intractable. We experimentally ﬁnd substantial improvement over existing methods for internet advertisement and approximate nearest neighbors. 1</p><p>3 0.14020827 <a title="156-tfidf-3" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>Author: Adam Ponzi, Jeff Wickens</p><p>Abstract: Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2]. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with ﬁxed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and ﬁring state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between ﬁring and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition[3], shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points. 1</p><p>4 0.10773381 <a title="156-tfidf-4" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>Author: Han Liu, Larry Wasserman, John D. Lafferty</p><p>Abstract: We propose new families of models and algorithms for high-dimensional nonparametric learning with joint sparsity constraints. Our approach is based on a regularization method that enforces common sparsity patterns across different function components in a nonparametric additive model. The algorithms employ a coordinate descent approach that is based on a functional soft-thresholding operator. The framework yields several new models, including multi-task sparse additive models, multi-response sparse additive models, and sparse additive multi-category logistic regression. The methods are illustrated with experiments on synthetic data and gene microarray data. 1</p><p>5 0.10078742 <a title="156-tfidf-5" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<p>Author: Tanya Schmah, Geoffrey E. Hinton, Steven L. Small, Stephen Strother, Richard S. Zemel</p><p>Abstract: Neuroimaging datasets often have a very large number of voxels and a very small number of training cases, which means that overﬁtting of models for this data can become a very serious problem. Working with a set of fMRI images from a study on stroke recovery, we consider a classiﬁcation task for which logistic regression performs poorly, even when L1- or L2- regularized. We show that much better discrimination can be achieved by ﬁtting a generative model to each separate condition and then seeing which model is most likely to have generated the data. We compare discriminative training of exactly the same set of models, and we also consider convex blends of generative and discriminative training. 1</p><p>6 0.079796746 <a title="156-tfidf-6" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>7 0.070230074 <a title="156-tfidf-7" href="./nips-2008-Dependence_of_Orientation_Tuning_on_Recurrent_Excitation_and_Inhibition_in_a_Network_Model_of_V1.html">58 nips-2008-Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1</a></p>
<p>8 0.069322884 <a title="156-tfidf-8" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>9 0.068731919 <a title="156-tfidf-9" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>10 0.061483447 <a title="156-tfidf-10" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>11 0.052829731 <a title="156-tfidf-11" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>12 0.047989193 <a title="156-tfidf-12" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>13 0.044006892 <a title="156-tfidf-13" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>14 0.042008471 <a title="156-tfidf-14" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>15 0.040811751 <a title="156-tfidf-15" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>16 0.039962981 <a title="156-tfidf-16" href="./nips-2008-An_interior-point_stochastic_approximation_method_and_an_L1-regularized_delta_rule.html">25 nips-2008-An interior-point stochastic approximation method and an L1-regularized delta rule</a></p>
<p>17 0.039908078 <a title="156-tfidf-17" href="./nips-2008-Bayesian_Experimental_Design_of_Magnetic_Resonance_Imaging_Sequences.html">30 nips-2008-Bayesian Experimental Design of Magnetic Resonance Imaging Sequences</a></p>
<p>18 0.038389377 <a title="156-tfidf-18" href="./nips-2008-Improving_on_Expectation_Propagation.html">105 nips-2008-Improving on Expectation Propagation</a></p>
<p>19 0.0374653 <a title="156-tfidf-19" href="./nips-2008-Recursive_Segmentation_and_Recognition_Templates_for_2D_Parsing.html">191 nips-2008-Recursive Segmentation and Recognition Templates for 2D Parsing</a></p>
<p>20 0.035783388 <a title="156-tfidf-20" href="./nips-2008-Natural_Image_Denoising_with_Convolutional_Networks.html">148 nips-2008-Natural Image Denoising with Convolutional Networks</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.11), (1, -0.011), (2, 0.096), (3, 0.032), (4, -0.007), (5, 0.014), (6, -0.043), (7, -0.035), (8, 0.072), (9, 0.095), (10, -0.068), (11, 0.046), (12, -0.091), (13, 0.074), (14, 0.017), (15, -0.132), (16, 0.06), (17, -0.016), (18, 0.209), (19, -0.117), (20, 0.082), (21, 0.201), (22, 0.007), (23, -0.144), (24, 0.039), (25, -0.052), (26, 0.231), (27, 0.069), (28, 0.031), (29, 0.003), (30, 0.065), (31, -0.039), (32, -0.0), (33, 0.137), (34, -0.032), (35, 0.025), (36, -0.029), (37, 0.026), (38, -0.309), (39, 0.143), (40, -0.019), (41, -0.065), (42, 0.005), (43, 0.001), (44, -0.065), (45, 0.151), (46, 0.042), (47, 0.045), (48, 0.071), (49, 0.006)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9551025 <a title="156-lsi-1" href="./nips-2008-Nonparametric_sparse_hierarchical_models_describe_V1_fMRI_responses_to_natural_images.html">156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</a></p>
<p>Author: Vincent Q. Vu, Bin Yu, Thomas Naselaris, Kendrick Kay, Jack Gallant, Pradeep K. Ravikumar</p><p>Abstract: We propose a novel hierarchical, nonlinear model that predicts brain activity in area V1 evoked by natural images. In the study reported here brain activity was measured by means of functional magnetic resonance imaging (fMRI), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (≈ 2mm cube) of brain tissue. Our model, which we call the V-SPAM model, is based on the reasonable assumption that fMRI measurements reﬂect the (possibly nonlinearly) pooled, rectiﬁed output of a large population of simple and complex cells in V1. It has a hierarchical ﬁltering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called “pooled-complex” cells). The pooling stage then obtains the measured fMRI signals as a sparse additive model (SpAM) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed. Our results show that the V-SPAM model predicts fMRI responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells. Furthermore, the spatial receptive ﬁelds, frequency tuning and orientation tuning curves of the V-SPAM model estimated for each voxel appears to be consistent with the known properties of V1, and with previous analyses of this data set. A visualization procedure applied to the V-SPAM model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities. 1</p><p>2 0.60263979 <a title="156-lsi-2" href="./nips-2008-Predictive_Indexing_for_Fast_Search.html">184 nips-2008-Predictive Indexing for Fast Search</a></p>
<p>Author: Sharad Goel, John Langford, Alexander L. Strehl</p><p>Abstract: We tackle the computational problem of query-conditioned search. Given a machine-learned scoring rule and a query distribution, we build a predictive index by precomputing lists of potential results sorted based on an expected score of the result over future queries. The predictive index datastructure supports an anytime algorithm for approximate retrieval of the top elements. The general approach is applicable to webpage ranking, internet advertisement, and approximate nearest neighbor search. It is particularly effective in settings where standard techniques (e.g., inverted indices) are intractable. We experimentally ﬁnd substantial improvement over existing methods for internet advertisement and approximate nearest neighbors. 1</p><p>3 0.54717731 <a title="156-lsi-3" href="./nips-2008-Cell_Assemblies_in_Large_Sparse_Inhibitory_Networks_of_Biologically_Realistic_Spiking_Neurons.html">43 nips-2008-Cell Assemblies in Large Sparse Inhibitory Networks of Biologically Realistic Spiking Neurons</a></p>
<p>Author: Adam Ponzi, Jeff Wickens</p><p>Abstract: Cell assemblies exhibiting episodes of recurrent coherent activity have been observed in several brain regions including the striatum[1] and hippocampus CA3[2]. Here we address the question of how coherent dynamically switching assemblies appear in large networks of biologically realistic spiking neurons interacting deterministically. We show by numerical simulations of large asymmetric inhibitory networks with ﬁxed external excitatory drive that if the network has intermediate to sparse connectivity, the individual cells are in the vicinity of a bifurcation between a quiescent and ﬁring state and the network inhibition varies slowly on the spiking timescale, then cells form assemblies whose members show strong positive correlation, while members of different assemblies show strong negative correlation. We show that cells and assemblies switch between ﬁring and quiescent states with time durations consistent with a power-law. Our results are in good qualitative agreement with the experimental studies. The deterministic dynamical behaviour is related to winner-less competition[3], shown in small closed loop inhibitory networks with heteroclinic cycles connecting saddle-points. 1</p><p>4 0.45850292 <a title="156-lsi-4" href="./nips-2008-Dependence_of_Orientation_Tuning_on_Recurrent_Excitation_and_Inhibition_in_a_Network_Model_of_V1.html">58 nips-2008-Dependence of Orientation Tuning on Recurrent Excitation and Inhibition in a Network Model of V1</a></p>
<p>Author: Klaus Wimmer, Marcel Stimberg, Robert Martin, Lars Schwabe, Jorge Mariño, James Schummers, David C. Lyon, Mriganka Sur, Klaus Obermayer</p><p>Abstract: The computational role of the local recurrent network in primary visual cortex is still a matter of debate. To address this issue, we analyze intracellular recording data of cat V1, which combine measuring the tuning of a range of neuronal properties with a precise localization of the recording sites in the orientation preference map. For the analysis, we consider a network model of Hodgkin-Huxley type neurons arranged according to a biologically plausible two-dimensional topographic orientation preference map. We then systematically vary the strength of the recurrent excitation and inhibition relative to the strength of the afferent input. Each parametrization gives rise to a different model instance for which the tuning of model neurons at different locations of the orientation map is compared to the experimentally measured orientation tuning of membrane potential, spike output, excitatory, and inhibitory conductances. A quantitative analysis shows that the data provides strong evidence for a network model in which the afferent input is dominated by strong, balanced contributions of recurrent excitation and inhibition. This recurrent regime is close to a regime of “instability”, where strong, self-sustained activity of the network occurs. The ﬁring rate of neurons in the best-ﬁtting network is particularly sensitive to small modulations of model parameters, which could be one of the functional beneﬁts of a network operating in this particular regime. 1</p><p>5 0.3868269 <a title="156-lsi-5" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>Author: Han Liu, Larry Wasserman, John D. Lafferty</p><p>Abstract: We propose new families of models and algorithms for high-dimensional nonparametric learning with joint sparsity constraints. Our approach is based on a regularization method that enforces common sparsity patterns across different function components in a nonparametric additive model. The algorithms employ a coordinate descent approach that is based on a functional soft-thresholding operator. The framework yields several new models, including multi-task sparse additive models, multi-response sparse additive models, and sparse additive multi-category logistic regression. The methods are illustrated with experiments on synthetic data and gene microarray data. 1</p><p>6 0.3492063 <a title="156-lsi-6" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>7 0.33591789 <a title="156-lsi-7" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>8 0.32221147 <a title="156-lsi-8" href="./nips-2008-Estimating_Robust_Query_Models_with_Convex_Optimization.html">73 nips-2008-Estimating Robust Query Models with Convex Optimization</a></p>
<p>9 0.30879456 <a title="156-lsi-9" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>10 0.26770815 <a title="156-lsi-10" href="./nips-2008-Fast_High-dimensional_Kernel_Summations_Using_the_Monte_Carlo_Multipole_Method.html">83 nips-2008-Fast High-dimensional Kernel Summations Using the Monte Carlo Multipole Method</a></p>
<p>11 0.25466812 <a title="156-lsi-11" href="./nips-2008-Generative_versus_discriminative_training_of_RBMs_for_classification_of_fMRI_images.html">92 nips-2008-Generative versus discriminative training of RBMs for classification of fMRI images</a></p>
<p>12 0.25230396 <a title="156-lsi-12" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>13 0.23640516 <a title="156-lsi-13" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>14 0.21478352 <a title="156-lsi-14" href="./nips-2008-Dynamic_visual_attention%3A_searching_for_coding_length_increments.html">66 nips-2008-Dynamic visual attention: searching for coding length increments</a></p>
<p>15 0.21385455 <a title="156-lsi-15" href="./nips-2008-Bio-inspired_Real_Time_Sensory_Map_Realignment_in_a_Robotic_Barn_Owl.html">38 nips-2008-Bio-inspired Real Time Sensory Map Realignment in a Robotic Barn Owl</a></p>
<p>16 0.2128796 <a title="156-lsi-16" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>17 0.20155685 <a title="156-lsi-17" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<p>18 0.19984937 <a title="156-lsi-18" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>19 0.19130215 <a title="156-lsi-19" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>20 0.19027098 <a title="156-lsi-20" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.127), (7, 0.066), (12, 0.022), (15, 0.013), (25, 0.014), (28, 0.123), (46, 0.021), (57, 0.072), (59, 0.02), (63, 0.022), (71, 0.024), (77, 0.033), (83, 0.058), (94, 0.012), (95, 0.261)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.7702409 <a title="156-lda-1" href="./nips-2008-Nonparametric_sparse_hierarchical_models_describe_V1_fMRI_responses_to_natural_images.html">156 nips-2008-Nonparametric sparse hierarchical models describe V1 fMRI responses to natural images</a></p>
<p>Author: Vincent Q. Vu, Bin Yu, Thomas Naselaris, Kendrick Kay, Jack Gallant, Pradeep K. Ravikumar</p><p>Abstract: We propose a novel hierarchical, nonlinear model that predicts brain activity in area V1 evoked by natural images. In the study reported here brain activity was measured by means of functional magnetic resonance imaging (fMRI), a noninvasive technique that provides an indirect measure of neural activity pooled over a small volume (≈ 2mm cube) of brain tissue. Our model, which we call the V-SPAM model, is based on the reasonable assumption that fMRI measurements reﬂect the (possibly nonlinearly) pooled, rectiﬁed output of a large population of simple and complex cells in V1. It has a hierarchical ﬁltering stage that consists of three layers: model simple cells, model complex cells, and a third layer in which the complex cells are linearly pooled (called “pooled-complex” cells). The pooling stage then obtains the measured fMRI signals as a sparse additive model (SpAM) in which a sparse nonparametric (nonlinear) combination of model complex cell and model pooled-complex cell outputs are summed. Our results show that the V-SPAM model predicts fMRI responses evoked by natural images better than a benchmark model that only provides linear pooling of model complex cells. Furthermore, the spatial receptive ﬁelds, frequency tuning and orientation tuning curves of the V-SPAM model estimated for each voxel appears to be consistent with the known properties of V1, and with previous analyses of this data set. A visualization procedure applied to the V-SPAM model shows that most of the nonlinear pooling consists of simple compressive or saturating nonlinearities. 1</p><p>2 0.60822606 <a title="156-lda-2" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>Author: J. A. Bagnell, David M. Bradley</p><p>Abstract: Prior work has shown that features which appear to be biologically plausible as well as empirically useful can be found by sparse coding with a prior such as a laplacian (L1 ) that promotes sparsity. We show how smoother priors can preserve the beneﬁts of these sparse priors while adding stability to the Maximum A-Posteriori (MAP) estimate that makes it more useful for prediction problems. Additionally, we show how to calculate the derivative of the MAP estimate efﬁciently with implicit differentiation. One prior that can be differentiated this way is KL-regularization. We demonstrate its effectiveness on a wide variety of applications, and ﬁnd that online optimization of the parameters of the KL-regularized model can signiﬁcantly improve prediction performance. 1</p><p>3 0.60397422 <a title="156-lda-3" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>Author: Stefan Haufe, Vadim V. Nikulin, Andreas Ziehe, Klaus-Robert Müller, Guido Nolte</p><p>Abstract: We introduce a novel framework for estimating vector ﬁelds using sparse basis ﬁeld expansions (S-FLEX). The notion of basis ﬁelds, which are an extension of scalar basis functions, arises naturally in our framework from a rotational invariance requirement. We consider a regression setting as well as inverse problems. All variants discussed lead to second-order cone programming formulations. While our framework is generally applicable to any type of vector ﬁeld, we focus in this paper on applying it to solving the EEG/MEG inverse problem. It is shown that signiﬁcantly more precise and neurophysiologically more plausible location and shape estimates of cerebral current sources from EEG/MEG measurements become possible with our method when comparing to the state-of-the-art. 1</p><p>4 0.60174996 <a title="156-lda-4" href="./nips-2008-Robust_Regression_and_Lasso.html">202 nips-2008-Robust Regression and Lasso</a></p>
<p>Author: Huan Xu, Constantine Caramanis, Shie Mannor</p><p>Abstract: We consider robust least-squares regression with feature-wise disturbance. We show that this formulation leads to tractable convex optimization problems, and we exhibit a particular uncertainty set for which the robust problem is equivalent to 1 regularized regression (Lasso). This provides an interpretation of Lasso from a robust optimization perspective. We generalize this robust formulation to consider more general uncertainty sets, which all lead to tractable convex optimization problems. Therefore, we provide a new methodology for designing regression algorithms, which generalize known formulations. The advantage is that robustness to disturbance is a physical property that can be exploited: in addition to obtaining new formulations, we use it directly to show sparsity properties of Lasso, as well as to prove a general consistency result for robust regression problems, including Lasso, from a uniﬁed robustness perspective. 1</p><p>5 0.59624463 <a title="156-lda-5" href="./nips-2008-Performance_analysis_for_L%5C_2_kernel_classification.html">178 nips-2008-Performance analysis for L\ 2 kernel classification</a></p>
<p>Author: Jooseuk Kim, Clayton Scott</p><p>Abstract: We provide statistical performance guarantees for a recently introduced kernel classiﬁer that optimizes the L2 or integrated squared error (ISE) of a difference of densities. The classiﬁer is similar to a support vector machine (SVM) in that it is the solution of a quadratic program and yields a sparse classiﬁer. Unlike SVMs, however, the L2 kernel classiﬁer does not involve a regularization parameter. We prove a distribution free concentration inequality for a cross-validation based estimate of the ISE, and apply this result to deduce an oracle inequality and consistency of the classiﬁer on the sense of both ISE and probability of error. Our results also specialize to give performance guarantees for an existing method of L2 kernel density estimation. 1</p><p>6 0.59321082 <a title="156-lda-6" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>7 0.59210116 <a title="156-lda-7" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>8 0.59075737 <a title="156-lda-8" href="./nips-2008-Risk_Bounds_for_Randomized_Sample_Compressed_Classifiers.html">199 nips-2008-Risk Bounds for Randomized Sample Compressed Classifiers</a></p>
<p>9 0.58905244 <a title="156-lda-9" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>10 0.58891672 <a title="156-lda-10" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>11 0.58766741 <a title="156-lda-11" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>12 0.58716965 <a title="156-lda-12" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>13 0.58658189 <a title="156-lda-13" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<p>14 0.58612239 <a title="156-lda-14" href="./nips-2008-Mind_the_Duality_Gap%3A_Logarithmic_regret_algorithms_for_online_optimization.html">133 nips-2008-Mind the Duality Gap: Logarithmic regret algorithms for online optimization</a></p>
<p>15 0.58558422 <a title="156-lda-15" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>16 0.58498418 <a title="156-lda-16" href="./nips-2008-Artificial_Olfactory_Brain_for_Mixture_Identification.html">27 nips-2008-Artificial Olfactory Brain for Mixture Identification</a></p>
<p>17 0.5848332 <a title="156-lda-17" href="./nips-2008-Sparse_Online_Learning_via_Truncated_Gradient.html">214 nips-2008-Sparse Online Learning via Truncated Gradient</a></p>
<p>18 0.58428997 <a title="156-lda-18" href="./nips-2008-On_the_Generalization_Ability_of_Online_Strongly_Convex_Programming_Algorithms.html">164 nips-2008-On the Generalization Ability of Online Strongly Convex Programming Algorithms</a></p>
<p>19 0.58428794 <a title="156-lda-19" href="./nips-2008-Estimating_the_Location_and_Orientation_of_Complex%2C_Correlated_Neural_Activity_using_MEG.html">74 nips-2008-Estimating the Location and Orientation of Complex, Correlated Neural Activity using MEG</a></p>
<p>20 0.58348602 <a title="156-lda-20" href="./nips-2008-An_improved_estimator_of_Variance_Explained_in_the_presence_of_noise.html">24 nips-2008-An improved estimator of Variance Explained in the presence of noise</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
