<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>126 nips-2008-Localized Sliced Inverse Regression</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-126" href="#">nips2008-126</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>126 nips-2008-Localized Sliced Inverse Regression</h1>
<br/><p>Source: <a title="nips-2008-126-pdf" href="http://papers.nips.cc/paper/3595-localized-sliced-inverse-regression.pdf">pdf</a></p><p>Author: Qiang Wu, Sayan Mukherjee, Feng Liang</p><p>Abstract: We developed localized sliced inverse regression for supervised dimension reduction. It has the advantages of preventing degeneracy, increasing estimation accuracy, and automatic subclass discovery in classiﬁcation problems. A semisupervised version is proposed for the use of unlabeled data. The utility is illustrated on simulated as well as real data sets.</p><p>Reference: <a title="nips-2008-126-reference" href="../nips2008_reference/nips-2008-Localized_Sliced_Inverse_Regression_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 edu  Abstract We developed localized sliced inverse regression for supervised dimension reduction. [sent-9, score-0.383]
</p><p>2 It has the advantages of preventing degeneracy, increasing estimation accuracy, and automatic subclass discovery in classiﬁcation problems. [sent-10, score-0.073]
</p><p>3 A semisupervised version is proposed for the use of unlabeled data. [sent-11, score-0.114]
</p><p>4 Characterizing this predictive subspace, supervised dimension reduction, requires both the response and explanatory variables. [sent-14, score-0.243]
</p><p>5 In machine learning community research on nonlinear dimension reduction in the spirit of [19] has been developed of late. [sent-17, score-0.241]
</p><p>6 This has led to a variety of manifold learning algorithms such as isometric mapping (ISOMAP, [16]), local linear embedding (LLE, [14]), Hessian Eigenmaps [5], and Laplacian Eigenmaps [1]. [sent-18, score-0.098]
</p><p>7 Two key differences exist between the paradigm explored in this approach and that of supervised dimension reduction. [sent-19, score-0.122]
</p><p>8 The ﬁrst difference is that the above methods are unsupervised in that the algorithms take into account only the explanatory variables. [sent-20, score-0.079]
</p><p>9 The bigger problem is that these manifold learning algorithms do not operate on the space of the explanatory variables and hence do not provide a predictive submanifold onto which the data should be projected. [sent-22, score-0.192]
</p><p>10 These methods are based on embedding the observed data onto a graph and then using spectral properties of the embedded graph for dimension reduction. [sent-23, score-0.168]
</p><p>11 The key observation in all of these manifold algorithms is that metrics must be local and properties that hold in an ambient Euclidean space are true locally on smooth manifolds. [sent-24, score-0.1]
</p><p>12 This suggests that the use of local information in supervised dimension reduction methods may be of use to extend methods for dimension reduction to the setting of nonlinear subspaces and submanifolds of the ambient space. [sent-25, score-0.532]
</p><p>13 1  In this paper we extend SIR by taking into account the local structure of the explanatory variables. [sent-27, score-0.12]
</p><p>14 This localized variant of SIR, LSIR, can be used for classiﬁcation as well as regression applications. [sent-28, score-0.085]
</p><p>15 Though the predictive directions obtained by LSIR are linear ones, they coded nonlinear information. [sent-29, score-0.153]
</p><p>16 Another advantage of our approach is that ancillary unlabeled data can be easily added to the dimension reduction analysis – semi-supervised learning. [sent-30, score-0.282]
</p><p>17 The utility with respect to predictive accuracy as well as exploratory data analysis via visualization is demonstrated on a variety of simulated and real data in Sections 4 and 5. [sent-34, score-0.141]
</p><p>18 Then we propose a generalization of SIR, called localized SIR, by incorporating some localization idea from manifold learning. [sent-37, score-0.13]
</p><p>19 1 Sliced inverse regression Assume the functional dependence between a response variable Y and an explanatory variable X ∈ Rp is given by t t (1) Y = f (β1 X, . [sent-40, score-0.173]
</p><p>20 Estimating B or βl ’s becomes the central problem in supervised dimension reduction. [sent-46, score-0.122]
</p><p>21 Following [4], we refer to B as the dimension reduction (d. [sent-48, score-0.194]
</p><p>22 Slice inverse regression (SIR) model is introduced [10] to estimate the d. [sent-53, score-0.087]
</p><p>23 The idea underlying this approach is that, if X has an identity covariance matrix, the centered inverse regression curve E(X|Y ) − EX is contained in the d. [sent-56, score-0.116]
</p><p>24 directions βl ’s are given by the top eigenvectors of the covariance matrix Γ = Cov(E(X|Y )). [sent-61, score-0.145]
</p><p>25 In general when the covariance matrix of X is Σ, the βl ’s can be obtained by solving a generalized eigen decomposition problem Γβ = λΣβ. [sent-62, score-0.078]
</p><p>26 Compute an empirical estimate of Γ, H  ˆ Γ= i=1  where mh =  1 nh  j∈Gh  nh (mh − m)(mh − m)T n  xj is the sample mean for group Gh with nh being the group size. [sent-70, score-0.285]
</p><p>27 directions β by solving a generalized eigen problem ˆ ˆ Γβ = λΣβ. [sent-74, score-0.135]
</p><p>28 2  (2)  When Y takes categorical values as in classiﬁcation problems, it is natural to divide the data into different groups by their group labels. [sent-75, score-0.143]
</p><p>29 1 Though SIR has been widely used for dimension reduction and yielded many useful results in practice, it has some known problems. [sent-77, score-0.194]
</p><p>30 For example, it is easy to construct a function f such that E(X|Y = y) = 0 then SIR fails to retrieve any useful directions [3]. [sent-78, score-0.148]
</p><p>31 Generalizations of SIR include SAVE [3], SIR-II [12] and covariance inverse regression estimation (CIRE, [2]) that exploit the information from the second moment of the conditional distribution of X|Y . [sent-81, score-0.136]
</p><p>32 However in some scenario the information in each slice can not be well described by a global statistics. [sent-82, score-0.067]
</p><p>33 For example, similar to the multimodal situation considered by [15], the data in a slice may form two clusters, then a good description of the data would not be a single number such as any moments, but the two cluster centers. [sent-83, score-0.129]
</p><p>34 2 Localization A key principle in manifold learning is that the Euclidean representation of a data point in Rp is only meaningful locally. [sent-86, score-0.06]
</p><p>35 Under this principle, it is dangerous to calculate the slice average mh , whenever the slice contains data that are far away. [sent-87, score-0.228]
</p><p>36 Motivated by this idea we introduce a localized SIR (LSIR) method for dimension reduction. [sent-89, score-0.149]
</p><p>37 Let us start with the transformed data set where the empirical covariance is identity, for example, the data set after PCA. [sent-91, score-0.086]
</p><p>38 In the original SIR method, we shift every data point xi to the corresponding group average, then apply PCA on the new data set to identify SIR directions. [sent-92, score-0.119]
</p><p>39 The underline rational for this approach is that if a direction does not differentiate different groups well, the group means projected to that direction would be very close, therefore the variance of the new data set will be small at that direction. [sent-93, score-0.118]
</p><p>40 A natural way to incorporate localization idea into this approach is to shift each data point xi to the average of a local neighborhood instead of the average of its global neighborhood (i. [sent-94, score-0.258]
</p><p>41 In manifolds learning, local neighborhood is often chosen by k nearest neighborhood (k-NN). [sent-97, score-0.2]
</p><p>42 Different from manifolds learning that is designed for unsupervised learning, the neighborhood selection for LSIR that is designed for supervised learning will also incorporate information from the response variable y. [sent-98, score-0.134]
</p><p>43 Recall that the group average mh is used in estimating ˆ Γ = Cov(E(X|Y )). [sent-100, score-0.128]
</p><p>44 The estimate Γ is equivalent to the sample covariance of a data set {mi }n i=1 where mi = mh , average of the group Gh to which xi belongs. [sent-101, score-0.235]
</p><p>45 In our LSIR algorithm, we set mi ˆ equal to some local average, and then use the corresponding sample covariance matrix to replace Γ in equation (2). [sent-102, score-0.105]
</p><p>46 For each sample (xi , yi ) we compute mi,loc =  1 k  xj , j∈si  where, with h being the group so that i ∈ Gh , si = {j : xj belongs to the k nearest neighbors of xi in Gh } . [sent-107, score-0.141]
</p><p>47 With a moderate choice of k, LSIR uses the local information within each slice and is expected to retrieve directions lost by SIR in case of SIR fails due to degeneracy. [sent-114, score-0.256]
</p><p>48 As showed in one of our examples, this property of LSIR is very useful in data analysis such as cancer subtype discovery using genomic data. [sent-121, score-0.118]
</p><p>49 3 Connection to Existing Work The idea of localization has been introduced to dimension reduction for classiﬁcation problems before. [sent-123, score-0.247]
</p><p>50 For example, the local discriminant information (LDI) introduced by [9] is one of the early work in this area. [sent-124, score-0.096]
</p><p>51 In LDI, the local information is used to compute the between-group covariance matrix Γi over a nearest neighborhood at every data point xi and then estimate the d. [sent-125, score-0.201]
</p><p>52 directions by n 1 the top eigenvector of the averaged between-group matrix n i=1 Γi . [sent-127, score-0.101]
</p><p>53 The local Fisher discriminant analysis (LFDA) introduced by [15] can be regarded as an improvement of LDI with the within-class covariance matrix also being localized. [sent-128, score-0.14]
</p><p>54 For example, for a problem of C classes, LDI needs to compute nC local mean points and n between-group covariance matrices, while LSIR computes only n local mean points and one covariance matrix. [sent-131, score-0.206]
</p><p>55 Another advantage is LSIR can be easily extended to handle unlabeled data in semi-supervised learning as explained in the next section. [sent-132, score-0.088]
</p><p>56 Such an extension is less straightforward for the other two approaches that operate on the covariance matrices instead of data points. [sent-133, score-0.065]
</p><p>57 How to incorporate the information from unlabeled data has been the main focus of research in semi-supervised learning. [sent-142, score-0.088]
</p><p>58 Our LSIR algorithm can be easily modiﬁed to take the unlabeled data into consideration. [sent-143, score-0.088]
</p><p>59 Since y of an unlabeled sample can take any possible values, we put the unlabeled data into every slice. [sent-144, score-0.155]
</p><p>60 So the neighborhood si is deﬁned as the following: for any point in the k-NN of xi , it belongs to si if it is unlabeled, or if it is labeled and belongs to the same slice as xi . [sent-145, score-0.274]
</p><p>61 The performance of LSIR is compared with other dimension reduction methods including SIR, SAVE, pHd, and LFDA. [sent-147, score-0.194]
</p><p>62 0008)  Table 1: Estimation accuracy (and standard deviation) of various dimension reduction methods for semisupervised learning in Example 1. [sent-156, score-0.277]
</p><p>63 The data points in red and blue are labeled and the ones in green are unlabeled when the semisupervised setting is considered. [sent-163, score-0.184]
</p><p>64 (c) Projection of data to the ﬁrst two LSIR directions when all the n = 400 data points are labeled. [sent-165, score-0.161]
</p><p>65 (d) Projection of the data to the ﬁrst two LSIR directions when only 20 points as indicated in (a) are labeled. [sent-166, score-0.14]
</p><p>66 directions are the ﬁrst two dimensions and the remaining eight dimensions are Gaussian noise. [sent-181, score-0.189]
</p><p>67 The data in the ﬁrst two relevant dimensions are plotted in Figure 1(a) with sample size n = 400. [sent-182, score-0.065]
</p><p>68 directions because the group averages of the two groups are roughly the same for the ﬁrst two dimensions, due to the symmetry in the data. [sent-185, score-0.18]
</p><p>69 Using local average instead of group average, LSIR can ﬁnd both directions, see Figure 1(c). [sent-186, score-0.114]
</p><p>70 The directions from PCA where one ignores the labels do not agree with the discriminant directions as shown in Figure 1(b). [sent-189, score-0.242]
</p><p>71 So to retrieve the relevant directions, the information from the labeled points has to be taken consideration. [sent-190, score-0.096]
</p><p>72 We evaluate the accuracy of LSIR (the semi-supervised version), SAVE and pHd where the latter two are operated on just the labeled set. [sent-191, score-0.067]
</p><p>73 The result for one iteration is displayed in Figure 1 where the labeled points are indicated in (a) and the projection to the top two directions from LSIR (with k = 40) is in (d). [sent-194, score-0.171]
</p><p>74 All the results clearly indicate that LSIR out-performs the other two supervised dimension reduction methods. [sent-195, score-0.22]
</p><p>75 We ﬁrst generate a 10-dimensional data set where the ﬁrst three dimensions are the Swiss roll data [14]: X1 = t cos t, X2 = 21h, X3 = t sin t, 3π where t = 2 (1 + 2θ), θ ∼ Uniform(0, 1) and h ∼ Uniform([0, 1]). [sent-197, score-0.131]
</p><p>76 4 200  300  400  500  600 700 sample size  800  900  1000  Figure 2: Estimation accuracy of various dimension methods for example 2. [sent-208, score-0.132]
</p><p>77 We randomly choose n samples as a training set and let n change from 200 to 1000 and compare the estimation accuracy for LSIR with SIR, SAVE and pHd. [sent-209, score-0.074]
</p><p>78 Note that Swiss roll (the ﬁrst three dimensions) is a benchmark data set in manifolds learning, where the goal is to “unroll” the data into the intrinsic two dimensional space. [sent-212, score-0.108]
</p><p>79 Since LSIR is a linear dimension reduction method we do not expect LSIR to unroll the data, but expect to retrieve the dimensions relevant to the prediction of Y . [sent-213, score-0.33]
</p><p>80 Meanwhile, with the noise, manifolds learning algorithms will not unroll the data either since the dominant directions are now the noise dimensions. [sent-214, score-0.203]
</p><p>81 The Tai Chi data set was ﬁrst used as a dimension reduction example in [12, Chapter 14]. [sent-222, score-0.215]
</p><p>82 By taking the local structure into account, LSIR can easily retrieve the relevant directions. [sent-227, score-0.088]
</p><p>83 (a) The training data in ﬁrst two dimensions; (b) The training data projected onto the ﬁrst two LSIR directions; (c) An independent test data projected onto the ﬁrst two LSIR directions. [sent-234, score-0.165]
</p><p>84 It contains 60, 000 images of handwritten digits as training data and 10, 000 images as test data. [sent-241, score-0.059]
</p><p>85 6  4  5  x 10  0  −5  −10 −5  0  5  10 4  x 10  Figure 4: Result for leukemia data by LSIR. [sent-243, score-0.061]
</p><p>86 Then we project the training data and 10000 test data onto these directions. [sent-249, score-0.075]
</p><p>87 2 Gene expression data Cancer classiﬁcation and discovery using gene expression data becomes an important technique in modern biology and medical science. [sent-277, score-0.102]
</p><p>88 In gene expression data number of genes is huge (usually up to thousands) and the samples is quite limited. [sent-278, score-0.07]
</p><p>89 As a typical large p small n problem, dimension reduction plays very essential role to understand the data structure and make inference. [sent-279, score-0.215]
</p><p>90 An interesting point is that LSIR automatically realizes subtype discovery while SIR cannot. [sent-286, score-0.069]
</p><p>91 By project the training data onto the ﬁrst two directions (Figure 4), we immediately notice that the ALL has two subtypes. [sent-287, score-0.155]
</p><p>92 Note that there are two samples (which are T-cell ALL) cannot be assigned to each subtype only by visualization. [sent-289, score-0.058]
</p><p>93 6 Discussion We developed LSIR method for dimension reduction by incorporating local information into the original SIR. [sent-291, score-0.25]
</p><p>94 A semisupervised version is developed for the use of unlabeled data. [sent-294, score-0.129]
</p><p>95 An extension of LSIR along this direction 7  can be helpful to realize nonlinear dimension reduction directions and to reduce the computational complexity in case of p ≫ n. [sent-297, score-0.327]
</p><p>96 Further research on LSIR and its kernelized version includes their asymptotic properties such as consistency and statistically more rigorous approaches for the choice of k, the size of local neighborhoods, and L, the dimensionality of the reduced space. [sent-298, score-0.058]
</p><p>97 Dimension reduction and visualization in discriminant analysis (with discussion). [sent-321, score-0.162]
</p><p>98 Molecular classiﬁcation of cancer: class discovery and class prediction by gene expression monitoring. [sent-363, score-0.06]
</p><p>99 On principal hessian directions for data visulization and dimension reduction: another application of stein’s lemma. [sent-381, score-0.274]
</p><p>100 Dimension reduction of multimodal labeled data by local ﬁsher discriminatn analysis. [sent-406, score-0.211]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('lsir', 0.757), ('sir', 0.464), ('sliced', 0.121), ('save', 0.113), ('directions', 0.101), ('reduction', 0.098), ('dimension', 0.096), ('explanatory', 0.079), ('chi', 0.076), ('loc', 0.076), ('tai', 0.076), ('gh', 0.073), ('unlabeled', 0.067), ('slice', 0.067), ('fda', 0.066), ('phd', 0.056), ('mh', 0.055), ('group', 0.055), ('localized', 0.053), ('ldi', 0.053), ('neighborhood', 0.05), ('eigenmaps', 0.049), ('retrieve', 0.047), ('semisupervised', 0.047), ('unroll', 0.045), ('covariance', 0.044), ('dimensions', 0.044), ('local', 0.041), ('pb', 0.041), ('classi', 0.04), ('discriminant', 0.04), ('subtype', 0.04), ('leukemia', 0.04), ('degeneracy', 0.04), ('nh', 0.04), ('inverse', 0.04), ('manifold', 0.039), ('subspace', 0.038), ('localization', 0.038), ('digits', 0.038), ('manifolds', 0.036), ('cook', 0.036), ('accuracy', 0.036), ('hessian', 0.034), ('eigen', 0.034), ('onto', 0.033), ('nonlinear', 0.032), ('regression', 0.032), ('gene', 0.031), ('labeled', 0.031), ('aml', 0.03), ('lfda', 0.03), ('roll', 0.03), ('sayan', 0.03), ('semilsir', 0.03), ('rp', 0.029), ('discovery', 0.029), ('cancer', 0.028), ('qiang', 0.026), ('supervised', 0.026), ('subspaces', 0.025), ('subclass', 0.024), ('cation', 0.024), ('visualization', 0.024), ('groups', 0.024), ('divide', 0.023), ('duke', 0.023), ('swiss', 0.023), ('nearest', 0.023), ('principal', 0.022), ('si', 0.022), ('xi', 0.022), ('response', 0.022), ('neighborhoods', 0.021), ('projection', 0.021), ('data', 0.021), ('multimodal', 0.02), ('ambient', 0.02), ('mi', 0.02), ('estimation', 0.02), ('categorical', 0.02), ('predictive', 0.02), ('utility', 0.019), ('nc', 0.019), ('belongs', 0.019), ('embedding', 0.018), ('projected', 0.018), ('pca', 0.018), ('cov', 0.018), ('samples', 0.018), ('average', 0.018), ('points', 0.018), ('moments', 0.018), ('liang', 0.017), ('dimensionality', 0.017), ('euclidean', 0.016), ('introduced', 0.015), ('sin', 0.015), ('developed', 0.015), ('regularization', 0.015)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0 <a title="126-tfidf-1" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>Author: Qiang Wu, Sayan Mukherjee, Feng Liang</p><p>Abstract: We developed localized sliced inverse regression for supervised dimension reduction. It has the advantages of preventing degeneracy, increasing estimation accuracy, and automatic subclass discovery in classiﬁcation problems. A semisupervised version is proposed for the use of unlabeled data. The utility is illustrated on simulated as well as real data sets.</p><p>2 0.068276778 <a title="126-tfidf-2" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>Author: Andrew Smith, Hongyuan Zha, Xiao-ming Wu</p><p>Abstract: We study the convergence and the rate of convergence of a local manifold learning algorithm: LTSA [13]. The main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of LTSA. We derive a worst-case upper bound of errors for LTSA which naturally leads to a convergence result. We then derive the rate of convergence for LTSA in a special case. 1</p><p>3 0.057297949 <a title="126-tfidf-3" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>4 0.055617575 <a title="126-tfidf-4" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>Author: Yuhong Guo</p><p>Abstract: Recently, supervised dimensionality reduction has been gaining attention, owing to the realization that data labels are often available and indicate important underlying structure in the data. In this paper, we present a novel convex supervised dimensionality reduction approach based on exponential family PCA, which is able to avoid the local optima of typical EM learning. Moreover, by introducing a sample-based approximation to exponential family models, it overcomes the limitation of the prevailing Gaussian assumptions of standard PCA, and produces a kernelized formulation for nonlinear supervised dimensionality reduction. A training algorithm is then devised based on a subgradient bundle method, whose scalability can be gained using a coordinate descent procedure. The advantage of our global optimization approach is demonstrated by empirical results over both synthetic and real data. 1</p><p>5 0.052935258 <a title="126-tfidf-5" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>Author: Aarti Singh, Robert Nowak, Xiaojin Zhu</p><p>Abstract: Empirical evidence shows that in favorable situations semi-supervised learning (SSL) algorithms can capitalize on the abundance of unlabeled training data to improve the performance of a learning task, in the sense that fewer labeled training data are needed to achieve a target error bound. However, in other situations unlabeled data do not seem to help. Recent attempts at theoretically characterizing SSL gains only provide a partial and sometimes apparently conﬂicting explanations of whether, and to what extent, unlabeled data can help. In this paper, we attempt to bridge the gap between the practice and theory of semi-supervised learning. We develop a ﬁnite sample analysis that characterizes the value of unlabeled data and quantiﬁes the performance improvement of SSL compared to supervised learning. We show that there are large classes of problems for which SSL can signiﬁcantly outperform supervised learning, in ﬁnite sample regimes and sometimes also in terms of error convergence rates.</p><p>6 0.048519179 <a title="126-tfidf-6" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>7 0.048202883 <a title="126-tfidf-7" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>8 0.047381606 <a title="126-tfidf-8" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>9 0.046809237 <a title="126-tfidf-9" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>10 0.046279769 <a title="126-tfidf-10" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>11 0.044855885 <a title="126-tfidf-11" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>12 0.044534776 <a title="126-tfidf-12" href="./nips-2008-Multi-stage_Convex_Relaxation_for_Learning_with_Sparse_Regularization.html">145 nips-2008-Multi-stage Convex Relaxation for Learning with Sparse Regularization</a></p>
<p>13 0.043401707 <a title="126-tfidf-13" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>14 0.038888454 <a title="126-tfidf-14" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>15 0.037419111 <a title="126-tfidf-15" href="./nips-2008-Multi-Level_Active_Prediction_of_Useful_Image_Annotations_for_Recognition.html">142 nips-2008-Multi-Level Active Prediction of Useful Image Annotations for Recognition</a></p>
<p>16 0.035430692 <a title="126-tfidf-16" href="./nips-2008-Extended_Grassmann_Kernels_for_Subspace-Based_Learning.html">80 nips-2008-Extended Grassmann Kernels for Subspace-Based Learning</a></p>
<p>17 0.034621604 <a title="126-tfidf-17" href="./nips-2008-DiscLDA%3A_Discriminative_Learning_for_Dimensionality_Reduction_and_Classification.html">64 nips-2008-DiscLDA: Discriminative Learning for Dimensionality Reduction and Classification</a></p>
<p>18 0.03287274 <a title="126-tfidf-18" href="./nips-2008-Deep_Learning_with_Kernel_Regularization_for_Visual_Recognition.html">56 nips-2008-Deep Learning with Kernel Regularization for Visual Recognition</a></p>
<p>19 0.03280523 <a title="126-tfidf-19" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>20 0.032435473 <a title="126-tfidf-20" href="./nips-2008-Learning_Hybrid_Models_for_Image_Annotation_with_Partially_Labeled_Data.html">116 nips-2008-Learning Hybrid Models for Image Annotation with Partially Labeled Data</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.112), (1, -0.038), (2, -0.004), (3, 0.02), (4, 0.03), (5, 0.001), (6, 0.008), (7, 0.018), (8, -0.029), (9, 0.003), (10, 0.093), (11, -0.036), (12, -0.074), (13, -0.041), (14, -0.027), (15, -0.028), (16, 0.026), (17, 0.082), (18, 0.026), (19, 0.006), (20, -0.023), (21, 0.014), (22, 0.005), (23, -0.052), (24, -0.029), (25, -0.011), (26, -0.05), (27, 0.009), (28, 0.037), (29, 0.051), (30, 0.031), (31, 0.034), (32, 0.026), (33, 0.015), (34, 0.009), (35, -0.144), (36, -0.02), (37, 0.033), (38, -0.052), (39, -0.025), (40, -0.017), (41, -0.028), (42, -0.042), (43, 0.06), (44, -0.004), (45, 0.009), (46, 0.098), (47, -0.092), (48, 0.033), (49, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.87495679 <a title="126-lsi-1" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>Author: Qiang Wu, Sayan Mukherjee, Feng Liang</p><p>Abstract: We developed localized sliced inverse regression for supervised dimension reduction. It has the advantages of preventing degeneracy, increasing estimation accuracy, and automatic subclass discovery in classiﬁcation problems. A semisupervised version is proposed for the use of unlabeled data. The utility is illustrated on simulated as well as real data sets.</p><p>2 0.62292159 <a title="126-lsi-2" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>Author: Andrew Smith, Hongyuan Zha, Xiao-ming Wu</p><p>Abstract: We study the convergence and the rate of convergence of a local manifold learning algorithm: LTSA [13]. The main technical tool is the perturbation analysis on the linear invariant subspace that corresponds to the solution of LTSA. We derive a worst-case upper bound of errors for LTSA which naturally leads to a convergence result. We then derive the rate of convergence for LTSA in a special case. 1</p><p>3 0.55608118 <a title="126-lsi-3" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>Author: Jeremy Lewi, Robert Butera, David M. Schneider, Sarah Woolley, Liam Paninski</p><p>Abstract: Sequential optimal design methods hold great promise for improving the efﬁciency of neurophysiology experiments. However, previous methods for optimal experimental design have incorporated only weak prior information about the underlying neural system (e.g., the sparseness or smoothness of the receptive ﬁeld). Here we describe how to use stronger prior information, in the form of parametric models of the receptive ﬁeld, in order to construct optimal stimuli and further improve the efﬁciency of our experiments. For example, if we believe that the receptive ﬁeld is well-approximated by a Gabor function, then our method constructs stimuli that optimally constrain the Gabor parameters (orientation, spatial frequency, etc.) using as few experimental trials as possible. More generally, we may believe a priori that the receptive ﬁeld lies near a known sub-manifold of the full parameter space; in this case, our method chooses stimuli in order to reduce the uncertainty along the tangent space of this sub-manifold as rapidly as possible. Applications to simulated and real data indicate that these methods may in many cases improve the experimental efﬁciency. 1</p><p>4 0.55529279 <a title="126-lsi-4" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>Author: Florian Steinke, Matthias Hein</p><p>Abstract: This paper discusses non-parametric regression between Riemannian manifolds. This learning problem arises frequently in many application areas ranging from signal processing, computer vision, over robotics to computer graphics. We present a new algorithmic scheme for the solution of this general learning problem based on regularized empirical risk minimization. The regularization functional takes into account the geometry of input and output manifold, and we show that it implements a prior which is particularly natural. Moreover, we demonstrate that our algorithm performs well in a difﬁcult surface registration problem. 1</p><p>5 0.53134644 <a title="126-lsi-5" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>6 0.5291124 <a title="126-lsi-6" href="./nips-2008-Look_Ma%2C_No_Hands%3A_Analyzing_the_Monotonic_Feature_Abstraction_for_Text_Classification.html">128 nips-2008-Look Ma, No Hands: Analyzing the Monotonic Feature Abstraction for Text Classification</a></p>
<p>7 0.50070322 <a title="126-lsi-7" href="./nips-2008-Diffeomorphic_Dimensionality_Reduction.html">61 nips-2008-Diffeomorphic Dimensionality Reduction</a></p>
<p>8 0.47918105 <a title="126-lsi-8" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>9 0.45609757 <a title="126-lsi-9" href="./nips-2008-QUIC-SVD%3A_Fast_SVD_Using_Cosine_Trees.html">188 nips-2008-QUIC-SVD: Fast SVD Using Cosine Trees</a></p>
<p>10 0.42848584 <a title="126-lsi-10" href="./nips-2008-Reducing_statistical_dependencies_in_natural_signals_using_radial_Gaussianization.html">192 nips-2008-Reducing statistical dependencies in natural signals using radial Gaussianization</a></p>
<p>11 0.42749453 <a title="126-lsi-11" href="./nips-2008-Learning_with_Consistency_between_Inductive_Functions_and_Kernels.html">122 nips-2008-Learning with Consistency between Inductive Functions and Kernels</a></p>
<p>12 0.41649225 <a title="126-lsi-12" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>13 0.41348279 <a title="126-lsi-13" href="./nips-2008-Learning_the_Semantic_Correlation%3A_An_Alternative_Way_to_Gain_from_Unlabeled_Text.html">120 nips-2008-Learning the Semantic Correlation: An Alternative Way to Gain from Unlabeled Text</a></p>
<p>14 0.41259992 <a title="126-lsi-14" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>15 0.40657505 <a title="126-lsi-15" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>16 0.40498096 <a title="126-lsi-16" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>17 0.40090042 <a title="126-lsi-17" href="./nips-2008-Extended_Grassmann_Kernels_for_Subspace-Based_Learning.html">80 nips-2008-Extended Grassmann Kernels for Subspace-Based Learning</a></p>
<p>18 0.36564115 <a title="126-lsi-18" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>19 0.3535428 <a title="126-lsi-19" href="./nips-2008-The_Conjoint_Effect_of_Divisive_Normalization_and_Orientation_Selectivity_on_Redundancy_Reduction.html">232 nips-2008-The Conjoint Effect of Divisive Normalization and Orientation Selectivity on Redundancy Reduction</a></p>
<p>20 0.34684563 <a title="126-lsi-20" href="./nips-2008-A_Transductive_Bound_for_the_Voted_Classifier_with_an_Application_to_Semi-supervised_Learning.html">5 nips-2008-A Transductive Bound for the Voted Classifier with an Application to Semi-supervised Learning</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.047), (7, 0.09), (12, 0.016), (28, 0.558), (57, 0.046), (63, 0.014), (71, 0.011), (77, 0.04), (83, 0.045)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.99836332 <a title="126-lda-1" href="./nips-2008-Learning_Taxonomies_by_Dependence_Maximization.html">117 nips-2008-Learning Taxonomies by Dependence Maximization</a></p>
<p>Author: Matthew Blaschko, Arthur Gretton</p><p>Abstract: We introduce a family of unsupervised algorithms, numerical taxonomy clustering, to simultaneously cluster data, and to learn a taxonomy that encodes the relationship between the clusters. The algorithms work by maximizing the dependence between the taxonomy and the original data. The resulting taxonomy is a more informative visualization of complex data than simple clustering; in addition, taking into account the relations between different clusters is shown to substantially improve the quality of the clustering, when compared with state-ofthe-art algorithms in the literature (both spectral clustering and a previous dependence maximization approach). We demonstrate our algorithm on image and text data. 1</p><p>2 0.99782288 <a title="126-lda-2" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>Author: Lavi Shpigelman, Hagai Lalazar, Eilon Vaadia</p><p>Abstract: Using machine learning algorithms to decode intended behavior from neural activity serves a dual purpose. First, these tools allow patients to interact with their environment through a Brain-Machine Interface (BMI). Second, analyzing the characteristics of such methods can reveal the relative signiﬁcance of various features of neural activity, task stimuli, and behavior. In this study we adapted, implemented and tested a machine learning method called Kernel Auto-Regressive Moving Average (KARMA), for the task of inferring movements from neural activity in primary motor cortex. Our version of this algorithm is used in an online learning setting and is updated after a sequence of inferred movements is completed. We ﬁrst used it to track real hand movements executed by a monkey in a standard 3D reaching task. We then applied it in a closed-loop BMI setting to infer intended movement, while the monkey’s arms were comfortably restrained, thus performing the task using the BMI alone. KARMA is a recurrent method that learns a nonlinear model of output dynamics. It uses similarity functions (termed kernels) to compare between inputs. These kernels can be structured to incorporate domain knowledge into the method. We compare KARMA to various state-of-the-art methods by evaluating tracking performance and present results from the KARMA based BMI experiments. 1</p><p>same-paper 3 0.99750543 <a title="126-lda-3" href="./nips-2008-Localized_Sliced_Inverse_Regression.html">126 nips-2008-Localized Sliced Inverse Regression</a></p>
<p>Author: Qiang Wu, Sayan Mukherjee, Feng Liang</p><p>Abstract: We developed localized sliced inverse regression for supervised dimension reduction. It has the advantages of preventing degeneracy, increasing estimation accuracy, and automatic subclass discovery in classiﬁcation problems. A semisupervised version is proposed for the use of unlabeled data. The utility is illustrated on simulated as well as real data sets.</p><p>4 0.99676853 <a title="126-lda-4" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>Author: Angela J. Yu, Jonathan D. Cohen</p><p>Abstract: In a variety of behavioral tasks, subjects exhibit an automatic and apparently suboptimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no real predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reﬂect the inadvertent engagement of mechanisms critical for adapting to a changing environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential ﬁltering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential ﬁlter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that parameter-tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities. 1</p><p>5 0.9960655 <a title="126-lda-5" href="./nips-2008-Reconciling_Real_Scores_with_Binary_Comparisons%3A_A_New_Logistic_Based_Model_for_Ranking.html">190 nips-2008-Reconciling Real Scores with Binary Comparisons: A New Logistic Based Model for Ranking</a></p>
<p>Author: Nir Ailon</p><p>Abstract: The problem of ranking arises ubiquitously in almost every aspect of life, and in particular in Machine Learning/Information Retrieval. A statistical model for ranking predicts how humans rank subsets V of some universe U . In this work we deﬁne a statistical model for ranking that satisﬁes certain desirable properties. The model automatically gives rise to a logistic regression based approach to learning how to rank, for which the score and comparison based approaches are dual views. This offers a new generative approach to ranking which can be used for IR. There are two main contexts for this work. The ﬁrst is the theory of econometrics and study of statistical models explaining human choice of alternatives. In this context, we will compare our model with other well known models. The second context is the problem of ranking in machine learning, usually arising in the context of information retrieval. Here, much work has been done in the discriminative setting, where different heuristics are used to deﬁne ranking risk functions. Our model is built rigorously and axiomatically based on very simple desirable properties deﬁned locally for comparisons, and automatically implies the existence of a global score function serving as a natural model parameter which can be efﬁciently ﬁtted to pairwise comparison judgment data by solving a convex optimization problem. 1</p><p>6 0.99555612 <a title="126-lda-6" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>7 0.99498367 <a title="126-lda-7" href="./nips-2008-Empirical_performance_maximization_for_linear_rank_statistics.html">72 nips-2008-Empirical performance maximization for linear rank statistics</a></p>
<p>8 0.99324799 <a title="126-lda-8" href="./nips-2008-Overlaying_classifiers%3A_a_practical_approach_for_optimal_ranking.html">174 nips-2008-Overlaying classifiers: a practical approach for optimal ranking</a></p>
<p>9 0.99067134 <a title="126-lda-9" href="./nips-2008-Stress%2C_noradrenaline%2C_and_realistic_prediction_of_mouse_behaviour_using_reinforcement_learning.html">222 nips-2008-Stress, noradrenaline, and realistic prediction of mouse behaviour using reinforcement learning</a></p>
<p>10 0.98418176 <a title="126-lda-10" href="./nips-2008-Evaluating_probabilities_under_high-dimensional_latent_variable_models.html">77 nips-2008-Evaluating probabilities under high-dimensional latent variable models</a></p>
<p>11 0.97682279 <a title="126-lda-11" href="./nips-2008-On_Bootstrapping_the_ROC_Curve.html">159 nips-2008-On Bootstrapping the ROC Curve</a></p>
<p>12 0.97008455 <a title="126-lda-12" href="./nips-2008-Global_Ranking_Using_Continuous_Conditional_Random_Fields.html">93 nips-2008-Global Ranking Using Continuous Conditional Random Fields</a></p>
<p>13 0.96325582 <a title="126-lda-13" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>14 0.96148956 <a title="126-lda-14" href="./nips-2008-Human_Active_Learning.html">101 nips-2008-Human Active Learning</a></p>
<p>15 0.95920271 <a title="126-lda-15" href="./nips-2008-Kernel_Measures_of_Independence_for_non-iid_Data.html">112 nips-2008-Kernel Measures of Independence for non-iid Data</a></p>
<p>16 0.95762658 <a title="126-lda-16" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>17 0.95758325 <a title="126-lda-17" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>18 0.95658362 <a title="126-lda-18" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>19 0.95562518 <a title="126-lda-19" href="./nips-2008-Structure_Learning_in_Human_Sequential_Decision-Making.html">223 nips-2008-Structure Learning in Human Sequential Decision-Making</a></p>
<p>20 0.94991243 <a title="126-lda-20" href="./nips-2008-Unifying_the_Sensory_and_Motor_Components_of_Sensorimotor_Adaptation.html">244 nips-2008-Unifying the Sensory and Motor Components of Sensorimotor Adaptation</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
