<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-34" href="#">nips2008-34</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</h1>
<br/><p>Source: <a title="nips-2008-34-pdf" href="http://papers.nips.cc/paper/3543-bayesian-network-score-approximation-using-a-metagraph-kernel.pdf">pdf</a></p><p>Author: Benjamin Yackley, Eduardo Corona, Terran Lane</p><p>Abstract: Many interesting problems, including Bayesian network structure-search, can be cast in terms of ﬁnding the optimum value of a function over the space of graphs. However, this function is often expensive to compute exactly. We here present a method derived from the study of Reproducing Kernel Hilbert Spaces which takes advantage of the regular structure of the space of all graphs on a ﬁxed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy. We then test this method on both a small testing set and a real-world Bayesian network; the results suggest that not only is this method reasonably accurate, but that the BDe score itself varies quadratically over the space of all graphs. 1</p><p>Reference: <a title="nips-2008-34-reference" href="../nips2008_reference/nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 We here present a method derived from the study of Reproducing Kernel Hilbert Spaces which takes advantage of the regular structure of the space of all graphs on a ﬁxed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy. [sent-3, score-0.433]
</p><p>2 We then test this method on both a small testing set and a real-world Bayesian network; the results suggest that not only is this method reasonably accurate, but that the BDe score itself varies quadratically over the space of all graphs. [sent-4, score-0.15]
</p><p>3 Specifically, the application we present here is that of estimating scores on the space of Bayesian networks as a ﬁrst step toward a quick way to obtain a network which is optimal given a set of data. [sent-6, score-0.251]
</p><p>4 Usually, the search process requires a full recomputation of the posterior likelihood of the graph at every step, and is therefore slow. [sent-7, score-0.191]
</p><p>5 We present a new approach to the problem of approximating functions such as this one, where the mapping is of an object (the graph, in this particular case) to a real number (its BDe score). [sent-8, score-0.029]
</p><p>6 In other words, we have a function f : Γn → R (where Γn is the set of all directed graphs on n nodes) from which we have a small number of samples, and we would like to interpolate the rest. [sent-9, score-0.32]
</p><p>7 The technique hinges on the set Γn having a structure which can be factored into a Cartesian product, as well as on the function we approximate being smooth over this structure. [sent-10, score-0.087]
</p><p>8 Although Bayesian networks are by deﬁnition acyclic, our approximation technique applies to the general directed-graph case. [sent-11, score-0.119]
</p><p>9 Because a given directed graph has n2 possible edges, we can imagine the set of all graphs as itself being a Hamming cube of degree n2 – a 2 “metagraph” with 2n nodes, since each edge can be independently present or absent. [sent-12, score-0.567]
</p><p>10 We say that two graphs are connected with an edge in our metagraph if they diﬀer in one and only one edge. [sent-13, score-0.756]
</p><p>11 We can similarly identify each graph with a bit string by “unraveling” the adjacency matrix into a long string of zeros and ones. [sent-14, score-0.411]
</p><p>12 However, if we know beforehand an ordering on the nodes of our graph to which all directed graphs must stay consistent (to enforce acyclicness), then there are only n possible edges, and the size of our metagraph 2 n drops to 2( 2 ) . [sent-15, score-1.159]
</p><p>13 The same correspondence can then be made between these graphs and bit strings of length n . [sent-16, score-0.336]
</p><p>14 2  Since the eigenvectors of the Laplacian of a graph form a basis for all smooth functions on the graph, then we can use our known sampled values (which correspond to a mapping from a subset of nodes on our metagraph to the real numbers) to interpolate the others. [sent-17, score-1.227]
</p><p>15 Despite the incredible size of the metagraph, we show that this problem is by no means intractable, and functions can in fact be approximated in polynomial time. [sent-18, score-0.029]
</p><p>16 We also demonstrate this technique both on a small network for which we can exhaustively compute the score of every possible directed acyclic graph, as well as on a larger real-world network. [sent-19, score-0.361]
</p><p>17 The results show that the method is accurate, and additionally suggest that the BDe scoring metric used is quadratic over the metagraph. [sent-20, score-0.076]
</p><p>18 1  Spectral Properties of the Hypercube The Kronecker Product and Kronecker Sum  The matrix operators known as the Kronecker product and Kronecker sum, denoted ⊗ and ⊕ respectively, play a key role in the derivation of the spectral properties of the hypercube. [sent-22, score-0.181]
</p><p>19 Given matrices A ∈ Ri×j and B ∈ Rk×l , A ⊗ B is the matrix in Rik×jl such that:   a11 B a12 B · · · a1j B a2j B   a21 B a22 B  A⊗B = . [sent-23, score-0.083]
</p><p>20 aj1 B aj2 B aij B  The Kronecker sum is deﬁned over a pair of square matrices A ∈ Rm×m and B ∈ Rn×n as A ⊕ B = A ⊗ In + Im ⊗ B, where In denotes an n × n identity matrix[8]. [sent-29, score-0.062]
</p><p>21 2  Cartesian Products of Graphs  The Cartesian product of two graphs G1 and G2 , denoted G1 × G2 , is intuitively deﬁned as the result of replacing every node in G1 with a copy of G2 and connecting corresponding edges together. [sent-31, score-0.36]
</p><p>22 More formally, if the product is the graph G = G1 × G2 , then the vertex set of G is the Cartesian product of the vertex sets of G1 and G2 . [sent-32, score-0.545]
</p><p>23 In other words, for any vertex v1 in G1 and any vertex v2 in G2 , there exists a vertex (v1 , v2 ) in G. [sent-33, score-0.312]
</p><p>24 Additionally, the edge set of G is such that, for any edge (u1 , u2 ) → (v1 , v2 ) in G, either u1 = v1 and u2 → v2 is an edge in G2 , or u2 = v2 and u1 → v1 is an edge in G1 . [sent-34, score-0.192]
</p><p>25 [7] In particular, the set of hypercube graphs (or, identically, the set of Hamming cubes) can be derived using the Cartesian product operator. [sent-35, score-0.5]
</p><p>26 If we denote the graph of an n-dimensional hypercube as Qn , then Qn+1 = Qn × Q1 , where the graph Q1 is a two-node graph with a single bidirectional edge. [sent-36, score-0.792]
</p><p>27 3  Spectral Properties of Cartesian Products  The Cartesian product has the property that, if we denote the adjacency matrix of a graph G as A(G), then A(G1 × G2 ) = A(G1 ) ⊕ A(G2 ). [sent-38, score-0.35]
</p><p>28 Additionally, if A(G1 ) has m eigenvectors φk and corresponding eigenvalues λk (with k = 1. [sent-39, score-0.283]
</p><p>29 m) while A(G2 ) has n eigenvectors ψl with corresponding eigenvalues µl (with l = 1. [sent-42, score-0.283]
</p><p>30 It should also be noted that, because hypercubes are all k-regular graphs (in particular, the hypercube Qn is n-regular), the form of the normalized Laplacian becomes simple. [sent-46, score-0.502]
</p><p>31 The usual formula for the normalized Laplacian is: ˜ L = I − D−1/2 AD−1/2 However, since the graph is regular, we have D = kI, and so 1 ˜ L = I − (kI)−1/2 A(kI)−1/2 = I − A. [sent-47, score-0.261]
</p><p>32 k  Also note that, because the formula for the combinatorial Laplacian is L = D − A, we also 1 ˜ have L = k L. [sent-48, score-0.114]
</p><p>33 The Laplacian also distributes over graph products, as shown in the following theorem. [sent-49, score-0.254]
</p><p>34 Theorem 1 Given two simple, undirected graphs G1 = (V1 , E1 ) and G2 = (V2 , E2 ), with combinatorial Laplacians LG1 and LG2 ,the combinatorial Laplacian of the Cartesian product graph G1 × G2 is then given by: LG1 ×G2 = LG1 ⊕ LG2 Proof. [sent-50, score-0.622]
</p><p>35 LG1 = DG1 − A(G1 ) LG2 = DG2 − A(G2 ) Here, DG denotes the degree diagonal matrix of the graph G. [sent-51, score-0.293]
</p><p>36 So, LQ1 = I −  1 A(Q1 ) = k  1 −1 −1 1  Its eigenvectors and eigenvalues can be easily computed; it has the eigenvector  1 1  with  1 −1  eigenvalue 0 and the eigenvector with eigenvalue 2. [sent-57, score-0.509]
</p><p>37 It should be noted here that an n-dimensional hypercube graph will have 2n eigenvalues with only n + 1 distinct values; they will be the values 2k for k = 0. [sent-59, score-0.467]
</p><p>38 n k If we arrange these columns in the proper order as a matrix, a familiar shape emerges:   1 1 1 1  1 −1 1 −1   1 1 −1 −1  1 −1 −1 1 This is, in fact, the Hadamard matrix of order 4, just as placing our original two eigenvectors side-by-side creates the order-2 Hadamard matrix. [sent-63, score-0.429]
</p><p>39 Since hypercubes can be recursively constructed using Kronecker sums, the basis for smooth functions on hypercubes (i. [sent-65, score-0.253]
</p><p>40 the set of eigenvectors of their graph Laplacian) is the Hadamard basis. [sent-67, score-0.417]
</p><p>41 Consequently, there is no need to ever compute a full eigenvector explicitly; there is an explicit formula for a given entry of any Hadamard matrix: (H2n )ij = (−1)  bi ,bj  The notation bx here means “the n-bit binary expansion of x interpreted as a vector of 0s and 1s”. [sent-68, score-0.181]
</p><p>42 This is the key to computing our kernel eﬃciently, not only because it takes very little time to compute arbitrary elements of eigenvectors, but because we are free to compute only the elements we need instead of entire eigenvectors at once. [sent-69, score-0.325]
</p><p>43 1  The Metagraph Kernel The Optimization Framework  Given the above, we now formulate the regression problem that will allow us to approximate k our desired function at arbitrary points. [sent-71, score-0.028]
</p><p>44 Given a set of k observations {yi }i=1 corresponding ˆ which minimizes the squared error to nodes xi in the metagraph, we wish to ﬁnd the f between our estimate and all observed points and also which is a suﬃciently smooth function on the graph to avoid overﬁtting. [sent-72, score-0.491]
</p><p>45 In other words, ˆ f = arg min f  1 k  k  f (xi ) − yi  2  + cf T Lm f  i=1  The variable m in this expression controls the type of smoothing; if m = 1, then we are penalizing ﬁrst-diﬀerences (i. [sent-73, score-0.044]
</p><p>46 We will take m = 2 in our experiments, to penalize second-diﬀerences (the usual case when using spline interpolation)[6]. [sent-76, score-0.066]
</p><p>47 In the case of our hypercube graph, Ω0 turns out to be particularly simple; it consists only of constant functions (i. [sent-78, score-0.248]
</p><p>48 Meanwhile, the space Ω1 is formulated under the RKHS framework as a set of columns of the kernel ˆ matrix (denoted K1 ). [sent-81, score-0.159]
</p><p>49 Consequently, we can write f = 1T d + K1 e, and so our formulation becomes: 1 k  ˆ f = arg min f  k  (1T d + K1 e)(xi ) − yi  2  + ceT K1 e  i=1  The solution to this optimization problem is for our coeﬃcients d and e to be linear estimates on y, our vector of observed values. [sent-82, score-0.029]
</p><p>50 Therefore, if we have an eﬃcient way to compute arbitrary entries of the kernel matrix K1 , we can estimate functions anywhere in the graph. [sent-85, score-0.178]
</p><p>51 2  Calculating entries of K1  First, we must choose an order r ∈ {1, 2. [sent-87, score-0.044]
</p><p>52 n}; this is equivalent to selecting the degree of a polynomial used to perform standard interpolation on the hypercube. [sent-90, score-0.108]
</p><p>53 The eﬀect that r will have on our problem will be to select the set of basis functions we consider; the eigenvectors corresponding to a given eigenvalue 2k are the n eigenvectors which divide the space n k into identically-valued regions which are themselves (n − k)-dimensional hypercubes. [sent-91, score-0.578]
</p><p>54 For example, the 3 eigenfunctions on the 3-dimensional hypercube which correspond to the 2 eigenvalue 3 (so k = 1) are those which separate the space into a positive plane and a negative plane along each of the three axes. [sent-92, score-0.408]
</p><p>55 Because these eigenfunctions are all equivalent apart from rotation, there is no reason to choose one to be in our basis over another, and so we can say that the total number of eigenfunctions we use in our approximation is equal r to k=0 n for our chosen value of r. [sent-93, score-0.199]
</p><p>56 Therefore, if we use the notation |x|1 to mean “the number of ones in the binary expansion of x”, then choosing the order r is equivalent to choosing a basis of eigenvectors φl such that |l|1 is less than or equal to r. [sent-95, score-0.341]
</p><p>57 Therefore, we have: (K1 )ij = 1≤|l|1 ≤r  n 2k  m  Hil Hjl  Because k is equal to |l|1 , and because we already have an explicit form for any Hxy , we can write m 1 n (K1 )ij = (−1)  |l|1 =k  ˙ The ∨ symbol here denotes exclusive-or, which is equivalent to addition mod 2 in this domain. [sent-96, score-0.039]
</p><p>58 The justiﬁcation for this is that only the parity of the exponent (odd or even) matters, and locations in the bit strings bi and bj which are both zero or both one contribute no change to the overall parity. [sent-97, score-0.355]
</p><p>59 Notably, this shows that the value of the kernel between ˙ any two bit strings bi and bj is dependent only on bi ∨bj , the key result which allows us to ˙ compute these values quickly. [sent-98, score-0.489]
</p><p>60 If we let Sk (bi , bj ) = |l|1 =k (−1)  , there is a recursive formulation for the computation of Sk (bi , bj ) in terms of Sk−1 (bi , bj ), which is the method used in the experiments due to its speed and feasability of computation. [sent-99, score-0.338]
</p><p>61 We generated a random “base truth” network and sampled it 1000 times, creating a data set. [sent-102, score-0.145]
</p><p>62 We then created an exhaustive set of 26 = 64 directed graphs; there are six possible edges in a four-node graph, assuming we already have some sort of node ordering that allows us to orient edges, and so this represented all possibilities. [sent-103, score-0.208]
</p><p>63 Because we chose the node ordering to be consistent with our base network, one of these graphs was in fact the correct network. [sent-104, score-0.269]
</p><p>64 We then gave each of the set of 64 graphs a log-marginal-likelihood score (i. [sent-105, score-0.291]
</p><p>65 As expected, the correct network came out to have the greatest likelihood. [sent-108, score-0.113]
</p><p>66 Additionally, computation of the Rayleigh quotient shows that the function is a globally smooth one over the graph topology. [sent-109, score-0.234]
</p><p>67 We then performed a set of experiments using the metagraph kernel. [sent-110, score-0.5]
</p><p>68 1  Randomly Drawn Observations  First, we partitioned the set of 64 observations randomly into two groups. [sent-113, score-0.059]
</p><p>69 The training group ranged in size from 3 to 63 samples, with the rest used as the testing group. [sent-114, score-0.066]
</p><p>70 We then used the training group as the set of observations, and queried the metagraph kernel to predict the values of the networks in the testing group. [sent-115, score-0.67]
</p><p>71 Note that order 3 performs the best overall for large numbers of observations, overtaking the order-2 approximation at 41 values observed and staying the best until the end. [sent-117, score-0.15]
</p><p>72 However, order 1 performs the best for small numbers of observations (perhaps due to overﬁtting errors caused by the higher orders) and order 2 performs the best in the middle. [sent-118, score-0.182]
</p><p>73 The data suggests that the proper order to which to compute the kernel in order to obtain the best approximations is a function of both the size of the space and the number of observations made within that space. [sent-119, score-0.257]
</p><p>74 2  Best/worst-case Observations  Secondly, we performed experiments where the observations were obtained from networks which were in the neighborhood around the known true maximum, as well as ones from networks which were as far from it as possible. [sent-122, score-0.165]
</p><p>75 Despite small diﬀerences in shape, the results are largely identical, indicating that the distribution of the samples throughout Γn matters very little. [sent-124, score-0.077]
</p><p>76 We ﬁrst generated data according to the true network, sampling it 1000 times, then generated random directed graphs over the 37 nodes to see if their scores could be predicted as well as in the smaller four-node case. [sent-127, score-0.478]
</p><p>77 We made no attempt to enforce an ordering; although the graphs were all acyclic, we placed no assumption on the graphs being consistent with the same node-ordering as the original. [sent-129, score-0.416]
</p><p>78 The scores of these sets, calculated using the data drawn from the true network, served as our observed data. [sent-130, score-0.1]
</p><p>79 The results, with root-meansquared error plotted against the order of the kernel, are shown in Figure 4. [sent-132, score-0.044]
</p><p>80 Additionally, we calculated a baseline by taking the mean of the 1000 sampled scores and calling that the estimated score for every graph in our testing set. [sent-133, score-0.408]
</p><p>81 The results show that the metagraph approximation method performs signiﬁcantly better than the baseline for low orders of approximation with higher amounts of sampled data. [sent-134, score-0.616]
</p><p>82 This makes intuitive sense; the more data there is, the better the approximation should be. [sent-135, score-0.042]
</p><p>83 Additionally, the spike at order 2 suggests that the BDe score itself varies quadratically over the metagraph. [sent-136, score-0.163]
</p><p>84 5  Conclusion  Functions of graphs to real numbers, such as the posterior likelihood of a Bayesian network given a set of data, can be approximated to a high degree of accuracy by taking advantage of a hybercubic “metagraph” structure. [sent-141, score-0.373]
</p><p>85 Because the metagraph is regular, standard techniques of interpolation can be used in a straightforward way to obtain predictions for the values at unknown points. [sent-142, score-0.556]
</p><p>86 6  Future Work  Although this technique allows for quick and accurate prediction of function values on the metagraph, it oﬀers no hints (as of yet) as to where the maximum of the function might be. [sent-143, score-0.078]
</p><p>87 This could, for instance, allow one to generate a Bayesian network which is likely to be close to optimal, and if true optimality is required, that approximate graph could be used  as a starting point for a stepwise method such as MCMC. [sent-144, score-0.304]
</p><p>88 Even without a direct way to ﬁnd such an optimum, though, it may be worth using this approximation technique inside an MCMC search instead of the usual exact-score computation in order to quickly converge on a something close to the desired optimum. [sent-145, score-0.161]
</p><p>89 In fact, this technique should be able to be used unchanged on any problem which involves the computation of a real-valued function over bit strings. [sent-147, score-0.116]
</p><p>90 For example, one may desire an approximation to a function of permutations of some number of elements to real numbers. [sent-149, score-0.109]
</p><p>91 The set of permutations of a given number of elements, denoted Sn , has a similarly regular structure (which can be seen as a graph in which two permutations are connected if a single swap leads from one to the other), but not a hypercubic one. [sent-150, score-0.38]
</p><p>92 The structure-search problem on Bayes Nets can also be cast as a search over orderings of nodes alone[5], so a way to approximate a function over permutations would be useful there as well. [sent-151, score-0.239]
</p><p>93 Other domains have this ability to be turned into regular graphs – the integers mod n with edges between numbers that diﬀer by 1 form a loop, for example. [sent-152, score-0.416]
</p><p>94 It should be possible to apply a similar trick to obtain function approximations not only on these domains, but on arbitrary Cartesian products of them. [sent-153, score-0.146]
</p><p>95 So, for instance, remembering that the directions of the edges of Bayesian network are completely speciﬁed given an ordering on the nodes, the network structure search problem on n nodes can be recast as a function approximation over the set Sn × Q( n ) . [sent-154, score-0.566]
</p><p>96 Many problems can be cast into the metagraph framework; we have 2 only just scratched the surface here. [sent-155, score-0.541]
</p><p>97 A Bayesian approach to learning Bayesian networks with local structure. [sent-176, score-0.033]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('metagraph', 0.5), ('kronecker', 0.234), ('eigenvectors', 0.226), ('hypercube', 0.219), ('graphs', 0.208), ('laplacian', 0.196), ('graph', 0.191), ('cartesian', 0.188), ('bde', 0.15), ('hadamard', 0.136), ('nodes', 0.131), ('qn', 0.125), ('network', 0.113), ('vertex', 0.104), ('im', 0.103), ('alarm', 0.101), ('bj', 0.1), ('bi', 0.095), ('score', 0.083), ('erences', 0.082), ('products', 0.079), ('edges', 0.079), ('additionally', 0.076), ('hypercubes', 0.075), ('combinatorial', 0.075), ('product', 0.073), ('bit', 0.072), ('kernel', 0.071), ('scores', 0.071), ('bayesian', 0.07), ('directed', 0.068), ('permutations', 0.067), ('eigenvalue', 0.066), ('eigenfunctions', 0.063), ('distributes', 0.063), ('mexico', 0.063), ('ordering', 0.061), ('observations', 0.059), ('spectral', 0.058), ('eigenvalues', 0.057), ('interpolation', 0.056), ('strings', 0.056), ('regular', 0.055), ('deg', 0.055), ('lm', 0.055), ('acyclic', 0.053), ('degree', 0.052), ('matrix', 0.05), ('sk', 0.049), ('edge', 0.048), ('ki', 0.048), ('coe', 0.047), ('spectra', 0.047), ('eigenvector', 0.047), ('di', 0.046), ('cf', 0.044), ('interpolate', 0.044), ('technique', 0.044), ('order', 0.044), ('smooth', 0.043), ('approximation', 0.042), ('cast', 0.041), ('matters', 0.04), ('ones', 0.04), ('root', 0.039), ('near', 0.039), ('formula', 0.039), ('mod', 0.039), ('approximations', 0.039), ('columns', 0.038), ('recursive', 0.038), ('squared', 0.038), ('samples', 0.037), ('quadratically', 0.036), ('adjacency', 0.036), ('spline', 0.035), ('numbers', 0.035), ('group', 0.035), ('quick', 0.034), ('spectrum', 0.033), ('hamming', 0.033), ('matrices', 0.033), ('networks', 0.033), ('sn', 0.033), ('sampled', 0.032), ('exponent', 0.032), ('smoothing', 0.032), ('testing', 0.031), ('string', 0.031), ('usual', 0.031), ('basis', 0.031), ('plane', 0.03), ('observed', 0.029), ('sum', 0.029), ('functions', 0.029), ('arbitrary', 0.028), ('ij', 0.028), ('recast', 0.027), ('arrange', 0.027), ('beinlich', 0.027)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000005 <a title="34-tfidf-1" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>Author: Benjamin Yackley, Eduardo Corona, Terran Lane</p><p>Abstract: Many interesting problems, including Bayesian network structure-search, can be cast in terms of ﬁnding the optimum value of a function over the space of graphs. However, this function is often expensive to compute exactly. We here present a method derived from the study of Reproducing Kernel Hilbert Spaces which takes advantage of the regular structure of the space of all graphs on a ﬁxed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy. We then test this method on both a small testing set and a real-world Bayesian network; the results suggest that not only is this method reasonably accurate, but that the BDe score itself varies quadratically over the space of all graphs. 1</p><p>2 0.16532634 <a title="34-tfidf-2" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>3 0.1554099 <a title="34-tfidf-3" href="./nips-2008-Fast_Prediction_on_a_Tree.html">84 nips-2008-Fast Prediction on a Tree</a></p>
<p>Author: Mark Herbster, Massimiliano Pontil, Sergio R. Galeano</p><p>Abstract: Given an n-vertex weighted tree with structural diameter S and a subset of m vertices, we present a technique to compute a corresponding m × m Gram matrix of the pseudoinverse of the graph Laplacian in O(n + m2 + mS) time. We discuss the application of this technique to fast label prediction on a generic graph. We approximate the graph with a spanning tree and then we predict with the kernel perceptron. We address the approximation of the graph with either a minimum spanning tree or a shortest path tree. The fast computation of the pseudoinverse enables us to address prediction problems on large graphs. We present experiments on two web-spam classiﬁcation tasks, one of which includes a graph with 400,000 vertices and more than 10,000,000 edges. The results indicate that the accuracy of our technique is competitive with previous methods using the full graph information. 1</p><p>4 0.13944435 <a title="34-tfidf-4" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>5 0.12993489 <a title="34-tfidf-5" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>Author: Yair Weiss, Antonio Torralba, Rob Fergus</p><p>Abstract: Semantic hashing[1] seeks compact binary codes of data-points so that the Hamming distance between codewords correlates with semantic similarity. In this paper, we show that the problem of ﬁnding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresholded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds, we show how to eﬃciently calculate the code of a novel datapoint. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes outperform the state-of-the art. 1</p><p>6 0.12975274 <a title="34-tfidf-6" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>7 0.12606744 <a title="34-tfidf-7" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<p>8 0.10981161 <a title="34-tfidf-8" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>9 0.10705766 <a title="34-tfidf-9" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>10 0.099823251 <a title="34-tfidf-10" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>11 0.09822648 <a title="34-tfidf-11" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>12 0.084405668 <a title="34-tfidf-12" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>13 0.07558547 <a title="34-tfidf-13" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>14 0.075460143 <a title="34-tfidf-14" href="./nips-2008-Convergence_and_Rate_of_Convergence_of_a_Manifold-Based_Dimension_Reduction_Algorithm.html">51 nips-2008-Convergence and Rate of Convergence of a Manifold-Based Dimension Reduction Algorithm</a></p>
<p>15 0.074540809 <a title="34-tfidf-15" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>16 0.073572457 <a title="34-tfidf-16" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>17 0.072038636 <a title="34-tfidf-17" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>18 0.071282543 <a title="34-tfidf-18" href="./nips-2008-Structured_ranking_learning_using_cumulative_distribution_networks.html">224 nips-2008-Structured ranking learning using cumulative distribution networks</a></p>
<p>19 0.069828108 <a title="34-tfidf-19" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>20 0.06933663 <a title="34-tfidf-20" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.211), (1, -0.038), (2, -0.019), (3, 0.091), (4, 0.118), (5, -0.167), (6, 0.113), (7, -0.012), (8, 0.079), (9, -0.255), (10, -0.042), (11, 0.004), (12, -0.103), (13, -0.034), (14, -0.013), (15, -0.102), (16, -0.001), (17, -0.0), (18, -0.054), (19, -0.069), (20, -0.002), (21, -0.053), (22, 0.074), (23, -0.069), (24, -0.023), (25, 0.075), (26, 0.004), (27, -0.033), (28, 0.038), (29, -0.022), (30, 0.057), (31, -0.032), (32, 0.031), (33, 0.047), (34, 0.006), (35, -0.058), (36, 0.062), (37, -0.076), (38, 0.002), (39, -0.031), (40, -0.063), (41, 0.041), (42, -0.003), (43, -0.182), (44, 0.086), (45, -0.061), (46, -0.017), (47, 0.088), (48, 0.011), (49, -0.013)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.9614746 <a title="34-lsi-1" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>Author: Benjamin Yackley, Eduardo Corona, Terran Lane</p><p>Abstract: Many interesting problems, including Bayesian network structure-search, can be cast in terms of ﬁnding the optimum value of a function over the space of graphs. However, this function is often expensive to compute exactly. We here present a method derived from the study of Reproducing Kernel Hilbert Spaces which takes advantage of the regular structure of the space of all graphs on a ﬁxed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy. We then test this method on both a small testing set and a real-world Bayesian network; the results suggest that not only is this method reasonably accurate, but that the BDe score itself varies quadratically over the space of all graphs. 1</p><p>2 0.72297555 <a title="34-lsi-2" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>Author: Gal Elidan, Stephen Gould</p><p>Abstract: With the increased availability of data for complex domains, it is desirable to learn Bayesian network structures that are sufﬁciently expressive for generalization while also allowing for tractable inference. While the method of thin junction trees can, in principle, be used for this purpose, its fully greedy nature makes it prone to overﬁtting, particularly when data is scarce. In this work we present a novel method for learning Bayesian networks of bounded treewidth that employs global structure modiﬁcations and that is polynomial in the size of the graph and the treewidth bound. At the heart of our method is a triangulated graph that we dynamically update in a way that facilitates the addition of chain structures that increase the bound on the model’s treewidth by at most one. We demonstrate the effectiveness of our “treewidth-friendly” method on several real-life datasets. Importantly, we also show that by using global operators, we are able to achieve better generalization even when learning Bayesian networks of unbounded treewidth. 1</p><p>3 0.72074735 <a title="34-lsi-3" href="./nips-2008-Efficient_Exact_Inference_in_Planar_Ising_Models.html">69 nips-2008-Efficient Exact Inference in Planar Ising Models</a></p>
<p>Author: Nicol N. Schraudolph, Dmitry Kamenetsky</p><p>Abstract: We give polynomial-time algorithms for the exact computation of lowest-energy states, worst margin violators, partition functions, and marginals in certain binary undirected graphical models. Our approach provides an interesting alternative to the well-known graph cut paradigm in that it does not impose any submodularity constraints; instead we require planarity to establish a correspondence with perfect matchings in an expanded dual graph. Maximum-margin parameter estimation for a boundary detection task shows our approach to be efﬁcient and effective. A C++ implementation is available from http://nic.schraudolph.org/isinf/. 1</p><p>4 0.70445621 <a title="34-lsi-4" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>Author: Markus Maier, Ulrike V. Luxburg, Matthias Hein</p><p>Abstract: Graph clustering methods such as spectral clustering are deﬁned for general weighted graphs. In machine learning, however, data often is not given in form of a graph, but in terms of similarity (or distance) values between points. In this case, ﬁrst a neighborhood graph is constructed using the similarities between the points and then a graph clustering algorithm is applied to this graph. In this paper we investigate the inﬂuence of the construction of the similarity graph on the clustering results. We ﬁrst study the convergence of graph clustering criteria such as the normalized cut (Ncut) as the sample size tends to inﬁnity. We ﬁnd that the limit expressions are different for different types of graph, for example the r-neighborhood graph or the k-nearest neighbor graph. In plain words: Ncut on a kNN graph does something systematically different than Ncut on an r-neighborhood graph! This ﬁnding shows that graph clustering criteria cannot be studied independently of the kind of graph they are applied to. We also provide examples which show that these differences can be observed for toy and real data already for rather small sample sizes. 1</p><p>5 0.69349748 <a title="34-lsi-5" href="./nips-2008-Fast_Prediction_on_a_Tree.html">84 nips-2008-Fast Prediction on a Tree</a></p>
<p>Author: Mark Herbster, Massimiliano Pontil, Sergio R. Galeano</p><p>Abstract: Given an n-vertex weighted tree with structural diameter S and a subset of m vertices, we present a technique to compute a corresponding m × m Gram matrix of the pseudoinverse of the graph Laplacian in O(n + m2 + mS) time. We discuss the application of this technique to fast label prediction on a generic graph. We approximate the graph with a spanning tree and then we predict with the kernel perceptron. We address the approximation of the graph with either a minimum spanning tree or a shortest path tree. The fast computation of the pseudoinverse enables us to address prediction problems on large graphs. We present experiments on two web-spam classiﬁcation tasks, one of which includes a graph with 400,000 vertices and more than 10,000,000 edges. The results indicate that the accuracy of our technique is competitive with previous methods using the full graph information. 1</p><p>6 0.66329169 <a title="34-lsi-6" href="./nips-2008-Supervised_Bipartite_Graph_Inference.html">225 nips-2008-Supervised Bipartite Graph Inference</a></p>
<p>7 0.61580861 <a title="34-lsi-7" href="./nips-2008-Online_Prediction_on_Large_Diameter_Graphs.html">171 nips-2008-Online Prediction on Large Diameter Graphs</a></p>
<p>8 0.61213428 <a title="34-lsi-8" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>9 0.59212083 <a title="34-lsi-9" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>10 0.57300717 <a title="34-lsi-10" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>11 0.53107393 <a title="34-lsi-11" href="./nips-2008-Skill_Characterization_Based_on_Betweenness.html">212 nips-2008-Skill Characterization Based on Betweenness</a></p>
<p>12 0.49362874 <a title="34-lsi-12" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>13 0.48233584 <a title="34-lsi-13" href="./nips-2008-One_sketch_for_all%3A_Theory_and_Application_of_Conditional_Random_Sampling.html">167 nips-2008-One sketch for all: Theory and Application of Conditional Random Sampling</a></p>
<p>14 0.47766757 <a title="34-lsi-14" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>15 0.47354725 <a title="34-lsi-15" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>16 0.45583656 <a title="34-lsi-16" href="./nips-2008-Regularized_Co-Clustering_with_Dual_Supervision.html">193 nips-2008-Regularized Co-Clustering with Dual Supervision</a></p>
<p>17 0.42498097 <a title="34-lsi-17" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>18 0.41575745 <a title="34-lsi-18" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>19 0.39158076 <a title="34-lsi-19" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>20 0.39132655 <a title="34-lsi-20" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(3, 0.235), (6, 0.042), (7, 0.085), (12, 0.035), (15, 0.017), (28, 0.213), (57, 0.065), (59, 0.029), (63, 0.03), (64, 0.016), (71, 0.023), (77, 0.061), (78, 0.018), (83, 0.05), (89, 0.014)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>1 0.83181423 <a title="34-lda-1" href="./nips-2008-Support_Vector_Machines_with_a_Reject_Option.html">228 nips-2008-Support Vector Machines with a Reject Option</a></p>
<p>Author: Yves Grandvalet, Alain Rakotomamonjy, Joseph Keshet, Stéphane Canu</p><p>Abstract: We consider the problem of binary classiﬁcation where the classiﬁer may abstain instead of classifying each observation. The Bayes decision rule for this setup, known as Chow’s rule, is deﬁned by two thresholds on posterior probabilities. From simple desiderata, namely the consistency and the sparsity of the classiﬁer, we derive the double hinge loss function that focuses on estimating conditional probabilities only in the vicinity of the threshold points of the optimal decision rule. We show that, for suitable kernel machines, our approach is universally consistent. We cast the problem of minimizing the double hinge loss as a quadratic program akin to the standard SVM optimization problem and propose an active set method to solve it efﬁciently. We ﬁnally provide preliminary experimental results illustrating the interest of our constructive approach to devising loss functions. 1</p><p>same-paper 2 0.82822162 <a title="34-lda-2" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>Author: Benjamin Yackley, Eduardo Corona, Terran Lane</p><p>Abstract: Many interesting problems, including Bayesian network structure-search, can be cast in terms of ﬁnding the optimum value of a function over the space of graphs. However, this function is often expensive to compute exactly. We here present a method derived from the study of Reproducing Kernel Hilbert Spaces which takes advantage of the regular structure of the space of all graphs on a ﬁxed number of nodes to obtain approximations to the desired function quickly and with reasonable accuracy. We then test this method on both a small testing set and a real-world Bayesian network; the results suggest that not only is this method reasonably accurate, but that the BDe score itself varies quadratically over the space of all graphs. 1</p><p>3 0.80008417 <a title="34-lda-3" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>Author: Ted Sandler, John Blitzer, Partha P. Talukdar, Lyle H. Ungar</p><p>Abstract: For many supervised learning problems, we possess prior knowledge about which features yield similar information about the target variable. In predicting the topic of a document, we might know that two words are synonyms, and when performing image recognition, we know which pixels are adjacent. Such synonymous or neighboring features are near-duplicates and should be expected to have similar weights in an accurate model. Here we present a framework for regularized learning when one has prior knowledge about which features are expected to have similar and dissimilar weights. The prior knowledge is encoded as a network whose vertices are features and whose edges represent similarities and dissimilarities between them. During learning, each feature’s weight is penalized by the amount it differs from the average weight of its neighbors. For text classiﬁcation, regularization using networks of word co-occurrences outperforms manifold learning and compares favorably to other recently proposed semi-supervised learning methods. For sentiment analysis, feature networks constructed from declarative human knowledge signiﬁcantly improve prediction accuracy. 1</p><p>4 0.72687554 <a title="34-lda-4" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>5 0.72672957 <a title="34-lda-5" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>Author: Jeremy Reynolds, Michael C. Mozer</p><p>Abstract: Cognitive control refers to the ﬂexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efﬁciently reﬁned with subsequent task experience. 1</p><p>6 0.72546089 <a title="34-lda-6" href="./nips-2008-Non-parametric_Regression_Between_Manifolds.html">151 nips-2008-Non-parametric Regression Between Manifolds</a></p>
<p>7 0.72473502 <a title="34-lda-7" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>8 0.72210854 <a title="34-lda-8" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>9 0.72138411 <a title="34-lda-9" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>10 0.7201007 <a title="34-lda-10" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>11 0.71929353 <a title="34-lda-11" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>12 0.71890622 <a title="34-lda-12" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>13 0.71841443 <a title="34-lda-13" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>14 0.71833289 <a title="34-lda-14" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>15 0.71800977 <a title="34-lda-15" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>16 0.7179836 <a title="34-lda-16" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>17 0.71778929 <a title="34-lda-17" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>18 0.71758562 <a title="34-lda-18" href="./nips-2008-Supervised_Exponential_Family_Principal_Component_Analysis_via_Convex_Optimization.html">227 nips-2008-Supervised Exponential Family Principal Component Analysis via Convex Optimization</a></p>
<p>19 0.71738988 <a title="34-lda-19" href="./nips-2008-Counting_Solution_Clusters_in_Graph_Coloring_Problems_Using_Belief_Propagation.html">53 nips-2008-Counting Solution Clusters in Graph Coloring Problems Using Belief Propagation</a></p>
<p>20 0.71736741 <a title="34-lda-20" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
