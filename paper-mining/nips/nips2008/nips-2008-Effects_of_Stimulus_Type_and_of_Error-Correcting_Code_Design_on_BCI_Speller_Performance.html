<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-67" href="#">nips2008-67</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</h1>
<br/><p>Source: <a title="nips-2008-67-pdf" href="http://papers.nips.cc/paper/3476-effects-of-stimulus-type-and-of-error-correcting-code-design-on-bci-speller-performance.pdf">pdf</a></p><p>Author: Jeremy Hill, Jason Farquhar, Suzanna Martens, Felix Biessmann, Bernhard Schölkopf</p><p>Abstract: From an information-theoretic perspective, a noisy transmission system such as a visual Brain-Computer Interface (BCI) speller could beneﬁt from the use of errorcorrecting codes. However, optimizing the code solely according to the maximal minimum-Hamming-distance criterion tends to lead to an overall increase in target frequency of target stimuli, and hence a signiﬁcantly reduced average target-to-target interval (TTI), leading to difﬁculties in classifying the individual event-related potentials (ERPs) due to overlap and refractory effects. Clearly any change to the stimulus setup must also respect the possible psychophysiological consequences. Here we report new EEG data from experiments in which we explore stimulus types and codebooks in a within-subject design, ﬁnding an interaction between the two factors. Our data demonstrate that the traditional, rowcolumn code has particular spatial properties that lead to better performance than one would expect from its TTIs and Hamming-distances alone, but nonetheless error-correcting codes can improve performance provided the right stimulus type is used. 1</p><p>Reference: <a title="nips-2008-67-reference" href="../nips2008_reference/nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 nl 3  Dept of Computer Science, TU Berlin, Germany  Abstract From an information-theoretic perspective, a noisy transmission system such as a visual Brain-Computer Interface (BCI) speller could beneﬁt from the use of errorcorrecting codes. [sent-7, score-0.274]
</p><p>2 Clearly any change to the stimulus setup must also respect the possible psychophysiological consequences. [sent-9, score-0.359]
</p><p>3 Here we report new EEG data from experiments in which we explore stimulus types and codebooks in a within-subject design, ﬁnding an interaction between the two factors. [sent-10, score-0.451]
</p><p>4 Our data demonstrate that the traditional, rowcolumn code has particular spatial properties that lead to better performance than one would expect from its TTIs and Hamming-distances alone, but nonetheless error-correcting codes can improve performance provided the right stimulus type is used. [sent-11, score-0.672]
</p><p>5 1  Introduction  The Farwell-Donchin speller [4], also known as the “P300 speller,” is a Brain-Computer Interface which enables users to spell words provided that they can see sufﬁciently well. [sent-12, score-0.234]
</p><p>6 This BCI determines the intent of the user by recording and classifying his electroencephalogram (EEG) in response to controlled stimulus presentations. [sent-13, score-0.356]
</p><p>7 The stimuli are intensiﬁcations of a number of letters which are organized in a grid and displayed on a screen. [sent-15, score-0.145]
</p><p>8 The intensiﬁcation of the row or column containing the letter that the user wants to communicate is a target in a stimulus sequence and induces a different brain response than the intensiﬁcation of the other rows and columns (the non-targets). [sent-17, score-0.689]
</p><p>9 In particular, targets and non-targets are expected to elicit certain event-related potential (ERP) components, such as the so-called P300, to different extents. [sent-18, score-0.136]
</p><p>10 the EEG segments following each stimulus event) into targets and non-targets, the target row and column can be predicted, resulting in the identiﬁcation of the letter of interest. [sent-21, score-0.763]
</p><p>11 The classiﬁcation process in the speller can be considered a noisy communication channel where the sequence of EEG epochs is a modulated version of a bit string denoting the user’s desired letter. [sent-22, score-0.507]
</p><p>12 1  Figure 1: Schematic of the visual speller system, illustrating the relationship between the spatial pattern of ﬂashes and one possible codebook for letter transmission (ﬂash rows then columns). [sent-23, score-0.852]
</p><p>13 These bit strings or codewords form the rows of a binary codebook C, a matrix in which a 1 at position (i, j) means the letter corresponding to row i ﬂashed at time-step j, and a 0 indicates that it did not. [sent-24, score-0.807]
</p><p>14 In practice, the poor signal-to-noise ratio of the ERPs hampers accurate classiﬁcation of the epochs, so the output bit string may differ from the transmitted bit string (decoding error). [sent-28, score-0.233]
</p><p>15 Also, the transmitted string may differ from the corresponding row in the codebook due to modulation error, for example if the user lost his attention and missed a stimulus event. [sent-29, score-0.846]
</p><p>16 Coding theory tells us that we can detect and correct transmission and decoding errors by adding redundancy to the transmitted bit string. [sent-30, score-0.305]
</p><p>17 The minimum Hamming distance dmin of all pairs of codewords is related to the error correcting abilities of the code by e = (dmin − 1)/2, where e is the maximum number of errors that a code can guarantee to correct [9]. [sent-32, score-0.715]
</p><p>18 In general, we ﬁnd the mean Hamming distance within a given codebook to be a rough predictor of that codebook’s performance. [sent-33, score-0.404]
</p><p>19 This leads to d = 4R between two letters not in the same row or column and dmin = 2R between two letters in the same row or column. [sent-35, score-0.577]
</p><p>20 However, the codes with a larger dmin are characterized by an increased weight compared to the RC code, i. [sent-37, score-0.265]
</p><p>21 As target stimulus events occur more frequently overall, the expected target-to-target interval (TTI) decreases. [sent-40, score-0.446]
</p><p>22 One cannot approach codebook optimization, therefore, without asking what effect this might have on the signals we are trying to measure and classify, namely the ERPs in response to the stimulus events. [sent-41, score-0.707]
</p><p>23 The speller was originally derived from an “oddball” paradigm, in which subjects are presented with a repetitive sequence of events, some of which are targets requiring a different response from the (more frequent) non-targets. [sent-42, score-0.451]
</p><p>24 The targets are expected to evoke a larger P300 than the non-targets. [sent-43, score-0.136]
</p><p>25 It was generally accepted that the amplitude of the target P300 decreases when the percentage of targets increases [3, 11]. [sent-44, score-0.24]
</p><p>26 In a different type of paradigm using only targets, it was shown that at TTIs smaller than about 1 second, the P300 amplitude is signiﬁcantly decreased due to refractory effects [15]. [sent-46, score-0.152]
</p><p>27 Typical stimulus onset asynchronies (SOAs) in the oddball paradigm are in the order of seconds since the P300 component shows up somewhere between 200 and 800 msec[12]. [sent-47, score-0.343]
</p><p>28 Consequently, one can expect a signiﬁcant ERP overlap into the epoch following a target epoch, and since row ﬂashes are often randomly mixed in with column ﬂashes, different targets may experience very different TTIs. [sent-49, score-0.4]
</p><p>29 For a 6 × 6 grid, the TTI ranges from 1×SOA to 20×SOA, so targets may suffer to varying degrees from any refractory and overlap effects. [sent-50, score-0.176]
</p><p>30 In order to quantify the detrimental effects of short TTI we examined data from the two subjects in dataset IIa+b from the BCI Competition III[2]. [sent-51, score-0.246]
</p><p>31 3, we estimated classiﬁcation performance on the individual epochs of both data sets by 10fold cross-validation within each subject’s data set. [sent-53, score-0.23]
</p><p>32 Binary (target versus non-target) classiﬁcation results were separated according to the time since the previous target (TPT)—for the targets this distance measure is equivalent to the TTI. [sent-54, score-0.24]
</p><p>33 The left panel of ﬁg 4 shows the average classiﬁcation error as a function of TPT (averaged across both subjects—both subjects show the same qualitative effect). [sent-55, score-0.155]
</p><p>34 5 sec, constituting about 20% of all target epochs in a RC code, do not appear to be useful for transmission [10]. [sent-59, score-0.356]
</p><p>35 Clearly, there is a potential conﬂict between information-theoretic factors, which favour increasing the minimum Hamming distance and hence the overall proportion of target stimuli, and the detrimental psychophysiological effects of doing so. [sent-60, score-0.262]
</p><p>36 We initially built a generative model of the BCI system, using the competition data illustrated in ﬁgure 4, and then used this model to guide the generation and selection of speller code books. [sent-62, score-0.439]
</p><p>37 The results were not unequivocally successful: though we were able to show effects of both TTIs and of the Hamming distances in our codebooks, our optimized codebook performed no better than the row-column code for the standard ﬂash stimulus. [sent-63, score-0.682]
</p><p>38 However, our series of experiments involved another kind of stimulus, and the effect of our codebook manipulation was found to interact with the kind of stimulus used. [sent-64, score-0.707]
</p><p>39 to present new data which ilustrate the stimulus/codebook interaction more clearly, and demonstrate the advantage to be gained by the correct choice of stimulus together with an error-correcting code. [sent-66, score-0.372]
</p><p>40 to present evidence for another effect, which we had not previously considered in modelling our subjects’ responses, which may explain why row-column codes perform better than expected: speciﬁcally, the spatial contiguity of rows and columns. [sent-68, score-0.208]
</p><p>41 1  Decoding Framework Probabilistic Approach to Classiﬁcation and Decoding  We assume an N -letter alphabet Γ and an N -letter by L-bit codebook C. [sent-70, score-0.376]
</p><p>42 The basic demodulation ˆ and decoding procedure consists of ﬁnding the letter T among the possible letters t ∈ Γ showing the largest probability Pr (t|X) of being the target letter T , given C and the measured brain signals X = [x1 , . [sent-71, score-0.647]
</p><p>43 A simple approach to decoding is to treat the individual binary epochs, with binary labels c = (Ct1 . [sent-77, score-0.17]
</p><p>44 This form of Bayesian decoding [5] forms the basis for our decoding scheme. [sent-85, score-0.208]
</p><p>45 As a result, we can obtain estimates of the probability Pr (t|X) that a particular letter t corresponds to the user-selected codeword. [sent-87, score-0.161]
</p><p>46 In fact, it is easy to show that during decoding this term cancels out the effect of the binary prior, which may therefore be set arbitrarily without affecting the decisions made by our decoder. [sent-90, score-0.159]
</p><p>47 1  Codebook Optimization  We used a simple model of subjects’ responses in each epoch in order to estimate the probability of making a prediction error with the above decoding method. [sent-95, score-0.211]
</p><p>48 We used it to compute the codebook loss, which is the sum of error probabilities, weighted by the probability of transmission of each letter. [sent-96, score-0.426]
</p><p>49 We present results from 6 healthy subjects in their 20s and 30s (5 male, 1 female). [sent-108, score-0.125]
</p><p>50 Two factors were compared in a fully within-subject design: codebook and stimulus. [sent-109, score-0.376]
</p><p>51 1  Codebook Comparison  In total, we explored 5 different stimulus codes: 1. [sent-112, score-0.309]
</p><p>52 RCmix : the 12-bit row-column code, with the 12 bits randomly permuted in time (row events mixed up randomly between column events) as in the competition data [2]. [sent-113, score-0.178]
</p><p>53 RC∗ : this code was generated by taking code RCsep and randomizing the assignment between codewords and letters. [sent-117, score-0.488]
</p><p>54 However, if a subject were to have “tunnel vision” and be unable to see any letters other than the target, this would be exactly equivalent to RCsep . [sent-119, score-0.196]
</p><p>55 As we shall see, for the purposes of the speller, our subjects do not have tunnel vision. [sent-120, score-0.159]
</p><p>56 4  code RCmix ×2 RCsep ×2 RC∗ ×2 D10 D8opt  L 24 24 24 24 24  dmin 4 4 4 10 8  E(d) 6. [sent-121, score-0.355]
</p><p>57 E(#11) means the average number of consecutive target letters per codeword, and Pr (1) the proportion of targets. [sent-147, score-0.252]
</p><p>58 D10: a 24-bit code with the largest minimum Hamming distance we could achieve (dmin = 10). [sent-150, score-0.222]
</p><p>59 To make it, our heuristic for codeword selection was to pick the codeword with the largest minimum distance between it and all previously selected codewords. [sent-151, score-0.204]
</p><p>60 A large number of candidate codebooks were generated this way, and the criteria for scoring a completed codebook were (ﬁrst) dmin and (second, to select among a large number of dmin = 10 candidates) the lowest number of consecutive targets. [sent-152, score-0.846]
</p><p>61 D8opt : a 24-bit code optimized according to our model. [sent-154, score-0.226]
</p><p>62 The heuristic for greedy codeword selection was the mean pairwise codebook loss w. [sent-155, score-0.464]
</p><p>63 previously selected codebook entries, and the ﬁnal scoring criterion was our overall codebook loss function. [sent-158, score-0.779]
</p><p>64 2  Stimulus Comparison  Two stimulus conditions were compared. [sent-160, score-0.309]
</p><p>65 In both conditions, stimulus events were repeated with a stimulus onset asynchrony (SOA) of 167 msec, which as close as our hardware could come to recreating the 175-msec SOA of competition III dataset II. [sent-161, score-0.734]
</p><p>66 Flashes: grey letters presented on a black background were ﬂashed in a conventional manner, being intensiﬁed to white for 33 msec (two video frames). [sent-162, score-0.249]
</p><p>67 Flips: each letter was superimposed on a small grey rectangle whose initial orientation was either horizontal or vertical (randomly determined for each letter). [sent-164, score-0.184]
</p><p>68 Instead of the letter ﬂashing, the rectangle ﬂipped its orientation instantaneously by 90◦ . [sent-165, score-0.161]
</p><p>69 Our previous experiments had led us to conclude that many subjects perform signiﬁcantly better with this stimulus, and ﬁnd it more pleasant, than the ﬂash. [sent-167, score-0.125]
</p><p>70 As we shall see, our results from this stimulus condition support this ﬁnding, and indicate a potentially useful interaction between stimulus type and codebook design. [sent-168, score-1.047]
</p><p>71 Each trial began with a red box which indicated to the subject which letter (randomly chosen on each trial) they should attend to—this cue came on for a second, and was removed 1 second before the start of the stimulus sequence. [sent-171, score-0.521]
</p><p>72 Subjects were instructed to count the stimulus events at the target location, and not to blink, move or swallow during the sequence. [sent-172, score-0.446]
</p><p>73 The sequence consisted of L = 72 stimulus events, their spatio-temporal arrangement being determined by one of the ﬁve code conditions. [sent-173, score-0.503]
</p><p>74 Each of the 5 code conditions occurred 4 times per block, the order of their occurrence being randomized. [sent-175, score-0.194]
</p><p>75 For a given block, the stimulus condition was held constant, but the stimulus type was alternated between blocks. [sent-176, score-0.646]
</p><p>76 Thus, in each of the 10 stimulus × code conditions, there were a total of 32 letter presentations or 2304 stimulus events. [sent-178, score-0.973]
</p><p>77 Using the 72-bit codebooks, all subjects were able to spell 5-15 letters with online performance ranging from 90 to 100%. [sent-183, score-0.314]
</p><p>78 The data were then cut into 600-msec (150-sample) epochs time-locked to the stimulus events, and these were downsampled to 25 Hz. [sent-188, score-0.539]
</p><p>79 We varied the number of consecutive epochs of the test letter that the decoder was allowed to use, from the minimum (12 or 24) up to the maximum 72. [sent-196, score-0.422]
</p><p>80 For each epoch of the left-out letter, we also recorded whether the binary classiﬁer correctly classiﬁed the epoch as a target or non-target. [sent-197, score-0.323]
</p><p>81 4  Results and Discussion  Estimates of 36-class letter prediction performance are shown in ﬁgures 2 (averaged across subjects, as a function of codeword length) and 3 (for each individual subject, presenting only the results for 24-bit codewords). [sent-198, score-0.249]
</p><p>82 The performance of the binary classiﬁer on individual epochs is shown in ﬁgure 4. [sent-199, score-0.263]
</p><p>83 flashes  flips  100  100 90  RC* 80  % letters correct  % letters correct  90  RCmix RCsep  70  D10 D8opt  60 50  RC* 80  RCmix RCsep  70  D10 D8opt  60 50  40 12  24  0 16. [sent-200, score-0.5]
</p><p>84 33 50 msec 36 48 60 72 length of code (epochs)  40 12  24  0 16. [sent-202, score-0.301]
</p><p>85 33 50 msec 36 48 60 72 length of code (epochs)  Figure 2: Ofﬂine (leave-one-letter-out) 36-class prediction performance as a function of codeword length (i. [sent-204, score-0.415]
</p><p>86 the number of consecutive epochs of the left-out letter that were used to make a prediction). [sent-206, score-0.422]
</p><p>87 Using the Donchin ﬂash stimulus, the deleterious effects of short TTIs were clear to see: D10 performed far worse than the other codes despite its larger Hamming distances. [sent-209, score-0.2]
</p><p>88 The rightmost column of each plot shows average classiﬁcation accuracy across all epochs (remember that short TTIs are relatively uncommon overall, and therefore downweighted in the average). [sent-212, score-0.306]
</p><p>89 However, the latter effect is not as large or as consistent across subjects as it was in our preliminary study [7]. [sent-214, score-0.147]
</p><p>90 Using the Donchin ﬂash stimulus, our optimized code D8opt performs about as well as traditional RC codes, but does not outperform them. [sent-216, score-0.226]
</p><p>91 Generally, performance using the ﬂip stimulus is better than with the ﬂash stimulus. [sent-218, score-0.309]
</p><p>92 A comparative analysis of the spatial locations of discriminative sources in the two stimulus conditions is beyond the scope of the current short report. [sent-222, score-0.386]
</p><p>93 Despite having identical TTIs and Hamming distances, RC∗ performs consistently worse than RCsep , in both stimulus conditions. [sent-224, score-0.309]
</p><p>94 In summary, we have obtained empirical support for the idea that TTI (ﬁnding #1), Hamming distance (ﬁnding #4) and stimulus type (ﬁnding #3) can all be manipulated to improve performance. [sent-225, score-0.365]
</p><p>95 In the ﬂash stimulus condition, the row-column codes performed better than expected, matching the performance of our optimized code. [sent-227, score-0.445]
</p><p>96 In the ﬂip stimulus condition, TTI effects were greatly reduced, making either D8opt or D10 suitable despite the short TTIs of the latter. [sent-228, score-0.405]
</p><p>97 Overall, best performance was obtained with the ﬂip stimulus, using either of the two errorcorrecting codes, D8opt or D10: this consistently outperforms the traditional row-column ﬂash design and shows that error-correcting code design has an important role to play in BCI speller development. [sent-232, score-0.418]
</p><p>98 As a ﬁnal note, one should remember that a language model can be used to improve performance in speller systems. [sent-233, score-0.213]
</p><p>99 In this case, the codebook optimization problem becomes more complicated than the simpliﬁed setting we examined, because the prior Pr (t) in (2) is no longer ﬂat. [sent-234, score-0.376]
</p><p>100 Ideally, the language model would be adaptive (for example, supplying a predictive prior for each letter based on the previous three) which might mean that the codewords should be reassigned optimally after each letter. [sent-236, score-0.261]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('codebook', 0.376), ('stimulus', 0.309), ('rcsep', 0.268), ('epochs', 0.23), ('rcmix', 0.201), ('code', 0.194), ('speller', 0.19), ('rc', 0.188), ('ash', 0.168), ('letter', 0.161), ('dmin', 0.161), ('pr', 0.16), ('ttis', 0.151), ('tti', 0.148), ('letters', 0.145), ('targets', 0.136), ('subjects', 0.125), ('codebooks', 0.117), ('hamming', 0.117), ('epoch', 0.107), ('codes', 0.104), ('decoding', 0.104), ('codewords', 0.1), ('codeword', 0.088), ('ashes', 0.084), ('intensi', 0.084), ('tpt', 0.084), ('msec', 0.081), ('eeg', 0.079), ('target', 0.076), ('bci', 0.071), ('ctj', 0.067), ('flashes', 0.067), ('flips', 0.067), ('soa', 0.067), ('events', 0.061), ('transmitted', 0.059), ('donchin', 0.059), ('effects', 0.056), ('classi', 0.055), ('competition', 0.055), ('bit', 0.054), ('ip', 0.052), ('subject', 0.051), ('transmission', 0.05), ('erps', 0.05), ('psychophysiological', 0.05), ('er', 0.047), ('row', 0.045), ('spell', 0.044), ('short', 0.04), ('classified', 0.04), ('refractory', 0.04), ('ashed', 0.04), ('rows', 0.038), ('correct', 0.038), ('spatial', 0.037), ('column', 0.036), ('avg', 0.036), ('coles', 0.034), ('erp', 0.034), ('errorcorrecting', 0.034), ('iia', 0.034), ('mgh', 0.034), ('oddball', 0.034), ('soas', 0.034), ('tunnel', 0.034), ('binary', 0.033), ('string', 0.033), ('optimized', 0.032), ('ine', 0.032), ('consecutive', 0.031), ('panel', 0.03), ('contiguity', 0.029), ('ashing', 0.029), ('eog', 0.029), ('distance', 0.028), ('gure', 0.028), ('amplitude', 0.028), ('type', 0.028), ('overall', 0.027), ('bits', 0.026), ('length', 0.026), ('nijmegen', 0.025), ('detrimental', 0.025), ('interaction', 0.025), ('nding', 0.024), ('supplementary', 0.024), ('user', 0.024), ('distances', 0.024), ('non', 0.023), ('classifying', 0.023), ('lr', 0.023), ('repetition', 0.023), ('remember', 0.023), ('grey', 0.023), ('sec', 0.023), ('effect', 0.022), ('cation', 0.022), ('iii', 0.022)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 1.0000002 <a title="67-tfidf-1" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>Author: Jeremy Hill, Jason Farquhar, Suzanna Martens, Felix Biessmann, Bernhard Schölkopf</p><p>Abstract: From an information-theoretic perspective, a noisy transmission system such as a visual Brain-Computer Interface (BCI) speller could beneﬁt from the use of errorcorrecting codes. However, optimizing the code solely according to the maximal minimum-Hamming-distance criterion tends to lead to an overall increase in target frequency of target stimuli, and hence a signiﬁcantly reduced average target-to-target interval (TTI), leading to difﬁculties in classifying the individual event-related potentials (ERPs) due to overlap and refractory effects. Clearly any change to the stimulus setup must also respect the possible psychophysiological consequences. Here we report new EEG data from experiments in which we explore stimulus types and codebooks in a within-subject design, ﬁnding an interaction between the two factors. Our data demonstrate that the traditional, rowcolumn code has particular spatial properties that lead to better performance than one would expect from its TTIs and Hamming-distances alone, but nonetheless error-correcting codes can improve performance provided the right stimulus type is used. 1</p><p>2 0.14324574 <a title="67-tfidf-2" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>Author: Dominik Endres, Peter Foldiak</p><p>Abstract: We propose a novel application of Formal Concept Analysis (FCA) to neural decoding: instead of just trying to ﬁgure out which stimulus was presented, we demonstrate how to explore the semantic relationships in the neural representation of large sets of stimuli. FCA provides a way of displaying and interpreting such relationships via concept lattices. We explore the effects of neural code sparsity on the lattice. We then analyze neurophysiological data from high-level visual cortical area STSa, using an exact Bayesian approach to construct the formal context needed by FCA. Prominent features of the resulting concept lattices are discussed, including hierarchical face representation and indications for a product-of-experts code in real neurons. 1</p><p>3 0.11699273 <a title="67-tfidf-3" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>Author: Jeremy Reynolds, Michael C. Mozer</p><p>Abstract: Cognitive control refers to the ﬂexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efﬁciently reﬁned with subsequent task experience. 1</p><p>4 0.11001711 <a title="67-tfidf-4" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>Author: Yair Weiss, Antonio Torralba, Rob Fergus</p><p>Abstract: Semantic hashing[1] seeks compact binary codes of data-points so that the Hamming distance between codewords correlates with semantic similarity. In this paper, we show that the problem of ﬁnding a best code for a given dataset is closely related to the problem of graph partitioning and can be shown to be NP hard. By relaxing the original problem, we obtain a spectral method whose solutions are simply a subset of thresholded eigenvectors of the graph Laplacian. By utilizing recent results on convergence of graph Laplacian eigenvectors to the Laplace-Beltrami eigenfunctions of manifolds, we show how to eﬃciently calculate the code of a novel datapoint. Taken together, both learning the code and applying it to a novel point are extremely simple. Our experiments show that our codes outperform the state-of-the art. 1</p><p>5 0.10282953 <a title="67-tfidf-5" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>Author: Angela J. Yu, Jonathan D. Cohen</p><p>Abstract: In a variety of behavioral tasks, subjects exhibit an automatic and apparently suboptimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no real predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reﬂect the inadvertent engagement of mechanisms critical for adapting to a changing environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential ﬁltering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential ﬁlter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that parameter-tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities. 1</p><p>6 0.096483544 <a title="67-tfidf-6" href="./nips-2008-Playing_Pinball_with_non-invasive_BCI.html">180 nips-2008-Playing Pinball with non-invasive BCI</a></p>
<p>7 0.081764124 <a title="67-tfidf-7" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>8 0.07836657 <a title="67-tfidf-8" href="./nips-2008-Understanding_Brain_Connectivity_Patterns_during_Motor_Imagery_for_Brain-Computer_Interfacing.html">243 nips-2008-Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing</a></p>
<p>9 0.069853105 <a title="67-tfidf-9" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>10 0.065688945 <a title="67-tfidf-10" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>11 0.062073871 <a title="67-tfidf-11" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>12 0.058074269 <a title="67-tfidf-12" href="./nips-2008-A_computational_model_of_hippocampal_function_in_trace_conditioning.html">7 nips-2008-A computational model of hippocampal function in trace conditioning</a></p>
<p>13 0.055761158 <a title="67-tfidf-13" href="./nips-2008-Characterizing_response_behavior_in_multisensory_perception_with_conflicting_cues.html">46 nips-2008-Characterizing response behavior in multisensory perception with conflicting cues</a></p>
<p>14 0.051590808 <a title="67-tfidf-14" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>15 0.048368432 <a title="67-tfidf-15" href="./nips-2008-On_the_Reliability_of_Clustering_Stability_in_the_Large_Sample_Regime.html">165 nips-2008-On the Reliability of Clustering Stability in the Large Sample Regime</a></p>
<p>16 0.046679471 <a title="67-tfidf-16" href="./nips-2008-On_Computational_Power_and_the_Order-Chaos_Phase_Transition_in_Reservoir_Computing.html">160 nips-2008-On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing</a></p>
<p>17 0.046502713 <a title="67-tfidf-17" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>18 0.045646545 <a title="67-tfidf-18" href="./nips-2008-Estimating_vector_fields_using_sparse_basis_field_expansions.html">75 nips-2008-Estimating vector fields using sparse basis field expansions</a></p>
<p>19 0.043487363 <a title="67-tfidf-19" href="./nips-2008-Transfer_Learning_by_Distribution_Matching_for_Targeted_Advertising.html">241 nips-2008-Transfer Learning by Distribution Matching for Targeted Advertising</a></p>
<p>20 0.04280287 <a title="67-tfidf-20" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.142), (1, 0.014), (2, 0.086), (3, 0.023), (4, -0.021), (5, 0.036), (6, -0.031), (7, 0.028), (8, 0.135), (9, 0.05), (10, -0.018), (11, 0.116), (12, -0.154), (13, 0.069), (14, -0.062), (15, 0.133), (16, 0.075), (17, -0.002), (18, 0.005), (19, -0.106), (20, 0.006), (21, -0.004), (22, 0.092), (23, -0.042), (24, -0.102), (25, 0.033), (26, -0.034), (27, 0.094), (28, -0.038), (29, -0.051), (30, -0.034), (31, -0.149), (32, 0.141), (33, -0.148), (34, 0.025), (35, -0.066), (36, 0.032), (37, -0.037), (38, 0.044), (39, 0.01), (40, -0.09), (41, 0.176), (42, -0.025), (43, -0.106), (44, 0.052), (45, -0.01), (46, -0.102), (47, -0.049), (48, -0.032), (49, 0.053)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96003562 <a title="67-lsi-1" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>Author: Jeremy Hill, Jason Farquhar, Suzanna Martens, Felix Biessmann, Bernhard Schölkopf</p><p>Abstract: From an information-theoretic perspective, a noisy transmission system such as a visual Brain-Computer Interface (BCI) speller could beneﬁt from the use of errorcorrecting codes. However, optimizing the code solely according to the maximal minimum-Hamming-distance criterion tends to lead to an overall increase in target frequency of target stimuli, and hence a signiﬁcantly reduced average target-to-target interval (TTI), leading to difﬁculties in classifying the individual event-related potentials (ERPs) due to overlap and refractory effects. Clearly any change to the stimulus setup must also respect the possible psychophysiological consequences. Here we report new EEG data from experiments in which we explore stimulus types and codebooks in a within-subject design, ﬁnding an interaction between the two factors. Our data demonstrate that the traditional, rowcolumn code has particular spatial properties that lead to better performance than one would expect from its TTIs and Hamming-distances alone, but nonetheless error-correcting codes can improve performance provided the right stimulus type is used. 1</p><p>2 0.63294435 <a title="67-lsi-2" href="./nips-2008-Interpreting_the_neural_code_with_Formal_Concept_Analysis.html">109 nips-2008-Interpreting the neural code with Formal Concept Analysis</a></p>
<p>Author: Dominik Endres, Peter Foldiak</p><p>Abstract: We propose a novel application of Formal Concept Analysis (FCA) to neural decoding: instead of just trying to ﬁgure out which stimulus was presented, we demonstrate how to explore the semantic relationships in the neural representation of large sets of stimuli. FCA provides a way of displaying and interpreting such relationships via concept lattices. We explore the effects of neural code sparsity on the lattice. We then analyze neurophysiological data from high-level visual cortical area STSa, using an exact Bayesian approach to construct the formal context needed by FCA. Prominent features of the resulting concept lattices are discussed, including hierarchical face representation and indications for a product-of-experts code in real neurons. 1</p><p>3 0.58668017 <a title="67-lsi-3" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>Author: Jeremy Reynolds, Michael C. Mozer</p><p>Abstract: Cognitive control refers to the ﬂexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efﬁciently reﬁned with subsequent task experience. 1</p><p>4 0.53440684 <a title="67-lsi-4" href="./nips-2008-Playing_Pinball_with_non-invasive_BCI.html">180 nips-2008-Playing Pinball with non-invasive BCI</a></p>
<p>Author: Matthias Krauledat, Konrad Grzeska, Max Sagebaum, Benjamin Blankertz, Carmen Vidaurre, Klaus-Robert Müller, Michael Schröder</p><p>Abstract: Compared to invasive Brain-Computer Interfaces (BCI), non-invasive BCI systems based on Electroencephalogram (EEG) signals have not been applied successfully for precisely timed control tasks. In the present study, however, we demonstrate and report on the interaction of subjects with a real device: a pinball machine. Results of this study clearly show that fast and well-timed control well beyond chance level is possible, even though the environment is extremely rich and requires precisely timed and complex predictive behavior. Using machine learning methods for mental state decoding, BCI-based pinball control is possible within the ﬁrst session without the necessity to employ lengthy subject training. The current study shows clearly that very compelling control with excellent timing and dynamics is possible for a non-invasive BCI. 1</p><p>5 0.52374864 <a title="67-lsi-5" href="./nips-2008-Optimal_Response_Initiation%3A_Why_Recent_Experience_Matters.html">172 nips-2008-Optimal Response Initiation: Why Recent Experience Matters</a></p>
<p>Author: Matt Jones, Sachiko Kinoshita, Michael C. Mozer</p><p>Abstract: In most cognitive and motor tasks, speed-accuracy tradeoffs are observed: Individuals can respond slowly and accurately, or quickly yet be prone to errors. Control mechanisms governing the initiation of behavioral responses are sensitive not only to task instructions and the stimulus being processed, but also to the recent stimulus history. When stimuli can be characterized on an easy-hard dimension (e.g., word frequency in a naming task), items preceded by easy trials are responded to more quickly, and with more errors, than items preceded by hard trials. We propose a rationally motivated mathematical model of this sequential adaptation of control, based on a diffusion model of the decision process in which difﬁculty corresponds to the drift rate for the correct response. The model assumes that responding is based on the posterior distribution over which response is correct, conditioned on the accumulated evidence. We derive this posterior as a function of the drift rate, and show that higher estimates of the drift rate lead to (normatively) faster responding. Trial-by-trial tracking of difﬁculty thus leads to sequential effects in speed and accuracy. Simulations show the model explains a variety of phenomena in human speeded decision making. We argue this passive statistical mechanism provides a more elegant and parsimonious account than extant theories based on elaborate control structures. 1</p><p>6 0.51094598 <a title="67-lsi-6" href="./nips-2008-Spectral_Hashing.html">219 nips-2008-Spectral Hashing</a></p>
<p>7 0.49636096 <a title="67-lsi-7" href="./nips-2008-A_computational_model_of_hippocampal_function_in_trace_conditioning.html">7 nips-2008-A computational model of hippocampal function in trace conditioning</a></p>
<p>8 0.49225888 <a title="67-lsi-8" href="./nips-2008-How_memory_biases_affect_information_transmission%3A_A_rational_analysis_of_serial_reproduction.html">100 nips-2008-How memory biases affect information transmission: A rational analysis of serial reproduction</a></p>
<p>9 0.47352567 <a title="67-lsi-9" href="./nips-2008-Tracking_Changing_Stimuli_in_Continuous_Attractor_Neural_Networks.html">240 nips-2008-Tracking Changing Stimuli in Continuous Attractor Neural Networks</a></p>
<p>10 0.44839042 <a title="67-lsi-10" href="./nips-2008-A_general_framework_for_investigating_how_far_the_decoding_process_in_the_brain_can_be_simplified.html">8 nips-2008-A general framework for investigating how far the decoding process in the brain can be simplified</a></p>
<p>11 0.38859516 <a title="67-lsi-11" href="./nips-2008-Psychiatry%3A_Insights_into_depression_through_normative_decision-making_models.html">187 nips-2008-Psychiatry: Insights into depression through normative decision-making models</a></p>
<p>12 0.37938374 <a title="67-lsi-12" href="./nips-2008-Designing_neurophysiology_experiments_to_optimally_constrain_receptive_field_models_along_parametric_submanifolds.html">60 nips-2008-Designing neurophysiology experiments to optimally constrain receptive field models along parametric submanifolds</a></p>
<p>13 0.36285889 <a title="67-lsi-13" href="./nips-2008-Load_and_Attentional_Bayes.html">124 nips-2008-Load and Attentional Bayes</a></p>
<p>14 0.36146349 <a title="67-lsi-14" href="./nips-2008-Kernel-ARMA_for_Hand_Tracking_and_Brain-Machine_interfacing_During_3D_Motor_Control.html">110 nips-2008-Kernel-ARMA for Hand Tracking and Brain-Machine interfacing During 3D Motor Control</a></p>
<p>15 0.346468 <a title="67-lsi-15" href="./nips-2008-Understanding_Brain_Connectivity_Patterns_during_Motor_Imagery_for_Brain-Computer_Interfacing.html">243 nips-2008-Understanding Brain Connectivity Patterns during Motor Imagery for Brain-Computer Interfacing</a></p>
<p>16 0.3414166 <a title="67-lsi-16" href="./nips-2008-Stress%2C_noradrenaline%2C_and_realistic_prediction_of_mouse_behaviour_using_reinforcement_learning.html">222 nips-2008-Stress, noradrenaline, and realistic prediction of mouse behaviour using reinforcement learning</a></p>
<p>17 0.33744884 <a title="67-lsi-17" href="./nips-2008-Sequential_effects%3A_Superstition_or_rational_behavior%3F.html">206 nips-2008-Sequential effects: Superstition or rational behavior?</a></p>
<p>18 0.33361998 <a title="67-lsi-18" href="./nips-2008-Breaking_Audio_CAPTCHAs.html">41 nips-2008-Breaking Audio CAPTCHAs</a></p>
<p>19 0.32585555 <a title="67-lsi-19" href="./nips-2008-On_Computational_Power_and_the_Order-Chaos_Phase_Transition_in_Reservoir_Computing.html">160 nips-2008-On Computational Power and the Order-Chaos Phase Transition in Reservoir Computing</a></p>
<p>20 0.32198113 <a title="67-lsi-20" href="./nips-2008-Beyond_Novelty_Detection%3A_Incongruent_Events%2C_when_General_and_Specific_Classifiers_Disagree.html">36 nips-2008-Beyond Novelty Detection: Incongruent Events, when General and Specific Classifiers Disagree</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.055), (7, 0.084), (12, 0.038), (15, 0.013), (28, 0.164), (57, 0.043), (59, 0.014), (63, 0.027), (71, 0.017), (77, 0.039), (81, 0.019), (83, 0.054), (91, 0.342)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77733576 <a title="67-lda-1" href="./nips-2008-Effects_of_Stimulus_Type_and_of_Error-Correcting_Code_Design_on_BCI_Speller_Performance.html">67 nips-2008-Effects of Stimulus Type and of Error-Correcting Code Design on BCI Speller Performance</a></p>
<p>Author: Jeremy Hill, Jason Farquhar, Suzanna Martens, Felix Biessmann, Bernhard Schölkopf</p><p>Abstract: From an information-theoretic perspective, a noisy transmission system such as a visual Brain-Computer Interface (BCI) speller could beneﬁt from the use of errorcorrecting codes. However, optimizing the code solely according to the maximal minimum-Hamming-distance criterion tends to lead to an overall increase in target frequency of target stimuli, and hence a signiﬁcantly reduced average target-to-target interval (TTI), leading to difﬁculties in classifying the individual event-related potentials (ERPs) due to overlap and refractory effects. Clearly any change to the stimulus setup must also respect the possible psychophysiological consequences. Here we report new EEG data from experiments in which we explore stimulus types and codebooks in a within-subject design, ﬁnding an interaction between the two factors. Our data demonstrate that the traditional, rowcolumn code has particular spatial properties that lead to better performance than one would expect from its TTIs and Hamming-distances alone, but nonetheless error-correcting codes can improve performance provided the right stimulus type is used. 1</p><p>2 0.65354592 <a title="67-lda-2" href="./nips-2008-Semi-supervised_Learning_with_Weakly-Related_Unlabeled_Data_%3A_Towards_Better_Text_Categorization.html">205 nips-2008-Semi-supervised Learning with Weakly-Related Unlabeled Data : Towards Better Text Categorization</a></p>
<p>Author: Liu Yang, Rong Jin, Rahul Sukthankar</p><p>Abstract: The cluster assumption is exploited by most semi-supervised learning (SSL) methods. However, if the unlabeled data is merely weakly related to the target classes, it becomes questionable whether driving the decision boundary to the low density regions of the unlabeled data will help the classiﬁcation. In such case, the cluster assumption may not be valid; and consequently how to leverage this type of unlabeled data to enhance the classiﬁcation accuracy becomes a challenge. We introduce “Semi-supervised Learning with Weakly-Related Unlabeled Data” (SSLW), an inductive method that builds upon the maximum-margin approach, towards a better usage of weakly-related unlabeled information. Although the SSLW could improve a wide range of classiﬁcation tasks, in this paper, we focus on text categorization with a small training pool. The key assumption behind this work is that, even with different topics, the word usage patterns across different corpora tends to be consistent. To this end, SSLW estimates the optimal wordcorrelation matrix that is consistent with both the co-occurrence information derived from the weakly-related unlabeled documents and the labeled documents. For empirical evaluation, we present a direct comparison with a number of stateof-the-art methods for inductive semi-supervised learning and text categorization. We show that SSLW results in a signiﬁcant improvement in categorization accuracy, equipped with a small training set and an unlabeled resource that is weakly related to the test domain.</p><p>3 0.64972681 <a title="67-lda-3" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>Author: Ydo Wexler, Christopher Meek</p><p>Abstract: We propose a multiplicative approximation scheme (MAS) for inference problems in graphical models, which can be applied to various inference algorithms. The method uses -decompositions which decompose functions used throughout the inference procedure into functions over smaller sets of variables with a known error . MAS translates these local approximations into bounds on the accuracy of the results. We show how to optimize -decompositions and provide a fast closed-form solution for an L2 approximation. Applying MAS to the Variable Elimination inference algorithm, we introduce an algorithm we call DynaDecomp which is extremely fast in practice and provides guaranteed error bounds on the result. The superior accuracy and efﬁciency of DynaDecomp is demonstrated. 1</p><p>4 0.52050442 <a title="67-lda-4" href="./nips-2008-Exploring_Large_Feature_Spaces_with_Hierarchical_Multiple_Kernel_Learning.html">79 nips-2008-Exploring Large Feature Spaces with Hierarchical Multiple Kernel Learning</a></p>
<p>Author: Francis R. Bach</p><p>Abstract: For supervised and unsupervised learning, positive deﬁnite kernels allow to use large and potentially inﬁnite dimensional feature spaces with a computational cost that only depends on the number of observations. This is usually done through the penalization of predictor functions by Euclidean or Hilbertian norms. In this paper, we explore penalizing by sparsity-inducing norms such as the ℓ1 -norm or the block ℓ1 -norm. We assume that the kernel decomposes into a large sum of individual basis kernels which can be embedded in a directed acyclic graph; we show that it is then possible to perform kernel selection through a hierarchical multiple kernel learning framework, in polynomial time in the number of selected kernels. This framework is naturally applied to non linear variable selection; our extensive simulations on synthetic datasets and datasets from the UCI repository show that efﬁciently exploring the large feature space through sparsity-inducing norms leads to state-of-the-art predictive performance.</p><p>5 0.51977956 <a title="67-lda-5" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>Author: Garvesh Raskutti, Bin Yu, Martin J. Wainwright, Pradeep K. Ravikumar</p><p>Abstract: We consider the problem of estimating the graph structure associated with a Gaussian Markov random ﬁeld (GMRF) from i.i.d. samples. We study the performance of study the performance of the ℓ1 -regularized maximum likelihood estimator in the high-dimensional setting, where the number of nodes in the graph p, the number of edges in the graph s and the maximum node degree d, are allowed to grow as a function of the number of samples n. Our main result provides sufﬁcient conditions on (n, p, d) for the ℓ1 -regularized MLE estimator to recover all the edges of the graph with high probability. Under some conditions on the model covariance, we show that model selection can be achieved for sample sizes n = Ω(d2 log(p)), with the error decaying as O(exp(−c log(p))) for some constant c. We illustrate our theoretical results via simulations and show good correspondences between the theoretical predictions and behavior in simulations.</p><p>6 0.51768857 <a title="67-lda-6" href="./nips-2008-Regularized_Learning_with_Networks_of_Features.html">194 nips-2008-Regularized Learning with Networks of Features</a></p>
<p>7 0.51761854 <a title="67-lda-7" href="./nips-2008-Supervised_Dictionary_Learning.html">226 nips-2008-Supervised Dictionary Learning</a></p>
<p>8 0.51611066 <a title="67-lda-8" href="./nips-2008-Dimensionality_Reduction_for_Data_in_Multiple_Feature_Representations.html">63 nips-2008-Dimensionality Reduction for Data in Multiple Feature Representations</a></p>
<p>9 0.51583141 <a title="67-lda-9" href="./nips-2008-Differentiable_Sparse_Coding.html">62 nips-2008-Differentiable Sparse Coding</a></p>
<p>10 0.51567703 <a title="67-lda-10" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>11 0.51537973 <a title="67-lda-11" href="./nips-2008-Relative_Margin_Machines.html">196 nips-2008-Relative Margin Machines</a></p>
<p>12 0.51482081 <a title="67-lda-12" href="./nips-2008-An_Homotopy_Algorithm_for_the_Lasso_with_Online_Observations.html">21 nips-2008-An Homotopy Algorithm for the Lasso with Online Observations</a></p>
<p>13 0.51442999 <a title="67-lda-13" href="./nips-2008-Robust_Near-Isometric_Matching_via_Structured_Learning_of_Graphical_Models.html">201 nips-2008-Robust Near-Isometric Matching via Structured Learning of Graphical Models</a></p>
<p>14 0.51392275 <a title="67-lda-14" href="./nips-2008-PSDBoost%3A_Matrix-Generation_Linear_Programming_for_Positive_Semidefinite_Matrices_Learning.html">175 nips-2008-PSDBoost: Matrix-Generation Linear Programming for Positive Semidefinite Matrices Learning</a></p>
<p>15 0.51368743 <a title="67-lda-15" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>16 0.51347816 <a title="67-lda-16" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>17 0.51346219 <a title="67-lda-17" href="./nips-2008-Unlabeled_data%3A_Now_it_helps%2C_now_it_doesn%27t.html">245 nips-2008-Unlabeled data: Now it helps, now it doesn't</a></p>
<p>18 0.51295954 <a title="67-lda-18" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>19 0.51244247 <a title="67-lda-19" href="./nips-2008-Phase_transitions_for_high-dimensional_joint_support_recovery.html">179 nips-2008-Phase transitions for high-dimensional joint support recovery</a></p>
<p>20 0.51241308 <a title="67-lda-20" href="./nips-2008-Adaptive_Forward-Backward_Greedy_Algorithm_for_Sparse_Learning_with_Linear_Models.html">14 nips-2008-Adaptive Forward-Backward Greedy Algorithm for Sparse Learning with Linear Models</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
