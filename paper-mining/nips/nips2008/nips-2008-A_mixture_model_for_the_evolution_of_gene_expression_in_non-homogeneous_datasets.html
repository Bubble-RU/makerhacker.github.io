<!DOCTYPE html>
<html>
<head>
<meta charset=utf-8>
<title>9 nips-2008-A mixture model for the evolution of gene expression in non-homogeneous datasets</title>
</head>

<body>
<p><a title="nips" href="../nips_home.html">nips</a> <a title="nips-2008" href="../home/nips2008_home.html">nips2008</a> <a title="nips-2008-9" href="#">nips2008-9</a> knowledge-graph by maker-knowledge-mining</p><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<!-- maker adsense -->
<ins class="adsbygoogle"
     style="display:inline-block;width:728px;height:90px"
     data-ad-client="ca-pub-5027806277543591"
     data-ad-slot="4192012269"></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
<h1>9 nips-2008-A mixture model for the evolution of gene expression in non-homogeneous datasets</h1>
<br/><p>Source: <a title="nips-2008-9-pdf" href="http://papers.nips.cc/paper/3384-a-mixture-model-for-the-evolution-of-gene-expression-in-non-homogeneous-datasets.pdf">pdf</a></p><p>Author: Gerald Quon, Yee W. Teh, Esther Chan, Timothy Hughes, Michael Brudno, Quaid D. Morris</p><p>Abstract: We address the challenge of assessing conservation of gene expression in complex, non-homogeneous datasets. Recent studies have demonstrated the success of probabilistic models in studying the evolution of gene expression in simple eukaryotic organisms such as yeast, for which measurements are typically scalar and independent. Models capable of studying expression evolution in much more complex organisms such as vertebrates are particularly important given the medical and scientiﬁc interest in species such as human and mouse. We present Brownian Factor Phylogenetic Analysis, a statistical model that makes a number of signiﬁcant extensions to previous models to enable characterization of changes in expression among highly complex organisms. We demonstrate the efﬁcacy of our method on a microarray dataset proﬁling diverse tissues from multiple vertebrate species. We anticipate that the model will be invaluable in the study of gene expression patterns in other diverse organisms as well, such as worms and insects. 1</p><p>Reference: <a title="nips-2008-9-reference" href="../nips2008_reference/nips-2008-A_mixture_model_for_the_evolution_of_gene_expression_in_non-homogeneous_datasets_reference.html">text</a></p><br/><h2>Summary: the most important sentenses genereted by tfidf model</h2><p>sentIndex sentText sentNum sentScore</p><p>1 ca  Abstract We address the challenge of assessing conservation of gene expression in complex, non-homogeneous datasets. [sent-4, score-0.937]
</p><p>2 Recent studies have demonstrated the success of probabilistic models in studying the evolution of gene expression in simple eukaryotic organisms such as yeast, for which measurements are typically scalar and independent. [sent-5, score-1.001]
</p><p>3 Models capable of studying expression evolution in much more complex organisms such as vertebrates are particularly important given the medical and scientiﬁc interest in species such as human and mouse. [sent-6, score-1.036]
</p><p>4 We present Brownian Factor Phylogenetic Analysis, a statistical model that makes a number of signiﬁcant extensions to previous models to enable characterization of changes in expression among highly complex organisms. [sent-7, score-0.299]
</p><p>5 We anticipate that the model will be invaluable in the study of gene expression patterns in other diverse organisms as well, such as worms and insects. [sent-9, score-0.982]
</p><p>6 1  Introduction  High-throughput functional data is emerging as an indispensible resource for generating a complete picture of genome-wide gene and protein function. [sent-10, score-0.454]
</p><p>7 Currently, gene function is often inferred through sequence comparisons with genes of known function in other species, though sequence similarity is no guarantee of shared biological function. [sent-11, score-0.683]
</p><p>8 Gene duplication, one of the primary forces of genomic evolution, often gives rise to genes with high sequence similarity but distinct biological roles [1]. [sent-12, score-0.255]
</p><p>9 Differences in temporal and spatial gene expression patterns have also been posited to explain phenotypic differences among animals despite a surprisingly large degree of gene sequence similarity [2]. [sent-13, score-1.186]
</p><p>10 Comparing gene expression patterns between distantly related multi-cellular organisms is challenging because it is difﬁcult to collect a wide range of functionally matching tissue samples. [sent-15, score-0.978]
</p><p>11 Matching samples can also be hard to collect because anatomical arrangements of some of the queried organisms make isolation of speciﬁc tissues virtually impossible. [sent-18, score-0.364]
</p><p>12 By allowing tissue samples to be mixed and heterogeneous, though functionally related, it 1  becomes possible to compare expression patterns describing a much larger range of functions across a much larger range of organisms. [sent-20, score-0.442]
</p><p>13 Current detailed statistical models of expression data assume measurements from matched samples in each organism. [sent-21, score-0.377]
</p><p>14 As such, comparative studies of gene expression to date have either resorted to simple, non-phylogenetic measures to compare expression patterns [4], or restricted their comparisons to single-cellular organisms [5] or clearly homologous tissues in mammals [6]. [sent-22, score-1.464]
</p><p>15 Here, we present Brownian Factor Phylogenetic Analysis (BFPA), a new model of gene expression evolution that removes the earlier limitations of matched samples, therefore allowing detailed comparisons of expression patterns from the widely diverged multi-cellular organisms. [sent-23, score-1.212]
</p><p>16 We model the expression data from related organisms using a mixture of Gaussians model related to a mixture of constrained factor analyzers [7]. [sent-25, score-0.487]
</p><p>17 In our model, each mixture component represents a different pattern of conservation and divergence of gene expression along each link of the phylogenetic tree. [sent-26, score-1.125]
</p><p>18 2  Previous work  Recent evolutionary models of gene expression treat it as a quantitative (i. [sent-29, score-0.804]
</p><p>19 real-valued) trait and model evolutionary change in expression levels as a Brownian motion process [8, 9]. [sent-31, score-0.541]
</p><p>20 The leaves of the phylogeny are associated with present-day species and the internal branch points with shared ancestors. [sent-34, score-0.694]
</p><p>21 The exact position of the root of the phylogeny (not shown in the ﬁgure, but somewhere along branch ”T”) cannot be established without additional information, and the outgroup species ”T” is often used in place of the root of the tree. [sent-35, score-0.842]
</p><p>22 Figure 1b, whose nodes are variables representing expression levels in the corresponding species and whose directed edges point from immediate ancestors to their children species. [sent-38, score-0.862]
</p><p>23 Typical uses of these evolutionary models are to compare different hypotheses about divergence times [8] or the structure of the phylogeny [9] by calculating the likelihood of the present-day expression levels under various hypotheses. [sent-40, score-0.627]
</p><p>24 We use xi s to indicate the hidden expression proﬁle of the i-th gene (out of N ortholog groups) in species s. [sent-43, score-1.255]
</p><p>25 2  ρ Λ  Σ  Λ  ρΣ  β  Figure 1: Our statistical model and associated species phylogenies. [sent-44, score-0.492]
</p><p>26 (a) The phylogeny of the species measured in our dataset of human (H), mouse (M), chicken (C), frog (F), and tetraodon (T), as well as an example phylogeny of three hypothetical species x1 , x2 , and x3 used to illustrate our model. [sent-45, score-1.436]
</p><p>27 (b) Our statistical model showing how the outgroup species x3 and its corresponding observed expression levels x3 is used as a gene expression prior. [sent-46, score-1.624]
</p><p>28 Edge weights on the graph depict scaling ˆ factors applied to the variance terms Σ, which are speciﬁed by each conservation pattern c. [sent-47, score-0.272]
</p><p>29 This particular conservation pattern represents a phylogeny where all species have conserved expression. [sent-49, score-1.017]
</p><p>30 (c) The same model except applied to a conservation pattern where species x3 is determined to exhibit signiﬁcantly different expression levels (rapid change). [sent-51, score-1.133]
</p><p>31 The input to our model are vectors of tissue-speciﬁc expression levels {ˆi }N for N genes over xs i=1 present-day species s ∈ {P ∪ o}; we distinguish the chosen outgroup species o from the rest of R the present-day species P . [sent-52, score-2.191]
</p><p>32 xi ∈ I ds , where ds is the number of tissues in species s. [sent-53, score-0.72]
</p><p>33 The goal of ˆs our model is to infer each gene’s corresponding pattern of gene expression evolution (conservation pattern) {ci }N and latent expression levels {xi }N for all species s ∈ {P ∪ o ∪ A}, where A s i=1 i=1 represents the internal ancestral species in the phylogenetic tree (Figure 1). [sent-54, score-2.399]
</p><p>34 First, we constrain all variances Σs to be diagonal in order to estimate tissue-speciﬁc drift rates, as tissues are known to vary widely in expression divergence rates [12]. [sent-58, score-0.522]
</p><p>35 Secondly, we note that in studying a diverse lineage such as vertebrates, we expect to see large changes in expression for genes that have diverged in function, as compared to genes of conserved function. [sent-59, score-0.963]
</p><p>36 We therefore model the drift of a gene’s expression levels along each branch of the tree as following one of two rates: a slow rate, reﬂecting a functional constraint, and a fast rate, reﬂecting neutral or selected change. [sent-60, score-0.543]
</p><p>37 Correspondingly, for each branch of the phylogenetic tree above the species s, we deﬁne two rate parameters, ρ2 or ρ1 , termed a short and long branch respectively (ρ2 < ρ1 ). [sent-61, score-0.787]
</p><p>38 We model tissues of child species as linear combinations of ancestral tissues. [sent-66, score-0.769]
</p><p>39 The matrix of coefﬁcients Λs that relate expression levels in the child species’ tissues to that of its parent species is heavily constrained to leverage our prior understanding of the relationships of speciﬁc tissues [14]. [sent-67, score-1.292]
</p><p>40 Equation 3 relates the observed expression levels of present-day species to the noiseless, inferred expression levels of the corresponding hidden nodes of each observed species. [sent-76, score-1.212]
</p><p>41 Our goal is to identify different types of expression evolution, including punctuated evolution, fully conserved expression, or rapid change along all branches of the phylogeny. [sent-79, score-0.489]
</p><p>42 We model the problem as a mixture model of conservation patterns, in which each conservation pattern speciﬁes either constrained or fast change along each branch of the tree. [sent-80, score-0.664]
</p><p>43 Each conservation pattern Kj ∈ {1, 2}|P ∪A| speciﬁes a conﬁguration of ρ1 or ρ2 for each species s s s K (Kj,s ∈ {1, 2} speciﬁes ρs j,s ). [sent-81, score-0.746]
</p><p>44 As a post-processing step, we consider short branches in those cases to be long, and sum over such ambiguous trees, leaving a total of J possible conservation patterns. [sent-84, score-0.263]
</p><p>45 First, note that in traditional Brownian motion models, the location of the root is arbitrary if one assumes a constant, improper prior over the root expression levels, since any choice of root would give rise to the same probability distribution over the expression levels. [sent-87, score-0.759]
</p><p>46 By using a present-day species with observed expression levels as the root node, we avoid integrating over this improper prior. [sent-88, score-0.91]
</p><p>47 Because the root node prior is constant, the likelihood of the other present-day species conditioned on this present-day root expression level is a constant times the likelihood of all present-day species expression levels. [sent-89, score-1.604]
</p><p>48 6  Results  We present the results of applying our model to a novel dataset consisting of gene expression measurements of 4770 genes with unique, unambiguous orthology, i. [sent-91, score-1.029]
</p><p>49 , each of the 4770 genes is present in only a single copy, across the following ﬁve present-day organisms: human, mouse, chicken, frog, and tetraodon. [sent-93, score-0.272]
</p><p>50 The phylogeny related these species is shown in Figure 1 with nodes labelled by the ﬁrst letter of the species name. [sent-94, score-1.089]
</p><p>51 Replicate microarray probe intensity measurements were taken for the 4770 genes across a total of 161 tissues (i. [sent-96, score-0.626]
</p><p>52 Within a species, all arrays share the same probe set, so we applied VSN [16] to the arrays from each species to estimate an array-speciﬁc afﬁne transform to transform the probe intensities to species-speciﬁc units. [sent-101, score-0.688]
</p><p>53 First, to remove probe bias in the transformed intensities, we subtracted the row median from each element and then to attempt to transform measurements from different species to a common scale, we subtracted the column means from each element and divided by the column length. [sent-105, score-0.598]
</p><p>54 First, we investigate the stability of our conservation pattern estimates by using parameters trained on different random subsamples of our genes. [sent-106, score-0.29]
</p><p>55 We then evaluate the predictive value of our algorithm BFPA using two tasks: a) predicting gene expression proﬁles in a new species given expression proﬁles in other species, and b) predicting Gene Ontology annotation using the conservation pattern inferred by our model. [sent-107, score-1.774]
</p><p>56 We then estimated P (ci |ˆi , θ) xs for the four other subsets of genes, and classiﬁed each gene into its most likely conservation pattern. [sent-109, score-0.729]
</p><p>57 Hence, each gene is classiﬁed four times by non-overlapping training sets. [sent-110, score-0.428]
</p><p>58 Figure 2 shows that the classiﬁcations are quite stable and that most genes are classiﬁed into few conservation patterns. [sent-111, score-0.483]
</p><p>59 Most genes that were uniquely classiﬁed into a single conservation pattern either were classiﬁed as fully (all) conserved or completely unconserved, resulting in relatively few high-conﬁdence lineagespeciﬁc genes. [sent-112, score-0.676]
</p><p>60 1  Functional associations of co-transcriptionally evolving genes  Pairs of genes exhibiting correlated expression also tend to perform similar function. [sent-114, score-0.791]
</p><p>61 (left) Each gene was placed into one of four bins, denoting the number of unique patterns it was classiﬁed into. [sent-117, score-0.477]
</p><p>62 Most genes were consistently classiﬁed into one conservation pattern for all four of its independent classiﬁcations. [sent-118, score-0.527]
</p><p>63 (right) For all genes uniquely classiﬁed into a single conservation pattern, the number of presentday species adjacent to conserved links was computed. [sent-119, score-1.106]
</p><p>64 Most genes were either classiﬁed as fully (all) conserved or completely unconserved. [sent-120, score-0.385]
</p><p>65 In this section, we introduce the evolutionary correlation coefﬁcient (ECC), a simple modiﬁcation of PCC to integrate model predictions, and examine whether genes with the same annotated function are more similar in rank according the ECC or PCC measures. [sent-122, score-0.438]
</p><p>66 ECC scales the positively-transformed PCC by the marginal probability of the genes following the same expression evolution, assuming independent evolution. [sent-123, score-0.536]
</p><p>67 First, the protein sequences of each gene were aligned using default parameters of MUSCLE [18]. [sent-128, score-0.428]
</p><p>68 These alignments were then inputted into PAML [19] together with the species tree shown in Figure 1 to estimate branch lengths. [sent-129, score-0.586]
</p><p>69 The PCC measure for each pair of genes was then scaled by the Pearson correlation coefﬁcient of the branch lengths estimated by PAML to produce ECC-sequence. [sent-130, score-0.334]
</p><p>70 For all models, we ﬁrst used the ECC/PCC similarity metric for each gene to rank all other genes in order of expression similarity. [sent-131, score-0.985]
</p><p>71 We then apply the Wilcoxon Rank Sum test to evaluate whether genes with the same GO annotations, as annotated for the mouse ortholog, are signiﬁcantly higher in rank than all other genes. [sent-132, score-0.38]
</p><p>72 For this analysis, we only considered GO Process categories which have at least one of the 4770 genes annotated in that category. [sent-133, score-0.304]
</p><p>73 We also removed all genes which were not annotated in any category, resulting in a total of 3319 genes and 4246 categories. [sent-134, score-0.559]
</p><p>74 Figure 3 illustrates the distribution of smallest p-values achieved by each gene over all of their annotated functions. [sent-135, score-0.477]
</p><p>75 A control measure ECC-random is shown, which is computed by randomizing the gene labels of the data in each of the ﬁve organisms before learning. [sent-140, score-0.572]
</p><p>76 Finally, Brown+prior measures the performance of the Brownian model when the conservation pattern priors are allowed to be estimated, and performs better than the Brownian model but worse than BFPA, as expected. [sent-141, score-0.308]
</p><p>77 6  3000  2000  difference in wins  2500  # genes  2000 1500 1000 500 0 5  BFPA Brown+prior Brown PCC ECC−sequence ECC−random 10 −log10(pvalue)  1500  [BFPA]wins − [Brown]wins [BFPA]wins − [baseline]wins  1000 500 0  15  H  M C species  F  Figure 3: Model performance. [sent-143, score-0.773]
</p><p>78 The smallest p-value achieved for each gene across all its annotated functions is used in the distribution. [sent-145, score-0.494]
</p><p>79 Higher lines on the graph translate into stronger associations between expression levels and gene function, which we interpret as better performance. [sent-147, score-0.797]
</p><p>80 (right) This graph shows the difference in the total number of expression values for which a particular method achieves the lowest error, sorted by species. [sent-148, score-0.281]
</p><p>81 2  Reconstruction of gene expression levels  Here we report the performance of our model in predicting the expression level of a gene in each of human, mouse, chicken, and frog, given its expression levels in the other species. [sent-150, score-1.912]
</p><p>82 The model was trained using 100 EM iterations on half of the dataset, which was then used to predict the expression levels for each gene in each species in the other half of the dataset, and vice versa. [sent-152, score-1.289]
</p><p>83 To create a baseline performance measure, we computed the error when using an average of the four other species to predict the expression level of a gene in the ﬁfth species. [sent-153, score-1.203]
</p><p>84 We only compute predictions for the ten matched samples across all species so that we can compare errors made by our model against those of Brownian and the baseline, which require matched samples. [sent-154, score-0.588]
</p><p>85 Figure 3 shows that with the exception of the comparison against Brownian in chicken, BFPA achieves lower error than both Brownian and baseline in predicting expression measurements. [sent-155, score-0.32]
</p><p>86 7  Discussion  We have presented a new model for the simultaneous evolution of gene expression levels across multiple tissues and organs. [sent-156, score-1.134]
</p><p>87 Given expression data from present-day species, our model can be used to simultaneously infer the ancestral expression levels of orthologous genes as well as determine where in the phylogeny the gene expression levels underwent substantial change. [sent-157, score-1.943]
</p><p>88 BFPA extends previous Brownian models [8, 9] by introducing a constrained factor analysis framework to account for complex tissue relationships between different species and by adapting the discrete gamma method [13] to model quantitative gene expression data. [sent-158, score-1.281]
</p><p>89 Our model performs better than other Brownian models in functional association and expression prediction experiments, demonstrating that the evolutionary history we infer better recovers the function of the gene. [sent-159, score-0.42]
</p><p>90 We also showed that gene expression-based phylogenetic data may provide information not contained in sequence-based phylogenetic data in terms of helping predict the functional association of genes. [sent-161, score-0.698]
</p><p>91 Our model has a number of other applications outside of using it to study the evolutionary history of gene expression. [sent-162, score-0.541]
</p><p>92 Our ability to identify genes with conserved expression across multiple species will help in the inference of gene function from annotated to non-annotated species because unconserved expression patterns indicate a likely change in the biological function of a gene. [sent-163, score-2.502]
</p><p>93 We also expect that by identifying species that share a conserved expression pattern, our model will aid in the 7  identiﬁcation of transcriptional cis-regulatory elements by focusing the search for cis-elements to those species identiﬁed as conserved in expression. [sent-164, score-1.507]
</p><p>94 While we have taken different proﬁled samples as representing different tissues, our methodology can be easily expanded to study evolutionary change in gene expression in response to different growth conditions or environmental stresses, as with those studied in [5]. [sent-165, score-0.846]
</p><p>95 Our methodology is also easily extendible to other model organisms for which there are genomes and expression data for multiple closely related species (e. [sent-166, score-0.917]
</p><p>96 We anticipate that the results obtained will be invaluable in the study of genome evolution and identiﬁcation of cis-regulatory elements, whose phylogeny should reﬂect that of the gene expression patterns. [sent-169, score-0.995]
</p><p>97 (2007) The evolution of gene regulation by transcription factors and microRNAs. [sent-179, score-0.529]
</p><p>98 (2005) bloodthirsty, an RBCC/TRIM gene required for erythropoiesis in zebraﬁsh. [sent-186, score-0.428]
</p><p>99 (2006) A genetic signature of interspecies variations in gene expression. [sent-202, score-0.428]
</p><p>100 (2004) Statistical framework for phylogenomic analysis of gene family expression proﬁles. [sent-218, score-0.709]
</p>
<br/>
<h2>similar papers computed by tfidf model</h2><h3>tfidf for this paper:</h3><p>wordName wordTfidf (topN-words)</p>
<p>[('species', 0.474), ('gene', 0.428), ('expression', 0.281), ('genes', 0.255), ('conservation', 0.228), ('tissues', 0.201), ('brownian', 0.193), ('kj', 0.182), ('bfpa', 0.149), ('organisms', 0.144), ('phylogeny', 0.141), ('pcc', 0.135), ('conserved', 0.13), ('phylogenetic', 0.122), ('evolution', 0.101), ('ecc', 0.095), ('evolutionary', 0.095), ('levels', 0.088), ('branch', 0.079), ('ci', 0.075), ('xs', 0.073), ('probe', 0.055), ('ancestral', 0.055), ('mouse', 0.055), ('tissue', 0.054), ('ontology', 0.054), ('outgroup', 0.054), ('pro', 0.053), ('annotated', 0.049), ('patterns', 0.049), ('chicken', 0.047), ('frog', 0.047), ('root', 0.047), ('measurements', 0.047), ('xi', 0.045), ('pattern', 0.044), ('wins', 0.044), ('bone', 0.041), ('marrow', 0.041), ('paml', 0.041), ('unconserved', 0.041), ('xo', 0.041), ('motion', 0.036), ('felsenstein', 0.036), ('homologous', 0.036), ('vertebrates', 0.036), ('branches', 0.035), ('tree', 0.033), ('yeast', 0.033), ('les', 0.032), ('brown', 0.031), ('hypothetical', 0.03), ('muscle', 0.03), ('matched', 0.03), ('ij', 0.028), ('khaitovich', 0.027), ('kidney', 0.027), ('ortholog', 0.027), ('orthologous', 0.027), ('spleen', 0.027), ('tetraodon', 0.027), ('constrained', 0.026), ('hm', 0.026), ('functional', 0.026), ('intensity', 0.026), ('ancestor', 0.025), ('microarray', 0.025), ('array', 0.025), ('intensities', 0.024), ('diverged', 0.024), ('mammals', 0.024), ('change', 0.023), ('divergence', 0.022), ('transform', 0.022), ('anticipate', 0.022), ('functionally', 0.022), ('invaluable', 0.022), ('wilcoxon', 0.022), ('child', 0.021), ('rank', 0.021), ('improper', 0.02), ('pearson', 0.02), ('comparative', 0.02), ('rapid', 0.02), ('baseline', 0.02), ('samples', 0.019), ('ancestors', 0.019), ('gu', 0.019), ('incident', 0.019), ('uniquely', 0.019), ('classi', 0.019), ('predicting', 0.019), ('arrays', 0.018), ('annotations', 0.018), ('diverse', 0.018), ('yang', 0.018), ('model', 0.018), ('stability', 0.018), ('drift', 0.018), ('across', 0.017)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.99999994 <a title="9-tfidf-1" href="./nips-2008-A_mixture_model_for_the_evolution_of_gene_expression_in_non-homogeneous_datasets.html">9 nips-2008-A mixture model for the evolution of gene expression in non-homogeneous datasets</a></p>
<p>Author: Gerald Quon, Yee W. Teh, Esther Chan, Timothy Hughes, Michael Brudno, Quaid D. Morris</p><p>Abstract: We address the challenge of assessing conservation of gene expression in complex, non-homogeneous datasets. Recent studies have demonstrated the success of probabilistic models in studying the evolution of gene expression in simple eukaryotic organisms such as yeast, for which measurements are typically scalar and independent. Models capable of studying expression evolution in much more complex organisms such as vertebrates are particularly important given the medical and scientiﬁc interest in species such as human and mouse. We present Brownian Factor Phylogenetic Analysis, a statistical model that makes a number of signiﬁcant extensions to previous models to enable characterization of changes in expression among highly complex organisms. We demonstrate the efﬁcacy of our method on a microarray dataset proﬁling diverse tissues from multiple vertebrate species. We anticipate that the model will be invaluable in the study of gene expression patterns in other diverse organisms as well, such as worms and insects. 1</p><p>2 0.1633459 <a title="9-tfidf-2" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman’s coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis. 1</p><p>3 0.14977659 <a title="9-tfidf-3" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>Author: Shuiwang Ji, Liang Sun, Rong Jin, Jieping Ye</p><p>Abstract: We present a multi-label multiple kernel learning (MKL) formulation in which the data are embedded into a low-dimensional space directed by the instancelabel correlations encoded into a hypergraph. We formulate the problem in the kernel-induced feature space and propose to learn the kernel matrix as a linear combination of a given collection of kernel matrices in the MKL framework. The proposed learning formulation leads to a non-smooth min-max problem, which can be cast into a semi-inﬁnite linear program (SILP). We further propose an approximate formulation with a guaranteed error bound which involves an unconstrained convex optimization problem. In addition, we show that the objective function of the approximate formulation is differentiable with Lipschitz continuous gradient, and hence existing methods can be employed to compute the optimal solution efﬁciently. We apply the proposed formulation to the automated annotation of Drosophila gene expression pattern images, and promising results have been reported in comparison with representative algorithms.</p><p>4 0.13611455 <a title="9-tfidf-4" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>Author: Steven J. Phillips, Miroslav Dudík</p><p>Abstract: We apply robust Bayesian decision theory to improve both generative and discriminative learners under bias in class proportions in labeled training data, when the true class proportions are unknown. For the generative case, we derive an entropybased weighting that maximizes expected log likelihood under the worst-case true class proportions. For the discriminative case, we derive a multinomial logistic model that minimizes worst-case conditional log loss. We apply our theory to the modeling of species geographic distributions from presence data, an extreme case of labeling bias since there is no absence data. On a benchmark dataset, we ﬁnd that entropy-based weighting offers an improvement over constant estimates of class proportions, consistently reducing log loss on unbiased test data. 1</p><p>5 0.12130242 <a title="9-tfidf-5" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>Author: Neil D. Lawrence, Magnus Rattray, Michalis K. Titsias</p><p>Abstract: Sampling functions in Gaussian process (GP) models is challenging because of the highly correlated posterior distribution. We describe an efﬁcient Markov chain Monte Carlo algorithm for sampling from the posterior process of the GP model. This algorithm uses control variables which are auxiliary function values that provide a low dimensional representation of the function. At each iteration, the algorithm proposes new values for the control variables and generates the function from the conditional GP prior. The control variable input locations are found by minimizing an objective function. We demonstrate the algorithm on regression and classiﬁcation problems and we use it to estimate the parameters of a differential equation model of gene regulation. 1</p><p>6 0.113368 <a title="9-tfidf-6" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>7 0.1109345 <a title="9-tfidf-7" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>8 0.11015169 <a title="9-tfidf-8" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>9 0.097342454 <a title="9-tfidf-9" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>10 0.09077125 <a title="9-tfidf-10" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>11 0.077040836 <a title="9-tfidf-11" href="./nips-2008-A_spatially_varying_two-sample_recombinant_coalescent%2C_with_applications_to_HIV_escape_response.html">11 nips-2008-A spatially varying two-sample recombinant coalescent, with applications to HIV escape response</a></p>
<p>12 0.061129238 <a title="9-tfidf-12" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>13 0.036340501 <a title="9-tfidf-13" href="./nips-2008-Cyclizing_Clusters_via_Zeta_Function_of_a_Graph.html">55 nips-2008-Cyclizing Clusters via Zeta Function of a Graph</a></p>
<p>14 0.034755021 <a title="9-tfidf-14" href="./nips-2008-Self-organization_using_synaptic_plasticity.html">204 nips-2008-Self-organization using synaptic plasticity</a></p>
<p>15 0.033495195 <a title="9-tfidf-15" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>16 0.033030998 <a title="9-tfidf-16" href="./nips-2008-High-dimensional_support_union_recovery_in_multivariate_regression.html">99 nips-2008-High-dimensional support union recovery in multivariate regression</a></p>
<p>17 0.032823801 <a title="9-tfidf-17" href="./nips-2008-Clusters_and_Coarse_Partitions_in_LP_Relaxations.html">49 nips-2008-Clusters and Coarse Partitions in LP Relaxations</a></p>
<p>18 0.031638421 <a title="9-tfidf-18" href="./nips-2008-Mixed_Membership_Stochastic_Blockmodels.html">134 nips-2008-Mixed Membership Stochastic Blockmodels</a></p>
<p>19 0.031127449 <a title="9-tfidf-19" href="./nips-2008-Unsupervised_Learning_of_Visual_Sense_Models_for_Polysemous_Words.html">246 nips-2008-Unsupervised Learning of Visual Sense Models for Polysemous Words</a></p>
<p>20 0.031036934 <a title="9-tfidf-20" href="./nips-2008-Model_selection_and_velocity_estimation_using_novel_priors_for_motion_patterns.html">136 nips-2008-Model selection and velocity estimation using novel priors for motion patterns</a></p>
<br/>
<h2>similar papers computed by <a title="lsi-model" href="../home/nips2008_lsi.html">lsi model</a></h2><h3>lsi for this paper:</h3><p>topicId topicWeight</p>
<p>[(0, -0.128), (1, -0.025), (2, 0.037), (3, 0.032), (4, 0.075), (5, -0.074), (6, 0.018), (7, 0.115), (8, 0.041), (9, 0.013), (10, 0.038), (11, 0.005), (12, 0.09), (13, -0.15), (14, 0.126), (15, 0.029), (16, -0.086), (17, -0.024), (18, 0.032), (19, -0.014), (20, 0.014), (21, 0.249), (22, -0.207), (23, -0.046), (24, -0.182), (25, 0.034), (26, -0.094), (27, 0.098), (28, 0.096), (29, -0.141), (30, 0.073), (31, 0.052), (32, 0.037), (33, 0.006), (34, 0.016), (35, 0.05), (36, -0.058), (37, -0.024), (38, -0.065), (39, -0.001), (40, 0.053), (41, -0.059), (42, 0.146), (43, 0.019), (44, 0.054), (45, -0.171), (46, -0.081), (47, -0.13), (48, 0.106), (49, 0.046)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.96815187 <a title="9-lsi-1" href="./nips-2008-A_mixture_model_for_the_evolution_of_gene_expression_in_non-homogeneous_datasets.html">9 nips-2008-A mixture model for the evolution of gene expression in non-homogeneous datasets</a></p>
<p>Author: Gerald Quon, Yee W. Teh, Esther Chan, Timothy Hughes, Michael Brudno, Quaid D. Morris</p><p>Abstract: We address the challenge of assessing conservation of gene expression in complex, non-homogeneous datasets. Recent studies have demonstrated the success of probabilistic models in studying the evolution of gene expression in simple eukaryotic organisms such as yeast, for which measurements are typically scalar and independent. Models capable of studying expression evolution in much more complex organisms such as vertebrates are particularly important given the medical and scientiﬁc interest in species such as human and mouse. We present Brownian Factor Phylogenetic Analysis, a statistical model that makes a number of signiﬁcant extensions to previous models to enable characterization of changes in expression among highly complex organisms. We demonstrate the efﬁcacy of our method on a microarray dataset proﬁling diverse tissues from multiple vertebrate species. We anticipate that the model will be invaluable in the study of gene expression patterns in other diverse organisms as well, such as worms and insects. 1</p><p>2 0.65097141 <a title="9-lsi-2" href="./nips-2008-A_spatially_varying_two-sample_recombinant_coalescent%2C_with_applications_to_HIV_escape_response.html">11 nips-2008-A spatially varying two-sample recombinant coalescent, with applications to HIV escape response</a></p>
<p>Author: Alexander Braunstein, Zhi Wei, Shane T. Jensen, Jon D. Mcauliffe</p><p>Abstract: Statistical evolutionary models provide an important mechanism for describing and understanding the escape response of a viral population under a particular therapy. We present a new hierarchical model that incorporates spatially varying mutation and recombination rates at the nucleotide level. It also maintains separate parameters for treatment and control groups, which allows us to estimate treatment effects explicitly. We use the model to investigate the sequence evolution of HIV populations exposed to a recently developed antisense gene therapy, as well as a more conventional drug therapy. The detection of biologically relevant and plausible signals in both therapy studies demonstrates the effectiveness of the method. 1</p><p>3 0.49799299 <a title="9-lsi-3" href="./nips-2008-The_Infinite_Hierarchical_Factor_Regression_Model.html">235 nips-2008-The Infinite Hierarchical Factor Regression Model</a></p>
<p>Author: Piyush Rai, Hal Daume</p><p>Abstract: We propose a nonparametric Bayesian factor regression model that accounts for uncertainty in the number of factors, and the relationship between factors. To accomplish this, we propose a sparse variant of the Indian Buffet Process and couple this with a hierarchical model over factors, based on Kingman’s coalescent. We apply this model to two problems (factor analysis and factor regression) in gene-expression data analysis. 1</p><p>4 0.46926755 <a title="9-lsi-4" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>Author: Alexandre Bouchard-côté, Dan Klein, Michael I. Jordan</p><p>Abstract: Accurate and efﬁcient inference in evolutionary trees is a central problem in computational biology. While classical treatments have made unrealistic site independence assumptions, ignoring insertions and deletions, realistic approaches require tracking insertions and deletions along the phylogenetic tree—a challenging and unsolved computational problem. We propose a new ancestry resampling procedure for inference in evolutionary trees. We evaluate our method in two problem domains—multiple sequence alignment and reconstruction of ancestral sequences—and show substantial improvement over the current state of the art. 1</p><p>5 0.45246494 <a title="9-lsi-5" href="./nips-2008-Nonparametric_regression_and_classification_with_joint_sparsity_constraints.html">155 nips-2008-Nonparametric regression and classification with joint sparsity constraints</a></p>
<p>Author: Han Liu, Larry Wasserman, John D. Lafferty</p><p>Abstract: We propose new families of models and algorithms for high-dimensional nonparametric learning with joint sparsity constraints. Our approach is based on a regularization method that enforces common sparsity patterns across different function components in a nonparametric additive model. The algorithms employ a coordinate descent approach that is based on a functional soft-thresholding operator. The framework yields several new models, including multi-task sparse additive models, multi-response sparse additive models, and sparse additive multi-category logistic regression. The methods are illustrated with experiments on synthetic data and gene microarray data. 1</p><p>6 0.43498668 <a title="9-lsi-6" href="./nips-2008-Multi-label_Multiple_Kernel_Learning.html">143 nips-2008-Multi-label Multiple Kernel Learning</a></p>
<p>7 0.41557047 <a title="9-lsi-7" href="./nips-2008-Efficient_Sampling_for_Gaussian_Process_Inference_using_Control_Variables.html">71 nips-2008-Efficient Sampling for Gaussian Process Inference using Control Variables</a></p>
<p>8 0.40602821 <a title="9-lsi-8" href="./nips-2008-An_Efficient_Sequential_Monte_Carlo_Algorithm_for_Coalescent_Clustering.html">18 nips-2008-An Efficient Sequential Monte Carlo Algorithm for Coalescent Clustering</a></p>
<p>9 0.4031055 <a title="9-lsi-9" href="./nips-2008-Non-stationary_dynamic_Bayesian_networks.html">152 nips-2008-Non-stationary dynamic Bayesian networks</a></p>
<p>10 0.38039249 <a title="9-lsi-10" href="./nips-2008-Multi-task_Gaussian_Process_Learning_of_Robot_Inverse_Dynamics.html">146 nips-2008-Multi-task Gaussian Process Learning of Robot Inverse Dynamics</a></p>
<p>11 0.37303969 <a title="9-lsi-11" href="./nips-2008-Accelerating_Bayesian_Inference_over_Nonlinear_Differential_Equations_with_Gaussian_Processes.html">12 nips-2008-Accelerating Bayesian Inference over Nonlinear Differential Equations with Gaussian Processes</a></p>
<p>12 0.34824583 <a title="9-lsi-12" href="./nips-2008-Generative_and_Discriminative_Learning_with_Unknown_Labeling_Bias.html">91 nips-2008-Generative and Discriminative Learning with Unknown Labeling Bias</a></p>
<p>13 0.31250155 <a title="9-lsi-13" href="./nips-2008-An_Empirical_Analysis_of_Domain_Adaptation_Algorithms_for_Genomic_Sequence_Analysis.html">19 nips-2008-An Empirical Analysis of Domain Adaptation Algorithms for Genomic Sequence Analysis</a></p>
<p>14 0.28083172 <a title="9-lsi-14" href="./nips-2008-An_Extended_Level_Method_for_Efficient_Multiple_Kernel_Learning.html">20 nips-2008-An Extended Level Method for Efficient Multiple Kernel Learning</a></p>
<p>15 0.27281284 <a title="9-lsi-15" href="./nips-2008-Learning_Bounded_Treewidth_Bayesian_Networks.html">115 nips-2008-Learning Bounded Treewidth Bayesian Networks</a></p>
<p>16 0.24548003 <a title="9-lsi-16" href="./nips-2008-The_Mondrian_Process.html">236 nips-2008-The Mondrian Process</a></p>
<p>17 0.19997327 <a title="9-lsi-17" href="./nips-2008-Model_selection_and_velocity_estimation_using_novel_priors_for_motion_patterns.html">136 nips-2008-Model selection and velocity estimation using novel priors for motion patterns</a></p>
<p>18 0.19309756 <a title="9-lsi-18" href="./nips-2008-Gaussian-process_factor_analysis_for_low-dimensional_single-trial_analysis_of_neural_population_activity.html">90 nips-2008-Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</a></p>
<p>19 0.19157788 <a title="9-lsi-19" href="./nips-2008-Predicting_the_Geometry_of_Metal_Binding_Sites_from_Protein_Sequence.html">183 nips-2008-Predicting the Geometry of Metal Binding Sites from Protein Sequence</a></p>
<p>20 0.19003712 <a title="9-lsi-20" href="./nips-2008-Sparse_Signal_Recovery_Using_Markov_Random_Fields.html">215 nips-2008-Sparse Signal Recovery Using Markov Random Fields</a></p>
<br/>
<h2>similar papers computed by <a title="lda-model" href="../home/nips2008_lda.html">lda model</a></h2><h3>lda for this paper:</h3><p>topicId topicWeight</p>
<p>[(6, 0.031), (7, 0.068), (12, 0.042), (15, 0.014), (28, 0.184), (57, 0.062), (59, 0.018), (63, 0.016), (67, 0.312), (71, 0.048), (77, 0.026), (78, 0.013), (83, 0.038), (87, 0.021)]</p>
<h3>similar papers list:</h3><p>simIndex simValue paperId paperTitle</p>
<p>same-paper 1 0.77122986 <a title="9-lda-1" href="./nips-2008-A_mixture_model_for_the_evolution_of_gene_expression_in_non-homogeneous_datasets.html">9 nips-2008-A mixture model for the evolution of gene expression in non-homogeneous datasets</a></p>
<p>Author: Gerald Quon, Yee W. Teh, Esther Chan, Timothy Hughes, Michael Brudno, Quaid D. Morris</p><p>Abstract: We address the challenge of assessing conservation of gene expression in complex, non-homogeneous datasets. Recent studies have demonstrated the success of probabilistic models in studying the evolution of gene expression in simple eukaryotic organisms such as yeast, for which measurements are typically scalar and independent. Models capable of studying expression evolution in much more complex organisms such as vertebrates are particularly important given the medical and scientiﬁc interest in species such as human and mouse. We present Brownian Factor Phylogenetic Analysis, a statistical model that makes a number of signiﬁcant extensions to previous models to enable characterization of changes in expression among highly complex organisms. We demonstrate the efﬁcacy of our method on a microarray dataset proﬁling diverse tissues from multiple vertebrate species. We anticipate that the model will be invaluable in the study of gene expression patterns in other diverse organisms as well, such as worms and insects. 1</p><p>2 0.63957047 <a title="9-lda-2" href="./nips-2008-A_Scalable_Hierarchical_Distributed_Language_Model.html">4 nips-2008-A Scalable Hierarchical Distributed Language Model</a></p>
<p>Author: Andriy Mnih, Geoffrey E. Hinton</p><p>Abstract: Neural probabilistic language models (NPLMs) have been shown to be competitive with and occasionally superior to the widely-used n-gram language models. The main drawback of NPLMs is their extremely long training and testing times. Morin and Bengio have proposed a hierarchical language model built around a binary tree of words, which was two orders of magnitude faster than the nonhierarchical model it was based on. However, it performed considerably worse than its non-hierarchical counterpart in spite of using a word tree created using expert knowledge. We introduce a fast hierarchical language model along with a simple feature-based algorithm for automatic construction of word trees from the data. We then show that the resulting models can outperform non-hierarchical neural models as well as the best n-gram models. 1</p><p>3 0.5653916 <a title="9-lda-3" href="./nips-2008-Modeling_human_function_learning_with_Gaussian_processes.html">138 nips-2008-Modeling human function learning with Gaussian processes</a></p>
<p>Author: Thomas L. Griffiths, Chris Lucas, Joseph Williams, Michael L. Kalish</p><p>Abstract: Accounts of how people learn functional relationships between continuous variables have tended to focus on two possibilities: that people are estimating explicit functions, or that they are performing associative learning supported by similarity. We provide a rational analysis of function learning, drawing on work on regression in machine learning and statistics. Using the equivalence of Bayesian linear regression and Gaussian processes, we show that learning explicit rules and using similarity can be seen as two views of one solution to this problem. We use this insight to deﬁne a Gaussian process model of human function learning that combines the strengths of both approaches. 1</p><p>4 0.56409162 <a title="9-lda-4" href="./nips-2008-Temporal_Dynamics_of_Cognitive_Control.html">231 nips-2008-Temporal Dynamics of Cognitive Control</a></p>
<p>Author: Jeremy Reynolds, Michael C. Mozer</p><p>Abstract: Cognitive control refers to the ﬂexible deployment of memory and attention in response to task demands and current goals. Control is often studied experimentally by presenting sequences of stimuli, some demanding a response, and others modulating the stimulus-response mapping. In these tasks, participants must maintain information about the current stimulus-response mapping in working memory. Prominent theories of cognitive control use recurrent neural nets to implement working memory, and optimize memory utilization via reinforcement learning. We present a novel perspective on cognitive control in which working memory representations are intrinsically probabilistic, and control operations that maintain and update working memory are dynamically determined via probabilistic inference. We show that our model provides a parsimonious account of behavioral and neuroimaging data, and suggest that it offers an elegant conceptualization of control in which behavior can be cast as optimal, subject to limitations on learning and the rate of information processing. Moreover, our model provides insight into how task instructions can be directly translated into appropriate behavior and then efﬁciently reﬁned with subsequent task experience. 1</p><p>5 0.56185722 <a title="9-lda-5" href="./nips-2008-Learning_Transformational_Invariants_from_Natural_Movies.html">118 nips-2008-Learning Transformational Invariants from Natural Movies</a></p>
<p>Author: Charles Cadieu, Bruno A. Olshausen</p><p>Abstract: We describe a hierarchical, probabilistic model that learns to extract complex motion from movies of the natural environment. The model consists of two hidden layers: the ﬁrst layer produces a sparse representation of the image that is expressed in terms of local amplitude and phase variables. The second layer learns the higher-order structure among the time-varying phase variables. After training on natural movies, the top layer units discover the structure of phase-shifts within the ﬁrst layer. We show that the top layer units encode transformational invariants: they are selective for the speed and direction of a moving pattern, but are invariant to its spatial structure (orientation/spatial-frequency). The diversity of units in both the intermediate and top layers of the model provides a set of testable predictions for representations that might be found in V1 and MT. In addition, the model demonstrates how feedback from higher levels can inﬂuence representations at lower levels as a by-product of inference in a graphical model. 1</p><p>6 0.56124073 <a title="9-lda-6" href="./nips-2008-Partially_Observed_Maximum_Entropy_Discrimination_Markov_Networks.html">176 nips-2008-Partially Observed Maximum Entropy Discrimination Markov Networks</a></p>
<p>7 0.56111848 <a title="9-lda-7" href="./nips-2008-Bounds_on_marginal_probability_distributions.html">40 nips-2008-Bounds on marginal probability distributions</a></p>
<p>8 0.56091005 <a title="9-lda-8" href="./nips-2008-Efficient_Inference_in_Phylogenetic_InDel_Trees.html">70 nips-2008-Efficient Inference in Phylogenetic InDel Trees</a></p>
<p>9 0.56058323 <a title="9-lda-9" href="./nips-2008-Model_Selection_in_Gaussian_Graphical_Models%3A_High-Dimensional_Consistency_of_%5Cboldmath%24%5Cell_1%24-regularized_MLE.html">135 nips-2008-Model Selection in Gaussian Graphical Models: High-Dimensional Consistency of \boldmath$\ell 1$-regularized MLE</a></p>
<p>10 0.56048155 <a title="9-lda-10" href="./nips-2008-MAS%3A_a_multiplicative_approximation_scheme_for_probabilistic_inference.html">129 nips-2008-MAS: a multiplicative approximation scheme for probabilistic inference</a></p>
<p>11 0.56009364 <a title="9-lda-11" href="./nips-2008-Using_Bayesian_Dynamical_Systems_for_Motion_Template_Libraries.html">247 nips-2008-Using Bayesian Dynamical Systems for Motion Template Libraries</a></p>
<p>12 0.55980319 <a title="9-lda-12" href="./nips-2008-Continuously-adaptive_discretization_for_message-passing_algorithms.html">50 nips-2008-Continuously-adaptive discretization for message-passing algorithms</a></p>
<p>13 0.55971813 <a title="9-lda-13" href="./nips-2008-Improved_Moves_for_Truncated_Convex_Models.html">104 nips-2008-Improved Moves for Truncated Convex Models</a></p>
<p>14 0.55896062 <a title="9-lda-14" href="./nips-2008-Kernelized_Sorting.html">113 nips-2008-Kernelized Sorting</a></p>
<p>15 0.55810022 <a title="9-lda-15" href="./nips-2008-Spectral_Clustering_with_Perturbed_Data.html">218 nips-2008-Spectral Clustering with Perturbed Data</a></p>
<p>16 0.55797696 <a title="9-lda-16" href="./nips-2008-Relative_Performance_Guarantees_for_Approximate_Inference_in_Latent_Dirichlet_Allocation.html">197 nips-2008-Relative Performance Guarantees for Approximate Inference in Latent Dirichlet Allocation</a></p>
<p>17 0.5577597 <a title="9-lda-17" href="./nips-2008-Influence_of_graph_construction_on_graph-based_clustering_measures.html">107 nips-2008-Influence of graph construction on graph-based clustering measures</a></p>
<p>18 0.55749917 <a title="9-lda-18" href="./nips-2008-Simple_Local_Models_for_Complex_Dynamical_Systems.html">211 nips-2008-Simple Local Models for Complex Dynamical Systems</a></p>
<p>19 0.55738974 <a title="9-lda-19" href="./nips-2008-Bayesian_Network_Score_Approximation_using_a_Metagraph_Kernel.html">34 nips-2008-Bayesian Network Score Approximation using a Metagraph Kernel</a></p>
<p>20 0.55717045 <a title="9-lda-20" href="./nips-2008-ICA_based_on_a_Smooth_Estimation_of_the_Differential_Entropy.html">102 nips-2008-ICA based on a Smooth Estimation of the Differential Entropy</a></p>
<br/><br/><br/>

<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-48522588-1', 'makerhacker.github.io');
ga('send', 'pageview');
</script>

</body>
</html>
